---
ver: rpa2
title: 'A Survey of Personalization: From RAG to Agent'
arxiv_id: '2504.10147'
source_url: https://arxiv.org/abs/2504.10147
tags:
- personalized
- arxiv
- user
- retrieval
- personalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey systematically examines personalization in Retrieval-Augmented\
  \ Generation (RAG) and agent-based systems across three core RAG stages\u2014pre-retrieval,\
  \ retrieval, and generation\u2014and extends to agentic capabilities like understanding,\
  \ planning, execution, and dynamic generation. It provides formal definitions, comprehensive\
  \ literature reviews, and summaries of key datasets and evaluation metrics."
---

# A Survey of Personalization: From RAG to Agent

## Quick Facts
- **arXiv ID:** 2504.10147
- **Source URL:** https://arxiv.org/abs/2504.10147
- **Reference count:** 40
- **Primary result:** Systematic examination of personalization in RAG and agent-based systems across pre-retrieval, retrieval, generation, and agentic capabilities with definitions, literature reviews, datasets, and metrics.

## Executive Summary
This survey provides a comprehensive analysis of personalization techniques in Retrieval-Augmented Generation (RAG) and agent-based systems. The authors systematically examine personalization across three core RAG stages—pre-retrieval, retrieval, and generation—and extend to agentic capabilities including understanding, planning, execution, and dynamic generation. The study provides formal definitions, comprehensive literature reviews, and summaries of key datasets and evaluation metrics while highlighting challenges such as balancing personalization with scalability, developing effective evaluation metrics, preserving privacy through device-cloud collaboration, and ensuring ethical, coherent systems.

## Method Summary
The authors conducted a systematic survey of personalization techniques in RAG and agent-based systems, categorizing approaches by their implementation stage (pre-retrieval, retrieval, generation) and mechanism (explicit vs. implicit personalization). The survey synthesizes existing literature, identifies key datasets and evaluation metrics, and proposes future directions for research. The methodology involves reviewing papers across multiple domains including recommendation systems, federated learning, and agentic workflows to provide a comprehensive overview of the field.

## Key Results
- Personalization can be implemented at three core RAG stages: pre-retrieval (query reformulation), retrieval (personalized indexing/reranking), and generation (explicit prompting or implicit fine-tuning)
- Agentic systems represent an evolution from static RAG by incorporating dynamic memory management and tool usage capabilities
- Key challenges include balancing personalization with scalability, developing effective evaluation metrics, preserving privacy through device-cloud collaboration, and ensuring ethical, coherent systems
- Future directions include lightweight frameworks, specialized benchmarks, and privacy-preserving techniques to advance personalized AI systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting user context ($p$) during pre-retrieval significantly enhances semantic relevance for ambiguous queries.
- **Mechanism:** Uses query reformulation function $q^* = Q(q, p)$ with user history/profile to rewrite queries before search.
- **Core assumption:** User history accurately reflects current intent without noise.
- **Evidence anchors:** Abstract mentions "tailoring of model predictions... to align with individual's preferences" and examining "pre-retrieval"; Section 4.1 defines pre-retrieval as enhancing query alignment; corpus evidence supports efficacy of retrieving from user history.
- **Break condition:** Stale user profile or exploratory queries where intent diverges from history.

### Mechanism 2
- **Claim:** Internalizing user preferences into model parameters allows more efficient fine-grained style alignment than explicit prompting.
- **Mechanism:** Generator uses Parameter-Efficient Fine-Tuning (e.g., LoRA) to embed user preferences into model weights.
- **Core assumption:** Sufficient user data exists for fine-tuning and inference hardware supports user-specific adapters.
- **Evidence anchors:** Section 4.3.3 contrasts implicit vs explicit methods, noting implicit adapts parameters for "fine-grained personalization"; Section 4.3.4 discusses internalizing info into parameters; corpus evidence on federated settings aligns with privacy challenges.
- **Break condition:** Low-data regimes or cold-start scenarios without sufficient user history.

### Mechanism 3
- **Claim:** "Personalized Agents" outperform static RAG pipelines in complex tasks using dynamic memory graphs and tool usage.
- **Mechanism:** Architecture evolves to "Personalized RAG++" where agents use dynamic memory management and API calling instead of static document indices.
- **Core assumption:** Tasks require multi-step reasoning or environmental interaction beyond simple lookup.
- **Evidence anchors:** Abstract extends capabilities to "agentic functionalities, including user understanding, personalized planning and execution"; Section 4.4.1 defines agents as "Personalized RAG++" with persistent memory; corpus evidence supports shift to agentic frameworks for dynamic scenarios.
- **Break condition:** Unreliable tool APIs or memory graph becomes too large causing latency or "catastrophic forgetting."

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** This is the substrate upon which all surveyed personalization techniques are built. Without understanding standard RAG, modifications for personalization are unintelligible.
  - **Quick check question:** Can you distinguish between the "Retriever" ($R$) and the "Generator" ($G$) in the standard RAG pipeline?

- **Concept: Explicit vs. Implicit User Profiles**
  - **Why needed here:** The survey categorizes personalization techniques based on whether they use direct prompting (Explicit) or model fine-tuning (Implicit). Understanding this trade-off is crucial for architectural decisions.
  - **Quick check question:** Does "Implicit Preference" injection require modifying the model weights ($\theta$), or can it be done at inference time via the prompt?

- **Concept: Agentic Workflows**
  - **Why needed here:** The paper argues for convergence where RAG systems become agents. Understanding agents possess "planning" and "execution" capabilities (unlike static RAG) is necessary to grasp the "From RAG to Agent" evolution.
  - **Quick check question:** What distinguishes an "Agent" from a standard RAG system in terms of memory usage? (Hint: Static vs. Editable).

## Architecture Onboarding

- **Component map:** User Context ($p$) -> Pre-Retrieval ($Q$) -> Retrieval ($R$) -> Post-Retrieval -> Generator ($G$) -> Agent Layer (Memory Graph + Tool APIs)

- **Critical path:**
  1. Ingest User Query ($q$) + User Data ($p$)
  2. **Pre-Retrieval:** Rewrite query to reflect $p$ (crucial for relevance)
  3. **Retrieval:** Fetch documents from $C$; if Agent, query Memory Graph/Tools
  4. **Generation:** Synthesize output using explicit $p$ in prompt OR implicit $p$ in adapter weights

- **Design tradeoffs:**
  - Scalability vs. Specificity: Integrating personalization data increases computational complexity
  - Privacy vs. Performance: On-device processing (privacy) vs. Cloud-based heavy retrieval (performance)
  - Explicit vs. Implicit: Explicit prompting is easier to implement but may hit context limits; Implicit tuning is computationally expensive to train per user

- **Failure signatures:**
  - Hallucinated Preferences: Model may over-fit to historical biases in implicit tuning
  - Context Window Exhaustion: Explicit prompting with long user histories can degrade performance
  - Static Drift: Non-agentic RAG fails to capture evolving preferences if index isn't updated

- **First 3 experiments:**
  1. **Baseline Alignment:** Implement standard RAG pipeline with simple "Pre-Retrieval" query rewriter using basic user profile text. Measure retrieval precision (MRR) to validate Mechanism 1.
  2. **Implicit vs. Explicit Test:** Compare Prompt-Based personalization approach against LoRA-based adapter approach. Evaluate on style consistency vs. factual correctness.
  3. **Agent Memory Simulation:** Replace static vector store with "Editable Memory Graph" concept. Test if system can answer queries requiring dynamic memory updates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can specialized benchmarks and metrics be developed to effectively evaluate the long-term adaptability and nuanced alignment of personalized RAG systems?
- **Basis in paper:** Authors state current metrics like BLEU and ROUGE "fall short in capturing the nuanced alignment of outputs with dynamic user preferences."
- **Why unresolved:** Existing evaluations focus on surface textual quality rather than user-centric satisfaction, lacking measures for adaptability to changing preferences.
- **What evidence would resolve it:** New benchmarks demonstrating strong correlation with longitudinal user satisfaction and ability to track preference shifts over time.

### Open Question 2
- **Question:** How can device-cloud collaboration architectures balance requirement for sensitive user data in personalized retrieval with strict privacy regulations?
- **Basis in paper:** Paper highlights challenge of "Preserving Privacy through Device–Cloud Collaboration" given global data protection regulations like GDPR.
- **Why unresolved:** Personalization typically requires processing sensitive history on cloud, conflicting with privacy mandates and raising security concerns.
- **What evidence would resolve it:** Frameworks where on-device small models successfully process sensitive data locally while orchestrating with cloud LLMs without raw data exposure.

### Open Question 3
- **Question:** How can personalized constraints be integrated into existing agent planning frameworks to enhance user experience without introducing computational bottlenecks?
- **Basis in paper:** Authors note "Personalized Agent Planning" is in early stages and "incorporation of personalized approaches has yet to be widely adopted" in current frameworks.
- **Why unresolved:** Current agent research focuses on foundational task execution rather than tailoring plans to individual user constraints or contexts.
- **What evidence would resolve it:** Agent architectures that successfully modify multi-step plans based on user profiles, demonstrating improved utility in standardized personalized agent benchmarks.

## Limitations
- Survey synthesizes broad field but faces trade-offs in breadth versus depth, with many techniques described conceptually without detailed implementation specifics
- Distinction between truly "agentic" systems and sophisticated RAG pipelines remains somewhat fluid, particularly around what constitutes "persistent memory"
- Privacy considerations are mentioned but lack quantitative analysis of threat models or attack surfaces
- Survey does not address economic feasibility of per-user fine-tuning at scale, which represents a significant practical barrier

## Confidence

- **High confidence:** The three-stage RAG personalization framework (pre-retrieval, retrieval, generation) and its basic mechanisms are well-established and supported by multiple citations
- **Medium confidence:** The convergence narrative from RAG to agents is compelling but relies on extrapolating current trends; boundary between "advanced RAG" and "agent" remains subjective
- **Low confidence:** Claims about superiority of implicit personalization over explicit methods lack comparative benchmarks, and assertion that agents inherently outperform static RAG pipelines is not empirically validated

## Next Checks
1. **Implement controlled comparison:** Build parallel pipelines using same dataset - one with explicit preference injection (prompt engineering) and one with implicit personalization (LoRA fine-tuning) - and measure both relevance metrics and style consistency
2. **Benchmark agentic vs. static RAG:** Create multi-turn task requiring dynamic memory updates and tool usage, comparing performance and latency between standard RAG pipeline and agentic framework with editable memory graphs
3. **Privacy-performance trade-off quantification:** Implement same personalization technique on-device versus cloud-based, measuring accuracy degradation alongside concrete privacy risk assessments (e.g., membership inference attack success rates)