---
ver: rpa2
title: 'JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy
  CT Projections'
arxiv_id: '2505.08123'
source_url: https://arxiv.org/abs/2505.08123
tags:
- semmd
- spectrum
- x-ray
- energy
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JSover is a novel one-step framework that jointly estimates the
  X-ray energy spectrum and performs multi-material decomposition from single-energy
  CT projections. It uses physics-informed spectral priors and implicit neural representation
  (INR) to reconstruct material compositions directly from projections, avoiding beam
  hardening artifacts.
---

# JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections

## Quick Facts
- **arXiv ID:** 2505.08123
- **Source URL:** https://arxiv.org/abs/2505.08123
- **Authors:** Qing Wu; Hongjiang Wei; Jingyi Yu; S. Kevin Zhou; Yuyao Zhang
- **Reference count:** 40
- **Primary result:** One-step framework achieving 0.0139-0.0272 RMSE vs 0.0959-0.1027 for prior methods

## Executive Summary
JSover is a novel one-step framework that jointly estimates X-ray energy spectrum and performs multi-material decomposition from single-energy CT projections. It uses physics-informed spectral priors and implicit neural representation (INR) to reconstruct material compositions directly from projections, avoiding beam hardening artifacts that plague traditional two-step methods. The framework employs a SoftMax-based spectrum model for unconstrained optimization and leverages INR's low-frequency bias to regularize the ill-posed inverse problem.

## Method Summary
JSover formulates a physics-based forward model in the projection domain that integrates energy-dependent attenuation through the spectrum and material-specific LACs. The spectrum is represented as a SoftMax-weighted combination of library spectra, enabling unconstrained gradient-based optimization while guaranteeing physical constraints. Material volume fractions are represented as continuous functions parameterized by an INR MLP with hash encoding. Joint optimization minimizes L1 data consistency loss between predicted and measured projections using Adam, training both spectrum weights and INR parameters simultaneously without intermediate image reconstruction steps.

## Key Results
- Achieves RMSE of 0.0139-0.0272 compared to 0.0959-0.1027 for prior methods on simulated XCAT phantoms
- Accurately estimates spectrum with minimal L1 distance between estimated and ground truth spectra
- Reduces computation time to 2-3 minutes versus 20+ minutes for two-step methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** One-step projection-domain optimization avoids beam hardening artifacts that plague two-step image-domain methods.
- **Mechanism:** Traditional SEMMD methods first reconstruct monochromatic CT images via FBP, which assumes energy-independent attenuation. This mismatch introduces nonlinear beam hardening artifacts and noise into subsequent decomposition. JSover instead formulates the polychromatic forward model directly in the projection domain, explicitly modeling energy-dependent attenuation through the spectrum and material-specific LACs. By never passing through an intermediate FBP reconstruction, the energy-dependent physics remains intact throughout optimization.
- **Core assumption:** The ideal solution assumption holds—that each voxel's attenuation is a linear combination of basis material LACs weighted by volume fractions, and the true X-ray spectrum can be represented as a convex combination of library spectra.
- **Evidence anchors:**
  - [abstract] "The initial reconstruction step, however, neglects the energy-dependent attenuation of human tissues, introducing severe nonlinear beam hardening artifacts and noise into the subsequent decomposition."
  - [Section III-A, Eq. 8] Derives the complete polychromatic forward model H that integrates spectrum η(E), material LACs μ_i(E), and volume fractions α_i(x).
  - [corpus] Weak direct evidence; related AI-augmented reconstruction work (Paper 25086) discusses physics-informed approaches but doesn't address beam hardening specifically for SEMMD.
- **Break condition:** If basis material LACs μ_i(E) are poorly characterized (e.g., for contrast agents not in NIST databases), or if the "ideal solution" assumption fails for heterogeneous materials like bone marrow mixtures, the forward model becomes misspecified and artifacts may persist.

### Mechanism 2
- **Claim:** SoftMax transformation enables unconstrained spectrum optimization while guaranteeing physical constraints (non-negativity, sum-to-one).
- **Mechanism:** Previous library-based spectrum methods required constrained optimization with regularization to enforce γ_i ≥ 0 and Σγ_i = 1. JSover applies SoftMax: η(E) = Σ SoftMax(γ_i)·η_i(E), where SoftMax(γ_i) = exp(γ_i)/Σexp(γ_j). Since SoftMax outputs are always positive and sum to 1 by construction, the spectrum constraints become automatic properties rather than optimization constraints. This allows standard gradient descent without Lagrange multipliers or penalty terms.
- **Core assumption:** The true spectrum lies within the convex hull of the library spectra {η_i}. If the actual spectrum has features outside this span (e.g., unusual filtration, different tube materials), SoftMax-weighted combinations cannot represent it accurately.
- **Evidence anchors:**
  - [abstract] "JSover employs a SoftMax-based spectrum model for unconstrained optimization"
  - [Section III-B-1, Eq. 11] Full derivation of SoftMax spectrum model with explicit differentiability and constraint guarantees.
  - [Figure 7] Shows estimated spectra closely matching ground truth for 80 kVP and 120 kVP sources, validating the library representation assumption for standard clinical spectra.
  - [corpus] No direct corpus evidence for SoftMax spectrum parameterization in CT.
- **Break condition:** If library spectra don't span the true spectrum space (e.g., custom filters, aging tube output), SoftMax weights may converge to a poor approximation regardless of optimization quality.

### Mechanism 3
- **Claim:** INR's spectral bias toward low-frequency patterns serves as an implicit regularizer for the ill-posed inverse problem.
- **Mechanism:** The volume fraction maps α(x) are represented as continuous functions F_Φ: x → α(x) parameterized by an MLP with hash encoding. Neural networks exhibit "spectral bias"—they learn low-frequency components faster than high-frequency ones [Rahaman et al., 2019, cited as ref 18]. This inductive bias naturally suppresses high-frequency noise and spurious solutions in the underdetermined optimization, effectively regularizing without explicit priors like TV.
- **Core assumption:** The true material distributions are predominantly piecewise-smooth with limited high-frequency content. If sharp material boundaries (e.g., bone-soft tissue interfaces) are critical, the spectral bias could over-smooth edges.
- **Evidence anchors:**
  - [abstract] "The inductive bias of INR toward continuous image patterns constrains the solution space and further enhances estimation quality."
  - [Section III-B-2] "Our key idea is to leverage the learning bias of neural networks toward low-frequency image patterns [18] as an implicit image prior."
  - [Table IV, Figure 12] Ablation comparing hash, position, and Fourier encodings shows all three architectures achieve comparable results (RMSE 0.0206-0.0367), suggesting the INR framework itself—not specific encoding choices—provides the regularization benefit.
  - [corpus] Weak; corpus mentions neural representations for CT reconstruction (Paper 25086) but not spectral bias mechanisms specifically.
- **Break condition:** For applications requiring fine spatial detail (micro-calcifications, thin cortical bone), the spectral bias may blur features. The TV-regularized variant (JSover-TV) shows slightly worse RMSE (0.0363 vs. 0.0139) but preserves some high-frequency edges differently—trade-offs may be task-dependent.

## Foundational Learning

- **Concept: Polychromatic X-ray attenuation and beam hardening**
  - **Why needed here:** Understanding why two-step methods fail requires grasping that X-ray attenuation is energy-dependent (μ(E)) and polyenergetic sources create nonlinear averaging in projections. FBP assumes monochromatic (single-energy) behavior, creating systematic errors.
  - **Quick check question:** If you reconstruct a uniform water phantom using standard FBP from polychromatic projections, will the resulting image show uniform Hounsfield units? Why or why not?

- **Concept: Implicit Neural Representations (INR) and coordinate-based MLPs**
  - **Why needed here:** JSover represents material maps as continuous functions F(x) → α rather than discrete voxel arrays. This requires understanding how MLPs can serve as functional approximators and why their spectral properties matter for regularization.
  - **Quick check question:** In a standard INR setup, what is the input and output of the network? How does the network "know" spatial relationships without convolutional layers?

- **Concept: Ill-posed inverse problems and regularization via priors**
  - **Why needed here:** Joint spectrum estimation + material decomposition from single-energy data is severely underdetermined. Understanding why constraints (SoftMax, INR bias, physics priors) are essential—not optional—prevents naive implementations that diverge or overfit.
  - **Quick check question:** If you remove all constraints from Eq. 9 and optimize with vanilla gradient descent on raw parameters, what behavior would you expect?

## Architecture Onboarding

- **Component map:**
  - **Input:** Single-energy CT projections ρ(r) for all rays r ∈ Π
  - **Spectrum Model S:** Learnable weights γ → SoftMax → weighted sum of library spectra {η_i} → estimated spectrum η(E)
  - **Material MLP F_Φ:** Hash encoding + 2-layer MLP (64 hidden units, ReLU) → SoftMax → volume fractions α(x) for any 3D coordinate x
  - **Forward Model H̃:** Differentiable implementation of Eq. 13—integrates estimated spectrum, material LACs (pre-computed from NIST), and sampled volume fractions along each ray to predict projections ρ̂(r)
  - **Loss:** L1 data consistency between predicted and measured projections (Eq. 14)
  - **Optimizer:** Adam, lr=1e-3, 4000 epochs, batch size 40 rays per iteration

- **Critical path:**
  1. Initialize spectrum weights γ_i = 1 (uniform) → initial spectrum = library average
  2. Initialize MLP with random weights
  3. For each iteration: sample 40 rays → query MLP at coordinates along each ray → compute predicted projections via H̃ → backpropagate loss → update γ and Φ jointly
  4. Output: final spectrum η*(E) and continuous material representation F_Φ(x)

- **Design tradeoffs:**
  - **Hash encoding vs. position/Fourier encoding:** Hash is 2-3x faster (2 min vs. 4-7 min) with comparable accuracy (Table IV). Trade-off is slightly more complex implementation and potential hash collision artifacts at extreme resolutions.
  - **INR vs. TV regularization:** INR (JSover-INR) achieves lower RMSE (0.0139 vs. 0.0363) and better visual quality for smooth regions. TV (JSover-TV) may preserve sharper edges but requires tuning λ and iterative optimization converges slower.
  - **L1 vs. L2 loss:** Paper uses L1 (Eq. 14) for robustness to outliers/noise in projections—consider this if your data has metal artifacts or photon starvation.

- **Failure signatures:**
  - **Spectrum collapse:** If all SoftMax weights converge to near-uniform despite distinct true spectrum, check that library spectra are sufficiently distinct (visualize Fig. 1 equivalents) and that projections span enough material diversity.
  - **Material cross-talk:** If α_i(x) maps show correlated errors across materials (e.g., adipose errors mirror muscle errors), the basis material LACs may be too similar at the target energy range—consider alternative basis choices.
  - **Over-smoothing:** If fine structures disappear, reduce hash encoding resolution (increase T) or experiment with fewer training epochs to capture higher frequencies before spectral bias dominates.
  - **Divergence:** If loss increases or oscillates, verify forward model implementation against analytical examples (e.g., pure water phantom should converge to correct spectrum and α_H2O ≈ 1).

- **First 3 experiments:**
  1. **Forward model validation:** Generate synthetic projections using Eq. 13 with known spectrum and material maps, then verify JSover recovers both correctly. Start with a simple 2-material phantom (e.g., water + bone cylinder) at 120 kVP. This isolates implementation correctness before tackling real data complexity.
  2. **Spectrum library sensitivity:** Create two spectrum libraries—one coarse (5 spectra) and one fine (20 spectra)—and run JSover on the same data. Compare estimated spectra and RMSE to assess how library density affects accuracy and whether your application tolerates coarser libraries (faster convergence, less memory).
  3. **Noise robustness test:** Add increasing levels of Poisson noise to simulated projections and plot RMSE vs. noise level for both JSover-INR and JSover-TV. This establishes the noise floor where INR's implicit regularization breaks down and helps set expectations for real low-dose acquisitions.

## Open Questions the Paper Calls Out

- **Question:** How does JSover's performance degrade if the true X-ray energy spectrum lies outside the convex hull of the pre-defined spectrum library?
  - **Basis in paper:** [inferred] Section III-B-1 formulates the estimated spectrum as a weighted sum of library spectra (Eq. 11), constraining the solution space to linear combinations of the library elements.
  - **Why unresolved:** The experiments utilize spectra generated by the SPEKTR toolkit, which is also used to generate the library (Fig. 1), effectively validating performance only within the library's distribution.
  - **Evidence:** Evaluation using physical phantoms scanned with X-ray filters or tube voltages that produce spectra significantly different from the library components.

- **Question:** To what extent does the "ideal solution assumption" introduce errors when decomposing complex biological tissues that do not adhere strictly to linear mixture models?
  - **Basis in paper:** [explicit] Section III-A explicitly bases the forward model on the "ideal solution assumption" (Eq. 5) to derive the relationship between mass fractions and volume fractions.
  - **Why unresolved:** The validation relies on digital XCAT phantoms and specific solution phantoms (water/CaCl2) that are designed to satisfy this assumption, leaving the error bounds for heterogeneous real tissues unexplored.
  - **Evidence:** Quantitative decomposition results on ex-vivo tissue samples or pathological phantoms where the chemical composition deviates from the ideal linear mixture model.

- **Question:** Does the inductive bias of Implicit Neural Representation (INR) toward low-frequency patterns limit the spatial resolution of fine structural details in the decomposed material maps?
  - **Basis in paper:** [explicit] Section III-B-2 states that the "inductive bias of INR toward low-frequency image patterns" is leveraged to constrain the solution space and enhance quality.
  - **Why unresolved:** While this bias helps regularize noise (visible in comparisons against TMA/MSC), the paper does not quantitatively assess potential over-smoothing of high-frequency edges or small anatomical features.
  - **Evidence:** Spatial resolution analysis using a high-contrast phantom with fine line-pairs to measure the Modulation Transfer Function (MTF) of the reconstructed material maps.

## Limitations

- Performance may degrade significantly if the true X-ray spectrum falls outside the convex hull of the pre-defined spectrum library
- The "ideal solution assumption" for linear material mixtures may not hold for complex heterogeneous biological tissues
- INR's spectral bias toward low-frequency patterns could over-smooth fine structural details like calcifications or thin cortical bone

## Confidence

- **Physics assumptions:** High confidence for standard clinical CT scenarios with well-characterized basis materials and library-represented spectra
- **SoftMax spectrum model:** High confidence based on mathematical guarantees and experimental validation
- **INR spectral bias regularization:** High confidence for smooth tissues, Medium confidence for applications requiring fine spatial detail
- **Computational efficiency:** High confidence demonstrated by 2-3 min vs 20+ min runtime comparison
- **Clinical generalization:** Medium confidence due to limited real-data validation (only 3 patient scans)

## Next Checks

1. **Basis Material Sensitivity Test**: Replace NIST bone/water/adipose/muscle with contrast agent LACs and evaluate whether JSover maintains accuracy for iodine/barium decomposition.

2. **Spectral Library Coverage Experiment**: Systematically vary library size (5, 10, 20 spectra) and assess how estimation accuracy degrades when the true spectrum falls outside the convex hull.

3. **Edge Preservation Benchmark**: Compare JSover-INR against JSover-TV on phantoms with sharp material boundaries (cortical bone, teeth) to quantify the trade-off between implicit regularization and spatial fidelity.