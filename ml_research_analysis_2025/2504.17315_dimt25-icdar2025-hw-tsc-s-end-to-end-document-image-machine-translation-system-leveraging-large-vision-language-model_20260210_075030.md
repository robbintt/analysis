---
ver: rpa2
title: 'DIMT25@ICDAR2025: HW-TSC''s End-to-End Document Image Machine Translation
  System Leveraging Large Vision-Language Model'
arxiv_id: '2504.17315'
source_url: https://arxiv.org/abs/2504.17315
tags:
- translation
- training
- arxiv
- document
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified framework for document image machine
  translation that addresses both OCR-based and OCR-free tasks. The method combines
  multi-task learning with perceptual chain-of-thought training to develop an end-to-end
  translation system using large vision-language models.
---

# DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model

## Quick Facts
- arXiv ID: 2504.17315
- Source URL: https://arxiv.org/abs/2504.17315
- Reference count: 28
- Primary result: 8B model achieves 97.66 BLEU on OCR test and 70.48 BLEU on MT test

## Executive Summary
This paper presents a unified framework for document image machine translation that addresses both OCR-based and OCR-free tasks. The method combines multi-task learning with perceptual chain-of-thought training to develop an end-to-end translation system using large vision-language models. During inference, minimum Bayesian decoding and post-processing strategies are applied to optimize output quality. Experimental results show that the MTL-PCOT approach outperforms single-task fine-tuning, achieving BLEU scores of 97.66 and 70.48 on OCR test and MT test sets respectively with the 8B model, demonstrating the effectiveness of the unified framework for handling complex document layouts.

## Method Summary
The system uses full-parameter SFT on InternVL2.5-MPO models with a unified training approach combining multi-task learning and perceptual chain-of-thought. The framework jointly trains OCR recognition and translation tasks using structured PCOT prompts that enforce a two-stage reasoning process. For inference, the system employs minimum Bayesian risk decoding with beam search and diverse sampling, followed by post-processing rules to handle special symbols and complex tables.

## Key Results
- MTL-PCOT SFT achieves 75.72 BLEU on Valid-MT vs 72.74 for single-task SFT (8B model)
- MBR decoding improves Test-OCR from 94.89→97.16 and Test-MT from 65.32→68.26
- 8B model outperforms 1B model by 5-8 BLEU points across all test sets

## Why This Works (Mechanism)

### Mechanism 1: Multi-Task Learning (MTL) for Cross-Task Representation Sharing
- Claim: Joint training on OCR and translation tasks improves generalization over isolated single-task training.
- Mechanism: MTL shares visual-linguistic representations across OCR-based and OCR-free tasks, allowing the model to leverage latent inter-task correlations. Loss functions are optimized jointly, reducing overfitting from data isolation.
- Core assumption: OCR recognition and translation share transferable visual-semantic features that benefit from co-training.
- Evidence anchors:
  - [abstract] "unified framework... addresses both OCR-based and OCR-free document image translation tasks"
  - [Section 2.1] "MTL concurrently optimizes multiple related tasks... mitigat[ing] issues prevalent in traditional single-task learning—such as model overfitting"
  - [corpus] Neighboring work (MT³, arXiv:2505.19714) similarly uses multi-task RL for text image MT, suggesting convergent evidence for MTL effectiveness—though direct comparison data is limited.

### Mechanism 2: Perceptual Chain-of-Thought (PCOT) Structured Decoding
- Claim: Explicit two-stage reasoning (OCR → translation) improves output consistency over end-to-end direct generation.
- Mechanism: PCOT enforces a hierarchical pipeline—first detecting/recognizing text, then performing cross-lingual transformation. This structured decomposition reduces semantic fragmentation by coupling visual perception with language understanding before translation.
- Core assumption: The LVLM can reliably follow chain-of-thought formatting without skipping or collapsing stages.
- Evidence anchors:
  - [abstract] "training framework that combines multi-task learning with perceptual chain-of-thought"
  - [Section 2.1] "first stage focuses on precise detection and recognition... second stage performs specialized cross-lingual transformation"
  - [Section 4.2, Table 1] MTL-PCOT SFT achieves 75.72 BLEU on Valid-MT vs. 72.74 for single-task SFT (8B model)—a 2.98 point gain.
  - [corpus] No direct corpus comparison for PCOT specifically; evidence is primarily internal to this paper.

### Mechanism 3: Minimum Bayesian Risk (MBR) Decoding for Output Selection
- Claim: Selecting outputs via pairwise BLEU similarity among diverse candidates improves final translation quality.
- Mechanism: MBR aggregates candidates from beam search and 10 diverse samples. The candidate with highest average BLEU similarity to others is selected, approximating consensus optimization.
- Core assumption: Higher intra-candidate BLEU correlation correlates with ground-truth quality.
- Evidence anchors:
  - [Section 2.2] "select the sample with the highest similarity score as the final system output"
  - [Section 4.2, Table 1] Adding MBR improves Test-OCR from 94.89→97.16 and Test-MT from 65.32→68.26 (8B model).
  - [corpus] Prior work (Farinhas et al., arXiv:2310.11430) demonstrates MBR effectiveness in MT—providing external validation of the approach.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: PCOT extends CoT to multimodal document tasks. Understanding how CoT structures reasoning helps diagnose whether the model follows the intended two-stage decomposition.
  - Quick check question: Can you explain why CoT improves performance on multi-step tasks compared to direct prompting?

- **Concept: Multi-Task Learning (MTL)**
  - Why needed here: The core training paradigm relies on sharing representations. Understanding negative transfer and task balancing is critical for debugging performance gaps.
  - Quick check question: What conditions cause MTL to underperform single-task learning?

- **Concept: Minimum Bayesian Risk (MBR) Decoding**
  - Why needed here: Inference-time optimization depends on MBR. Understanding utility functions and candidate diversity informs hyperparameter choices.
  - Quick check question: How does MBR differ from standard beam search in its selection criterion?

## Architecture Onboarding

- **Component map:** Document image → Vision encoder (InternVL2.5-MPO) → PCOT-formatted training → MTL-PCOT SFT → MBR decoding → Post-processing → Translated text

- **Critical path:**
  1. Prepare training data in PCOT format (Figure 1: separate OCR result → translation result stages)
  2. Full-parameter SFT with MTL across Track 1 (WebDoc-300K) and Track 2 (arXiv-124K)
  3. At inference, generate 11+ candidates (1 beam + 10 sampled)
  4. Compute pairwise BLEU; select highest-scoring candidate
  5. Apply post-processing rules (truncate repeated symbols >10, normalize Chinese spaces)

- **Design tradeoffs:**
  - **Unified vs. separate pipelines:** Unified framework increases flexibility but requires careful data formatting to avoid task interference.
  - **MBR sampling cost:** 10+ candidates per input increases latency ~10× vs. single beam search.
  - **Model scale:** 8B model outperforms 1B by 5–8 BLEU points but requires 8-GPU training setup.

- **Failure signatures:**
  - **Over-translation of special symbols:** Repetitive characters (---, ***) expanded excessively → addressed by post-processing rule.
  - **Complex table degradation:** Model produces garbled output for dense tables → current mitigation is removal, not correction.
  - **PCOT stage collapse:** If output skips OCR stage and jumps to translation, inspect training data formatting consistency.

- **First 3 experiments:**
  1. **Baseline comparison:** Run single-task SFT vs. MTL-PCOT on same data split; expect ~3 BLEU improvement on validation per Table 1.
  2. **Ablate MBR:** Compare single beam output vs. MBR-selected output; quantify BLEU gain and latency cost.
  3. **Post-processing impact:** Measure error reduction on test cases with heavy special-symbol content before/after truncation rules.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system's architecture be improved to robustly handle the translation of overly complex tables without resorting to the current post-processing strategy of removing them?
- Basis in paper: [explicit] The authors state in Section 2.2 that they established a rule for "removing translation outputs for overly complex tables" to address model failures, indicating this remains a significant unresolved challenge.
- Why unresolved: The current solution bypasses the failure mode by exclusion rather than solving the underlying alignment and translation difficulties presented by dense tabular data.
- What evidence would resolve it: A qualitative and quantitative evaluation on a filtered test set of "overly complex tables" showing successful translation retention and accuracy without the removal heuristic.

### Open Question 2
- Question: What are the independent performance contributions of the Multi-Task Learning (MTL) framework versus the Perceptual Chain-of-Thought (PCOT) training strategy?
- Basis in paper: [inferred] Table 1 presents results comparing standard "SFT" against the combined "MTL-PCOT SFT," but does not isolate the two distinct methodological components to determine which drives the performance gains.
- Why unresolved: Without an ablation study separating the unified framework (MTL) from the reasoning pipeline (PCOT), it is unclear if the improvement stems from data diversity or the structured reasoning steps.
- What evidence would resolve it: Experimental results comparing single-task PCOT, multi-task standard prompting, and the combined approach on the same validation sets.

### Open Question 3
- Question: What is the computational latency cost of the Minimum Bayesian Decoding (MBR) strategy relative to the observed translation quality improvements?
- Basis in paper: [inferred] Section 2.2 describes the MBR method requiring the concurrent generation of "10 diverse samples" plus beam search output, which imposes a significant inference-time compute overhead compared to single-pass decoding.
- Why unresolved: The paper highlights the BLEU score improvements (+2.94 BLEU on 8B Valid-MT) but does not quantify the trade-off regarding generation speed or resource consumption, which is critical for end-to-end systems.
- What evidence would resolve it: A comparison of average inference time per document and GPU memory usage for standard beam search versus the full MBR pipeline.

## Limitations

- **Evaluation constraints**: Results are competition-specific without independent replication on external datasets.
- **Dataset accessibility**: Exact training prompt templates and MBR implementation details remain unspecified.
- **Table handling**: Complex table translation remains problematic, with current solution being exclusion rather than correction.

## Confidence

- **High confidence**: The unified MTL-PCOT framework's superior BLEU scores over single-task baselines (75.72 vs 72.74 on validation) are well-supported by controlled comparisons within the same experimental setup.
- **Medium confidence**: The MBR decoding improvement claims (4-5 BLEU point gains) rely on internal comparisons but lack external validation of the consensus selection mechanism's effectiveness across different model scales.
- **Low confidence**: Claims about PCOT's specific contribution to structured reasoning are primarily supported by aggregated MTL-PCOT results rather than isolated PCOT ablation studies.

## Next Checks

1. **Independent replication**: Re-implement the framework using publicly available datasets to verify the 3+ BLEU improvement from MTL over single-task training is reproducible outside the competition environment.
2. **PCOT ablation study**: Train identical models with and without PCOT formatting while holding all other variables constant to isolate the chain-of-thought contribution to the 2.98 BLEU improvement observed.
3. **Robustness testing**: Systematically evaluate model performance on documents with varying table complexity, symbol density, and layout density to quantify the "over-translation" and "complex table" failure modes mentioned in the discussion.