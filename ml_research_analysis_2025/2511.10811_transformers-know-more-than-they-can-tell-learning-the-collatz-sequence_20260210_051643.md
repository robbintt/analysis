---
ver: rpa2
title: Transformers know more than they can tell -- Learning the Collatz sequence
arxiv_id: '2511.10811'
source_url: https://arxiv.org/abs/2511.10811
tags:
- arxiv
- learning
- collatz
- errors
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates transformer models' ability to predict
  long Collatz steps, a complex arithmetic function. The authors train small transformer
  models to predict the long Collatz successor of odd integers from 1 to 10^12, using
  different bases to encode input and output.
---

# Transformers know more than they can tell -- Learning the Collatz sequence

## Quick Facts
- **arXiv ID:** 2511.10811
- **Source URL:** https://arxiv.org/abs/2511.10811
- **Reference count:** 13
- **Primary result:** Transformers can predict long Collatz steps with up to 99.7% accuracy when using power-of-two bases, but struggle with odd bases, revealing that they learn to classify inputs by binary suffixes rather than simulate full algorithms.

## Executive Summary
This paper investigates transformer models' ability to predict long Collatz steps, a complex arithmetic function. The authors train small transformer models to predict the long Collatz successor of odd integers from 1 to 10^12, using different bases to encode input and output. The models achieve near-perfect accuracy (up to 99.7%) for certain bases (24, 16, 32), but performance varies significantly with the base used. All models, regardless of base, follow a common learning pattern, learning to predict specific classes of inputs associated with particular loop lengths in the Collatz sequence. The authors find that model errors are predictable and follow specific patterns, with most errors involving underestimating loop lengths rather than random hallucinations. The study reveals that the difficulty lies in understanding the control structure of the computation rather than the arithmetic operations themselves, and suggests that mathematical problems can be valuable tools for understanding how transformers learn.

## Method Summary
The paper trains sequence-to-sequence transformers to predict the "long successor" $\kappa(n)$ of odd integers, which involves multiple Collatz steps. The models use an asymmetric architecture (4-layer encoder, 1-layer decoder) and are trained on 300 million random pairs $(n, \kappa(n))$ where $n < 10^{12}$. Inputs and outputs are encoded as digit sequences in various bases (tested from 2 to 57). The key innovation is using the base as an inductive bias - power-of-two bases allow the model to directly access binary suffix features that determine loop lengths, while odd bases obscure this structure. The models are evaluated on exact match accuracy (token-by-token) on a test set of 100,000 examples.

## Key Results
- **Base-dependent accuracy:** Models achieve ~99.7% accuracy with bases 24, 32, 16, but <40% with odd bases 3, 11, showing that base choice is critical for performance
- **Stepped learning pattern:** All models learn in discrete "jumps" (plateaus in accuracy) rather than smooth improvement, corresponding to learning specific binary suffix classes
- **Predictable errors:** Over 90% of failures involve correct arithmetic but wrong loop length estimates, with errors clustering around powers of 2 rather than being random hallucinations

## Why This Works (Mechanism)

### Mechanism 1: Binary Suffix Classification for Control Flow
- **Claim:** Transformers predict long Collatz steps by classifying inputs based on the binary suffixes of their representations, which determine the required loop lengths, rather than simulating the full recursive process.
- **Mechanism:** The model identifies the residual of the input $n$ modulo $2^p$ (corresponding to the last $p$ bits of the binary representation). It maps these suffixes to specific values of $k$ (up-steps) and $k'$ (down-steps) defined in the paper's Theorem 2. Instead of executing the loop, the model likely performs a direct computation $\kappa(n) = (3/2)^k(n+1)-1/2^{k'}$ using these pre-identified parameters.
- **Core assumption:** The model can effectively extract modular arithmetic features (specifically modulo powers of 2) from the input sequence.
- **Evidence anchors:** [abstract] "As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$." [Section 4.2] Theorem 2 proves that $k$ and $k'$ can be deduced from the binary representation of the input via suffix matching.

### Mechanism 2: Hierarchical Template Application
- **Claim:** Models do not learn a single universal algorithm but rather a hierarchy of template functions $\kappa_{l,l'}(n)$ applied to specific input classes, leading to predictable, non-random errors when inputs fall outside learned classes.
- **Mechanism:** The model learns the arithmetic template for the Collatz successor. When an input requires loop lengths $(k, k')$ larger than the model has effectively learned (i.e., larger than $k_{max}$ or $l'_k$), the model defaults to predicting using the largest valid parameters it knows. For example, it may apply the template with parameters $(k_{max}, 1)$ or apply the correct arithmetic but underestimate $k'$ by a specific amount.
- **Core assumption:** The loss landscape encourages the model to settle on correct calculations for frequent inputs (small $k, k'$) while allowing systematic deviations on rare inputs (large $k, k'$).
- **Evidence anchors:** [abstract] "In over 90% of failures, the model performs the correct calculation, but wrongly estimates loop lengths." [Section 5.2] Describes "power-of-two errors" where the model outputs $2^m \cdot \kappa(n)$, effectively missing $m$ down-steps.

### Mechanism 3: Inductive Bias of Power-of-Two Bases
- **Claim:** The choice of encoding base acts as a strong inductive bias; bases aligned with binary (powers of 2 or highly composite numbers) allow the model to bypass explicit base conversion and directly access the structural features needed for the task.
- **Mechanism:** In bases like 24 or 32, the lower-order digits of the input sequence directly correlate with the binary suffixes determining loop lengths. The model exploits this correlation. In odd bases, the relationship between digit tokens and binary suffixes is entangled, and the paper demonstrates that transformers struggle to perform the explicit base conversion required to resolve this.
- **Core assumption:** The model is not performing explicit algorithmic base conversion (which the paper shows it cannot do, per Section 6.2) but is rather learning a direct mapping from token patterns to outcomes.
- **Evidence anchors:** [Section 4] Table 1 shows accuracy clustering: ~99.7% for bases 24, 32, 16 vs. <40% for bases 3, 11. [Section 6.2] Explicitly states models fail to learn base conversion.

## Foundational Learning

- **Concept: The Collatz Conjecture & Long Steps**
  - **Why needed here:** The paper defines the task not as single-step prediction, but predicting the "long successor" $\kappa(n)$, which involves a sequence of $3n+1$ (up) and $n/2$ (down) steps. Understanding that the difficulty lies in determining the number of steps ($k$ and $k'$) is central to the paper's findings.
  - **Quick check question:** If an odd integer $n$ transforms into $c_k$ after $k$ up-steps, what determines the subsequent number of down-steps $k'$?

- **Concept: Modular Arithmetic & Binary Representation**
  - **Why needed here:** The core theoretical result (Theorem 2) links the loop lengths to the binary suffix of the input. You must understand that $n \pmod{2^p}$ effectively extracts the last $p$ bits of $n$.
  - **Quick check question:** Why does knowing $n \pmod 8$ (binary suffix of length 3) give you information about the first few steps of the Collatz sequence?

- **Concept: Seq2Seq Transformer Architecture**
  - **Why needed here:** The paper uses a specific architecture (4-layer encoder, 1-layer decoder). Understanding the separation between the bidirectional encoder (processing the input number) and the autoregressive decoder (generating the output number) helps explain how the model predicts digits based on the encoded "class" of the input.
  - **Quick check question:** How does the cross-attention mechanism in the decoder utilize the encoded representation of the input integer to generate the prediction?

## Architecture Onboarding

- **Component map:** Input integers -> Encoder (4 layers) -> Decoder (1 layer) -> Output integers
- **Critical path:** 1. Choose Base B (use 24, 32, or 16) 2. Train on random pairs (n, Îº(n)) 3. Model learns in discrete accuracy jumps corresponding to binary suffix classes
- **Design tradeoffs:** Base Selection: Higher power-of-2 bases allow better feature alignment but increase vocabulary size. Odd bases obscure binary features, degrading performance. Task Complexity: Training on full complex regression task yields better convergence than simpler classification task.
- **Failure signatures:** Power-of-Two Errors: Prediction is exactly $2^m \times$ Target. Factor-of-3 Errors: Prediction is roughly $(2/3)^m \times$ Target. Hallucination: Almost absent; errors are structurally principled "misses."
- **First 3 experiments:** 1. Baseline Reproduction: Train with Base 24 and Base 11, verify stepped learning pattern and performance gap. 2. Error Taxonomy Validation: Run inference, plot $p/t$ distribution, verify clustering around powers of 2. 3. Loop Length Ablation: Train separate models to predict only $(k, k')$, compare convergence to full calculation model.

## Open Questions the Paper Calls Out
1. **Binary suffix learning mystery:** How do odd-base transformers learn to classify inputs by binary suffixes to determine loop lengths, despite failing to learn explicit base conversion? [explicit] The authors state, "How odd-base models learn the binary representation of their input remains a mystery."
2. **Regression vs classification optimization:** Do relaxed regression problems generally offer better optimization landscapes for transformers than their equivalent, simpler classification sub-tasks? [inferred] The authors observe that predicting the full long Collatz step was easier than predicting just loop lengths.
3. **Control structure generalization:** Is the inability to generalize control structures to values outside training distribution a fundamental limitation of standard transformer architectures? [inferred] Models learn near-perfect approximations for seen classes but fail on inputs with large loop lengths.

## Limitations
- **Architecture specificity:** Findings are based on a specific 4-encoder, 1-decoder layer configuration that may not generalize to other transformer architectures.
- **Task specificity:** Results are derived from a single mathematical problem (Collatz sequence), limiting conclusions about broader transformer learning capabilities.
- **Measurement focus:** Exact-match accuracy may not fully capture the model's understanding of underlying arithmetic relationships.

## Confidence
- **High confidence:** Empirical findings about base-dependent accuracy and stepped learning pattern are directly observable and reproducible.
- **Medium confidence:** Theoretical framework linking binary suffixes to loop lengths is mathematically sound, but extent of explicit learning is uncertain.
- **Low confidence:** Claim that mathematical problems can be valuable tools for understanding how transformers learn is more speculative.

## Next Checks
1. **Base conversion capability test:** Train a model to predict the base-10 representation of numbers given in base-24. If the model fails at this explicit base conversion task, it would strongly support the paper's claim that models learn direct mappings rather than performing algorithmic conversions.
2. **Loop length prediction ablation:** Train separate models to predict only $(k, k')$ versus full $\kappa(n)$. Compare their learning curves and final accuracies to test whether the "easier" classification task actually converges more slowly due to flat local minima.
3. **Architecture sensitivity analysis:** Vary the encoder/decoder layer ratio (e.g., 2-2, 4-2, 8-1) to determine whether the specific asymmetry is critical for learning the suffix-based pattern, or if other configurations achieve similar performance through different mechanisms.