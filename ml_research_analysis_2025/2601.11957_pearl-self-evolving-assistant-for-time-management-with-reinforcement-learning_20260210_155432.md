---
ver: rpa2
title: 'PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning'
arxiv_id: '2601.11957'
source_url: https://arxiv.org/abs/2601.11957
tags:
- decision
- each
- conflict
- calendar
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles calendar conflict resolution, where agents must
  choose which overlapping meetings to attend. It introduces CalConflictBench, a benchmark
  that tests whether agents can infer and adapt to user preferences over many sequential
  decisions.
---

# PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.11957
- Source URL: https://arxiv.org/abs/2601.11957
- Reference count: 40
- Primary result: PEARL achieves 0.76 error reduction rate and 55% improvement in average error rate versus strongest baseline.

## Executive Summary
PEARL addresses the challenge of learning user preferences in calendar conflict resolution through sequential decision-making. Standard LLM agents fail to improve over time, showing little preference-learning capability. PEARL introduces a reinforcement learning framework with an external Strategy Hub memory and round-wise rewards that encourage preference inference early and consistent application later. The system achieves a 0.76 error reduction rate and 55% improvement in average error rate compared to the strongest baseline.

## Method Summary
PEARL uses GRPO with a Qwen3-4B base model to train an agent that resolves calendar conflicts through sequential rounds. The key innovation is the Strategy Hub, an external memory storing textual decision strategies (max 10 entries, ≤350 chars each) that the agent can query and update. Round-dependent curriculum weights shift from preference inference to execution over N rounds. Round-wise advantages are computed separately per round position across sampled rollouts to stabilize credit assignment under non-stationary rewards. Training uses 20-round episodes on synthetic data with 8×H100 GPUs for ~40 GPU hours.

## Key Results
- PEARL achieves ERR 0.761 vs. SFT's 0.325, indicating strong adaptation over rounds
- Zero-shot + StrategyHub alone yields only modest improvement (AER 0.45→0.41), showing memory without learning is insufficient
- Early-round error remains high, but late-round error drops significantly, validating curriculum effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Externalized Preference State
The Strategy Hub stores a fixed-size set of natural-language decision strategies. At each round, the agent explicitly queries or updates S before deciding. This decomposes preference inference from preference execution, compressing learning into an interpretable, reusable state. Break condition: If preferences are highly transient or non-verbalizable, the Strategy Hub's textual representation may fail.

### Mechanism 2: Curriculum-Based Reward Scheduling
Round-dependent weights interpolate linearly: λr_t = 0.5 * (t/N) increases ranking reward weight, while λi_t = 0.5 * (1 - t/N) decreases Strategy Hub interaction reward weight. Early rounds emphasize extracting preferences; later rounds emphasize applying them correctly. Break condition: If preferences drift rapidly, curriculum assumptions about stabilization will fail.

### Mechanism 3: Round-Wise Advantage Estimation
Advantages are computed separately per round position (µ_t, σ_t) across G sampled rollouts. This prevents later high-reward rounds from dominating gradient updates and reduces variance in early-round learning. Break condition: If reward variance is uniform across rounds, per-round normalization adds overhead without benefit.

## Foundational Learning

- **Sequential Decision Processes with State Transitions**: The task is modeled as sequential rounds with calendar state Ct updating after each decision. Quick check: Can you explain why a single-turn LLM prompt cannot capture the dependency between round t and round t+1's optimal action?

- **Advantage Estimation in Policy Gradient Methods**: PEARL uses GRPO with round-wise advantages. Quick check: If advantages were computed trajectory-wide instead of round-wise, what symptom would you expect in early-round gradients?

- **Curriculum Learning in RL**: The reward weights λr_t and λi_t change over rounds. Quick check: If the curriculum weight for Strategy Hub interaction (λi_t) were too low in early rounds, what failure mode would you predict?

## Architecture Onboarding

- **Component map**: Policy Model -> Strategy Hub -> Rollout Engine -> Reward Model -> Advantage Estimator
- **Critical path**: 1) Agent receives (Ct, Et, history, S) 2) Agent calls Strategy Hub within K turns 3) Agent emits decision action 4) Environment computes round reward 5) After N rounds, advantages computed per-round and policy updated via clipped GRPO
- **Design tradeoffs**: Strategy Hub capacity (10 entries) balances compression vs. expressiveness; larger capacity may introduce noise. Training N=20 vs. eval N=104 reduces compute but may under-train long-horizon credit assignment.
- **Failure signatures**: Flat or negative ERR indicates agent not learning preferences; check Strategy Hub update frequency and curriculum weights. High early-round error, low late-round error indicates curriculum working but cold-start problem persists.
- **First 3 experiments**: 1) Ablate Strategy Hub: Run PEARL without Strategy Hub access to isolate memory contribution 2) Ablate Curriculum: Set λr_t and λi_t constant to test curriculum necessity 3) Vary Hub Capacity: Test Strategy Hub capacity ∈ {3, 10, 20} to find saturation point

## Open Questions the Paper Calls Out
- Can PEARL generalize to real-world calendar settings where preferences are driven by transient, hard-to-observe factors (mood, fatigue, stress) rather than structured role-conditioned rules?
- How can agents dynamically select and summarize relevant context over horizons exceeding current context window limits?
- Does PEARL's effectiveness transfer to larger base models, or does the RL training framework require model-specific tuning?

## Limitations
- Synthetic benchmark design limits external validity; no evidence of transfer to real-world calendar data or non-calendar domains
- Curriculum reward weighting is heuristic with no ablation studies quantifying impact of different schedules
- Single-user episode design prevents cross-user preference transfer, limiting data efficiency

## Confidence
- **High confidence**: Architectural design of Strategy Hub and curriculum scheduling is clearly specified and internally consistent
- **Medium confidence**: Decomposition of preference inference from execution via external memory is theoretically sound but depends on preference verbalizability assumptions not validated beyond synthetic domain
- **Low confidence**: Necessity of round-wise advantage estimation over trajectory-level baselines is asserted but not empirically validated

## Next Checks
1. **Ablate Strategy Hub**: Run PEARL without Strategy Hub access to quantify memory contribution versus pure RL learning
2. **Ablate Curriculum**: Set reward weights constant (no scheduling) to test whether curriculum timing is essential for early-round learning
3. **Vary Hub Capacity**: Test Strategy Hub capacity ∈ {3, 10, 20} to find the saturation point where additional entries add noise without benefit