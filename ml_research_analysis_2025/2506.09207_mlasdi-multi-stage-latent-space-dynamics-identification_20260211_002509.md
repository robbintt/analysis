---
ver: rpa2
title: 'mLaSDI: Multi-stage latent space dynamics identification'
arxiv_id: '2506.09207'
source_url: https://arxiv.org/abs/2506.09207
tags:
- training
- mlasdi
- latent
- dynamics
- gplasdi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing reconstruction
  accuracy and interpretable latent dynamics in data-driven reduced-order modeling,
  particularly for complex or high-frequency phenomena. The authors propose multi-stage
  Latent Space Dynamics Identification (mLaSDI), which extends the standard LaSDI
  framework by training additional decoders sequentially to correct residual errors
  from earlier stages.
---

# mLaSDI: Multi-stage latent space dynamics identification

## Quick Facts
- **arXiv ID:** 2506.09207
- **Source URL:** https://arxiv.org/abs/2506.09207
- **Reference count:** 40
- **Key outcome:** Multi-stage residual learning enables recovery of high-frequency content in parametric ROM without sacrificing latent interpretability.

## Executive Summary
This paper addresses the challenge of balancing reconstruction accuracy and interpretable latent dynamics in data-driven reduced-order modeling, particularly for complex or high-frequency phenomena. The authors propose multi-stage Latent Space Dynamics Identification (mLaSDI), which extends the standard LaSDI framework by training additional decoders sequentially to correct residual errors from earlier stages. This staged residual learning, combined with periodic activation functions, enables recovery of high-frequency content without sacrificing interpretability of the latent dynamics. Numerical experiments on a multiscale oscillating system, unsteady wake flow, and the 1D-1V Vlasov equation demonstrate that mLaSDI achieves significantly lower reconstruction and prediction errors, often by an order of magnitude, while requiring less training time and reduced hyperparameter tuning compared to standard LaSDI.

## Method Summary
mLaSDI extends LaSDI by introducing staged residual learning. First, an autoencoder is trained using standard LaSDI to learn interpretable latent dynamics (typically linear ODEs via SINDy). The residual error between true data and first-stage reconstruction is then computed and normalized. A second decoder, trained on the same latent trajectories, learns to predict this normalized residual using periodic activation functions (Sine). The final output is the sum of first-stage reconstruction and scaled second-stage residual prediction. This approach decouples high-frequency reconstruction from smooth latent dynamics, mitigating spectral bias while maintaining stability during parameter interpolation through enforced linear latent dynamics.

## Key Results
- mLaSDI achieves order-of-magnitude lower reconstruction and prediction errors compared to standard LaSDI
- The method requires less training time and reduced hyperparameter tuning than single-stage approaches
- Staged residual learning enables recovery of high-frequency content (e.g., small oscillations) that standard LaSDI cannot capture
- Linear latent dynamics ensure stability during parameter interpolation while multi-stage decoders recover physical complexity

## Why This Works (Mechanism)

### Mechanism 1: Staged Residual Learning
Standard LaSDI forces smooth, interpretable ODEs in the latent space, filtering out high-frequency content due to spectral bias. mLaSDI trains a second decoder to map the same latent trajectories to the residual error from the first stage, allowing the first stage to focus on accurate, smooth dynamics while the second stage corrects reconstruction loss without altering latent dynamics. Core assumption: residuals contain high-frequency content that can be regressed from existing latent space. Break condition: if first-stage latent space is too compressed to encode correlation to residual, second decoder fails.

### Mechanism 2: Periodic Activation Functions
The second-stage decoder uses SIREN-like Sine activations to accelerate learning of high-frequency residuals. Standard activations struggle with spectral bias, while Sine activations naturally fit the oscillatory nature of residuals left by smooth first stage. Core assumption: residual error signal has oscillatory or high-frequency characteristics aligned with periodic activations. Break condition: if system is purely diffusive or low-frequency, Sine activations may introduce ringing artifacts.

### Mechanism 3: Linear Latent Dynamics
Enforcing linear ODEs ($\dot{z} = Az + b$) in latent space guarantees stability during parameter interpolation, while multi-stage decoders recover physical complexity. This restricts latent manifold but ensures no finite-time blow-up when interpolating SINDy coefficients. Core assumption: non-linear decoding capability is sufficient to map simple linear latent trajectories to complex physical manifolds. Break condition: if underlying physics is genuinely chaotic or requires non-linear latent coupling, linear assumption will fail.

## Foundational Learning

- **Spectral Bias (F-principle)**: Neural networks inherently learn low frequencies first. Forcing smooth latent space exacerbates this bias, making high-frequency learning impossible in a single pass. *Quick check:* Why does a standard autoencoder trained on a "noisy" high-frequency signal output a blurred version of that signal?

- **SINDy (Sparse Identification of Nonlinear Dynamics)**: Fits interpretable ODEs to latent time series for extrapolation. *Quick check:* If SINDy discovers $\dot{z} = -0.5z$, what physical behavior does the latent variable exhibit?

- **Residual Learning**: Instead of one massive network, learn the "error" of the previous attempt. Easier than learning full signal from scratch. *Quick check:* In mLaSDI, if Stage 1 output is $\tilde{U}_1$ and true data is $U$, what is the target label for Stage 2 decoder?

## Architecture Onboarding

- **Component map:** Data $\to$ Encoder $\to$ Latent $z$ $\to$ SINDy (linear dynamics) $\to$ Decoder 1 $\to$ $\tilde{U}_1$ $\to$ Residual $R_1 = U - \tilde{U}_1$ $\to$ Decoder 2 (Sine activation) $\to$ $\tilde{U}_2$ $\to$ Final output: $\tilde{U}_1 + \epsilon_1 \cdot \tilde{U}_2$

- **Critical path:**
  1. Train Stage 1 (Autoencoder + SINDy) using reconstruction + dynamics loss
  2. Freeze Encoder and Decoder 1 weights
  3. Calculate residual $R_1 = U_{data} - \tilde{U}_1$ and normalize by $\epsilon_1$
  4. Train Decoder 2 to map latent $z$ to $R_1/\epsilon_1$

- **Design tradeoffs:**
  - Linear vs. Nonlinear Dynamics: Linear ensures stability during interpolation but restricts expressivity
  - Stage 1 vs. Stage 2 Depth: Stage 1 shallow for smoothness; Stage 2 often needs depth to capture residual structure

- **Failure signatures:**
  - Divergent Predictions: Poor SINDy fitting or nonlinear terms cause latent trajectories to explode during interpolation
  - Stagnant Loss: If Stage 1 loss stalls but reconstruction is poor, proceed to Stage 2
  - Overfitting Noise: Stage 2 fits noise rather than signal; check scaling factor $\kappa$

- **First 3 experiments:**
  1. **Multiscale Oscillator (Toy):** Replicate $A[\sin(2x-t) + 0.1\cos(40x...)]$ example to visualize spectral bias
  2. **Wake Flow (Interpolation):** Train on $\nu = [0.00032, 0.00054]$, test on $\nu = 0.00043$ to test interpolation stability
  3. **Ablation on Activations:** Compare Stage 2 with Tanh vs. Sine activations to validate spectral correction hypothesis

## Open Questions the Paper Calls Out
- How can mLaSDI be modified to avoid overfitting when trained on noisy data?
- Can richer nonlinear dynamics be incorporated into the latent space without inducing instability during parameter interpolation?
- What are systematic guidelines for selecting optimal number of training stages and allocating training budgets between them?
- Does integrating weak-form SINDy into mLaSDI framework improve prediction accuracy compared to strong-form differentiation?

## Limitations
- Theoretical guarantees about convergence rates or error bounds are lacking
- Linear latent dynamics assumption may fail for genuinely nonlinear dynamical systems
- The choice of $\kappa=1$ for Sine activation is empirically chosen without systematic exploration

## Confidence
- **High confidence:** Reconstruction accuracy improvements on benchmark problems
- **Medium confidence:** Claims about spectral bias mitigation
- **Medium confidence:** Stability guarantees during parameter interpolation

## Next Checks
1. **Residual correlation test:** Measure correlation coefficient between first-stage latent variables and true residuals; staged approach should fail if correlation drops below 0.3
2. **Activation ablation:** Train Stage 2 with Tanh, Softplus, and Sine activations on same residual target; quantify convergence speed and final error
3. **Nonlinear dynamics stress test:** Replace linear SINDy with quadratic library and test prediction stability during GP interpolation; document finite-time blow-up occurrence