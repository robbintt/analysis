---
ver: rpa2
title: 'Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking Function
  Variation'
arxiv_id: '2502.19657'
source_url: https://arxiv.org/abs/2502.19657
tags:
- search
- ranking
- function
- architecture
- variation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the issue of ranking function variability in
  zero-shot neural architecture search (NAS). Traditional approaches mitigate this
  by averaging, but this study proposes leveraging the variation itself.
---

# Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking Function Variation

## Quick Facts
- arXiv ID: 2502.19657
- Source URL: https://arxiv.org/abs/2502.19657
- Reference count: 40
- Primary result: Statistical comparison via Mann-Whitney U-test improves zero-shot NAS search performance by leveraging ranking function variation rather than mitigating it.

## Executive Summary
This paper addresses the challenge of ranking function variability in zero-shot neural architecture search (NAS). Instead of averaging multiple evaluations to reduce noise, the authors propose treating ranking function outputs as random variables and using statistical comparison (Mann-Whitney U-test) to determine stochastic dominance between architectures. This approach, integrated into random and evolutionary search algorithms, consistently improves search performance compared to simple averaging, particularly for ranking functions with moderate to high variance. The study also analyzes ranking function variation, finding that Eigenvalue score and ReLU Hamming distance exhibit the least variability, and demonstrates that caching evaluation results further improves search stability.

## Method Summary
The method replaces traditional averaging of ranking function outputs with statistical hypothesis testing to determine stochastic dominance between architectures. For each architecture, the ranking function is evaluated V=10 times using different data batches (B=64) and weight initializations. The Mann-Whitney U-test compares the distributions of these evaluations rather than comparing means. This statistical comparison is integrated into search algorithms (Random Search, REA, FreeREA) through iterative selection algorithms (Stat-MAX, Stat-TOP-K) that return stochastically dominating architectures. The approach treats ranking function outputs as distributions and selects architectures that stochastically dominate others, which implies higher mean performance but is a stricter criterion than simple mean comparison.

## Key Results
- Statistical comparison improves search performance over averaging for most ranking functions, particularly those with moderate to high variance
- The coefficient of variation (CV) of ranking function outputs correlates negatively with architecture accuracy for most metrics, suggesting variation signals quality
- Caching evaluation results consistently improves search stability and final architecture quality by preventing re-evaluation noise
- Data-agnostic ranking functions (SynFlow, LogSynFlow) show minimal benefit from statistical comparison due to low inherent variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical comparison via Mann-Whitney U-test improves architecture selection over simple averaging.
- Mechanism: Multiple evaluations of a ranking function are treated as samples from a distribution. The Mann-Whitney U-test determines stochastic dominance (P(X > k) ≥ P(Y > k) for all k) rather than comparing means. Stochastic dominance implies higher mean, but the converse is not true—this stricter criterion filters out architectures that appear better only due to random noise.
- Core assumption: Ranking function variability is systematic enough that stochastic dominance reflects genuine performance differences.
- Evidence anchors:
  - [abstract] "we strive to construct a stochastic ordering of the performance metrics to determine the best architecture"
  - [section 4.2] "This test differs from comparing the mean performance, X̄ > Ȳ. Stochastic dominance implies X̄ > Ȳ, but the converse is not true."
  - [corpus] Weak direct corpus support; related papers focus on different ranking functions rather than statistical comparison methods.
- Break condition: When ranking functions have very low inherent variation (e.g., data-agnostic SynFlow), statistical comparison provides no consistent benefit—averaging performs comparably.

### Mechanism 2
- Claim: Ranking function variation correlates with architecture quality—lower variation signals higher-accuracy architectures for most metrics.
- Mechanism: The coefficient of variation (CV) across multiple batch evaluations decreases as architecture accuracy increases. This negative correlation suggests that well-designed architectures produce more stable ranking function outputs across different data samples.
- Core assumption: The negative CV-accuracy correlation is causal and not an artifact of benchmark structure.
- Evidence anchors:
  - [section 5.1] "For all ranking functions, the coefficient of variation of an architecture tends to decrease as the average accuracy increases"
  - [figure 3] Kendall-τ correlation shows negative values between CV and accuracy for most ranking functions
  - [corpus] No direct corpus validation of this specific variation-quality relationship.
- Break condition: For label-dependent metrics (LGA, EPE-NAS), correlation can be near-zero or positive—CV is not a universal quality signal.

### Mechanism 3
- Claim: Caching evaluation results improves search stability and final architecture quality.
- Mechanism: In evolutionary search, architectures can be encountered multiple times. Re-evaluation introduces noise that can overturn previous decisions (architecture A was ranked higher than B, but re-evaluation reverses this). Caching locks in the initial comparison, maintaining search trajectory consistency.
- Core assumption: The initial evaluation is sufficiently representative; locking in early decisions does not propagate errors.
- Evidence anchors:
  - [section F / table 7] "caching, which is typically applied primarily with the motivation of reducing computational overhead, always improves the search performance"
  - [section F] "The efficiency of the search is improved by committing to a decision"
  - [corpus] No corpus papers address caching effects in zero-shot NAS.
- Break condition: If the ranking function has extremely high variance, locking in a single noisy evaluation could mislead the entire search.

## Foundational Learning

- Concept: **Stochastic dominance vs. mean comparison**
  - Why needed here: The core innovation replaces mean-based ranking with statistical hypothesis testing. Understanding that stochastic dominance is a stricter criterion (implies mean ordering, but not vice versa) is essential.
  - Quick check question: If distribution A has mean 5.0 with high variance and distribution B has mean 4.8 with very low variance, which might stochastically dominate?

- Concept: **Coefficient of variation (CV)**
  - Why needed here: The paper uses CV to quantify ranking function variability and shows it correlates with architecture quality. CV = standard deviation / mean normalizes variability across different scales.
  - Quick check question: Why use CV instead of raw variance when comparing variability across different ranking functions?

- Concept: **Zero-shot NAS ranking functions**
  - Why needed here: The method is evaluated on existing ranking functions (Eigenvalue score, ReLU Hamming distance, NTK condition number). Understanding what these proxy—gradient information, activation patterns, kernel properties—helps interpret when the method applies.
  - Quick check question: Why do data-agnostic ranking functions (SynFlow, LogSynFlow) show less benefit from statistical comparison than data-dependent ones?

## Architecture Onboarding

- Component map:
  Ranking function evaluator -> Statistical comparator -> Stat-MAX/Stat-TOP-K -> Search algorithm wrapper -> Cache

- Critical path:
  1. Sample architecture -> 2. Evaluate ranking function V times -> 3. Store all V values (not just mean) -> 4. When comparing two architectures, run Mann-Whitney U-test -> 5. If p < threshold, declare dominance; otherwise treat as tie -> 6. Cache comparison result

- Design tradeoffs:
  - **V (number of evaluations)**: Higher V improves statistical power but increases compute. Paper uses V=10 as default.
  - **Significance threshold**: Lower threshold (0.025) is more stringent, fewer false dominances but more ties; higher threshold (0.075) is more permissive. Paper finds 0.05–0.075 range optimal.
  - **Caching vs. re-evaluation**: Caching improves stability but prevents updating beliefs with new samples; hybrid approaches (accumulate evaluations) are possible but underperform pure caching.

- Failure signatures:
  1. No improvement over averaging: Likely using a data-agnostic ranking function (SynFlow, LogSynFlow) with inherently low variation
  2. High variance in search results: Caching not implemented; re-evaluation introducing noise
  3. Too many ties: Significance threshold too stringent (e.g., 0.01) for the variance level

- First 3 experiments:
  1. Reproduce variance analysis (Figure 1): Compute CV for Eigenvalue score and NTK condition number on NAS-Bench-201 to confirm which ranking functions have low vs. high variance
  2. Ablate significance threshold (Figure 5): Run FreeREA with Eigenvalue score on a single benchmark, sweeping threshold from 0.01 to 0.10, plot accuracy vs. threshold
  3. Caching vs. on-the-fly comparison (Table 7): Implement REA with and without caching on NAS-Bench-201 CIFAR-10, measure both accuracy and standard deviation over 10 runs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can zero-shot NAS be reformulated as a stochastic optimization problem where the objective is the distribution of the ranking function rather than a scalar score?
- Basis in paper: [explicit] The conclusion states, "Formulating a zero-shot neural architecture search as a stochastic optimisation problem that interprets the output of a ranking function not as a score, but as a distribution could be a potential research direction."
- Why unresolved: The current work maintains compatibility with existing pipelines by using statistical tests (Mann-Whitney) to enforce a stochastic ordering, rather than optimizing the distribution directly.
- What evidence would resolve it: A new search algorithm that inputs distribution parameters (e.g., mean and variance) into an optimizer (like Bayesian Optimization) to directly maximize the likelihood of high performance.

### Open Question 2
- Question: Is it possible to construct ranking functions that are inherently invariant to initialization and batch noise, thereby removing the need for statistical mitigation?
- Basis in paper: [explicit] The conclusion suggests, "An alternative strategy to the statistical approach is to develop techniques that are minimally affected by the data batch and initialisation."
- Why unresolved: The paper demonstrates that data-agnostic methods (SynFlow) have lower variance but the statistical approach is not consistently beneficial for them, pointing to a need for robustness-by-design.
- What evidence would resolve it: The derivation of a theoretical bound on ranking function variance with respect to batch sampling, followed by a new metric satisfying this bound.

### Open Question 3
- Question: Does the proposed statistical comparison method improve performance when applied to prune-based search algorithms?
- Basis in paper: [inferred] The authors validate the method on random and evolutionary search, but exclude prune-based search from experiments despite identifying it as a major search algorithm category in the background section.
- Why unresolved: Prune-based search operates by estimating differences in ranking scores for edges in a hypergraph, and it is unclear if statistical testing adds overhead or noise to this specific iterative elimination process.
- What evidence would resolve it: Experimental results on NAS-Bench-101/201 applying statistical comparison (Mann-Whitney U-test) within a prune-based search loop.

## Limitations

- The variation-quality correlation mechanism is not universal—it breaks down for label-dependent metrics like LGA and EPE-NAS, suggesting incomplete understanding of when CV predicts accuracy.
- The computational overhead of V=10 evaluations per architecture may not be justified for expensive ranking functions, and the paper lacks a systematic cost-benefit analysis.
- The recommendation to embrace variation over mitigation lacks nuance, as simple averaging remains a strong baseline particularly for low-variance ranking functions.

## Confidence

- **High Confidence**: The statistical comparison mechanism (Mann-Whitney U-test replacing mean comparison) is well-supported with consistent experimental improvements across multiple search algorithms and benchmarks.
- **Medium Confidence**: The variation-quality correlation mechanism is empirically demonstrated but lacks theoretical grounding and shows inconsistent results across different ranking functions.
- **Low Confidence**: The blanket recommendation to embrace variation over mitigation, as the paper does not adequately address when mitigation strategies might be preferable or how to automatically choose between approaches.

## Next Checks

1. Ablate ranking function variance: Systematically modify ranking functions to control their inherent variance (e.g., add controlled noise to Eigenvalue score outputs) and measure how the benefit of statistical comparison scales with baseline variance.

2. Cross-benchmark validation: Apply the method to a non-standard NAS benchmark (e.g., DARTS search space) to test whether the variation-quality correlation holds outside the NAS-Bench/TransNAS-Bench ecosystems.

3. Cost-benefit analysis: Implement a hybrid approach that uses statistical comparison only when CV exceeds a threshold, and compare total compute cost and accuracy against pure averaging and pure statistical comparison across multiple benchmarks.