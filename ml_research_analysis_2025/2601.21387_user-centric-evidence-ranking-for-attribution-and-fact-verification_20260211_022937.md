---
ver: rpa2
title: User-Centric Evidence Ranking for Attribution and Fact Verification
arxiv_id: '2601.21387'
source_url: https://arxiv.org/abs/2601.21387
tags:
- evidence
- sentences
- ranking
- sentence
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces evidence ranking, a task designed to minimize
  user reading effort during claim verification by ordering evidence sentences so
  that a minimal sufficient set appears as early as possible. This approach contrasts
  with traditional evidence selection, which outputs a fixed set without considering
  reading order.
---

# User-Centric Evidence Ranking for Attribution and Fact Verification

## Quick Facts
- **arXiv ID**: 2601.21387
- **Source URL**: https://arxiv.org/abs/2601.21387
- **Reference count**: 40
- **Primary result**: LLM-based incremental ranking achieves MRR 0.75, reducing user reading effort while improving verification accuracy versus evidence selection.

## Executive Summary
This paper introduces evidence ranking, a task that minimizes user reading effort during claim verification by ordering evidence sentences so that a minimal sufficient set appears as early as possible. Unlike traditional evidence selection, which outputs a fixed set without considering reading order, evidence ranking enables sequential verification where users can stop once sufficiency is reached. The authors propose one-shot and incremental ranking strategies, evaluate them using IR-inspired metrics, and demonstrate that incremental methods outperform one-shot approaches by better capturing complementary evidence. LLM-based methods achieve the best results (MRR 0.75), and a user study confirms that incremental evidence ranking reduces reading effort while improving verification accuracy compared to evidence selection.

## Method Summary
The method adapts standard IR metrics to measure how quickly a ranking surfaces sufficient evidence. Evidence ranking is formulated as ordering candidate sentences so a minimal sufficient set appears as early as possible, enabling users to stop reading once sufficiency is reached. The authors propose four ranking approaches: embedding-based similarity, NLI scoring, fine-tuned reranking, and LLM-based reasoning, each in one-shot and incremental variants. Incremental methods select sentences iteratively, conditioning each choice on previously selected evidence to capture complementarity. Evaluation uses adapted MRR, Success Rate, and NDCG metrics on a benchmark aggregating FEVER, HoVer, and WICE datasets.

## Key Results
- LLM-based incremental ranking achieves MRR 0.75 and SR 62.9%, outperforming all other methods across all metrics.
- Incremental ranking strategies consistently outperform one-shot approaches by +0.06 MRR for LLMs, better capturing complementary evidence.
- User study confirms evidence ranking reduces reading effort (2.48 sentences vs. 3.85) and improves verification accuracy (94% vs. 74%) compared to evidence selection.

## Why This Works (Mechanism)

### Mechanism 1
Incremental ranking strategies outperform one-shot approaches by capturing complementary evidence. By conditioning each selection on previously chosen sentences, the model explicitly avoids redundancy and prioritizes sentences that provide new, complementary information toward claim verification. Core assumption: Users process evidence sequentially and benefit from early exposure to a minimal sufficient set; complementary information is more valuable than redundant but relevant information. Evidence anchors: [abstract] "incremental ranking strategies better capture complementary evidence"; [section 5] "incremental variant: after selecting the most similar sentence, subsequent selections are made by averaging each remaining candidate's embedding with those of the already-selected sentences"; [corpus] Weak direct evidence in neighbors; related work on evidence retrieval exists but doesn't specifically test incremental ranking for sufficiency. Break condition: When the minimal sufficient set requires only one sentence, incremental and one-shot methods converge; gains diminish.

### Mechanism 2
LLM-based methods achieve superior evidence ranking by jointly reasoning over all candidates. LLMs consider the full candidate set in context, enabling global assessment of sufficiency, redundancy, and complementarity that local similarity or NLI scores cannot capture. Core assumption: LLMs can reliably identify which sentences directly support or refute a claim without inference chains, and can follow structured output instructions. Evidence anchors: [abstract] "LLM-based methods achieve the best results (MRR 0.75)"; [section 6.1] "LLM-based approaches consistently outperform all other methods across all metrics. Incremental LLM ranking with GPT-4o achieves an MRR of 0.75 and a success rate of 62.9%"; [corpus] Neighbor papers (e.g., "Beyond Retrieval: Improving Evidence Quality for LLM-based Multimodal Fact-Checking") support LLMs for evidence quality but don't specifically address ranking formulations. Break condition: When evidence requires multi-hop reasoning across many sentences, LLM performance degrades (MRR drops to 0.52 for 3+ sentences).

### Mechanism 3
Evidence ranking reduces user reading effort and improves verification accuracy compared to evidence selection. Users can stop reading once they reach sufficiency, rather than being presented with a fixed set that may be insufficient or overwhelming. Core assumption: Users can accurately identify when they have sufficient evidence and will stop reading accordingly; the "minimal sufficiency" construct maps to real user behavior. Evidence anchors: [abstract] "A user study confirms that incremental evidence ranking reduces reading effort while improving verification accuracy compared to evidence selection"; [section 7.2] "participants achieved higher verification success with evidence ranking (94%) compared to evidence selection (74%)... participants read an average of 2.48 sentences... compared to 3.85 with evidence selection"; [corpus] No direct replication in neighbor papers; this is a novel user-study contribution. Break condition: If users cannot accurately judge sufficiency or the gold annotations don't reflect true sufficiency, the efficiency gains may not transfer to real-world settings.

## Foundational Learning

- **Information Retrieval Ranking Metrics (MRR, NDCG)**: The paper adapts standard IR metrics to measure how quickly a ranking surfaces sufficient evidence; understanding these metrics is essential for interpreting results and designing evaluation. Quick check question: Given a ranking where the first sufficient set appears at position 5, but the optimal (shortest) sufficient set has size 3, how would you compute the normalized reciprocal rank?

- **Natural Language Inference (NLI) and Its Limitations**: NLI models are a baseline approach but underperform; understanding why (they score single sentences independently and struggle with partial support) explains the need for more global reasoning approaches. Quick check question: Why would an NLI model assign a low entailment score to a sentence that is partially but critically relevant to verifying a claim?

- **Sequential Decision-Making and Early Stopping**: The task formulation assumes users stop reading once sufficiency is reached; implementing this requires modeling the stopping criterion and evaluating cumulative efficiency, not just final accuracy. Quick check question: In a user interface for evidence ranking, how would you detect and measure when a user has reached sufficient evidence without explicit feedback?

## Architecture Onboarding

- **Component map**: Claim + Candidate Evidence Pool -> Ranking Engine (one-shot or incremental) -> Sufficiency Estimator (evaluation) -> User Interface (sequential reveal)

- **Critical path**: 1. Receive claim + candidate evidence sentences 2. Apply ranking method (one-shot or incremental) 3. Output ordered list 4. User reads sequentially until sufficiency (simulated in evaluation by measuring MSR/IMSR)

- **Design tradeoffs**:
  - One-shot vs. Incremental: Incremental improves performance (+0.06 MRR for LLMs) but increases latency (N model calls vs. 1)
  - LLM vs. Fine-tuned Models: LLMs achieve best results but are costlier; fine-tuned rerankers offer middle ground
  - Metric selection: MRR optimizes for early sufficiency; NDCG captures overall ranking quality; SR measures strict optimality

- **Failure signatures**:
  - NLI models on multi-sentence evidence: MRR drops to 0.12 for 2-sentence sets, 0.05 for 3+ (Table 3)
  - LLM reasoning models that don't output selections: Qwen3-235B-Thinking produces reasoning but fails to rank, degrading to random order (Appendix E)
  - Users reading beyond sufficiency: Even with ranking, users read 0.6 sentences beyond gold minimum on average (Section 7.2)

- **First 3 experiments**:
  1. Implement and benchmark similarity-based one-shot ranking using BAAI/bge-large-en-v1.5 embeddings; establish baseline MRR and analyze failure cases on multi-sentence evidence.
  2. Add incremental selection to similarity method by averaging embeddings of selected sentences; measure MRR gain and latency increase to quantify the tradeoff.
  3. Pilot LLM incremental ranking with GPT-4o or open alternative on a held-out subset; compare MRR, success rate, and cost-per-query against fine-tuned reranker baseline.

## Open Questions the Paper Calls Out
- Can task-specific fine-tuning for evidence ranking match the performance of large reasoning models (LRMs) while reducing computational cost? Basis: The authors state "LRMs are notoriously costly, and future work could focus on methods fine-tuned for the evidence ranking task." Unresolved because current fine-tuned approaches (ReasonRank-7B) underperform (MRR 0.67) compared to LRMs (MRR 0.75-0.88).
- Can coherence-based ordering of evidence improve verification efficiency when evidence spans multiple documents? Basis: The authors note "These datasets do not supply any internal ordering of evidence sentences... Developing and assessing methods for producing coherent evidence orderings is therefore an important direction for future work." Unresolved because current evaluation only measures sufficiency-based ordering.
- How can evidence ranking systems handle claims where candidate evidence contains simultaneous supporting and contradictory statements? Basis: The authors state "future work could experiment with datasets where evidence can simultaneously include both supporting and contradictory statements." Unresolved because current datasets assume evidence sets are either supporting or refuting, not mixed.
- Why do fine-tuned NLI models struggle with multi-sentence evidence, and can architectural changes improve their incremental ranking capabilities? Basis: Table 3 shows NLI models drop to MRR 0.12 for 2-sentence evidence vs. 0.67 for 1-sentence. Section 6.4 notes NLI models "frequently struggle to assign sufficiently high scores to partially supporting sentences." Unresolved because the paper identifies the limitation but does not propose solutions.

## Limitations
- Evaluation relies on static gold-standard minimal sufficient sets that may not capture real-world claim complexity or evolving information.
- User study has limited sample size (26 participants) and may not generalize across diverse user populations or claim types.
- LLM-based methods achieve best results but introduce significant computational costs and potential variability across model versions.

## Confidence
- **High Confidence**: Evidence ranking as a task formulation is well-grounded and clearly defined; the adaptation of IR metrics (MRR, NDCG) to measure early sufficiency is methodologically sound.
- **Medium Confidence**: The superiority of incremental ranking and LLM-based methods is demonstrated within the experimental framework, but results may vary with different claim distributions or evidence sources.
- **Medium Confidence**: The user study findings are compelling but limited by sample size; the observed efficiency gains may not hold in real-world deployment with diverse user behaviors.

## Next Checks
1. Replicate MRR benchmark: Implement the exact sampling strategy and run incremental LLM ranking on the 1000-instance benchmark to verify MRR 0.75 and SR 62.9% claims.
2. Stress-test incremental vs one-shot: Systematically vary claim complexity (single vs. multi-hop) and evidence set size to confirm that incremental gains (+0.06 MRR for LLMs) persist across all conditions.
3. User study replication: Conduct a follow-up user study with 50+ participants across multiple claim types to validate that evidence ranking consistently reduces reading effort (target: <2.5 sentences read vs. 3.85) and improves verification accuracy.