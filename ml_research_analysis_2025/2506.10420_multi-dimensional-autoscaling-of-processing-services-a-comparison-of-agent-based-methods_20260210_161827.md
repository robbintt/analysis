---
ver: rpa2
title: 'Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based
  Methods'
arxiv_id: '2506.10420'
source_url: https://arxiv.org/abs/2506.10420
tags:
- agent
- service
- processing
- services
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comparative evaluation of four agent-based
  approaches for multi-dimensional autoscaling in edge computing environments, where
  services must dynamically adjust both hardware resources and internal configurations
  to meet Service Level Objectives (SLOs) under strict resource constraints. The evaluation
  compares Active Inference (AIF), Deep Active Inference (DACI), Deep Q-Network (DQN),
  and Analysis of Structural Knowledge (ASK) agents managing two real-world processing
  services (YOLOv8 and OpenCV) on an edge device with 8 CPU cores.
---

# Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods

## Quick Facts
- arXiv ID: 2506.10420
- Source URL: https://arxiv.org/abs/2506.10420
- Reference count: 20
- This paper presents a comparative evaluation of four agent-based approaches for multi-dimensional autoscaling in edge computing environments.

## Executive Summary
This paper evaluates four agent-based methods for multi-dimensional autoscaling of processing services at the edge, where agents must simultaneously adjust hardware resources (CPU cores) and service configurations (video quality, model size) to meet Service Level Objectives under strict resource constraints. The evaluation compares Active Inference, Deep Active Inference, Deep Q-Network, and Analysis of Structural Knowledge agents managing two real-world services on an 8-core edge device. ASK achieves the highest SLO fulfillment (0.87), while DQN offers superior execution speed under 60ms per iteration. The study demonstrates that multi-dimensional autoscaling can outperform resource-only approaches in constrained environments, with different agents offering tradeoffs between performance, speed, and scalability.

## Method Summary
The study evaluates four agent-based approaches (Active Inference, Deep Active Inference, Deep Q-Network, and Analysis of Structural Knowledge) for multi-dimensional autoscaling of two processing services (YOLOv8 object detection and OpenCV QR detection) on an 8-core edge device. Agents operate in 5-second orchestration cycles, simultaneously adjusting CPU core allocation and service configurations (quality, model size) to maximize SLO fulfillment. The evaluation uses a Linear Gaussian Bayesian Network simulation for DQN pre-training and runs 50 iterations × 10 repetitions. SLO fulfillment is measured as m/t_q (capped at 1.0), with execution time monitored to ensure practical deployment within orchestration constraints.

## Key Results
- ASK agent achieves highest SLO fulfillment (0.87) after 20 iterations of exploration
- DQN agent provides fastest execution (under 60ms per iteration) due to pre-training on LGBN simulation
- AIF agent converges in 10 iterations with 1.7s inference time after vectorization optimizations
- DACI agent shows longest execution time (2.8s) but maintains practical performance within 5-second orchestration window

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional autoscaling (hardware + service configuration) outperforms resource-only scaling in constrained edge environments.
- Mechanism: Agents simultaneously adjust elastic configurations—CPU cores allocation AND service-internal parameters (video quality, model size)—to maximize SLO fulfillment when resources are scarce. When one dimension hits constraints (e.g., no more cores), the agent compensates via another (e.g., reducing model complexity to maintain throughput).
- Core assumption: Service output (throughput) has learnable statistical dependencies on actionable variables (cores, quality, model size), enabling prediction of SLO outcomes for unseen configurations.
- Evidence anchors:
  - [abstract] "dynamically adjusts both hardware resources and internal service configurations to maximize requirements fulfillment in constrained environments"
  - [section 1] "operating in multiple elasticity dimensions... protects the service execution and promises higher requirements fulfillment"
  - [corpus] Related work on edge autoscaling confirms resource-only scaling is insufficient when c_phy is bounded (neighbor papers on stream processing at edge)
- Break condition: If throughput dependencies on configuration variables are non-stationary (e.g., input stream characteristics change unpredictably), learned models degrade and SLO fulfillment drops.

### Mechanism 2
- Claim: Agent convergence speed and SLO performance depend on whether the agent learns online (AIF, ASK) or leverages pre-training (DQN).
- Mechanism:
  - AIF/ASK learn transition dynamics during deployment (online learning), requiring 10-20 iterations to stabilize
  - DQN pre-trains on a Linear Gaussian Bayesian Network (LGBN) simulation, achieving stable performance from iteration 0
  - DACI learns latent representations online but requires more iterations due to higher-dimensional optimization
- Core assumption: The LGBN simulation adequately approximates real processing environment transition probabilities for DQN pre-training to transfer.
- Evidence anchors:
  - [section 4.2] "ASK agent requires approximately 20 iterations to train and stabilize... AIF agent takes around 10 steps... DQN agent, having been pre-trained... maintains stable performance throughout"
  - [section 3.1] "To estimate the state transition probabilities for state-action pairs, we use a part of the monitored metrics to train a Linear Gaussian Bayesian Network"
  - [corpus] Corpus shows limited direct evidence on transfer learning quality for autoscaling (weak external validation)
- Break condition: If LGBN simulation diverges significantly from real environment dynamics (e.g., non-linear throughput scaling), pre-trained DQN policies may be suboptimal.

### Mechanism 3
- Claim: Execution time determines practical deployability within orchestration intervals; agent complexity must stay below the 5-second decision window.
- Mechanism: Each agent architecture has different inference complexity:
  - DQN: Forward pass through neural network (~60ms)
  - ASK: Numerical optimization via SLSQP (1.2s inference, 2.8s exploration)
  - AIF: Bayesian belief updating + policy evaluation (1.7s with vectorization, 20s unoptimized)
  - DACI: Deep learning inference + variational free energy computation (2.8s)
- Core assumption: The 5-second orchestration interval provides sufficient time for all agents to complete perception → action cycles without blocking service execution.
- Evidence anchors:
  - [section 4.2] "all agents (incl. the AIF) now operate within practical time constraints for the 5-second scaling interval"
  - [section 3.3] "pymdp implementation... requiring approximately 20 seconds per iteration... vectorized policy evaluation... 20-30x speedup"
  - [corpus] No direct corpus comparison of inference timing across agent types for edge orchestration (gap in literature)
- Break condition: If service count or action space dimensionality increases significantly, AIF and DACI may exceed orchestration interval, causing delayed scaling decisions and SLO violations.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: All four agents (AIF, DACI, DQN, ASK) formulate autoscaling as sequential decision-making under uncertainty; understanding state spaces, action spaces, and transition dynamics is essential.
  - Quick check question: Can you explain why throughput is a dependent (non-actionable) state variable while cores and quality are actionable?

- Concept: Bayesian Networks and Conditional Probability Tables
  - Why needed here: AIF uses CPTs for transition models; ASK and training environment use LGBN for throughput prediction; understanding probabilistic dependencies is critical.
  - Quick check question: How would uniform priors in CPTs affect early-stage AIF exploration behavior?

- Concept: Service Level Objectives (SLOs) and Fulfillment Metrics
  - Why needed here: The optimization objective (Eq. 1) caps fulfillment at 1.0; understanding this non-overfulfillable target function shapes how agents prioritize actions.
  - Quick check question: Why does the paper define SLO fulfillment as m/t_q when m ≤ t_q, rather than a continuous distance metric?

## Architecture Onboarding

- Component map:
  Edge Device (8 CPU cores) -> Processing Services (containerized via Docker) -> Time-Series DB (monitors metrics, 1-second batches, 5-second sliding windows) -> Scaling Agent (5-second orchestration cycle) -> Training Environment (LGBN simulation for DQN pre-training)

- Critical path:
  1. Deploy CV and QR services in Docker with resource constraints
  2. Configure time-series DB ingestion for throughput and configuration metrics
  3. Initialize scaling agent with SLO targets (Table 1 & 2)
  4. Run 50 iterations (250 seconds) per experiment; repeat 10 times for stability
  5. Log SLO fulfillment per iteration and inference duration per cycle

- Design tradeoffs:
  - ASK vs. DQN: ASK achieves highest SLO (0.87) but requires 20-iteration exploration; DQN is faster (60ms) but depends on pre-training quality
  - AIF vs. DACI: AIF is interpretable but discretizes state space; DACI scales to high dimensions but is computationally heavier (2.8s)
  - Action granularity: AIF single-action-per-service constraint reduces computation but limits fine-grained control

- Failure signatures:
  - SLO fulfillment stuck below 0.6 after 20 iterations → check action space constraints or learning rate
  - Inference time exceeds 5 seconds → AIF may need further vectorization; reduce state factors
  - Throughput variance spikes → sliding window may be too short; increase from 5 seconds
  - Core allocation exceeds c_phy → constraint solver bug in ASK; verify Eq. (2) implementation

- First 3 experiments:
  1. Baseline convergence test: Run AIF agent for 50 iterations; verify ~10-iteration stabilization to ~0.70 SLO fulfillment
  2. Pre-training ablation: Run DQN with and without LGBN pre-training; quantify initial vs. stable performance gap
  3. Scalability stress test: Add a third processing service (e.g., audio inference); measure DACI and AIF inference time growth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can these agent architectures maintain performance in distributed, multi-device continuum systems?
- Basis in paper: [explicit] The authors state that future work will "refine agents for distributed settings" and "extend the framework to support a broader set of real-world use cases."
- Why unresolved: The current evaluation is limited to two services running on a single Edge device (8 cores).
- Evidence: Successful evaluation of agent coordination and SLO fulfillment across multiple physical nodes or a larger cluster.

### Open Question 2
- Question: Can Deep Active Inference (DACI) agents be optimized to achieve faster execution times suitable for large-scale systems?
- Basis in paper: [explicit] The authors list "optimizations that can exploit parallel processing" and "further improvements to the neural architecture" as works in progress.
- Why unresolved: DACI exhibited the longest execution time (2.8s) and lowest SLO fulfillment (0.72) compared to ASK, despite being identified as theoretically promising.
- Evidence: Architectural refinements that reduce DACI inference latency to match or beat the DQN agent (under 60ms).

### Open Question 3
- Question: How robust is the DQN agent's pre-trained policy when transferred to environments significantly different from the Linear Gaussian Bayesian Network (LGBN) simulation?
- Basis in paper: [inferred] The paper notes the training environment is a "simplified replication" of the real environment using an LGBN to estimate transition probabilities.
- Why unresolved: While DQN performed well, it relied on pre-training in a simulation that may not capture the full complexity or variability of diverse real-world hardware configurations.
- Evidence: Performance stability of the pre-trained DQN agent when deployed on hardware with substantially different resource-throughput characteristics than the training model.

## Limitations

- Pre-training simulation fidelity uncertainty: DQN performance depends on LGBN accurately capturing real-world throughput dynamics, but transfer quality validation is missing
- Neural architecture specification gaps: DACI's neural network structure and regularization parameters are unspecified, making exact replication difficult
- Discretization sensitivity lack of analysis: AIF discretization granularity effects on accuracy and robustness are not quantified in the paper

## Confidence

- High confidence: ASK performance claims (0.87 SLO fulfillment, 20-iteration convergence) - directly measured in experiments with clear methodology
- Medium confidence: DQN speed claims (60ms inference) - pre-training details missing but execution time measurement straightforward
- Medium confidence: AIF convergence timeline (10 iterations, 1.7s inference) - vectorization details provided but original unoptimized performance unclear
- Low confidence: DACI scalability claims - neural architecture and regularization parameters unspecified

## Next Checks

1. Transfer learning validation: Compare DQN performance with and without LGBN pre-training across multiple random seeds to quantify simulation-real transfer reliability
2. Architectural sensitivity analysis: Systematically vary AIF discretization granularity and DACI neural network depth to measure impact on SLO fulfillment and execution time
3. Constraint violation monitoring: Instrument all agents to log core allocation violations during exploration phases to verify constraint handling robustness