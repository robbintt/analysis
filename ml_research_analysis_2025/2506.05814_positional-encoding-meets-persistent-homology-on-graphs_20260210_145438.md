---
ver: rpa2
title: Positional Encoding meets Persistent Homology on Graphs
arxiv_id: '2506.05814'
source_url: https://arxiv.org/abs/2506.05814
tags:
- graphs
- graph
- positional
- pipe
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited expressiveness of message-passing
  graph neural networks (GNNs) in capturing global structural information like cycles
  and connectivity. The authors investigate the relative merits of positional encoding
  (PE) and persistent homology (PH) methods, showing neither is strictly more expressive
  than the other.
---

# Positional Encoding meets Persistent Homology on Graphs

## Quick Facts
- arXiv ID: 2506.05814
- Source URL: https://arxiv.org/abs/2506.05814
- Reference count: 40
- Primary result: PiPE achieves 0.0599 MAE on ZINC vs 0.0693 for SPE, 0.798 AUC-ROC on OGBG-MOLHIV vs 0.762 for RW, and 70.92 AUC-ROC on DrugOOD OOD-Test vs 69.64 for SPE

## Executive Summary
This paper addresses the limited expressiveness of message-passing graph neural networks by combining positional encoding (PE) and persistent homology (PH) methods. The authors establish that neither PE nor PH is strictly more expressive than the other, motivating their novel PiPE architecture that integrates both through message-passing networks. Empirically, PiPE demonstrates state-of-the-art performance across molecule property prediction, graph classification, and out-of-distribution generalization tasks while maintaining theoretical guarantees on expressiveness.

## Method Summary
PiPE is a message-passing GNN architecture that iteratively combines positional encodings with persistent homology. Starting from base PE embeddings (Laplacian, Random Walk, or Distance PE), PiPE computes persistence diagrams using the PE values as vertex colors for filtration functions. These diagrams are vectorized into topological embeddings that feed back into PE updates at each layer. The architecture concatenates node features with positional and topological embeddings for standard GNN message passing. The key innovation is using PE-informed filtrations where the birth times in persistence diagrams encode the original PE values, preserving their distinguishing power while adding topological structure.

## Key Results
- Achieves 0.0599 MAE on ZINC molecule property prediction (vs 0.0693 for SPE)
- Reaches 0.798 AUC-ROC on OGBG-MOLHIV graph classification (vs 0.762 for RW)
- Improves out-of-distribution generalization on DrugOOD: 70.92 AUC-ROC on OOD-Test (vs 69.64 for SPE)
- Demonstrates theoretical expressiveness beyond both base PE and PH methods
- Shows scalability with runtime overhead <2× base PE methods

## Why This Works (Mechanism)

### Mechanism 1: Complementary Expressiveness Gaps
PE and PH capture fundamentally different structural information - PE encodes spectral properties while PH captures topological invariants. This creates complementary blind spots where each method fails on graphs the other can distinguish, making their combination more expressive than either alone.

### Mechanism 2: PE-Informed Filtration Functions
Using positional encodings as vertex colors for filtration yields persistence diagrams at least as discriminative as the base PE alone. Injective filtration functions preserve color information in birth times, maintaining PE's distinguishing power while adding topological structure.

### Mechanism 3: Iterative Topological Embedding Updates
Alternating between positional embedding updates and persistence diagram computation creates representations that accumulate both spectral and topological information across layers. Layer-wise refinement allows gradual integration of local message-passing with global topological features.

## Foundational Learning

- **Persistent Homology & Filtrations**
  - Why needed here: Core to understanding how PiPE extracts topological features from PE-colored vertices
  - Quick check question: Given a graph with nodes colored {1, 2, 3} and filtration f(c) = c, what 0-dim persistence pairs would you expect?

- **Laplacian Positional Encoding**
  - Why needed here: Base PE method; eigenvector sign/basis ambiguities motivate the search for complementary features
  - Quick check question: Why do the k smallest eigenvectors of the graph Laplacian capture positional information?

- **1-WL and k-WL Expressiveness Hierarchy**
  - Why needed here: Paper frames expressiveness gains relative to WL tests; Proposition 4.3 relates RW-based PiPE to 3-WL limitations
  - Quick check question: What graph pairs does 1-WL fail to distinguish that 2-WL can?

## Architecture Onboarding

- **Component map**: Base PE module → Filtration function f_ℓ → PH computation → Vectorization (Ψ_0, Ψ_1) → Positional update (Upd^p_ℓ) → Backbone GNN
- **Critical path**: Base PE → Filtration → PH diagrams → Vectorization → Positional update → (repeat for L layers) → Readout with pooled topological embeddings
- **Design tradeoffs**: PE choice affects spectral vs computational cost; PH dimensionality impacts feature richness vs computation; Vectorization method (VC vs RePHINE) trades parameter efficiency vs expressiveness
- **Failure signatures**: Expressiveness plateau on BREC's "Regular" or "CFI" categories; training instability from PH computation producing NaN; OOD generalization gap indicating overfitting to training topology
- **First 3 experiments**:
  1. Ablation on identity vs learned filtration on ZINC subset - compare VC-I vs VC
  2. Expressiveness probe on BREC "Basic" category - test PH-only vs PH+LPE vs PiPE
  3. Runtime scaling check on Alchemy - measure epoch time with varying graph sizes

## Open Questions the Paper Calls Out

- **Can PiPE be generalized to higher-dimensional topological domains?** The current methodology is restricted to 1-dim simplicial complexes, but extending to simplicial or cell complexes could capture higher-order interactions beyond dyadic edges.

- **How can the computational expense of calculating persistence diagrams be optimized?** While PiPE shows state-of-the-art performance, the O(n³) complexity of PH computation may limit scalability for very large graphs.

- **Can a variant of PiPE utilizing Random Walk PE achieve 3-WL discriminative power?** Proposition 4.3 shows RW-based PiPE cannot distinguish certain graphs that 3-WL can, raising questions about bridging this expressiveness gap.

## Limitations

- The theoretical incomparability between PE and PH expressiveness relies on specific graph constructions that may not generalize to real-world datasets
- Computational overhead of PH computation (O(n³) worst-case) may limit scalability despite reported modest runtime increases
- Learned filtration functions' stability and robustness across different graph distributions remains underexplored

## Confidence

- **High confidence**: Empirical performance improvements on standard benchmarks (ZINC MAE reduction, MOLHIV AUC-ROC increase)
- **Medium confidence**: Theoretical expressiveness claims - while Proposition 3.1 establishes incomparability, practical significance depends on dataset characteristics
- **Medium confidence**: OOD generalization results - improvements are promising but mechanism isn't fully disambiguated

## Next Checks

1. **Ablation on filtration learning**: Compare PiPE with fixed vs learned filtration functions on ZINC to isolate the contribution of adaptive topological feature extraction

2. **Expressiveness stress test**: Evaluate PiPE variants on BREC's most challenging graph pairs (CFI category) to quantify the practical gap between PE, PH, and combined approaches

3. **Filtration function analysis**: Visualize learned filtration functions across different molecular scaffolds to understand what topological features are being prioritized