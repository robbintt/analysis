---
ver: rpa2
title: 'FINCH: Locally Visualizing Higher-Order Feature Interactions in Black Box
  Models'
arxiv_id: '2503.16445'
source_url: https://arxiv.org/abs/2503.16445
tags:
- feature
- features
- interactions
- data
- finch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FINCH addresses the challenge of visualizing higher-order feature
  interactions in black-box AI models by employing a subset-based approach that preserves
  original data distributions. Unlike traditional methods that rely on data permutation,
  FINCH uses actual data instances to calculate local feature interactions incrementally.
---

# FINCH: Locally Visualizing Higher-Order Feature Interactions in Black Box Models

## Quick Facts
- arXiv ID: 2503.16445
- Source URL: https://arxiv.org/abs/2503.16445
- Reference count: 39
- Primary result: Visual analytics tool achieving SUS score of 82 for revealing higher-order feature interactions in black-box models

## Executive Summary
FINCH addresses the challenge of visualizing higher-order feature interactions in black-box AI models by employing a subset-based approach that preserves original data distributions. Unlike traditional methods that rely on data permutation, FINCH uses actual data instances to calculate local feature interactions incrementally. The visual analytics tool employs coloring and highlighting techniques to show how each additional feature influences the prediction outcome. A comprehensive evaluation with five machine learning experts using the SUS scale yielded a score of 82, indicating excellent usability.

## Method Summary
FINCH employs a subset-based approach to reveal local feature interactions by filtering the dataset to include only instances similar to a target instance based on selected features. The method calculates prediction curves using actual data points rather than artificial permutations, preserving feature distributions and interactions. For a target instance, users incrementally add features to refine subsets, with the tool displaying curves showing how each new feature influences and interacts with previous ones. The algorithm uses heuristics including the 5% most similar instances or a minimum of 50 instances, whichever yields more, to ensure statistically meaningful subsets while avoiding sparsity issues.

## Key Results
- SUS score of 82 from evaluation with five machine learning experts indicates excellent usability
- Successfully reveals feature interactions up to high orders through incremental visualization
- Provides effective trust calibration through distribution heatmaps and uncertainty bands
- Meets requirements for understandability, usability, and helpfulness in local explanation

## Why This Works (Mechanism)

### Mechanism 1: Subset-Based Filtering for Interaction Preservation
Filtering to realistic instances matching the current instance's feature values preserves true feature interactions better than permutation-based approaches. FINCH calculates prediction curves using only instances from the original dataset that match the target instance's values for selected features, avoiding artificial data points that could violate conditional dependencies between features. This approach works when the dataset contains sufficient similar instances to form statistically meaningful subsets; sparse regions may produce unreliable curves.

### Mechanism 2: Incremental Curve Comparison for Interaction Attribution
Showing prediction curves sequentially as features are added allows users to isolate how each new feature interacts with previously selected ones. FINCH displays three curves—the base curve (all instances), the previous subset curve, and the current subset curve—with highlighting on the difference between successive subsets to emphasize the marginal interaction effect. This visualization works when users can cognitively track curve changes across incremental steps; ordering of feature addition affects interpretability but not correctness.

### Mechanism 3: Trust Calibration Through Distribution and Uncertainty Views
Providing auxiliary visualizations of subset composition and prediction variance helps users assess explanation reliability. Distribution heatmaps show how similar subset instances are to the target; uncertainty plots display standard deviation bands; ground truth overlay reveals model-data gaps. This approach assumes users will consult these auxiliary views rather than accepting the primary visualization at face value.

## Foundational Learning

- Concept: Partial Dependence Plots (PDPs)
  - Why needed here: FINCH's visualizations inherit PDP conventions (x-axis: feature values, y-axis: predictions); understanding PDP assumptions helps recognize where FINCH improves upon them.
  - Quick check question: Can you explain why PDPs can produce misleading results when features are highly correlated?

- Concept: Local vs. Global Explanation Methods
  - Why needed here: FINCH operates at the instance level; distinguishing this from global methods (feature importance across entire model) clarifies the tool's scope and limitations.
  - Quick check question: Given a model's prediction for one specific loan applicant, would you use a local or global explanation method to justify that decision?

- Concept: Feature Interaction Effects
  - Why needed here: FINCH's core purpose is revealing interactions beyond individual feature contributions; understanding the difference between main effects and interaction effects is essential for interpreting the visualization.
  - Quick check question: In a model predicting house prices where f(x,y) = x + y + xy, which term represents the interaction between square footage (x) and neighborhood quality (y)?

## Architecture Onboarding

- Component map:
  Data Layer: CSV dataset loader, sklearn model loader, instance selector
  Computation Layer: Subset similarity calculator (Euclidean distance on normalized features with 5%/50-instance heuristics), curve generator, main/interaction effect separator
  Visualization Layer: Primary dependence plot with colored backgrounds, distribution heatmaps, uncertainty bands, ground truth overlay
  Interaction Layer: Feature selector with small-multiple previews, incremental curve browser

- Critical path:
  1. Load dataset (CSV) and trained model (sklearn .pkl)
  2. Select target instance from data or define custom values
  3. Choose primary feature for x-axis
  4. Incrementally add conditioning features to refine subset
  5. Interpret curve changes; consult distribution/uncertainty views to validate
  6. Optionally enable ground truth or interaction effect separation

- Design tradeoffs:
  - Real data only (no permutation) vs. potential sparsity issues in low-density regions
  - Fuzzy neighborhood matching (ensures ≥50 instances) vs. potential inclusion of dissimilar instances
  - Incremental visualization vs. cognitive load from multiple curves
  - Local-only scope vs. inability to provide global model understanding

- Failure signatures:
  - Flat or erratic curves in sparse regions: subset contains dissimilar instances; check distribution heatmap density
  - Large uncertainty bands: insufficient instances for the feature combination; consider reducing conditioning features
  - Ground truth divergence: model systematic bias, not explanation artifact
  - Slow performance on large datasets: pre-compute predictions or sample dataset

- First 3 experiments:
  1. Replicate the bike-sharing example from Section 4.2: Select an instance from a winter weekend, add "weekday" then "season" as conditioning features, and verify the curve shows the expected shift from commuter peaks to afternoon leisure pattern.
  2. Test sparsity limits: Choose a rare instance (e.g., extreme outlier in multiple features) and observe how subset quality degrades; compare distribution heatmap to identify where the 50-instance minimum forces inclusion of dissimilar neighbors.
  3. Compare FINCH to SHAP on a correlated-feature dataset: Load a model where two features have strong correlation (e.g., income and education level), select an instance, and observe whether FINCH's subset view reveals interaction patterns that SHAP's independent attribution obscures.

## Open Questions the Paper Calls Out

- How can local feature interactions be automatically detected without requiring users to manually select features to explore?
- How can reliable visualizations be generated when data instances are sparse at the edges of feature distributions?
- Can the subset-based interaction visualization approach be effectively adapted to non-tabular data such as images?
- How can the main effect and interaction effect separation visualization be redesigned to be more intuitively understandable?

## Limitations
- The subset-based approach's effectiveness depends heavily on dataset density; sparse regions may produce unreliable curves
- SUS score of 82 from only 5 ML experts limits generalizability to broader audiences
- Paper doesn't specify computational complexity or runtime performance for large datasets

## Confidence
- **High confidence**: The mechanism of preserving data distributions through subset filtering is sound and addresses a known limitation of permutation-based methods
- **Medium confidence**: The incremental curve visualization approach is novel but lacks direct comparison to established methods like SHAP or PDP+ICE
- **Low confidence**: Claims about the tool's effectiveness in revealing high-order interactions are based on expert opinion rather than systematic quantitative validation

## Next Checks
1. Test FINCH on a deliberately sparse dataset to identify the point where subset quality degrades and the 50-instance heuristic begins including dissimilar instances
2. Implement a controlled experiment comparing FINCH's interaction detection to SHAP values on a dataset with known feature correlations
3. Measure computation time and memory usage as dataset size and feature dimensionality increase to identify practical limits for real-world deployment