---
ver: rpa2
title: What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action
  Alignment
arxiv_id: '2510.08847'
source_url: https://arxiv.org/abs/2510.08847
tags:
- agent
- plan
- tool
- errors
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Agent GPA (Goal-Plan-Action) framework
  for systematically evaluating LLM agents. The framework includes five alignment
  metrics: Goal Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality,
  and Plan Adherence.'
---

# What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment

## Quick Facts
- arXiv ID: 2510.08847
- Source URL: https://arxiv.org/abs/2510.08847
- Reference count: 40
- Primary result: GPA judges capture 95% of human-annotated errors vs. 54% baseline, with 86% error localization accuracy

## Executive Summary
This paper introduces the Agent GPA (Goal-Plan-Action) framework for systematically evaluating LLM agents. The framework includes five alignment metrics: Goal Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan Adherence. These metrics are assessed using specialized LLM judges that evaluate different aspects of agent behavior. Experimental results on TRAIL/GAIA and a production data agent show that GPA judges capture 95% of human-annotated errors (up from 54% baseline) and localize 86% of errors for targeted debugging. The judges show strong agreement with human evaluations (82% accuracy) and exhibit high consistency across repeated runs (Krippendorff's α up to 0.77).

## Method Summary
The GPA framework evaluates LLM agents through seven specialized LLM judges (Goal Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, Plan Adherence, Tool Selection, Tool Calling) that assess different aspects of agent behavior. Judges receive custom prompts with agent architecture descriptions, few-shot examples, and structured output templates. The framework processes OpenTelemetry traces by extracting system instructions and messages while managing context window limits. Judges operate independently and in parallel, producing scores with rationales that are aggregated and correlated with human annotations. The approach decomposes evaluation into narrow, scoped tasks rather than using monolithic evaluators.

## Key Results
- GPA judges capture 95% of human-annotated errors compared to 54% baseline monolithic evaluators
- Judges localize 86% of errors to specific spans, enabling targeted debugging
- High consistency across repeated runs (Krippendorff's α up to 0.77) with 82% agreement with human evaluations

## Why This Works (Mechanism)

### Mechanism 1
Decomposing agent evaluation into specialized, scoped judges substantially improves error coverage and localization over monolithic evaluators. Instead of one LLM judge processing an entire trace for all failure types, the framework uses seven specialized judges, each with narrow evaluation criteria and custom prompts. This reduces cognitive load per judge and enables targeted detection of specific failure modes. The core assumption is that LLM judges perform better when tasked with narrow, well-defined evaluation criteria rather than global holistic assessments. Evidence shows GPA judges collectively outperform TRAIL's baseline monolithic judge with complementary precision/recall profiles across different error types.

### Mechanism 2
Providing explicit agent architecture context in judge prompts improves error localization accuracy. Judges receive custom instructions describing the agent's control flow and available tools. This context helps judges attribute failures to specific spans and understand inter-agent dependencies. The core assumption is that judges can better localize errors when they understand the hierarchical structure and tool contracts of the evaluated agent. Evidence shows baseline with control flow localized 49% of errors versus 31% without architecture context.

### Mechanism 3
Specialized judges exhibit contextual performance variation based on error severity, enabling strategic ensemble deployment. Judges show different precision/recall profiles depending on error impact. For example, Plan Adherence fails on low-impact errors but excels at localizing high-impact failures (F1=0.85). Tool Selection has high recall for detection but becomes highest-precision for high-impact localization. The core assumption is that error severity correlates with detectable patterns that different judges are differentially sensitive to. Evidence shows detailed per-judge performance by impact level reveals role reversals and specialization shifts.

## Foundational Learning

- **Concept: Agent operational loop (Goal → Plan → Action)**
  - Why needed here: The entire GPA framework is structured around this triad; understanding how goals decompose into plans, and plans into actions, is prerequisite to interpreting judge outputs.
  - Quick check question: Given a trace where an agent generates a plan then calls tools, can you identify which GPA dimension evaluates plan-to-action alignment?

- **Concept: LLM-as-Judge evaluation paradigm**
  - Why needed here: All GPA metrics rely on LLM judges with structured output templates; understanding prompt engineering, scoring rubrics, and judge reliability metrics (Krippendorff's α) is essential.
  - Quick check question: What does Krippendorff's α = 0.77 indicate about judge consistency, and why might Plan Quality's α = 0.628 be concerning?

- **Concept: OpenTelemetry trace spans**
  - Why needed here: Judges localize errors by citing span IDs; traces are preprocessed to extract system instructions and messages while managing context window limits.
  - Quick check question: If a trace exceeds the LLM context window, what preprocessing steps does the paper describe to make it evaluable?

## Architecture Onboarding

- **Component map:** Seven LLM judges organized around three core dimensions: Goal (Goal Fulfillment), Plan (Plan Quality, Plan Adherence), Action (Logical Consistency, Execution Efficiency, Tool Selection, Tool Calling). Each judge operates independently on preprocessed traces via TruLens OSS integration.

- **Critical path:** (1) Trace preprocessing (span traversal, deduplication, context window management) → (2) Judge initialization with custom instructions + few-shot examples → (3) Parallel judge execution → (4) Human verification of error identification and localization → (5) Score aggregation and correlation analysis.

- **Design tradeoffs:** Specialized judges improve coverage and interpretability but increase evaluation cost (7 LLM calls per trace). High-recall judges (Tool Selection) generate more false positives; high-precision judges (Tool Calling, Logical Consistency) may miss errors. Plan Quality shows lowest reliability (α=0.628)—consider omitting or redesigning rubrics.

- **Failure signatures:** (1) Judges disagree on middle scores (1-2 on 4-point scale)—paper buckets to 3-point scale to address; (2) Low-impact error detection unreliable across most judges; (3) Plan Quality inconsistent across dev/test splits, suggesting overfitting or insufficient exemplars.

- **First 3 experiments:**
  1. Replicate TRAIL/GAIA evaluation with all seven judges on 10-20 traces; measure error coverage against human annotations and compare to baseline monolithic judge.
  2. Ablate custom architecture instructions from judge prompts; quantify localization accuracy drop to validate context contribution.
  3. Run each judge 5 times on fixed traces; compute Krippendorff's α and standard deviation to verify reported consistency claims before production deployment.

## Open Questions the Paper Calls Out

- **Question:** Can the GPA framework be effectively extended to evaluate embodied agents operating in physical environments?
  - Basis in paper: The conclusion explicitly states future work should "extend the framework to embodied agents."
  - Why unresolved: The current study validates the framework only on text-based software agents (GAIA, Snowflake Intelligence) using tool calls, whereas embodied agents involve physical state changes, sensory feedback, and real-time constraints not covered by the current trace definitions.
  - What evidence would resolve it: Successful application of GPA metrics to robotics simulations (e.g., AI2-THOR or Habitat) showing high agreement between GPA judges and human evaluators on physical task completion.

- **Question:** Can rubric generation for LLM judges be automated while maintaining or improving upon manual prompt engineering?
  - Basis in paper: The conclusion lists "automate rubric generation" as a necessary step for future work.
  - Why unresolved: The current judges rely on manually iterated prompts refined to avoid overfitting; scalability to new domains or agent architectures currently requires significant human effort.
  - What evidence would resolve it: A system that automatically generates judge prompts that achieves comparable Krippendorff's α (>0.77) and error coverage (>95%) on the TRAIL/GAIA benchmark without manual refinement.

- **Question:** How can reference-free metrics for Goal Fulfillment and Plan Quality be refined to improve reliability and reduce false positives?
  - Basis in paper: The conclusion calls to "refine reference-free metrics for goal fulfillment and plan quality."
  - Why unresolved: The experimental results showed that the Plan Quality judge suffered from poor precision (high false positive rates) and lower consistency (α = 0.628) compared to action-based judges like Tool Calling.
  - What evidence would resolve it: Development of new reference-free evaluation algorithms that demonstrate higher precision and correlation with human judgment on plan quality than the current few-shot LLM judge implementation.

## Limitations
- Small sample sizes for certain error categories (Plan Quality n=15 test errors) make performance estimates unreliable
- Increased computational cost (7 LLM calls per trace) and notable reliability variance, particularly for Plan Quality (α=0.628)
- Critical implementation details like exact few-shot examples and preprocessing code remain unspecified

## Confidence
- **High confidence**: Error coverage improvements (95% vs 54% baseline) and error localization (86%) are well-supported by experimental data and human annotation comparisons
- **Medium confidence**: Judge consistency claims are supported but show significant variance, particularly for Plan Quality; severity-based judge performance patterns need more robust sample sizes
- **Low confidence**: Framework's performance on production data is only briefly mentioned without detailed results; claims about strategic ensemble deployment based on error severity lack sufficient validation data

## Next Checks
1. Run the complete GPA framework on a held-out subset of TRAIL/GAIA traces (10-20 samples) to verify the 95% error coverage claim independently and measure computational overhead
2. Systematically ablate the custom architecture context instructions from judge prompts and measure the impact on error localization accuracy to validate this mechanism
3. Conduct inter-annotator agreement studies with human evaluators on the same traces to establish ground truth reliability and assess whether LLM judges are capturing the same error patterns humans identify