---
ver: rpa2
title: 'DeepEyeNet: Generating Medical Report for Retinal Images'
arxiv_id: '2509.12534'
source_url: https://arxiv.org/abs/2509.12534
tags:
- medical
- retinal
- image
- huang
- jia-hong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses the growing challenge of retinal disease
  diagnosis, where the demand for ophthalmologists exceeds available expertise, leading
  to bottlenecks in patient care. The research proposes AI-based methods to automate
  medical report generation from retinal images, reducing clinicians' workloads and
  improving diagnostic efficiency.
---

# DeepEyeNet: Generating Medical Report for Retinal Images

## Quick Facts
- arXiv ID: 2509.12534
- Source URL: https://arxiv.org/abs/2509.12534
- Authors: Jia-Hong Huang
- Reference count: 40
- Key outcome: Multi-modal deep learning approach for automated retinal report generation, achieving state-of-the-art BLEU-1 of 0.697 and CIDEr of 0.565 with expert-defined keywords.

## Executive Summary
This thesis addresses the critical bottleneck in retinal disease diagnosis caused by a shortage of ophthalmologists by proposing AI-driven automated medical report generation from retinal images. The research introduces a multi-modal deep learning framework that combines textual keywords with retinal images to produce comprehensive diagnostic reports, significantly reducing clinician workload while maintaining high accuracy. The proposed methods demonstrate superior performance on established metrics including BLEU, ROUGE, CIDER, and METEOR scores, showing the potential to revolutionize retinal disease diagnosis through improved efficiency and accuracy.

## Method Summary
The approach uses a multi-modal deep learning architecture that fuses expert-defined or predicted keywords with retinal image features to generate medical reports. The system employs a CNN-based image feature extractor, a contextualized keyword encoder, and a TransFuser module that uses non-local attention to combine image and keyword features before decoding. The decoder can be either LSTM-based or Transformer-based, with attention mechanisms to guide the generation process. The model is trained end-to-end using cross-entropy loss, with inference employing beam search or scheduled sampling strategies.

## Key Results
- Achieved BLEU-1 score of 0.697 and CIDEr score of 0.565 using expert-defined keywords
- Demonstrated significant performance improvement over image-only baselines (BLEU-1: 0.423 for "Show and tell" baseline)
- Showed performance degradation to BLEU-1 of 0.527 when using predicted keywords instead of expert-defined keywords

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal fusion of expert-defined keywords with retinal images improves medical report generation quality over image-only approaches.
- Mechanism: Keywords provide explicit semantic anchors that guide the decoder toward clinically relevant terminology, reducing the search space for medical language generation. The fusion occurs via average-based combination and attention mechanisms that align visual features with textual concepts.
- Core assumption: Expert-defined keywords accurately summarize the critical clinical findings present in the retinal image.
- Evidence anchors:
  - [abstract]: "A multi-modal deep learning approach captures interactions between textual keywords and retinal images, resulting in more comprehensive medical reports"
  - [section]: Table 1 shows keyword-driven models (BLEU-1: 0.697) outperform non-keyword baselines like "Show and tell" (BLEU-1: 0.423)
  - [corpus]: RetinalGPT and similar works confirm multi-modal guidance improves medical captioning; however, direct replication of keyword-fusion specifically is limited in corpus
- Break condition: If keywords are noisy, predicted rather than expert-defined, or semantically misaligned with image content, performance degrades (Table 3 shows BLEU-1 drops from 0.697 to 0.527 with predicted keywords).

### Mechanism 2
- Claim: Contextualized keyword representations capture medical terminology nuances better than static embeddings.
- Mechanism: Keywords are encoded through a contextualized encoder (Figure 6) that reinforces textual features against visual context, enabling the model to disambiguate terms like "vascular" or "optic" based on co-occurring image features.
- Core assumption: Medical keywords have context-dependent meanings that static embeddings cannot capture.
- Evidence anchors:
  - [abstract]: "Improved methods for medical keyword representation enhance the system's ability to capture nuances in medical terminology"
  - [section]: Figure 6 depicts "Contextualized Keyword Encoder" and "Textual Feature Reinforcement" modules
  - [corpus]: Corpus papers do not directly validate contextualized keyword encoding; evidence is paper-internal
- Break condition: If the keyword vocabulary is too small or terms are overly generic, contextualization provides marginal benefit.

### Mechanism 3
- Claim: Non-local attention (TransFuser) mitigates RNN long-range dependency limitations in lengthy medical descriptions.
- Mechanism: TransFuser computes attention across all keyword-image feature pairs without sequential constraints, allowing the decoder to reference distant semantic cues mid-generation. This bypasses RNN forgetting over long sequences.
- Core assumption: Critical diagnostic information may appear early in the report but must inform later sentences.
- Evidence anchors:
  - [abstract]: "Strategies to overcome RNN-based models' limitations, particularly in capturing long-range dependencies"
  - [section]: Figure 7 shows TransFuser integrating image and keyword features via scaled dot-product attention before LSTM decoding
  - [corpus]: DRetNet and related works emphasize attention for medical imaging but do not specifically validate TransFuser's long-dependency claim
- Break condition: If medical reports are short (few sentences), the long-dependency advantage diminishes; standard attention suffices.

## Foundational Learning

- Concept: Multi-modal attention mechanisms
  - Why needed here: The architecture relies on aligning visual features with textual keywords through attention; misunderstanding this blocks comprehension of the fusion strategy.
  - Quick check question: Can you explain how attention weights determine which image regions influence which generated words?

- Concept: RNN seq2seq limitations (vanishing gradients, long-range forgetting)
  - Why needed here: The paper frames TransFuser as a solution to RNN weaknesses; you must understand the problem to evaluate the solution.
  - Quick check question: Why does an LSTM struggle to maintain context from sentence 1 when generating sentence 10?

- Concept: Medical image captioning evaluation metrics (BLEU, ROUGE, CIDEr, METEOR)
  - Why needed here: All performance claims are anchored to these metrics; without understanding them, you cannot assess whether the improvements are meaningful.
  - Quick check question: What does CIDEr measure that BLEU does not, and why is it relevant for medical reports?

## Architecture Onboarding

- Component map: Retinal Image -> CNN Feature Extractor -> TransFuser -> LSTM/Transformer Decoder -> Generated Report
- Critical path: Image → CNN features → fuse with keyword embeddings via TransFuser → attend and decode → generate report tokens sequentially. Keywords must be available at inference (expert-defined or predicted).
- Design tradeoffs:
  - Using predicted keywords vs. expert-defined keywords: predicted keywords reduce human input but lower BLEU-1 by ~0.17 (Table 3).
  - LSTM decoder vs. Transformer: LSTM is simpler but struggles with long reports; Transformer handles long-dependency better but requires more data.
  - Attention visualization improves interpretability but adds computational overhead and may not align perfectly with clinical reasoning.
- Failure signatures:
  - Generated reports are grammatically fluent but clinically nonsensical → keyword-image misalignment or insufficient multi-modal fusion.
  - Performance collapses with predicted keywords → keyword classifier is weak; consider joint training or better keyword prediction.
  - Attention maps highlight irrelevant regions → attention module not properly trained; check supervision signals or regularization.
- First 3 experiments:
  1. Replicate Table 1 baseline comparison on EyeNet or DeepEyeNet dataset to validate that keyword-driven models outperform image-only models by reported margins.
  2. Ablate TransFuser by replacing it with simple concatenation; measure BLEU/CIDEr drop to quantify the non-local attention contribution.
  3. Test with predicted keywords from a separately trained classifier; confirm performance degradation aligns with Table 3, then explore joint keyword-report training to close the gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Large Language Models (LLMs) and diffusion models be effectively integrated to further improve the retinal disease diagnosis process beyond current RNN/CNN-based methods?
- Basis in paper: [explicit] The author states in the Conclusions and Future Work section: "I plan to explore LLMs and diffusion models for further improvement in the retinal disease diagnosis process."
- Why unresolved: The current thesis focuses on RNN-based units and multi-modal deep learning approaches (CNNs/LSTMs) with attention mechanisms. The potential of generative AI architectures like LLMs and diffusion models remains unexplored within this specific retinal image context.
- What evidence would resolve it: Comparative studies evaluating the performance of LLM-based or diffusion-based report generators against the current state-of-the-art metrics (BLEU, CIDEr) and qualitative clinical evaluations.

### Open Question 2
- Question: How can the performance gap between using expert-defined (ground-truth) keywords and predicted keywords be minimized?
- Basis in paper: [inferred] Table 3 demonstrates a significant performance drop when the model uses "predicted keywords" (e.g., BLEU-1 drops from 0.6969 to 0.5268). The abstract and introduction emphasize the success using expert-defined keywords, implying the robustness of the "predicted" pathway is an unresolved limitation for fully automated workflows.
- Why unresolved: While the model performs well with ground-truth inputs, a fully autonomous system must rely on predicted keywords. The substantial metric degradation indicates the feature extraction or reinforcement strategy for non-expert inputs is not yet optimized.
- What evidence would resolve it: Development of a pipeline where the performance delta between predicted and ground-truth keyword inputs is statistically insignificant on standard metrics like BLEU and ROUGE.

### Open Question 3
- Question: To what extent does the proposed keyword-driven attention mechanism translate into clinical trust and acceptance for ophthalmologists?
- Basis in paper: [explicit] The introduction asks: "How can we improve the explainability of automated medical report generation for retinal images?" and notes that "The interpretability... remains a significant challenge, hindering their widespread acceptance."
- Why unresolved: While the paper proposes technical solutions (visual explanations, keyword attention) to address explainability, clinical acceptance is a socio-technical challenge. Technical interpretability does not automatically equate to user trust in a high-stakes medical environment.
- What evidence would resolve it: Results from clinical user studies involving ophthalmologists, measuring diagnostic confidence and perceived utility when using the "DeepEyeNet" system compared to traditional methods.

## Limitations

- Architectural detail gaps: Key hyperparameters (learning rates, batch sizes, attention head counts) and exact network dimensions are unspecified, limiting exact reproduction capability.
- Dataset dependency: Claims rely on proprietary or under-specified datasets (EyeNet, DeepEyeNet) without open-access validation, constraining external verification.
- Keyword quality assumption: The system's performance critically depends on high-quality expert-defined keywords, but no quantitative analysis is provided on keyword relevance or coverage relative to report content.

## Confidence

- High confidence: Multi-modal fusion with keywords improves report quality over image-only baselines (supported by Table 1 and consistent with corpus literature on medical captioning).
- Medium confidence: Contextualized keyword representations enhance terminology handling (inferred from module description but not empirically validated against static embeddings).
- Low confidence: TransFuser effectively mitigates long-range dependency issues (stated but not directly tested against comparable RNN-based or attention-only baselines in the provided evidence).

## Next Checks

1. **Ablation study on TransFuser**: Replace TransFuser with simple concatenation or standard attention; measure changes in BLEU/CIDEr to isolate the contribution of non-local attention.
2. **Keyword quality impact test**: Compare performance using noisy, predicted, and expert-defined keywords; quantify the degradation to assess keyword dependency.
3. **External dataset replication**: Apply the method to a public retinal dataset (e.g., EyePACS) to verify generalization beyond the proprietary EyeNet/DeepEyeNet data.