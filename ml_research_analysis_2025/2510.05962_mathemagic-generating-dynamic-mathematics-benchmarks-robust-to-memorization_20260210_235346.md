---
ver: rpa2
title: 'MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization'
arxiv_id: '2510.05962'
source_url: https://arxiv.org/abs/2510.05962
tags:
- qwen2
- math
- reasoning
- test
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MATHEMAGIC, a framework for benchmarking mathematical
  reasoning in large language models using dynamically generated, counterfactual math
  problems. The core method involves systematically altering arithmetic rules (e.g.,
  redefining operators or number representations) at test time, controlled by random
  seeds, to create problems that penalize memorization and require genuine reasoning.
---

# MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization

## Quick Facts
- arXiv ID: 2510.05962
- Source URL: https://arxiv.org/abs/2510.05962
- Authors: Dayyán O'Brien; Barry Haddow; Emily Allaway; Pinzhen Chen
- Reference count: 40
- Primary result: Models perform significantly better under deductive reasoning (when given explicit rules) than inductive reasoning (when required to infer rules from examples)

## Executive Summary
This paper introduces MATHEMAGIC, a framework for benchmarking mathematical reasoning in large language models using dynamically generated, counterfactual math problems. The core method involves systematically altering arithmetic rules (e.g., redefining operators or number representations) at test time, controlled by random seeds, to create problems that penalize memorization and require genuine reasoning. Experiments on a range of models (including GPT-5, GPT-4o-mini, Llama3, and Qwen2.5 variants) reveal that models perform significantly better under deductive reasoning than inductive reasoning. Fine-tuning on these tasks does not impart a general inductive reasoning skill but instead leads to memorization of superficial patterns. The study highlights that even strong models struggle to suppress ingrained knowledge, and their reasoning capabilities are limited, especially for complex, multi-step transformations.

## Method Summary
MATHEMAGIC generates dynamic math benchmarks by transforming GSM8K expressions using 9 counterfactual operations (e.g., digit swaps, operator redefinitions, base changes) controlled by random seeds. The framework evaluates models under both inductive settings (inferring rules from examples) and deductive settings (applying explicit rules), using greedy decoding and automatic answer verification. Test instances are procedurally generated at evaluation time, enabling reproducibility through fixed seeds while resisting contamination through seed variation. The approach quantifies both accuracy and reversion to standard math, tracking when models default to memorized arithmetic despite counterfactual contexts.

## Key Results
- Models perform significantly better under deductive reasoning (given explicit rules) than inductive reasoning (inferring from examples)
- Fine-tuning on counterfactual tasks improves performance on seen transformations (+60%) but degrades on held-out transformations (-21%), showing memorization without generalization
- Even strong models show high reversion rates to standard math (40-60% for smaller models), indicating difficulty suppressing ingrained knowledge
- Larger models show better few-shot learning scaling, but performance plateaus or degrades beyond certain shot counts

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Rule Transformation
Altering arithmetic rules forces models to reason from context rather than relying on memorized patterns. By systematically redefining operators, number representations, or applying global operations, the benchmark creates problems where pre-trained mathematical knowledge is systematically incorrect. If models cannot rely on memorized arithmetic patterns, they must perform genuine reasoning from context to succeed.

### Mechanism 2: Seeded Procedural Generation
Random-seed-controlled test generation enables both reproducibility and resistance to contamination. Each test instance uses a random seed to select base expressions and transformation parameters; same seed produces identical tests for reproducibility, different seeds create novel instances, creating a "moving target" that resists memorization.

### Mechanism 3: Inductive vs Deductive Separation
The gap between rule inference (induction) and rule application (deduction) reveals whether models exhibit genuine reasoning capability. Inductive setting requires models to infer rules solely from examples; deductive setting provides explicit natural-language rule descriptions; comparing both isolates inference capability from execution capability.

## Foundational Learning

- Concept: In-context learning scaling
  - Why needed here: The benchmark relies on models adapting to novel rules through few-shot examples; understanding ICL scaling helps interpret why performance improves then plateaus
  - Quick check question: How does accuracy scale from 0 to 512 shots for GPT-5 vs Qwen2.5 0.5B? (GPT-5: ~20% → 87%; Qwen 0.5B: flat ~5%)

- Concept: Benchmark contamination and memorization
  - Why needed here: The paper's core motivation is that static benchmarks like GSM8K may be memorized during training, inflating capability estimates
  - Quick check question: What does "correct w.r.t. original math" measure and why does it matter? (Tracks when models output standard arithmetic answers despite counterfactual rules; persistent high rates indicate memorization bias)

- Concept: Inductive vs deductive reasoning distinction
  - Why needed here: The benchmark explicitly separates these modes to measure whether models can infer rules vs merely apply given rules
  - Quick check question: For GPT-4o-mini, what is the accuracy difference between 512-shot inductive and zero-shot deductive? (47.2% vs 71.4%—explicit rule beats 512 examples)

## Architecture Onboarding

- Component map:
  - GSM8K expression extraction → Transformation application → Seeded test generation → Prompt formatting (inductive/deductive) → Model inference → Answer extraction and verification

- Critical path:
  1. Extract ground truth expressions from GSM8K (parse «15 + 4 = 19» format)
  2. Select expression via random seed
  3. Apply transformation with seed-determined parameters
  4. Generate few-shot examples using same transformation and parameters
  5. Format prompt: inductive (examples only) or deductive (examples + natural language rule hint)
  6. Model inference with greedy decoding
  7. Extract answer from \boxed{}, compare to computed ground truth within tolerance

- Design tradeoffs:
  - Using GSM8K expressions: Intentionally uses potentially memorized equations to measure reversion behavior vs creating novel expressions
  - Fixed transformation set: Enables cross-model comparison vs may become overfittable over time
  - 512 max shots: Tests many-shot learning limits vs context length constraints
  - Automatic answer verification: Enables scale vs may miss valid alternative reasoning paths

- Failure signatures:
  - Reversion errors: Model outputs standard math answer despite counterfactual context (persists at 40-60% rate for smaller models even at 512 shots)
  - Novel errors: Incorrect attempts to apply new rules (increases with shots as model tries but fails procedurally)
  - Format errors: Unparsable output (especially high for small models: 64% at zero-shot for Qwen 0.5B)
  - Performance plateau/degradation: Accuracy stalls or drops beyond certain shot counts (Llama3 8B degrades after 128 shots)
  - Training-test gap: Fine-tuned models improve +60% on seen transformations but degrade -21% on held-out ones

- First 3 experiments:
  1. Validate benchmark stability: Run zero-shot and 8-shot evaluation across 5 seeds and 4 prompt variants; expect SD < 3% across seeds for most models
  2. Establish inductive-deductive gap: Compare 512-shot inductive vs zero-shot deductive across model scales; expect deductive > inductive with larger gaps for smaller models
  3. Test memorization hypothesis: Plot "correct w.r.t. original math" rate vs shot count; expect rate decreases but remains above dashed "Transformed Answer Match" baseline

## Open Questions the Paper Calls Out

- Can the counterfactual dynamic benchmarking methodology be successfully extended to semi-open-ended domains like linguistic reasoning or machine translation?
- What is the human performance upper bound on MatheMagic, particularly regarding the ability to process large inductive contexts (e.g., 512 examples)?
- Do specialized math-adapted models permanently sacrifice general reasoning plasticity, and can this be reversed?

## Limitations

- The specific GSM8K expressions used for evaluation are not fully disclosed, preventing exact replication
- Transformation parameter ranges (e.g., constant bounds for ADD_C, valid bases for CHG_B) remain unspecified
- The 45 GSM8K expressions represent a relatively small evaluation set that may not capture full mathematical reasoning diversity
- The study focuses on 7B-70B parameter models, leaving open questions about performance on frontier models or smaller architectures

## Confidence

**High Confidence**: The benchmark design and procedural generation mechanism - The methodology for creating counterfactual transformations and seeded evaluation is clearly specified and technically sound.

**Medium Confidence**: The empirical findings on inductive vs deductive reasoning gaps - While the general pattern of deductive superiority is clearly demonstrated, specific accuracy values depend on undisclosed GSM8K subset and transformation parameters.

**Medium Confidence**: The fine-tuning generalization results - The core finding that fine-tuning improves performance on seen transformations but degrades on held-out ones is well-supported, but magnitude of effects may vary with different training hyperparameters.

## Next Checks

1. **Benchmark Stability Validation**: Run the proposed stability experiment across 5 random seeds using the published transformation library and expression extraction pipeline. Measure standard deviation in accuracy across seeds to verify the claimed stability (expected SD < 3% for most models).

2. **Generalization Test**: Fine-tune a 7B model on the proposed training procedure (QLoRA, rank=64, α=128, LR=5×10^-4, epochs=5, batch=64, bf16, seed=8) and evaluate both on seen and held-out transformations. Confirm the reported generalization gap pattern where seen transformations improve +60% while held-out transformations degrade -21%.

3. **Meme Memorization Quantification**: Implement the "correct w.r.t. original math" metric tracking and plot its decay curve across shot counts (0, 8, 32, 128, 512). Verify that rates decrease from baseline but remain above the dashed "Transformed Answer Match" baseline, confirming incomplete suppression of memorized arithmetic knowledge.