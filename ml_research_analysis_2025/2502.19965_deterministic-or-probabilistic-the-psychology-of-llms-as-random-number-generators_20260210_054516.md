---
ver: rpa2
title: Deterministic or probabilistic? The psychology of LLMs as random number generators
arxiv_id: '2502.19965'
source_url: https://arxiv.org/abs/2502.19965
tags:
- number
- random
- range
- llms
- numbers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how Large Language Models (LLMs) perform\
  \ as random number generators, revealing that despite their probabilistic design,\
  \ LLMs often produce deterministic outputs due to inherent training data biases.\
  \ The study tests six models (DeepSeek-R1, Gemini 2.0, GPT-4o-mini, Llama 3.1-8b,\
  \ Mistral-7b, and Phi-4-14b) across seven languages and three numerical ranges (1\u2013\
  5, 1\u201310, 1\u2013100), with 100 calls per configuration."
---

# Deterministic or probabilistic? The psychology of LLMs as random number generators

## Quick Facts
- arXiv ID: 2502.19965
- Source URL: https://arxiv.org/abs/2502.19965
- Reference count: 8
- Key outcome: LLMs produce deterministic number outputs despite probabilistic design due to training data biases, with strong preferences for specific numbers (3 for 1-5 range, 7 for 1-10 range) across models and languages

## Executive Summary
This study reveals that Large Language Models, despite being designed as probabilistic systems, exhibit deterministic behavior when generating random numbers. Testing six models across seven languages and three numerical ranges shows consistent biases toward specific numbers (like 3, 7, and 42) that reflect human cognitive preferences rather than true randomness. Even high temperature settings fail to eliminate these patterns, suggesting that training data biases fundamentally constrain LLM behavior in tasks requiring genuine randomness.

## Method Summary
The study tests six LLMs (DeepSeek-R1, Gemini 2.0, GPT-4o-mini, Llama 3.1-8b, Mistral-7b, Phi-4-14b) across seven languages using the prompt "Give me a random number between 1 and X. Please only return the number with no additional text." For each model-language-range-temperature combination (100 calls per configuration), the study computes a Randomness Index incorporating range, standard deviation, and normalized Shannon entropy, then compares results against Python's `randint()` baseline. Temperature varies from 0.1 to 2.0 while default top-k/top-p parameters remain unspecified.

## Key Results
- LLMs consistently prefer specific numbers: 3 dominates 1-5 range, 7 dominates 1-10 range, and 42 appears frequently in 1-100 range
- Temperature adjustments (0.1-2.0) show minimal impact on distribution diversity across all tested models
- Cross-lingual tests reveal language-specific variations but persistent model-level "fingerprint" preferences
- Randomness Index scores remain low compared to Python's `randint()`, indicating deterministic rather than probabilistic behavior

## Why This Works (Mechanism)

### Mechanism 1: Training Data Bias Transfer
LLMs reproduce human cognitive biases when generating "random" numbers by treating numbers as linguistic tokens rather than mathematical entities. During pretraining, models learn statistical patterns from human-generated text where numbers appear in culturally significant contexts, creating biased probability distributions. Self-attention captures these contextual associations, not just raw frequencies, leading to preferences for "lucky" numbers and central tendency bias.

### Mechanism 2: Token-Level Probability Concentration
Number tokens occupy specific positions in embedding space with learned probability peaks that resist temperature-based smoothing. Even with temperature scaling (T=2.0), the logits for preferred number tokens remain disproportionately high because temperature divides logits uniformly, but large initial differences in logit values for preferred numbers persist. The model's vocabulary structure, treating single-digit numbers as individual tokens and multi-digit numbers as sequences, creates additional concentration effects.

### Mechanism 3: Cross-Lingual Bias Variation with Persistent Fingerprints
Prompt language affects output distribution, but model-specific "fingerprint" preferences persist across languages, indicating dual-source bias. Language-specific training corpora have different statistical patterns, but shared multilingual representations retain model-level biases. Self-attention computes language-dependent context within a unified embedding space where certain number associations transfer across languages.

## Foundational Learning

- **Concept: Transformer Self-Attention and Next-Token Prediction**
  - Why needed here: Understanding why "probabilistic" models produce deterministic outputs requires grasping that attention computes probability distributions over tokens, which can be sharply peaked.
  - Quick check question: Can you explain why increasing temperature from 0.1 to 2.0 might not produce uniform distributions if the initial logit differences are large?

- **Concept: Token Embeddings vs. Numerical Semantics**
  - Why needed here: The paper emphasizes that LLMs treat "2" as a token vector, not the mathematical concept of two. This explains why frequency/association patterns dominate.
  - Quick check question: How would an LLM's behavior differ if it had access to a symbolic math module during generation?

- **Concept: Randomness Metrics (Entropy, χ² Test, Cramér's V)**
  - Why needed here: The paper's "randomness index" combines normalized range, standard deviation, and Shannon entropy. Understanding these components is necessary to interpret results.
  - Quick check question: Given the randomness index formula RI = R* · σ* · Hnorm / (log(range) · √T), why does it penalize high-temperature models that still produce narrow distributions?

## Architecture Onboarding

- **Component map:** Input prompt in one of 7 languages → 6 model architectures (DeepSeek-R1/Gemini 2.0/GPT-4o-mini/Llama 3.1-8b/Mistral-7b/Phi-4-14b) → Sampling parameters (Temperature 0.1–2.0, default top-k/top-p) → Output parsing → Randomness index computation, χ² test, Cramér's V, distribution visualization

- **Critical path:** Construct language-specific prompts → Execute 100 calls per configuration → Extract numeric outputs, filter malformed responses → Compute randomness index per configuration → Compare against Python `randint()` baseline

- **Design tradeoffs:** Sample size (100 vs. 1000) found sufficient for detecting strong biases; model selection excluded SLMs and similar models to reduce redundancy; prompt minimalism avoided engineering that might hide natural biases

- **Failure signatures:** Phi-4 + Japanese returns number lists or text instead of single number; GPT-4o-mini at T=2.0 outputs incoherent text after number in ~10-15% of calls; DeepSeek-R1 occasionally produces out-of-range numbers

- **First 3 experiments:**
  1. Replicate baseline with Llama 3.1-8b, English prompt, 1-10 range, T=1.0; compute randomness index (expected strong "7" preference)
  2. Temperature sensitivity test for Mistral-7b: 100 calls per temperature [0.1, 0.5, 1.0, 2.0] for 1-10 range in Spanish; verify minimal distribution shape changes despite 20× temperature increase
  3. Cross-lingual comparison for Gemini 2.0: 1-5 range across all 7 languages at T=1.0; confirm Japanese preference shift toward "1" vs. "3" for other languages

## Open Questions the Paper Calls Out

- **Question:** How does adjusting top-p and top-k sampling parameters influence the randomness of LLM outputs compared to temperature modifications alone?
  - Basis: The authors state future work could include modification of top-p and top-k parameters left at default values
  - Why unresolved: Experiments focused exclusively on temperature settings, leaving nucleus sampling impact untested
  - Evidence needed: Systematic variation of top-p and top-k values to observe changes in Randomness Index

- **Question:** Does altering numerical ranges to non-standard intervals or using multi-token numbers change deterministic output patterns?
  - Basis: Paper lists "giving a range not starting in 1, or a range so big the numbers are composed by more than one token" as possible extensions
  - Why unresolved: Current study limited to ranges starting at 1 (1–5, 1–10, 1–100)
  - Evidence needed: Experimental data from prompts requesting ranges like 50–60 or 1000–2000

- **Question:** Can specific prompt engineering strategies successfully mitigate the deterministic biases inherent in LLMs?
  - Basis: Conclusion suggests "detailed prompt engineering could partially change the results"
  - Why unresolved: Authors deliberately used naive prompt to expose raw biases without instructions that might "hide" natural tendencies
  - Evidence needed: Comparative tests using engineered prompts to see if distribution uniformity improves

## Limitations

- Temperature implementation dependency: Paper tests temperature scaling but doesn't specify how different models handle temperature parameters internally, which may affect observed minimal impact
- Corpus bias verification gap: Relies on logical inference rather than direct corpus analysis to prove training data causes observed number generation biases
- Multilingual representation complexity: Cross-lingual bias persistence assumes shared embedding spaces without investigating whether models use separate or shared parameter subsets for different languages

## Confidence

- **High Confidence (Level 4/5):** Empirical findings of strong number preferences across multiple models and languages are highly reproducible with robust quantitative evidence from randomness index and χ² tests
- **Medium Confidence (Level 3/5):** Causal explanation linking training data biases to observed number preferences is well-reasoned but not directly proven without corpus-level frequency analysis
- **Low Confidence (Level 2/5):** Cross-lingual "fingerprint" persistence mechanism is most speculative claim without architectural analysis of multilingual model parameter sharing

## Next Checks

1. **Corpus Frequency Analysis:** Analyze actual frequency distributions of numbers 1-100 in training corpora of tested models to verify whether numbers like 7 and 3 appear disproportionately often in contexts suggesting "randomness" or "choice" scenarios

2. **Architectural Parameter Isolation:** For multilingual models like Llama 3.1-8b, examine whether number-related tokens use shared versus language-specific embedding parameters by comparing embedding similarity across languages and testing fine-tuning effects

3. **Alternative Sampling Strategy Comparison:** Test whether alternative sampling strategies (nucleus sampling with varying top-p, temperature annealing schedules, or custom probability smoothing) can overcome observed concentration effects better than uniform temperature scaling