---
ver: rpa2
title: Aligning to What? Limits to RLHF Based Alignment
arxiv_id: '2503.09025'
source_url: https://arxiv.org/abs/2503.09025
tags:
- biases
- llama
- covert
- rlhf
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of Reinforcement Learning
  from Human Feedback (RLHF) in mitigating both overt and covert biases in large language
  models (LLMs), focusing on biases against African Americans. The authors train models
  using various RLHF techniques (DPO, ORPO, and RLOO) and evaluate their biases using
  matched-guise probing and explicit bias testing.
---

# Aligning to What? Limits to RLHF Based Alignment

## Quick Facts
- arXiv ID: 2503.09025
- Source URL: https://arxiv.org/abs/2503.09025
- Authors: Logan Barnhart; Reza Akbarian Bafghi; Stephen Becker; Maziar Raissi
- Reference count: 13
- Primary result: RLHF alignment shows limited effectiveness in mitigating covert biases against African Americans

## Executive Summary
This paper investigates whether Reinforcement Learning from Human Feedback (RLHF) can effectively mitigate both overt and covert biases in large language models, with a focus on biases against African American English (AAE). The authors train multiple models using different RLHF techniques (DPO, ORPO, and RLOO) and evaluate their performance using both explicit bias testing and matched-guise probing methods. Despite applying various alignment approaches, the study finds that current RLHF methods have limited impact on reducing covert biases, with biases persisting or even being amplified in some cases. The research reveals that models tend to associate extreme positive and negative traits with AAE while linking neutral traits to Standard American English (SAE).

## Method Summary
The authors conducted experiments using three different RLHF methods: Direct Preference Optimization (DPO), Output Relative Preference Optimization (ORPO), and Reinforcement Learning with Optimized Objectives (RLOO). They trained models on three datasets: OpenHermes2.5, UltraFeedback, and a novel curated dataset called "Neutralize" designed to specifically target covert biases. The study used Llama 3 and Mistral 8B models, applying LoRA-based fine-tuning for efficiency. Bias evaluation was performed using matched-guise probing to detect covert associations and explicit testing for overt biases. The research also extended bias measurement tools to multimodal models by adapting covert bias detection methods for image-based contexts.

## Key Results
- RLHF alignment techniques show limited effectiveness in reducing overt biases in tested models
- Current RLHF methods may not adequately address covert biases and can potentially amplify them in some cases
- Models exhibit polarized trait associations with AAE (extreme positive/negative) versus SAE (neutral traits)
- Overt and covert biases can be polar opposites in multimodal models

## Why This Works (Mechanism)
RLHF works by optimizing model outputs based on human preference data, using reward models trained on labeled preference pairs. The mechanism involves creating a reward signal that guides the language model toward more desirable outputs through iterative fine-tuning. For overt biases, this process can effectively shift model behavior since these biases manifest in explicit, measurable ways. However, for covert biases, which operate through subtle linguistic associations rather than explicit content, the preference-based optimization struggles to capture and modify these implicit patterns. The reward models used in RLHF are typically trained on semantic content rather than stylistic features, making them poorly equipped to detect and correct dialect-based biases that don't alter the fundamental meaning of text.

## Foundational Learning

**Reinforcement Learning from Human Feedback (RLHF)**: A training paradigm where models learn from human preference data rather than direct labels. Why needed: RLHF is the dominant alignment technique for modern LLMs. Quick check: Can you explain how reward modeling works in RLHF?

**Direct Preference Optimization (DPO)**: A computationally efficient RLHF variant that optimizes preferences directly without reinforcement learning. Why needed: DPO has become popular due to its simplicity and effectiveness. Quick check: What makes DPO different from traditional RLHF approaches?

**Matched-Guise Probing**: A technique for measuring implicit biases by presenting identical content in different linguistic styles. Why needed: This method reveals covert biases that standard evaluation misses. Quick check: How does matched-guise testing detect dialect-based biases?

**Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning method that reduces computational requirements. Why needed: LoRA enables practical RLHF training on consumer hardware. Quick check: What are the trade-offs between LoRA and full fine-tuning?

**Covert vs. Overt Bias**: Overt biases are explicit and measurable, while covert biases operate through subtle associations. Why needed: Understanding this distinction is crucial for bias mitigation research. Quick check: Can you give an example of covert bias that might not appear in explicit testing?

## Architecture Onboarding

**Component Map**: Base LLM -> Reward Model -> RLHF Algorithm (DPO/ORPO/RLOO) -> Aligned Model

**Critical Path**: Base model fine-tuning → Reward model training → RLHF optimization → Bias evaluation → Analysis

**Design Tradeoffs**: The study prioritizes computational efficiency through LoRA fine-tuning and focuses on specific bias types rather than comprehensive bias evaluation, trading breadth for depth in understanding dialect-based biases.

**Failure Signatures**: When RLHF fails to reduce covert biases, the model may show polarized associations (extreme positive/negative for AAE) or maintain the status quo of existing biases. Overt biases may shift while covert biases remain calcified.

**3 First Experiments**:
1. Apply DPO to Llama 3 8B using OpenHermes2.5 dataset and measure changes in covert bias scores
2. Train a reward model on the Neutralize dataset and evaluate its ability to distinguish dialect-based preferences
3. Test matched-guise probing on a VLM to establish baseline multimodal covert bias measurements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can alignment datasets be curated to reduce covert biases when the specific dialectical features causing the bias are naturally omitted from the text?
- Basis in paper: The authors explicitly ask, "how does one curate data for a task that is specifically omitted from text?" noting that standard preference datasets focus on semantics rather than dialectical style.
- Why unresolved: Current preference datasets are not "contrastive" enough to signal a preference against specific dialects without altering the semantic meaning, making standard data curation techniques ineffective for this specific problem.
- What evidence would resolve it: The development of a synthetic data curation pipeline that generates preference pairs identical in meaning but differing in dialect, which successfully reduces covert association scores during training.

### Open Question 2
- Question: Does RLHF on multimodal models reduce the observed discrepancy where overt and covert biases appear as "polar opposites," or does it decouple them further?
- Basis in paper: The authors state their "preliminary experiments on measuring multi-modal covert biases seems to indicate a model's overt and covert biases can be polar opposites of one another," but they could not perform training on the VLM to verify the effect of alignment.
- Why unresolved: The study was computationally constrained from performing post-training on the Llama 3.2 Vision model, leaving the plasticity of these specific multimodal biases untested.
- What evidence would resolve it: A follow-up experiment applying DPO or RLOO to a Vision Language Model and measuring the resulting shifts in both image-based overt biases and text-based covert biases.

### Open Question 3
- Question: To what extent does the use of Low-Rank Adaptation (LoRA) restrict the ability of RLHF to modify covert biases compared to full-parameter fine-tuning?
- Basis in paper: The authors note in the limitations that they "were only capable of training the models with LoRA... Perhaps fully training the model would influence biases more."
- Why unresolved: The study isolated variables regarding algorithms (DPO, ORPO) and datasets but did not isolate the parameter-update mechanism (LoRA vs. dense training), leaving a potential confounding factor in the "rigidity" of biases.
- What evidence would resolve it: A controlled comparison where identical RLHF runs are performed on the same base model using LoRA versus full fine-tuning, measuring the magnitude of change in covert association scores.

### Open Question 4
- Question: Do covert biases and their resistance to alignment scale with model size, or do they diminish in larger parameter regimes?
- Basis in paper: The authors mention they "did not look at a large variation of model size" and suggest that "the behavior of biases differs as model size increases."
- Why unresolved: The experiments were restricted to 8B parameter models (Llama 3, Mistral), so it is unclear if the observed "calcification" of bias is a feature of smaller models or a universal property of the architecture.
- What evidence would resolve it: Evaluating the covert bias association scores of the Llama 3 family (8B, 70B, 405B) before and after alignment to identify scaling laws for covert bias.

## Limitations
- The study focuses primarily on African American English (AAE) and Standard American English (SAE) biases, which may not extend to other linguistic or cultural biases
- Covert bias assessment relies on matched-guise probing, which measures implicit associations rather than actual behavioral outcomes
- Results are based on specific RLHF variants (DPO, ORPO, RLOO) and curated datasets, limiting generalizability to other alignment techniques

## Confidence

**High confidence**: RLHF alignment techniques show limited effectiveness in reducing overt biases in the tested models
**Medium confidence**: RLHF may not adequately address covert biases and can potentially amplify them in some cases
**Medium confidence**: Models exhibit polarized trait associations with AAE (extreme positive/negative) versus SAE (neutral traits)

## Next Checks
1. Test RLHF alignment effectiveness across a broader range of demographic groups and linguistic varieties beyond AAE/SAE
2. Conduct behavioral experiments to validate whether covert bias measurements predict real-world model outputs
3. Evaluate the impact of different RLHF dataset compositions and training durations on bias mitigation outcomes