---
ver: rpa2
title: 'PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual
  Training'
arxiv_id: '2503.06486'
source_url: https://arxiv.org/abs/2503.06486
tags:
- object
- list
- node
- image
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in multimodal large language
  models (MLLMs) for dense image captioning. The authors introduce HalFscore, a concept-level
  metric for evaluating accuracy and completeness of dense captions, and propose PerturboLLaVA,
  a training method that reduces the model's reliance on language priors by incorporating
  adversarially perturbed text during training.
---

# PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training

## Quick Facts
- **arXiv ID:** 2503.06486
- **Source URL:** https://arxiv.org/abs/2503.06486
- **Reference count:** 40
- **Primary result:** Introduces HalFscore metric and PerturboLLaVA training method to reduce hallucinations in dense image captioning

## Executive Summary
This paper addresses hallucinations in multimodal large language models (MLLMs) for dense image captioning. The authors introduce HalFscore, a concept-level metric for evaluating accuracy and completeness of dense captions, and propose PerturboLLaVA, a training method that reduces the model's reliance on language priors by incorporating adversarially perturbed text during training. This approach enhances focus on visual inputs and reduces hallucinations. PerturboLLaVA achieves a HalFscore of 52.2, outperforming baselines with a 6.2 point increase in precision and a 2.4 point increase in Fscore, while also improving general multimodal benchmark performance.

## Method Summary
The method addresses hallucinations by introducing adversarial text perturbations during training. The approach generates misleading text that contradicts visual content (e.g., claiming bananas are yellow when image shows green ones) and prepends this to training prompts. The model learns to down-weight these language priors and focus on visual evidence to minimize loss. This supervised fine-tuning technique modifies the input data distribution rather than requiring special loss functions or inference-time strategies. The training enforces the model to scrutinize image content rather than relying on statistical linguistic patterns.

## Key Results
- HalFscore of 52.2 achieved, outperforming baselines
- 6.2 point increase in precision for reducing hallucinations
- 2.4 point increase in Fscore for overall performance
- General multimodal benchmark performance also improved

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Disruption of Language Priors
The method posits that hallucinations arise from over-reliance on pre-trained linguistic knowledge rather than visual evidence. By injecting adversarially generated text that contradicts visual content, training creates a conflict forcing the model to down-weight text influence and up-weight visual features. The perturbation triggers the language bias and penalizes the resulting error.

### Mechanism 2: Conditional Distribution Re-calibration
The generation probability is framed as interplay between "language prior" prediction and "visual" prediction. Perturbation maximizes error of the language prior term, forcing optimization toward the visual grounding term. This mathematically shifts the model's conditional generation distribution to favor visual evidence over textual likelihood.

### Mechanism 3: Fine-Grained Evaluation via Concept Graphs (HalFscore)
Standard metrics fail to capture the trade-off between hallucinations (precision) and completeness (recall) in dense captioning. HalFscore decomposes captions into structured knowledge graphs (triplets of entities and relations), explicitly categorizing errors into "hallucinations" (spurious nodes) and "omissions" (missing nodes).

## Foundational Learning

- **Concept: Language Priors in MLLMs**
  - Why needed: Central thesis is that MLLMs act as "unimodal" text generators when falling back on statistical linguistic patterns instead of observing the image
  - Quick check: If an MLLM describes a "blue apple" as red because "apples are usually red" in its training data, is this a failure of the visual encoder or the language prior dominance?

- **Concept: Supervised Fine-Tuning (SFT) vs. Preference Alignment (RLHF/DPO)**
  - Why needed: Paper positions PerturboLLaVA as an SFT enhancement, unlike RLAIF-V or decoding strategies
  - Quick check: Why does the author claim this method is a "free lunch" compared to RLAIF-V? (Answer: No extra inference cost or reward model training)

- **Concept: Knowledge Graphs / Triplets**
  - Why needed: Understanding HalFscore evaluation requires knowing how to structure text into (Subject, Object, Relation) triplets
  - Quick check: In the triplet `<mirror, is, pink>`, which part represents the attribute, and how does this structure help detect hallucinations better than simple object detection?

## Architecture Onboarding

- **Component map:** GPT-4o script -> Data Loader (modified LLaVA 1.5) -> Vision Encoder + Projector + LLM Backbone -> HalFscore Evaluator (GPT-4o)

- **Critical path:**
  1. Generate adversarial text dataset
  2. Inject text into prompt template: `<image> <hint1> <perturbation_text> <hint2> <Question>`
  3. Run SFT training with standard loss
  4. Evaluate using Beam Search (N=5) for deterministic comparison

- **Design tradeoffs:**
  - Perturbation Strength: "Version 3" (no warning hints) increases training difficulty and lowers hallucinations but reduces recall
  - Data Source: GPT-4o dependency for perturbation generation introduces scalability concerns

- **Failure signatures:**
  - Random Perturbation: If text is irrelevant, gains are minimal
  - Overfitting: Static hint prompts may cause model to overfit to prompt format
  - Mitigation: Randomize hint prompts

- **First 3 experiments:**
  1. Baseline Reproduction: Train LLaVA 1.5 on standard 665k SFT dataset to establish floor for hallucination rates
  2. Perturbation Injection (Version 1): Apply weakest perturbation with warning hints to verify convergence and HalFscore precision improvement
  3. Strength Ablation: Compare "Version 1" vs "Version 3" (no hints) to measure trade-off between hallucination suppression and caption completeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can designing targeted perturbation texts specifically for objects, rather than general context, further reduce object-level hallucinations?
- Basis: Authors state "Moving forward, we plan to design targeted perturbation texts for objects to further enhance performance"
- Why unresolved: Current method underperforms RLAIF-V on CHAIR metric (object hallucination)
- Evidence needed: Experiments applying perturbations focused strictly on object descriptions and comparing CHAIR scores against current baselines

### Open Question 2
- Question: What is the optimal perturbation intensity that minimizes hallucinations without degrading the model's recall or general multimodal reasoning capabilities?
- Basis: Paper notes increasing perturbation levels reduces hallucinations but also hurts recall
- Why unresolved: Paper identifies trade-off but does not define optimal equilibrium or adaptive strategy
- Evidence needed: Study analyzing curve of perturbation strength against HalFscore, Recall, and general benchmarks (MMBench) to find "sweet spot"

### Open Question 3
- Question: Is the efficacy of PerturboLLaVA dependent on high capability of GPT-4o for generating perturbations, or can smaller models suffice?
- Basis: Method relies on GPT-4o to generate perturbations based on "world knowledge"
- Why unresolved: If training requires proprietary high-capacity model, it may limit reproducibility and accessibility
- Evidence needed: Ablation studies comparing performance when perturbations are generated by GPT-4o versus smaller, open-source models

## Limitations
- The core assumption that adversarial perturbation reliably decouples language priors from visual reasoning has limited empirical validation
- GPT-4o-based triplet extraction for HalFscore introduces black-box dependency that may not generalize across domains
- Stronger perturbations reduce hallucinations but also hurt recall, suggesting model may become overly conservative

## Confidence
- **High confidence:** Experimental results showing HalFscore improvements (52.2 vs baselines) are reproducible and methodology is clearly described
- **Medium confidence:** Claim that method "reduces reliance on language priors" is supported by directional evidence but lacks direct causal proof
- **Low confidence:** Assertion that HalFscore represents a "free lunch" compared to inference-time methods is overstated due to GPT-4o preprocessing dependency

## Next Checks
1. **Direct visual grounding test:** Create controlled dataset where visual features directly contradict language priors and measure whether PerturboLLaVA correctly overrides prior while baselines fail

2. **Cross-domain generalization:** Evaluate on domains outside training distribution (medical imaging, satellite imagery) to verify HalFscore improvements transfer beyond perturbation dataset's scope

3. **Ablation of triplet extraction:** Replace GPT-4o with deterministic rule-based triplet extractor on subset of data to verify HalFscore improvements aren't artifacts of black-box evaluation pipeline