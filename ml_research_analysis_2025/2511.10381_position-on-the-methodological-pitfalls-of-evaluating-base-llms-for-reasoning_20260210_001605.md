---
ver: rpa2
title: 'Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning'
arxiv_id: '2511.10381'
source_url: https://arxiv.org/abs/2511.10381
tags:
- reasoning
- llms
- base
- zhang
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the assumption that base LLMs (pre-trained
  only) can be reliably evaluated for reasoning capabilities, since they are optimized
  for linguistic plausibility rather than correctness. The authors argue that evaluating
  base LLMs on reasoning tasks introduces a fundamental mismatch, as their outputs
  may be valid or invalid conclusions purely by chance, due to statistical patterns
  rather than genuine reasoning.
---

# Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning

## Quick Facts
- **arXiv ID:** 2511.10381
- **Source URL:** https://arxiv.org/abs/2511.10381
- **Reference count:** 31
- **Primary result:** Base LLMs generate valid and invalid conclusions equally, demonstrating that reasoning evaluations are confounded by linguistic plausibility rather than correctness optimization.

## Executive Summary
This paper argues that evaluating base large language models (pre-trained only) on reasoning tasks introduces a fundamental methodological confound. Base LLMs are optimized for linguistic plausibility (next-token prediction) rather than correctness, so their outputs on reasoning tasks may be valid or invalid conclusions purely by chance. An empirical experiment with 13 base models (0.5B–32B) on valid and invalid logical forms shows that models tend to generate target strings for both, supporting the claim that outputs are incidental byproducts of linguistic patterns rather than goal-directed reasoning attempts. The paper concludes that reasoning assessments should prioritize instruct LLMs, which are explicitly optimized for correctness, and that base LLM reasoning findings cannot be generalized to instruct LLMs.

## Method Summary
The authors evaluate 13 base LLMs (0.5B–32B parameters) on valid-form (modus ponens) and invalid-form (affirming the consequent) logical reasoning prompts. They use 7,600 valid and 7,600 invalid templates generated from 20 name pairs and 380 predicate pairs. The evaluation measures match rates against target strings (linguistically predicted continuations) for both prompt types. Greedy decoding (temperature=0) is used across all models, and results are plotted as valid match rate versus invalid match rate to assess "validity-indifference."

## Key Results
- Base LLMs show high match rates on both valid and invalid logical forms, clustering in the top-right quadrant of the validity-indifference plot
- Models like Qwen2.5 and Llama 3.1 demonstrate near-equal performance on valid and invalid forms
- The paper provides evidence that reasoning-like outputs from base LLMs are incidental byproducts of linguistic pattern matching rather than genuine reasoning attempts

## Why This Works (Mechanism)

### Mechanism 1: Objective Mismatch as Evaluation Confound
- **Claim:** Base LLMs optimized for linguistic plausibility produce reasoning-like outputs as incidental byproducts, not goal-directed reasoning attempts, making correctness evaluation fundamentally confounded.
- **Mechanism:** Pre-training optimizes for next-token prediction probability across diverse text (including pedagogical errors, counterfactuals). When prompted with reasoning tasks, the model selects continuations based on statistical plausibility patterns rather than correctness optimization. Identical surface patterns (e.g., "Therefore, X") can follow both valid and invalid logical structures because the model tracks linguistic regularities, not truth conditions.
- **Core assumption:** A model's outputs can only be meaningfully evaluated against criteria it was explicitly optimized to satisfy (e.g., correctness, validity).
- **Evidence anchors:**
  - [abstract] "We highlight the fundamental mismatch between base LLMs' pretraining objective and normative qualities, such as correctness, by which reasoning is assessed."
  - [Section 4.2] "Base LLMs are not trained for the purpose of producing outputs that conform to our standards for what counts as 'good' reasoning; instead, they are trained only for the purpose of producing outputs that are statistically the most likely continuation of a given context."
  - [corpus] Limited direct corpus support; related work on evaluation pitfalls focuses on different domains (forecasting, causal inference).
- **Break condition:** If post-training or inference-time steering explicitly re-optimizes the model for instruction-following/correctness (via RLHF, DPO, or activation steering), the confound may be partially controlled—but this must be explicitly verified, not assumed.

### Mechanism 2: Linguistic Circuits Generate Valid and Invalid Conclusions Equally
- **Claim:** Known mechanistic circuits (Indirect Object Identification, Induction Heads) suffice to produce both logically valid and invalid conclusions without any reasoning process.
- **Mechanism:** 
  1. IOI circuit identifies context names and suppresses repeated tokens.
  2. IH circuit copies previously observed token sequences.
  3. For "If A, then B. A. Therefore," the model outputs "B" via IOI+IH (valid modus ponens).
  4. For "If A, then B. B. Therefore," the same circuits output "A" (invalid: affirming the consequent).
  The model never evaluates logical validity; it tracks token co-occurrence patterns.
- **Core assumption:** The identified circuits (IOI, IH) are representative of how base LLMs process such prompts, not exhaustive or exclusive explanations.
- **Evidence anchors:**
  - [abstract] "We show how base LLMs can produce logically valid or invalid conclusions purely by chance, as incidental byproducts of conforming to linguistic patterns."
  - [Section 5.4, Results] "Most base LLMs cluster towards the top-right corner... they tend to generate outputs that match the target strings in response to most prompts, regardless of whether they are valid-form... or invalid-form prompts."
  - [corpus] Weak corpus connection; related interpretability work focuses on circuit analysis but not this specific validity-indifference phenomenon.
- **Break condition:** If a model systematically suppresses invalid-form outputs significantly more than valid-form outputs (bottom-left quadrant clustering in Figure 2), this suggests additional mechanisms beyond pure linguistic pattern-matching may be active—though this still doesn't prove reasoning intent.

### Mechanism 3: Instruct-Tuning Re-Optimizes Toward Normative Goals
- **Claim:** Post-training with instruction-response pairs (formatted with special tokens like "user") creates a new optimization target where correctness becomes a component of linguistic plausibility, enabling more valid reasoning evaluation.
- **Mechanism:** 
  1. Instruction-tuning data consists exclusively of successful instruction-following demonstrations.
  2. Chat template markers (e.g., "user" → "assistant") signal that continuations must fulfill the instruction.
  3. The model's learned distribution now associates "Therefore," with correct conclusions in instructional contexts.
  4. Preference-tuning further sharpens this by rewarding preferred (correct, helpful) over rejected responses.
  Validity becomes instrumental to the new objective rather than incidental.
- **Core assumption:** The signal from instruction-tuning data is sufficiently strong and consistent to shift the model's behavior toward correctness as a component of plausibility.
- **Evidence anchors:**
  - [abstract] "We call for a critical re-examination of existing work that relies implicitly on these assumptions [that base LLMs' outputs can be assessed as bona fide attempts at correct answers]."
  - [Section 2] "In PT, the model is further optimized... a common objective is for the model to maximize the difference between the log likelihood assigned to the preferred response and that assigned to the rejected response."
  - [corpus] Weak direct corpus support; related work on RL with verifiable rewards suggests gains may not fully survive parity-controlled evaluation (arXiv:2509.21882).
- **Break condition:** If instruction-tuning data contains systematic errors, adversarial examples, or inconsistent quality, the re-optimization toward correctness may be incomplete or noisy—evaluation still requires caution.

## Foundational Learning

- **Concept: Confounding Variable**
  - **Why needed here:** The paper's core argument is that "instruction-following confound" undermines base LLM reasoning evaluation. You must understand confounds to grasp why observed outputs (valid/invalid conclusions) cannot be attributed to reasoning ability when the model's goal (plausibility) differs from the evaluation criterion (correctness).
  - **Quick check question:** If a medical classifier trained only on tumor images shows bias in predicting patient gender, can you validly conclude it has inherent gender-prediction bias? Why or why not?

- **Concept: Optimization Objective vs. Evaluation Metric**
  - **Why needed here:** The paper hinges on distinguishing what a model is trained to optimize (next-token probability) from what researchers evaluate it on (logical validity, correctness). Mismatch between these invalidates causal claims about model capabilities.
  - **Quick check question:** A student is trained to maximize exam scores using memorization. If you evaluate their "understanding" by novel problem-solving, are you measuring their optimized capability or an incidental byproduct?

- **Concept: Mechanistic Interpretability (Circuits)**
  - **Why needed here:** The paper uses circuit explanations (IOI, IH) to show how surface reasoning outputs emerge from non-reasoning mechanisms. Understanding circuit-level analysis is necessary to evaluate mechanistic claims about model behavior.
  - **Quick check question:** If an induction head copies previous token patterns, and this produces a "valid" conclusion in one context and an "invalid" conclusion in another, does the circuit "know" about validity?

## Architecture Onboarding

- **Component map:** Base LLM (pre-trained transformer, next-token prediction) → Post-training pipeline (Instruction-tuning with masked loss on instruction tokens, Preference-tuning with RLHF/DPO) → Chat template (special tokens like "user"/"assistant") → Evaluation harness (benchmark prompts → model outputs → automatic extraction/scoring)

- **Critical path:**
  1. Identify evaluation objective (e.g., logical validity).
  2. Verify model's training objective matches evaluation criterion.
  3. If mismatch (base LLM), recognize confound: outputs may not be bona fide reasoning attempts.
  4. If match (instruct LLM after IT+PT), evaluation is more justified but still requires caution for data quality issues.
  5. Design controlled experiments comparing base vs. instruct on identical prompts to isolate instruction-following effects.

- **Design tradeoffs:**
  - **Base LLM evaluation:** Pro—access to "pure" pre-training dynamics, useful for mechanistic interpretability; Con—fundamentally confounded for reasoning claims, outputs ambiguous in intent.
  - **Instruct LLM evaluation:** Pro—objective alignment with correctness enables valid reasoning assessment; Con—post-training obscures pre-training mechanisms, harder to isolate innate capabilities.
  - **Hybrid approaches:** Few-shot prompting in base LLMs may partially control confound but introduces new confounds (pattern-matching vs. instruction-following).

- **Failure signatures:**
  - **High accuracy on both valid and invalid logical forms:** Indicates linguistic pattern-matching, not reasoning (see Figure 2 top-right clustering).
  - **Outputs that continue context with pedagogical errors or meta-questions:** Signals model is completing statistical text patterns, not answering the question.
  - **Instruction-tuned model failing on novel logical forms not seen in post-training data:** Suggests overfitting to demonstration patterns rather than generalized reasoning.

- **First 3 experiments:**
  1. **Replicate validity-indifference test:** For a given base LLM, measure target-string completion rates on minimally differing valid-form (modus ponens) and invalid-form (affirming the consequent) prompts. Expect high rates on both (top-right quadrant).
  2. **Base vs. instruct comparison:** Run identical logical reasoning prompts through base and instruct versions of the same model family. Measure divergence in validity-indifference—expect instruct models to suppress invalid-form completions more than base.
  3. **Instruction-following probe:** Design prompts where the instruction explicitly requests an invalid conclusion (e.g., "Given these premises, provide an invalid conclusion"). Compare base vs. instruct compliance. Instruct models should comply if instruction-following is the dominant objective; base models may ignore the instruction and produce statistically plausible continuations. Note: This tests objective alignment, not reasoning ability.

## Open Questions the Paper Calls Out

- **Question 1:** How much of the reasoning improvement observed after post-training is attributable to genuine improvements in reasoning capability versus improvements in instruction-following?
  - **Basis in paper:** The authors state "our position does draw attention to the challenge in disentangling how much of the reasoning improvement observed after post-training is attributable to genuine improvements in reasoning capability versus improvements in instruction-following."
  - **Why unresolved:** The confounding variable of instruction-following is not controlled in base LLM evaluations, making it difficult to isolate genuine reasoning gains.
  - **What evidence would resolve it:** Experimental designs that separately measure instruction-following ability and reasoning accuracy across matched base and instruct models.

- **Question 2:** Do reasoning "circuits" identified in base LLMs via mechanistic interpretability methods persist or change fundamentally after instruction-tuning?
  - **Basis in paper:** The authors argue "it is also unwarranted to assume that the mechanisms by which a base LLM produces its outputs are the same ones it will use when it is re-optimized as an instruct LLM."
  - **Why unresolved:** Most mechanistic interpretability studies (e.g., Kim et al., 2025; Stolfo et al., 2023) conduct experiments exclusively on base LLMs.
  - **What evidence would resolve it:** Comparative circuit analysis using activation patching on matched base and instruct versions of the same model family.

- **Question 3:** How can the instruction-following confound be adequately controlled when evaluating base LLMs for reasoning?
  - **Basis in paper:** The authors identify this as a "critical confounding variable" but acknowledge that existing techniques (few-shot, steering) introduce their own confounds.
  - **Why unresolved:** The paper notes that few-shot demonstrations "often fails as a control because it introduces its own confounding variables."
  - **What evidence would resolve it:** Development of evaluation protocols that can distinguish between models attempting correct answers versus generating linguistically plausible continuations.

## Limitations

- The claim that base LLM reasoning evaluations are "fundamentally confounded" assumes no post-training steering or inference-time control is applied—this is often not verified in prior work.
- The specific circuit-level explanations (IOI+IH) for validity-indifference are based on limited mechanistic interpretability evidence and may not be exhaustive.
- The instruction-tuning re-optimization claim relies on indirect evidence; corpus analysis shows weak direct support for the strength of the correctness signal.

## Confidence

- **High Confidence:** The objective mismatch confound is well-established and clearly demonstrated through the validity-indifference experiment.
- **Medium Confidence:** The circuit-level mechanism for generating valid/invalid conclusions is plausible but based on limited interpretability evidence.
- **Low Confidence:** The strength and consistency of instruction-tuning's re-optimization toward correctness is weakly supported by corpus evidence.

## Next Checks

1. **Verify objective alignment in prior work:** Systematically review existing base LLM reasoning evaluations to determine if post-training steering or inference-time control was applied, and assess the confound's actual prevalence.
2. **Test circuit-level mechanisms:** Conduct ablation studies on the identified circuits (IOI, IH) to determine their specific contributions to validity-indifference, and explore alternative mechanistic explanations.
3. **Measure instruction-tuning signal strength:** Analyze instruction-tuning datasets for consistency and quality of correctness demonstrations, and assess the impact of data errors or adversarial examples on post-training behavior.