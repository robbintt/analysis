---
ver: rpa2
title: The Seismic Wavefield Common Task Framework
arxiv_id: '2512.19927'
source_url: https://arxiv.org/abs/2512.19927
tags:
- data
- datasets
- hyperparameter
- time
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'A Common Task Framework (CTF) for evaluating machine learning
  models on seismic wavefields is introduced, featuring three challenging datasets:
  global wavefields, distributed acoustic sensing (DAS), and 3D crustal wavefields.
  Twelve task-specific metrics assess forecasting, reconstruction, noise robustness,
  limited data performance, and parametric generalization.'
---

# The Seismic Wavefield Common Task Framework

## Quick Facts
- arXiv ID: 2512.19927
- Source URL: https://arxiv.org/abs/2512.19927
- Authors: Alexey Yermakov; Yue Zhao; Marine Denolle; Yiyu Ni; Philippe M. Wyder; Judah Goldfeder; Stefano Riva; Jan Williams; David Zoro; Amy Sara Rude; Matteo Tomasetto; Joe Germany; Joseph Bakarji; Georg Maierhofer; Miles Cranmer; J. Nathan Kutz
- Reference count: 40
- Primary result: Current ML architectures fail to outperform zero-baseline on realistic seismic wavefield problems

## Executive Summary
The Seismic Wavefield Common Task Framework (CTF) introduces a standardized evaluation platform for assessing machine learning models on challenging seismic wavefield reconstruction and forecasting tasks. The framework features three datasets: global wavefields (2048 sensors, 1 Hz), distributed acoustic sensing (3000 channels, 5 Hz), and 3D crustal wavefields (94×94 grid, 50 Hz). Twelve task-specific metrics evaluate forecasting, denoising, limited-data performance, and parametric generalization, revealing that even sophisticated architectures struggle with real-world seismic complexity.

The CTF demonstrates that current ML methods are far from solving practical seismic wavefield problems, with common architectures including DeepONet, Fourier Neural Operators, and foundation models failing to exceed a zero-baseline prediction. RNN-based models (LSTM, ODE-LSTM) perform best overall but still struggle on limited-data tasks. The framework provides a rigorous, reproducible platform for advancing scientific ML in seismology.

## Method Summary
The CTF evaluates ML models using three seismic datasets with specific data properties: global wavefields (P=25 simulations, 2048 sensors, 1 Hz, 3600s), DAS (P=50 recordings, 3000 channels, 5 Hz, 60s), and 3D crustal wavefields (P=30 simulations, 94×94 grid, 50 Hz, 6s). Models are trained via Ray Tune hyperparameter search with ASHA scheduler, using 80/20 train/validation splits for most tasks and custom splits for parametric generalization. The framework scores models across 12 metrics (E1-E12) covering forecasting, denoising, limited-data regimes, and parametric interpolation/extrapolation, with scores bounded between -100 and 100. Hardware requirements include 1× A100 GPU (40GB), 18 CPUs, and 120GB RAM.

## Key Results
- Current ML architectures including DeepONet, FNO, and foundation models fail to outperform zero-baseline on global and DAS datasets
- LSTM and ODE-LSTM perform best overall, likely due to parameter efficiency on limited training data
- Foundation models score -100 on reconstruction tasks, indicating severe limitations for seismic applications
- Limited-data tasks (E7-E9) prove particularly challenging, with many models scoring negatively due to overfitting
- Multi-metric scoring reveals task-specific failures that single-score aggregation would conceal

## Why This Works (Mechanism)

### Mechanism 1
Multi-metric scoring reveals task-specific failures that single-score aggregation would conceal. The CTF's 12-metric suite decomposes model capability into forecasting, noise robustness, limited-data performance, and parametric generalization. By evaluating each dimension independently, the framework prevents a "winner-take-all" outcome where a model appears competent despite catastrophic failure on specific regimes. Core assumption: Seismological applications require different model properties depending on task, so aggregate scores obscure critical deficiencies. Evidence: The abstract states the framework assesses "forecasting, reconstruction, noise robustness, limited data performance, and parametric generalization" through 12 task-specific metrics.

### Mechanism 2
RNN-based architectures outperform neural operators and foundation models on limited training data due to implicit regularization from modest parameter counts. LSTMs and ODE-LSTMs have relatively few parameters compared to FNOs, DeepONets, and foundation models. When training data is constrained, these smaller models avoid overfitting noise while maintaining expressive capacity for auto-regressive forecasting. The MSE loss further regularizes predictions toward meaningful dynamics. Core assumption: Model parameter count relative to training data volume determines generalization on seismic wavefields. Evidence: Page 9 notes "their advantage likely stems from a relatively modest parameter count combined with relatively strong expressive power."

### Mechanism 3
Zero-baseline comparison establishes that seismic wavefield dynamics remain fundamentally unsolved for current ML methods. The datasets are normalized to zero mean, making "predict all zeros" a non-trivial baseline. When sophisticated architectures fail to exceed this baseline, it indicates the spatiotemporal complexity exceeds their inductive biases. The multi-path, multi-frequency, scattered nature of seismic waves violates assumptions in these models. Core assumption: Seismic wavefields have inherent complexity that exceeds what current ML architectures can capture from limited observations. Evidence: Page 4 states "even small-scale heterogeneities distort and scatter waves—creating highly nonstationary, multi-frequency, and multi-path signals."

## Foundational Learning

- **Elastodynamic Wave Equation Basics**
  - Why needed here: Seismic wavefields are governed by 3D elastic wave propagation in spatially heterogeneous media. Understanding P/S wave separation, surface waves (Rayleigh/Love), and scattering from heterogeneities is essential to interpret why ML models fail.
  - Quick check question: Can you explain why a uniform medium assumption would fail for realistic earthquake ground motion prediction?

- **Recurrent Neural Networks and Vanishing Gradients**
  - Why needed here: LSTM and ODE-LSTM performed best; understanding their gating mechanisms and why ODE-formulations help with long-term dependencies clarifies the results.
  - Quick check question: Why might an ODE-based hidden state help with forecasting chaotic spatiotemporal dynamics compared to standard LSTMs?

- **Neural Operators and Spectral Methods**
  - Why needed here: FNO and DeepONet are evaluated; these learn mappings between function spaces. Understanding their inductive biases explains why they struggle on heterogeneous seismic media.
  - Quick check question: What assumption does FNO's Fourier-space convolution make about the underlying PDE, and how might spatially-varying seismic velocities violate it?

## Architecture Onboarding

- **Component map:**
  Data layer (3 datasets) -> Task layer (12 metrics) -> Model layer (17 architectures) -> Evaluation layer (RMSE, spectral error, composite score)

- **Critical path:**
  1. Load training matrices from .npz files (shapes in Table 3)
  2. Split for validation (80/20 for E1-E10; specific splits for E11-E12)
  3. Hyperparameter tuning via Ray Tune with ASHA scheduler
  4. Generate predictions for held-out test set
  5. Submit to Kaggle leaderboard (planned March 2026)

- **Design tradeoffs:**
  - Limited data by design democratizes access but may underrepresent large-model capabilities
  - Zero-baseline is strict but may penalize models that capture partial dynamics
  - Spectral error for long-term forecasting accepts statistical fidelity over trajectory matching (appropriate for chaotic systems)

- **Failure signatures:**
  - Model returns all zeros → underfitting or collapse to baseline
  - Negative scores on E7-E9 → overfitting to noise in limited-data regime
  - -100 on any metric → model timeout or crash (common for foundation models)
  - High E3/E5 but low E1/E2 → good at denoising, poor at forecasting

- **First 3 experiments:**
  1. **Establish baseline with LSTM**: Train LSTM on Global dataset for E1-E2 only; expect ~9.75 AvgScore. Vary `hidden_state_size` (8-256) and `seq_length` (5-512) to understand capacity requirements.
  2. **Diagnose foundation model failure**: Run Chronos (best-performing foundation model) on DAS dataset; inspect why it achieves 44.46 on E1 but -100 on reconstruction tasks (likely context-length limitations).
  3. **Test physics-informed approach**: Implement basic PINN or physics-constrained loss on 3D Crustal dataset; compare against zero-baseline to validate whether elastodynamic priors help.

## Open Questions the Paper Calls Out

### Open Question 1
Can foundation models outperform zero-baselines and RNNs when provided with the significantly larger training datasets planned for the Kaggle launch? Basis: Section 4 notes that for the upcoming competition, the authors will provide P=100 simulations to ensure "larger models' approximation capabilities are not limited by data." Why unresolved: Current results show complex foundation models failing, but this may be due to the intentionally limited volume of the initial training data rather than architectural limitations. Evidence: Leaderboard results on the expanded Kaggle dataset showing foundation models achieving positive average scores.

### Open Question 2
How does model performance degrade when transitioning from axisymmetric Earth models to realistic heterogeneous geological structures? Basis: Section 4 states the global wavefield dataset uses axisymmetric models that "omit the heterogeneous geological structures that drive real-world geodynamics." Why unresolved: Current models are evaluated on simplified physics; real-world seismic hazard analysis requires handling sharp spatial variations and near-surface heterogeneities not present in the current CTF. Evidence: Evaluation scores on future datasets incorporating the REVEAL project or Mars data mentioned in the limitations.

### Open Question 3
Can current architectures effectively capture high-frequency earthquake wavefields in DAS data, distinct from the dispersive ocean swells in the current dataset? Basis: Section 4 highlights that future DAS datasets with dominant earthquake features would "add another layer of complexity" and test "extreme scattering due to real, near-surface structure heterogeneity." Why unresolved: The current DAS dataset is dominated by dispersive surface gravity waves, leaving the challenge of high-frequency, highly scattering earthquake ground motion untested. Evidence: Model reconstruction scores on DAS recordings specifically containing high-frequency earthquake signals.

## Limitations

- Modest dataset size (max P=50 simulations) may artificially constrain larger models and foundation models
- Zero-baseline comparison may not reflect partial progress, penalizing models that capture some wavefield features
- Parametric generalization tasks use hidden parameter values, making it impossible to verify whether failures stem from insufficient capacity or inadequate parameter representation
- Current datasets use simplified axisymmetric Earth models, omitting realistic heterogeneous geological structures

## Confidence

- **High confidence**: Multi-metric scoring reveals task-specific failures; RNNs outperform larger models on limited data due to parameter efficiency
- **Medium confidence**: Zero-baseline establishes unsolved status; current architectures' inductive biases mismatch seismic wavefield complexity
- **Low confidence**: Foundation models' failure is fundamental rather than implementation/time-limited; parameter count alone determines limited-data performance

## Next Checks

1. Replicate foundation model experiments with extended time limits and larger context windows to distinguish architectural limitations from computational constraints
2. Test whether models trained on synthetic seismic data (e.g., homogeneous media) can generalize to heterogeneous datasets, isolating whether complexity or heterogeneity drives failure
3. Implement hybrid physics-ML architectures (e.g., PINNs with learned components) and evaluate whether elastodynamic constraints improve performance on forecasting and parametric tasks