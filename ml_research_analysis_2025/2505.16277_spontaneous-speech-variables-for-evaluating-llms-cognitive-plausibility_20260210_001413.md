---
ver: rpa2
title: Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility
arxiv_id: '2505.16277'
source_url: https://arxiv.org/abs/2505.16277
tags:
- language
- speech
- data
- these
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces spontaneous speech variables (speech reductions
  and prosodic prominences) as benchmarks for evaluating large language models (LLMs)
  from a cognitive plausibility perspective. Using conversational corpora in English,
  French, and Mandarin, the authors extracted these production variables and tested
  various pretrained models' ability to predict them.
---

# Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility

## Quick Facts
- **arXiv ID**: 2505.16277
- **Source URL**: https://arxiv.org/abs/2505.16277
- **Reference count**: 19
- **Primary result**: Spontaneous speech variables (speech reductions and prosodic prominences) can be predicted from text transcripts above baseline, with conversational training data outperforming written genres.

## Executive Summary
This study introduces spontaneous speech variables—speech reductions and prosodic prominences—as benchmarks for evaluating large language models (LLMs) from a cognitive plausibility perspective. Using conversational corpora in English, French, and Mandarin, the authors extracted these production variables and tested various pretrained models' ability to predict them. Models fine-tuned on the prediction task outperformed baselines, with conversational genre training data yielding more accurate predictions than written genres. The results demonstrate that speech-based benchmarks can effectively differentiate LLM performance across training data types and languages, offering a complementary evaluation approach to existing syntactic and semantic benchmarks.

## Method Summary
The study employed a three-stage methodology: (1) preparing genre-specific pre-training corpora (conversational, written, and mixed) of 10M tokens per language, (2) pretraining RoBERTa models from XLM-RoBERTa initialization on these datasets, and (3) fine-tuning on token classification tasks to predict speech reduction and prosodic prominence labels extracted from spontaneous speech corpora. Reduction labels were based on duration ratios (actual/expected duration) with language-specific thresholds, while prominence labels used wavelet scores. Models were evaluated using 8-fold cross-validation by speaker groups, with hyperparameter optimization over learning rates and batch sizes.

## Key Results
- Models fine-tuned on spontaneous speech variable prediction outperformed baseline label-distribution approaches
- Conversational training data consistently yielded more accurate predictions than written genres across all three languages
- The performance advantage was most pronounced for speech reduction prediction, with F1 scores around 0.44 for English and French versus 0.17-0.24 for Mandarin
- Perplexity and fine-tuning performance showed partial correlation for prosodic prominence prediction, suggesting alignment with human production patterns

## Why This Works (Mechanism)

### Mechanism 1: Information Density–Production Mapping
Models predict speech reduction patterns from text transcripts because speech reductions correlate with statistical properties like lexical frequency, contextual predictability, and informativity. Conversational training data provides stronger signal than written genres because it contains distributional patterns that mirror spontaneous speech production.

### Mechanism 2: Text-to-Prosody Redundancy
Prosodic prominence is partially predictable from text context because lexical and contextual information encode redundancy with acoustic-prosodic features. Models capture this through learned associations between token context and prominence likelihood.

### Mechanism 3: Genre-Matched Training Advantage
Models pretrained on conversational transcripts outperform those trained on written text when predicting spontaneous speech production variables because conversational data contains distributional patterns—discourse markers, reduced forms, non-canonical syntax—that mirror the benchmarks' spontaneous speech origins.

### Mechanism 4: Perplexity–Label Correlation (Partial)
For prosodic prominence, model surprisal (perplexity) shows expected positive correlation with prominence labels in conversational-trained models, suggesting partial alignment with human production patterns where higher surprisal tokens are more likely to be prominent.

## Foundational Learning

- **Information Density Hypothesis (Aylett & Turk, 2004)**
  - Why needed here: Provides the theoretical basis for why speech reductions and prominences correlate with predictability; essential for interpreting why models can predict these variables from text
  - Quick check question: Can you explain why a speaker might reduce a highly predictable word but emphasize an unpredictable one?

- **Genre/Register Distinction in NLP**
  - Why needed here: The paper's core comparison depends on understanding how conversational and written language differ syntactically, lexically, and pragmatically
  - Quick check question: What linguistic features distinguish spontaneous conversation from Wikipedia-style text?

- **Token Classification with BIO Encoding**
  - Why needed here: The fine-tuning task frames reduction/prominence prediction as token classification; understanding BIO tagging is necessary to implement or extend this work
  - Quick check question: How would you encode a 3-token sequence where only the middle token is prominent using BIO format?

## Architecture Onboarding

- **Component map**: Text corpus → SentencePiece tokenizer (10k vocab) → RoBERTa (XLM-R initialized) → Genre-specific pretraining → Token classification head → Fine-tuning on speech variable prediction

- **Critical path**: 1. Prepare genre-specific pre-training corpora (conversational vs. written vs. mixed) 2. Pre-train RoBERTa models from XLM-R initialization for 100 epochs 3. Extract reduction labels (duration ratio < 0.5–0.6 threshold) and prominence labels (wavelet score > 1.25) 4. Fine-tune on token classification with hyperparameter sweep (5 learning rates × 2 batch sizes) 5. Evaluate F1 scores against label-distribution baselines

- **Design tradeoffs**: Vocabulary size (10K) smaller than typical for efficiency; threshold selection (0.5–0.6 for reduction, 1.25 for prominence) introduces non-comparability; text-only approach cannot capture full acoustic reality

- **Failure signatures**: Mandarin reduction task shows much lower performance (F1 ~0.17–0.24 vs ~0.44 for other languages); over-prediction of function words for reduction; perplexity–performance divergence in Mandarin suggests different evaluation criteria capture different capabilities

- **First 3 experiments**: 1. Reproduce English results with Switchboard+BNC conversational data and Buckeye corpus 2. Ablate genre effect with shuffled data holding content constant 3. Probe information-theoretic contribution by computing correlation between token surprisal and gold labels

## Open Questions the Paper Calls Out

- How does varying the balance of conversational speech, child-directed speech, and simple written text in pre-training data impact the prediction accuracy of spontaneous speech variables?
- To what extent do the models rely on surface-level shortcuts—such as word frequency or sentence length—rather than abstract linguistic representations to predict speech reductions and prosodic prominences?
- Why does pre-training on conversational data improve speech variable prediction in Mandarin even when it results in higher perplexity compared to models trained on written data?

## Limitations
- The acoustic-prosodic reality of speech is only partially captured by text-based predictions
- Language-specific variability in performance (notably weaker Mandarin results) suggests corpus or language-specific challenges
- The advantage of conversational training data could stem from surface-level distributional similarities rather than learned cognitive patterns

## Confidence
- **High Confidence**: Conversational training data outperforms written genres for predicting spontaneous speech variables
- **Medium Confidence**: These benchmarks effectively differentiate LLM performance for cognitive plausibility evaluation
- **Low Confidence**: Spontaneous speech variables should replace or complement existing syntactic/semantic benchmarks for LLM evaluation

## Next Checks
1. Cross-linguistic robustness validation: Replicate Mandarin reduction prediction with alternative corpus to determine if poor performance is language-specific or corpus-specific
2. Genre disentanglement experiment: Create controlled pre-training datasets with different genre ratios while holding vocabulary constant to isolate genre-specific effects
3. Acoustic information ablation: Compare text-only prediction to text-plus-approximate-duration prediction to quantify information-theoretic limits of text-based prediction