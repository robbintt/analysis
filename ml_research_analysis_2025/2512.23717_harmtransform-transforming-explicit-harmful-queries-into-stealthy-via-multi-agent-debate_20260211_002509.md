---
ver: rpa2
title: 'HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent
  Debate'
arxiv_id: '2512.23717'
source_url: https://arxiv.org/abs/2512.23717
tags:
- arxiv
- harmful
- debate
- intent
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HarmTransform, a multi-agent debate framework
  designed to systematically transform explicit harmful queries into stealthier forms
  while preserving their underlying intent. The framework leverages iterative critique
  and refinement among multiple agents to generate high-quality, covert harmful query
  transformations.
---

# HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate

## Quick Facts
- arXiv ID: 2512.23717
- Source URL: https://arxiv.org/abs/2512.23717
- Reference count: 15
- Primary result: Introduces multi-agent debate framework for transforming harmful queries into stealthy forms, achieving effectiveness score of 0.36 (vs. 0.24 baseline)

## Executive Summary
This paper presents HarmTransform, a multi-agent debate framework that transforms explicit harmful queries into stealthy forms while preserving their underlying intent. The system employs persona-enhanced debaters who critique and refine each other's proposals through iterative rounds, with a summarizer distilling consensus and a generator producing the final transformed query. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in generating effective query transformations, achieving an attack effectiveness score of 0.36 compared to 0.24 for the best baseline. The analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity.

## Method Summary
HarmTransform transforms harmful queries through a multi-agent debate process where M debaters with safety-related personas propose and critique transformation strategies over R_N rounds, sharing only the most recent round's arguments. A summarizer agent extracts consensus from the full debate, which is then combined with the original query and fed to a generation model to produce the final stealthy transformation. The framework uses DeepSeek-V3 for all LLM components and evaluates transformations using binary intent preservation and effectiveness (1 - refusal rate) metrics. The approach is validated on the Safe-RLHF dataset using the first 100 harmful queries.

## Key Results
- HarmTransform achieves effectiveness score of 0.36 (significantly higher than baseline of 0.24)
- Preservation score of 0.67 demonstrates ability to maintain harmful intent while increasing stealth
- Debate mechanism shows diminishing returns beyond 1 round and 3 debaters
- Persona diversity hypothesized to increase transformation strategy diversity but not empirically isolated

## Why This Works (Mechanism)

### Mechanism 1: Persona-Induced Perspective Diversification
- Claim: Assigning safety-related personas (e.g., reformed criminal, security consultant) to debater agents increases the diversity of transformation strategies explored.
- Mechanism: Personas constrain agent reasoning toward domain-specific expertise, producing transformation strategies that single-perspective approaches miss. Agents assume roles that "foster more diverse and comprehensive arguments" (Section 3.1).
- Core assumption: Persona assignment causally influences the lexical and framing choices agents propose, rather than merely adding stylistic variation.
- Evidence anchors:
  - [abstract]: "multi-agent debate framework... leverages iterative critique and refinement among multiple agents"
  - [section]: "each debater is equipped with a persona, encouraging them to assume safety-related roles and engage in debate under more realistic conditions" (Section 3.1, Figure 2)
  - [corpus]: Related work "Talk Isn't Always Cheap" explores how diversity in model capabilities influences debate outcomes, supporting the diversity-to-performance link—but does not directly validate persona effects.

### Mechanism 2: Iterative Critique-Refinement Loop with Local History
- Claim: Limiting debaters to only the most recent round's arguments (not full history) maintains focus while enabling progressive sharpening of stealth strategies.
- Mechanism: Agents critique prior-round proposals and refine them, producing progressively more subtle transformations. Local-history sharing "controls context length and encourages focused reasoning" (Section 3.1).
- Core assumption: Agents can improve proposals through targeted critique without needing full debate history; longer contexts would degrade rather than help.
- Evidence anchors:
  - [abstract]: "iterative critique and refinement among multiple agents"
  - [section]: "debaters only have access to the most recent round's discussion, rather than the entire debate history" (Section 3.1)
  - [corpus]: "InfoFlood" paper shows information overload can degrade model performance, indirectly supporting local-history design—but no direct test in this paper.

### Mechanism 3: Consensus Distillation via Summarization-to-Generation Pipeline
- Claim: A summarizer agent extracting consensus arguments, followed by a generator producing the final query, preserves harmful intent while enabling stealth.
- Mechanism: Summarization filters debate noise; generation conditions on distilled strategies plus the original query to produce an intent-aligned, stealthy transformation.
- Core assumption: Consensus extraction correctly identifies high-quality strategies; the generator will not over-sanitize or drift to benign outputs when conditioned on the original query.
- Evidence anchors:
  - [abstract]: "generating high-quality, covert harmful query transformations"
  - [section]: "summarizer agent... extracts the core arguments on which debaters have reached consensus... combined with the original harmful query Q_EXP and fed into a generation model" (Section 3.2)
  - [corpus]: Related multi-agent safety work (RedDebate) uses similar debate+distillation patterns for safety, but no direct validation of the summarization step's contribution.

## Foundational Learning

- Concept: Multi-Agent Debate Dynamics
  - Why needed here: The framework's core mechanism relies on agents critiquing and refining each other; understanding when debate improves vs. degrades outputs is critical.
  - Quick check question: Can you explain one condition under which debate causes "regression" rather than improvement?

- Concept: Harmful Intent Preservation vs. Stealth Tradeoff
  - Why needed here: The paper explicitly measures both preservation and effectiveness; improving one can hurt the other.
  - Quick check question: If a transformed query achieves high stealth (low refusal rate) but scores 0 on intent preservation, is it useful for safety training data?

- Concept: Local vs. Global Context in Agent Communication
  - Why needed here: The architecture restricts agents to local-history sharing; this design choice directly affects debate quality and scalability.
  - Quick check question: What failure mode might occur if agents had access to the full debate history instead of just the prior round?

## Architecture Onboarding

- Component map: Persona-Enhanced Debaters (M agents, sampled personas, R_N rounds) -> Local-History Communication Layer (passes only prior-round statements) -> Summarizer Agent (reviews full dialogue, extracts consensus) -> Generator Model (takes summary + original query → transformed query) -> Evaluation Judges (intent preservation binary, refusal indicator for effectiveness)

- Critical path: Original query Q_EXP → Round 0 independent statements → R_1...R_N debate rounds → Summarization → Generation → Q_IMP → Evaluation (preservation + effectiveness)

- Design tradeoffs:
  - More debaters (M=6) improve preservation but not effectiveness (Figure 4)
  - More rounds (N>1) show diminishing or negative returns (Figure 5)
  - Debate can cause "over-specification" or "optimization backfire" (Section 6.3.2)
  - SingleLLM is simpler and sometimes competitive, suggesting debate overhead may not always be justified

- Failure signatures:
  - Topic shift / intent drift: transformed query becomes benign or unrelated
  - Over-specification: debate adds explicit red-flag terms, increasing detectability
  - Information overload: summarizer struggles with long debates, losing key signals

- First 3 experiments:
  1. Replicate Table 1 baselines (SingleLLM, SingleLLMReflect, HarmTransform-NoDebate, HarmTransform) on a held-out subset of Safe-RLHF queries to confirm relative rankings.
  2. Ablate persona diversity: run HarmTransform with identical personas vs. diverse personas to isolate persona contribution (not directly tested in paper).
  3. Test local vs. full history: compare standard local-history sharing against full-history access on a small query set to validate the context-length assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can effective monitoring mechanisms (human-in-the-loop or automatic) be designed to detect and correct intent deviations during multi-agent debates?
- Basis in paper: [explicit] The authors state in Section 7 that "intent shift" is a major limitation and suggest future work explore monitoring mechanisms to ensure debates remain aligned with transformation goals.
- Why unresolved: The current framework lacks a feedback loop to correct "drifting" arguments during the debate process, leading to benign or off-topic outputs.
- What evidence would resolve it: A modified HarmTransform framework including a deviation detector that maintains intent preservation rates >90% without manual intervention.

### Open Question 2
- Question: Does incorporating prompt compression or summarization into agent communication effectively mitigate information overload?
- Basis in paper: [explicit] Section 7 identifies "information overload" as a limitation and proposes integrating information compression as a "promising direction" to distill key signals.
- Why unresolved: Current ablation studies show diminishing returns with more rounds/agents, hypothesized to be caused by redundant information overwhelming the summarizer.
- What evidence would resolve it: Experiments comparing standard context propagation against compressed context propagation, showing stable or improved effectiveness scores with increased rounds.

### Open Question 3
- Question: Why does increasing the number of debaters or rounds fail to improve transformation effectiveness, and can these dynamics be optimized?
- Basis in paper: [inferred] Section 5.3 ablation studies show that increasing debaters (3-6) or rounds (>1) results in flat effectiveness or negative returns, a phenomenon the paper describes but does not solve.
- Why unresolved: The underlying mechanism causing "collaborative camouflage" to succeed in small groups but fail in larger/longer interactions is not fully characterized.
- What evidence would resolve it: Identification of specific interaction patterns (e.g., argument diversity scores) that correlate with success in larger multi-agent configurations.

## Limitations

- Framework validated only on first 100 queries from Safe-RLHF dataset, limiting generalizability
- Key mechanisms (personas, local history) not rigorously tested through ablation studies
- Effectiveness measured against single LLM target (DeepSeek-V3), ignoring model-specific refusal behaviors
- Debate's "double-edged" nature (improving stealth vs. causing topic shift) based on qualitative observation rather than systematic analysis

## Confidence

- High Confidence: HarmTransform outperforms baselines on reported metrics (effectiveness: 0.36 vs. 0.24; preservation: 0.67 vs. 0.66). Core architecture clearly specified and reproducible.
- Medium Confidence: Personas increase transformation strategy diversity, but magnitude and mechanism not empirically isolated. Local-history design justified by related work but not directly tested.
- Low Confidence: Claims about debate's "double-edged" nature based on qualitative observation rather than systematic analysis. No clear threshold for when debate causes regression.

## Next Checks

1. **Persona Ablation Test**: Run HarmTransform with all debaters assigned identical personas versus diverse personas on a held-out query set to quantify persona-induced diversity gains.

2. **Local vs. Full History Comparison**: Implement and compare local-history (standard) and full-history sharing modes on a small query set to test whether context length affects debate quality or causes information loss.

3. **Cross-LLM Effectiveness Validation**: Evaluate HarmTransform's effectiveness across multiple LLM targets (e.g., GPT-4, Claude) to assess robustness of the stealth mechanism beyond DeepSeek-V3.