---
ver: rpa2
title: Keeping Yourself is Important in Downstream Tuning Multimodal Large Language
  Model
arxiv_id: '2503.04543'
source_url: https://arxiv.org/abs/2503.04543
tags:
- tuning
- language
- arxiv
- large
- downstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically reviews Multimodal Large Language Model
  (MLLM) tuning methodologies and benchmarks them across diverse downstream tasks
  to establish standardized evaluation analysis and systematic tuning principles.
  The study addresses two key challenges: Task-Expert Specialization (distribution
  shifts between pre-training and target datasets) and Open-World Stabilization (catastrophic
  forgetting of general knowledge).'
---

# Keeping Yourself is Important in Downstream Tuning Multimodal Large Language Model

## Quick Facts
- **arXiv ID:** 2503.04543
- **Source URL:** https://arxiv.org/abs/2503.04543
- **Reference count:** 40
- **Primary result:** Benchmark of MLLM tuning methods across 6 downstream tasks, establishing trade-off between Task-Expert Specialization and Open-World Stabilization

## Executive Summary
This paper presents a systematic benchmark of Multimodal Large Language Model (MLLM) tuning methodologies across diverse downstream tasks. The study addresses two fundamental challenges in MLLM tuning: Task-Expert Specialization (distribution shifts between pre-training and target datasets) and Open-World Stabilization (catastrophic forgetting of general knowledge). Through comprehensive experiments on LLaVA-OV and VILA architectures across six downstream datasets, the authors establish a standardized evaluation framework and identify key tuning principles. The research provides actionable guidance for selecting appropriate tuning methods based on task characteristics and highlights critical areas for future research in federated MLLM tuning and large-small MLLM collaboration.

## Method Summary
The paper benchmarks three MLLM tuning paradigms - Selective Tuning, Additive Tuning, and Reparameterization Tuning - across LLaVA-OV and VILA architectures. Experiments were conducted on six downstream datasets (ScienceQA, IconQA, RSVQA, PathVQA, Flickr30k, CHD) using upstream datasets (OKVQA, GQA, TextVQA, OCRVQA, COCO-Cap, MME) for stability evaluation. The study compares LoRA, DoRA, Full Layer Selective Tuning, Top Layer, and Last Layer tuning approaches, measuring Specialization Improvement (E), Stabilization Forgetting (F), and O-Average (O) derived from Accuracy and CIDEr metrics. Training used 16 A100 GPUs with learning rates of 1e-5 for LLaVA and 1e-4 for VILA (except Full-ST at 1e-5), with 3 epochs and batch size 16.

## Key Results
- LoRA demonstrates superior knowledge preservation while maintaining competitive task specialization compared to full fine-tuning
- Full Layer Selective Tuning maximizes target task accuracy but causes significant catastrophic forgetting of general knowledge
- Visual projector adaptation plays a critical role in maintaining cross-modal reasoning capabilities during tuning
- Model-specific behaviors observed: VILA shows training instability with full fine-tuning at higher learning rates
- The O-Average metric effectively captures the trade-off between specialization and stabilization across tuning methods

## Why This Works (Mechanism)
The paper establishes a framework for understanding how different tuning paradigms affect the balance between task specialization and knowledge preservation. The mechanism works by quantifying the trade-off between improving performance on target tasks while maintaining capabilities on source datasets. LoRA's parameter-efficient approach preserves the frozen backbone parameters, preventing catastrophic forgetting of pre-trained knowledge. Selective tuning methods demonstrate that layer-wise adaptation can target specific capabilities while minimizing disruption to other learned behaviors. The evaluation framework provides concrete metrics (E, F, O) that capture these trade-offs, enabling systematic comparison of tuning approaches.

## Foundational Learning

**MLLM Architecture Basics**: Understanding how vision and language components integrate in models like LLaVA and VILA is crucial for interpreting tuning effects on cross-modal reasoning.
*Why needed*: The tuning methods directly interact with the multimodal integration layers, affecting how visual and textual information is processed together.
*Quick check*: Can you identify where visual projections occur in the MLLM architecture and how they differ from pure language models?

**Catastrophic Forgetting**: The phenomenon where models lose previously learned capabilities when adapted to new tasks.
*Why needed*: This is the central challenge the paper addresses - finding tuning methods that balance new task learning with preserving general knowledge.
*Quick check*: Can you explain why full fine-tuning often leads to better target performance but worse generalization?

**Parameter-Efficient Fine-Tuning**: Methods like LoRA that modify a small subset of parameters rather than full model weights.
*Why needed*: These methods are shown to be effective for maintaining knowledge while adapting to new tasks.
*Quick check*: Can you describe how LoRA modifies attention weights without updating all model parameters?

## Architecture Onboarding

**Component Map**: Vision Encoder -> Visual Projector -> Language Model Backbone -> Output Head
- Vision Encoder processes visual input
- Visual Projector adapts visual features to language model format
- Language Model Backbone (LLaVA/VILA) handles cross-modal reasoning
- Output Head generates final predictions

**Critical Path**: Visual input → Vision Encoder → Visual Projector → Cross-Attention Layers → LLM → Output
- Tuning methods primarily affect Cross-Attention Layers and Visual Projector
- The balance between these components determines specialization vs. stabilization

**Design Tradeoffs**:
- Full tuning: Maximum specialization, maximum forgetting
- LoRA: Good specialization, excellent stabilization
- Selective tuning: Variable results depending on which layers are tuned

**Failure Signatures**:
- Training instability: Loss divergence or NaNs (particularly in VILA full fine-tuning)
- Catastrophic forgetting: >20-30% drop in upstream dataset accuracy
- Poor specialization: Target task performance close to zero-shot baseline

**Three First Experiments**:
1. Compare LoRA vs Full-ST on ScienceQA to verify stabilization advantage
2. Test different layer selections in Selective Tuning to identify optimal configurations
3. Evaluate visual projector-only tuning to assess its impact on cross-modal reasoning

## Open Questions the Paper Calls Out

**Federated MLLM Tuning**: How can parameter-efficient federated learning frameworks be optimized for MLLMs to effectively mitigate multi-client drift caused by heterogeneous data distributions while maintaining communication efficiency?
*Basis*: The paper identifies this as a future direction, noting that transferring full MLLMs incurs high costs and data heterogeneity leads to divergent optimization directions.

**Large and Small MLLM Collaboration**: What efficient collaborative paradigms can be established between large and small MLLMs to enable deployment on resource-constrained edge devices beyond simple output distribution alignment?
*Basis*: Section 5.1.2 proposes this research avenue, arguing that aligning output distributions offers limited knowledge transfer potential.

**Additive Tuning Performance**: How do Additive Tuning methods (e.g., adapters, prompts) compare to Selective and Reparameterization methods regarding the "Specialization vs. Stabilization" trade-off?
*Basis*: The authors benchmark Selective and Reparameterization tuning but exclude Additive Tuning, asserting it "brings less transferability" without experimental data.

## Limitations
- Private CHD dataset prevents full replication of medical imaging results
- Limited downstream task diversity (6 tasks) may not capture all MLLM use cases
- Underspecified hyperparameters for LoRA (rank, alpha, target modules) affect reproducibility
- Model-specific behaviors (VILA instability) may not generalize beyond tested configurations

## Confidence

**Major Claims Confidence**:
- **High**: LoRA's superior knowledge preservation and generalization benefits
- **Medium**: Task-specific method recommendations due to limited dataset coverage  
- **Low**: Conclusions requiring private data (CHD) or model-specific tuning (VILA full-layer instability)

## Next Checks

1. Replicate the LoRA vs Full-ST comparison on ScienceQA using the specified hyperparameters to verify the stabilization advantage
2. Test additional downstream tasks beyond the six presented to assess method generalizability across different visual domains
3. Conduct ablation studies on LoRA rank and layer selection to validate the sensitivity analyses suggested by the authors