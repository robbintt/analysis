---
ver: rpa2
title: Enhancing Large Vision-Language Models with Layout Modality for Table Question
  Answering on Japanese Annual Securities Reports
arxiv_id: '2505.17625'
source_url: https://arxiv.org/abs/2505.17625
tags:
- table
- layout
- text
- information
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of table question answering
  (QA) in financial documents, where tables exist in various formats and require accurate
  structural understanding. The authors propose enhancing Large Vision-Language Models
  (LVLMs) by incorporating three modalities: text, layout (bounding-box coordinates),
  and image, instead of relying on image-only inputs.'
---

# Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports

## Quick Facts
- arXiv ID: 2505.17625
- Source URL: https://arxiv.org/abs/2505.17625
- Authors: Hayato Aida; Kosuke Takahashi; Takahiro Omi
- Reference count: 16
- Primary result: Text + layout modalities significantly outperform image-only inputs for Japanese financial table QA

## Executive Summary
This paper addresses the challenge of table question answering in financial documents by enhancing Large Vision-Language Models (LVLMs) with explicit layout modality. The authors demonstrate that incorporating bounding box coordinates alongside text and image modalities significantly improves performance on the TableCellQA dataset, a simplified version of NTCIR-18 U4 focused on direct cell value extraction. Their approach achieves state-of-the-art results, with text and layout combinations achieving 95.09% accuracy and the full multimodal setup reaching 96.66% ANLS. The work highlights that while image-only approaches suffer from OCR limitations, structured text and spatial layout information are crucial for robust table understanding.

## Method Summary
The authors propose a multimodal approach that combines text, layout (bounding box coordinates), and image inputs for table QA. HTML tables from Japanese annual securities reports are rendered to PDF, then processed to extract three modalities: image screenshots, OCR-extracted text with bounding boxes, and the original text content. A two-layer MLP projects 4D bounding box coordinates into the LLM's hidden dimension, which are concatenated with text embeddings. The fused representation is combined with image and question tokens for input to LLaVA-OneVision-qwen2-7b-ov. The model is fine-tuned on 10,278 training examples from TableCellQA (filtered from NTCIR-18 U4) for 2 epochs with batch size 8 and learning rate 1e-5.

## Key Results
- Text and layout combination (L+T) achieves highest accuracy at 95.09%
- Full multimodal approach (L+T+I) achieves highest ANLS at 96.66%
- Image-only approach performs significantly worse at 87.64% accuracy
- Adding LayoutLLM-SFT pre-training decreased performance from 94.78% to 93.55%
- Structured text formats (HTML, JSON, Markdown) outperform all multimodal combinations at 96.55% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit layout embeddings compensate for LVLMs' weak spatial reasoning in tabular data.
- Mechanism: Bounding box coordinates (xmin, ymin, xmax, ymax) are projected via a two-layer MLP into the LLM's hidden dimension, then concatenated with text embeddings. This creates paired representations where each token carries both semantic content and absolute spatial position, enabling the model to distinguish "Q1 Revenue" in row 3 from "Q1 Revenue" in row 7.
- Core assumption: The model can learn meaningful spatial relationships from absolute coordinates without explicit row/column structure encoding.
- Evidence anchors:
  - [abstract]: "current Large Vision-Language Models (LVLMs)...still face challenges in accurately understanding characters and their spatial relationships within documents"
  - [section IV-B]: "models using only text input are not provided with positional relationships beyond sequence order, such errors are likely due to a lack of spatial context"
  - [corpus]: Limited direct validation; HIPPO (arXiv:2502.17315) shows similar hybrid-modal gains but uses different fusion strategy
- Break condition: Performance degrades on tables with non-standard layouts (nested tables, spanning cells) where bounding boxes don't align with logical structure.

### Mechanism 2
- Claim: Text modality provides the dominant signal for cell value extraction because OCR reliability degrades on dense financial tables.
- Mechanism: Direct text embeddings from OCR-extracted content bypass visual character recognition entirely. The model receives clean tokenized text rather than noisy visual features, eliminating OCR error propagation for the semantic content while layout handles position.
- Core assumption: OCR extraction is available and reliable at the text-extraction stage, even if visual OCR within LVLMs fails.
- Evidence anchors:
  - [section IV-B]: "text contributes the most to performance, followed by layout information"
  - [section IV-B]: "we observed basic OCR errors, such as predicting '3,468' instead of the correct value '3,466'"
  - [corpus]: MultiFinRAG (arXiv:2506.20821) similarly notes modality-specific processing for financial documents but uses retrieval rather than direct fusion
- Break condition: When OCR fails to extract text entirely (scanned documents with poor quality), text modality provides no signal.

### Mechanism 3
- Claim: Image modality provides complementary evidence despite introducing noise, improving character-level matching.
- Mechanism: Visual features capture font styling, cell borders, and visual emphasis (bold, highlighting) that text+layout cannot represent. Even with OCR errors, the visual channel offers disambiguating signals for edge cases.
- Core assumption: The LVLM's vision encoder can extract meaningful document features beyond text recognition.
- Evidence anchors:
  - [section IV-B]: "L+T+I...led to a higher ANLS score...incorporating image information...provides complementary evidence that improves character-level similarity metrics"
  - [section IV-B]: "image-only condition...performance degradation stems from the limitations of current LVLM OCR capabilities"
  - [corpus]: MMTBENCH (arXiv:2505.21771) confirms VLMs struggle with multimodal table reasoning but doesn't isolate image contribution
- Break condition: When image encoding adds computational cost without accuracy gain (observed <0.5% difference between L+T and L+T+I on accuracy).

## Foundational Learning

- Concept: Multimodal token fusion in autoregressive LLMs
  - Why needed here: The architecture interleaves layout and text tokens as a single sequence to the LLM. Understanding how different modalities are embedded and concatenated is essential for debugging and extending.
  - Quick check question: Given a table with 50 text spans, how many layout tokens does the model receive, and in what order are they combined with text tokens?

- Concept: Bounding box normalization and coordinate systems
  - Why needed here: Layout embeddings depend on consistent coordinate representation. Understanding whether coordinates are absolute pixels, normalized [0,1], or relative to image dimensions affects MLP learning.
  - Quick check question: If a table image is resized from 1000×800 to 500×400, what happens to bounding box coordinates that weren't normalized?

- Concept: Fine-tuning vs. task-specific pre-training tradeoffs
  - Why needed here: The paper shows LayoutLLM-SFT pre-training decreased performance, suggesting generalization capability matters more than domain-specific pre-training for this task.
  - Quick check question: What metric dropped when using LayoutLLM-SFT pre-training, and what does this suggest about the base model's capabilities?

## Architecture Onboarding

- Component map: HTML Table → Render to PDF → Extract modalities: Image (I): Table screenshot → Vision encoder → e_image; Text (T): OCR extraction → Text tokenizer → e_text tokens; Layout (L): Bounding boxes → 2-layer MLP → e_layout tokens → Fusion: h_i = [e_layout,i; e_text,i] for each span → Input sequence: X = [HL+T; e_image; e_question] → LLM → Answer

- Critical path: The layout MLP projection is the only learned component specific to this architecture. All other components (vision encoder, LLM backbone) are frozen from pre-trained LLaVA-OneVision-Qwen2-7b.

- Design tradeoffs:
  - L+T achieves best accuracy (0.9509) but requires OCR pipeline; I-only (0.8764) requires no preprocessing but fails on dense tables
  - HTML text (0.9655) outperforms all multimodal combinations but requires structured source data unavailable in scanned documents
  - Adding LayoutLLM-SFT pre-training decreased accuracy (0.9355 vs 0.9478), suggesting task-specific pre-training can harm generalization

- Failure signatures:
  - Adjacent cell confusion: Model selects semantically similar cell in wrong row/column (see Figure 5)—indicates layout not encoding properly
  - OCR-digit errors: "3,468" instead of "3,466" in image-only mode—indicates vision encoder limitations on dense text
  - Long sequence degradation: Large tables with many spans may exceed context window or dilute attention

- First 3 experiments:
  1. Establish baselines: Run I-only, T-only (if possible), L+T, L+T+I on a held-out validation split to confirm modality contribution hierarchy matches paper (text > layout > image).
  2. Ablate layout encoding: Replace MLP projection with (a) random projections, (b) learned 1D positional embeddings, (c) no layout tokens—compare accuracy drops to quantify layout mechanism contribution.
  3. Test coordinate normalization sensitivity: Train separate models with absolute pixel coordinates vs. normalized [0,1] coordinates vs. relative coordinates; measure convergence speed and final accuracy to determine optimal encoding.

## Open Questions the Paper Calls Out
The paper explicitly calls out the need to extend the method beyond text-only tables to handle more complex documents containing mixtures of text, images, and figures. This represents an acknowledgment of the current limitation to isolated tabular structures.

## Limitations
- The experimental scope is limited to direct cell value extraction without arithmetic reasoning, which represents only a subset of real-world financial table QA scenarios.
- The dataset originates from Japanese annual securities reports with relatively standardized table layouts, raising questions about performance on diverse document formats.
- The proposed layout embedding mechanism assumes consistent bounding box extraction, but performance degrades on non-standard layouts where absolute coordinates don't align with logical structure.

## Confidence

- **High confidence**: Text modality contribution (clearly shown to dominate performance across experiments), ANLS vs accuracy tradeoff (L+T+I achieves highest ANLS despite L+T having better accuracy), and the negative impact of LayoutLLM-SFT pre-training (clear performance degradation observed).

- **Medium confidence**: Layout embedding mechanism (supported by ablation showing spatial confusion without layout, but lacks detailed analysis of coordinate encoding choices), image modality contribution (small but consistent improvements, but computational cost-benefit unclear), and generalizability to non-financial tables (limited by dataset scope).

- **Low confidence**: Exact MLP architecture details (hidden layer dimensions and activation functions not specified), coordinate normalization strategy (absolute vs normalized coordinates impact unclear), and performance on tables requiring multi-hop reasoning (outside experimental scope).

## Next Checks

1. **Layout coordinate sensitivity test**: Train separate models using absolute pixel coordinates vs. normalized [0,1] coordinates vs. relative coordinates (to image dimensions). Measure convergence speed, final accuracy, and analyze whether coordinate scaling affects the model's ability to learn spatial relationships. This validates whether the layout mechanism depends on coordinate representation choices.

2. **Non-standard table layout stress test**: Create synthetic test cases with nested tables, spanning cells, and irregular column structures. Compare L+T performance on standard vs. irregular layouts to quantify the mechanism's robustness limits. This identifies the boundary conditions where absolute coordinate-based layout embeddings fail.

3. **Cross-domain table QA transfer**: Apply the best-performing L+T model to tables from different domains (scientific papers, web tables, invoices) without fine-tuning. Measure accuracy drop and analyze failure patterns to assess generalizability beyond Japanese financial documents. This validates whether the multimodal fusion approach transfers to diverse table understanding scenarios.