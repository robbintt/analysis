---
ver: rpa2
title: 'SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language
  Models'
arxiv_id: '2505.23713'
source_url: https://arxiv.org/abs/2505.23713
tags:
- player
- criminal
- round
- reasoning
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SocialMaze is a benchmark designed to evaluate social reasoning
  in large language models by incorporating three core challenges: deep reasoning,
  dynamic interaction, and information uncertainty. It includes six diverse tasks
  across social reasoning games, daily-life interactions, and digital community platforms.'
---

# SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models

## Quick Facts
- **arXiv ID:** 2505.23713
- **Source URL:** https://arxiv.org/abs/2505.23713
- **Reference count:** 40
- **Primary result:** SocialMaze is a benchmark designed to evaluate social reasoning in LLMs by incorporating three core challenges: deep reasoning, dynamic interaction, and information uncertainty, across six diverse tasks.

## Executive Summary
SocialMaze is a benchmark that evaluates social reasoning in large language models through three core challenges: deep reasoning, dynamic interaction, and information uncertainty. It includes six diverse tasks across social reasoning games, daily-life interactions, and digital community platforms. The benchmark uses layered social interaction graphs to model evolving social contexts and assesses models on tasks ranging from hidden role deduction to user profile inference. Experiments reveal that models with strong chain-of-thought reasoning perform better on deep reasoning tasks, dynamic interaction affects performance variably, and information uncertainty significantly degrades reasoning accuracy. Fine-tuning on curated reasoning examples substantially improves model performance in complex social scenarios. The dataset is publicly available for research use.

## Method Summary
SocialMaze evaluates social reasoning in LLMs through six tasks (Hidden Role Deduction, Find the Spy, Rating Estimation, Social Graph Analysis, Review Decision Prediction, User Profile Inference) using a 1:1:1 mixture of synthetic LLM-generated, synthetic rule-based, and real-world data. The benchmark employs layered social interaction graphs to model temporal social dynamics and assesses models across three sociologically motivated dimensions: deep reasoning, dynamic interaction, and information uncertainty. Performance is measured through accuracy metrics, with human baselines ranging from 70.8% to 96.0% across tasks. The dataset contains 70,000 total instances and is publicly available at https://huggingface.co/datasets/MBZUAI/SocialMaze.

## Key Results
- Long chain-of-thought models achieve 90.2% accuracy on Hidden Role Deduction vs. 8.2% for short chain-of-thought models
- Information uncertainty severely degrades performance, with short chain-of-thought models near-random on compromised identity tasks
- Fine-tuning on curated reasoning examples substantially improves model performance in complex social scenarios
- Dynamic interaction effects vary by task: accuracy improves in Hidden Role Deduction but degrades in Review Decision Prediction after rebuttal stage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extended chain-of-thought reasoning improves performance on tasks requiring latent state inference and multi-step deduction
- Mechanism: Long CoT models generate detailed intermediate reasoning chains, enabling hypothesis testing, evidence evaluation, and systematic elimination of possibilities—critical when surface-level cues are insufficient
- Core assumption: Longer reasoning traces reflect genuine inference rather than simply more tokens; manual inspection in the paper confirms reasoning is "sound and coherent in correct predictions"
- Evidence anchors:
  - [abstract] "models with strong chain-of-thought reasoning perform better on tasks requiring deeper inference beyond surface-level cues"
  - [Section 4.1] Long CoT models achieve substantially higher accuracy on Deep Reasoning tasks (Graph analysis, Role deduction), with output tokens nearly 8x higher than Short CoT models on these tasks
  - [corpus] HiddenBench (arXiv:2505.11556) assesses collective reasoning in multi-agent LLMs, suggesting similar reasoning depth challenges—though direct comparison is not available
- Break condition: When tasks are solvable through pattern matching or shallow heuristics (e.g., "Find the Spy" with low uncertainty), extended reasoning provides marginal gains

### Mechanism 2
- Claim: Temporal information accumulation across interaction rounds improves reasoning accuracy, but trajectory depends on information structure
- Mechanism: Models integrate evidence sequentially; when additional rounds provide consistent, discriminate signals, accuracy increases. When information is contradictory or strategically deceptive, additional rounds can mislead
- Core assumption: Models can maintain and update beliefs across context; failure to do so indicates limitations in temporal reasoning capacity
- Evidence anchors:
  - [abstract] "models vary substantially in their ability to handle dynamic interactions and integrate temporally evolving information"
  - [Table 2] Criminal identification accuracy in Hidden Role Deduction improves across rounds (e.g., Gemini-2.5-Pro: 43.3%→74.3%→87.6%); however, [Table 3] Review Decision Prediction shows accuracy *drops* after rebuttal stage for most models (e.g., Llama-3.3-70B: 87.4%→72.2%)
  - [corpus] SI-Bench (arXiv:2510.23182) evaluates social intelligence in human-to-human conversations with similar temporal dynamics
- Break condition: When later information is persuasive but incorrect (e.g., author rebuttals that convince models but fail to change actual decisions), accuracy degrades

### Mechanism 3
- Claim: Information uncertainty—particularly when it affects self-perception—severely degrades reasoning, with Short CoT models near-random on compromised identity tasks
- Mechanism: Models must simultaneously evaluate external information reliability AND their own epistemic status. When assigned roles like Rumormonger (believing they're truthful but actually random) or Lunatic (believing they're Criminal but aren't), models struggle to reconcile conflicting evidence with compromised priors
- Core assumption: This reflects a lack of meta-reasoning capacity—ability to question one's own assumptions and information sources
- Evidence anchors:
  - [abstract] "model reasoning degrades significantly under uncertainty"
  - [Figure 5] Short CoT models are "almost entirely unable to deduce their true identity when assigned as a Rumormonger or Lunatic"; Long CoT models show better resilience
  - [corpus] Limited direct corpus evidence on self-referential uncertainty; related work on ToM benchmarks (FANToM, ToMValley) addresses belief tracking but not compromised self-perception
- Break condition: Long CoT models partially overcome this through explicit hypothesis exploration, though performance still degrades compared to baseline

## Foundational Learning

- Concept: Layered Graph Representation
  - Why needed here: SocialMaze models social interactions as temporal graphs G = (G₁, G₂, ..., G_T) where vertex sets are consistent but edge sets evolve across rounds
  - Quick check question: Can you sketch how a 6-player Hidden Role Deduction game maps to 3 graph layers with the same 6 vertices?

- Concept: Belief Updating Under Uncertainty
  - Why needed here: Models must maintain probability distributions over role assignments and update them as statements accumulate, accounting for source reliability
  - Quick check question: If Investigator A says "B is not Criminal" but Rumormonger C says the same, how should your belief update differ?

- Concept: Chain-of-Thought Reasoning Depth
  - Why needed here: Performance gaps between Long and Short CoT models (e.g., 90.2% vs 8.2% on Hidden Role Deduction) indicate that explicit intermediate steps are mechanistically important for complex deduction
  - Quick check question: Given the output token ratio of ~8x for Deep Reasoning tasks, what inference budget considerations apply?

## Architecture Onboarding

- Component map:
  - Data Generation: Three pipelines—LLM-assisted (Find the Spy, User Profile Inference), rule-based simulation (Hidden Role Deduction, Social Graph Analysis), real-world data (Rating Estimation, Review Decision Prediction)
  - Layered Graph Encoder: Converts social scenarios into temporal graph representations, then to natural language descriptions (not raw graphs)
  - Query Interface: Three types—Vertex-centric (Q_v for individual attributes), Edge-centric (Q_e for relationships), Graph-level (Q_G for holistic properties)
  - Evaluation Framework: Accuracy metrics per task, with round-by-round tracking for dynamic tasks

- Critical path:
  1. Understand task-specific role behaviors and rules (e.g., Investigators always truthful, Criminals may lie)
  2. Parse natural language input into internal belief state over graph
  3. Generate reasoning chain (explicit for Long CoT models)
  4. Output predictions for both criminal identification AND self-role identification

- Design tradeoffs:
  - Synthetic vs. real data: Paper uses ~1:1:1 ratio (synthetic-LLM : synthetic-rule : real); synthetic enables controlled difficulty but may not capture real-world unpredictability
  - Solvability verification: Algorithmic search ensures unique solutions for Hidden Role Deduction, but this may reduce ecological validity
  - Token budget: Long CoT models achieve higher accuracy but with 8x output tokens—deployment requires accuracy-cost analysis

- Failure signatures:
  - Short CoT models on Rumormonger/Lunatic roles: Near-zero self-role identification accuracy
  - Models swayed by persuasive rebuttals: Accuracy drops in Review Decision Prediction Stage 3
  - Early-round overconfidence: Models that converge on incorrect hypotheses and fail to update

- First 3 experiments:
  1. Baseline evaluation: Run target model on all 6 tasks with temperature=0.7, track accuracy across rounds for dynamic tasks
  2. Ablation on reasoning depth: Compare performance with/without explicit chain-of-thought prompting on Hidden Role Deduction Full Task
  3. Uncertainty sensitivity test: Evaluate model on Original→Rumormonger→Lunatic→Full task variants to quantify degradation curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can rigorous, task-agnostic numerical metrics be developed to quantify Deep Reasoning, Dynamic Interaction, and Information Uncertainty in social reasoning tasks?
- Basis in paper: [explicit] "Designing rigorous, task-agnostic numerical measures for such social constructs remains an open problem that we leave to future work."
- Why unresolved: The three sociologically motivated dimensions are difficult to express as single scalars; the paper relies on qualitative task annotations and a posteriori validation rather than formal metrics.
- What evidence would resolve it: A validated measurement framework that assigns consistent scores across tasks, correlates with model performance gaps, and enables cross-benchmark comparisons.

### Open Question 2
- Question: To what extent do conclusions from SocialMaze's synthetic tasks (LLM-generated and rule-based) transfer to settings involving only organic human social interactions?
- Basis in paper: [explicit] "It also implies that conclusions drawn from the benchmark may not transfer verbatim to settings where only organic human language is present."
- Why unresolved: The benchmark uses a 1:1:1 mixture of synthetic-LLM, synthetic-rule, and real data; human spontaneity and unpredictability differ from simulated interactions.
- What evidence would resolve it: Comparative evaluation showing similar model performance patterns on matched synthetic vs. purely human-collected social reasoning tasks.

### Open Question 3
- Question: What architectural or training interventions could enable short-chain-of-thought models to perform self-doubt and meta-reasoning under compromised information (e.g., Rumormonger/Lunatic roles)?
- Basis in paper: [inferred] "Short CoT models are almost entirely unable to deduce their true identity when assigned as a Rumormonger or Lunatic, suggesting a profound lack of capacity for self-doubt and meta-reasoning."
- Why unresolved: Current short-CoT models fail at reconciling internal beliefs with conflicting external evidence, while long-CoT models show resilience but at high computational cost.
- What evidence would resolve it: Demonstrated improvements in self-role identification accuracy for short-CoT models through specific interventions (e.g., uncertainty-aware training, reflective prompting, or specialized fine-tuning).

### Open Question 4
- Question: What underlying factors determine why dynamic interaction improves model performance in some tasks (Hidden Role Deduction) but degrades it in others (Review Decision Prediction after rebuttal)?
- Basis in paper: [inferred] From Section 4.2: "model accuracy generally improves with quantitatively increasing interaction, but the trajectory of performance evolution and sensitivity to dynamic information vary significantly across different tasks and models."
- Why unresolved: The non-linear trajectory in Review Decision Prediction (accuracy drops after rebuttal) suggests models are swayed by persuasive but misleading content, but generalizable principles remain unclear.
- What evidence would resolve it: Systematic ablation studies identifying which properties of accumulating information (persuasiveness, conflict level, source credibility) predict performance changes across tasks.

## Limitations
- Synthetic components may not capture the full complexity and unpredictability of real social interactions
- Performance patterns may be task-specific rather than reflecting universal LLM social reasoning capabilities
- The benchmark relies on accuracy metrics that may not fully capture nuanced aspects of social reasoning

## Confidence
- **High confidence**: Chain-of-thought reasoning benefits (well-supported by substantial performance gaps and manual inspection)
- **Medium confidence**: Dynamic interaction handling (supported by round-by-round accuracy patterns but mixed results)
- **Medium confidence**: Uncertainty degradation (strong evidence for performance degradation but limited corpus support for self-referential uncertainty specifically)
- **Low confidence**: Generalizability claims (benchmark performance patterns may not transfer across different social reasoning domains)

## Next Checks
1. **Transferability validation**: Test whether models that perform well on SocialMaze Hidden Role Deduction also show similar performance patterns on HiddenBench's collective reasoning tasks. This would validate whether the observed reasoning patterns are domain-general or task-specific.

2. **Temporal reasoning probe**: For models showing accuracy degradation in later rounds of Review Decision Prediction, analyze their reasoning chains to determine if they're genuinely updating beliefs versus simply pattern-matching temporal cues. Compare against a counterfactual where round order is shuffled.

3. **Uncertainty robustness scaling**: Systematically vary the degree of information uncertainty across tasks (e.g., partial vs. full identity compromise) and measure the relationship between uncertainty magnitude and performance degradation. This would test whether current uncertainty handling scales proportionally or hits capacity limits.