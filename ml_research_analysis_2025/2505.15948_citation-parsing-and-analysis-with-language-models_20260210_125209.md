---
ver: rpa2
title: Citation Parsing and Analysis with Language Models
arxiv_id: '2505.15948'
source_url: https://arxiv.org/abs/2505.15948
tags:
- citation
- surname
- citations
- language
- open
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of open-weight language models
  to accurately parse and annotate bibliographic citations, addressing the global
  knowledge inequality gap where research from the Global South is systematically
  underrepresented in scholarly databases. The authors assembled a dataset of 2,000
  matched plaintext and JATS-annotated citations from preprints and published articles,
  then evaluated 11 different open-weight language models on their ability to identify
  citation components like author surnames, article titles, and publication years.
---

# Citation Parsing and Analysis with Language Models

## Quick Facts
- arXiv ID: 2505.15948
- Source URL: https://arxiv.org/abs/2505.15948
- Reference count: 13
- Modern language models achieve over 90% accuracy on citation component identification, significantly outperforming GROBID and Crossref search

## Executive Summary
This study evaluates open-weight language models for parsing bibliographic citations and annotating them with JATS XML markup. The authors assembled a dataset of 2,000 matched plaintext and JATS-annotated citations from preprints and published articles, then tested 11 different open-weight models. Results show modern language models significantly outperform existing state-of-the-art methods like GROBID and Crossref search, with most models achieving over 90% accuracy on key fields. The research addresses global knowledge inequality by providing a tool that can be deployed in low-resource settings to make Global South research more discoverable.

## Method Summary
The authors created a dataset of 2,000 matched plaintext and JATS-annotated citations from two sources: the Garnett/PKP corpus and Open Research Europe. Citations were extracted from article markdown using Llama-3.1-8B-Instruct, then matched to JATS XML via similarity scoring with a 0.75 threshold. Eleven open-weight language models were evaluated using two-shot prompting with examples showing plaintext-to-JATS transformations. Models were tested under both chain-of-thought and direct-output conditions, with accuracy measured per field using exact match or edit-distance thresholds. Pass@64 sampling was used to assess small-model capability.

## Key Results
- Language models achieved over 90% accuracy on identifying citation components, outperforming GROBID by 8.76 percentage points for article titles
- Even the smallest model tested (Qwen3-0.6B) demonstrated high accuracy within 64 sampling passes, suggesting RLVR post-training could improve efficiency
- Coverage (percentage of valid XML outputs) was high across all models, with invalid XML counting as incorrect for all fields

## Why This Works (Mechanism)

### Mechanism 1: Pre-trained structural pattern recognition in bibliographic text
Open-weight decoder LLMs accurately identify citation components because academic citation patterns are well-represented in pre-training corpora, enabling zero-shot generalization to structured annotation. Models learn statistical regularities of bibliographic formatting—author surnames typically precede given names, years appear near volume/issue numbers, article titles precede journal names—during pre-training on academic text. At inference, few-shot examples activate these latent patterns.

### Mechanism 2: Few-shot prompting establishes input-output template alignment
Two-shot prompting with plaintext-input/JATS-output pairs provides sufficient format specification for models to generate structured XML without fine-tuning. The prompt includes two complete examples showing the exact transformation expected. The model performs pattern completion—given a new plaintext citation, it extends the demonstrated mapping to produce JATS XML following the same template structure.

### Mechanism 3: Pass@k sampling reveals latent capability tractable to RLVR post-training
Small models (0.6B parameters) encode sufficient citation-parsing capability, but the correct output isn't the top-ranked token; multiple samples surface correct answers, indicating reinforcement learning with verifiable rewards (RLVR) could improve pass@1. Pass@64 measures whether the correct annotation exists in the model's output distribution at all. High pass@k with low pass@1 indicates the model "knows" the answer but has poor sampling efficiency—exactly what RLVR addresses by shaping the reward landscape.

## Foundational Learning

- **Concept: JATS XML (Journal Article Tag Suite)**
  - Why needed here: This is the target output format. Understanding `<mixed-citation>`, `<article-title>`, `<surname>`, `<year>`, `<source>`, and related tags is essential for evaluating model outputs and debugging failures.
  - Quick check question: Given a plaintext citation "Smith J, Jones M. 2023. Deep Learning for Parsing. Nature 15:100-110.", identify which portions map to `<surname>`, `<article-title>`, `<source>`, and `<year>` tags.

- **Concept: Pass@k evaluation and its relationship to model capacity**
  - Why needed here: The paper's core claim about small-model viability rests on pass@64 results; understanding this metric distinguishes "model can't do the task" from "model can but samples inefficiently."
  - Quick check question: If a model achieves 50% pass@1 but 95% pass@64 on citation parsing, what does this imply about (a) whether the task is learnable, and (b) what intervention might help?

- **Concept: Constrained decoding for structured outputs**
  - Why needed here: The Discussion mentions grammar-constrained decoding as an unexplored technique to guarantee valid XML. Understanding this helps evaluate deployment options.
  - Quick check question: How would a JATS-XML grammar constraint prevent the model from generating `<article-title>2023</article-title>` when "2023" is a year, not a title?

## Architecture Onboarding

- **Component map:**
  ```
  [Plaintext article (markdown)]
           ↓
  [Citation extraction: Llama-3.1-8B-Instruct + Appendix A.1 prompt]
           ↓
  [Plaintext citations] ←match via similarity→ [JATS citations from XML]
           ↓                                    (ground truth)
  [Few-shot prompt construction: 2 examples + target citation]
           ↓
  [Language model inference: 0.6B–14B params, temp/top-p/k settings]
           ↓
  [Raw model output: may or may not be valid XML]
           ↓
  [XML validation → field extraction → accuracy scoring]
           ↓
  [Per-field metrics: exact match OR edit-distance threshold]
  ```

- **Critical path:**
  1. **Citation extraction** (currently unaudited): Llama-3.1-8B-Instruct extracts citations from markdown—this step's error rate propagates through all downstream evaluation.
  2. **Similarity matching**: Plaintext-JATS pairs with similarity < 0.75 are discarded; this threshold determines dataset composition.
  3. **Field-level evaluation**: Different fields use different match criteria—year/surname require exact match; article-title allows edit distance ≤10; source allows ≤5.

- **Design tradeoffs:**
  - **Coverage vs. accuracy**: Invalid XML counts as incorrect for all fields. Constrained decoding could guarantee 100% coverage but wasn't tested.
  - **Model size vs. deployment context**: 0.6B model runs in browser/on modest servers but needs 64 samples for reliability; 14B model has higher pass@1 but requires server-grade GPU.
  - **CoT vs. direct output**: Reasoning traces improve small-model accuracy but add latency and may exceed context limits for long citations.
  - **Edit distance thresholds**: Tight thresholds (exact match only) increase precision but penalize minor formatting differences that are semantically equivalent.

- **Failure signatures:**
  - **Low similarity scores** (ORE corpus ~0.85 vs PKP ~0.92): Indicates plaintext includes extra content (links, DOIs as clickable text) absent from JATS markup.
  - **Coverage < 100%**: Model failed to produce valid XML—likely hit token limit or entered repetitive loop.
  - **High pass@64, low pass@1**: Correct answer exists in distribution but model hasn't learned to prioritize it—candidate for RLVR.
  - **Title accuracy lower than surname/year**: Titles have higher variance (edit distance threshold) and may contain special characters or nested formatting that confuses extraction.

- **First 3 experiments:**
  1. **Reproduce baseline on 200-citation subset**: Sample 200 citations from the 2,000-citation dataset. Test 3 models (Qwen3-0.6B, Llama-3.1-8B-Instruct, DeepSeek-R1-Distill-Qwen-7B) using Appendix A.2 prompts. Verify your accuracy calculation matches paper's methodology (exact match for year/surname, edit distance thresholds for title/source). Document coverage rate separately from field accuracy.
  2. **Quantify citation extraction error rate**: Manually annotate 50 plaintext extractions from the Llama-3.1-8B-Instruct step. Calculate precision/recall for citation boundary detection and content preservation. This audits the currently unevaluated pipeline stage.
  3. **Test grammar-constrained decoding for coverage improvement**: Implement constrained decoding using a JATS-XML grammar (tools: Guidance, Outlines, or Synchromesh). Run on 200 citations with Qwen3-0.6B. Compare coverage and field accuracy against unconstrained baseline. Expect: coverage → 100%, accuracy may improve or degrade depending on constraint quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language models maintain high citation parsing accuracy when quantized and deployed in low-resource contexts such as browsers or modest journal servers?
- Basis in paper: [explicit] The authors state "additional research is needed to evaluate accuracy and feasibility in the low-resource context that is typically available for academic journals."
- Why unresolved: All experiments used non-quantized models on H200 GPUs; real-world journal infrastructure has far less compute.
- What evidence would resolve it: Benchmark results for quantized models running in browser/low-server environments on the same citation parsing task.

### Open Question 2
- Question: Would distillation or supervised fine-tuning produce smaller, more efficient citation parsing models with high pass@1 accuracy?
- Basis in paper: [explicit] The authors note "distillation and supervised fine-tuning may be effective in learning a high-accuracy annotation system, especially because this problem has a lot of available training data."
- Why unresolved: Only RLVR potential was tested (via pass@64); other post-training methods were not evaluated.
- What evidence would resolve it: Comparative experiments showing pass@1 accuracy for distilled or fine-tuned small models against the base models tested.

### Open Question 3
- Question: Can constrained decoding (grammar-based) guarantee valid JATS XML output while maintaining or improving field-level accuracy?
- Basis in paper: [explicit] The authors mention "one can define a grammar and use constrained decoding to guarantee that the language model produces valid JATS XML, boosting coverage to 100%."
- Why unresolved: Test-time techniques including constrained decoding were not explored in this work.
- What evidence would resolve it: Experiments showing coverage and field accuracy with constrained decoding compared to unconstrained sampling.

### Open Question 4
- Question: What is the accuracy of the initial citation extraction step (identifying plaintext citations from article markdown)?
- Basis in paper: [explicit] The authors acknowledge: "One other limitation of our work is that it relies on an initial extraction phase... We did not test the accuracy of this step."
- Why unresolved: Llama-3.1-8B-Instruct was used to extract citations but evaluation focused only on annotation, not extraction.
- What evidence would resolve it: Manual or automated evaluation of citation extraction accuracy on a sample of articles.

## Limitations
- The evaluation pipeline depends on an unvalidated citation extraction step using Llama-3.1-8B-Instruct, which could introduce systematic errors not captured in field-level accuracy metrics
- The dataset composition (2,000 citations) represents a specific subset of academic publishing formats, potentially limiting generalizability to non-Western citation styles, pre-1900 references, or highly irregular formatting
- The pass@k→RLVR potential correlation (established by Yue et al. 2025) transfers to structured extraction tasks without direct validation for citation parsing specifically

## Confidence
- **High confidence**: Models outperform GROBID and Crossref search on the tested dataset; pass@k analysis correctly identifies small-model capability ceilings
- **Medium confidence**: Two-shot prompting provides sufficient format specification across diverse citation styles; high pass@64 correlates with RLVR potential for this task
- **Low confidence**: Citation extraction error rate is negligible; dataset represents global citation diversity adequately

## Next Checks
1. Manually audit 100 extracted citations from the Llama-3.1-8B-Instruct step to quantify precision/recall for citation boundary detection and content preservation
2. Test model performance on non-Western citation formats (APA, Vancouver, Bluebook legal citations) to validate cross-cultural generalizability
3. Implement grammar-constrained decoding and measure coverage vs. accuracy tradeoffs compared to unconstrained sampling