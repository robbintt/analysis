---
ver: rpa2
title: 'Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models'
arxiv_id: '2512.03056'
source_url: https://arxiv.org/abs/2512.03056
tags:
- lora
- target
- adaptation
- diffusion
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Delta Sampling (DS) enables knowledge transfer across diffusion
  models with different architectures by leveraging the prediction difference (delta)
  between a base model and its adapted counterpart. The method operates entirely at
  inference time, injecting this delta into the denoising process of a target model
  to guide generation toward desired effects (e.g., visual styles, semantic concepts,
  or structural constraints).
---

# Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models

## Quick Facts
- arXiv ID: 2512.03056
- Source URL: https://arxiv.org/abs/2512.03056
- Reference count: 40
- Primary result: Delta Sampling enables cross-model knowledge transfer without retraining or original data

## Executive Summary
Delta Sampling (DS) introduces a novel approach for transferring knowledge across diffusion models with different architectures by operating entirely at inference time. The method leverages the prediction difference (delta) between a base model and its adapted counterpart, injecting this delta into the denoising process of a target model to guide generation toward desired effects. DS is model-agnostic, plug-and-play, and supports complex compositions of multiple adaptation modules without requiring retraining or access to original training data. Evaluations across various Stable Diffusion versions, samplers, and adaptation types show consistent improvements in transferring stylistic and structural effects while maintaining visual diversity.

## Method Summary
Delta Sampling works by computing the difference between noise predictions from a base model and an adapted model at each denoising step, then injecting this delta into the denoising process of a target model. The approach uses a guidance strength parameter λ to control the influence of the delta during sampling. The method operates on the latent space and can be applied to any diffusion model without architectural modifications. DS supports multi-step sampling with optional λ decay schedules (linear, exponential, cosine) to prevent over-amplification. The framework enables composition of multiple adaptations through sequential delta application, making it flexible for complex transfer scenarios.

## Key Results
- Enables cross-architecture knowledge transfer between SD-1.5, SD-2.1, SD-XL, SD-3, and SD-3.5 without retraining
- Successfully transfers stylistic effects (e.g., painting styles, color schemes) and structural constraints (e.g., pose, composition)
- Maintains visual diversity while achieving higher CLIP similarity to target concepts compared to baseline methods
- Supports composition of multiple adaptations through sequential delta application
- Operates entirely at inference time without requiring access to original training data

## Why This Works (Mechanism)
Delta Sampling works by exploiting the observation that prediction-space differences between base and adapted models capture transferable adaptation effects that can be applied to different architectures. The delta represents the learned adaptation in prediction space, which remains meaningful even when transferred to models with different latent spaces or parameterizations. By injecting this delta during the denoising process, DS guides the target model's generation toward the desired effect while preserving the model's original capabilities. The method's effectiveness across diverse architectural gaps suggests that prediction-space representations contain transferable semantic information about adaptation effects.

## Foundational Learning
- Diffusion model sampling mechanics - why needed: Understanding denoising process and prediction-space operations; quick check: Can trace forward and reverse diffusion steps
- Latent space representations - why needed: DS operates on latents rather than pixel space; quick check: Can explain VAE encoder/decoder role in SD
- CLIP embedding similarity - why needed: Primary quantitative metric for adaptation quality; quick check: Can compute CLIP similarity between generated and reference images
- Prediction-space delta computation - why needed: Core mechanism for cross-model transfer; quick check: Can calculate ε_base - ε_adapt for given latents
- Guidance strength parameter λ - why needed: Controls delta influence magnitude; quick check: Can explain effect of different λ values on generation stability

## Architecture Onboarding

Component Map:
Latent z₀ → ε_base(z, t) + ε_adapt(z, t) → Δ = ε_base - ε_adapt → ε_target(z, t) + λΔ → next latent → ... → final image

Critical Path:
At each timestep: Compute both base and adapted predictions → calculate delta → inject into target prediction → denoise latent → repeat until completion

Design Tradeoffs:
- Dual model execution increases inference time vs. single-model approaches
- λ tuning required for each adaptation scenario vs. automated parameter selection
- Prediction-space transfer works across architectures vs. potential information loss

Failure Signatures:
- Over-amplification causing generation instability (λ too high)
- Loss of visual diversity from excessive delta injection
- Incomplete adaptation transfer when architectural gaps are too large

First Experiments:
1. SD-1.5 to SD-2.1 transfer of basic color scheme adaptation
2. Multi-module composition: style + structural constraints transfer
3. SD-XL to SD-3.5 transfer with linear λ decay schedule

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical foundations explaining why prediction-space deltas transfer effectively across diffusion models with different architectures and latent spaces?
- Basis in paper: The conclusion states "future work could include deeper quantitative analysis to optimize, understand, and extend the DS mechanism."
- Why unresolved: The paper empirically demonstrates DS works across SD-1.5, SD-2.1, SD-XL, SD-3, and SD-3.5 but provides no theoretical analysis of why noise prediction differences capture transferable adaptation effects across models with different parameterizations and training distributions.
- What evidence would resolve it: Formal analysis connecting prediction-space deltas to weight-space adaptations; theoretical bounds on transfer effectiveness as a function of architectural divergence or latent space discrepancy.

### Open Question 2
- Question: Can guidance strength λ be automatically adapted per-timestep or per-adaptation without manual tuning?
- Basis in paper: The paper states "The optimal choice of λ can vary across tasks, depending on the strength and specificity of the underlying adaptation" and notes that "excessively large values may over-amplify the residual and destabilize generation," yet provides only manual selection.
- Why unresolved: Current approach requires empirical tuning for each adaptation scenario, and while λ-decay schedules (linear, exponential, cosine) are proposed, no principled method for selecting schedule parameters is provided.
- What evidence would resolve it: Development and systematic evaluation of adaptive λ methods (e.g., based on delta magnitude, timestep, or target model uncertainty) across diverse adaptations with quantitative fidelity metrics.

### Open Question 3
- Question: Under what conditions does Delta Sampling fail or cause negative transfer?
- Basis in paper: The paper demonstrates successful transfers across multiple architectures and adaptation types but does not systematically investigate failure modes, degradation cases, or architectural limits where transfer quality collapses.
- Why unresolved: Identifying edge cases where DS harms generation quality or fails to capture adaptation effects would establish practical applicability boundaries and inform users when to avoid this approach.
- What evidence would resolve it: Controlled experiments systematically varying architectural gaps (e.g., SD-1.5 to SD-3.5), adaptation complexity (single vs. multi-module), and concept types to identify failure boundaries with quantitative quality metrics.

### Open Question 4
- Question: Can the computational overhead of running both base and adapted models be reduced through approximation or distillation while preserving transfer quality?
- Basis in paper: DS requires computing predictions from both ε_base and ε_adapt at each denoising step, effectively doubling forward passes compared to standard sampling, which is acknowledged as inference-time overhead without proposed solutions.
- Why unresolved: The paper does not address efficiency optimizations such as delta caching, learned delta predictors, or distillation approaches that could reduce the requirement for dual model execution.
- What evidence would resolve it: Systematic comparison of approximation methods (e.g., caching early-stage deltas, training compact delta predictors) measuring inference speed vs. CLIP similarity and visual fidelity trade-offs.

## Limitations
- No theoretical explanation for cross-architecture transfer effectiveness
- Requires manual tuning of guidance strength λ for each adaptation scenario
- Doubles inference time by requiring dual model execution
- No systematic investigation of failure modes or architectural limits

## Confidence
- Method validity: High - Extensive empirical evaluation across multiple architectures and adaptations
- Transfer effectiveness: Medium - Strong qualitative results but limited theoretical understanding
- Scalability: Medium - Works for tested architectures but unknown performance on larger gaps
- Efficiency claims: Low - No analysis of computational overhead or optimization approaches

## Next Checks
1. Replicate cross-architecture transfer from SD-1.5 to SD-3.5 with multi-module composition
2. Systematically test λ values (0.1 to 1.0) on a single adaptation type to map stability boundaries
3. Measure actual inference time overhead comparing DS vs. single-model sampling on same hardware