---
ver: rpa2
title: Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in
  Human Decision-Making
arxiv_id: '2505.08049'
source_url: https://arxiv.org/abs/2505.08049
tags:
- learning
- rates
- bayesian
- q-learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the interpretation of "confirmation bias"
  in human decision-making by showing that standard Q-learning models with asymmetric
  learning rates can misattribute decreasing learning rates to cognitive bias. When
  Bayesian inference (an optimal learning strategy) is recast in a Q-learning framework,
  it exhibits symmetric but decreasing learning rates over time.
---

# Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making

## Quick Facts
- arXiv ID: 2505.08049
- Source URL: https://arxiv.org/abs/2505.08049
- Reference count: 12
- Primary result: Standard Q-learning models misattribute Bayesian inference to confirmation bias when learning rates decrease over time

## Executive Summary
This paper challenges the interpretation of "confirmation bias" in human decision-making by showing that standard Q-learning models with asymmetric learning rates can misattribute decreasing learning rates to cognitive bias. When Bayesian inference (an optimal learning strategy) is recast in a Q-learning framework, it exhibits symmetric but decreasing learning rates over time. By analyzing the stochastic dynamics using master equations, the authors demonstrate that both confirmation bias and unbiased decreasing learning rates produce identical behavioral signatures—reduced action-switching probabilities. They validate this by fitting models to human data from Palminteri et al. [2017], showing that Bayesian and confirmation-biased models fit equally well. The paper proposes novel experimental protocols to distinguish true cognitive biases from artifacts of decreasing learning rates, emphasizing the need for more nuanced model comparisons in cognitive science.

## Method Summary
The study compares Bayesian inference and Q-learning models in two-armed Bernoulli bandit tasks with counterfactual feedback. The authors derive that Bayesian inference maps to Q-learning with symmetric, time-varying learning rates (α_t = 1/(t+3)). They develop a master equation framework to analyze action-switching dynamics, showing that both confirmation bias and unbiased decreasing rates reduce switching probability. Models are fit to human data from Palminteri et al. (2017) using maximum likelihood estimation, with model comparison via BIC scores. The paper also proposes a new experimental design involving a third arm to distinguish between true bias and optimal learning.

## Key Results
- Bayesian inference cast as Q-learning yields symmetric but decreasing learning rates (α_t = 1/(t+3))
- Fitting constant-rate Q-learning to Bayesian data produces spurious confirmation bias signatures
- Both confirmation bias and unbiased decreasing rates produce identical reduced action-switching probabilities
- Bayesian and confirmation-biased models fit human data equally well (similar BIC scores)
- Proposed three-arm experiment can distinguish true bias from optimal learning

## Why This Works (Mechanism)

### Mechanism 1: Bayesian-to-Q-Learning Mapping via Decreasing Learning Rates
- Claim: Objective Bayesian inference can be expressed as Q-learning with symmetric but monotonically decreasing learning rates
- Mechanism: Beta-distributed beliefs over Bernoulli parameters yield update rules where α± = 1/(t+3), meaning learning rates decay as ~1/t with experience. This arises because Bayesian uncertainty shrinks with each observation, naturally reducing update magnitude
- Core assumption: Bernoulli bandit tasks with uniform priors; counterfactual feedback available
- Evidence anchors: [abstract] "Bayesian inference cast as an effective Q-learning algorithm has symmetric, though decreasing, learning rates"; [section] Eq. 7: "α± = 1/(αv,t + βv,t + 3) = 1/(t+3)"

### Mechanism 2: Model Misspecification Induces Spurious Bias Signatures
- Claim: Fitting constant-learning-rate Q-learning models to Bayesian-generated data produces apparent positivity and confirmation biases that are statistical artifacts, not true cognitive distortions
- Mechanism: Constant-rate models cannot capture the 1/t decay of Bayesian learning. To compensate for the mismatch in action switching dynamics, parameter fitting assigns asymmetric learning rates (αc+ > αc−, αu− > αu+), mimicking bias signatures
- Core assumption: The standard Q-learning formulation (Eq. 1) with stationary α is misspecified for Bayesian agents
- Evidence anchors: [abstract] "fitting the standard Q-learning model with asymmetric learning rates still recovers both biases...even if the agent updates its belief via objective Bayesian inference"; [section] Fig. 1(a): Ensemble-averaged best-fit learning rates from Bayes-optimal agents show asymmetry patterns similar to human data

### Mechanism 3: Convergent Behavioral Signatures via Distinct Dynamic Routes
- Claim: Both confirmation bias (with constant rates) and unbiased decreasing learning rates reduce action switching probability ⟨K⟩t through different mechanisms that are behaviorally indistinguishable in standard experiments
- Mechanism: Confirmation bias increases steady-state Q-value separation ∆∗, reducing switching. Decreasing rates first allow noise to drive ∆t up (small α early), then "freeze" Q-values by making them sluggish (small α late), sustaining low switching. Both lower ⟨K⟩t relative to constant unbiased learning
- Core assumption: Symmetric environments (p1 = p2) for analytical tractability; softmax action selection
- Evidence anchors: [abstract] "both confirmation bias and unbiased but decreasing learning rates yield the same behavioral signatures"; [section] Eq. 30: "∆t+1 = (1−αt)²∆t [epistemic drift] + p(1−p)αt² [noise]"; Fig. 2(c) shows ⟨K⟩t curves overlap for biased and decreasing-rate cases

## Foundational Learning

- **Q-learning with asymmetric learning rates**: Core model being critiqued; formalizes positivity/confirmation biases through differential α+ vs α− for chosen/unchosen arms. Quick check: Can you write the update rule distinguishing positive vs negative prediction errors?
- **Beta-Bernoulli conjugate inference**: Provides the Bayesian benchmark; enables closed-form belief updates and the αt = 1/(t+3) mapping. Quick check: Given successes α and failures β, what is the posterior mean of a Beta(α+1, β+1) distribution?
- **Master equation formulation of stochastic processes**: Analytical tool for deriving action switching dynamics without Monte Carlo simulation; enables moment-closure approximations. Quick check: How does the master equation P_{t+1}(Q) = ∫P_t(Q')T(Q'→Q)dQ' differ from simulating individual trajectories?

## Architecture Onboarding

- **Component map**: Data source -> Fitting models -> Analysis layer -> Comparison layer
  - Data source: Bayes-optimal agents or human subjects in TABB tasks with counterfactual feedback
  - Fitting models: (1) Bayes-greedy (1 df), (2) Unbiased Q-learning (2 df), (3) Full Q-learning (5 df), (4) Confirmation model (3 df)
  - Analysis layer: Master equation derivation -> moment equations (Eqs. 14, 16, 17) -> action switching probability ⟨K⟩t
  - Comparison layer: BIC/NLL model comparison; behavioral signature matching (⟨K⟩t curves)

- **Critical path**: 1. Generate/simulate behavioral data from agents with known ground truth (Bayes vs. biased) 2. Fit Q-learning models via maximum likelihood to extract learning rate parameters 3. Compute theoretical ⟨K⟩t using master equation framework with Taylor approximation (Eq. 20) 4. Compare fitted parameters and predicted switching rates across models

- **Design tradeoffs**: Constant vs. time-varying α: Constant simplifies fitting but misrepresents Bayesian learning; time-varying is principled but increases parameter space. Model complexity: Full 5-parameter model fits better (lower NLL) but BIC penalizes complexity; Bayesian model (1 df) often wins on BIC. Analytical vs. simulation: Master equation enables closed-form insights but requires Taylor approximations; simulation is exact but obscures mechanisms

- **Failure signatures**: Asymmetric best-fit learning rates emerging from known-unbiased Bayes agents (indicator of misspecification). Unbiased constant-rate models unable to match human ⟨K⟩t dynamics (underfitting). Q-values diverging to extremes (0 or 1) rather than converging to true p values (suggests confirmation bias, not Bayes)

- **First 3 experiments**: 1. Reproduction test: Generate data from Bayes-optimal agents with counterfactual feedback; fit Eq. 1 Q-learning; verify spurious asymmetry in recovered α parameters matching Fig. 1(a). 2. Model comparison validation: Fit all four model classes to human data from Palminteri et al. (2017); compute BIC scores; check if Bayes model wins on complexity-adjusted metric. 3. Distinguishing experiment: Implement the proposed protocol from Section 3.2—introduce a third arm with p3 between Bayesian and Q-learning predicted Q-values; measure choice probability to discriminate Bayes vs. biased agents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed "new arm" experimental protocol effectively distinguish between Bayesian inference and biased Q-learning in human subjects?
- Basis in paper: [explicit] The authors propose introducing a new arm with reward probability between the Bayesian and Q-learning predicted Q-values, stating this "might help distinguish between Bayesian and Q-learning agents" (Section 3.2)
- Why unresolved: This is a theoretical proposal that has not been empirically tested with human participants
- What evidence would resolve it: Running the proposed experiment with human subjects and analyzing whether choice patterns match Bayesian predictions (converging to true reward rates) or biased Q-learning predictions (polarized Q-values)

### Open Question 2
- Question: Do the relationships between Bayesian inference and Q-learning generalize beyond Bernoulli bandits to broader settings?
- Basis in paper: [explicit] Footnote 5 states: "More general relationships between Bayesian inference and Q-learning in broader settings remain an open question"
- Why unresolved: The formal mapping was derived specifically for Bernoulli bandits; the mathematical correspondence may not hold for Gaussian bandits, non-stationary environments, or other reward distributions
- What evidence would resolve it: Analytical derivation or simulation studies showing whether similar bias artifacts emerge when fitting Q-learning to Bayesian agents in other bandit variants

### Open Question 3
- Question: Are human decision-making behaviors better described by hybrid models combining both cognitive biases and temporally decreasing learning rates?
- Basis in paper: [inferred] The authors state "behavior may be better described via hybrid models which have a combination of bias and temporally decreasing learning rates. Therefore, further investigation would be needed" (Section 3.1)
- Why unresolved: Model comparison showed both Bayesian and biased models fit well, but the paper did not test hybrid models with both features simultaneously
- What evidence would resolve it: Fitting hybrid models with asymmetric and decreasing learning rates to human data and comparing model evidence against pure Bayesian and pure biased models

## Limitations

- Data accessibility: Direct validation of model fitting on human data depends on obtaining Palminteri et al. (2017) raw behavioral data, which was not provided
- Model misspecification scope: Analysis focuses on Bernoulli bandits with counterfactual feedback; extension to other reward distributions requires separate validation
- Analytical approximations: Master equation approach relies on Taylor approximations that assume small variances in Q-values

## Confidence

- **High confidence**: The theoretical derivation that Bayesian inference maps to Q-learning with symmetric decreasing rates (α_t = 1/(t+3)) is mathematically sound and well-supported by the conjugate prior structure
- **Medium confidence**: The behavioral equivalence between confirmation bias and decreasing learning rates is demonstrated analytically and through simulations, but requires independent replication with real human data
- **Medium confidence**: The proposed experimental protocol for distinguishing true bias from decreasing rates is conceptually sound, but its practical feasibility and sensitivity require empirical validation

## Next Checks

1. **Independent replication**: Fit the four model classes (Bayes, Full Q, Unbiased, Confirmation) to the original Palminteri et al. (2017) human data and verify that BIC scores consistently favor the Bayesian model when controlling for complexity

2. **Parameter recovery test**: Using synthetic data from known-biased agents, verify that the proposed experimental protocol (three-arm design) successfully discriminates between true confirmation bias and Bayesian inference across varying levels of decision noise and task difficulty

3. **Generalization test**: Extend the analysis to non-Bernoulli reward distributions (e.g., Gaussian bandits) and verify whether the decreasing-rate equivalence to Bayesian inference holds, or whether confirmation bias signatures remain distinguishable in these settings