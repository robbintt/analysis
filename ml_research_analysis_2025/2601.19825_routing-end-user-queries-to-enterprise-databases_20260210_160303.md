---
ver: rpa2
title: Routing End User Queries to Enterprise Databases
arxiv_id: '2601.19825'
source_url: https://arxiv.org/abs/2601.19825
tags:
- query
- schema
- queries
- score
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a more realistic benchmark for routing natural
  language queries to enterprise databases by extending existing text-to-SQL datasets
  (Spider and BirdSQL). The authors construct a unified repository of 206 and 80 databases,
  respectively, and perform a 50-50% random split of queries within each database.
---

# Routing End User Queries to Enterprise Databases

## Quick Facts
- arXiv ID: 2601.19825
- Source URL: https://arxiv.org/abs/2601.19825
- Authors: Saikrishna Sudarshan; Tanay Kulkarni; Manasi Patwardhan; Lovekesh Vig; Ashwin Srinivasan; Tanmay Tulsidas Verlekar
- Reference count: 8
- Primary Result: Novel training-free method achieves 79.62% recall@1 on BirdRoute vs 71.20% baseline

## Executive Summary
This paper addresses the challenge of routing natural language queries to the correct enterprise databases from a large repository. The authors extend existing text-to-SQL datasets (Spider and BirdSQL) to create more realistic benchmarks (BirdRoute and SpiderRoute) that better reflect enterprise scenarios with multiple similar databases. They propose a training-free routing method that combines schema entity recognition with large language models, achieving state-of-the-art performance on both benchmarks. The approach demonstrates particular effectiveness in challenging scenarios involving closely similar databases.

## Method Summary
The paper introduces a training-free routing method that combines schema entity recognition with large language models (LLMs) to route natural language queries to appropriate enterprise databases. The method uses a modular, reasoning-driven re-ranking strategy that explicitly models three key aspects: schema coverage (how well a database schema matches query entities), structural connectivity (relationships between schema elements), and fine-grained semantic alignment (deeper semantic understanding of query intent). The approach operates without requiring training on routing-specific data, instead leveraging pre-trained LLMs for semantic understanding while using schema-based features for precise matching.

## Key Results
- BirdRoute dataset: 79.62% recall@1 compared to 71.20% best baseline
- SpiderRoute dataset: 78.65% recall@1 compared to 67.99% best baseline
- Training-free approach eliminates need for dataset-specific fine-tuning
- Demonstrates effectiveness particularly in scenarios with closely similar databases

## Why This Works (Mechanism)
The method succeeds by combining multiple complementary signals for database selection. Schema entity recognition provides precise matching of query terms to database elements, while LLM-based semantic understanding captures nuanced query intent beyond exact keyword matching. The modular re-ranking framework allows weighted combination of these signals, with explicit modeling of schema coverage, structural relationships, and semantic alignment. This multi-faceted approach is particularly effective for distinguishing between similar databases where single-signal approaches might fail.

## Foundational Learning
- Schema Entity Recognition: Identifies database schema elements mentioned in queries; needed for precise structural matching, quick check: verify entity extraction accuracy on sample queries
- Semantic Alignment: Measures semantic similarity between query intent and database capabilities; needed for capturing intent beyond keywords, quick check: test semantic similarity scoring on diverse query pairs
- Structural Connectivity Analysis: Evaluates relationships between schema elements; needed for understanding complex query requirements, quick check: validate relationship mapping on multi-table queries
- LLM-based Reasoning: Provides flexible semantic understanding without task-specific training; needed for handling diverse query patterns, quick check: test reasoning consistency across similar queries
- Modular Re-ranking: Combines multiple ranking signals with learned weights; needed for balancing different matching criteria, quick check: analyze ranking stability with varying weights
- Database Repository Management: Handles large collections of heterogeneous schemas; needed for scalability in enterprise environments, quick check: test query routing speed with increasing database count

## Architecture Onboarding

**Component Map:** Query -> Schema Entity Recognition -> Structural Connectivity -> Semantic Alignment -> Re-ranking -> Database Selection

**Critical Path:** The critical path flows from query input through schema entity recognition to structural connectivity analysis, then to semantic alignment scoring, and finally through the re-ranking module to produce the final database selection. Each component builds on the previous one, with the re-ranking module synthesizing all signals for the final decision.

**Design Tradeoffs:** The training-free approach sacrifices potential performance gains from task-specific fine-tuning in exchange for flexibility and reduced data requirements. The modular design allows component-level optimization but may introduce integration complexity. The use of LLMs provides strong semantic understanding but depends on model availability and cost considerations.

**Failure Signatures:** Performance degradation may occur when queries contain ambiguous terminology not present in schemas, when databases have highly similar structures making differentiation difficult, or when LLM semantic understanding is inconsistent across similar queries. The method may also struggle with queries that span multiple databases or require complex multi-step reasoning.

**3 First Experiments:**
1. Test routing accuracy on queries with varying levels of schema specificity to understand the impact of schema entity recognition
2. Evaluate performance degradation when removing each re-ranking component individually to quantify their contributions
3. Measure routing consistency across multiple LLM model versions to assess dependency on specific model capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Real-world applicability across diverse enterprise environments remains untested
- 50-50% random split within databases may not capture real query distribution complexity
- Heavy reliance on LLM capabilities that may vary across different model versions and providers
- Performance claims limited by evaluation on only two adapted datasets

## Confidence

**Methodological Framework (High):** The modular re-ranking strategy with explicit schema coverage and semantic alignment components is well-justified and reproducible. The technical implementation appears sound.

**Dataset Construction (Medium):** While the unified repository creation is valuable, the sampling methodology and representativeness across enterprise domains could benefit from more rigorous validation.

**Performance Claims (Medium):** The reported improvements over baselines are substantial, but the evaluation on only two adapted datasets limits generalizability claims.

## Next Checks
1. Conduct ablation studies on each component of the re-ranking framework (schema coverage, structural connectivity, semantic alignment) to quantify their individual contributions to the final performance.

2. Test the routing system on at least two additional real-world enterprise database environments with different domain characteristics to assess robustness beyond the academic datasets.

3. Evaluate performance degradation under varying query distributions and database sizes to establish practical deployment boundaries and resource requirements.