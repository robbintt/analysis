---
ver: rpa2
title: Algorithm for Automatic Legislative Text Consolidation
arxiv_id: '2501.16794'
source_url: https://arxiv.org/abs/2501.16794
tags:
- article
- consolidation
- dataset
- text
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a generative method to automate legislative
  text consolidation, demonstrating a significant capability to process and automatically
  apply changes to legislative texts. The authors determine that the quality of the
  dataset and the size of the pre-trained model are two parameters that most significantly
  influence consolidation performance.
---

# Algorithm for Automatic Legislative Text Consolidation

## Quick Facts
- arXiv ID: 2501.16794
- Source URL: https://arxiv.org/abs/2501.16794
- Reference count: 2
- This study introduces a generative method to automate legislative text consolidation, demonstrating significant capability to process and automatically apply changes to legislative texts.

## Executive Summary
This study introduces a generative method to automate legislative text consolidation by applying modification instructions to existing articles to produce updated legal texts. The approach employs quantized generative models fine-tuned with LoRA, demonstrating that dataset quality and model size are the two most significant parameters influencing performance. Despite exceptional performance from GPT-4, the authors prefer open-source models for handling sensitive legal data. The consolidation method proved highly effective on real-time legislative bills, though occasional generation issues could result in nonsensical consolidations.

## Method Summary
The method employs light quantized generative models fine-tuned with LoRA, targeting projection layers (query, key, value) with approximately 3% of parameters. The approach uses a prompt format with Instruction, Input, and Response fields, training exclusively on the Response field to achieve superior performance. Dataset curation focuses on meaningful consolidations by removing non-modification cases and table-based consolidations. The pipeline includes section splitting, entity recognition, article retrieval from Légifrance, and post-processing for special character normalization.

## Key Results
- Open-source models handle only 49.8% of cases due to prompt length limitations vs GPT-4's 91.3% coverage
- Dataset curation from 3,124 to 1,784 triplets improved average WER from 17.0% to 12.0% and median WER from 7.0% to 4.0%
- OpenLLaMA-13B achieved 63.2% correctness rate on valid consolidation cases
- GPT-4 significantly outperformed open-source models on real-time legislative consolidation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA fine-tuning with quantization enables cost-effective adaptation of generative models for legislative consolidation tasks.
- Mechanism: Low-rank matrices are added to projection layers (query, key, value) of the pretrained model, targeting ~3% of original parameters. 4-bit quantization reduces memory requirements, allowing training on consumer-grade GPUs (T4 16GB, A10G 24GB). The pretrained weights retain legal language understanding while low-rank adaptations specialize for consolidation.
- Core assumption: Consolidation requires limited task-specific adaptation rather than fundamental retraining of language capabilities.
- Evidence anchors: [abstract] "Our method employs light quantized generative model, fine-tuned with LoRA"; [section 4.2.1] "The LoRA technique was applied to the projection layers... targeting approximately 3% of the parameters"

### Mechanism 2
- Claim: Training only on the target output (modified article) rather than the full prompt yields superior consolidation performance.
- Mechanism: By masking loss computation to the Response field only, the model avoids learning to reproduce the modification instruction and existing article, focusing capacity on the transformation task. This reduces average word error from 18.6% to 17.0% and median from 10.5% to 7.0%.
- Core assumption: The model's pretrained capabilities are sufficient for comprehension without additional supervised reinforcement of input understanding.
- Evidence anchors: [section 4.2.2] "training a model exclusively on the Response field yields superior performance, of +9.4%"

### Mechanism 3
- Claim: Dataset curation focusing on meaningful consolidations improves performance more than dataset size expansion.
- Mechanism: Removing non-modification cases and table-based consolidations from 3124 to 1784 triplets reduced average word error from 17.0% to 12.0% and median from 7.0% to 4.0%. The model learns cleaner patterns without noise from edge cases.
- Core assumption: Noise in training data (unmodified articles, tables) introduces conflicting gradients that degrade consolidation learning.
- Evidence anchors: [section 4.2.3] Table 2 shows curated dataset outperforms full dataset; "underscores the significance of dataset quality in influencing model performance"

## Foundational Learning

- Concept: **Legislative text structure (articles, sections, modification types)**
  - Why needed here: The pipeline relies on hierarchical parsing (bills → articles → sections → modifications). Without understanding deletion/addition/substitution patterns, you cannot design preprocessing or interpret errors.
  - Quick check question: Given a modification that says "replace '0.125%' with '0.094%'", what is the modification type and what spans must the model identify?

- Concept: **LoRA (Low-Rank Adaptation) mechanics**
  - Why needed here: This is the core fine-tuning method. Understanding rank r and multiplier α hyperparameters is essential for tuning performance and avoiding under/over-parameterization.
  - Quick check question: If increasing rank r from 16 to 64 yields minimal improvement (Table 3: 12.0% vs 11.7% WER), what does this suggest about task complexity relative to model capacity?

- Concept: **Word Error Rate (WER) for generation evaluation**
  - Why needed here: The paper uses WER as the primary metric rather than accuracy or F1. Understanding how WER penalizes insertions, deletions, and substitutions in legal text is critical for interpreting results.
  - Quick check question: Why might median WER (0.5% for 7B) be lower than average WER (13.5%) in Table 4, and what failure mode does this gap indicate?

## Architecture Onboarding

- Component map: Regex-based section splitting → Entity recognition → Article retrieval from Légifrance → OpenLLaMA model with LoRA fine-tuning → Special character normalization

- Critical path:
  1. Regex-based section splitting is fragile; errors here cascade to all downstream components
  2. Entity recognition (82% success rate) gates which consolidations are attempted
  3. Prompt length filter (1024 tokens for OpenLLaMA) excludes 50.2% of cases—context window is the primary bottleneck for open-source deployment
  4. Model inference → correctness evaluation (63.2% for OpenLLaMA-13B on valid cases)

- Design tradeoffs:
  - **Open-source vs. GPT-4**: OpenLLaMA handles only 49.8% of cases (prompt length) vs. GPT-4's 91.3%, but enables on-premise deployment for sensitive legal data
  - **Model size vs. hallucination risk**: 7B hallucinates more than 3B on complex cases (higher generative capacity without sufficient grounding); 13B resolves this with better performance
  - **Rank r selection**: r=64 trains faster but converges similarly to r=16; minimal performance gain suggests task is not highly complex

- Failure signatures:
  - **Hallucination**: 7B model generates interpretations rather than consolidations, producing lengthy nonsensical texts (Section 4.2.5)
  - **Attention degradation**: Performance drops for prompts >1000 tokens in OpenLLaMA (Figure 5)
  - **Entity recognition failure**: 18% of cases fail at article identification, preventing consolidation attempt
  - **Table handling**: Both models fail on table-based modifications (8.7% of GPT-4 cases)

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train CamemBERT span extraction model (110M params) vs. OpenLLaMA-3B with LoRA on the curated dataset. Verify ~3x WER reduction (36.2% → 12.0%) on the simple test set before proceeding.
  2. **Ablate training objective**: Compare full-prompt vs. response-only training on a held-out validation set. Expect ~9% relative WER improvement. If improvement is absent, inspect data quality for instruction complexity.
  3. **Probe context window limits**: Sample 50 consolidations with prompt lengths 800-1200 tokens. Plot correctness vs. length to identify your deployment's effective context ceiling before investing in longer-context models (e.g., LLaMA 3.1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Mixture of LoRA Experts fine-tuning technique yield significantly higher accuracy or efficiency than standard LoRA for legislative consolidation?
- Basis in paper: [explicit] The conclusion explicitly states an objective to delve into innovative fine-tuning techniques, specifically naming the "Mixture of LoRA Experts approach."
- Why unresolved: The paper only experiments with standard LoRA adaptation; the Mixture of Experts architecture is proposed as future work but not yet tested.
- What evidence would resolve it: Comparative benchmarks showing Word Error Rates and correctness rates for models fine-tuned using Mixture of LoRA Experts versus the standard LoRA implementation described in Section 4.2.1.

### Open Question 2
- Question: To what extent do larger context windows in open-source models (e.g., LLaMA 3.1) bridge the performance gap with proprietary models like GPT-4 on lengthy legislative texts?
- Basis in paper: [explicit] The authors note in the conclusion that exploring models like LLaMA 3.1 offers new possibilities specifically because they "feature larger context windows, enabling the consolidation of more samples."
- Why unresolved: The experiments revealed that the limited context size of OpenLLaMa-13b restricted it to only 49.8% of possible consolidations, whereas GPT-4 handled 91.3%. It is unproven if simply increasing context in an open-source model matches GPT-4's performance.
- What evidence would resolve it: A comparison of "correctness rates" and "rate of possible consolidations" between GPT-4 and an open-source model with a larger context window (e.g., >8k tokens) on the same lengthy bill samples.

### Open Question 3
- Question: Can specific decoding strategies or constraints effectively mitigate the "hallucinations" and "nonsensical consolidations" produced by generative models without reducing valid text generation?
- Basis in paper: [inferred] The paper notes that while generative models ensure grammatical correctness, they occasionally result in "aberrations" and "nonsensical consolidations" (hallucinations), particularly noting the 7b model generated interpretations rather than strict consolidations.
- Why unresolved: The current study relies on standard generative fine-tuning, accepting hallucinations as a trade-off for fluency over the span-extraction baseline. No specific method to restrain these hallucinations is tested.
- What evidence would resolve it: Experiments utilizing constrained decoding or factual consistency metrics on the generated output, measuring a reduction in hallucination rates while maintaining the Word Error improvements seen in Section 4.3.

## Limitations
- The evaluation relies entirely on automatic metrics (WER) rather than human assessment of legal correctness
- The curated dataset excludes 42% of original cases due to no-modification or table-based consolidations, potentially limiting real-world applicability
- The 1,024 token context window excludes nearly half of real consolidation cases, creating a deployment gap between benchmark performance and practical utility

## Confidence
- **High confidence**: LoRA fine-tuning with quantization enables cost-effective adaptation; training only on target output yields superior performance; dataset curation focusing on meaningful consolidations improves performance; GPT-4 significantly outperforms open-source models
- **Medium confidence**: Dataset quality and model size are the two most significant parameters; open-source models can handle sensitive legal data while maintaining competitive performance on simpler cases; hallucination in 7B model represents fundamental limitation
- **Low confidence**: Legal sensitivity concerns justify preference for open-source models over GPT-4; the consolidation method generalizes to other legal domains; WER remains appropriate metric for legal text consolidation

## Next Checks
1. **Human evaluation study**: Conduct blind legal expert review of 50 consolidations from the harder test set, comparing OpenLLaMA-13B outputs to ground truth. Measure inter-annotator agreement and assess whether WER correlates with legal validity.

2. **Context window stress test**: Systematically evaluate model performance across the full range of prompt lengths (400-2000 tokens) using the complete dataset. Plot correctness rate against token count to quantify the exact deployment threshold where performance degrades.

3. **Legal domain generalization experiment**: Apply the best-performing consolidation pipeline to a different legal corpus (e.g., US federal regulations or EU directives) with modifications expressed in a different legislative style. Compare performance drop to the French parliamentary baseline.