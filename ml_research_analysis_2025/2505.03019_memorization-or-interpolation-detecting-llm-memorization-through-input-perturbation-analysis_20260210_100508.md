---
ver: rpa2
title: Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation
  Analysis
arxiv_id: '2505.03019'
source_url: https://arxiv.org/abs/2505.03019
tags:
- memorization
- data
- training
- humaneval
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEARL, a framework that detects memorization
  in Large Language Models (LLMs) by measuring sensitivity to input perturbations.
  The core idea is that memorized content exhibits high sensitivity to small input
  changes, leading to drastic performance drops, while generalization maintains stable
  performance.
---

# Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis

## Quick Facts
- arXiv ID: 2505.03019
- Source URL: https://arxiv.org/abs/2505.03019
- Reference count: 16
- Primary result: Introduces PEARL framework that detects LLM memorization through input perturbation sensitivity analysis

## Executive Summary
This paper introduces PEARL, a framework that detects memorization in Large Language Models (LLMs) by measuring sensitivity to input perturbations. The core idea is that memorized content exhibits high sensitivity to small input changes, leading to drastic performance drops, while generalization maintains stable performance. PEARL generates perturbed versions of inputs, evaluates LLM performance using metrics like Normalized Compression Distance (NCD) or ROUGE-L, and quantifies sensitivity by comparing performance across perturbation intensities. Applied to GPT-4o and Pythia models, PEARL successfully identified memorization in datasets like HumanEval (60 instances), the Bible (42 instances), and NY Times articles (5 instances), with sensitivity thresholds controlling false positives. The approach validated PSH as a reliable memorization indicator, particularly for tasks requiring exact content reproduction, and demonstrated that memorization is task-dependent and more prevalent in unique content. PEARL offers a black-box method for assessing memorization risks without requiring access to model internals or training data.

## Method Summary
PEARL detects memorization by measuring how LLM performance degrades under controlled input perturbations. The framework applies bit-flip perturbations at increasing intensities (0-5% of bits flipped) to input text, generates outputs from the LLM at each perturbation level, and computes similarity metrics (NCD for completion, ROUGE-L for summarization) between outputs and reference targets. Memorization is quantified as the maximum consecutive performance drop across perturbation intensities - memorized content shows sharp drops while generalized content shows gradual decline. The method requires calibration on known non-training data to set sensitivity thresholds and works as a black-box assessment without needing model internals or training data access.

## Key Results
- PEARL successfully detected memorization in Pythia models fine-tuned on Pile training data, with detection rates increasing with training epochs
- Applied to GPT-4o, PEARL identified 60 memorized instances in HumanEval, 42 in Bible, and 5 in NY Times articles
- Memorization detection is task-dependent: completion tasks showed 6x more memorized instances than summarization for the same inputs
- RefinedWeb (non-training data) showed ~2% false positive memorization detection, validating the approach's specificity
- Larger models showed higher memorization detection rates, suggesting scale-dependent memorization behavior

## Why This Works (Mechanism)

### Mechanism 1: Perturbation Sensitivity Hypothesis (PSH)
- Claim: Memorized inputs exhibit abrupt performance degradation under perturbation while generalized inputs show gradual decline
- Mechanism: Memorization creates rigid input-output mappings tied to exact token sequences. When perturbations break this sequence, the model cannot rely on recalled training data and performance drops sharply. Generalized knowledge forms flexible abstractions that tolerate input variation
- Core assumption: Memorized representations are "fragile" neural pathways; generalized representations encode underlying patterns rather than surface tokens
- Evidence anchors:
  - [abstract]: "memorized content exhibits high sensitivity to small input changes, leading to drastic performance drops, while generalization maintains stable performance"
  - [section 2, Figure 1]: Harry Potter text (presumed memorized) fails completion under perturbation while Wikipedia text (generalized) succeeds
  - [corpus]: "Memorize or Generalize? Evaluating LLM Code Generation" explores similar memorization vs. generalization tension in code tasks
- Break condition: Tasks where input context suffices (summarization) weaken detection; Section 4.5 shows only 6 of 107 memorized instances overlapped between completion and summarization tasks

### Mechanism 2: Bit-flip Perturbation Injection
- Claim: Flipping k bits in binary-encoded text creates controlled corruptions that disproportionately affect memorized sequences
- Mechanism: Input text → tokenizer encode → binary representation → random k bit flips → decode → perturbed input. This introduces character-level noise while preserving approximate semantic structure
- Core assumption: Memorized sequences depend on exact token patterns; bit-flips break these patterns more disruptively than they affect learned abstractions
- Evidence anchors:
  - [section 3.1]: "The function thus consists in flipping k bits in the binary version of the input text and then decoding it back"
  - [section 4, Table 1]: k ∈ {0, 1, 2, 3, 4, 5}% perturbation intensities used across experiments
  - [corpus]: No direct corpus evidence on bit-flip approach; limited external validation
- Break condition: Excessive perturbation rates create unintelligible inputs; α threshold requires calibration per model/task

### Mechanism 3: Maximum Performance Falloff Quantification
- Claim: The maximum consecutive performance drop across perturbation intensities reliably discriminates memorization from generalization
- Mechanism: Generate outputs at each perturbation intensity k, compute distance metric (NCD/ROUGE-L) to reference, calculate sensitivity = max(m(Yⱼ*) − m(Yⱼ₊₁*)). Classify as memorized if sensitivity > α
- Core assumption: Memorized content produces step-function performance curves; generalized content produces gradual slopes
- Evidence anchors:
  - [section 3.3]: Equation 3 defines sensitivity as maximum falloff between consecutive perturbation levels
  - [section 4.2, Figure 7]: Pile (training data) shows >20% memorized instances at epoch 10; RefineWeb (non-training) shows ~2%
  - [corpus]: "The Landscape of Memorization in LLMs" confirms measurement approaches are actively researched
- Break condition: Requires calibration data known to be outside training; FPR varies by model size (Table 2 shows larger models have more detected memorization)

## Foundational Learning

- Concept: **Membership Inference**
  - Why needed here: PEARL is a black-box membership inference technique—determining whether specific data appeared in training without model internals
  - Quick check question: Why is detecting "this data was trained on" different from detecting "this data was memorized"?

- Concept: **Normalized Compression Distance (NCD)**
  - Why needed here: Primary metric for completion tasks; measures output similarity via compressibility, sensitive to verbatim reproduction
  - Quick check question: Why would compression-based distance detect exact memorization better than semantic similarity?

- Concept: **Overfitting vs. Memorization**
  - Why needed here: Paper fine-tunes Pythia with multiple epochs to induce memorization; understanding this relationship is critical for interpreting results
  - Quick check question: Can a model generalize well overall yet still memorize specific training examples?

## Architecture Onboarding

- **Component map:**
  - Input → Perturbation Generator → {X₀*, X₁*, ..., X₅*} at k = 0-5% → LLM (10 samples each) → Metric Calculator → Distance scores → Sensitivity Analyzer → Max falloff → Classifier → Memorized/Not Memorized

- **Critical path:**
  1. Input X → Perturbation Generator → {X₀*, X₁*, ..., X₅*} at k = 0-5%
  2. Each Xₖ* → LLM → i=10 output samples Yₖ*
  3. All outputs → Metric Calculator → distance scores m(Yₖ*)
  4. Scores → Sensitivity Analyzer → max falloff value
  5. Sensitivity → Classifier → memorized / not memorized

- **Design tradeoffs:**
  - Higher α: Fewer false positives but misses weak memorization
  - More k values: Better sensitivity detection; 6× API cost per input
  - More samples (i): Statistical stability; 10× API calls
  - Task selection: Completion outperforms summarization for detection (Figure 9)

- **Failure signatures:**
  - High FPR on non-training data → α too low
  - No detection in known training data → perturbation range misconfigured
  - Noisy sensitivity from single runs → increase i for statistical stability
  - Task mismatch: Summarization yields near-zero sensitivity differences

- **First 3 experiments:**
  1. Calibrate α on RefinedWeb (not in Pythia training) targeting FPR < 0.05 (Section 4.2, Figure 6)
  2. Fine-tune Pythia on Pile subset for 10 epochs; verify memorization detection increases with epochs (Figure 7 pattern)
  3. Run identical Bible/HumanEval samples through completion and summarization on GPT-4o; confirm completion shows higher sensitivity variance (Figure 9)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the sensitivity threshold (α) be calibrated for closed-source models when verified data outside the training set is unavailable?
- Basis in paper: [explicit] Section 5.2 states that calibration requires data known to be outside the training set, which is "challenging for closed-source models where training data information is unavailable."
- Why unresolved: The current methodology relies on the assumption that benchmark datasets (like LBPP) are clean negatives, which may not hold for proprietary models with unknown training corpora
- What evidence would resolve it: A theoretical or heuristic method for setting α using only model outputs or internal confidence scores, without requiring an external "unseen" dataset

### Open Question 2
- Question: To what extent do different perturbation functions (e.g., character swaps vs. semantic paraphrasing) affect detection reliability and bias?
- Basis in paper: [inferred] Section 5.3 notes that "perturbation techniques could introduce unintended biases," and Section 3.1 limits the study to a single bit-flip function
- Why unresolved: The paper demonstrates the efficacy of bit-flips but does not explore if other perturbation types might trigger different memorization retrieval behaviors or yield better signal-to-noise ratios
- What evidence would resolve it: A comparative analysis of PEARL's performance using various perturbation strategies on the same memorized and non-memorized datasets

### Open Question 3
- Question: Can PEARL be adapted to detect memorization in tasks where output variance is inherent, such as summarization or translation?
- Basis in paper: [explicit] Section 5.2 highlights that effectiveness varies by task, noting that "perturbation sensitivity may not be equally revealing of memorization across all model capabilities" like summarization
- Why unresolved: The Perturbation Sensitivity Hypothesis relies on performance drops measured by similarity metrics, but valid summarizations of perturbed inputs naturally diverge, making "falloff" difficult to attribute to memorization loss
- What evidence would resolve it: The development of a specialized metric or prompting strategy that distinguishes between valid generalization changes and memorization loss in semantic tasks

## Limitations

- Task-dependent detection: PEARL works best for completion tasks requiring exact reproduction; summarization shows minimal sensitivity differences, limiting detection capability
- Calibration requirement: Method requires known non-training data to set sensitivity thresholds, which is challenging for closed-source models with unknown training corpora
- Perturbation specificity: Relies on bit-flip perturbation without exploring whether alternative perturbation strategies might yield better detection accuracy or different biases

## Confidence

**High Confidence**: The core observation that memorized content exhibits higher sensitivity to perturbations than generalized content (validated through controlled Pythia fine-tuning experiments showing epoch-dependent memorization detection). The task-dependent nature of PSH is well-supported (completion tasks detect more memorization than summarization).

**Medium Confidence**: The effectiveness of bit-flip perturbation as a general detection mechanism (limited external validation beyond this paper's controlled experiments). The NCD metric's superiority for memorization detection over semantic similarity measures.

**Low Confidence**: The assumption that bit-flip perturbation creates meaningful, controlled corruption that uniformly affects memorized vs. generalized content. The generalizability of α thresholds across different model architectures and data domains without recalibration.

## Next Checks

1. **Cross-domain perturbation validation**: Apply PEARL to models trained on diverse data types (images, audio, structured data) to verify that bit-flip perturbation consistently discriminates memorization across modalities, not just text.

2. **Alternative perturbation comparison**: Implement and compare PEARL with alternative perturbation methods (synonym replacement, syntactic transformation, adversarial noise) to determine if bit-flip perturbation is optimal or if task-specific perturbation strategies yield better detection accuracy.

3. **Semantic preservation analysis**: For detected memorized instances, analyze whether the original training data could be recovered through semantic completion rather than exact reproduction, testing the assumption that memorization always manifests as rigid token-level recall rather than flexible pattern learning.