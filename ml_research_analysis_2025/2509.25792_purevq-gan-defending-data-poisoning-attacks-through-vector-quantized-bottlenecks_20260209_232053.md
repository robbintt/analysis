---
ver: rpa2
title: 'PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks'
arxiv_id: '2509.25792'
source_url: https://arxiv.org/abs/2509.25792
tags:
- purevq-gan
- training
- defense
- data
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PUREVQ-GAN, a defense against data poisoning
  attacks that leverages the discrete bottleneck of Vector-Quantized Variational Autoencoders
  combined with adversarial training. The key insight is that forcing representations
  through a finite codebook of learned prototypes destroys adversarial perturbations
  while preserving semantic content, with the GAN discriminator ensuring outputs match
  the natural image distribution.
---

# PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks

## Quick Facts
- **arXiv ID**: 2509.25792
- **Source URL**: https://arxiv.org/abs/2509.25792
- **Reference count**: 20
- **Primary result**: Achieves 0% poison success rate against Gradient Matching and Bullseye Polytope attacks on CIFAR-10 while maintaining 91-95% clean accuracy

## Executive Summary
PUREVQ-GAN introduces a novel defense against clean-label data poisoning attacks by leveraging the discrete bottleneck of Vector-Quantized Variational Autoencoders. The key insight is that forcing representations through a finite codebook of learned prototypes destroys adversarial perturbations while preserving semantic content. Combined with adversarial training via a GAN discriminator, the method ensures outputs match the natural image distribution. On CIFAR-10, PUREVQ-GAN achieves 0% poison success rate against Gradient Matching and Bullseye Polytope attacks, and 1.64% against Narcissus while maintaining 91-95% clean accuracy.

## Method Summary
PUREVQ-GAN defends against data poisoning by purifying poisoned samples before training the target classifier. The method trains a VQ-GAN (encoder → 8×8×256 latents → codebook K=512 → decoder) with adversarial discriminator on poisoned CIFAR-10. The VQ-GAN learns to map poisoned inputs to their natural image distribution through discrete quantization. During purification, poisoned samples are passed through the trained VQ-GAN in a single forward pass. The purified samples are then used to train a standard classifier. The approach combines reconstruction loss, vector quantization loss (β=0.25), and adversarial loss (λ=0.1) to learn meaningful prototypes that preserve semantic content while removing adversarial perturbations.

## Key Results
- 0% poison success rate against Gradient Matching and Bullseye Polytope attacks
- 1.64% poison success rate against Narcissus attack
- 91-95% clean accuracy maintenance on CIFAR-10
- Over 50× faster than diffusion-based defenses, requiring only single forward pass for purification

## Why This Works (Mechanism)
The discrete bottleneck of VQ-VAEs forces representations into a finite codebook of learned prototypes. Adversarial perturbations in poisoned images are destroyed during this quantization process because they cannot be represented by the learned prototypes that capture natural image statistics. The GAN discriminator ensures that reconstructed outputs match the natural image distribution, preventing the model from simply memorizing poisoned patterns. This combination effectively separates semantic content from adversarial noise, resulting in purified images that fool the poison generation process while maintaining classification accuracy.

## Foundational Learning
- **Vector Quantization**: Discrete representation learning through codebook lookup; needed to destroy adversarial perturbations; quick check: monitor codebook usage diversity
- **GAN Training**: Adversarial learning to match data distribution; needed to ensure purified outputs look natural; quick check: evaluate discriminator loss convergence
- **Poison Generation**: Creating poisoned samples that appear natural; needed to evaluate defense effectiveness; quick check: verify PSR of poisoned samples on untrained classifier
- **Adversarial Examples**: Perturbations that fool classifiers; needed to understand attack mechanism; quick check: visualize perturbations with ε budget
- **Clean-Label Poisoning**: Attacks that use only clean-appearing samples; needed to understand threat model; quick check: confirm poison samples appear natural to human inspection

## Architecture Onboarding
- **Component map**: Input → Encoder → Discrete latents → Codebook → Decoder → Discriminator
- **Critical path**: Input → Encoder → Discrete latents → Codebook → Decoder → Classifier training
- **Design tradeoffs**: Discrete vs continuous bottleneck (faster but less expressive), single vs multiple purification passes (faster but potentially less thorough), codebook size (larger K preserves more detail but increases memory)
- **Failure signatures**: Blurry reconstructions (PSNR <40dB), codebook collapse (<100 active codes), insufficient poison removal (visual artifacts remain)
- **3 first experiments**:
  1. Train VQ-GAN on clean CIFAR-10 and evaluate reconstruction quality (PSNR, SSIM)
  2. Generate poisoned samples using GM attack and verify they achieve >90% PSR on untrained classifier
  3. Purify poisoned samples and evaluate PSR drop and reconstruction quality

## Open Questions the Paper Calls Out
None

## Limitations
- Requires complete retraining of VQ-GAN for each new dataset/task, limiting adaptability
- Performance depends on codebook size and architecture choices that are not fully specified
- Evaluation limited to CIFAR-10 with specific attack implementations and hyperparameters

## Confidence
- **High confidence**: The core VQ-GAN methodology and its theoretical basis for destroying adversarial perturbations through discrete quantization
- **Medium confidence**: The 0% PSR against GM and BP attacks on CIFAR-10, as these results are most sensitive to attack implementation details and exact hyperparameters
- **Medium confidence**: The 1.64% PSR against Narcissus attack, as this attack is more complex and the defense's effectiveness may vary with attack strength
- **Medium confidence**: The 91-95% clean accuracy maintenance, as this depends on the balance between reconstruction quality and preservation of semantic content

## Next Checks
1. **Codebook utilization analysis**: Verify that the codebook learns a diverse set of prototypes (target >300 active codes out of 512) and that reconstructions maintain PSNR >40dB to ensure semantic preservation
2. **Attack strength validation**: Reproduce the exact poison generation process for all three attacks using reported ε values and optimization hyperparameters to confirm the claimed PSR rates are achievable
3. **Scalability verification**: Test the method on a second dataset (e.g., CIFAR-100 or SVHN) to validate that the approach generalizes beyond CIFAR-10 and that the computational efficiency claims hold for larger-scale problems