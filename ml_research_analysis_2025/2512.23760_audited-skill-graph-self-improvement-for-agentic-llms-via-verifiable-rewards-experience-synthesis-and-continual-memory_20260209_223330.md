---
ver: rpa2
title: Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards,
  Experience Synthesis, and Continual Memory
arxiv_id: '2512.23760'
source_url: https://arxiv.org/abs/2512.23760
tags:
- skill
- evidence
- asg-si
- tool
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ASG-SI introduces an auditable skill-graph framework for secure\
  \ self-improvement in agentic LLMs, where each improvement is compiled into a reusable\
  \ skill with explicit interfaces and gated by verifier-backed replay and contract\
  \ checks. It decomposes rewards into reconstructible, evidence-backed components\u2014\
  tool validity, outcome verification, skill reuse, and composition integrity\u2014\
  enabling reproducible evaluation and detecting reward hacking."
---

# Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory

## Quick Facts
- arXiv ID: 2512.23760
- Source URL: https://arxiv.org/abs/2512.23760
- Reference count: 14
- Introduces auditable skill-graph framework for secure self-improvement in agentic LLMs with verifier-backed replay and contract checks

## Executive Summary
ASG-SI presents a framework where agentic LLMs improve themselves through verifiable rewards and auditable skill graphs. The system decomposes rewards into reconstructible components derived from replayable artifacts, enabling reproducible evaluation and detection of reward hacking. Skills are compiled into a graph structure with explicit interfaces and promoted only after independent verification on held-out tasks. This approach reframes self-improvement as accumulation of auditable capabilities rather than parameter updates, offering a path toward reproducible evaluation and operational governance of self-improving AI agents.

## Method Summary
The method involves a policy agent generating trajectories that are logged with tool calls and artifacts, from which a skill compiler extracts candidate skills with explicit interfaces (preconditions/postconditions). A separate verifier-auditor replays these candidates on held-out tasks, producing evidence bundles that are cryptographically logged and used to compute decomposed rewards. Only skills passing verification criteria are promoted to the audited skill graph. Progressive reward shaping uses structural validity early and correctness/composition/efficiency later. The system integrates experience synthesis for stress testing and continual memory control to prevent forgetting under task streams.

## Key Results
- Introduces auditable skill-graph framework with verifier-backed replay and contract checks
- Decomposes rewards into reconstructible, evidence-backed components for reproducible evaluation
- Demonstrates measurable improvement via verifiable artifacts and skill reuse metrics
- Integrates experience synthesis and continual memory control to prevent catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling agent execution from skill promotion via an independent verifier prevents reward hacking and ensures improvement integrity.
- **Mechanism:** The policy agent generates trajectories, but skill promotion is gated by a separate "Verifier & Auditor" service that replays candidates on held-out tasks and checks interface contracts before compilation into the skill graph.
- **Core assumption:** The verifier and evidence stores are isolated and cannot be written to by the policy runtime, and the environment permits deterministic or statistically reliable replay.
- **Evidence anchors:** [abstract] "gated by verifier-backed replay and contract checks"; [Section 5] "The verifier–auditor then evaluates candidates on held-out tasks... Only candidates that pass promotion criteria are added to the audited skill graph."

### Mechanism 2
- **Claim:** Decomposing rewards into reconstructible components derived from replayable artifacts creates resistance to opaque reward gaming.
- **Mechanism:** Instead of a single scalar reward, the system computes scores based on specific evidence (tool schemas, exact-match outcomes, hashes) that are logged cryptographically, allowing the reward signal to be independently reconstructed and audited.
- **Core assumption:** Tool outputs and environment states can be captured deterministically, and the storage mechanism is append-only/tamper-evident.
- **Evidence anchors:** [abstract] "decomposes rewards into reconstructible, evidence-backed components—tool validity, outcome verification... enabling reproducible evaluation"; [Section 6] "The verifier–auditor is responsible for emitting the artifacts needed for reconstruction... supports stronger measurement discipline."

### Mechanism 3
- **Claim:** Persisting verified skills as immutable artifacts in a graph structure mitigates catastrophic forgetting compared to pure parameter updates.
- **Mechanism:** Learned capabilities are compiled into a directed "Skill Graph" (nodes=skills, edges=contracts). Even if the base LLM policy drifts or forgets due to fine-tuning, the agent retains access to these callable skill artifacts as stable fallbacks.
- **Core assumption:** The agent's runtime can effectively retrieve and invoke graph skills when preconditions are met, and the graph does not grow unmanageably complex.
- **Evidence anchors:** [abstract] "improvement is compiled into a reusable skill... continual memory control to prevent forgetting"; [Section 11] "Verified skills persist as callable artifacts in the graph, making them available as stable fallbacks even if the base policy’s behavior drifts."

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** The system relies on rewards derived from deterministic checks (e.g., unit tests, schema validation) rather than human preference.
  - **Quick check question:** Can you distinguish between a reward derived from a unit test pass/fail vs. a reward derived from a human "thumbs up"?

- **Concept: Continual Learning & Catastrophic Forgetting**
  - **Why needed here:** The paper specifically targets the failure mode where agents lose previously learned capabilities while training on new task streams.
  - **Quick check question:** What happens to a standard neural network's performance on Task A if it is trained exclusively on Task B for 1000 steps?

- **Concept: Formal Interfaces (Preconditions/Postconditions)**
  - **Why needed here:** The safety of the skill graph depends on explicit contracts. You must understand how to define what a skill requires (input) and what it guarantees (output) to enable automated verification.
  - **Quick check question:** If a skill "Send Email" has a postcondition "Message Sent," what is a verifiable precondition regarding the recipient?

## Architecture Onboarding

- **Component map:** Policy Agent -> Trajectory Log -> Skill Compiler -> Verifier & Auditor -> Audited Skill Graph -> RL Trainer
- **Critical path:** The "Verification Boundary." You must trace how a raw trajectory becomes a promoted skill. The critical step is the **Verifier's replay**—if this is skipped or mocked, the security guarantees of the entire system collapse.
- **Design tradeoffs:**
  - **Overhead vs. Security:** Replay-based verification is computationally expensive but required for auditability.
  - **Strictness vs. Growth:** If skill interfaces are too strict, the graph never grows; if too loose, unsafe skills are promoted.
  - **Memory vs. Context:** Bounded memory prevents context explosion but risks dropping critical long-horizon dependencies.
- **Failure signatures:**
  - **Reward Hacking:** High outcome scores but low tool validity scores (e.g., the agent found a loophole to pass tests without using tools correctly).
  - **Verifier Drift:** A skill passes verification but fails in production (indicates held-out test sets are stale or non-representative).
  - **Graph Stagnation:** Skill compiler generates candidates, but Verifier rejects 100% due to strict schema requirements.
- **First 3 experiments:**
  1. **Run the Reference Implementation (asg_si_demo.py):** Execute the provided demo to observe a trajectory being compiled into a skill, verified, and then reused.
  2. **Corrupt an Evidence Bundle:** Manually modify a hash in a log file to see if the system detects tampering during the replay/audit phase.
  3. **Stress Test Skill Promotion:** Lower the pass threshold in the verifier and observe if "low quality" skills begin to degrade agent performance on complex composition tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can skill interfaces be automatically inferred to balance under-specification (skills pass tests but fail in edge cases) against over-specification (useful skills never get promoted)?
- **Basis in paper:** [explicit] Section 14 states: "These tradeoffs motivate future work on interface inference, adversarial verification suites, and principled composition typing for complex tool and environment interactions."
- **Why unresolved:** The paper proposes explicit interfaces but does not provide methods for inferring appropriate preconditions/postconditions.
- **What evidence would resolve it:** A systematic method or algorithm for interface inference, evaluated on skill promotion rates vs. edge-case failure rates.

### Open Question 2
- **Question:** How can replay-based verification remain meaningful in nondeterministic environments without prohibitively expensive controlled harnesses?
- **Basis in paper:** [explicit] Section 14 notes: "nondeterministic environments require controlled harnesses, record-and-replay mechanisms, or robust statistical testing to keep evidence meaningful."
- **Why unresolved:** The prototype uses deterministic tools; real deployments face stochasticity that undermines exact replay.
- **What evidence would resolve it:** Statistical bounds on verification confidence under controlled nondeterminism, or efficient record-and-replay mechanisms with measured fidelity.

### Open Question 3
- **Question:** How can verifiable reward regimes prevent agents from optimizing toward easily-verified but undesired behaviors?
- **Basis in paper:** [explicit] Section 14: "Verifiable rewards can also create measurement incentives that shift behavior toward what is easily verified rather than what is desired."
- **Why unresolved:** The framework assumes verifiability aligns with intent, but optimization pressure may exploit verification gaps.
- **What evidence would resolve it:** Detection methods for verification-gaming behaviors, or reward formulations robust to Goodhart-like effects under adversarial evaluation.

## Limitations

- The system's security depends critically on verifier isolation and evidence storage integrity - if these are compromised, the entire audibility guarantee fails
- Non-deterministic environments pose fundamental challenges to reliable replay-based verification
- Skill interface specification remains an under-constrained problem that could lead to brittle or unsafe skill promotion

## Confidence

- **High confidence** in the decomposition of rewards into reconstructible components - this follows established auditability principles
- **Medium confidence** in the effectiveness of independent verifier isolation for preventing reward hacking - depends heavily on implementation details
- **Medium confidence** in the skill graph's ability to prevent catastrophic forgetting - artifact persistence is promising but interface specification remains challenging

## Next Checks

1. **Verifier Integrity Test:** Implement a penetration test where an adversary attempts to manipulate evidence bundles post-hoc and verify detection rates
2. **Non-Determinism Robustness:** Deploy the system in a stochastic environment and measure verification reliability degradation
3. **Skill Interface Stress Test:** Systematically fuzz input conditions for promoted skills to identify interface specification gaps and failure modes