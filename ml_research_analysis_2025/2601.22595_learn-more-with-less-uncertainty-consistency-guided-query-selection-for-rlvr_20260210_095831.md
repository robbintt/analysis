---
ver: rpa2
title: 'Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR'
arxiv_id: '2601.22595'
source_url: https://arxiv.org/abs/2601.22595
tags:
- uncertainty
- online
- training
- samples
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the high query cost of reinforcement learning
  with verifiable rewards (RLVR) for mathematical reasoning tasks by introducing active
  learning to RLVR. It identifies that classic active learning methods fail because
  they ignore the relationship between subjective uncertainty (model perplexity) and
  objective uncertainty (correctness).
---

# Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR

## Quick Facts
- **arXiv ID**: 2601.22595
- **Source URL**: https://arxiv.org/abs/2601.22595
- **Reference count**: 30
- **Primary result**: Achieves full-dataset RLVR performance using only 30% of training data through uncertainty consistency-guided query selection

## Executive Summary
This work addresses the high query cost of reinforcement learning with verifiable rewards (RLVR) for mathematical reasoning tasks by introducing active learning to RLVR. The key insight is that classic active learning methods fail because they ignore the relationship between subjective uncertainty (model perplexity) and objective uncertainty (correctness). The authors propose an uncertainty consistency metric based on the Point-Biserial Correlation Coefficient (PBC) to measure alignment between these uncertainties. In offline settings, they select samples with lowest PBC; in online settings, they introduce an equivalent online metric derived from normalized advantages and subjective uncertainty. Experiments show their method achieves full-dataset performance while using only 30% of the data across multiple model sizes and datasets.

## Method Summary
The method introduces uncertainty consistency-guided query selection for RLVR. It computes an offline uncertainty consistency metric using Point-Biserial Correlation (PBC) between subjective uncertainty (perplexity) and objective correctness, selecting samples with lowest PBC. For practical online application, it derives an equivalent online metric from normalized advantages and subjective uncertainty, theoretically proven to be negatively correlated with offline PBC. The method maximizes sample uncertainty reduction during training, achieving comparable performance to full-dataset training while using only 30% of the data.

## Key Results
- Achieves full-dataset RLVR performance using only 30% of training data across multiple model sizes
- Classic active learning methods (PPL, Entropy, K-center, AskLLM) fail in RLVR due to ignoring objective uncertainty
- Online uncertainty consistency metric shows R² = 0.71-0.90 correlation with offline PBC across experiments
- Method generalizes across RLVR algorithms (GRPO, RLOO, DAPO, REINFORCE++)

## Why This Works (Mechanism)

### Mechanism 1: Gradient Variance Reduction via Uncertainty Consistency
Selecting samples where subjective and objective uncertainty align reduces policy gradient variance, stabilizing RLVR training. Inconsistent samples (e.g., high perplexity but correct answer) have low generation probabilities (~0.3 vs ~0.9 for consistent samples), creating larger margins for probability improvement and thus larger, more unstable gradient norms. Consistent samples yield bounded gradients.

### Mechanism 2: Online Metric as Proxy for Offline PBC
The online uncertainty consistency metric computed from normalized advantages and current model uncertainty serves as a practical proxy for the offline Point-Biserial Correlation. Because PBC requires many samples (K responses per query) and stable output distributions—both unavailable online—the authors derive an online metric theoretically proven to have strictly negative covariance with PBC.

### Mechanism 3: Sample Uncertainty Reduction Equivalence
Maximizing the online metric per minibatch is equivalent to maximizing the decrease in sample-level subjective uncertainty after one gradient step. Under orthogonality and bounded gradient assumptions, the uncertainty reduction can be decomposed to show it is bounded below by a term proportional to the online metric.

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Needed to understand the specific RL setting where binary rewards are computed from verifiable answers. Quick check: Given K=8 responses with rewards [1,0,0,0,1,0,0,0], compute the group-normalized advantage for the first response.
- **Point-Biserial Correlation Coefficient (PBC)**: Needed as the core metric for measuring alignment between continuous subjective uncertainty and binary objective correctness. Quick check: If correct responses have mean perplexity 5.0, incorrect have mean 15.0, and pooled std is 8.0 with 4 correct and 4 incorrect samples, compute r_pb.
- **Policy Gradient Variance and Entropy Dynamics**: Needed to understand the variance reduction hypothesis and explore training stability. Quick check: In early RLVR training, would you expect policy entropy to increase, decrease, or stay constant? What does rapid entropy collapse signal?

## Architecture Onboarding

- **Component map**: Reference model (frozen, computes subjective uncertainty) -> Policy model (trainable, being optimized) -> Uncertainty estimator (computes U_k^(i) per response) -> Consistency scorer (computes r_pb or r_online_pb) -> Selection filter (retains top-p% samples) -> RLVR loss (GRPO/RLOO/DAPO loss on selected samples)
- **Critical path**: Generate K responses per query, compute per-response uncertainty and rewards, calculate consistency metric, select top-p% queries, compute RLVR loss only on selected subset, update policy parameters
- **Design tradeoffs**: K (responses per query): Higher K improves estimation but increases compute (paper uses K=8 online, K=64 offline); γ in Equation 5: Balances positive vs negative response contributions (paper finds smaller γ 0.1-0.5 works better); Sample ratio p: 30% achieves near-full performance; 10% causes degradation; 50% provides marginal gains
- **Failure signatures**: Gradient norm variance remains high despite consistency selection (orthogonality assumption violated); Policy entropy drops rapidly in first few steps (samples lack diversity); Response length decreases significantly (selection biased toward easy samples); Performance plateaus below Full baseline even at 50% ratio (domain mismatch)
- **First 3 experiments**: 1) Replicate Table 1 pilot on Qwen2.5-0.5B/MATH with 10% sampling to verify classic AL fails; 2) Ablation on γ ∈ {0.1, 0.5, 1.0, 1.5, 2.0} with online selection at 30% ratio; 3) Cross-algorithm validation applying online consistency selection to GRPO, RLOO, DAPO, REINFORCE++ on same held-out test split

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain: generalization to non-mathematical reasoning domains, performance with imperfect reward signals, computational overhead trade-offs, and hyperparameter sensitivity across different tasks and model scales.

## Limitations
- Relies on critical assumptions (Sample Gradient Orthogonality and Bounded Gradient Norm) that are empirically shown but not theoretically proven across diverse architectures
- Method's effectiveness on non-mathematical reasoning domains remains untested despite claims of general applicability
- Online-offline metric correlation (R² = 0.71-0.90) suggests reasonable but imperfect correspondence

## Confidence
- **High confidence**: Classic active learning methods fail in RLVR due to ignoring objective uncertainty (Section 5, Table 1)
- **Medium confidence**: Theoretical proofs of online metric properties under stated assumptions (Theorems 1 and 2)
- **Medium confidence**: Practical effectiveness of 30% sampling achieving near-full performance across multiple datasets and model sizes (Table 2)

## Next Checks
1. Test uncertainty consistency selection on non-mathematical domains (e.g., code generation or text summarization) to verify cross-domain applicability
2. Evaluate performance sensitivity to K (responses per query) values below 8 to determine minimum requirements for reliable metric estimation
3. Implement controlled experiment comparing gradient variance trajectories between consistent vs inconsistent sample selection during training to validate the variance reduction hypothesis