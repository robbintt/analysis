---
ver: rpa2
title: Adobe Summit Concierge Evaluation with Human in the Loop
arxiv_id: '2511.03186'
source_url: https://arxiv.org/abs/2511.03186
tags:
- data
- evaluation
- queries
- questions
- adobe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adobe Summit Concierge is a generative AI assistant for Adobe Summit
  events that handles diverse event-related queries. To address data sparsity, response
  reliability, and rapid deployment challenges, the team adopted a human-in-the-loop
  development workflow combining prompt engineering, retrieval grounding, and lightweight
  human validation.
---

# Adobe Summit Concierge Evaluation with Human in the Loop

## Quick Facts
- arXiv ID: 2511.03186
- Source URL: https://arxiv.org/abs/2511.03186
- Authors: Yiru Chen; Sally Fang; Sai Sree Harsha; Dan Luo; Vaishnavi Muppala; Fei Wu; Shun Jiang; Kun Qian; Yunyao Li
- Reference count: 11
- Primary result: Generative AI assistant for event queries deployed with human-in-the-loop evaluation, reducing manual review from 3,000 to 220 queries.

## Executive Summary
Adobe Summit Concierge is a generative AI assistant deployed for Adobe Summit events that handles diverse queries about sessions, speakers, schedules, and event logistics. The system addresses data sparsity, response reliability, and rapid deployment challenges through a human-in-the-loop development workflow combining prompt engineering, retrieval grounding, and lightweight human validation. The approach successfully reduced manual annotation burden while maintaining evaluation quality, with significant improvements in autocomplete relevance, query rewriting accuracy, and routing precision.

## Method Summary
The system uses intent detection to route queries to either structured (SQL/knowledge graph) or unstructured (RAG) processing paths. Structured queries are evaluated using gold SQL benchmarks with LLM-assisted first-pass judging, while unstructured data uses correctness scoring, side-by-side comparison, and brand compliance checks with human review. Synthetic query generation via SQLSynth creates in-scope evaluation questions from executable SQL, and chain-of-thought query rewriting resolves multi-turn dialogue ambiguities. The workflow includes iterative refinement through bug bashes and daily annotation retrospectives.

## Key Results
- Autocomplete suggestions improved from 27% to 58% relevance and keystroke savings from 6.09 to 11.45 characters
- Multi-turn query handling reduced rewrite error rate from 4.35% to 1.45%
- Intent routing accuracy improved from 89.1% to 96.1%
- Manual review reduced from 3,000 to 220 queries through LLM-as-judge triage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-as-judge with human review triage can dramatically reduce manual annotation burden while maintaining evaluation quality.
- Mechanism: The system uses LLMs to perform first-pass judgment with confidence scores, automatically verifying high-confidence cases and routing only uncertain/low-confidence samples to human reviewers.
- Core assumption: LLM judges can reliably identify cases where they are uncertain, and human reviewers catch the most impactful errors.
- Evidence anchors:
  - [section] "Out of 3,000 templated queries, only 220 requires human reviews. The rest are automatically verified"
  - [section] "This approach reduced manual annotation needs from 1500 queries to just 276"
  - [corpus] Neighbor paper "Evaluation and Incident Prevention in an Enterprise AI Assistant" discusses similar LLM-assisted evaluation frameworks but does not quantify the reduction ratio.
- Break condition: If LLM confidence scores become miscalibrated, the triage fails and errors slip through without human review.

### Mechanism 2
- Claim: Synthetic query generation from executable SQL guarantees schema-grounded, in-scope evaluation questions without hallucinated database fields.
- Mechanism: SQLSynth reverse-engineers natural language questions from programmatically generated SQL queries over the actual schema, guaranteeing all queries are answerable by the system.
- Core assumption: The database schema covers the meaningful query space users will explore; diversity from programmatic SQL exploration approximates real user intent distribution.
- Evidence anchors:
  - [section] "By generating questions from executable SQL over the schema, all questions are guaranteed to be in-scope"
  - [section] "Using 269 auto-generated queries produced less than 30 minutes, we found the router achieved 100% accuracy"
  - [corpus] Weak corpus support—no neighbor papers directly validate SQL-to-question reverse engineering for evaluation coverage.
- Break condition: If the schema is incomplete or if user queries require cross-database reasoning not captured in the schema, synthetic queries will systematically miss real failure modes.

### Mechanism 3
- Claim: Chain-of-thought query rewriting with temporal/contextual resolution enables multi-turn dialogue handling in event settings with inherent temporal ambiguity.
- Mechanism: A reasoning-oriented LLM analyzes ambiguous user inputs, infers context from conversation history and event structure, and produces explicit, grounded rewrites with resolved references.
- Core assumption: Explicit reasoning traces improve context resolution compared to direct generation, and few-shot examples sufficiently cover ambiguity patterns.
- Evidence anchors:
  - [section] "Rewrite Error Rate 4.35% → 1.45%" and "Routing Accuracy 89.1% → 96.1%"
  - [section] "This approach enables the model to explicitly analyze the user's input, resolve ambiguities, and produce clear, contextually grounded rewrites"
  - [corpus] Neighbor paper "Detecting Ambiguities to Guide Query Rewrite for Robust Conversations" proposes similar NLU-NLG frameworks for ambiguity detection but does not report comparative error rates.
- Break condition: If conversation history exceeds context window limits or if temporal ambiguity patterns drift beyond few-shot coverage, rewrite quality degrades without systematic detection.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The unstructured data path retrieves ABC Guide passages to ground responses, reducing hallucinations on event logistics, venue maps, and FAQs.
  - Quick check question: Given a user query "Where can I park during the Summit?", can you trace how RAG retrieves relevant passages before answer generation?

- Concept: LLM-as-Judge Evaluation Paradigm
  - Why needed here: Multiple evaluation modes use LLM judges with chain-of-thought reasoning, calibrated for human alignment.
  - Quick check question: What is the difference between using an LLM as a first-pass judge versus a final arbiter, and why does this paper use both?

- Concept: Intent Routing / Query Classification
  - Why needed here: The system must distinguish structured queries from unstructured queries before processing.
  - Quick check question: If intent detection misclassifies a structured query as unstructured, what downstream failures would you expect?

## Architecture Onboarding

- Component map: User Query → Autocomplete Module (suggests from question pool) → Query Rewriter (resolves multi-turn context) → Intent Detection (routes to structured vs. unstructured path) → Structured: NL2SQL → Knowledge Graph / Snowflake DB + RainFocus API (for personalization) → Unstructured: RAG Retrieval → ABC Guide + live-authored content → Unified Answer Generation → Response

- Critical path: Intent detection accuracy (96.1% post-improvement) gates whether queries reach the correct backend. Query rewriting error rate (1.45%) directly impacts downstream RAG and SQL quality. Autocomplete relevance (58%) determines first-contact user experience.

- Design tradeoffs:
  - Cold-start data sparsity was addressed via synthetic queries rather than waiting for real logs—trades some distribution mismatch for rapid deployment capability.
  - Attendee schedule data kept out of Snowflake and queried via RainFocus API—trades query latency for privacy/freshness guarantees.
  - LLM-as-judge reduces human review but introduces calibration risk if confidence scores become unreliable.

- Failure signatures:
  - Intent detection errors (3.2% of 624 annotated interactions) caused queries to hit wrong pipeline.
  - Rewrite errors (3.4% of annotations) propagated ambiguous queries downstream, reducing retrieval precision.
  - OOS routing errors (reduced from 4% to 3%) indicate boundary definition issues between valid and unsupported queries.

- First 3 experiments:
  1. **Calibration audit of LLM-as-judge confidence scores**: Sample 200 queries where LLM marked high-confidence correct; manually verify false-positive rate to ensure triage reliability.
  2. **Intent detection confusion matrix on synthetic vs. real queries**: Compare routing accuracy on SQLSynth-generated queries (100% accuracy reported) against live user queries to identify distribution shift.
  3. **Rewrite error analysis by ambiguity type**: Categorize the 21 rewrite errors by temporal, reference, or ellipsis ambiguity patterns to prioritize few-shot example coverage gaps.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can this agile, human-in-the-loop workflow be effectively generalized to other enterprise domains with longer lifecycles and higher precision requirements?
  - Basis in paper: [explicit] The conclusion states, "Looking forward, we see opportunities to generalize our methodology to other enterprise domains, particularly those that involve event support, internal knowledge access, or customer service."
  - Why unresolved: The current study is limited to a specific, short-term event (Adobe Summit), whereas other domains may not tolerate the "lightweight" error rates seen during the event's agile iteration.
  - What evidence would resolve it: Successful deployment metrics from applying this specific HITL pipeline to a long-term enterprise knowledge base or customer service platform.

- **Open Question 2**: How can the process of benchmark evolution be automated to keep pace with changing product features without reintroducing high manual labeling costs?
  - Basis in paper: [explicit] Section 2 notes that "evaluation benchmarks for domain AI assistants must evolve" and explicitly calls for "lightweight mechanisms to continuously refine benchmarks" to avoid extensive manual labeling.
  - Why unresolved: While the paper demonstrates efficient evaluation, the creation of the benchmarks themselves still relies heavily on human curation and expert review.
  - What evidence would resolve it: A method that autonomously updates evaluation datasets based on detected shifts in user intent or knowledge base updates, maintaining accuracy without manual intervention.

- **Open Question 3**: Does reliance on synthetic data for cold-start scenarios introduce blind spots in handling ambiguous or out-of-scope queries compared to systems trained on historical logs?
  - Basis in paper: [inferred] Section 4.1 highlights that synthetic generation guarantees "in-scope" queries, but Section 6 reveals that 3.2% of errors were due to intent detection and 4% of queries were initially routed incorrectly as out-of-scope.
  - Why unresolved: Synthetic data effectively covers the database schema but may fail to replicate the messy, ambiguous nature of real human queries that lead to routing errors.
  - What evidence would resolve it: A comparative study measuring the "unknown unknown" rate (valid queries misclassified as OOS) between a synthetic-only bootstrapping approach and one using a small sample of real-world interaction data.

## Limitations

- The evaluation relies heavily on LLM-as-judge systems whose reliability depends on prompt quality and model calibration, but specific prompt templates and confidence threshold settings are not disclosed.
- The synthetic query generation via SQLSynth guarantees in-scope questions but may not capture the full diversity of real user intent patterns, potentially creating evaluation blind spots.
- The reported improvements in autocomplete relevance and keystroke savings are based on system metrics without independent human preference studies.

## Confidence

- **High confidence**: Structured query evaluation methodology with gold SQL benchmarks and LLM-assisted first-pass judging (quantified reduction from 3,000 to 220 human reviews).
- **Medium confidence**: Multi-turn query rewriting error rate improvements and routing accuracy gains (based on annotated samples but limited by sample size).
- **Medium confidence**: Autocomplete relevance and keystroke savings metrics (system-reported improvements without independent validation).
- **Low confidence**: Brand compliance evaluation effectiveness and the full coverage of synthetic query generation for real-world user behavior.

## Next Checks

1. **LLM-as-judge calibration audit**: Sample 200 high-confidence auto-approved responses for manual verification to establish false-positive rates and determine if current confidence thresholds are appropriate.
2. **Real-vs-synthetic query distribution analysis**: Deploy the system to capture 1,000 live user queries and compare their routing patterns and ambiguity types against the SQLSynth-generated query set to identify systematic coverage gaps.
3. **Brand compliance effectiveness study**: Recruit 50 participants to rate responses for brand voice adherence across 100 randomly sampled interactions, comparing LLM-assessed scores against human preferences to validate the compliance evaluation framework.