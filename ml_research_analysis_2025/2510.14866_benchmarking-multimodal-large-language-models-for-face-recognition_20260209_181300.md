---
ver: rpa2
title: Benchmarking Multimodal Large Language Models for Face Recognition
arxiv_id: '2510.14866'
source_url: https://arxiv.org/abs/2510.14866
tags:
- face
- recognition
- mllms
- arxiv
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks multimodal large language models (MLLMs)
  on standard face recognition datasets including LFW, CALFW, CPLFW, CFP, AgeDB, and
  RFW. The evaluation follows the same verification protocol used for traditional
  face recognition models, where two images are presented and the MLLM is asked to
  determine if they are of the same person.
---

# Benchmarking Multimodal Large Language Models for Face Recognition

## Quick Facts
- arXiv ID: 2510.14866
- Source URL: https://arxiv.org/abs/2510.14866
- Reference count: 0
- MLLMs significantly underperform specialized face recognition models on standard benchmarks

## Executive Summary
This paper benchmarks multimodal large language models (MLLMs) on standard face recognition datasets including LFW, CALFW, CPLFW, CFP, AgeDB, and RFW. The evaluation follows the same verification protocol used for traditional face recognition models, where two images are presented and the MLLM is asked to determine if they are of the same person. Results show that MLLMs significantly underperform compared to specialized face recognition models, with accuracies around 50-90% versus 90-99% for state-of-the-art models. While larger MLLMs and fine-tuning on face data (e.g., FaceLLM) improve performance, they still cannot match the precision of dedicated face recognition systems.

## Method Summary
The benchmark evaluates MLLMs on face verification tasks using standard datasets (LFW, CALFW, CPLFW, CFP-FF, CFP-FP, AgeDB-30, RFW) with 6,000 image pairs each. Models are evaluated zero-shot using a binary prompt ("Are these two images of the same person? Answer 'yes' or 'no'.") and accuracy is measured. The evaluation uses VLMEvalKit and cropped images from the Insightface repository, testing both open-source and closed-source MLLMs across various scales (3B-38B parameters).

## Key Results
- MLLMs achieve 50-90% accuracy on face verification benchmarks, significantly below 90-99% for specialized models
- Larger MLLMs show improved performance with diminishing returns (e.g., InternVL3-1B: 61.65% vs InternVL3-38B: 72.55% avg)
- Fine-tuning on face data (FaceLLM) improves performance over base models but still lags specialized systems
- Cross-dataset performance reveals robustness gaps, especially for cross-age and cross-pose verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model scale correlates with face verification accuracy, but with diminishing returns and eventual saturation.
- Mechanism: Larger parameter counts enable finer-grained visual feature representation and more nuanced similarity reasoning. The vision encoder and language model jointly develop capacity to distinguish subtle facial differences that smaller models cannot resolve.
- Core assumption: Performance gains from scale are bounded by training data quality and task alignment, not just parameter count.
- Evidence anchors:
  - [abstract] "While larger MLLMs and fine-tuning on face data (e.g., FaceLLM) improve performance, they still cannot match the precision of dedicated face recognition systems."
  - [section] Table 1 shows InternVL3-1B (61.65% avg) vs InternVL3-38B (72.55% avg); Qwen2.5-VL-3B (66.93%) vs Qwen2.5-VL-7B (76.60%); paper notes "increasing the size of MLLM can improve the performance on benchmarks, it also saturates for each MLLM."
  - [corpus] Related benchmarking papers show similar scale-dependent patterns across multimodal tasks; no direct contradictory evidence found.
- Break condition: Scale improvements plateau when model capacity exceeds the information content of general-purpose training data for identity-specific features.

### Mechanism 2
- Claim: Domain-specific fine-tuning on face data improves MLLM face recognition performance relative to general-purpose base models.
- Mechanism: Fine-tuning re-aligns visual features and language outputs toward identity-discriminative patterns, reducing the gap between general visual semantics and fine-grained biometric features required for verification.
- Core assumption: The improvement stems from domain adaptation rather than model architecture changes.
- Evidence anchors:
  - [abstract] "fine-tuning on face data (e.g., FaceLLM) improve performance"
  - [section] "FaceLLM is based on InternVL3 and finetuned for face understanding. The results in Table 1 show that the finetuning in FaceLLM has increased the performance compared to the base model (InternVL3) on different face recognition benchmarks."
  - [corpus] Weak corpus evidence on face-specific fine-tuning mechanisms; corpus neighbor "Benchmarking Foundation Models for Zero-Shot Biometric Tasks" mentions foundation models for biometrics but details are limited.
- Break condition: Fine-tuning gains may not transfer to out-of-distribution demographics or extreme pose/age variations not represented in fine-tuning data.

### Mechanism 3
- Claim: MLLMs trained on general-purpose image-text pairs capture high-level semantic cues but lack the fine-grained feature precision required for reliable identity verification.
- Mechanism: General-purpose training optimizes for broad visual-linguistic alignment (e.g., describing appearance, attributes) rather than learning discriminative embeddings that separate identities under challenging conditions (cross-age, cross-pose).
- Core assumption: The semantic-level understanding is fundamentally different from metric-space identity learning used in specialized face recognition.
- Evidence anchors:
  - [abstract] "MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios"
  - [section] "most are trained mainly on general-purpose datasets or large-scale imageâ€“text pairs collected from the web... they frequently struggle with more details which are required to recognize identity"
  - [corpus] No direct corpus evidence on this specific mechanism; related benchmarking papers focus on different modalities/tasks.
- Break condition: If MLLM architectures incorporated explicit metric learning objectives or face-specific pretraining, this mechanism would weaken.

## Foundational Learning

- Concept: Face verification protocol (binary same/different identity decision)
  - Why needed here: The benchmark uses pairwise verification rather than identification or attribute prediction; understanding this distinction is critical for interpreting accuracy results.
  - Quick check question: Given two face images, does the task require identifying who they are, or deciding if they match?

- Concept: Intra-class variation (age, pose, illumination) vs. inter-class separation
  - Why needed here: The datasets (CALFW, CPLFW, AgeDB, CFP-FP) explicitly test robustness to these factors; performance gaps reveal where MLLMs fail.
  - Quick check question: Why might a model achieve 90%+ on LFW but only 55% on AgeDB-30?

- Concept: Zero-shot evaluation (no task-specific training)
  - Why needed here: All MLLMs are evaluated zero-shot; comparing them to supervised face recognition models (IResNet-50 trained on MS1MV2) reveals inherent capability differences rather than training differences.
  - Quick check question: If an MLLM were fine-tuned on MS1MV2, would the comparison still isolate architectural differences?

## Architecture Onboarding

- Component map:
  Vision encoder -> Vision-language projector -> Large language model backbone -> Prompt interface

- Critical path:
  1. Load and preprocess both face images (cropped versions from Insightface repository)
  2. Pass each image through vision encoder independently
  3. Fuse visual representations with text prompt via projector
  4. LLM generates binary response
  5. Parse response and compute accuracy over 6,000 pairs per dataset

- Design tradeoffs:
  - Larger models (7B-32B parameters) improve accuracy but increase inference cost and latency
  - General-purpose MLLMs offer flexibility but sacrifice precision; specialized face models offer precision but lack general reasoning
  - Binary prompt format aligns with standard FR evaluation but may underutilize MLLM reasoning capabilities

- Failure signatures:
  - Near-chance performance (49-51%): Model not engaging with visual content (e.g., LLaVA-v1.5, ShareGPT4v)
  - Large cross-dataset variance: Overfitting to specific image distributions
  - Demographic disparities: Table 2 shows higher variance across racial groups for some MLLMs (Qwen2-VL-7B: 5.90 std) compared to specialized models (IResNet-50 MS1MV2: 0.58 std)

- First 3 experiments:
  1. Establish baseline with Qwen2-VL-7B-Instruct (top open-source performer at 81.10% avg) on LFW to validate evaluation pipeline matches paper results.
  2. Ablate prompt format: Compare binary "yes/no" prompt against chain-of-thought prompting to assess whether reasoning improves accuracy or introduces noise.
  3. Cross-condition stress test: Evaluate top-3 MLLMs on CPLFW and AgeDB-30 specifically to quantify robustness to pose and age variation; compare gap relative to LFW baseline.

## Open Questions the Paper Calls Out

- Question: How can the significant performance gap between general-purpose MLLMs and specialized face recognition models be effectively bridged?
  - Basis in paper: [explicit] The conclusion states that the lack of task-specific precision "poses challenges for applications of MLLMs in face recognition and requires further study in future."
  - Why unresolved: While fine-tuning (e.g., FaceLLM) improves performance, the results show MLLMs still lag behind specialized models (90-93% vs. 99%+ on LFW).
  - What evidence would resolve it: Development of an MLLM architecture or training paradigm that achieves parity (e.g., >99% on LFW) with specialized IResNet models.

- Question: How do commercial, closed-source MLLMs compare to open-source models on standard face verification benchmarks?
  - Basis in paper: [explicit] Footnote 4 states, "given the restrictions in license of each of the benchmark datasets, we were not able to use commercial MLLMs in this study."
  - Why unresolved: The benchmark is restricted to open-source models, leaving the capabilities of proprietary systems (like GPT-4o) on this specific protocol unmeasured.
  - What evidence would resolve it: Evaluation of leading commercial MLLMs on the LFW, CALFW, and AgeDB datasets using the paper's verification protocol.

- Question: Does the observed performance saturation in larger MLLMs indicate a fundamental limitation in the current architecture or training data for face tasks?
  - Basis in paper: [inferred] The authors note in Section 4 that while increasing size helps, performance "saturates for each MLLM" and does not linearly scale to match specialized models.
  - Why unresolved: It is unclear if the plateau is due to the generalist nature of the training data or architectural inefficiencies in processing fine-grained facial features.
  - What evidence would resolve it: A scaling law analysis specifically for face recognition tasks, showing whether accuracy continues to improve with significantly larger parameters or requires architectural shifts.

## Limitations

- Binary "yes/no" prompt format may artificially constrain MLLM performance ceiling and underutilize reasoning capabilities
- Zero-shot evaluation protocol introduces fundamental architectural mismatch when comparing to supervised face recognition models
- Datasets may not fully represent real-world diversity, particularly cross-cultural and cross-environmental variations

## Confidence

**High Confidence**: The core finding that MLLMs underperform specialized face recognition models in verification accuracy is well-supported by experimental results across multiple datasets and model scales.

**Medium Confidence**: The relationship between model scale and face verification performance shows clear positive correlation with diminishing returns, but exact nature of saturation point remains uncertain.

**Low Confidence**: Specific mechanisms by which domain-specific fine-tuning improves performance and whether improvements transfer to out-of-distribution scenarios are not fully characterized.

## Next Checks

1. Systematically test alternative prompt formats (chain-of-thought, multi-step reasoning, confidence scoring) to determine if binary response constraint artificially limits MLLM performance.

2. Re-evaluate top-performing MLLMs on disaggregated RFW results to quantify demographic bias patterns and compare variance across racial groups with specialized models.

3. Fine-tune a subset of MLLMs on MS1MV2 and evaluate zero-shot performance on the same datasets to measure whether gains transfer to challenging conditions.