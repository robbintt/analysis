---
ver: rpa2
title: 'CANDI: Hybrid Discrete-Continuous Diffusion Models'
arxiv_id: '2510.22510'
source_url: https://arxiv.org/abs/2510.22510
tags:
- diffusion
- continuous
- discrete
- corruption
- candi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the poor performance of continuous diffusion
  models on discrete data, caused by a mismatch between discrete identity corruption
  and continuous rank degradation under Gaussian noise. The authors introduce token
  identifiability to analyze this issue and propose CANDI (Continuous ANd Discrete
  diffusion), a hybrid framework that decouples discrete masking from continuous Gaussian
  noise, allowing simultaneous learning of discrete conditional structure and continuous
  geometry.
---

# CANDI: Hybrid Discrete-Continuous Diffusion Models

## Quick Facts
- arXiv ID: 2510.22510
- Source URL: https://arxiv.org/abs/2510.22510
- Reference count: 40
- Hybrid diffusion framework outperforms masked diffusion models at low NFE

## Executive Summary
CANDI addresses the fundamental limitation of continuous diffusion models on discrete data by diagnosing and resolving a temporal dissonance between discrete identity corruption and continuous rank degradation. The authors show that these two corruption mechanisms scale differently with vocabulary size, causing discrete tokens to become unidentifiable before continuous denoising becomes learnable. CANDI decouples discrete masking from continuous Gaussian noise, enabling simultaneous learning of discrete conditional structure and continuous geometry. Empirically, CANDI resolves the temporal dissonance problem and outperforms masked diffusion models at low NFE, while enabling simple classifier-based guidance using off-the-shelf classifiers.

## Method Summary
CANDI introduces a hybrid noising kernel that decouples discrete masking from continuous Gaussian corruption. The method uses a binary mask M_t ~ Bernoulli(α(t)) where α(t)=1-t to preserve clean token positions, while applying Gaussian noise σ(t) = -1/(Φ⁻¹(r*(t))·√2) only to corrupted positions. The model is trained to predict both the discrete tokens and the continuous corrupted embeddings, with a corruption bias term added to distinguish clean from noisy positions. During inference, an approximate algorithm uses embedding lookup instead of full matrix operations. The framework is implemented with a Diffusion Transformer (110M params, 768 hidden dim, 12 blocks) trained on text and molecular datasets.

## Key Results
- CANDI resolves temporal dissonance between discrete identity corruption and continuous rank degradation
- Outperforms masked diffusion models at low NFE (8-64 steps) on Text8 and OpenWebText
- Enables simple classifier-based guidance using off-the-shelf classifiers
- Maintains strong performance on molecular generation task (QM9)

## Why This Works (Mechanism)

### Mechanism 1: Temporal Dissonance Diagnosis
Continuous diffusion fails on large-vocabulary discrete data because discrete identity corruption and continuous rank degradation scale asymmetrically with vocabulary size |V|. Discrete identity corruption ρ(t) has exponential |V|-dependence while continuous rank degradation r(t) is |V|-independent. At large |V|, tokens become unidentifiable before continuous denoising becomes learnable.

### Mechanism 2: Structured Hybrid Noising Kernel
Decoupling discrete masking (α(t) = 1−t) from Gaussian noise (σ(t) via inverted r*(t)) realigns both corruption mechanisms to scale linearly with time. Binary mask determines which positions remain clean vs. corrupted, removing |V|-dependent discrete corruption while preserving learnable continuous gradients.

### Mechanism 3: Gradient-Based Joint Updates at Low NFE
Continuous score functions enable coordinated multi-position updates, improving generation quality when few denoising steps are available. Learned score ∇log p_t(X_t) provides joint gradient signal across all positions, unlike independent conditional sampling in pure discrete diffusion.

## Foundational Learning

- **Score functions and score-based generative models**: Why needed - CANDI learns ∇log p_t(X_t) to enable continuous-gradient updates. Quick check - Can you explain why adding −∇log p_t(X_t) moves samples toward higher-density regions?

- **Markov chain transition kernels for discrete diffusion**: Why needed - The discrete masking process follows standard masked diffusion theory. Quick check - Given forward corruption q(X_t|X_0), how would you derive the reverse transition q(X_s|X_t, X_0)?

- **Frontier analysis for generative model evaluation**: Why needed - Single-temperature evaluations are misleading. Quick check - Why might two models rank differently at different sampling temperatures?

## Architecture Onboarding

- **Component map**: Input: X_0 (discrete tokens) → One-hot encoding Ẋ_0 → Forward process: M_t (binary mask via α(t)) + Gaussian noise σ(t) → Hybrid state: X_t = X_0 ⊙ M_t + Ẋ_t ⊙ (1−M_t) → Model: Transformer predicting P_θ(X_0|X_t) and E[Ẋ_0|Ẋ_t] → Reverse: Discrete token sampling + ODE step for continuous component → Output: Discrete tokens via argmax

- **Critical path**: 1. Corruption bias injection: Add learnable bias b to corrupted positions so model can distinguish clean vs. noisy 2. Preconditioning: Rescale noisy embeddings by √(σ²+1) but leave clean positions unchanged 3. Approximate inference: Use embedding lookup instead of full |V|×L×B matrix multiplication

- **Design tradeoffs**: Exact vs. approximate inference (computational cost vs. accuracy), linear schedule assumption (simplicity vs. optimality), one-hot vs. embedding diffusion (analytical control vs. evolved geometry)

- **Failure signatures**: Mode collapse at large |V| with pure continuous diffusion, unable to distinguish clean vs. corrupted positions without bias term, performance degradation at high NFE (>128)

- **First 3 experiments**: 1. Vocabulary scaling validation: Train pure continuous diffusion on Text8 (|V|=27) vs. OpenWebText (|V|=50K) and compare frontiers 2. Ablation on corruption bias: Train CANDI with λ=0, λ=1, and learned λ 3. NFE sweep with frontier analysis: Compare CANDI vs. MDLM vs. DUO at NFE ∈ {8, 16, 32, 64, 128} with temperature sweep τ ∈ [0.7, 1.0]

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal functional relationship between the continuous rank degradation target ($r^*(t)$) and the discrete identity corruption rate ($\rho(t)$?
- Basis in paper: [explicit] Page 7 states, "It should be noted that this linear schedule was selected for simplicity, the optimal relation between $r^*$ and $\rho(t) = 1-\alpha(t)$ is left for future work."
- Why unresolved: The authors chose a linear schedule for simplicity but did not investigate whether a non-linear coupling could improve simultaneous learning of conditional structure and continuous geometry.
- What evidence would resolve it: An ablation study comparing various non-linear schedules against the linear baseline on generative perplexity and diversity metrics.

### Open Question 2
- Question: Do the observed trade-offs between NFE, temperature, and generative quality (frontier analysis) persist at larger model scales, or are they artifacts of the 110M parameter size?
- Basis in paper: [explicit] Page 28 notes regarding frontier analysis, "We do not know whether this is simply an artifact of this specific scale of models, or whether this is a persistent phenomenon for larger diffusion models."
- Why unresolved: Experiments were conducted on a 110M parameter Diffusion Transformer; it is unverified if efficiency benefits at low NFE hold for billion-parameter models.
- What evidence would resolve it: Training and evaluating CANDI on larger architectures (e.g., 1B+ parameters) to observe if relative performance compared to baselines shifts or remains consistent.

### Open Question 3
- Question: Can the token identifiability framework and the decoupled corruption strategy be effectively adapted to embedding diffusion to resolve evolving geometry issues?
- Basis in paper: [explicit] Page 6 states, "While disentangling discrete corruption from continuous noise level is also possible in the embedding space, we focus on solving this problem in the one-hot space..." and Page 22 notes, "We note that applying the insights of CANDI to embedding diffusion may work in practice."
- Why unresolved: While the theory applies, the authors highlight that embedding diffusion suffers from an evolving geometry where a fixed noise level corresponds to different corruption levels during training.
- What evidence would resolve it: An implementation of CANDI using latent embeddings rather than one-hot vectors, demonstrating that the decoupling strategy stabilizes training and improves performance over standard embedding diffusion baselines.

## Limitations

- The linear scheduling assumption (α(t) = 1-t) lacks theoretical justification for optimality and may not be ideal for all datasets
- The corruption bias mechanism introduces an additional hyperparameter (λ) that significantly affects performance but isn't thoroughly explored
- The approximate inference method trades exactness for computational efficiency without quantifying the approximation error

## Confidence

**High Confidence**: The temporal dissonance diagnosis is mathematically sound with correctly derived analytical forms for identity corruption and rank degradation. The hybrid kernel implementation is straightforward and performance gains at low NFE are empirically demonstrated.

**Medium Confidence**: The claim that continuous score functions provide meaningful joint updates that improve low-NFE generation quality. While performance data supports this, the mechanism could also be explained by other factors like better conditioning on clean tokens.

**Low Confidence**: The optimality of the linear scheduling choice and the specific relationship between r*(t) and ρ(t). The paper presents these as reasonable choices but doesn't provide theoretical or empirical justification that they are optimal.

## Next Checks

1. **Vocabulary scaling experiment**: Train pure continuous diffusion and CANDI on datasets with controlled vocabulary sizes (10, 100, 1000, 10000 tokens) while keeping other factors constant. Measure performance gap as a function of |V| to directly validate temporal dissonance hypothesis across the full spectrum.

2. **NFE threshold investigation**: Systematically sweep NFE from 8 to 256 and identify the exact point where CANDI's advantage over MDLM disappears. Analyze entropy-perplexity frontiers at this threshold to determine whether continuous geometry benefit is truly NFE-dependent.

3. **Corruption bias ablation study**: Train CANDI variants with λ ∈ {0.0, 0.3, 0.5, 0.7, 1.0} and compare performance across all NFE settings. This reveals whether corruption bias is merely enabling basic functionality or has a more nuanced role in learning dynamics.