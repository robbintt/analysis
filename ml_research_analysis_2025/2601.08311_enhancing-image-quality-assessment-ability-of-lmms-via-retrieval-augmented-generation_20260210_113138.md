---
ver: rpa2
title: Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented
  Generation
arxiv_id: '2601.08311'
source_url: https://arxiv.org/abs/2601.08311
tags:
- image
- quality
- images
- lmms
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing image quality assessment
  (IQA) ability of large multimodal models (LMMs) without requiring computationally
  expensive fine-tuning. The authors propose IQARAG, a novel training-free framework
  that leverages Retrieval-Augmented Generation (RAG) to retrieve semantically similar
  but quality-variant reference images with corresponding Mean Opinion Scores (MOSs)
  to serve as visual anchors for IQA.
---

# Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2601.08311
- Source URL: https://arxiv.org/abs/2601.08311
- Reference count: 39
- Key outcome: IQARAG improves LMM-based IQA performance with SRCC 0.7972 and PLCC 0.7757 on average across four datasets

## Executive Summary
This paper introduces IQARAG, a training-free framework that enhances large multimodal models' image quality assessment capabilities through retrieval-augmented generation. The approach retrieves semantically similar but quality-variant reference images with Mean Opinion Scores to serve as visual anchors, eliminating the need for computationally expensive fine-tuning. Extensive experiments across KADID, KonIQ, LIVE Challenge, and SPAQ datasets demonstrate significant performance improvements across multiple LMMs including Qwen3-VL, InternVL3.5, and Kimi-VL.

## Method Summary
IQARAG operates through three key phases: Retrieval Feature Extraction where query images are processed to extract quality-relevant features; Image Retrieval where semantically similar but quality-variant reference images with MOS scores are retrieved; and Integration & Quality Score Generation where retrieved visual anchors are combined with the query image to generate quality assessments. The framework leverages RAG principles to provide LMMs with additional quality-relevant context without requiring model fine-tuning, making it a resource-efficient alternative for quality assessment tasks.

## Key Results
- Achieved SRCC of 0.7972 and PLCC of 0.7757 on average across all four tested IQA datasets
- Demonstrated consistent performance improvements across different LMMs (Qwen3-VL, InternVL3.5, Kimi-VL)
- Showed effectiveness across multiple dataset splitting ratios, validating robustness
- Outperformed baseline LMM approaches without requiring computationally expensive fine-tuning

## Why This Works (Mechanism)
IQARAG leverages the principle that providing LMMs with semantically similar but quality-variant reference images serves as effective visual anchors for quality assessment. By retrieving images with known quality scores that share semantic content with the query image, the framework creates a comparative context that helps LMMs better discern quality differences. The retrieval-augmented generation approach allows the model to leverage existing quality-labeled data without fine-tuning, while the visual anchors provide concrete examples of both high and low quality instances within the same semantic domain, enabling more accurate quality scoring.

## Foundational Learning
- Image Quality Assessment (IQA): The task of automatically predicting perceived image quality scores
  * Why needed: Core problem being addressed - enabling LMMs to assess image quality
  * Quick check: Understanding the difference between objective and subjective quality metrics

- Retrieval-Augmented Generation (RAG): Technique combining retrieval of relevant information with generation
  * Why needed: Core mechanism enabling IQARAG to provide quality-relevant context
  * Quick check: Understanding how retrieved content augments model generation capabilities

- Mean Opinion Scores (MOS): Subjective quality ratings assigned by human evaluators
  * Why needed: Ground truth quality labels used for training and evaluation
  * Quick check: Understanding MOS scale and collection methodology

- Semantic Similarity in Image Retrieval: Finding images with similar content but potentially different quality
  * Why needed: Enables retrieval of quality-variant anchors that share semantic content
  * Quick check: Understanding feature extraction methods for semantic similarity

- SRCC and PLCC Metrics: Statistical measures for evaluating quality assessment performance
  * Why needed: Standard evaluation metrics for IQA tasks
  * Quick check: Understanding the difference between Spearman and Pearson correlation coefficients

## Architecture Onboarding

**Component Map:** Input Image -> Feature Extraction -> Retrieval Module -> Integration Layer -> Quality Score Generation

**Critical Path:** Query Image → Feature Extraction → Image Retrieval → Integration & Quality Score Generation

**Design Tradeoffs:** The framework prioritizes training-free operation and computational efficiency over potentially higher performance that could be achieved through fine-tuning, making it more accessible for resource-constrained applications.

**Failure Signatures:** Performance degradation when reference datasets lack sufficient quality diversity, retrieval failures returning irrelevant images, or when semantic similarity does not correlate with useful quality anchors.

**First Experiments to Run:**
1. Ablation study removing the retrieval phase to quantify its contribution to performance improvements
2. Testing framework with synthetic quality-degraded images to evaluate controlled performance
3. Cross-dataset evaluation to assess generalization capabilities across different image domains

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit limitations remain regarding the framework's dependence on existing reference datasets with MOS scores and the computational overhead of the retrieval phase.

## Limitations
- Framework effectiveness heavily depends on the quality and diversity of reference datasets, which may limit generalizability to domains lacking such data
- Computational overhead of the retrieval phase is not quantified, raising questions about scalability for large-scale applications
- Limited analysis of failure cases where semantically similar images may not provide useful quality anchors for assessment
- No evaluation of model robustness to adversarial or out-of-distribution images that could reveal potential vulnerabilities

## Confidence
- Overall framework efficacy: **High** - consistent improvements across multiple datasets and LMMs demonstrate robust performance
- Computational efficiency claims: **Medium** - lack of quantitative comparison of retrieval overhead creates uncertainty about real-world efficiency
- Generalization to novel domains: **Low** - strong dependence on existing reference datasets with MOS scores limits applicability to domains without such data

## Next Checks
1. Conduct ablation studies to quantify the contribution of each framework component (feature extraction, retrieval, integration) to overall performance improvements
2. Measure and report the computational overhead of the retrieval phase, including latency and resource usage, to validate efficiency claims
3. Test the framework on a domain-specific IQA task (e.g., medical imaging or satellite imagery) where reference datasets are limited to evaluate real-world applicability and generalization capabilities