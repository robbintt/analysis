---
ver: rpa2
title: 'Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential
  Equations'
arxiv_id: '2601.01021'
source_url: https://arxiv.org/abs/2601.01021
tags:
- stochastic
- chaos
- neural
- operator
- equations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents neural operator architectures for stochastic
  differential equations (SDEs) and stochastic partial differential equations (SPDEs)
  built on Wiener chaos expansions. The key idea is to project driving noise paths
  onto orthonormal Wick-Hermite features and use neural operators to parameterize
  the resulting chaos coefficients, enabling reconstruction of full trajectories from
  noise in a single forward pass.
---

# Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations

## Quick Facts
- **arXiv ID**: 2601.01021
- **Source URL**: https://arxiv.org/abs/2601.01021
- **Reference count**: 40
- **Primary result**: Wiener-chaos-based neural operators achieve competitive accuracy across SPDE benchmarks, diffusion sampling, and diverse stochastic prediction tasks.

## Executive Summary
This paper presents neural operator architectures for stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) built on Wiener chaos expansions. The key innovation is projecting driving noise paths onto orthonormal Wick-Hermite features and using neural operators to parameterize the resulting chaos coefficients, enabling reconstruction of full trajectories from noise in a single forward pass. The authors make the underlying WCE structure explicit by showing coupled deterministic ODE/PDE systems governing chaos coefficients for multi-dimensional SDEs and semilinear SPDEs. Empirically, they achieve competitive accuracy across diverse tasks including SPDE benchmarks, diffusion one-step image sampling, topological graph interpolation, financial extrapolation, parameter estimation, and manifold SDE flood forecasting.

## Method Summary
The framework converts stochastic problems into deterministic ones via Wiener chaos expansion. For SPDEs, F-SPDENO expands propagators in temporal basis (e.g., Haar) and learns time-independent coefficient fields via FNO backbone. For SDEs, SDENO variants directly parameterize time-dependent propagators using architectures like MLP, UNet, or GNN. Wick features are computed from noise increments and fed to the neural operator along with initial conditions. The full trajectory is reconstructed via linear combination of outputs with chaos and temporal bases, requiring only a single forward pass without iterative time-stepping.

## Key Results
- F-SPDENO achieves lower L2 error than baselines on $\Phi^4_1$ SPDE benchmark
- U-SDENO reconstructs images with high fidelity from diffusion noise in a single forward pass
- S-SDENO achieves 1-Wasserstein distance of 0.14 on topological graph interpolation
- Models demonstrate strong extrapolation on Heston financial model and manifold flood forecasting

## Why This Works (Mechanism)

### Mechanism 1: Wiener–Chaos Expansion (WCE) Converts Stochastic Problem to Deterministic One
The solution of SDEs/SPDEs admits an expansion in orthonormal Wick-Hermite basis, with stochasticity captured by basis polynomials and deterministic dynamics encoded in propagator coefficients. Theorems 1-2 show these propagators satisfy coupled deterministic ODE/PDE systems, isolating randomness to the basis. Core assumption: solution is square-integrable and measurable with respect to driving noise $\sigma$-algebra.

### Mechanism 2: Neural Operators Approximate Deterministic Propagator Maps
Once noise is projected onto fixed Wick features, learning reduces to approximating deterministic propagator fields, which can be parameterized by neural operators (FNO, UNet, GNN). Core assumption: propagator maps are smooth enough for neural operator approximation. Evidence: F-SPDENO outperforms baselines on $\Phi^4_1$ with lower L2 error.

### Mechanism 3: One-Shot Trajectory Reconstruction via Linear Combination
A full space-time trajectory is reconstructed in a single forward pass by linear combination of learned propagator outputs with pre-computed Wick features and temporal basis functions. No iterative time-stepping required. Core assumption: driving noise trajectory is fully observed or can be inferred. Evidence: U-SDENO achieves high-fidelity image reconstruction in one pass.

## Foundational Learning

- **Wiener–Chaos Expansion / Polynomial Chaos**: Core mathematical machinery transforming stochastic equations into deterministic propagator systems. Understanding orthonormal Wick–Hermite bases and their construction from Brownian increments is essential. *Quick check*: Explain why Wick polynomials form an orthonormal basis for $L^2(\Omega, \sigma(\Xi), P)$ and their construction from Brownian increments.

- **Neural Operators (FNO, DeepONet, etc.)**: Backbone architectures learning deterministic propagator maps. Familiarity with function-space learning, Fourier layers, and discretization-invariance is required. *Quick check*: What is the input/output of an FNO layer, and why does it learn a mapping between function spaces rather than discrete vectors?

- **Stochastic Differential Equations & SPDEs**: Basic knowledge of SDEs/SPDEs, drift/diffusion terms, Brownian/Q-Brownian motion, and existence/uniqueness conditions. Needed to interpret Theorems 1/2 and adapt framework to new equations. *Quick check*: In the SPDE $dX_t = (AX_t + F(t,\cdot,X_t))dt + B(t,\cdot,X_t)dW_t$, what is the role of the $C_0$-semigroup $S_t = e^{tA}$ in the mild solution?

## Architecture Onboarding

- **Component map**: Noise preprocessing -> Wick feature computation -> Neural operator backbone -> Reconstruction layer -> Training
- **Critical path**: Wick feature computation must correctly implement chaos basis for specific noise type. Errors here propagate through entire pipeline and are not recoverable by training.
- **Design tradeoffs**:
  - Truncation orders $(I, J, K)$: Higher orders capture more stochastic moments but increase feature dimension
  - Temporal basis: Haar is simple; Fourier/wavelet may be more efficient for smooth solutions
  - Backbone capacity: Deeper networks increase expressive power but risk overfitting
  - Noise observability: Framework assumes observed noise paths; latent noise requires additional inference
- **Failure signatures**:
  - High training error but low validation error: Check Wick feature computation
  - Error dominated by first-order chaos modes: May indicate poor backbone approximation of linear dynamics
  - Error increases with Hermite order: May indicate numerical instability or insufficient solution regularity
  - Extrapolation fails: Model may be overfitting to training noise distribution
- **First 3 experiments**:
  1. Reproduce $\Phi^4_1$ benchmark with F-SPDENO, verifying relative L2 error matches reported values
  2. Implement U-SDENO for 1D OU process, comparing one-step prediction against Euler–Maruyama integration
  3. Add new SPDE (e.g., stochastic heat equation), generating data and comparing against analytical chaos coefficients

## Open Questions the Paper Calls Out

### Open Question 1
Can the WCE-based neural operator framework be effectively extended to truly singular SPDEs (e.g., $\Phi^4_2$) that require renormalisation? The authors state this extension is left for future work, as current evaluation focuses on non-singular dynamics where low-order truncation suffices. Evidence would be successful application to singular SPDE benchmarks handling renormalisation and high-order chaos interactions.

### Open Question 2
How can the model be adapted for settings where driving noise trajectory is latent or partially observed rather than explicitly available? The limitations section notes this assumption and suggests extending SDENO to latent noise settings. Evidence would be a modified framework combining chaos representation with inference techniques to estimate solution operators from noisy solution observations alone.

### Open Question 3
Can the "global flattening" approach for manifold SDEs be generalized to complex Riemannian manifolds without distortion from parallel transport? The authors identify limitations of single global reference frame and state extending to intrinsic constructions is left for future work. Evidence would be development of intrinsic, manifold-native Wiener–chaos construction avoiding path-dependent transport distortion.

## Limitations
- Framework assumes full observability of driving noise paths, limiting applicability when noise is latent
- Computational complexity scales exponentially with Hermite order and temporal modes, challenging for high-dimensional problems
- Validated primarily on synthetic benchmarks and controlled image generation tasks, not real-world SPDE applications

## Confidence
- **High confidence**: Mathematical foundation connecting Wiener chaos to deterministic propagators (Theorems 1-2) and experimental results on standard SPDE benchmarks
- **Medium confidence**: Generalization claims to diverse domains based on single experiments per domain and extrapolation performance on Heston model
- **Low confidence**: Practical scalability to very high-dimensional problems and performance when only solution trajectories are available

## Next Checks
1. **Noise inference capability**: Design experiment with only solution trajectories observed, implementing variational inference framework to jointly learn noise reconstruction and chaos coefficient prediction.

2. **Singular SPDE extension**: Apply framework to a singular SPDE requiring renormalization (e.g., dynamical Φ⁴₃ in 3D), investigating whether standard Wick-Hermite expansions need modification.

3. **Domain generalization**: Test learned operators across different domains using transfer learning, training on one spatial discretization and evaluating on another to measure discretization-invariance.