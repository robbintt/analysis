---
ver: rpa2
title: Exploring State Tracking Capabilities of Large Language Models
arxiv_id: '2511.10457'
source_url: https://arxiv.org/abs/2511.10457
tags:
- person
- position
- state
- moves
- swap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates state tracking capabilities of large language
  models (LLMs) on three carefully designed tasks: LinearWorld (tracking entity positions
  after swaps), HandSwap (tracking object exchanges), and Lights (tracking light states
  after switch presses). The authors assess performance across different depths of
  update sequences, comparing models with and without Chain-of-Thought (CoT) prompting.'
---

# Exploring State Tracking Capabilities of Large Language Models

## Quick Facts
- **arXiv ID**: 2511.10457
- **Source URL**: https://arxiv.org/abs/2511.10457
- **Reference count**: 40
- **Primary result**: Chain-of-Thought prompting significantly improves LLM state tracking performance, with newer models maintaining accuracy at higher sequence depths

## Executive Summary
This paper investigates how well large language models can track and update states through sequential operations. The authors evaluate models on three carefully designed tasks: LinearWorld (tracking entity positions after swaps), HandSwap (tracking object exchanges), and Lights (tracking light states after switch presses). The study systematically examines performance across different depths of update sequences and compares models with and without Chain-of-Thought prompting.

Results demonstrate that newer models like GPT-4o and Llama3-70B with CoT can maintain accurate state tracking even at higher depths, while older models (GPT-3.5, Mixtral) and smaller models (Llama3-8B) show significant degradation after only 2-3 steps. CoT substantially improves performance by allowing models to use their input window as temporary memory, revealing that LLMs struggle with maintaining state through sequential updates unless provided explicit reasoning mechanisms.

## Method Summary
The authors designed three state tracking tasks to evaluate LLM capabilities. LinearWorld involves tracking entity positions on a 1D line after swap operations, HandSwap tracks object exchanges between hands, and Lights tracks light states after switch presses. Each task was tested across multiple sequence depths (1-5 steps). The evaluation compared multiple models including GPT-4o, GPT-3.5, Llama3-70B, Llama3-8B, and Mixtral, both with and without Chain-of-Thought prompting. Models were prompted to predict final states after sequential updates, with CoT allowing step-by-step reasoning.

## Key Results
- Newer models (GPT-4o, Llama3-70B) with CoT maintain >80% accuracy even at depth 5, while older models degrade to <50% accuracy after 2-3 steps
- Chain-of-Thought prompting improves performance by 20-40 percentage points across all models and tasks
- Smaller models (Llama3-8B) show consistent performance degradation with depth, unable to maintain accuracy beyond 2-3 steps
- State tracking accuracy correlates strongly with model size and recency, but CoT enables smaller models to partially compensate

## Why This Works (Mechanism)
LLMs struggle with state tracking because they lack persistent memory across sequential operations. Without explicit reasoning mechanisms, models must compress and retain state information in their context window while processing subsequent updates. Chain-of-Thought prompting works by externalizing intermediate reasoning steps, effectively using the input window as temporary memory. This allows models to reference previous states and update them systematically rather than attempting to maintain everything internally.

## Foundational Learning
- **State representation**: How entities/objects and their properties are encoded and tracked (needed for understanding task complexity; quick check: can the model correctly represent initial states)
- **Sequential update mechanisms**: How models process and apply transformations to existing states (needed for evaluating reasoning capabilities; quick check: can the model correctly apply single-step updates)
- **Memory constraints**: The limitations of transformer context windows for maintaining information across multiple steps (needed for understanding performance degradation; quick check: does accuracy decrease with sequence length)
- **Reasoning decomposition**: The process of breaking complex multi-step problems into manageable intermediate steps (needed for understanding CoT benefits; quick check: does CoT improve accuracy on multi-step problems)
- **Task abstraction**: The ability to generalize from specific examples to broader state tracking principles (needed for evaluating true understanding; quick check: performance consistency across task variations)
- **Context window utilization**: How models leverage available context for intermediate storage and reference (needed for understanding CoT mechanism; quick check: accuracy difference with and without CoT)

## Architecture Onboarding
**Component map**: Input prompt -> Token embedding -> Transformer layers -> Attention mechanisms -> Output prediction
**Critical path**: State initialization -> Sequential updates -> Final state prediction (with CoT: intermediate reasoning steps inserted between updates)
**Design tradeoffs**: Context window size vs. depth of reasoning, model size vs. computational efficiency, explicit reasoning vs. implicit understanding
**Failure signatures**: Performance degradation with sequence depth, confusion between entities, incorrect state transitions, CoT failures when reasoning becomes too complex
**3 first experiments**:
1. Test single-step state tracking accuracy to establish baseline capability
2. Evaluate performance with varying context window sizes to identify memory limitations
3. Compare different reasoning strategies (CoT vs. direct prediction) on the same tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Tasks use simplified, discrete state spaces that may not reflect real-world complexity
- Evaluation focuses on small state spaces (5 positions, 3 objects) rather than practical applications
- Analysis doesn't explore alternative memory augmentation techniques beyond CoT
- Performance differences between model families may reflect training data or instruction tuning rather than pure state tracking capability

## Confidence
- **High**: CoT substantially improves state tracking performance; models degrade with sequence depth without explicit reasoning
- **Medium**: Relative performance differences between specific model families (GPT-4o vs GPT-3.5, Llama3-70B vs Llama3-8B)

## Next Checks
1. Test state tracking performance on larger state spaces (10+ entities/objects) to assess scalability limitations
2. Evaluate alternative memory augmentation approaches (external memory, iterative refinement) beyond Chain-of-Thought
3. Compare performance across different prompt engineering strategies (few-shot examples, structured output formats) to isolate the specific benefits of CoT