---
ver: rpa2
title: Language Models to Support Multi-Label Classification of Industrial Data
arxiv_id: '2504.15922'
source_url: https://arxiv.org/abs/2504.15922
tags:
- requirements
- classifier
- performance
- classification
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of zero-shot learning
  for multi-label requirements classification using large language models (LLMs) and
  smaller language models (LMs). The problem of taxonomic-trace links, which involves
  assigning multiple labels from a large hierarchical taxonomy to natural language
  requirements, is particularly challenging when labeled data is scarce.
---

# Language Models to Support Multi-Label Classification of Industrial Data

## Quick Facts
- arXiv ID: 2504.15922
- Source URL: https://arxiv.org/abs/2504.15922
- Reference count: 40
- Primary result: T5-xl outperformed other models in 5 out of 6 output spaces, achieving a maximum Fβ of 0.78 and Dn of 0.04 in zero-shot multi-label requirements classification

## Executive Summary
This study investigates zero-shot learning for multi-label requirements classification using large language models (LLMs) and smaller language models (LMs). The research addresses the challenge of taxonomic-trace links, where multiple labels from a large hierarchical taxonomy must be assigned to natural language requirements, particularly when labeled data is scarce. The authors evaluated 14 state-of-the-art LMs and LLMs, including BERT, T5, Llama2, and Mistral, using a dataset of 377 requirements annotated with 1968 labels across 6 output spaces. A novel label distance metric (Dn) was introduced to better capture the hierarchical nature of the classification task.

The results demonstrate that T5-xl outperformed other models in 5 out of 6 output spaces, with smaller LMs generally outperforming larger LLMs, making them more practical for real-world applications. Model architecture significantly influenced performance, with sequence-to-sequence and autoencoding models performing better than autoregressive models. These findings highlight the potential of zero-shot learning for requirements classification and emphasize the importance of selecting appropriate models and metrics for industrial applications.

## Method Summary
The study employed a comprehensive evaluation framework to compare 14 state-of-the-art language models for multi-label requirements classification. The dataset consisted of 377 requirements from the Automotive Software Performance Improvement and Capability dEtermination (ASPICE) framework, annotated with 1968 labels across 6 different output spaces representing various hierarchical levels of the taxonomy. The evaluation used a synthetic zero-shot setting where models were prompted without fine-tuning. A novel label distance metric (Dn) was introduced to capture hierarchical relationships between predicted and actual labels. Performance was measured using standard metrics including Fβ-score alongside the new Dn metric to assess both exact matches and hierarchical proximity of predictions.

## Key Results
- T5-xl achieved the highest performance, outperforming other models in 5 out of 6 output spaces with Fβ of 0.78 and Dn of 0.04
- Smaller language models consistently outperformed larger LLMs across most output spaces, challenging the assumption that bigger models are always better
- Model architecture significantly impacted performance, with sequence-to-sequence and autoencoding models showing superior results compared to autoregressive models

## Why This Works (Mechanism)
The effectiveness of smaller models in this zero-shot multi-label classification task stems from their ability to generalize from prompt patterns without the overhead of larger model architectures. The hierarchical nature of the taxonomy allows for partial credit when predictions are semantically close but not exact, which the novel Dn metric captures effectively. Sequence-to-sequence models like T5-xl excel because they can generate multiple label tokens in a single pass, while autoencoding models like BERT can effectively encode contextual information from requirements text. The zero-shot approach works because the prompt engineering provides sufficient context for models to map requirements to appropriate taxonomy nodes without requiring task-specific fine-tuning.

## Foundational Learning
- **Zero-shot learning**: Why needed - Enables classification without labeled training data; Quick check - Model can generate relevant labels from prompt alone
- **Multi-label classification**: Why needed - Requirements often map to multiple taxonomy nodes simultaneously; Quick check - Output contains multiple distinct labels per input
- **Hierarchical taxonomies**: Why needed - Industrial requirements follow structured classification schemes; Quick check - Parent-child relationships exist between labels
- **Label distance metrics**: Why needed - Standard metrics don't capture semantic proximity in hierarchies; Quick check - Dn metric rewards near-misses in taxonomy tree
- **Model architecture types**: Why needed - Different architectures handle sequence generation differently; Quick check - Performance varies significantly by architecture family
- **Prompt engineering**: Why needed - Guides model behavior without fine-tuning; Quick check - Well-crafted prompts improve zero-shot performance

## Architecture Onboarding

Component map: Requirements text -> Model architecture -> Label generation -> Hierarchical evaluation -> Performance metrics

Critical path: The model architecture selection and prompt engineering represent the most critical decisions, as they directly determine the quality of label generation. The hierarchical evaluation step is crucial for assessing practical utility beyond simple exact match metrics.

Design tradeoffs: The study balances model size against inference efficiency, finding that smaller models often provide better practical utility. The choice between autoregressive, autoencoding, and sequence-to-sequence architectures represents a fundamental tradeoff between generation flexibility and contextual understanding. The synthetic zero-shot evaluation prioritizes generalizability over task-specific optimization.

Failure signatures: Models may fail by generating labels outside the taxonomy, producing duplicate labels, or missing critical parent-child relationships in the hierarchy. Autoregressive models may struggle with multi-label generation due to sequential dependencies. Poor prompt engineering can lead to irrelevant or overly generic label predictions.

3 first experiments:
1. Compare T5-xl performance across different prompt formulations to optimize zero-shot accuracy
2. Test smaller LMs against fine-tuned versions of the same models to quantify zero-shot efficiency gains
3. Evaluate the Dn metric against human annotators to validate its effectiveness in capturing hierarchical relationships

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The relatively small dataset of 377 requirements with 1968 labels may not capture the full complexity of industrial requirements classification scenarios
- Results are limited to the ASPICE framework taxonomy, potentially limiting generalizability to other domains or taxonomy structures
- Evaluation relies on synthetic zero-shot settings rather than demonstrating practical utility in real-world industrial applications with domain expert interaction

## Confidence

**High confidence:**
- The general finding that model architecture (sequence-to-sequence vs. autoregressive) significantly impacts performance

**Medium confidence:**
- Specific model rankings and performance metrics within the studied taxonomy
- The practical recommendation favoring smaller models for industrial applications

## Next Checks
1. Test the proposed approach on larger industrial datasets with varying taxonomy depths and label distributions to assess scalability and robustness
2. Conduct human evaluation studies with domain experts to measure practical utility and error tolerance in real industrial settings
3. Compare the novel Dn metric against established hierarchical classification metrics across multiple domains to validate its general applicability