---
ver: rpa2
title: Unifying Speech Editing Detection and Content Localization via Prior-Enhanced
  Audio LLMs
arxiv_id: '2601.21463'
source_url: https://arxiv.org/abs/2601.21463
tags:
- speech
- editing
- audio
- detection
- localization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting and localizing
  speech editing in audio content, where neural editing techniques create highly seamless
  forgeries that are difficult to identify using existing methods. To tackle this,
  the authors construct AiEdit, a large-scale bilingual dataset featuring diverse
  editing operations (addition, deletion, modification) generated using advanced neural
  speech editing models and LLM-driven semantic tampering.
---

# Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs

## Quick Facts
- arXiv ID: 2601.21463
- Source URL: https://arxiv.org/abs/2601.21463
- Reference count: 33
- Primary result: PELM achieves 0.57% EER on HumanEdit and 9.28% EER (localization) on AiEdit, outperforming state-of-the-art methods for speech editing detection and content localization.

## Executive Summary
This paper addresses the challenge of detecting and localizing speech editing in audio content, where neural editing techniques create highly seamless forgeries that are difficult to identify using existing methods. To tackle this, the authors construct AiEdit, a large-scale bilingual dataset featuring diverse editing operations (addition, deletion, modification) generated using advanced neural speech editing models and LLM-driven semantic tampering. They propose PELM (Prior-Enhanced Audio Large Language Model), a framework that reformulates speech editing detection and localization as an audio question answering task. PELM incorporates word-level probability priors to inject acoustic evidence and an acoustic consistency-aware loss to enforce subtle anomaly detection. Extensive experiments show that PELM significantly outperforms state-of-the-art methods, achieving 0.57% EER on HumanEdit and 9.28% EER (localization) on AiEdit, while also demonstrating strong robustness across model scales and architectural variations.

## Method Summary
PELM reformulates speech editing detection and localization as an audio question answering task, where an Audio LLM processes audio input along with structured prior information to generate structured text outputs indicating whether editing was detected and, if so, which words were edited. The method incorporates word-level probability priors derived from a frame-level detector (BAM with WavLM backbone) to provide explicit acoustic cues, and an acoustic consistency-aware loss that enforces modeling of subtle local distribution anomalies. The model is trained using cross-entropy loss combined with the acoustic consistency loss (λ=0.5), with full fine-tuning on the last three audio encoder layers and LoRA fine-tuning elsewhere. The framework is evaluated on the AiEdit dataset (59,554 samples) and benchmarked against HumanEdit, demonstrating superior performance in both detection and localization tasks.

## Key Results
- PELM achieves 0.57% EER on HumanEdit benchmark, significantly outperforming existing methods
- PELM achieves 9.28% EER for localization on AiEdit dataset, demonstrating precise word-level editing identification
- Ablation studies show prior injection and acoustic consistency loss each contribute substantially to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Word-level probabilistic priors derived from frame-level detectors calibrate Audio LLM reasoning boundaries and reduce forgery bias.
- **Mechanism:** A pre-trained frame-level detector (BAM with WavLM backbone) generates per-frame tampering probabilities, which are aggregated using Montreal Forced Aligner word boundaries into word-level probability sequences. These sequences are injected as structured prior prompts (format: "Word:Probability") into the LLM input, providing explicit acoustic evidence alongside audio tokens.
- **Core assumption:** Frame-level acoustic anomalies correlate with word-level editing boundaries; the prior detector's predictions are sufficiently accurate to guide high-level reasoning without introducing systematic error.
- **Evidence anchors:**
  - [abstract] "PELM incorporates word-level probability priors to provide explicit acoustic cues"
  - [Section 4.2] Details prior construction: frame-wise probabilities extracted by detector D, aggregated over word boundaries to obtain sequence p, encoded into structured prior prompt Qp
  - [Table 4] Ablation shows prior alone achieves 99.80 AUC on HumanEdit vs 83.44 without any enhancement
  - [corpus] Limited direct evidence; related work "Towards Explicit Acoustic Evidence Perception in Audio LLMs" addresses similar acoustic bias issues but uses different prior construction methods
- **Break condition:** If frame-level detector accuracy degrades significantly on unseen editing models or languages, prior quality may mislead rather than calibrate the LLM.

### Mechanism 2
- **Claim:** Centroid-aggregation acoustic consistency loss forces the audio encoder to capture local distribution anomalies by asymmetrically constraining bona fide and edited feature distributions.
- **Mechanism:** Extract audio features from the encoder's last layer, compute a centroid (mean of all frame features), and apply asymmetric optimization: (1) for bona fide audio, minimize all frame-to-centroid distances plus worst-case deviation (cohesion constraint); (2) for edited audio, maximize separation between centroid and top-K% most deviant frames (dispersion constraint). This amplifies subtle acoustic discontinuities that neural editing leaves behind.
- **Core assumption:** Neural speech editing, despite seamless perceptual transitions, introduces measurable distributional shifts in learned feature space; bona fide speech exhibits temporal acoustic consistency that edited speech violates locally.
- **Evidence anchors:**
  - [abstract] "designs a centroid-aggregation-based acoustic consistency perception loss to explicitly enforce the modeling of subtle local distribution anomalies"
  - [Section 4.3] Formal definition: L_audio applies cohesion constraint to bona fide (minimize distances) and dispersion constraint to edited (enlarge top-K deviations beyond margin)
  - [Figure 2] Self-attention heatmaps show diffuse attention under zero-shot/SFT vs concentrated attention on anomalous regions with PELM
  - [corpus] No direct corpus evidence for this specific loss design; DSA-Tokenizer paper addresses semantic-acoustic disentanglement but via tokenization, not consistency loss
- **Break condition:** If edited audio exhibits centroid deviations smaller than the margin threshold (0.9 per Table 10), dispersion constraint provides no gradient signal; tuning margin and top-K is dataset-sensitive.

### Mechanism 3
- **Claim:** Formulating detection and localization as structured Audio Question Answering enables unified reasoning with precise output parsing.
- **Mechanism:** Given audio A, instruction Q, and prior Qp, the model generates natural language responses following strict templates: "No evidence of speech editing was detected" for authentic audio, or "Yes, '<exact words>' was [added/deleted/modified] in speech" for edits. This constrains the output space for reliable evaluation while leveraging LLM generative capabilities.
- **Core assumption:** The base Audio LLM (Qwen2.5-Omni) possesses sufficient cross-modal alignment to map acoustic tokens to linguistic spans; structured outputs can be reliably parsed for metric computation.
- **Evidence anchors:**
  - [Section 4.1] Task formulation: y = M(A, Q, Qp) with strict output format constraints for standardized evaluation
  - [Table 3] Prompt strategy ablation shows detailed instructions with strict format constraints achieve best performance (95.2% detection accuracy on AiEdit vs 91.48% with generic prompts)
  - [Section 5.4] Qwen2.5-Omni series outperforms Qwen2-Audio, attributed to stronger pre-training generalization and multimodal instruction alignment
  - [corpus] Ming-UniAudio paper demonstrates unified speech LLMs for understanding, generation, and editing, supporting feasibility of instruction-guided audio reasoning
- **Break condition:** If exact word extraction fails due to transcription errors or tokenization mismatches between audio and text representations, localization accuracy degrades independent of detection capability.

## Foundational Learning

- **Neural Speech Editing Paradigms**
  - **Why needed here:** AiEdit samples are generated using three categories: end-to-end reconstruction (SSR, VoiceCraft), region-aware inpainting (FluentSpeech), and instruction-driven LLM-based editing (Ming-UniAudio). Understanding these informs why traditional splicing-artifact detectors fail.
  - **Quick check question:** Can you explain why FluentSpeech requires an alignment matching algorithm to resolve index offset issues when generating edits?

- **Forgery Bias and Semantic-Priority Bias in Audio LLMs**
  - **Why needed here:** The paper explicitly targets these two biases. Forgery bias causes over-conservative predictions (misclassifying low-quality bona fide as fake); semantic-priority bias causes the model to attend to linguistic content while ignoring acoustic anomalies.
  - **Quick check question:** How does RLHF contribute to forgery bias, according to Section 2.2?

- **LoRA Fine-tuning Mechanics**
  - **Why needed here:** The architecture uses hybrid fine-tuning: full-parameter tuning on last 3 encoder layers + projection layer, with LoRA elsewhere. Understanding rank (r), alpha (α), and their interaction is necessary to interpret Table 3 ablations.
  - **Quick check question:** What is the relationship between LoRA rank r=32, alpha α=64, and why does this configuration outperform r=16, α=16 on AiEdit?

## Architecture Onboarding

- **Component map:** Input Audio → WavLM-based Frame Detector → Frame Probabilities → (aggregated via MFA word boundaries) → Audio Encoder (Qwen2.5-Omni, last 3 layers FT) → Word-level Prior Prompt Qp → Audio Tokens + Instruction Q + Prior Qp → LLM Backbone (LoRA) → Structured Text Output → Parsed Detection + Localization

- **Critical path:**
  1. Prior quality: Frame detector accuracy directly impacts word-level prior reliability
  2. Acoustic feature extraction: Last 3 layers of audio encoder must capture subtle distributional anomalies
  3. Prior-to-text alignment: Prior prompt must be formatted consistently with LLM tokenization expectations
  4. Loss balancing: λ=0.5; margin=0.9; top-k=0.1 for acoustic consistency constraint

- **Design tradeoffs:**
  - **Fine-tuning scope:** Full encoder FT (11.19% params, +1.4GB VRAM) achieves marginally better performance than last-3-layer FT (2.27% params), but with diminishing returns
  - **LoRA rank:** r=32/α=64 balances expressiveness and stability; lower ranks underfit complex acoustic patterns
  - **Prompt complexity:** Detailed prompts with strict format constraints improve performance but increase inference latency and parsing brittleness

- **Failure signatures:**
  - **High EER on AiEdit with low EER on HumanEdit:** Model overfitting to splicing artifacts; increase acoustic consistency loss weight or training diversity across editing models
  - **Localization accuracy >> Detection accuracy:** Semantic drift; check if prior injection is working (verify prior prompt appears in input)
  - **Over-prediction of forgeries:** Forgery bias not mitigated; verify prior calibration—ensure frame detector not producing systematically high probabilities

- **First 3 experiments:**
  1. **Baseline reproduction:** Run zero-shot Qwen2.5-Omni-3B on HumanEdit/AiEdit test splits without prior or acoustic loss to establish forgery bias baseline (expect ~35% EER on AiEdit per Table 4)
  2. **Prior-only ablation:** Enable prior injection without acoustic consistency loss; verify attention heatmap concentration on edited regions (compare to Figure 2 pattern)
  3. **Full PELM with hyperparameter sweep:** Train with λ ∈ {0.3, 0.5, 0.7} and top-k ∈ {0.05, 0.1, 0.2}; log EER separately for each editing model type (SSR vs FluentSpeech vs Ming-UniAudio) to identify model-specific robustness gaps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Audio LLM be trained to overcome "semantic-priority bias" without relying on the external frame-level probabilistic prior, or is this prior strictly necessary for fine-grained localization?
- **Basis in paper:** [inferred] Table 4 shows a drastic performance drop (EER increasing from 0.55% to 23.90% on HumanEdit) when the prior is removed, suggesting the LLM struggles to focus on acoustic details independently.
- **Why unresolved:** It is unclear if this failure is due to the inherent architecture of Audio LLMs or simply insufficient pre-training data, leaving open the possibility that a sufficiently scaled model might internalize these acoustic cues.
- **What evidence would resolve it:** An analysis of the "No Prior" model's attention maps to see if it attends to *any* acoustic regions, or if it fails to look at the audio signal entirely for localization tasks.

### Open Question 2
- **Question:** To what extent does PELM generalize to speech editing algorithms not represented in the AiEdit training split (specifically A3T and SSR)?
- **Basis in paper:** [inferred] Table 8 indicates that while the test set includes A3T and SSR models, the training split uses only FluentSpeech, Ming, PlayDiffusion, and VoiceCraft.
- **Why unresolved:** The aggregate results on the AiEdit test set mask the model's specific performance on the unseen editing architectures (A3T and SSR), leaving their zero-shot detection capability unquantified.
- **What evidence would resolve it:** A breakdown of detection and localization Equal Error Rates (EER) specifically for the A3T and SSR subsets of the test data.

### Open Question 3
- **Question:** How does PELM differentiate between neural reconstruction artifacts and acoustic distortions caused by standard audio compression (e.g., MP3, Opus) in real-world communication scenarios?
- **Basis in paper:** [explicit] The Introduction explicitly states that reconstruction artifacts in neural editing are "acoustically highly similar to the compression distortions encountered in real-world communication scenarios."
- **Why unresolved:** The experimental evaluation is conducted on 16kHz WAV files (Section 3.1), leaving the specific vulnerability to "compression vs. forgery" confusion mentioned in the motivation untested.
- **What evidence would resolve it:** Evaluating the False Positive Rate (FPR) of the model on bona fide speech samples that have been subjected to standard lossy compression codecs.

## Limitations

- **Detector dependency:** Performance relies heavily on the accuracy of the frame-level detector for prior generation, which may not generalize to all editing paradigms
- **Dataset specificity:** The AiEdit dataset, while large and diverse, may not fully capture the range of real-world audio tampering scenarios
- **Hyperparameter sensitivity:** The acoustic consistency loss requires careful tuning of λ, margin, and top-k parameters, which may not transfer well across datasets

## Confidence

- **High Confidence:** The unified AudioQA formulation and its impact on structured output parsing (Mechanism 3). The ablation studies in Table 3 provide clear evidence that detailed prompt strategies with strict format constraints significantly improve performance.
- **Medium Confidence:** The effectiveness of word-level probability priors in mitigating forgery bias (Mechanism 1). While Table 4 shows strong results, the prior's performance is contingent on the frame detector's reliability, which is not independently evaluated across diverse editing models.
- **Medium Confidence:** The centroid-aggregation acoustic consistency loss's role in enforcing subtle anomaly detection (Mechanism 2). The mechanism is well-defined, but the specific hyperparameters (λ=0.5, margin=0.9, topk=0.1) appear tuned to AiEdit, and their robustness across datasets is not demonstrated.

## Next Checks

1. **Frame Detector Robustness Test:** Evaluate the BAM detector's frame-level accuracy on a held-out set of edits generated by models not seen during AiEdit construction (e.g., newer VoiceClone variants). Correlate detector degradation with PELM's prior-calibration effectiveness.
2. **Cross-Dataset Generalization:** Fine-tune PELM on AiEdit and evaluate on a separate, real-world dataset of human-annotated speech edits (if available) to assess overfitting to synthetic tampering patterns.
3. **Loss Ablation with Varied Hyperparameters:** Systematically sweep the acoustic consistency loss weight (λ) and top-k threshold across a broader range (e.g., λ ∈ [0.1, 1.0], top-k ∈ [0.05, 0.5]) to identify the sensitivity of performance to these critical hyperparameters.