---
ver: rpa2
title: Geometric Median Matching for Robust k-Subset Selection from Noisy Data
arxiv_id: '2504.00564'
source_url: https://arxiv.org/abs/2504.00564
tags:
- matching
- corruption
- data
- samples
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Geometric Median Matching for Robust k-Subset Selection from Noisy\
  \ Data Anish Acharya et al. propose GM Matching, a novel k-subset selection method\
  \ that leverages the geometric median\u2014a robust estimator with an optimal breakdown\
  \ point of 1/2\u2014to enhance resilience against noisy data."
---

# Geometric Median Matching for Robust k-Subset Selection from Noisy Data

## Quick Facts
- arXiv ID: 2504.00564
- Source URL: https://arxiv.org/abs/2504.00564
- Reference count: 40
- Key outcome: Geometric Median Matching for Robust k-Subset Selection from Noisy Data

## Executive Summary
This paper proposes GM Matching, a novel k-subset selection method that leverages the geometric median—a robust estimator with an optimal breakdown point of 1/2—to enhance resilience against noisy data. Existing data pruning methods often fail under high corruption rates due to their reliance on empirical mean estimation, which is highly sensitive to outliers. GM Matching iteratively selects a k-subset such that the mean of the subset approximates the geometric median of the (potentially) noisy dataset, ensuring robustness even under arbitrary corruption. The method provides theoretical guarantees, showing that GM Matching enjoys an improved O(1/k) convergence rate—a quadratic improvement over random sampling, even under arbitrary corruption.

## Method Summary
GM Matching is a k-subset selection method that iteratively selects samples to match the geometric median of the dataset in a Reproducing Kernel Hilbert Space. The method uses a pre-trained proxy encoder to map inputs to embeddings, computes the geometric median via the Weiszfeld algorithm on a sub-sample, then greedily selects samples that maximize alignment with the current residual vector. This creates a robust selection process that remains effective even when up to 50% of the data is corrupted.

## Key Results
- GM Matching achieves O(1/k) convergence rate—quadratic improvement over random sampling even under arbitrary corruption
- Outperforms existing pruning approaches in high-corruption settings and at high pruning rates across image classification and generation tasks
- Maintains optimal 50% breakdown point for robustness, unlike empirical mean methods which fail immediately with outliers

## Why This Works (Mechanism)

### Mechanism 1: Robust Target via Geometric Median
Replacing the empirical mean with the Geometric Median prevents the subset selection target from drifting arbitrarily, provided corruption is below 50%. The Geometric Median minimizes the sum of distances (L1 norm) rather than squared distances (L2 norm), preventing outliers from exerting disproportionate influence.

### Mechanism 2: Greedy Moment Matching (Herding)
Iteratively selecting samples that align with the current residual vector allows the subset mean to converge to the GM at a rate of O(1/k). The algorithm maintains a direction vector θt representing the discrepancy between the target (GM) and the current subset sum, selecting the sample maximizing the inner product with θt.

### Mechanism 3: Proxy Encoder Generalization
Using a pre-trained encoder to map raw inputs allows the method to select robust subsets even when the downstream model architecture differs. The method operates on embeddings in a Reproducing Kernel Hilbert Space where semantic similarity is linear, enabling pruning in a universal space.

## Foundational Learning

- **Concept: Breakdown Point (Robust Statistics)**
  - Why needed: To understand why the empirical mean fails (breakdown = 0) and why GM works (breakdown = 0.5) under corruption
  - Quick check: If 1% of your data is arbitrarily large outliers, does the mean or the median change more significantly?

- **Concept: Kernel Herding**
  - Why needed: The paper frames GM Matching as a robust generalization of Kernel Herding
  - Quick check: In herding, does selecting the sample closest to the current mean or the sample that maximizes the residual error lead to faster convergence?

- **Concept: Characteristic Kernels / RKHS**
  - Why needed: The method relies on the "kernel trick" to perform median calculations in a high-dimensional space where distributions are separable
  - Quick check: Why must the mapping p → μp be injective to ensure that matching the mean embedding implies matching the distribution?

## Architecture Onboarding

- **Component map**: Input -> Proxy Encoder -> GM Estimator -> Selector -> Output Subset Indices
- **Critical path**: The Weiszfeld algorithm loop (computing distances to current estimate) is the computational bottleneck for large n or high embedding dimension s
- **Design tradeoffs**:
  - Sub-sampling (γGM): Computing GM on 100% of data is expensive; the paper suggests sub-sampling (e.g., 50%) for speed, trading off some estimation accuracy
  - Batch size (B): Larger batches speed up wall-clock time via parallelism but introduce a convergence penalty of O(B/k)
- **Failure signatures**:
  - Weiszfeld Stalling: If the GM estimate hits an actual data point, the gradient becomes undefined
  - Encoder Mismatch: If the proxy encoder fails to separate clean and corrupt data, GM Matching will select corrupted points
- **First 3 experiments**:
  1. Sanity Check (Synthetic): Generate an anisotropic Gaussian with 40% adversarial noise; verify that GM stays near the clean mean while the empirical mean drifts
  2. Robustness to Label Noise: Prune CIFAR-100 with 20% label noise; compare GM Matching vs. "Hard" and "Easy" baselines
  3. Transferability Test: Use a ResNet-50 proxy to select data, but train a VGG-16 on the selected subset

## Open Questions the Paper Calls Out

- Can scalable, structure-aware approximations of the geometric median be developed to improve efficiency in very high-dimensional settings?
- How can GM Matching be adapted to account for uncertainty within the proxy encoder's representation space?
- How does the geometric median computation behave in "degenerate" spatial configurations, and can the selection strategy be stabilized?

## Limitations

- Performance depends critically on the proxy encoder being well-calibrated and capturing task-relevant semantics
- Weiszfeld algorithm can be computationally intensive and may stall in degenerate scenarios
- Requires corruption fraction to remain strictly below 50%, but no practical guidance is provided for verifying this

## Confidence

- **High Confidence**: The improved O(1/k) convergence rate over random sampling
- **Medium Confidence**: The 50% breakdown point guarantee for geometric median
- **Low Confidence**: The claim that GM Matching "consistently outperforms" all baselines across all settings

## Next Checks

1. **Breakdown Point Verification**: Systematically vary corruption levels from 10% to 60% on synthetic data to empirically determine the point where GM Matching performance degrades relative to baselines

2. **Hyperparameter Sensitivity**: Test multiple values of Weiszfeld regularization δ (e.g., 10^-8, 10^-6, 10^-4) to assess impact on convergence stability and downstream accuracy

3. **Proxy Encoder Transferability**: Evaluate whether different proxy encoders (e.g., CLIP vs. ResNet vs. ViT) yield consistent subset selection quality when measured by downstream task performance on unseen architectures