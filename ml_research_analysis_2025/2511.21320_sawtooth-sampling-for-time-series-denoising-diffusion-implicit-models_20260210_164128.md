---
ver: rpa2
title: Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models
arxiv_id: '2511.21320'
source_url: https://arxiv.org/abs/2511.21320
tags:
- denoising
- data
- sawtooth
- sampling
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Sawtooth Sampler to accelerate denoising
  diffusion implicit models (DDIM) for time series generation, specifically for IMU-based
  human activity recognition (HAR) and climbing fall detection. Standard DDIM sampling
  reduces denoising steps from 3000 to 100 but struggles with high-frequency noise
  suppression in IMU data.
---

# Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models

## Quick Facts
- arXiv ID: 2511.21320
- Source URL: https://arxiv.org/abs/2511.21320
- Reference count: 7
- Macro F1-score of 0.97 achieved on HAR dataset with lowest-performing participant using DDIM-K2

## Executive Summary
This work introduces the Sawtooth Sampler to accelerate denoising diffusion implicit models (DDIM) for time series generation, specifically for IMU-based human activity recognition (HAR) and climbing fall detection. Standard DDIM sampling reduces denoising steps from 3000 to 100 but struggles with high-frequency noise suppression in IMU data. The Sawtooth Sampler resets the denoising trajectory at intermediate steps, periodically restarting the reverse process. Across 12 HAR participants, DDIM-K2 (Sawtooth with K=2) achieved a macro F1-score of 0.97 for the lowest-performing participant, outperforming real-data baselines and matching DDPM-3000 for most users. For the climbing dataset, Sawtooth variants improved geometric mean classification scores (0.884-0.913) over standard DDIM (0.770) and matched or exceeded baselines.

## Method Summary
The Sawtooth Sampler accelerates DDIM sampling by dividing a fixed budget of 100 denoising steps into K iterations. After each iteration completes (e.g., 50 steps when K=2), the variance scheduler resets to its initial state, causing the model to re-engage its noise estimation capacity on the partially-denoised signal. The method uses deterministic sampling (σ=0) throughout and was evaluated on HAR and climbing fall detection datasets. The approach achieves a 30x speed-up over DDPM-3000 while improving high-frequency noise suppression, with K=2 identified as the optimal configuration for the evaluated IMU datasets.

## Key Results
- DDIM-K2 achieved macro F1-score of 0.97 for lowest-performing HAR participant, outperforming real-data baselines
- Sawtooth variants improved geometric mean classification scores to 0.884-0.913 on climbing dataset vs 0.770 for standard DDIM
- 30x speed-up achieved, reducing generation time from 5.5 minutes to 11 seconds per 128 samples
- Excessive resampling (K≥10) reintroduced artifacts and degraded performance

## Why This Works (Mechanism)

### Mechanism 1: Periodic Trajectory Reset for Iterative Noise Refinement
Resetting the variance scheduler at intermediate denoising steps improves high-frequency noise suppression in time series data compared to single-pass DDIM. The Sawtooth Sampler divides 100 steps into K iterations, resetting the scheduler after each iteration to re-engage noise estimation capacity. This works because deterministic DDIM can continue meaningful denoising even when intermediate states deviate from standard normal distribution.

### Mechanism 2: Deterministic Non-Markovian Sampling for Speed
Setting σ=0 enables deterministic, faster sampling while preserving generation quality. DDIM reformulates diffusion as non-Markovian, making x_0 dependent only on initial state x_T when σ_τi=0. This eliminates stochastic sampling across thousands of steps while maintaining classification performance.

### Mechanism 3: High-Frequency Noise Attenuation Through Multi-Pass Denoising
Multiple passes through shortened denoising trajectories more effectively suppress high-frequency artifacts common in IMU sensor data. Each sawtooth reset provides a fresh denoising window, allowing the model to target residual high-frequency components. C-Opt GAK similarity metric shows local maxima aligned with each K reset point.

## Foundational Learning

- **DDPM Forward and Reverse Processes**: Understanding why baseline DDPM requires 3000 steps explains motivation for DDIM acceleration. Quick check: Can you sketch the forward diffusion process that gradually adds Gaussian noise, and explain why the reverse process requires many iterative denoising steps?

- **Variance Scheduling (β_t, α_t, ᾱ_t)**: The Sawtooth method explicitly resets the variance scheduler. Quick check: What does ᾱ_t represent in the diffusion process, and what constraint must hold when resetting the scheduler mid-sampling?

- **Markovian vs. Non-Markovian Diffusion**: DDIM's core innovation is reformulating diffusion as non-Markovian. Quick check: In a Markovian diffusion process, what does p(x_{t-1}|x_t) depend on, and how does DDIM's non-Markovian formulation change this dependency?

## Architecture Onboarding

- **Component map**: Pretrained DDPM backbone -> DDIM sampler wrapper -> Sawtooth loop controller -> Variance scheduler -> C-Opt GAK similarity metric -> Downstream CNN classifier
- **Critical path**: Load pretrained diffusion model weights → Sample initial noise x_T ~ N(0, I) → For each of K iterations: reset scheduler → run (100/K) DDIM steps → pass output to next iteration → Return final x_0 after K complete passes
- **Design tradeoffs**: K=1 (standard DDIM): fastest but poorest high-frequency suppression; K=2: optimal in paper—best F1 scores, maintained 30x speedup; K=5-10: diminishing returns; K=10 showed degraded performance
- **Failure signatures**: High-frequency artifacts visible in spectrograms with K=1 → increase to K=2; Classifier F1 drops significantly at K≥5 → excessive resampling reintroducing noise; Similarity scores decline after initial improvement → model lacking noise reference, reduce K
- **First 3 experiments**: 1) Replicate K-sweep (K∈{1,2,5,10}) on held-out data, plotting C-Opt GAK scores at each denoising step to confirm local maxima align with reset points. 2) Compare generation time and downstream classifier performance between DDPM-3000, DDIM-K1, and DDIM-K2 to validate 30x speedup claim on your hardware. 3) Visualize power spectral density of generated samples across K values to correlate high-frequency suppression with classification metrics.

## Open Questions the Paper Calls Out

- **Comparison with Xu et al.'s Restart Sampling**: The authors explicitly state a comparison with Xu et al.'s Restart approach would be valuable, as Sawtooth omits the additional forward steps and noise injection used in Restart, which may explain better IMU performance.

- **Noise Reference Hypothesis**: The authors hypothesize that for K=10, "the model lacks a meaningful noise reference and may estimate residuals that inadvertently amplify high frequency components," but provide no theoretical proof or ablation study to confirm this mechanism.

- **Generalization Beyond IMU Data**: The method is evaluated exclusively on IMU data, and it's unclear if the trajectory resetting is universally beneficial for all time series or if its success is tied to the specific noise profiles of IMU sensors.

## Limitations

- Architectural details of the DDPM backbone remain unspecified, creating uncertainty in reproducing baseline performance
- Climbing dataset is not publicly available, preventing independent verification of results
- C-Opt GAK similarity metric implementation details are not provided, affecting reproducibility of quality assessments

## Confidence

- **High Confidence**: The core mechanism of periodic scheduler reset is well-supported by empirical results showing local maxima in C-Opt GAK scores aligned with reset points
- **Medium Confidence**: Deterministic sampling approach shows consistent improvements across both HAR and climbing datasets, though optimal K range may be dataset-specific
- **Low Confidence**: Generalizability beyond IMU data remains uncertain, with performance gains potentially tied to specific sensor characteristics

## Next Checks

1. **Architecture Sensitivity Analysis**: Implement the Sawtooth Sampler across multiple U-Net depths (3, 4, 5 layers) and channel configurations (64, 128, 256) to quantify how architecture choices affect the K=2 optimal point and overall performance variance.

2. **Cross-Dataset Generalization**: Apply the Sawtooth method to a different time series modality (e.g., ECG, audio, or financial data) with known high-frequency noise characteristics. Compare the effective K range and performance gains against the IMU-specific results to assess generalizability.

3. **Spectral Analysis Validation**: Generate synthetic samples using DDIM-K1, DDIM-K2, and DDPM-3000, then compute and compare power spectral densities. Quantify the actual high-frequency noise reduction percentages and correlate with classification metric improvements to validate the proposed mechanism.