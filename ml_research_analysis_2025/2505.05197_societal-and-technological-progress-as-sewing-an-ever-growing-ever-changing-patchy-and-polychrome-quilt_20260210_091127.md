---
ver: rpa2
title: Societal and technological progress as sewing an ever-growing, ever-changing,
  patchy, and polychrome quilt
arxiv_id: '2505.05197'
source_url: https://arxiv.org/abs/2505.05197
tags:
- social
- systems
- quilt
- human
- norms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the \"Axiom of Rational Convergence\" in\
  \ AI alignment\u2014the assumption that rational agents will converge on a single\
  \ set of values under ideal conditions. Instead, the authors propose an \"appropriateness\
  \ framework\" that treats persistent disagreement as normal and designs for it through\
  \ four principles: contextual grounding, community customization, continual adaptation,\
  \ and polycentric governance."
---

# Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt

## Quick Facts
- arXiv ID: 2505.05197
- Source URL: https://arxiv.org/abs/2505.05197
- Reference count: 11
- Primary result: AI alignment should treat persistent disagreement as normal and design for conflict management through contextual grounding, community customization, continual adaptation, and polycentric governance.

## Executive Summary
This paper challenges the "Axiom of Rational Convergence"—the assumption that rational agents will converge on a single set of values under ideal conditions. Instead, it proposes an "appropriateness framework" that treats persistent disagreement as the normal case and designs AI systems around managing conflict rather than achieving moral unification. The framework draws on conflict theory, cultural evolution, and institutional economics to create AI systems that are context-sensitive, customizable by communities, continually adaptive through feedback, and governed through distributed authority structures. The authors argue this approach is more practical and socially stable than seeking universal alignment.

## Method Summary
The paper proposes a theoretical framework for AI alignment based on four principles: contextual grounding (providing rich situational information to AI systems), community customization (allowing communities to shape local norms), continual adaptation (ongoing feedback-based learning), and polycentric governance (distributed decision-making authority). Rather than attempting to encode universal values, the framework accepts that disagreements are enduring features of human society and focuses on building practical social technologies—conventions, norms, and institutions—that enable coexistence despite divergent values. The approach is presented as an alternative to monolithic AI systems that attempt to be universally applicable.

## Key Results
- The Axiom of Rational Convergence is treated as an optional and doubtful assumption that should be questioned in AI alignment design.
- Context-starved AI systems default to bland, lowest-common-denominator behavior that is functionally inadequate across diverse use cases.
- Polycentric governance structures are more robust against power concentration than monolithic control, even when individual agents have power-seeking incentives.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating persistent disagreement as basic and designing for conflict management produces more socially stable AI systems than pursuing value convergence.
- Mechanism: The framework replaces the goal of "moral unification" with "conflict management." By accepting that disagreements are enduring features rather than errors to correct, system designers focus on building practical social technologies (conventions, norms, institutions) that enable coexistence despite divergent values.
- Core assumption: The Axiom of Rational Convergence is optional and doubtful—rational agents do not necessarily converge on a single ethics even under ideal epistemic conditions.
- Evidence anchors:
  - [abstract] "The appropriateness framework treats persistent disagreement as the normal case and designs for it by applying four principles"
  - [section] "We proceed from the core assumption that stable human coexistence... is made possible not by achieving rational convergence on values, but by relying on practical social technologies—like conventions, norms, and institutions—to manage conflict"
  - [corpus] Neighbor papers on adaptive AI governance (arXiv:2504.00652) and societal alignment frameworks (arXiv:2503.00069) discuss regional/value variation but do not directly test conflict management vs. convergence outcomes
- Break condition: If empirical evidence emerges that rational agents do converge on shared values under specifiable conditions, the framework's premise weakens

### Mechanism 2
- Claim: Context-starved AI systems default to bland, lowest-common-denominator behavior that is functionally inadequate across diverse use cases.
- Mechanism: Without access to contextual information (stakeholders, social roles, cultural practices), AI systems cannot behave appropriately. Designers must choose defaults safe for the most restrictive context, which produces "corporate-speak" that fails in contexts requiring empathy, creativity, or expertise.
- Core assumption: Appropriateness is context-dependent; a single behavioral standard cannot be optimal across all contexts even with identical cognitive capabilities.
- Evidence anchors:
  - [section] "Defaulting to blandness is a symptom of the monolithic mindset. We think that a more pluralistic and decentralized ecosystem would offer a path beyond blandness"
  - [section] "Crucially, this is not a point about AI capability generality... the actual requirement of having a single system that operates in every context would entail bland LLM-speak"
  - [corpus] No direct corpus evidence tests this mechanism; related work on LLM alignment (arXiv:2503.00069) notes alignment challenges but doesn't compare monolithic vs. pluralistic architectures
- Break condition: If a single system can reliably infer or request context without privacy violations or user burden, the decentralization argument weakens

### Mechanism 3
- Claim: Polycentric governance structures are more robust against power concentration than monolithic control, even when individual agents have power-seeking incentives.
- Mechanism: Distributing decision-making authority across multiple overlapping centers (users, developers, platforms, regulators) creates checks and balances. No single entity can capture the system, and interdependent groups have pragmatic incentives for compromise.
- Core assumption: Power-seeking behavior is expected in complex multi-agent systems; the risk is concentration of overwhelming power, not its mere existence.
- Evidence anchors:
  - [section] "The fundamental risk lies not merely in the motivation to seek power, but in the potential for any single entity... to successfully concentrate overwhelming power, thereby destabilizing the entire system"
  - [section] "Designing robust, decentralized feedback mechanisms that become stronger, not weaker, in the face of attempts to manipulate them"
  - [corpus] Corpus lacks direct tests of polycentric vs. centralized AI governance; Ostrom's institutional economics is cited but not empirically validated in AI contexts in neighbor papers
- Break condition: If decentralized systems prove more vulnerable to coordination failures or capture by coalitions, the polycentric advantage dissolves

## Foundational Learning

- Concept: **Thick vs. Thin Morality (Walzer)**
  - Why needed here: The paper argues that "thin" universal principles cannot capture "thick" culturally-embedded norms; AI systems must engage with thick morality to behave appropriately
  - Quick check question: Can you explain why translating "fairness" across cultures requires engaging with thick moral systems rather than abstract principles?

- Concept: **Mistake Theory vs. Conflict Theory (Alexander)**
  - Why needed here: The paper explicitly aligns with Conflict Theory—the view that disagreements arise from genuine value conflicts, not correctable errors
  - Quick check question: What would a Mistake Theorist and a Conflict Theorist each predict about the outcome of extended deliberation between Christians and Buddhists on the existence of the soul?

- Concept: **Specification Gap**
  - Why needed here: The framework takes the specification gap (unavoidable differences between human values and encoded objectives) as given and builds around it
  - Quick check question: Why does the paper treat the specification gap as a reason to focus on conflict management rather than better specification?

## Architecture Onboarding

- Component map:
  - **Contextual grounding layer**: Data pipelines for situational context (geography, roles, events, cultural practices)
  - **Community customization layer**: Interfaces for communities to shape local norms within broader constraints
  - **Continual adaptation layer**: Feedback collection and learning systems that update appropriateness models
  - **Polycentric governance layer**: Multi-scale authority structure (users → apps → platforms → regulators)

- Critical path:
  1. Identify specific deployment context and stakeholder communities
  2. Map existing norms and sanctioning mechanisms in that context
  3. Implement feedback interfaces that expose appropriateness signals
  4. Design governance boundaries (what's decided locally vs. globally)
  5. Build monitoring for conflict escalation and system manipulation attempts

- Design tradeoffs:
  - **Context vs. privacy**: More contextual data improves appropriateness but raises privacy concerns; requires privacy-preserving ML, on-device processing, and clear data governance
  - **Specialization vs. capability**: Specialized systems outperform monolithic ones in specific contexts even with identical base capabilities
  - **Local autonomy vs. global constraints**: Communities customize norms, but base model providers maintain fundamental safety guardrails

- Failure signatures:
  - Context-inappropriate behavior (e.g., comedy bot formality in casual settings)
  - Resistance and trust erosion from communities whose norms are overridden
  - Feedback manipulation where powerful actors game sanctioning mechanisms
  - Coordination failures between polycentric authorities producing conflicting requirements

- First 3 experiments:
  1. **Contextual grounding test**: Deploy identical base model with varying context richness (minimal vs. rich situational data) across two distinct user communities; measure appropriateness ratings and conflict incidents
  2. **Community customization pilot**: Give two communities explicit norm-setting interfaces for the same application type; measure whether local customization reduces resistance compared to externally-imposed defaults
  3. **Feedback robustness probe**: Simulate manipulation attempts against sanctioning mechanisms; measure whether decentralized feedback structures resist capture better than centralized ones

## Open Questions the Paper Calls Out

- **Open Question 1**: How can decentralized feedback mechanisms be designed to remain robust when powerful AI systems attempt to shape, manipulate, or ignore them?
  - Basis in paper: The authors explicitly ask: "What happens when AI systems become powerful enough to shape, manipulate, or simply ignore the feedback mechanisms themselves?"
  - Why unresolved: Current learning paradigms generally assume the feedback signal is a fixed ground truth, whereas the paper argues these mechanisms are socially constructed and vulnerable to manipulation by advanced agents.
  - What evidence would resolve it: A technical architecture or governance protocol where the cost of manipulating the feedback loop exceeds the benefit for a powerful agent.

- **Open Question 2**: How can global AI safety initiatives overcome the collective action "start-up" and "free-rider" problems in a heterogeneous, non-convergent society?
  - Basis in paper: The paper states that the alignment community has "substantially underinvested" in these two fundamental subproblems of collective action relative to their importance for X-risk mitigation.
  - Why unresolved: Standard coordination mechanisms often assume shared values or rational convergence, which this framework rejects; solutions must be found that function amidst "persistently divergent" interests.
  - What evidence would resolve it: Theoretical models or empirical case studies demonstrating stable global coordination on safety standards without requiring value uniformity or a single dominant enforcer.

- **Open Question 3**: What technical architectures can provide the necessary rich contextual grounding for AI while strictly adhering to the constraints of "contextual integrity" and privacy?
  - Basis in paper: The paper notes that "contextual grounding" requires access to rich data (PII), creating a direct conflict with the principle that privacy is the "appropriateness of information flow."
  - Why unresolved: While the authors call for privacy-preserving machine learning, they do not specify how to technically balance the deep data access needed for "thick" moral reasoning with data minimization.
  - What evidence would resolve it: A system design that successfully navigates context-dependent norms via on-device processing or secure enclaves without exposing sensitive user data to a central model.

## Limitations
- The framework lacks empirical validation and specific technical implementation details for its proposed principles.
- Architecture details for polycentric governance and community customization are not specified—e.g., how to structure authority boundaries or handle conflicts between community norms.
- No formalization of "appropriateness" or how to measure context-appropriate behavior is provided in this paper.

## Confidence
- **Medium confidence**: Core claims about advantages of conflict management over value convergence are philosophically compelling but lack empirical validation
- **Low confidence**: Specific mechanisms for preventing feedback manipulation and coordinating polycentric governance, as these are acknowledged as open problems without proposed solutions

## Next Checks
1. **Formalize the Appropriateness Specification**: Develop a formal definition of "appropriateness" that can be measured across contexts and tested empirically, addressing the specification gap identified in the framework.

2. **Design Governance Boundaries**: Create a concrete architecture for polycentric governance that specifies how authority is distributed, how conflicts between community norms are resolved, and what constitutes acceptable local customization versus necessary global constraints.

3. **Test Decentralized Feedback Robustness**: Build a simulation testbed with heterogeneous agents to empirically compare centralized versus decentralized feedback mechanisms under manipulation attempts, measuring resilience to capture and coordination failures.