---
ver: rpa2
title: 'ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning'
arxiv_id: '2512.07795'
source_url: https://arxiv.org/abs/2512.07795
tags:
- reasoning
- arxiv
- across
- cost
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the instability of LLM reasoning, which is
  critical for safety-critical applications. Current evaluation practices, which report
  single-run accuracy, ignore the inherent uncertainty from stochastic decoding, leading
  to unreliable performance assessments.
---

# ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning

## Quick Facts
- arXiv ID: 2512.07795
- Source URL: https://arxiv.org/abs/2512.07795
- Reference count: 9
- Key outcome: Single-run accuracy systematically underestimates LLM reasoning instability; multi-run evaluation with confidence intervals reveals hidden variance, with top-performing methods often incurring higher and less stable costs.

## Executive Summary
Current LLM reasoning evaluation practices report single-run accuracy while ignoring the intrinsic uncertainty from stochastic decoding, leading to unreliable performance assessments. This paper introduces ReasonBENCH, a modular evaluation library with a multi-run protocol and public leaderboard to quantify reasoning instability. Across tasks and reasoning strategies, they find high instability with some methods showing confidence intervals up to four times wider than others. The authors analyze how prompts, model families, and scale affect the trade-off between solve rate and stability, demonstrating that reproducibility is a critical dimension for reliable LLM reasoning.

## Method Summary
ReasonBENCH provides a modular evaluation library that standardizes multi-run evaluation across 11 reasoning strategies, 5 reasoning models, and 7 tasks. The benchmark uses a 10-run protocol per model-algorithm-task combination with harmonized decoding parameters and zero-shot evaluation. Methods are orchestrated through agents that build prompts from states, query unified LLM APIs via CacheSaver for reproducibility, parse responses, and update states via environments. The evaluation measures both quality (mean±CI, CV, MAD) and cost (USD with mean±CI, CV, MAD), revealing substantial variance that single-run accuracy obscures.

## Key Results
- Single-run accuracy underestimates reasoning instability by up to 4x in confidence interval width
- Top-performing methods often incur higher and less stable costs
- Refined prompts reduce extrinsic variance by strengthening parsing logic and clarifying instructions
- Model scale within families improves both quality and stability, but price does not predict consistency across families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-run accuracy systematically underestimates LLM reasoning instability, while multi-run evaluation reveals hidden variance.
- Mechanism: Stochastic decoding (temperature-based sampling) introduces run-to-run variability in both reasoning paths and final answers. Aggregating across 10+ independent runs captures this variance as confidence intervals, exposing the gap between mean performance and worst-case reliability.
- Core assumption: Decoding stochasticity is the dominant source of variance; other sources (API instability, model updates) are secondary.
- Evidence anchors:
  - [abstract] "current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding"
  - [Section 4.1] Reports show GoT with 10.0±2.4 quality (CI spanning ~50% of mean) vs. FoA at 36.0±1.4 (tight CI), demonstrating methods with similar sophistication can have vastly different stability profiles.
  - [corpus] PERSIST paper confirms personality measurements in LLMs show persistent instability across scales and conversation histories.
- Break condition: If temperature=0 (greedy decoding) eliminates variance, mechanism shifts from stochasticity to prompt brittleness.

### Mechanism 2
- Claim: Prompt clarity and parser robustness directly reduce extrinsic variance without altering reasoning logic.
- Mechanism: Ambiguous prompts and brittle parsers introduce noise—models interpret instructions differently across runs, and parsing failures masquerade as reasoning errors. Clarifying instructions and standardizing output expectations reduce this "artificial" variance.
- Core assumption: The refined prompts preserve the original method's reasoning logic (fidelity-preserving).
- Evidence anchors:
  - [Section 5.2] "Across all frameworks, clarifying prompts and strengthening the parsing logic consistently reduce variance... structured and search-based approaches show the largest reductions"
  - [Table 3] IO prompting improved from 3.0±0.8 to 31.3±0.7 (+28.3 absolute) with refined prompts—suggesting much "failure" was parsing/format issues.
  - [corpus] STaR paper emphasizes slow-thinking approaches for table reasoning stability, aligning with structured prompt benefits.
- Break condition: If prompt changes alter the reasoning task itself, gains reflect task simplification rather than variance reduction.

### Mechanism 3
- Claim: Model scale within a family improves both quality AND stability; price does not predict consistency across families.
- Mechanism: Larger models within the same architecture have more stable internal representations, reducing variance in output distributions. However, cross-family comparisons show training/data differences dominate scale effects.
- Core assumption: Observed stability differences reflect model properties, not API-level variability.
- Evidence anchors:
  - [Section 5.1] "GPT-4.1-Mini achieves higher mean quality and exhibits substantially tighter distributions than GPT-4.1-Nano"
  - [Table 2] Qwen3-235B (expensive) shows 46.2±12.4 quality (CV=0.773) while cheaper Llama 4 Maverick shows 45.7±4.5 (CV=0.380)—inverting price-stability expectations.
  - [corpus] Weak direct corpus evidence on cross-family stability; limited to evaluation framework papers.
- Break condition: If API rate-limiting or infrastructure differences cause variance, mechanism attribution shifts from model to deployment.

## Foundational Learning

- Concept: **Bias-Variance Tradeoff**
  - Why needed here: The paper reframes LLM evaluation through this lens—single-run accuracy captures bias, while multi-run variance captures reliability. Without this, practitioners conflate "performs well on average" with "performs reliably."
  - Quick check question: If a method achieves 80% accuracy on one run and 60% on another, is its problem bias or variance?

- Concept: **Stochastic Decoding (Temperature Sampling)**
  - Why needed here: The root cause of instability. Temperature > 0 introduces randomness in token selection, creating different reasoning chains for identical inputs.
  - Quick check question: Why would setting temperature=0 NOT guarantee identical outputs across API calls?

- Concept: **Confidence Intervals and Statistical Significance**
  - Why needed here: The paper's core intervention. Reporting mean±CI (vs. point estimates) reveals whether performance differences are meaningful or noise.
  - Quick check question: Two methods report 70% vs 75% accuracy. What additional information do you need to conclude one is truly better?

## Architecture Onboarding

- Component map:
  ```
  Method (orchestrator)
    ├── Agent (prompt builder + response parser)
    ├── Environment (task rules + scoring)
    ├── State (intermediate trajectory)
    └── Model (unified LLM API interface + CacheSaver)
  ```

- Critical path: Method → Agent.construct_prompt(State) → Model.query() → Agent.parse_response() → Environment.step(State, action) → repeat or terminate

- Design tradeoffs:
  - Standardized prompts enable fair comparison but may underrepresent method-specific optimizations
  - CacheSaver enables reproducibility but requires storage; stale cache invalidation is manual
  - 10-run protocol balances statistical reliability vs. cost; complex methods (FoA, MCTS*) become expensive

- Failure signatures:
  - High variance with narrow CIs: Likely parsing failures silently dropping samples
  - Method works in isolation but fails in benchmark: Check prompt/State mismatch
  - Cost spikes without quality gains: Check for infinite loops in Environment termination logic

- First 3 experiments:
  1. Run IO prompting on Game of 24 with 10 trials; verify CI width matches Table 1 baseline (3.0±0.8)
  2. Compare CoT vs. CoT-SC on same task; confirm variance reduction from self-consistency (15.0±1.2 shows CV=0.14 vs CoT's 0.38)
  3. Swap GPT-4.1-Nano for Mini; confirm quality rises AND CI narrows (scaling effect validation)

## Open Questions the Paper Calls Out
None

## Limitations

- Multi-run protocol sensitivity: The paper does not systematically explore how the number of runs affects CI estimates, potentially understating true worst-case variability for high-variance methods.
- Source attribution for variance: While attributing variance primarily to stochastic decoding, the paper does not isolate contributions from API rate-limiting, network jitter, or model version drift.
- Cross-family stability comparison validity: The finding that price does not predict stability across families is based on limited samples without controlling for training corpus differences or task alignment.

## Confidence

- Single-run accuracy underestimates instability (High) — Directly supported by systematic CI reporting
- Refined prompts reduce extrinsic variance (High) — Strong empirical evidence from IO prompting improvements
- Scale improves both quality and stability within families (Medium) — Consistent with GPT-4.1 Mini vs. Nano results
- Price does not predict stability across families (Low) — Based on handful of comparisons without controlling for confounding factors

## Next Checks

1. Run sensitivity analysis — Repeat Game of 24 benchmark with IO prompting using 5, 10, 20, and 50 runs to quantify how CI width converges and whether 10 runs provide adequate coverage for high-variance methods.

2. Ablation on variance sources — Run identical prompts with temperature=0 (greedy decoding) across three methods with high variance (GoT, MCTS*, FoA) to isolate stochastic decoding contribution from other sources.

3. Cross-family controlled comparison — Select three tasks where GPT-4.1 and Llama 4 perform similarly in mean quality, then compare their variance profiles using identical prompts and decoding parameters to test whether observed stability differences persist after controlling for task alignment.