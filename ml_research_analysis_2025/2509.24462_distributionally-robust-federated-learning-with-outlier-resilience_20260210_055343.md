---
ver: rpa2
title: Distributionally Robust Federated Learning with Outlier Resilience
arxiv_id: '2509.24462'
source_url: https://arxiv.org/abs/2509.24462
tags:
- robust
- learning
- distribution
- distributionally
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of outlier-robust federated learning
  in the presence of data heterogeneity and distribution shifts. The authors propose
  a novel distributionally robust optimization framework that uses an unbalanced Wasserstein
  distance to construct an ambiguity set that jointly captures geometric distributional
  shifts and mitigates outlier influence through Kullback-Leibler penalization.
---

# Distributionally Robust Federated Learning with Outlier Resilience

## Quick Facts
- arXiv ID: 2509.24462
- Source URL: https://arxiv.org/abs/2509.24462
- Reference count: 40
- Primary result: Proposed DOR-FL algorithm achieves 95.4% accuracy on synthetic data and 84.6% on UCI Adult dataset with improved fairness across demographic groups

## Executive Summary
This paper addresses outlier-robust federated learning in heterogeneous data environments by proposing a distributionally robust optimization framework. The key innovation is using an unbalanced Wasserstein distance with Kullback-Leibler penalization to construct an ambiguity set that both captures geometric distributional shifts and mitigates outlier influence. The formulation leads to a challenging min-max-max optimization problem that is reformulated as a tractable Lagrangian penalty optimization enabling decentralized training. Experimental results demonstrate significant improvements over existing methods on both synthetic and real-world datasets.

## Method Summary
The authors formulate federated learning with outliers as a distributionally robust optimization problem using unbalanced Wasserstein distance. They reformulate the hard Wasserstein constraint into a Lagrangian penalty, converting an intractable infinite-dimensional problem into a decentralized weighted sum of local expectations. The resulting DOR-FL algorithm achieves O(T^{-1/2} + ϵ) convergence rate where T is iterations and ϵ is inner optimization accuracy. The method is evaluated on synthetic Gaussian data with contamination and the UCI Adult Income dataset partitioned by race, using logistic regression with up to 1000 iterations.

## Key Results
- Synthetic data: DOR-FL achieves 95.4% accuracy vs 61.5-77.1% for baselines (ERM, AFL, WAFL, GDRFL)
- UCI Adult Income: DOR-FL achieves 84.6% overall accuracy with 0.34 excess risk, outperforming ERM (83.1%, 0.37), AFL (81.7%, 0.44), WAFL (76.7%, 0.64), and GDRFL (79.5%, 0.61)
- Demonstrates improved fairness across demographic groups in the Adult dataset
- Achieves theoretical convergence rate of O(T^{-1/2} + ϵ)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Unbalanced Wasserstein distance allows the ambiguity set to effectively "discard" outliers by penalizing their mass rather than forcing their geometric transportation.
- **Mechanism:** Standard Wasserstein distance forces the ambiguity set to include all points by paying transportation cost. The Unbalanced Wasserstein distance introduces a KL divergence term allowing the optimization to "destroy" outlier probability mass by paying a penalty proportional to their likelihood rather than forcing geometric transportation.
- **Core assumption:** Outliers are sparse and have low probability under the clean distribution such that the KL cost of removing them is lower than the transportation cost of keeping them.
- **Evidence anchors:** Abstract states ambiguity set "mitigates outlier influence through Kullback–Leibler penalization"; Section 2.1 proves UW distance remains small with contaminated data.
- **Break condition:** If outliers are not geometrically distinct or represent significant data mass, the KL penalty will be insufficient to exclude them.

### Mechanism 2
- **Claim:** Reformulating the hard Wasserstein constraint into a Lagrangian penalty transforms an intractable infinite-dimensional problem into a decentralized weighted sum of local expectations.
- **Mechanism:** The original DRO problem requires a supremum over a constrained set of distributions. By converting the constraint into a penalty term in the objective, the inner maximization becomes unconstrained and admits a closed-form solution that decomposes into a sum over clients.
- **Core assumption:** The loss function is convex in θ and concave in ξ, and the transportation cost is convex, ensuring strong duality holds for the Lagrangian reformulation.
- **Evidence anchors:** Abstract mentions "tractable Lagrangian penalty optimization problem enabling decentralized training"; Section 3 Proposition 1 derives the separable form.
- **Break condition:** If the penalty coefficient is set too low, the constraint is ignored; if too high, the problem becomes ill-conditioned.

### Mechanism 3
- **Claim:** The inclusion of an explicit outlier scoring function biases the "worst-case" search away from outlier-containing distributions.
- **Mechanism:** The robust objective optimizes a modified loss L(θ, ξ) - h(ξ). By subtracting h(ξ), the algorithm artificially reduces the loss of suspected outliers, causing the inner maximization to disregard these samples as they no longer represent the worst-case scenario.
- **Core assumption:** The function h(ξ) can be designed with sufficient accuracy to assign large values to outliers and small values to normal data.
- **Evidence anchors:** Section 2.2 states that for outlier-containing distributions, the expected h(ξ) is large and thus less likely to attain the supremum; Section 5.2 demonstrates specific design for Adult dataset.
- **Break condition:** If h(ξ) misfires by assigning high scores to legitimate high-loss samples, the model will ignore difficult but valid examples.

## Foundational Learning

### Concept: Unbalanced Optimal Transport
- **Why needed here:** This is the mathematical core differentiating DOR-FL from standard Wasserstein DRO. You must understand that "Unbalanced" refers to relaxing the mass conservation constraint (allowing mass to be destroyed/created via KL divergence), not an imbalanced dataset.
- **Quick check question:** Why does standard Wasserstein distance fail (become overly conservative) when outliers are present in the support?

### Concept: Min-Max-Max Optimization
- **Why needed here:** The problem structure involves minimizing model parameters (θ), maximizing client weights (λ), and maximizing over distributions (P). Understanding the role of each "max" is crucial for debugging the convergence.
- **Quick check question:** In the DOR-FL formulation, does the "max over λ" help or hurt convergence speed, and why is it necessary for fairness?

### Concept: Strong Duality & Convex Conjugates
- **Why needed here:** The transition from the intractable constrained problem to the tractable unconstrained one relies on duality.
- **Quick check question:** The reformulation swaps a hard constraint for a soft penalty. What mathematical property guarantees that these two problems result in the same optimal θ?

## Architecture Onboarding

### Component map:
Server -> Client (Local) -> Server

### Critical path:
1. Setup: Define h(ξ) based on domain knowledge
2. Inner Loop (Client): For sampled data point, solve adversarial problem max_ξ L(θ, ξ) - ρc(ξ, ζ)
3. Gradient Step: Weight resulting gradient by exponential term exp((L - ρc)/(ρβ))
4. Aggregation: Server updates λ to upweight clients with "cleaner" data distributions

### Design tradeoffs:
- Inner Accuracy ϵ vs. Speed: High precision ensures convergence but slows local training
- Penalty ρ vs. Robustness Radius: Tuning ρ is heuristic balancing "ignoring data" vs. "fitting data"

### Failure signatures:
- Numerical Instability: Gradient involves e^(L/ρβ); small ρβ relative to loss scale causes explosion
- Mode Collapse in λ: Perfectly clean data client may cause λ to converge to one-hot vector
- Outlier Overwrite: Weak h(ξ) causes inner maximization to consistently find outliers

### First 3 experiments:
1. Sanity Check (Synthetic): Implement synthetic setup and verify accuracy gap widens with contamination
2. Hyperparameter Sensitivity (ρ): Run sweep to confirm very small ρ leads to underfitting and very large ρ acts like standard ERM
3. Ablation on h(ξ): Run with h(ξ)=0 and check if performance degrades to match WAFL/GDRFL baselines

## Open Questions the Paper Calls Out
- Developing communication-efficient variants to reduce transmission overhead in large-scale networks
- Exploring scalable algorithms for high-dimensional applications
- Extending theoretical guarantees to non-convex loss functions used in deep learning
- Deriving outlier scoring function automatically rather than requiring domain knowledge

## Limitations
- Specific hyperparameter settings (ρ, β, h(ξ) parameters) that were critical for reported performance are not fully specified
- Limited baseline comparisons and potential sensitivity to hyperparameter tuning
- Theoretical convergence relies on convexity assumptions that don't hold for deep neural networks
- Manual specification of outlier scoring function limits applicability in dynamic environments

## Confidence
- Theoretical convergence rate (O(T^{-1/2} + ϵ)): Medium confidence - well-supported by mathematical proofs
- Empirical superiority claims: Medium-Low confidence - limited baselines and hyperparameter sensitivity
- Mechanism explanations for Unbalanced Wasserstein handling outliers: Medium confidence - KL divergence formulation supported but practical effectiveness depends on problem geometry

## Next Checks
1. Reproduce synthetic experiment with varying contamination levels to verify O(T^{-1/2} + ϵ) convergence rate holds across different outlier proportions
2. Conduct ablation study removing the KL divergence term to demonstrate its necessity for outlier resilience
3. Test algorithm's behavior when h(ξ) is incorrectly specified (assigning high scores to legitimate data) to validate mechanism claims about outlier scoring