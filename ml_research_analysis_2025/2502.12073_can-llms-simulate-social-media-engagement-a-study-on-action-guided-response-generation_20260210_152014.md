---
ver: rpa2
title: Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response
  Generation
arxiv_id: '2502.12073'
source_url: https://arxiv.org/abs/2502.12073
tags:
- user
- llms
- action
- social
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether large language models (LLMs) can simulate
  social media engagement by predicting user actions (retweet, quote, rewrite) and
  generating responses accordingly. The authors benchmark GPT-4o-mini, O1-mini, and
  DeepSeek-R1 on a COVID-19 vaccination dataset from X (formerly Twitter).
---

# Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation

## Quick Facts
- **arXiv ID:** 2502.12073
- **Source URL:** https://arxiv.org/abs/2502.12073
- **Reference count:** 15
- **Primary result:** Zero-shot LLMs are biased toward "quote" actions and underperform BERT in action prediction, but few-shot LLMs generate responses with stronger semantic alignment to ground truth posts

## Executive Summary
This paper investigates whether large language models can simulate social media engagement by predicting user actions (retweet, quote, rewrite) and generating responses accordingly. The authors benchmark GPT-4o-mini, O1-mini, and DeepSeek-R1 on a COVID-19 vaccination dataset from X (formerly Twitter). Key findings show that zero-shot LLMs are biased toward predicting "quote" actions and underperform BERT in action prediction, while few-shot prompting initially reduces accuracy but improves with more examples. Despite lower action prediction performance, few-shot LLMs generate responses with stronger semantic alignment to ground truth posts. The study highlights the importance of prompt design and demonstrates both the potential and limitations of LLMs in modeling realistic social media interactions.

## Method Summary
The study uses a COVID-19 vaccination tweet dataset from X (Sep 28–Nov 4, 2020), filtered to 3,990 balanced entries across three action types. Three LLM configurations (zero-shot base, zero-shot with user info, few-shot with historical examples) are tested on GPT-4o-mini, O1-mini, and DeepSeek-R1. Action prediction accuracy is compared against BERT baselines (frozen classifier and fully fine-tuned). Response generation is evaluated using GPT-4o-mini, with semantic alignment assessed via Llama-3.3-70B for sentiment, emotion, stance, age, and gender, plus LIWC for style. The methodology involves analyzing the impact of incorporating user information and historical behavior patterns on model performance.

## Key Results
- Zero-shot LLMs underperform BERT in action prediction, with a strong bias toward predicting "quote" actions
- Few-shot prompting initially reduces accuracy but improves with more examples (5+ examples needed for recovery)
- Few-shot LLMs generate responses with stronger semantic alignment to ground truth posts despite lower action prediction accuracy
- Historical behavioral rates (retweet/quote/rewrite rates) are the most influential user features for improving LLM action prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-shot LLMs exhibit a strong "quote" bias in action prediction, causing them to underperform compared to fine-tuned discriminative models.
- **Mechanism:** LLMs, when acting as open-ended generators in a classification task without examples, default to frequently observed or dominant behaviors from their pre-training data. In the context of social media simulation, they disproportionately predict the "quote" action, likely due to the prevalence of this engagement type or its linguistic markers in their training data, leading to a skewed prediction distribution and lower overall accuracy.
- **Core assumption:** This assumes the LLM's pre-training data contains biases toward certain social media actions and that without specific guidance (like few-shot examples), the model cannot correctly calibrate its predictions based on the provided user/post information alone.
- **Evidence anchors:**
  - [abstract] "Our findings reveal that zero-shot LLMs underperform BERT in action prediction... zero-shot LLMs are biased toward predicting 'quote' actions..."
  - [section] Section 4.1: "...LLMs predominantly predicts the quote action, indicating a strong bias toward this category."
  - [corpus] Related work (Paper 58657, BluePrint) highlights the need for "standardized data resources" to make LLMs "realistic social media agents," implying that default behavior is not realistic without proper data/alignment.

### Mechanism 2
- **Claim:** Few-shot prompting with limited examples can initially degrade LLM action prediction accuracy due to overfitting to sparse context.
- **Mechanism:** Providing a very small number of examples (e.g., 1-shot) in the prompt creates an unrepresentative sample of the user's behavior distribution. The LLM, designed to complete patterns, may overfit to this limited, potentially noisy context, overriding its broader pre-trained knowledge or the user information provided, leading to worse performance than the zero-shot baseline.
- **Core assumption:** This assumes the LLM's context-learning mechanism is sensitive to the quality and quantity of in-context examples and will prioritize pattern completion from the prompt over its internal probabilistic model when the context is sparse.
- **Evidence anchors:**
  - [abstract] "...few-shot prompting initially reduces accuracy but improves with more examples."
  - [section] Section 4.2: "This suggests that using only a few examples in prompts can negatively impact LLMs' action prediction accuracy, likely due to overfitting to a limited context." and Figure 3 shows accuracy dip at 1-shot.
  - [corpus] No direct corpus evidence found for this specific failure mode of few-shot learning in social media simulation.

### Mechanism 3
- **Claim:** Historical behavioral rates (e.g., retweet rate, quote rate) are the most influential user features for improving LLM action prediction.
- **Mechanism:** A user's past behavior is the strongest predictor of future actions. When these explicit numerical rates are included in the prompt, they provide a direct statistical signal that helps the LLM calibrate its prior beliefs, moving away from its pre-training bias and aligning its prediction with the user's established engagement patterns.
- **Core assumption:** This assumes that past behavior is a reliable indicator of future actions and that LLMs can effectively interpret and use numerical rates in text prompts to adjust their probabilistic output.
- **Evidence anchors:**
  - [abstract] The paper "analyzes the impact of incorporating user information and historical behavior."
  - [section] Table 3 and Appendix A.3.1 show that removing "rates" leads to the largest drop in accuracy (-8.90%), while other features have a smaller impact.
  - [corpus] Paper 57328 ("The Ranking Effect") indirectly supports the importance of user behavior data, noting that "feed signals influence behaviors and perceptions," implying behavior is a key modeling target.

## Foundational Learning

- **Concept: Zero-Shot vs. Few-Shot Prompting**
  - **Why needed here:** This paper's core experimental design contrasts these two paradigms, revealing a critical non-linear performance relationship where few-shot can initially be *worse* than zero-shot. Understanding this is essential for any engineer implementing an LLM-based simulation.
  - **Quick check question:** In this paper's findings, what happens to action prediction accuracy when you provide a single historical example (1-shot) compared to a zero-shot baseline?

- **Concept: Generative vs. Discriminative Models for Classification**
  - **Why needed here:** The paper benchmarks LLMs (generative) against BERT (discriminative, via fine-tuning). A core finding is that fine-tuned BERT outperforms zero-shot LLMs in classification, highlighting the continuing relevance of specialized discriminative models for this sub-task.
  - **Quick check question:** Why does the paper argue that "a well-designed prompt combined with relevant information is crucial" when using LLMs for the action classification task?

- **Concept: Semantic Alignment vs. Action Prediction Decoupling**
  - **Why needed here:** The paper presents a counter-intuitive result: models that are worse at predicting the correct action can still generate responses with better semantic alignment to ground truth. This challenges the assumption that task performance is monolithic.
  - **Quick check question:** Which model configuration achieves the lowest action prediction accuracy but the highest semantic alignment scores for generated text?

## Architecture Onboarding

- **Component map:** Data Processing Pipeline -> User Feature Extractor -> Action Predictor (LLM/BERT) -> Response Generator (LLM) -> Evaluation (Llama-3.3-70B/LIWC)
- **Critical path:** The critical path for simulation quality is: `User/Post Data -> Action Prediction -> Response Generation`. A failure in action prediction propagates directly to the generation step. For *semantic alignment*, the paper shows the generation step is surprisingly resilient, but for accurate *action simulation*, the prediction step is the primary point of failure for zero-shot LLMs.
- **Design tradeoffs:**
  - **LLM vs. BERT for Action Prediction:** Using an LLM provides a unified model for both prediction and generation but introduces a strong "quote" bias in zero-shot settings and requires careful few-shot prompt engineering. BERT requires fine-tuning and separate data but is more accurate and stable.
  - **Zero-Shot vs. Few-Shot:** Zero-shot is simpler but biased. Few-shot with limited examples reduces accuracy; performance only recovers with more (5+) examples, which increases token cost and latency.
  - **Feature Inclusion:** Adding user "rates" is the most impactful lever for improving LLM action prediction accuracy (+8.9% over a baseline without them).
- **Failure signatures:**
  - **Quote Bias:** Model predicts "quote" action for a vast majority of instances, regardless of the user or post content. This is a signature of a zero-shot or under-informed LLM prompt.
  - **Few-Shot Dip:** Action prediction accuracy drops below zero-shot levels when only 1-3 historical examples are provided. This is a signature of overfitting to sparse context.
  - **Cold Start:** The model reverts to default biases for new users with no historical rate data.
- **First 3 experiments:**
  1. **Feature Ablation on Action Prediction:** Start with a zero-shot LLM. Incrementally add user feature groups (name, counts, rates) to the prompt and measure the impact on action prediction accuracy. This will confirm the relative importance of historical rates as reported in the paper.
  2. **Few-Shot Scaling Analysis:** Systematically vary the number of historical examples provided in the prompt (1-shot, 2-shot, ..., 5+ shot) and plot the action prediction accuracy curve. This will validate the "dip-and-recover" pattern and identify the minimum number of examples needed for improvement.
  3. **Hybrid Architecture Test:** Evaluate a hybrid system where a fine-tuned BERT model handles action prediction and its output is passed to an LLM for response generation. Compare the overall simulation quality (action accuracy + semantic alignment) against a pure LLM approach. This tests the paper's implied recommendation of combining models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can combining LLMs with structured classifiers (e.g., BERT) for action prediction improve overall simulation accuracy while preserving the semantic alignment strengths of few-shot LLMs in response generation?
- **Basis in paper:** [explicit] The Limitations section states: "Future work should explore combining LLMs with structured classifiers for action prediction."
- **Why unresolved:** The paper shows a performance gap—fine-tuned BERT excels at action prediction (57.14%) while few-shot LLMs generate semantically superior responses—but no hybrid approach was tested.
- **What evidence would resolve it:** Experiments with pipeline or ensemble architectures where a classifier handles action prediction and an LLM handles generation, evaluated on both metrics.

### Open Question 2
- **Question:** How well do these findings generalize across different social media platforms (e.g., Reddit, Facebook, Bluesky) and topic domains beyond COVID-19 vaccination?
- **Basis in paper:** [explicit] The Limitations section notes "our evaluation is limited to a single social media context, which may affect generalization" and calls for "cross-platform testing."
- **Why unresolved:** The study uses only X (Twitter) data from a specific event (Sept–Nov 2020) with unique engagement norms and user behaviors.
- **What evidence would resolve it:** Replicating the action-guided framework on datasets from other platforms and topics, comparing action prediction distributions and semantic alignment scores.

### Open Question 3
- **Question:** What causes zero-shot LLMs' systematic bias toward predicting "quote" actions, and can prompt engineering or fine-tuning mitigate this?
- **Basis in paper:** [explicit] The authors observe that "LLMs predominantly predict the quote action, indicating a strong bias toward this category" but do not determine the underlying cause.
- **Why unresolved:** The bias is documented but not explained—it may stem from training data distributions, instruction-tuning, or the framing of the task itself.
- **What evidence would resolve it:** Ablation studies varying prompt phrasing, analyzing pretraining corpora for quote tweet prevalence, and testing debiasing techniques (e.g., class-balanced few-shot examples).

### Open Question 4
- **Question:** Can retrieval-augmented generation (RAG) improve historical context integration for few-shot prompting, reversing the initial accuracy degradation observed with limited examples?
- **Basis in paper:** [explicit] The Limitations section suggests "leveraging retrieval-augmented generation (RAG) for better historical context integration."
- **Why unresolved:** Few-shot prompting initially degrades action prediction accuracy (as low as 19.61% for GPT-4o-mini at 1-shot), possibly due to overfitting to sparse context.
- **What evidence would resolve it:** Experiments comparing standard few-shot prompting against RAG-based retrieval of relevant historical examples, measuring action accuracy across varying retrieved context sizes.

## Limitations
- The study is limited to a single social media platform (X) and a specific domain (COVID-19 vaccination), which may affect generalizability
- The rewrite detection methodology relies on heuristic thresholds (cosine similarity, keyword matching) that may not generalize to other content types
- The paper doesn't explore the impact of different LLM architectures or sizes on the observed phenomena

## Confidence

- **High Confidence:** The observation that zero-shot LLMs exhibit quote bias and underperform BERT in action prediction; the finding that few-shot prompting initially reduces accuracy before improving with more examples; the identification of historical behavioral rates as the most impactful user feature
- **Medium Confidence:** The semantic alignment findings showing that worse action predictors can generate better-aligned responses; the recommendation that user rates are the most critical feature for improving LLM performance
- **Low Confidence:** The generalizability of the quote bias across different domains, platforms, and LLM architectures; the specific thresholds for rewrite detection and their sensitivity to parameter changes

## Next Checks
1. **Cross-Domain Validation:** Replicate the action prediction experiment using a different social media dataset (e.g., Reddit comments or Twitter data on a different topic) to test whether the quote bias persists across domains
2. **Hybrid Architecture Test:** Implement a system where BERT handles action prediction and LLM handles response generation, then compare overall simulation quality against pure LLM approaches to validate the paper's implied recommendation
3. **Rewrite Detection Sensitivity:** Vary the cosine similarity threshold and keyword list used for rewrite detection to quantify their impact on the final dataset composition and subsequent model performance