---
ver: rpa2
title: 'AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features'
arxiv_id: '2510.00404'
source_url: https://arxiv.org/abs/2510.00404
tags:
- abstopk
- sparse
- learning
- saes
- jumprelu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AbsTopK SAE is a new sparse autoencoder variant that uses absolute
  value-based hard thresholding (AbsTopK) to enable single features to encode bidirectional
  concepts, addressing the fragmentation problem in conventional SAEs caused by non-negativity
  constraints. The method is derived from unrolling proximal gradient updates for
  sparse coding, connecting it to classical dictionary learning theory.
---

# AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features

## Quick Facts
- **arXiv ID:** 2510.00404
- **Source URL:** https://arxiv.org/abs/2510.00404
- **Reference count:** 40
- **Primary result:** AbsTopK SAE uses absolute value-based hard thresholding to enable single features to encode bidirectional concepts, addressing fragmentation in conventional SAEs caused by non-negativity constraints.

## Executive Summary
AbsTopK introduces a sparse autoencoder variant that uses absolute value-based hard thresholding to enable single features to encode bidirectional concepts, addressing the fragmentation problem in conventional SAEs caused by non-negativity constraints. The method is derived from unrolling proximal gradient updates for sparse coding, connecting it to classical dictionary learning theory. Experiments across four large language models and seven probing/steering tasks show AbsTopK improves reconstruction fidelity and interpretability compared to TopK and JumpReLU baselines, with particularly strong performance on bidirectional steering tasks. Notably, AbsTopK achieves comparable or better results than the Difference-in-Mean method, a supervised approach requiring labeled data.

## Method Summary
AbsTopK SAE implements a hard thresholding operator that selects the k highest-magnitude entries (positive or negative) from the encoder output while zeroing the rest. Unlike TopK SAEs which only select positive values, AbsTopK preserves both positive and negative activations among the top-k magnitudes. The encoder computes latents as z = AbsTopK_k(W^T x + b_e), and the decoder reconstructs x̂ = D z + b. Training uses Adam with batch size 4096, learning rate 3e-4, for approximately 30k steps. The method is derived from proximal gradient methods for sparse coding, where AbsTopK emerges as the proximal operator for the k-sparsity constraint without non-negativity.

## Key Results
- AbsTopK achieves lower normalized MSE and higher loss-recovered scores across Qwen, Gemma, Pythia, and GPT-2 compared to TopK and JumpReLU baselines
- AbsTopK enables single features to capture bidirectional concepts, as demonstrated by controlled experiments with contrastive pairs like "man"/"woman" and "good"/"bad"
- On steering tasks, AbsTopK matches or exceeds Difference-in-Mean (DiM), a supervised method requiring labeled data, particularly for bidirectional concepts
- The method shows particular strength on safety steering tasks where bidirectional concepts are crucial for identifying harmful content

## Why This Works (Mechanism)

### Mechanism 1: Proximal Operators Reveal Implicit Constraints in SAE Architectures
ReLU, JumpReLU, and TopK SAEs are all single-step proximal gradient updates with different sparse regularizers, and all three regularizers include explicit non-negativity constraints. The proximal operator `prox_{λR}(u)` solves a constrained optimization where `R(z) = ‖z‖₀ + ι_{z≥0}(z)` yields JumpReLU and `R(z) = ι_{‖z‖₀≤k, z≥0}(z)` yields TopK. Both include `z≥0`, structurally enforcing this constraint regardless of learned weights.

### Mechanism 2: Non-Negativity Fragments Bidirectional Concepts into Redundant Features
Enforcing `z ≥ 0` forces opposing concepts to be encoded by separate dictionary atoms rather than opposite signs of one atom. Given a semantic direction `h`, an ideal sparse code would represent it as `α·d` where `α`'s sign indicates direction. Under `z≥0`, the SAE must allocate two atoms `d_i, d_j` with opposite orientations, each activated only for one direction, doubling the dictionary size for every bipolar axis.

### Mechanism 3: Hard Thresholding on Magnitude Preserves Both Poles While Enforcing Sparsity
AbsTopK (selecting top-k entries by absolute value) achieves k-sparsity without discarding negative activations, enabling single features to capture bipolar semantics. The proximal operator for `R(z) = ι_{‖z‖₀≤k}` without non-negativity is hard thresholding: `(AbsTopK_k(u))_i = u_i` if `i ∈ H_k(u)`, else `0`. This retains both positive and negative entries among the top-k magnitudes while enforcing exact sparsity.

## Foundational Learning

- **Concept: Dictionary Learning and Sparse Coding**
  - Why needed here: SAEs are formulated as dictionary learning where the encoder approximates the sparse coding step
  - Quick check question: Given a dictionary D and input x, what optimization problem does sparse coding solve? (Answer: `min_z ½‖x - Dz‖² + λR(z)` for sparse regularizer R)

- **Concept: Proximal Operators and Proximal Gradient Methods**
  - Why needed here: The paper's core theoretical contribution is deriving SAE activation functions as proximal operators of specific regularizers
  - Quick check question: What is the proximal operator for the ℓ₁ norm? (Answer: Soft thresholding: `prox_{λ‖·‖₁}(u)_i = sign(u_i)·max(|u_i| - λ, 0)`)

- **Concept: Linear Representation Hypothesis**
  - Why needed here: The motivation for bidirectional features assumes LLM hidden states are linear combinations of concept vectors where signs are meaningful
  - Quick check question: In the linear representation view, what does the vector offset `v_king - v_man + v_woman` represent? (Answer: An analogy transformation; it approximates `v_queen`)

## Architecture Onboarding

- **Component map:**
  Input x → Encoder z = AbsTopK_k(W^T x + b_e) → Decoder x̂ = D z + b → Loss L = E_x[½‖x - x̂‖²]

- **Critical path:**
  1. Extract residual stream activations from target layer on training data
  2. Initialize encoder/decoder randomly
  3. Forward pass: compute `u = W^T x + b_e`, apply AbsTopK_k to get sparse z, decode to x̂
  4. Backprop through decoder and encoder (AbsTopK has subgradient 1 on kept entries, 0 elsewhere)
  5. Evaluate with normalized MSE, loss-recovered score, and downstream steering tasks

- **Design tradeoffs:**
  - **k selection:** k ≈ d/10 works well; lower k increases sparsity but hurts reconstruction
  - **Expansion factor (P/d):** Paper uses 16×; lower reduces capacity for rare features
  - **Layer choice:** Middle layers (12-20 depending on model) capture semantic features best; early layers are syntactic, late layers are output-bound
  - **vs. TopK:** AbsTopK doubles representational capacity per feature (captures ±) but requires sign-aware interpretation

- **Failure signatures:**
  - **Dead features:** Latents that never activate—reduce learning rate or increase k
  - **High reconstruction error with high k:** Suggests dictionary size P is insufficient; increase expansion factor
  - **Steering fails to generalize:** Feature may be polysemantic; inspect with controlled token pairs
  - **AbsTopK underperforms TopK:** Check if target model actually uses bidirectional representations

- **First 3 experiments:**
  1. **Reconstruction baseline:** Train AbsTopK, TopK, JumpReLU on GPT-2 Small layer 6 with k=76. Plot normalized MSE vs. training steps. Expect AbsTopK to converge faster and reach lower MSE.
  2. **Bidirectional feature probe:** For a known contrastive pair (e.g., "he"/"she", "good"/"bad"), identify the top-activated AbsTopK feature. Verify it activates with opposite signs on minimal pair sentences. Compare to TopK, which should require two features.
  3. **Steering transfer:** Clamp an AbsTopK feature identified as encoding "refusal" to negative values on harmful prompts. Measure HarmBench attack success rate vs. MMLU preservation. Compare to DiM steering baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can multi-step proximal gradient extensions yield SAEs with improved sparse code accuracy and finer-grained feature structure compared to single-step encoders? The paper states this could capture finer-grained structure but leaves it to future work.

- **Open Question 2:** Does applying bidirectional thresholding to JumpReLU variants produce similar improvements in capturing bidirectional concepts as AbsTopK does for TopK? The paper mentions this principle can be applied but remains JumpReLU variants for future investigation.

- **Open Question 3:** What are the theoretical and practical limits of AbsTopK's ability to match supervised methods like Difference-in-Mean, and under what conditions does the remaining performance gap emerge? The paper demonstrates competitiveness but doesn't analyze why DiM still outperforms on certain metrics.

## Limitations
- The method's advantage relies on the assumption that LLM hidden states are linear combinations of bidirectional semantic axes; if the model uses different representations, benefits may not materialize
- Results are demonstrated only on relatively small models (up to 4B parameters), and scaling to frontier models with more complex representations is unproven
- The steering task improvements depend on post-hoc interpretation of feature behavior and subjective evaluation of what constitutes "bidirectional" steering

## Confidence
- **High Confidence:** The proximal operator derivation is mathematically rigorous and correctly identifies that ReLU, JumpReLU, and TopK all implicitly enforce non-negativity through their regularizers
- **Medium Confidence:** The claim that AbsTopK enables single features to encode bidirectional concepts is supported by controlled experiments but relies on post-hoc interpretation
- **Low Confidence:** The comparison to Difference-in-Mean assumes DiM captures the same bidirectional concepts that AbsTopK discovers, but DiM may learn different features entirely

## Next Checks
1. **Ablation on non-negativity:** Train a variant of AbsTopK where decoder weights are constrained to be non-negative. If performance drops to TopK levels, this confirms the key advantage comes from allowing negative latents rather than just different thresholding.

2. **Cross-layer transferability:** Train AbsTopK on one layer and evaluate steering performance on a different layer of the same model. This tests whether bidirectional features are layer-specific or represent more fundamental semantic axes.

3. **Dictionary coherence analysis:** Compute pairwise correlations between dictionary atoms in AbsTopK vs. TopK. If AbsTopK shows lower coherence (more orthogonal atoms), this provides quantitative evidence that bidirectional encoding reduces feature redundancy.