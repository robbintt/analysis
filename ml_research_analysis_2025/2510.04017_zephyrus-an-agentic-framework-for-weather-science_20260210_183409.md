---
ver: rpa2
title: 'Zephyrus: An Agentic Framework for Weather Science'
arxiv_id: '2510.04017'
source_url: https://arxiv.org/abs/2510.04017
tags:
- weather
- only
- text
- data
- flash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZEPHYRUS, an agentic framework for weather
  science that enables large language models to interact with meteorological data
  through Python code generation and execution. The framework provides ZEPHYRUSWORLD,
  a comprehensive execution environment with tools for weather data access, geolocation,
  forecasting, and climate simulation.
---

# Zephyrus: An Agentic Framework for Weather Science

## Quick Facts
- arXiv ID: 2510.04017
- Source URL: https://arxiv.org/abs/2510.04017
- Reference count: 40
- Primary result: ZEPHYRUS agents achieve 54.7% accuracy vs 19.9% for text-only on weather science tasks

## Executive Summary
This paper introduces ZEPHYRUS, an agentic framework that enables large language models to interact with meteorological data through Python code generation and execution. The framework provides ZEPHYRUSWORLD, a comprehensive execution environment with tools for weather data access, geolocation, forecasting, and climate simulation. Two agent variants are developed: ZEPHYRUS-DIRECT for single-step code generation and ZEPHYRUS-REFLECTIVE for iterative execution-refinement workflows. The authors construct ZEPHYRUSBENCH, a benchmark with 2158 question-answer pairs across 46 meteorological tasks spanning basic lookups to advanced forecasting and counterfactual reasoning.

## Method Summary
The ZEPHYRUS framework consists of ZEPHYRUSWORLD environment with integrated weather tools (WeatherBench 2 indexer, Geolocator, Stormer forecaster, JAX-GCM simulator) accessed through a FastAPI execution server. The system uses two agent variants: DIRECT (single-step generation with max 5 error-correction loops) and REFLECTIVE (iterative execution-refinement with max 20 interaction loops). The ZEPHYRUSBENCH benchmark includes 2158 QA pairs across 46 tasks with four answer types: numerical, temporal, spatial, and descriptive. Evaluation uses standardized metrics including absolute error, location accuracy, Earth Mover's Distance, and discussion scores. The framework was evaluated using GPT-5-Mini/Nano, Gemini 2.5 Flash, and gpt-oss-120b models.

## Key Results
- ZEPHYRUS agents significantly outperform text-only baselines by 28.6-35.4% in correctness
- ZEPHYRUS-REFLECTIVE achieves 54.7% accuracy on GPT-5-Mini compared to 19.9% for text-only approaches
- REFLECTIVE outperforms DIRECT by 3.5-6.2% across most models but degrades performance on smaller models like GPT-5-Nano

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code execution grounds LLM reasoning in actual meteorological data rather than parametric knowledge.
- Mechanism: LLM generates Python code → executes in ZEPHYRUSWORLD sandbox → receives numerical outputs/errors → answers from data-derived evidence.
- Core assumption: The LLM can map natural language weather queries to correct tool APIs and data operations.
- Evidence anchors: "outperforming them by up to 35 percentage points in correctness" and "ZEPHYRUS agents significantly outperform the text-only baseline across all models... 28.6-35.4% higher correctness"

### Mechanism 2
- Claim: Iterative execution-refinement improves complex reasoning over single-shot generation.
- Mechanism: ZEPHYRUS-REFLECTIVE executes code → observes outputs → analyzes plausibility → refines subsequent code blocks → repeats (max 20 turns) → synthesizes final answer.
- Core assumption: LLM can detect anomalies in intermediate outputs and formulate corrective actions.
- Evidence anchors: "ZEPHYRUS-REFLECTIVE for iterative execution-refinement workflows" and "ZEPHYRUS-REFLECTIVE enables it to outperform ZEPHYRUS-DIRECT by 3.5-6.2% across most models"

### Mechanism 3
- Claim: Unified tool abstraction reduces complexity barrier for weather science workflows.
- Mechanism: ZEPHYRUSWORLD wraps disparate resources (WeatherBench 2, Stormer forecaster, JAX-GCM simulator, Geolocator) behind consistent Python APIs with documentation injected into LLM context.
- Core assumption: High-level APIs sufficiently capture tool capabilities for diverse task types.
- Evidence anchors: "comprehensive execution environment with tools for weather data access, geolocation, forecasting, and climate simulation" and detailed tool descriptions with FastAPI-based parallel execution server

## Foundational Learning

- **Concept: Agentic Code Execution Environments**
  - Why needed here: LLMs cannot natively process high-dimensional spatiotemporal weather data; code generation bridges language and numerical computation.
  - Quick check question: Can you explain why a text-only LLM fails to answer "What was the maximum temperature in Cairo on 2020-06-15?" without external data access?

- **Concept: Reanalysis Data (ERA5/WeatherBench 2)**
  - Why needed here: ZEPHYRUSBENCH is built on ERA5; understanding gridded atmospheric variables (pressure levels, spatial/temporal resolution) is prerequisite for task design.
  - Quick check question: What is the difference between reanalysis data and forecast model output?

- **Concept: Execution-Observation-Refinement Loop**
  - Why needed here: Distinguishes ZEPHYRUS-DIRECT from ZEPHYRUS-REFLECTIVE; core pattern enabling self-correction.
  - Quick check question: In a ReAct-style loop, what information should the agent receive after each code execution to enable effective refinement?

## Architecture Onboarding

- **Component map:**
  - LLM Agent (GPT-5-Mini, Gemini-2.5-Flash, etc.) → generates Python code using tool documentation
  - Code Execution Server (FastAPI) → receives code, dispatches to workers
  - Resource Pools → manage concurrent access to tools (Data Indexer, Geolocator, Forecaster, Simulator)
  - ZEPHYRUSWORLD → Python environment with injected tool instances and xarray datasets
  - Evaluation Pipeline → verifies, extracts, scores outputs by type (numeric, location, descriptive)

- **Critical path:**
  1. Question + tool documentation → LLM generates `run()` function
  2. Code sent to execution server → worker acquires tools from pools
  3. Code executes → returns output or error
  4. If error: retry (DIRECT: max 5; REFLECTIVE: continues loop)
  5. If success: output returned to LLM (REFLECTIVE) or final answer extracted (DIRECT)

- **Design tradeoffs:**
  - DIRECT vs REFLECTIVE: Single-shot faster but less accurate; multi-turn more robust but higher latency and token cost.
  - Tool pool sizing: More workers = higher throughput but more memory; paper doesn't specify optimal pool size.
  - Timeout protection prevents hangs but may truncate valid long-running simulations.

- **Failure signatures:**
  - **Tool misuse errors**: Incorrect API calls (e.g., wrong Geolocator method) → caught by error-correction loop.
  - **Hard task collapse**: Discussion score ≤0.18 on forecast reports; correctness ~17-21% on hard tasks (similar to text-only baseline).
  - **Extreme outliers**: Some Q99 standardized errors reach 10²–10⁴, indicating occasional catastrophic failures.
  - **Smaller model degradation**: GPT-5-Nano REFLECTIVE underperforms DIRECT, suggesting context/reasoning limits.

- **First 3 experiments:**
  1. Replicate DIRECT vs REFLECTIVE comparison on ZEPHYRUSBENCH subset (e.g., 100 samples each difficulty level) using GPT-4o-mini; verify 28-35% gap holds.
  2. Ablate individual tools (remove Forecaster or Simulator) on medium/hard tasks to measure tool contribution; expect forecasting tasks to collapse without Forecaster.
  3. Stress-test error recovery: inject synthetic syntax/logic errors in generated code; measure REFLECTIVE's correction success rate within 20 turns vs DIRECT's 5 retries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agentic frameworks be enhanced to effectively reason about abstract weather phenomena and counterfactuals?
- Basis in paper: The authors state in Section 5 that "on hard tasks, Zephyrus performs similarly to text-only baselines," and the Conclusion notes "current LLMs... do not yet possess the capability to reason about abstract weather phenomena."
- Why unresolved: Current code-generation capabilities effectively handle data lookups but fail at complex, multi-step causal reasoning required for counterfactuals.
- What evidence would resolve it: Significant performance improvements on the "Hard" tasks in ZephyrusBench, particularly counterfactual reasoning, surpassing text-only baselines.

### Open Question 2
- Question: Can training agents on larger meteorological datasets improve the scientific accuracy of their generated responses?
- Basis in paper: The Conclusion explicitly states, "Future work could explore using larger datasets to train agents that produce more scientifically accurate responses."
- Why unresolved: The current study relies on pre-trained generalist LLMs; the specific impact of scaling domain-specific training data for weather agents is unknown.
- What evidence would resolve it: Evaluation of a Zephyrus agent fine-tuned on an expanded corpus of meteorological reports and simulation data showing reduced hallucination and higher correctness.

### Open Question 3
- Question: What architectural changes are required to bridge the gap between numerical data analysis and coherent text generation?
- Basis in paper: Section 5 shows that while agents excel at numerical tasks, they struggle with descriptive tasks (best discussion score only 0.177), and the "Direct" agent produces "rigid answers."
- Why unresolved: The pipeline of code execution to text output struggles to synthesize complex atmospheric interactions into nuanced narrative discussions.
- What evidence would resolve it: Zephyrus agents achieving discussion scores comparable to human expert reports (scores approaching 0.8-1.0) on forecast discussion tasks.

## Limitations

- Performance degrades significantly on hard tasks (discussion scores ≤0.18, correctness ~17-21%) suggesting current LLMs cannot reason about complex weather phenomena
- Framework benefits depend heavily on model scale, with smaller models (GPT-5-Nano) performing worse with REFLECTIVE approach
- Requires specific model weights and datasets (Stormer, NeuralGCM) that are not publicly available, limiting reproducibility

## Confidence

**High Confidence:** The core claim that code execution environments improve LLM weather reasoning is well-supported by quantitative results (28.6-35.4% accuracy gains) and aligns with established agentic computing principles. The ZEPHYRUSWORLD architecture and evaluation methodology are clearly specified.

**Medium Confidence:** Claims about iterative refinement providing additional benefits (3.5-6.2% gains) are supported but show model-dependent variability. The performance degradation on hard tasks is documented but the reasons for LLM limitations on complex weather phenomena remain speculative.

**Low Confidence:** The assertion that ZEPHYRUS enables "unprecedented weather science capabilities" overstates the practical impact given the observed performance ceilings on challenging tasks. The framework addresses tool access but does not fundamentally solve the underlying reasoning challenges in meteorological forecasting.

## Next Checks

1. **Ablation study on tool dependencies:** Remove the Forecaster and Simulator tools sequentially from the DIRECT agent on medium/hard tasks to quantify each component's contribution to overall performance. Expect forecast-related tasks to drop to near-baseline levels without the Stormer integration.

2. **Cross-model consistency analysis:** Evaluate all three model sizes (Nano, Mini, 120B) on identical task subsets using both DIRECT and REFLECTIVE approaches. Measure whether REFLECTIVE's benefits scale monotonically with model capacity or if there's an inflection point where complexity outweighs gains.

3. **Error recovery benchmarking:** Create synthetic error patterns (syntax errors, invalid API calls, out-of-bounds indexing) and measure REFLECTIVE's success rate in correcting them within the 20-turn limit versus DIRECT's 5 retries. This validates whether the iterative approach provides meaningful resilience against code generation failures.