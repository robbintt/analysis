---
ver: rpa2
title: 'TIB AIssistant: a Platform for AI-Supported Research Across Research Life
  Cycles'
arxiv_id: '2512.16442'
source_url: https://arxiv.org/abs/2512.16442
tags:
- research
- assistants
- aissistant
- tools
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The TIB AIssistant is a platform for AI-supported research that
  integrates multiple domain-agnostic assistants and tools to help researchers throughout
  the research life cycle. It provides a curated library of prompts and research-oriented
  tools, including literature search, proofreading, and review assistants.
---

# TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles

## Quick Facts
- arXiv ID: 2512.16442
- Source URL: https://arxiv.org/abs/2512.16442
- Reference count: 22
- Primary result: AI-supported research platform with sequential domain-agnostic assistants and RO-Crate provenance export

## Executive Summary
The TIB AIssistant is a platform designed to support researchers throughout the research life cycle using AI-powered assistants. It features a collection of specialized assistants (ideation, literature search, paper writing, proofreading, review) that work either individually or in sequence. The platform stores generated data as structured assets that can be exported as RO-Crate bundles with provenance information using ontologies like DOCO and DEO. Built with TypeScript/React and Next.js, it leverages GPT-4o-mini via the OpenAI API and uses IndexedDB for local storage.

## Method Summary
The platform implements a sequential research workflow where domain-agnostic assistants handle specific tasks, passing structured assets between them. Each assistant is defined by a curated system prompt and can access external scholarly services through tool calling mechanisms. The frontend uses a Generative UI approach to render specialized components based on tool outputs. Generated assets are stored locally in IndexedDB and can be exported as RO-Crate bundles with semantic annotations. The implementation uses the Vercel AI SDK for model abstraction and Next.js API routes as a secure proxy to the OpenAI API.

## Key Results
- Sequential workflow enables passing structured research assets between specialized assistants
- Tool calling mechanism provides access to external scholarly services for factual information
- Generative UI renders specialized interfaces for different tool outputs on demand
- RO-Crate export with DOCO/DEO ontologies enables provenance tracking and reproducibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured data assets passed between specialized assistants enable a sequential research workflow.
- Mechanism: The platform decouples the research process into discrete tasks, each handled by a dedicated assistant with a curated system prompt. Outputs are explicitly saved as structured "assets," which are then loaded as inputs for downstream assistants.
- Core assumption: The research life cycle can be effectively decomposed into a linear sequence of independent tasks where the output of one task is a sufficient and well-formed input for the next.
- Evidence anchors:
  - [abstract]: "The AIssistant consists of a collection of assistants, each responsible for a specific research task... Generated data is stored in the assets..."
  - [section]: "If the assistants are used in sequence, generated output is stored and are used as input for the next assistant." (Page 1)
- Break condition: The pipeline fails if an upstream assistant generates an asset that is malformed, lacks context, or is of poor quality.

### Mechanism 2
- Claim: Tool calling via REST APIs grounds the LLM in real-world scholarly data.
- Mechanism: Assistants are equipped with tools defined by a name, description, and input schema. The LLM can decide to call external scholarly services (e.g., Semantic Scholar, ORKG Ask) via REST endpoints to retrieve factual, current information.
- Core assumption: The LLM can reliably determine the correct tool, construct a valid API request from the user's natural language query, and accurately synthesize the API's structured response.
- Evidence anchors:
  - [abstract]: "...tools are provided to give access to external scholarly services."
  - [section]: "For example, this makes it possible for assistants to find related work via actual scholarly search platforms..." (Page 1)
- Break condition: The mechanism breaks if the LLM hallucinates tool parameters, calls the wrong tool, or if the external service is unavailable or returns irrelevant results.

### Mechanism 3
- Claim: A Generative UI paradigm creates specialized interfaces for tool outputs on demand.
- Mechanism: When a specific tool is invoked, the frontend dynamically renders a specialized, interactive UI component tailored to that tool's data structure, rather than rendering all outputs as plain text.
- Core assumption: The user experience for research tasks is improved by providing interactive, structured UI elements rather than relying solely on conversational text for complex data like search results.
- Evidence anchors:
  - [section]: "This follows the paradigm of Generative UI, where UI components are shown on demand, depending on the task at hand." (Page 3)
- Break condition: The mechanism fails if the frontend lacks a component definition for a newly added tool or if the mapping between a tool's output and its UI component is ambiguous.

## Foundational Learning

### Concept: Tool Calling / Function Calling
- Why needed here: This is the core technical bridge allowing the LLM to interact with external scholarly APIs. An engineer must understand how to define a tool's schema and how the LLM's output maps to a function execution.
- Quick check question: How does an LLM decide which tool to call and what parameters to use from a user's natural language query?

### Concept: Provenance and RO-Crate
- Why needed here: A central goal of the platform is transparency and reproducibility via RO-Crate export. Understanding how to generate and structure metadata is essential for implementing the export functionality.
- Quick check question: What is the primary purpose of an RO-Crate, and what key provenance information would a user need to provide during export?

### Concept: Generative User Interfaces (Generative UI)
- Why needed here: The platform dynamically renders UI components based on tool output. This requires a non-static frontend architecture that can map tool results to React components.
- Quick check question: How does the frontend decide whether to render a standard text message or a specialized UI component like a literature search list?

## Architecture Onboarding

### Component map:
Frontend (React/Next.js) -> Backend (Next.js API routes) -> LLM Service (OpenAI GPT-4o-mini) -> External Tool Services (REST APIs) -> Asset Store (IndexedDB)

### Critical path:
1. User selects an assistant (e.g., "Related literature")
2. User provides input, either text or via assets from a previous step
3. Frontend sends the prompt + available tool definitions to the Backend
4. Backend calls the LLM
5. LLM returns either a text response or a tool call request
6. If a tool call is requested, Backend executes it against the External Tool Service
7. Tool output is returned to the LLM for synthesis (and potentially to the frontend for Generative UI)
8. Final response is rendered, with interactive UI components displayed if applicable

### Design tradeoffs:
- **Client-side storage (IndexedDB)** vs. server-side database: Prioritizes user privacy and offline capability but limits cross-device access and collaboration
- **GPT-4o-mini for all assistants** vs. task-specific models: Reduces complexity and cost but may limit performance on tasks requiring more advanced reasoning
- **Curated prompts (assistants)** vs. open prompting: Lowers the barrier for non-expert users but reduces flexibility for advanced users

### Failure signatures:
- **Tool calling loop:** The LLM repeatedly calls the wrong tool or with invalid parameters
- **Asset mismatch:** An assistant fails to parse the asset provided by a preceding assistant due to unexpected format or content
- **Stale local data:** Assets stored in IndexedDB become outdated or corrupted, affecting downstream assistants

### First 3 experiments:
1. **Tool Calling Verification:** Select the "Related literature" assistant with a clear research question. Verify that the correct API (e.g., Semantic Scholar) is called with valid parameters and that results are rendered in the specialized UI.
2. **Sequential Pipeline Test:** Run the full sequence from "Ideation" to "Paper writing: related work." Check if the "Research questions" asset from step 2 is correctly available and used by the "Related literature" assistant in step 3.
3. **RO-Crate Export Inspection:** Generate artifacts through a couple of assistants and trigger the export. Examine the resulting JSON-LD metadata file to confirm that provenance data and semantic annotations (DOCO/DEO) are present and correctly structured.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the objective effectiveness of specific assistants and the overall usability of the platform for researchers?
- Basis in paper: [explicit] The authors state that "future work is the evaluation of our approach," focusing on both "specific assistants" and "evaluating the platform from a usability perspective."
- Why unresolved: The current work serves as a proof-of-concept demonstration and implementation walk-through, lacking formal quantitative or qualitative assessment of the system's utility.
- Evidence: Results from user studies measuring task completion rates and System Usability Scale (SUS) scores for the sequential workflow compared to baseline LLM interfaces.

### Open Question 2
- Question: Can smaller, locally hosted, or fine-tuned models match the performance of GPT-4o-mini for specific assistant tasks while reducing resource consumption?
- Basis in paper: [explicit] The authors identify using task-tailored models as an "important research direction for future work" to benefit costs and environmental impact.
- Why unresolved: The current prototype uses GPT-4o-mini uniformly for all assistants; experimenting with other models is explicitly "out of scope for this work."
- Evidence: Comparative benchmarks showing accuracy, latency, and resource usage trade-offs between GPT-4o-mini and specialized smaller models on tasks like ideation or proofreading.

### Open Question 3
- Question: How does the sequential reliance on generated assets affect the robustness of the research output regarding error propagation?
- Basis in paper: [inferred] The paper describes a workflow where output assets (e.g., "Ideation topics") become input for subsequent assistants. If an early assistant hallucinates, the error may compound through the pipeline.
- Why unresolved: The demonstration covers a successful "happy path" walk-through, but does not analyze failure modes or the sensitivity of later stages to noisy intermediate inputs.
- Evidence: Analysis of final draft quality when injecting perturbations or hallucinations into upstream assets to measure output degradation.

## Limitations
- **Missing implementation details**: System prompts for assistants and tool schemas are not provided, making faithful reproduction difficult
- **Limited validation**: Only demonstrates workflow without quantitative performance metrics or user studies
- **Scalability concerns**: Single GPT-4o-mini dependency creates potential cost and performance bottlenecks

## Confidence

**High Confidence (9/10)**: The architectural framework is clearly specified - React/Next.js frontend, IndexedDB storage, OpenAI API integration, and the general concept of domain-agnostic assistants working sequentially. The RO-Crate export mechanism and provenance tracking using DOCO/DEO ontologies are well-defined technical implementations.

**Medium Confidence (6/10)**: The sequential workflow concept and tool calling mechanism are logically sound and technically feasible, but the actual implementation details (system prompts, tool schemas, UI component mappings) are missing. The platform's effectiveness in real research scenarios cannot be verified without these specifications.

**Low Confidence (3/10)**: Claims about the platform's practical utility for researchers are largely untested. Without user studies, performance benchmarks, or quality assessments of generated research artifacts, the real-world value proposition remains speculative.

## Next Checks
1. **Tool Calling Accuracy Test**: Implement the "Related literature" assistant with Semantic Scholar API integration. Measure the success rate of correct tool selection and parameter construction across 20 diverse research queries. Document failure modes when the LLM selects wrong tools or constructs invalid requests.

2. **Sequential Pipeline Quality Assessment**: Run the full workflow from "Ideation" through "Paper writing" with three different research topics. Evaluate whether assets (especially research questions and literature summaries) maintain coherence and relevance when passed between assistants. Rate the logical flow and quality of the final draft sections.

3. **RO-Crate Export Completeness Verification**: Generate research artifacts using at least two assistants, then export as RO-Crate. Validate that all required provenance fields are populated, DOCO/DEO annotations are correctly applied, and the resulting metadata file validates against the RO-Crate specification. Test import capability in a separate RO-Crate viewer.