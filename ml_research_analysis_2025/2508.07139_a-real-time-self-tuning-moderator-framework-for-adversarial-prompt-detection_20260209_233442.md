---
ver: rpa2
title: A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection
arxiv_id: '2508.07139'
source_url: https://arxiv.org/abs/2508.07139
tags:
- arxiv
- prompt
- prompts
- adversarial
- rtst
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RTST is a real-time, self-tuning moderator framework that defends\
  \ against adversarial prompts without degrading responses to benign prompts. It\
  \ uses two LLM agents\u2014an Evaluator and a Reviewer\u2014to score prompts against\
  \ a set of Behaviors and adapt weights in real time."
---

# A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection

## Quick Facts
- **arXiv ID:** 2508.07139
- **Source URL:** https://arxiv.org/abs/2508.07139
- **Reference count:** 40
- **Primary result:** Real-time self-tuning moderator reduces jailbreak success rates from 12-63% to 0-17% while modestly increasing false refusals.

## Executive Summary
RTST is a real-time, self-tuning moderator framework that defends against adversarial prompts without degrading responses to benign prompts. It uses two LLM agents—an Evaluator and a Reviewer—to score prompts against a set of Behaviors and adapt weights in real time. Experiments on Gemini 2.5 Flash showed RTST reduced attack success rates from 12–63% to 0–17% across multiple jailbreak datasets while increasing false refusal rates modestly (from 12% to 18%). Ablation tests indicated real-time weight tuning improved detection accuracy and F1 scores, supporting the viability of lightweight, adaptive defense against evolving adversarial attacks.

## Method Summary
RTST employs a two-agent LLM architecture: an Evaluator agent scores prompts against a Behavior vector, and a Reviewer agent decides whether to flag the prompt as adversarial based on these scores. The framework self-tunes by dynamically adjusting the weights of the Behavior vector in real time, enabling it to adapt to evolving adversarial patterns without manual intervention. This design allows RTST to maintain high detection accuracy while minimizing false refusals for benign prompts.

## Key Results
- RTST reduced attack success rates from 12-63% to 0-17% across multiple jailbreak datasets.
- False refusal rates increased modestly from 12% to 18%.
- Ablation studies showed real-time weight tuning improved detection accuracy and F1 scores.

## Why This Works (Mechanism)
RTST's real-time self-tuning mechanism allows it to dynamically adapt to new adversarial patterns as they emerge, maintaining robust detection without degrading benign prompt handling. The two-agent design separates scoring (Evaluator) from decision-making (Reviewer), enabling specialized reasoning and reducing cognitive load on each agent. The Behavior vector provides a structured, interpretable representation of prompt characteristics, allowing targeted weight adjustments. This combination of adaptive weighting, specialized agents, and structured scoring enables effective defense against evolving adversarial attacks.

## Foundational Learning
- **LLM-based moderation:** Using LLMs to detect adversarial prompts leverages their reasoning capabilities but requires careful design to avoid false positives and negatives. *Why needed:* Traditional rule-based filters are brittle and easily bypassed. *Quick check:* Compare LLM-based moderation accuracy against static filters on a benchmark dataset.
- **Self-tuning via weight adaptation:** Dynamically adjusting detection criteria weights allows the system to respond to new attack patterns without manual retraining. *Why needed:* Adversarial techniques evolve rapidly, outpacing static defenses. *Quick check:* Measure detection performance before and after weight adaptation on shifted adversarial datasets.
- **Two-agent architecture:** Separating scoring and decision-making reduces cognitive load and improves reasoning quality. *Why needed:* Single-agent systems may struggle with complex trade-offs between detection and usability. *Quick check:* Compare single vs. two-agent performance on edge-case prompts.

## Architecture Onboarding

**Component map:** User Input -> Evaluator -> Reviewer -> Output Decision

**Critical path:** Prompt is scored by Evaluator, scores are reviewed by Reviewer, decision is made on whether to flag as adversarial.

**Design tradeoffs:** Two-agent architecture adds inference overhead but improves reasoning quality and reduces false positives. Real-time weight tuning increases complexity but enables adaptation to new attacks. Behavior vector provides interpretability but requires careful weight initialization.

**Failure signatures:** High false refusal rates indicate overly aggressive weights; persistent attack success suggests insufficient weight adaptation or poor Behavior vector design.

**First experiments:**
1. Measure baseline detection accuracy and false refusal rate on standard jailbreak datasets.
2. Test weight adaptation performance by introducing a shifted adversarial dataset mid-evaluation.
3. Compare RTST performance against static moderation baselines (e.g., rule-based filters, single-agent LLM moderation).

## Open Questions the Paper Calls Out
None

## Limitations
- RTST's performance is evaluated primarily on Gemini 2.5 Flash, so generalizability to other LLM architectures and modalities remains uncertain.
- The reported false refusal rate increase (12% → 18%) is modest but still introduces friction; real-world deployment would need to assess usability impact and user trust erosion.
- The weight-adaptation mechanism assumes adversarial patterns shift gradually enough for online learning to track—sudden shifts or coordinated attacks could outpace the adaptation rate.

## Confidence
- **High:** RTST reduces attack success rates across tested jailbreak datasets; real-time weight tuning improves detection metrics in ablation studies.
- **Medium:** Generalization to other LLM models and attack types; real-world false refusal impact.
- **Low:** Robustness against sudden adversarial shifts; scalability of inference overhead in production.

## Next Checks
1. Replicate RTST on at least two additional LLM families (e.g., GPT-4, open-source models) to test cross-model robustness.
2. Conduct a user study measuring the impact of increased false refusals on task completion and perceived helpfulness.
3. Stress-test the adaptive weights against a synthetic attack that shifts patterns faster than the update interval to quantify tracking limits.