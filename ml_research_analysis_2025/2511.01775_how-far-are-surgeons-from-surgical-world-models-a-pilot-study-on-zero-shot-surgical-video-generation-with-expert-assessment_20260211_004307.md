---
ver: rpa2
title: How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot
  Surgical Video Generation with Expert Assessment
arxiv_id: '2511.01775'
source_url: https://arxiv.org/abs/2511.01775
tags:
- surgical
- video
- plausibility
- surgery
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates whether advanced video generation models can
  serve as world models in surgery by curating SurgVeo, the first expert-curated benchmark
  for surgical video generation, and introducing the Surgical Plausibility Pyramid
  (SPP) framework to assess model outputs across four dimensions: Visual Perceptual,
  Instrument Operation, Environment Feedback, and Surgical Intent Plausibility. Using
  the Veo-3 model on 50 surgical clips from laparoscopic and neurosurgical procedures,
  a panel of four board-certified surgeons evaluated the generated videos.'
---

# How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment

## Quick Facts
- arXiv ID: 2511.01775
- Source URL: https://arxiv.org/abs/2511.01775
- Reference count: 40
- Key outcome: Veo-3 achieves high visual perceptual plausibility but critically fails at higher SPP levels, revealing that visually convincing mimicry does not equate to causal understanding in surgery.

## Executive Summary
This pilot study investigates whether advanced video generation models can serve as world models in surgery by curating SurgVeo, the first expert-curated benchmark for surgical video generation. Using the Veo-3 model on 50 surgical clips from laparoscopic and neurosurgical procedures, a panel of four board-certified surgeons evaluated generated videos across the Surgical Plausibility Pyramid framework. Results reveal a "plausibility gap": while Veo-3 produces visually convincing outputs, it fails at higher-level dimensions of surgical understanding including instrument operation, environment feedback, and surgical intent. The study establishes that current models mimic appearance without capturing the causal knowledge essential for surgical reasoning.

## Method Summary
The study curated SurgVeo, a benchmark of 50 surgical video clips from laparoscopic hysterectomy and endoscopic pituitary procedures, and introduced the Surgical Plausibility Pyramid (SPP) framework with four hierarchical evaluation dimensions. Veo-3 was accessed through Google Flow platform to generate 8-second video continuations from single input frames under baseline and stage-aware prompting conditions. Four board-certified surgeons (two per specialty) evaluated generated videos against reference videos at 1, 3, and 8-second time points using 5-point SPP rubrics. The study compared baseline versus stage-aware prompting and tracked temporal degradation across the 8-second generation horizon.

## Key Results
- Veo-3 achieved high scores in Visual Perceptual Plausibility (mean ~4.5/5) but failed at higher SPP levels, with Instrument Operation Plausibility dropping to ~2.0/5
- Environment Feedback Plausibility plummeted from 3.06±0.08 at 1 second to 1.64±0.12 by 8 seconds, a drop of nearly 46%
- Stage-aware prompting provided no significant improvement, indicating the model lacks internal representations to translate procedural context into appropriate actions
- Across all SPP dimensions, generated videos showed statistically significant differences from reference videos (p < 0.001) except for Visual Perceptual Plausibility at early time points

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Plausibility Decomposition via SPP
- Claim: Evaluating surgical video generation requires a four-tier hierarchy separating visual fidelity from causal reasoning.
- Mechanism: The SPP decomposes evaluation into Visual Perceptual → Instrument Operation → Environment Feedback → Surgical Intent. Each level requires progressively more abstract causal knowledge. Models can succeed at lower levels while failing completely at higher ones.
- Core assumption: Surgical expertise is hierarchical and can be meaningfully separated into four distinct competence levels.
- Evidence anchors: Abstract states SPP is "a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy"; Section 2.3.2 describes the multi-level assessment; related work uses similar hierarchical error taxonomies.

### Mechanism 2: Temporal Error Accumulation in Prediction Horizons
- Claim: Surgical plausibility degrades predictably over time because models lack causal coherence to maintain logical consistency beyond short windows.
- Mechanism: Zero-shot generation from single frames requires extrapolating surgical dynamics. Without causal understanding, errors compound: wrong instrument trajectory leads to impossible tissue feedback, cascading into incoherent surgical intent.
- Core assumption: The 8-second horizon is sufficient to capture complete surgical actions while being long enough to reveal causal failures.
- Evidence anchors: Section 3.2 shows Environment Feedback Plausibility dropped from 3.06±0.08 at 1s to 1.64±0.12 by 8s (nearly 46%); Fig. 3, Fig. 4 show Visual Perceptual scores remain stable while higher dimensions compress toward bottom.

### Mechanism 3: Context Injection Failure via Stage-Aware Prompting
- Claim: Providing explicit procedural context does not improve surgical plausibility because the model lacks internal representations to translate stage labels into appropriate action sequences.
- Mechanism: Stage-aware prompts add textual context ("current stage is vessel ligation") to visual input. If the model had learned surgical knowledge, this would constrain generation to stage-appropriate actions. Null result indicates the model cannot map abstract procedural concepts to concrete physical operations.
- Core assumption: Veo-3 architecture is capable of integrating text and visual modalities for conditional generation in other domains; failure here is specific to surgical knowledge gaps.
- Evidence anchors: Section 3.4 states "providing additional contextual information yields no significant or consistent improvement"; Table 1, Table 2 show baseline vs. stage-aware scores show no meaningful differences.

## Foundational Learning

- Concept: **World Models as Simulators**
  - Why needed here: The paper positions video generation models as candidate world models that should predict how environments evolve under actions. Understanding this framing is essential to interpret why visual fidelity ≠ world modeling capability.
  - Quick check question: Given a frame of a ball mid-air, can you distinguish a model that learned physics from one that memorized ball trajectories?

- Concept: **Causal vs. Correlational Learning in Video**
  - Why needed here: The "plausibility gap" reveals that models learned statistical patterns (instruments appear near tissue) rather than causal mechanisms (cutting causes bleeding proportional to vessel size).
  - Quick check question: If you flip a surgical video temporally, which SPP dimensions would remain plausible and which would become obviously wrong?

- Concept: **Expert Evaluation as Ground Truth**
  - Why needed here: Automated metrics (FID, FVD) cannot capture surgical plausibility; board-certified surgeons provide the only reliable assessment of domain-specific causal reasoning.
  - Quick check question: Why would an automated metric rate a hallucinated instrument highly if it's visually coherent?

## Architecture Onboarding

- Component map: Input frame + text prompt → Veo-3 with Chain-of-Frames reasoning → 8-second generated video → Expert scoring across SPP dimensions → Temporal degradation analysis

- Critical path: Input frame preparation → Prompt construction → Video generation → Expert scoring across SPP dimensions → Temporal degradation analysis

- Design tradeoffs:
  - Single-frame input vs. multi-frame context: Single frame is harder but better isolates genuine predictive capability
  - 8-second horizon: Long enough to reveal cascading errors, short enough for tractable expert review
  - Two surgical specialties: Laparoscopic (larger movements, more forgiving) vs. neurosurgery (microscopic precision, higher stakes)

- Failure signatures:
  - Instrument hallucination: Non-existent tools, wrong tool types for procedure stage
  - Physics violations: Suction pulling solid masses, tissue stretching without tearing
  - Intent errors: Performing completed actions, ignoring active bleeding to cauterize elsewhere
  - Visual quality remains high while all other dimensions fail (the core "plausibility gap")

- First 3 experiments:
  1. Reproduce SPP evaluation on SurgVeo benchmark to validate inter-rater reliability across your own expert panel
  2. Test alternative prompting strategies: chain-of-thought prompts that explicitly state "first the surgeon will X, then Y because Z" to diagnose whether the gap is promptable
  3. Fine-tune a smaller video generation model on surgical videos only; compare SPP scores to zero-shot Veo-3 to isolate whether domain-specific training closes the plausibility gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-specific fine-tuning or architectural modifications enable video generation models to capture surgical causal knowledge, or is fundamental architectural innovation required?
- Basis in paper: Authors state "merely scaling up training on general data will be insufficient to bridge this divide. Achieving true world modeling in expert domains will likely require new architectural paradigms capable of integrating structured, domain-specific knowledge."
- Why unresolved: Only tested zero-shot Veo-3; no fine-tuning experiments or alternative architectures evaluated.
- What evidence would resolve it: Comparative experiments fine-tuning video generation models on surgical data versus new architectures with explicit causal reasoning modules.

### Open Question 2
- Question: What minimum scale and diversity of surgical video data would be needed to achieve clinically useful plausibility across all four SPP dimensions?
- Basis in paper: Benchmark contains only 50 clips from two procedures; authors note surgery requires "specialized domain expertise spanning anatomy, pathology, biomechanics, and clinical reasoning accumulated through years of rigorous training."
- Why unresolved: No systematic investigation of data scaling effects; current benchmark too limited to establish data requirements.
- What evidence would resolve it: Controlled experiments varying surgical dataset size, specialty coverage, and procedural stage diversity while measuring SPP performance.

### Open Question 3
- Question: Can automated evaluation metrics be developed that correlate with expert surgeon assessments across all SPP levels?
- Basis in paper: Authors state "human expertise is indispensable for accurately assessing the subtle yet critical aspects of surgical plausibility, a nuance that cannot be captured by automated metrics."
- Why unresolved: No automated metrics proposed or validated against expert scores; expert evaluation remains the only reliable assessment method.
- What evidence would resolve it: Development and validation of automated metrics showing strong correlation (r > 0.8) with expert surgeon SPP scores across large-scale datasets.

## Limitations

- Reliance on single proprietary model (Veo-3) through Google's Flow platform limits reproducibility and generalization to other architectures
- Expert evaluation depends on subjective interpretation of surgical plausibility, though standardized rubrics mitigate this
- 8-second temporal horizon may not capture longer-term surgical planning failures

## Confidence

- **High Confidence**: Visual Perceptual Plausibility results—the clear gap between visual quality (high) and higher SPP dimensions (low) is consistently observed and directly measurable
- **Medium Confidence**: The broader claim that this represents a fundamental limitation of current world models in surgery
- **Low Confidence**: The assertion that surgical expertise is cleanly decomposable into the four SPP dimensions

## Next Checks

1. Test the SPP framework on at least three different video generation architectures to determine whether the plausibility gap is architecture-specific or universal to current approaches

2. Conduct a formal inter-rater reliability study with 8-12 surgeons scoring the same generated videos independently, calculating Cohen's kappa to quantify agreement

3. Fine-tune a smaller, accessible video generation model on surgical videos only, then compare SPP scores to zero-shot Veo-3 results to isolate whether domain-specific training can close the plausibility gap