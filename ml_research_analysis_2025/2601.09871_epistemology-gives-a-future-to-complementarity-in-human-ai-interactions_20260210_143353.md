---
ver: rpa2
title: Epistemology gives a Future to Complementarity in Human-AI Interactions
arxiv_id: '2601.09871'
source_url: https://arxiv.org/abs/2601.09871
tags:
- complementarity
- human-ai
- interaction
- human
- reliability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of grounding the concept of complementarity
  in human-AI interactions within a rigorous epistemological framework. It reframes
  complementarity not as a post-hoc relative accuracy measure, but as a reliability
  indicator within the framework of computational reliabilism (CR).
---

# Epistemology gives a Future to Complementarity in Human-AI Interactions

## Quick Facts
- arXiv ID: 2601.09871
- Source URL: https://arxiv.org/abs/2601.09871
- Authors: Andrea Ferrario; Alessandro Facchini; Juan M. Durán
- Reference count: 9
- The paper reframes complementarity as a reliability indicator within computational reliabilism, offering actionable design guidance for human-AI interactions.

## Executive Summary
The paper addresses the challenge of grounding complementarity in human-AI interactions within a rigorous epistemological framework. It reframes complementarity not as a post-hoc relative accuracy measure, but as a reliability indicator within computational reliabilism (CR). Complementarity (CTP) is treated as a central type-RI1 reliability indicator, signaling whether the human-AI team outperforms either component alone. The paper identifies and addresses four key theoretical challenges: lack of theoretical anchoring, limited applicability at decision time, neglect of desiderata beyond relative accuracy, and abstraction from the magnitude-cost profile of gains. By embedding complementarity within CR, the paper offers actionable design guidance, including a minimal reporting checklist for 'justificatory human-AI interactions' and measures of efficient complementarity. This repositioning shifts the focus from achieving complementarity for its own sake to using it as evidence for the overall reliability of the interaction process, supported by additional reliability indicators (type-RI2 and type-RI3) covering epistemic standards, validation, governance, and socio-technical practices.

## Method Summary
The paper reframes complementarity within computational reliabilism by defining a prediction-task human-AI interaction (PT-HAI) process and its associated reliability indicators. The method computes Complementarity Team Performance (CTP) as a binary indicator of whether the team outperforms either human or AI alone, then introduces gross and net gain measures that account for interaction costs. The approach requires logging human, AI, and team predictions per instance, computing task-appropriate losses, and evaluating efficiency via net gain relative to institutional thresholds. The framework is positioned as a theoretical repositioning rather than an empirical validation, with emphasis on providing a structured checklist and conceptual tools for practitioners.

## Key Results
- Complementarity (CTP) is repositioned as a type-RI1 reliability indicator within computational reliabilism rather than a standalone accuracy metric
- Reliability emerges from the interaction of three indicator families: type-RI1 (technical performance), type-RI2 (epistemic standards), and type-RI3 (socio-technical practices)
- Efficient complementarity requires positive net gain after accounting for interaction costs, operationalized via the formula Δnetτ(D) = Δτ(D) - λc(D)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Historical complementarity provides defeasible evidence for PT-HAI reliability.
- Mechanism: Complementarity (CTP=1) signals that the interaction protocol enables epistemic gain beyond either component alone. Repeated instances accumulate into a reliability indicator supporting justification for third parties to accept outputs as epistemically adequate.
- Core assumption: PT-HAIs function as computational processes whose reliability comes in degrees and is supported by heterogeneous markers.
- Evidence anchors:
  - [abstract] "Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human–AI interaction is a reliable epistemic process for a given predictive task."
  - [section 3.2] "Epistemically, historical complementarity supports a counterfactual claim: that had either component acted alone, the PT-HAI would have been less reliable."
  - [corpus] Limited direct evidence in corpus; related work on "Human-AI Collaborative Uncertainty Quantification" addresses uncertainty in collaborative settings but not this specific reliability mechanism.
- Break condition: If complementarity is unstable across time, distribution shifts, or model updates, its justificatory weight weakens (type-RI3 indicators erode).

### Mechanism 2
- Claim: Reliability emerges from interaction of three indicator families, not from complementarity alone.
- Mechanism: type-RI1 (technical performance including CTP) + type-RI2 (epistemic standards: task validity, uncertainty discipline) + type-RI3 (socio-technical practices: training, governance, monitoring) jointly determine PT-HAI reliability. Complementarity is one signal among many.
- Core assumption: No single reliability indicator can secure reliability across all processes and contexts.
- Evidence anchors:
  - [section 3.1] "The reliability of the computational process is given by families of properties, capabilities, and practices that provide defeasible evidence that the process is reliable in the context where it is performed."
  - [section 3.2] "High PT-HAI reliability therefore requires complementarity together with additional indicators of epistemic adequacy."
  - [corpus] "Development of Mental Models in Human-AI Collaboration" emphasizes designing collaboration setups but does not address multi-indicator reliability frameworks.
- Break condition: Treating CTP as a stand-alone gold standard ignores epistemic constraints (fairness, robustness) and institutional requirements, leading to unreliable deployments despite complementarity.

### Mechanism 3
- Claim: Efficient complementarity requires net gain analysis accounting for interaction costs.
- Mechanism: Define gross gain Δτ(D) = min{LH, LAI} − LHAI, then net gain Δnetτ(D) = Δτ(D) − λc(D) where c(D) captures monitoring, review, documentation costs. Positive net gain means complementarity is worth its cost.
- Core assumption: Epistemic gains must be commensurate with cognitive and institutional burdens required to sustain them.
- Evidence anchors:
  - [section 7.3] "We say that complementarity on D is efficient when it produces positive net gain Δnetτ(D), i.e., when the observed gain Δτ(D) per unit of interaction cost exceeds a threshold λ."
  - [section 5.1] "Designers should target and report efficient complementarity explicitly: if gains require sustained monitoring, longer deliberation, or extensive retraining, the design may merely shift epistemic burden onto users."
  - [corpus] No direct corpus evidence on cost-aware complementarity measures.
- Break condition: Small performance gains requiring disproportionate expert time, documentation, or throughput loss erode practical value and may render PT-HAI less defensible than simpler reliance protocols.

## Foundational Learning

- Concept: **Process Reliabilism**
  - Why needed here: CR evaluates the PT-HAI process, not isolated agent accuracy. Understanding this shift is essential before implementing complementarity metrics.
  - Quick check question: Can you explain why a perfect AI with human oversight might still yield an unreliable PT-HAI if governance practices are absent?

- Concept: **Complementarity Team Performance (CTP)**
  - Why needed here: CTP is the formal criterion the paper repositions; understanding its limitations (ex-post, ground-truth-dependent, relative) motivates the broader reliability framework.
  - Quick check question: Why does CTP=1 not guarantee that a PT-HAI is reliable enough for deployment?

- Concept: **Efficiency Threshold (λ)**
  - Why needed here: λ operationalizes institutional judgments about acceptable gain-cost trade-offs; selecting λ is a type-RI3 governance decision.
  - Quick check question: What factors should determine whether λ is high (strict) or low (permissive) in a clinical deployment versus a training environment?

## Architecture Onboarding

- Component map:
  - PT-HAI Process Layer: Human + AI + Protocol Π → Team output ŷHAI
  - Measurement Layer: Compute LH, LAI, LHAI, CTP, gross gain Δτ(D), cost c(D)
  - Reliability Assessment Layer: Aggregate type-RI1 (CTP, stability), type-RI2 (validity, uncertainty discipline), type-RI3 (training, monitoring, governance)
  - Justification Layer: Evidence package for third parties (patients, regulators)

- Critical path:
  1. Define task τ, dataset D, and protocol Π
  2. Log human predictions ŷH, AI predictions ŷAI, and team outputs ŷHAI per instance
  3. Compute CTP and track stability across datasets/updates
  4. Estimate interaction cost c(D) in context-appropriate units
  5. Compare Δτ(D)/c(D) against institutional threshold λ
  6. Populate minimal reporting checklist (Table 1)

- Design tradeoffs:
  - Higher λ (stricter efficiency) → fewer deployments pass, but those that do are more defensible
  - Richer interaction protocols → more opportunity for complementarity but higher c(D)
  - Allowing "degenerate cases" (perfect AI, human defers) simplifies but removes complementarity as justification

- Failure signatures:
  - CTP unstable across time or model versions → type-RI1 weakened, requires investigation
  - Δτ(D) positive but Δnetτ(D) negative → inefficient design, burden shifted to users
  - Strong type-RI1 but weak type-RI3 → process may be technically sound but institutionally fragile

- First 3 experiments:
  1. **Baseline CTP estimation**: Run PT-HAI on held-out dataset; compute CTP, LH, LAI, LHAI. If CTP=0, no complementarity to justify.
  2. **Cost instrumentation**: Track time-on-task, review events, and escalation frequency per case; estimate c(D) to compute net gain.
  3. **Stress test for stability**: Introduce distribution shift or model update; re-measure CTP and Δnetτ(D) to assess robustness of reliability profile.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does complementarity persist over time across data distribution shifts, AI system updates, and human learning?
- Basis in paper: [explicit] The paper states that "its stability under AI system updates and human learning is poorly understood" (Challenge II, Section 2.4) and notes that current formulations are "silent on how robust CTP must be across temporal changes."
- Why unresolved: The paper repositions complementarity as historical evidence for reliability but does not empirically or theoretically specify the conditions under which past complementarity generalizes to future instances.
- What evidence would resolve it: Longitudinal studies measuring CTP stability under controlled distribution shifts, model version changes, and user expertise gains across multiple domains.

### Open Question 2
- Question: How should the efficiency threshold λ for net complementarity gain be justified and set in different institutional contexts?
- Basis in paper: [explicit] Section 7.3 introduces λ as "an institutionally-fixed limit" but states only that "a value for λ can be introduced using internal policies in a given social context" without providing principled methods for its determination.
- Why unresolved: The framework requires domain-specific calibration of cost-benefit trade-offs, but no methodology is provided for deriving defensible λ values.
- What evidence would resolve it: Comparative studies across domains (e.g., healthcare vs. forensics) showing how different λ values affect net complementarity and downstream decision quality.

### Open Question 3
- Question: Can the type-RI1, type-RI2, and type-RI3 reliability indicators be operationalized into measurable, domain-specific metrics that support cross-context comparison?
- Basis in paper: [inferred] The paper provides abstract characterizations of reliability indicator families (Section 3.1) and a minimal checklist (Table 1), but acknowledges these are "intentionally minimal" and that "richer domains may require additional constraints."
- Why unresolved: The conceptual framework does not specify concrete measurement procedures or validation criteria for each indicator type.
- What evidence would resolve it: Development and empirical testing of standardized measurement instruments for each reliability indicator category across multiple high-stakes domains.

### Open Question 4
- Question: How does warrant transmission function when multiple reliability indicators conflict—for example, when strong complementarity coexists with weak type-RI3 governance indicators?
- Basis in paper: [explicit] Section 3.5 mentions "warrant transmission and credit assignment" as recent challenges for computational reliabilism, and Section 3.1 notes that indicators "interlock in practice and jointly support the process reliability" without specifying aggregation rules.
- Why unresolved: The paper states reliability "comes in degrees" but provides no mechanism for weighing conflicting indicators.
- What evidence would resolve it: Experimental or case-study evidence showing how stakeholders weigh and integrate conflicting reliability signals in real decision contexts.

## Limitations

- The framework provides theoretical grounding but lacks empirical validation of the three-mechanism model in real-world deployments
- Interaction costs c(D) and efficiency threshold λ are left to institutional discretion without domain-specific calibration guidance
- Stability of complementarity under distribution shifts, model updates, and human learning is acknowledged as an open challenge but not empirically tested

## Confidence

- **High**: Theoretical grounding of complementarity within computational reliabilism is well-supported
- **Medium**: Operational recommendations and minimal reporting checklist are conceptually sound but underdeveloped
- **Low**: Practical calibration of cost-efficiency measures and λ thresholds lacks empirical benchmarks

## Next Checks

1. Empirical test of the three-indicator model: Track a deployed PT-HAI system over time to measure how type-RI1, type-RI2, and type-RI3 indicators jointly predict reliability versus CTP alone.
2. Cost-benefit calibration study: Collect interaction time, review effort, and escalation frequency data across multiple domains to derive domain-specific c(D) estimates and λ thresholds.
3. Stability experiment: Evaluate CTP stability when introducing dataset shifts, model updates, or protocol modifications to test the robustness of complementarity as a reliability indicator.