---
ver: rpa2
title: Out-of-Vocabulary Sampling Boosts Speculative Decoding
arxiv_id: '2506.03206'
source_url: https://arxiv.org/abs/2506.03206
tags:
- acceptance
- drafter
- distribution
- target
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Redistributing Drafter Kernels (RDK) enables speculative decoding\
  \ with highly pruned drafters by recovering out-of-vocabulary tokens. RDK uses token-affinity\
  \ priors to redistribute the drafter\u2019s probability mass toward high-overlap\
  \ regions, boosting acceptance rates even after removing over 75% of the drafter\u2019\
  s vocabulary."
---

# Out-of-Vocabulary Sampling Boosts Speculative Decoding

## Quick Facts
- **arXiv ID**: 2506.03206
- **Source URL**: https://arxiv.org/abs/2506.03206
- **Reference count**: 40
- **Primary result**: RDK achieves higher acceptance rates than vanilla and state-of-the-art samplers with efficient O(N) approximation versus O(N²) for exact method

## Executive Summary
RDK (Redistributing Drafter Kernels) addresses a critical bottleneck in speculative decoding by enabling highly pruned drafters to maintain reasonable acceptance rates through recovery of out-of-vocabulary tokens. The method uses token-affinity priors to redistribute probability mass toward high-overlap regions between drafter and acceptor vocabularies, allowing dramatic vocabulary pruning (over 75%) while maintaining performance. This enables previously impractical drafter pruning strategies that could significantly reduce computational costs in production systems.

## Method Summary
RDK works by redistributing the drafter's probability mass toward tokens that have high overlap with the acceptor's vocabulary, using a token-affinity prior mechanism. The key innovation is recovering probability mass from out-of-vocabulary tokens rather than discarding them, which traditional pruning methods do. The paper provides both an exact O(N²) method and an efficient O(N) approximation, proving that RDK achieves higher acceptance rates than vanilla sampling and state-of-the-art alternatives. The approach is particularly effective at extreme pruning levels where other methods fail completely.

## Key Results
- RDK maintains almost constant acceptance rate of 26.7% across extreme pruning levels
- Achieves higher acceptance rates than vanilla and state-of-the-art samplers
- Enables over 75% drafter vocabulary pruning while maintaining practical performance
- O(N) approximation provides significant efficiency gains over O(N²) exact method

## Why This Works (Mechanism)
The mechanism works by preserving and redistributing probability mass from out-of-vocabulary tokens rather than discarding it. When a drafter has tokens that don't exist in the acceptor's vocabulary, traditional pruning methods simply drop those probabilities, reducing overall acceptance rates. RDK instead uses token-affinity priors to identify which out-of-vocabulary tokens have semantic or contextual overlap with in-vocabulary tokens, then redistributes that probability mass to improve acceptance likelihood. This allows the drafter to maintain expressive power even with dramatically reduced vocabulary.

## Foundational Learning
**Speculative decoding**: A technique where a smaller "drafter" model generates candidates that are then verified by a larger "acceptor" model, trading off draft quality for speed gains.
*Why needed*: Enables faster inference by leveraging cheaper models for initial generation while maintaining quality through verification.
*Quick check*: Understand the speed-quality tradeoff curve and typical acceptance rate ranges (10-30%).

**Token affinity and vocabulary overlap**: The semantic or contextual similarity between tokens across different model vocabularies.
*Why needed*: Forms the basis for redistributing probability mass from out-of-vocabulary tokens.
*Quick check*: Can you explain how token embeddings might be used to measure affinity?

**Acceptance rate optimization**: The process of maximizing the probability that drafter outputs are accepted by the acceptor model.
*Why needed*: Directly impacts the efficiency gains from speculative decoding.
*Quick check*: Know how acceptance rate affects overall speedup (acceptance rate × draft speed / acceptor speed).

## Architecture Onboarding

**Component map**: Drafter model -> RDK sampling layer -> Acceptor verification -> Final output

**Critical path**: Input → Drafter forward pass → RDK redistribution → Acceptor verification → Output decision

**Design tradeoffs**: 
- Exact O(N²) method provides optimal redistribution but is computationally expensive
- O(N) approximation sacrifices some optimality for practical efficiency
- Extreme pruning reduces memory/compute but risks unacceptable quality degradation without RDK

**Failure signatures**: 
- Very low acceptance rates indicate poor token-affinity estimation
- Inconsistent quality across different pruning levels suggests instability in redistribution mechanism
- Performance degradation on specific token types may reveal affinity bias issues

**3 first experiments**:
1. Measure acceptance rate vs. pruning percentage with and without RDK on a standard benchmark
2. Compare O(N) approximation quality against exact O(N²) method across different vocabulary sizes
3. Ablation study removing token-affinity priors to quantify their contribution to performance

## Open Questions the Paper Calls Out
None

## Limitations
- Performance relies heavily on token-affinity prior mechanism, which may not generalize well to highly non-uniform or pathological token distributions
- O(N) approximation efficiency gains haven't been thoroughly validated in real-world deployment scenarios beyond wall-clock timing
- Primary evaluation focuses on vocabulary pruning, potentially limiting generalizability to other speculative decoding optimization scenarios

## Confidence

**High confidence**: Theoretical proof of higher acceptance rates compared to vanilla and state-of-the-art samplers
**Medium confidence**: Empirical results showing constant acceptance rates across pruning levels, dependent on dataset characteristics
**Medium confidence**: O(N) complexity improvement claim, though real-world implementation trade-offs not thoroughly examined

## Next Checks

1. Test RDK's performance on datasets with highly skewed or non-standard token distributions to verify robustness of the token-affinity prior mechanism
2. Conduct comprehensive ablation studies isolating the contribution of the O(N) approximation versus the exact O(N²) method to quantify practical performance gains
3. Evaluate RDK in multi-stage speculative decoding pipelines to assess compatibility and potential synergies with other optimization strategies