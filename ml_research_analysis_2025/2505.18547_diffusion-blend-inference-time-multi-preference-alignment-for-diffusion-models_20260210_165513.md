---
ver: rpa2
title: 'Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models'
arxiv_id: '2505.18547'
source_url: https://arxiv.org/abs/2505.18547
tags:
- diffusion
- reward
- alignment
- xpre
- db-mpa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Diffusion Blend, a novel approach for inference-time
  multi-preference alignment of diffusion models without additional fine-tuning. It
  blends backward diffusion processes from models fine-tuned for basis reward functions
  to generate images aligned with any user-specified linear combination of rewards
  and KL regularization.
---

# Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models

## Quick Facts
- arXiv ID: 2505.18547
- Source URL: https://arxiv.org/abs/2505.18547
- Reference count: 40
- Key result: Introduces DB-MPA and DB-KLA algorithms that blend backward diffusion drifts at inference time to align with multi-preference linear combinations of rewards without additional fine-tuning.

## Executive Summary
Diffusion Blend proposes a novel inference-time approach for multi-preference alignment of diffusion models without additional fine-tuning. The method blends backward diffusion processes from models fine-tuned for basis reward functions to generate images aligned with any user-specified linear combination of rewards and KL regularization. Experiments using Stable Diffusion v1.5 show that Diffusion Blend consistently outperforms relevant baselines and closely matches or exceeds individually fine-tuned models in both text-image alignment and aesthetic quality across various preference weights.

## Method Summary
Diffusion Blend introduces two inference-time algorithms for multi-preference alignment. DB-MPA blends backward diffusion drifts from multiple fine-tuned models to approximate any linear combination of basis rewards, while DB-KLA interpolates between pre-trained and fine-tuned drifts to control KL regularization strength. The method relies on a Jensen gap-inspired approximation that allows linear combination of drift terms, implemented through weighted averaging of denoiser outputs during the reverse diffusion process. This approach avoids the computational cost of retraining for each preference while maintaining flexibility to satisfy arbitrary user preferences at inference time.

## Key Results
- DB-MPA consistently outperforms baselines and approaches the MORL oracle in multi-reward alignment tasks
- DB-KLA effectively controls the trade-off between reward maximization and fidelity to the base distribution
- The method maintains performance across various preference weights without requiring additional fine-tuning
- Generated images show improved text-image alignment and aesthetic quality compared to single-reward fine-tuned models

## Why This Works (Mechanism)

### Mechanism 1: Linear Composition of Backward Diffusion Drifts for Multi-Reward Alignment
By linearly blending the backward diffusion drift terms from models individually fine-tuned on basis rewards, Diffusion Blend (DB-MPA) approximates the diffusion process for any user-specified linear combination of rewards at inference time. The theoretical foundation relies on approximating a term involving the exponential of expected rewards by the expected value of the rewards themselves, leveraging the linearity of expectation. This approximation works best when rewards are predictable from noisy states, when the KL regularization is not too low, and when the conditional distribution of the clean image given the noisy state is well-behaved.

### Mechanism 2: Interpolation Between Pre-trained and Fine-tuned Drifts for KL Regularization Control
DB-KLA controls the effective KL regularization strength by interpolating the backward diffusion drift of a pre-trained model and a single-reward fine-tuned model. The method shows that the drift for a modified regularization parameter can be approximated as a linear combination of the fine-tuned and base model drifts. By blending the outputs of these models with appropriate weights, the algorithm samples from a distribution closer to or further from the pre-trained model, effectively tuning the trade-off between reward maximization and fidelity to the base distribution.

### Mechanism 3: Inference-Time Weighted Averaging of Denoiser Outputs
The algorithm implements drift blending by taking a weighted average of the outputs from multiple independently fine-tuned denoiser networks at each step of the reverse diffusion process. Rather than averaging model parameters, the method operates on the functional outputs at each timestep, computing denoised predictions from each fine-tuned model and combining them according to user preferences. This approach maintains the flexibility of inference-time alignment while avoiding the computational overhead of retraining for each preference combination.

## Foundational Learning

- **Diffusion Models (Reverse SDE/Score-Based Generative Models)**: The core mechanism involves manipulating the drift term of the reverse-time SDE that defines the generative diffusion process. Quick check: What are the two main components of a reverse-time SDE that defines the generative diffusion process? (Answer: A drift term and a diffusion/noise term).

- **KL Divergence as Regularization in Fine-Tuning**: The alignment problem is framed as maximizing reward while keeping the fine-tuned model close to the pre-trained model via KL-divergence. Quick check: In RL fine-tuning, what is the primary purpose of the KL-divergence term in the objective function? (Answer: To prevent reward hacking and preserve desirable properties from the pre-trained model).

- **Jensen's Inequality / Jensen Gap Approximation**: The theoretical foundation relies on approximating a nonlinear function of an expectation with the expectation of the function's argument. Quick check: For a convex function like e^x, what is the relationship between E[e^x] and e^(E[x])? (Answer: By Jensen's inequality, E[e^x] ≥ e^(E[x])).

## Architecture Onboarding

- **Component map**: Pre-trained Diffusion Model -> Basis Fine-Tuned Models -> Inference-Time Blender -> User Interface

- **Critical path**: 
  1. Offline Training: RL fine-tune the base model for each basis reward to get parameters
  2. Inference Setup: Load the pre-trained model and all required fine-tuned models into memory
  3. Sampling Loop: For each step, sample noise, get predictions from each loaded denoiser, compute weighted average based on user weights, update the image

- **Design tradeoffs**:
  - Memory vs. Flexibility: Loading multiple models provides inference-time flexibility but has linear memory cost
  - Approximation vs. Oracle: The blended process is an approximation of the true multi-objective aligned process, faster than retraining but may not be exact

- **Failure signatures**:
  - High Approximation Error: Generated images may not properly balance rewards if the Jensen gap approximation is poor
  - Out-of-Memory (OOM): System crashes if too many basis models are loaded simultaneously
  - Sub-optimal Balance: Images may exhibit artifacts or misalignment if basis rewards conflict

- **First 3 experiments**:
  1. Replicate DB-MPA Baseline: Fine-tune SD v1.5 on ImageReward and VILA separately, implement drift blending for 50-50 weight preference, compare against baselines
  2. Implement DB-KLA Interpolation: Using a single model fine-tuned for ImageReward, implement inference-time blending with base model for various lambda values
  3. Ablation on Number of Basis Rewards: Extend DB-MPA to 3-reward setting and evaluate Pareto front performance and approximation error

## Open Questions the Paper Calls Out
- Can model distillation or parameter sharing techniques reduce the memory overhead of storing m fine-tuned models while preserving DB-MPA's alignment performance?
- How does DB-MPA performance degrade as the KL regularization weight α approaches zero, and what is the effective lower bound?
- Does the linear reward combination assumption limit alignment performance for users with non-linear preference structures?

## Limitations
- The method requires loading multiple fine-tuned models at inference time, creating linear memory overhead
- The Jensen gap approximation error may become significant for highly unpredictable rewards or extreme KL weights
- The empirical evaluation uses a limited prompt set and only two basis rewards, leaving questions about scalability

## Confidence
- Multi-reward alignment mechanism (DB-MPA): Medium confidence - works well empirically but theoretical approximation error not quantified on real data
- KL control mechanism (DB-KLA): Higher confidence - has more direct theoretical foundation and cleaner empirical validation
- Overall claim of outperforming baselines: Medium-High confidence given quantitative results but limited evaluation scope

## Next Checks
1. Implement the Jensen gap error metric from Lemma 1 and compute it for the ImageReward and VILA rewards on real diffusion model outputs to quantify approximation quality
2. Extend the experimental evaluation to a 3-reward setting (adding PickScore or similar) to test scalability and identify where linear drift combination breaks down
3. Compare the quality of DB-MPA-generated images against a Pareto front of individually fine-tuned models across the full weight spectrum to validate claimed performance gains