---
ver: rpa2
title: 'Fine Flood Forecasts: Incorporating local data into global models through
  fine-tuning'
arxiv_id: '2504.12559'
source_url: https://arxiv.org/abs/2504.12559
tags:
- data
- fine-tuning
- global
- basins
- kratzert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that fine-tuning a globally pre-trained
  LSTM model on individual basins significantly improves flood prediction accuracy
  compared to using the pre-trained model alone. Fine-tuning increased mean NSE from
  0.473 to 0.541 (14%) and mean KGE from 0.520 to 0.599 (15%).
---

# Fine Flood Forecasts: Incorporating local data into global models through fine-tuning

## Quick Facts
- **arXiv ID:** 2504.12559
- **Source URL:** https://arxiv.org/abs/2504.12559
- **Reference count:** 21
- **Key outcome:** Fine-tuning a globally pre-trained LSTM model on individual basins significantly improves flood prediction accuracy compared to using the pre-trained model alone

## Executive Summary
This paper demonstrates a methodology for improving global flood forecasting by fine-tuning pre-trained models on local basin data. The approach leverages a globally pre-trained LSTM model trained on 6,375 basins and adapts it to individual watersheds using their local streamflow records. The results show significant performance improvements, particularly in basins where the global model performed poorly. This methodology enables national forecasters to adapt global models using their own data, facilitating operational deployment without requiring large computing resources.

## Method Summary
The methodology employs transfer learning through fine-tuning, where a pre-trained LSTM model (trained on 6,375 basins globally) is adapted to individual basins using local data. The model uses a 365-day lookback window of atmospheric and hydrological inputs, with a single LSTM layer (256 units) and static attribute embeddings. Fine-tuning is performed through a hyperparameter sweep (50 evaluations) across learning rates, loss functions, epochs, and fine-tuning modes (Full vs. Head-only). The study validates the approach on 159 randomly selected basins from the Caravan dataset, comparing pre-trained performance against fine-tuned results using NSE and KGE metrics.

## Key Results
- Fine-tuning increased mean NSE from 0.473 to 0.541 (14% improvement)
- Fine-tuning increased mean KGE from 0.520 to 0.599 (15% improvement)
- Improvements were most pronounced in basins where the pre-trained model performed poorly
- 22% of model-basin pairs showed performance degradation after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Transfer of Generalizable Hydrological Representations
The global model learns universal, regional, and local hydrological behaviors from 6,375 basins. Fine-tuning then adjusts learned weights to capture basin-specific information not fully represented in the global training distribution. Local data contains unique information that was not fully captured during global training but can be incorporated by adjusting model parameters.

### Mechanism 2: Disproportionate Gains for Underperforming Basins
Fine-tuning provides the greatest performance improvements in basins where the pre-trained model performs poorly. Basins with low pre-trained scores likely have underrepresented hydrological characteristics in the global training set. Fine-tuning allows the model to specialize for these unique conditions using local data.

### Mechanism 3: LSTM State Space as Hydrological Memory
LSTM architectures are effective for streamflow prediction because their internal state space mirrors the storage and release dynamics of physical watersheds. LSTM memory cells maintain information over the 365-day lookback window, modeling hydrological dynamics such as reservoirs, soil moisture storage, and snowpack that influence river flow with temporal lag.

## Foundational Learning

- **Concept: Transfer Learning via Fine-tuning**
  - **Why needed here:** Understanding how pre-training provides a foundation that can be efficiently adapted with limited local data is essential for practical deployment.
  - **Quick check question:** Why does fine-tuning a 260K-parameter model on a single basin outperform training an equivalent model from scratch on that same basin?

- **Concept: Nash-Sutcliffe Efficiency (NSE) and Kling-Gupta Efficiency (KGE)**
  - **Why needed here:** These are the primary evaluation metrics; understanding them is necessary to interpret the reported 14% mean NSE improvement.
  - **Quick check question:** If NSE ranges from -∞ to 1 (with 1 being perfect), what does a mean improvement from 0.473 to 0.541 indicate about practical forecast quality?

- **Concept: Hydrological Temporal Dynamics**
  - **Why needed here:** Grasping why a 365-day lookback window matters for flood prediction helps explain the LSTM architecture choice.
  - **Quick check question:** What hydrological processes might require memory of conditions from 6-12 months prior to accurately predict tomorrow's streamflow?

## Architecture Onboarding

- **Component map**: Input (365-day time series) -> Static embedding (10 neurons) -> LSTM (256 units) -> Output head (linear layer) -> Single streamflow prediction
- **Critical path**: Download pre-trained model from Hugging Face via the provided repository → Format local basin data into Caravan schema → Configure fine-tuning with hyperparameter sweep → Run fine-tuning with selected hyperparameters → Validate against NSE on held-out period
- **Design tradeoffs**: Full fine-tuning vs. Head-only (Full adapts all weights for maximum basin-specific learning; Head-only is faster but limited); Learning rate schedule (5×10^-5 then 5×10^-6); Single-basin training vs. Fine-tuning (Training from scratch underperforms due to insufficient data diversity)
- **Failure signatures**: Negative fine-tuning impact (22% of pairs degraded); Overfitting (excessive epochs or high learning rates); Poor baseline performance (if pre-trained NSE < 0.2, fine-tuning may not recover)
- **First 3 experiments**: 1) Baseline assessment: Run pre-trained model on all local basins to identify candidates with NSE < 0.5; 2) Hyperparameter sensitivity: On 3-5 basins, run 20-configuration sweep varying learning rate and fine-tuning mode; 3) Cross-validation robustness: For top-performing hyperparameters, run fine-tuning with 8 different random seeds to quantify variance

## Open Questions the Paper Calls Out
- How do specific hydrological basin characteristics (e.g., aridity, area, soil type) correlate with the magnitude of performance improvement gained from fine-tuning?
- Can a predictive model be developed to determine a priori whether fine-tuning will be beneficial for a specific basin?
- What mechanisms drive the degradation of performance in the 22% of model-basin pairs where fine-tuning reduced accuracy?

## Limitations
- The analysis is limited to the Caravan dataset with specific basin characteristics
- The 22% failure rate for fine-tuning indicates this approach is not universally beneficial
- The mechanism behind the negative correlation between pre-trained performance and fine-tuning gains remains partially unexplained

## Confidence
- **High confidence**: The reported mean improvements (NSE +0.068, KGE +0.079) are well-supported by the experimental design and statistical analysis across multiple basins and seeds
- **Medium confidence**: The hypothesis that local data contains unique information not captured in global training is supported by results but relies on assumptions about dataset representativeness
- **Medium confidence**: The recommendation that national forecasters can implement this approach with limited computing resources is practical but untested outside the study's computational environment

## Next Checks
1. Apply the fine-tuning methodology to a geographically and climatically distinct basin dataset to verify generalizability beyond Caravan's catchment characteristics
2. Systematically investigate the 22% of cases where fine-tuning degraded performance to identify whether this stems from data quality issues or fundamental limitations
3. Evaluate model performance over extended forecast horizons (7-14 days) rather than single-step predictions to assess whether fine-tuning provides sustained benefits for operational flood forecasting