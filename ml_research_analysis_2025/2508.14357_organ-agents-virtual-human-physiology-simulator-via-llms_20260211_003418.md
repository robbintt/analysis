---
ver: rpa2
title: 'Organ-Agents: Virtual Human Physiology Simulator via LLMs'
arxiv_id: '2508.14357'
source_url: https://arxiv.org/abs/2508.14357
tags:
- simulation
- respiratory
- system
- physiological
- 'null'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Organ-Agents introduces a multi-agent LLM framework to simulate
  human physiology by modeling nine organ systems with dedicated agents trained on
  ICU time-series data. The approach uses supervised fine-tuning for local dynamics,
  then reinforcement learning to enable inter-system coordination.
---

# Organ-Agents: Virtual Human Physiology Simulator via LLMs

## Quick Facts
- **arXiv ID:** 2508.14357
- **Source URL:** https://arxiv.org/abs/2508.14357
- **Reference count:** 40
- **Primary result:** Multi-agent LLM framework simulates 9 organ systems, achieving system-level MSE below 0.16 on ICU time-series data

## Executive Summary
Organ-Agents introduces a multi-agent LLM framework to simulate human physiology by modeling nine organ systems with dedicated agents trained on ICU time-series data. The approach uses supervised fine-tuning for local dynamics, then reinforcement learning to enable inter-system coordination. Evaluated on 25,064 admissions, the model achieved system-level MSE below 0.16, maintained stable accuracy across SOFA severity strata, and preserved clinical patterns in downstream tasks. External validation on 22,689 patients showed moderate degradation under distribution shift but retained coherent event chains. Expert assessment confirmed high trajectory realism and physiological plausibility, supporting its use as a credible, interpretable digital twin for precision diagnosis and treatment simulation.

## Method Summary
The framework trains 9 organ-specific LLM agents using Supervised Fine-Tuning on ICU time-series data, then employs reinforcement learning to coordinate inter-system dependencies through a Correlator agent. A Compensator agent corrects prediction errors based on confidence scores. The model processes 125 clinical variables across 9 organ systems, using 30-minute resampling and sliding window inputs. Training occurs in two stages: first training individual Simulators, then training coordination policies via PPO with MSE reduction as reward.

## Key Results
- System-level MSE below 0.16 across 9 organ systems on 25,064 admissions
- Maintained stable accuracy across SOFA severity strata
- Preserved clinical patterns in downstream classification tasks with AUROC retention

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modularizing physiology into organ-specific agents decouples complex system dynamics, allowing for more stable local learning.
- **Mechanism:** The system trains 9 distinct "Simulator" agents using Supervised Fine-Tuning (SFT) on isolated time-series windows, learning local temporal patterns before modeling inter-organ dependencies.
- **Core assumption:** Physiological dynamics can be decomposed such that intra-system stability exceeds inter-system volatility in the short term.
- **Evidence anchors:** "[abstract] supervised fine-tuning on system-specific time-series data..."; "[section 4.2.1] much of the local dynamics... can be effectively learned in isolation."

### Mechanism 2
- **Claim:** A reinforcement-learned "Correlator" agent adaptively selects cross-system references to capture non-linear state dependencies.
- **Mechanism:** The Correlator acts as a policy network (PPO) that observes current state and selects specific external variables, rewarded based on reduction in Mean Squared Error (MSE).
- **Core assumption:** The utility of external clinical signals is context-dependent and cannot be captured by static, rule-based graph edges.
- **Evidence anchors:** "[abstract] reinforcement-guided inter-agent coordination using dynamic reference selection..."; "[section 4.2.2] reward is then defined as the reduction in simulation error."

### Mechanism 3
- **Claim:** A "Compensator" agent mitigates error accumulation by learning to predict simulation residuals conditional on confidence scores.
- **Mechanism:** This post-hoc corrector predicts the likely error based on historical trends and corrects the output, specifically addressing drift common in autoregressive generation.
- **Core assumption:** Simulation errors follow a learnable pattern correlated with the model's uncertainty and recent trajectory history.
- **Evidence anchors:** "[abstract] error correction with assistantive agents."; "[section 4.2.2] The Compensator operates as a post-hoc correction module..."

## Foundational Learning

- **Concept: Time-Series as Text Serialization**
  - **Why needed here:** Converts dense ICU numerical data into structured text prompts for the LLM
  - **Quick check question:** Can you format a multivariate numerical window into a string that preserves temporal order and handles missing values?

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** The Correlator agent uses PPO to learn a discrete action space (selecting which other organ systems to consult)
  - **Quick check question:** Explain how PPO stabilizes policy updates compared to standard policy gradients, particularly regarding the "clipping" mechanism.

- **Concept: Residual Error Modeling**
  - **Why needed here:** The Compensator predicts the error of the primary Simulator rather than the value directly
  - **Quick check question:** Why is predicting the residual (y - ŷ) often more stable numerically than predicting the target (y) directly in regression correction tasks?

## Architecture Onboarding

- **Component map:** Simulators (9x) -> Analyzer -> Correlator -> Compensator
- **Critical path:**
  1. Preprocessing: Resample ICU data to 30-min windows → Serialize to JSON
  2. Stage I (SFT): Train individual Simulators on local windows
  3. Stage II (RL): Train Correlator via PPO using Simulator errors as reward
  4. Inference: Sliding window input → Analyzer Summary → Correlator Selection → Simulator Output → Compensator Correction

- **Design tradeoffs:**
  - Interpretability vs. Accuracy: Multi-agent design allows inspection of "who talked to whom" versus monolithic black box
  - Distribution Shift: Performance drop in external validation suggests fitting training distribution over zero-shot generalization

- **Failure signatures:**
  - Schema Drift: LLM generating un-parsable text
  - Static References: Correlator ignoring RL policy and defaulting to fixed connections
  - Error Explosion: Cumulative drift if Compensator threshold is too high

- **First 3 experiments:**
  1. Unit Validation: Run Stage I Simulator on held-out cardiovascular window; plot MSE without cross-system data
  2. Ablation Study: Disable Correlator (random references) vs. Rule-based vs. RL-Correlator to measure MSE delta
  3. Counterfactual Injection: Input "fluid resuscitation" event at t=0 and verify if Renal/Cardio agents update trajectories plausibly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does simulator-assisted care improve patient survival and organ function restoration compared to standard care in a prospective RCT?
- Basis in paper: [explicit] Authors state they are planning a prospective RCT to evaluate real-world performance
- Why unresolved: All evaluations were retrospective using de-identified EHR data
- What evidence would resolve it: Results from planned RCT showing statistically significant improvements in survival rates

### Open Question 2
- Question: Can the framework effectively integrate high-dimensional, multimodal inputs such as medical imaging, biosignals, and unstructured clinical notes?
- Basis in paper: [explicit] Authors note current implementation is "not yet optimized" for these data types
- Why unresolved: Model currently relies on structured time-series data and static demographics
- What evidence would resolve it: Evaluation on model variant trained to process imaging and text data

### Open Question 3
- Question: To what extent does the static multi-agent architecture limit simulation fidelity for rare diseases or atypical organ interactions?
- Basis in paper: [explicit] Authors identify static system architecture as limiting generalizability to rare diseases
- Why unresolved: Static architecture assumes fixed organ definitions, potentially failing to capture emergent interactions in rare conditions
- What evidence would resolve it: Benchmarking MSE and trajectory realism on cohorts with rare multi-system presentations

## Limitations

- Distribution shift causes moderate performance degradation in external validation, suggesting potential overfitting to MIMIC-IV protocols
- Static multi-agent architecture limits generalizability to rare diseases and evolving pathophysiological states
- Absence of explicit mechanistic constraints beyond confidence calibration raises questions about long-term plausibility in edge cases

## Confidence

**High Confidence:**
- Local dynamics can be effectively learned in isolation by organ-specific Simulators
- The Compensator correction mechanism reduces error accumulation in autoregressive generation
- Expert assessment confirms trajectory realism and clinical pattern preservation

**Medium Confidence:**
- The RL-based Correlator successfully captures non-linear inter-system dependencies through adaptive reference selection
- The approach maintains clinical utility under distribution shift, albeit with performance degradation
- The error reduction reward signal effectively guides the Correlator policy

**Low Confidence:**
- The Analyzer's impact on overall performance given limited specification in methods
- The stability of the RL training process across different hyperparameter settings
- The approach's effectiveness for extremely rare or novel physiological scenarios

## Next Checks

1. **Cross-Hospital Generalization Test:** Evaluate the trained model on ICU data from a different healthcare system with distinct protocols to quantify true zero-shot generalization capabilities.

2. **Rare Event Simulation:** Systematically test the model's ability to generate plausible trajectories for rare but clinically significant events not well-represented in training data.

3. **Policy Transfer Experiment:** Freeze the trained Simulator agents and use the framework as a virtual environment to train reinforcement learning agents for treatment decision-making.