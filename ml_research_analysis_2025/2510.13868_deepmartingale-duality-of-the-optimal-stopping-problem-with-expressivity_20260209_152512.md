---
ver: rpa2
title: 'DeepMartingale: Duality of the Optimal Stopping Problem with Expressivity'
arxiv_id: '2510.13868'
source_url: https://arxiv.org/abs/2510.13868
tags:
- deepmartingale
- expressivity
- stopping
- optimal
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DeepMartingale, a novel deep learning approach
  for solving the duality of discrete-monitoring optimal stopping problems in continuous
  time. The method provides a tight upper bound for the primal value function, even
  in high-dimensional settings, and theoretically guarantees overcoming the curse
  of dimensionality.
---

# DeepMartingale: Duality of the Optimal Stopping Problem with Expressivity

## Quick Facts
- **arXiv ID:** 2510.13868
- **Source URL:** https://arxiv.org/abs/2510.13868
- **Reference count:** 40
- **One-line primary result:** DeepMartingale provides tight upper bounds for discrete-monitoring optimal stopping problems in continuous time while theoretically overcoming the curse of dimensionality.

## Executive Summary
DeepMartingale introduces a novel deep learning approach for solving the duality of discrete-monitoring optimal stopping problems in continuous time. The method provides tight upper bounds for the primal value function by approximating the Doob martingale of the Snell envelope using neural networks, without relying on primal information. The core innovation is an independent primal-dual architecture that guarantees unbiased upper bounds even if the primal solution is suboptimal. Theoretical analysis proves convergence under mild assumptions and establishes expressivity: the neural network size required to achieve error ε grows polynomially in dimension D and 1/ε, specifically bounded by cD^q ε^-r where c, q, r are independent of D and ε.

## Method Summary
DeepMartingale solves discrete-monitoring optimal stopping problems by constructing a martingale representation and approximating the Doob martingale using neural networks. The method uses a dual formulation where the upper bound is the infimum over all martingales of the expected value of the payoff minus the martingale. A neural network parameterizes the integrand of the martingale representation, and parameters are optimized by minimizing the dual objective recursively backwards in time. The approach is independent of the primal solution, ensuring unbiased upper bounds. Expressivity analysis shows that under structural conditions on the Itô process coefficients (logarithmic growth with dimension), the method theoretically overcomes the curse of dimensionality with polynomial complexity.

## Key Results
- Provides tight upper bounds for Bermudan options in high-dimensional settings, outperforming existing methods
- Theoretically guarantees overcoming the curse of dimensionality with polynomial neural network size complexity
- Demonstrates superior stability and robustness compared to primal-dual approaches that rely on accurate primal solutions
- Achieves expressivity where neural network size to achieve accuracy ε is bounded by cD^q ε^-r, independent of D and ε

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The approach obtains a tight upper bound by approximating the Doob martingale of the Snell envelope rather than the stopping policy itself.
- **Mechanism:** Uses dual formulation where upper bound is infimum over martingales of E[max_n(g - M)_n]. Neural networks parameterize the integrand Z of the martingale representation (M_t = ∫Z dW) and minimize dual objective recursively backwards in time.
- **Core assumption:** Snell envelope is square-integrable and admits stochastic integral representation with respect to Brownian motion.
- **Evidence anchors:** Abstract mentions constructing martingale representation to approximate Doob martingale; Section 2.1 defines Duality Lemma and Surely Optimal property of Doob martingale.
- **Break condition:** If martingale increments ξ_θ fail zero-conditional-expectation constraint, dual bound remains valid but becomes loose.

### Mechanism 2
- **Claim:** Method theoretically overcomes curse of dimensionality by exploiting specific growth rate constraints on Itô process coefficients.
- **Mechanism:** Expressivity analysis shows polynomial growth of neural network size under assumption that drift/diffusion coefficients grow at most logarithmically with dimension (O((log D)^(1/2))).
- **Core assumption:** Underlying process satisfies Assumption 1 (Lipschitz/Hölder regularity with logarithmic dependence on D) or is Affine Itô Diffusion.
- **Evidence anchors:** Abstract states guarantee of polynomial complexity; Section 4.3.5 Theorem 10 links polynomial growth to structural conditions of dynamics.
- **Break condition:** If SDE coefficients grow exponentially with dimension, theoretical guarantee of polynomial complexity fails.

### Mechanism 3
- **Claim:** Independent primal-dual architecture ensures unbiased upper bounds even with suboptimal primal solutions.
- **Mechanism:** Trains dual martingale network independently of primal stopping policy network, minimizing purely dual loss function rather than using primal value function as input.
- **Core assumption:** Neural network for martingale integrand z_θ is sufficiently expressive and activation function allows Universal Approximation Theorem application.
- **Evidence anchors:** Section 5.1.2 describes Algorithm 1 with independent updates of dual and primal parameters; Section 1.1 claims distinctness from algorithms relying on primal accuracy.
- **Break condition:** If discretization step N₀ is too low, discrete-time approximation introduces bias regardless of primal independence.

## Foundational Learning

- **Concept: Martingale Representation Theorem**
  - **Why needed here:** Justifies that any square-integrable martingale can be written as integral against Brownian motion (M_t = ∫Z dW), providing basis for modeling hedging strategy/integrand Z with neural network.
  - **Quick check question:** Can you explain why approximating the integrand Z is equivalent to approximating the hedging strategy in this context?

- **Concept: The Snell Envelope**
  - **Why needed here:** Central object of optimal stopping problems representing value function. Understanding optimal martingale derivation from Doob decomposition of Snell envelope is critical for dual formulation.
  - **Quick check question:** How does backward recursion differ in dual setting compared to standard dynamic programming?

- **Concept: Curse of Dimensionality in PDE/SDE**
  - **Why needed here:** Paper explicitly claims to solve this. Understanding why standard methods fail (basis function explosion) clarifies why structural conditions (log-growth of coefficients) are key to solution.
  - **Quick check question:** Why does constraint that coefficients grow as O((log D)^(1/2)) help bound complexity while unconstrained coefficients might not?

## Architecture Onboarding

- **Component map:** Simulator -> DeepMartingale Network (z_θ) -> Loss Function -> Independent Primal Network
- **Critical path:**
  1. Initialization: Generate J sample paths of X_t
  2. Backward Loop: For n = N-1 down to 0:
     - Compute Monte Carlo estimate of U_{n+1}
     - Optimize parameters θ_n to minimize dual upper bound loss
     - Store θ_n
  3. Evaluation: Generate fresh out-of-sample paths to calculate final unbiased upper bound U*_0

- **Design tradeoffs:**
  - N₀ (Discretization granularity): High N₀ reduces continuous-time approximation error but linearly increases computational cost
  - Bounded vs. Unbounded Activation: Theory relies on bounded activations for UAT convergence, but experiments show unbounded ReLU works well empirically

- **Failure signatures:**
  - Infinite Duality Gap: Large gap between Upper Bound and Lower Bound indicates NN width insufficient for dimension D
  - Upper < Lower: Implementation bug indicating martingale property violated numerically
  - Gradient Explosion: High standard deviation in U suggests Monte Carlo batch size too small

- **First 3 experiments:**
  1. Baseline Convergence (Bermudan Max-Call, D=2): Validate implementation by checking computed U_DM and L_BK converge to known reference values
  2. Scalability Stress Test (D=50): Run on high-dimensional basket options and monitor runtime vs dimension for polynomial scaling
  3. Discretization Sensitivity: Vary N₀ (10, 50, 150) on fixed dimensional problem and plot change in Upper Bound value

## Open Questions the Paper Calls Out

- **Open Question 1:** Can theoretical expressivity framework be extended to cover more general models beyond current structural conditions?
  - **Basis in paper:** Conclusion states "by extending the RKBS framework for neural-network representability, more general models can be incorporated"
  - **Why unresolved:** Current analysis relies heavily on specific structural conditions regarding growth and Lipschitz rates of coefficient functions

- **Open Question 2:** Can DeepMartingale be naturally generalized to solve multiple stopping problems and Reflected BSDEs (RBSDEs)?
  - **Basis in paper:** Conclusion lists "Extensions to Multiple-Stopping and RBSDEs" as promising research direction
  - **Why unresolved:** Current methodology and proofs are tailored for single stopping problems under discrete monitoring

- **Open Question 3:** Can approach be adapted for Lévy-type processes and models requiring martingale arguments in non-Brownian filtrations?
  - **Basis in paper:** Conclusion mentions potential developments in "Lévy-type processes and other advanced stochastic models that require martingale arguments"
  - **Why unresolved:** Current method is derived specifically for Itô processes driven by Brownian motion

## Limitations
- Theoretical expressivity claims rely on logarithmic coefficient growth assumption that may not hold for all SDEs
- Practical network size may be insufficient for very high dimensions (>100) without further architectural adjustments
- Gradient stability can be problematic due to variance introduced by stochastic integral approximation

## Confidence

- **High:** Dual formulation correctness and convergence under Assumptions 1-2 (theoretical foundation)
- **Medium:** Numerical results for D≤50 (Bermudan options show consistent improvements)
- **Low:** Expressivity claims for D≫50 without empirical validation

## Next Checks

1. **Dimension Stress Test:** Run DeepMartingale on basket options with D=100, 200, 500 and measure runtime/accuracy scaling to verify polynomial vs exponential complexity empirically

2. **Activation Sensitivity:** Compare bounded ReLU vs unbounded ReLU across all experiments to quantify impact of theoretical activation constraint on practical performance

3. **Discretization Convergence:** Systematically vary N₀ (10, 50, 150, 300) and plot continuous-time approximation error to empirically validate Theorem 1's convergence rate