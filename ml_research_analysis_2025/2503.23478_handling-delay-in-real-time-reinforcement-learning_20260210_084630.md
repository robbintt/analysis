---
ver: rpa2
title: Handling Delay in Real-Time Reinforcement Learning
arxiv_id: '2503.23478'
source_url: https://arxiv.org/abs/2503.23478
tags:
- skip
- layers
- connections
- delay
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses delays in real-time reinforcement learning
  caused by parallel computations of neurons. The authors propose temporal skip connections
  combined with history-augmented observations to reduce observational delay and improve
  training stability.
---

# Handling Delay in Real-Time Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.23478
- Source URL: https://arxiv.org/abs/2503.23478
- Reference count: 40
- Primary result: Temporal skip connections reduce observational delay from Nδ to δ steps, achieving strong performance across Mujoco, MinAtar, and MiniGrid while accelerating inference by 6-350%

## Executive Summary
This paper addresses delays in real-time reinforcement learning caused by parallel computations of neurons. The authors propose temporal skip connections combined with history-augmented observations to reduce observational delay and improve training stability. They theoretically prove that skip connections can exponentially reduce delay-related regret and make the environment Markovian. Experiments on Mujoco, MinAtar, and MiniGrid show that agents with skip connections and history augmentation achieve strong performance across various environments and neuron execution times, often matching or exceeding baseline methods. Parallel computations also accelerate inference by 6-350% on standard hardware. The approach is particularly effective when neuron execution time is small, enabling real-time RL agents to perform comparably to those without delay.

## Method Summary
The authors propose a parallel computation framework where each layer processes data simultaneously from different timesteps, creating observational delay of Nδ steps for an N-layer network. They introduce temporal skip connections that shortcut along both depth and time, reducing effective delay to δ steps. Action history augmentation restores Markovian properties to the delayed decision process. The framework uses SAC for continuous control (MuJoCo) and PPO for discrete control (MinAtar, MiniGrid). Actor networks use 3-layer MLPs or 5-layer CNNs with temporal skip connections, while critics are trained without delay. The method achieves 6-350% speed-up on GPU through batched matrix multiplication, with sparse matrices required for deep networks.

## Key Results
- Temporal skip connections reduce effective observational delay from Nδ to δ steps in parallel computation frameworks
- History augmentation with recent actions restores Markovian properties to delayed decision processes
- Parallel layer computation accelerates inference throughput by 6-350% on GPUs with minimal performance loss
- Agents with skip connections and history augmentation match or exceed baseline performance across Mujoco, MinAtar, and MiniGrid

## Why This Works (Mechanism)

### Mechanism 1
Temporal skip connections reduce effective observational delay from Nδ to δ steps by allowing the most recent observation to influence action after only one layer's computational delay rather than N layers. In pipelined execution, each layer processes data from different timesteps simultaneously—layer 1 at time t, layer 2 at time t−δ, etc. Skip connections shortcut not just along depth but along time. Proposition 1 shows networks with skip connections achieve delay regret bound of Ω(t(1 − (pminimax)^⌈δ⌉)) versus Ω(t(1 − (pminimax)^⌈Nδ⌉)) for vanilla networks.

### Mechanism 2
History augmentation with recent actions restores Markovian properties to the delayed decision process. Without past actions, the transition probability P(s'|st−Nδ, at) depends on π(at−1|st−Nδ−1), which is non-stationary during learning. Augmenting state with recent actions (st−Nδ, at−Nδ:t−1) removes the policy term, making transitions stationary and enabling stable learning. Proposition 2 formalizes the Markovian property restoration.

### Mechanism 3
Parallel layer computation accelerates inference throughput by 6-350% on GPUs with minimal performance loss. By executing all layers simultaneously on different timesteps (pipelining), throughput scales with network depth. Each layer processes its input immediately upon receiving it rather than waiting for the full forward pass to complete. Regular matrix multiplication reached peak speed-up around 30 layers, while sparse matrix multiplication achieved 350% speed-up for 100-layer MLPs.

## Foundational Learning

- **Pipelining and Throughput vs. Latency Trade-offs**: The parallel computation framework relies on understanding that pipelining improves throughput (actions/second) while increasing per-action latency (time from observation to action based on that observation). Quick check: Can you explain why a 3-layer network with δ=1 has latency of 3 steps but throughput of 1 action per step in the parallel regime?

- **Markovian vs. Non-Markovian Decision Processes**: Understanding why delayed observations create non-Markovian dynamics (and why action history fixes this) is essential for grasping Proposition 2 and the training stability issues. Quick check: If an agent acts based on st−2 without knowing at−1, what makes the transition function non-stationary during training?

- **Skip Connection Architectures (ResNet-style)**: The paper extends traditional skip connections to temporal skip connections; familiarity with residual connections helps understand both the implementation and the gradient-flow benefits. Quick check: What is the difference between a traditional residual connection and a temporal skip connection in the parallel computation framework?

## Architecture Onboarding

- **Component map**: Actor network (N-layer MLP/CNN with temporal skip connections) -> History buffer (stores past N observations and actions) -> Parallel execution engine (executes all layers simultaneously) -> Action output

- **Critical path**: 1. Observation arrives at time t; 2. Layer 1 immediately processes st, produces h1t; 3. Simultaneously, Layer 2 processes h1t−δ from previous timestep, etc.; 4. Skip connections route st−δ directly to later layers (bypassing intermediate delays); 5. Action at is produced combining all available representations; 6. Critic evaluates (st, at) without delay for TD learning

- **Design tradeoffs**: More layers → more expressivity but higher delay without skip connections; Skip connection type: "Projection from observation" works best for Mujoco; "Projection to action + residual" for MinAtar/MiniGrid; Network depth with skip connections: 3 layers recommended; 2 layers underperforms, >3 layers shows diminishing returns; Augmentation strategy: 2 recent actions for Mujoco, 1 for MinAtar, 4 past observations for MiniGrid

- **Failure signatures**: Performance plateau significantly below oracle: network expressivity insufficient for task complexity; Training instability with vanilla parallel networks: non-Markovian dynamics cause oscillating losses; No speed-up on CPU: thread synchronization overhead dominates; Sparse matrix slower than dense at <30 layers: overhead exceeds benefits

- **First 3 experiments**: 1. Baseline comparison: Run vanilla 3-layer MLP with parallel computation (no skip, no augmentation) on HalfCheetah-v4 with δ=1,2,3,4 to establish performance degradation curve; 2. Skip connection ablation: Add temporal skip connections (projection-from-observation variant) and measure recovery toward oracle performance across same δ values; 3. Augmentation validation: Add 2-step action history to skip-connection agent and verify training stability (loss curve smoothness) and final performance match or exceed non-delayed SAC

## Open Questions the Paper Calls Out

### Open Question 1
How can the parallel computation framework be adapted to handle stochastic neuron execution times (δ) rather than fixed delays? The authors state in Section 6 that assuming a fixed δ is a limitation and propose "exploring handling stochastic δ" as a future line of work. The current theoretical regret bounds and Algorithm 1 implementation rely on deterministic, fixed execution time per layer to manage the pipeline and observations.

### Open Question 2
Can the performance gap between instantaneous actors and parallel-computation agents be closed in highly complex environments or cases with large execution times? Section 4.2 notes skip connections may lack expressivity, and Section 7 states that when execution times are larger or environments are complex, "the performance gap... widens" and "Further research is needed to... mitigate this gap." The "fast path" provided by temporal skip connections trades off network depth for reduced delay, potentially limiting representational capacity needed for complex state differentiation.

### Open Question 3
How can the framework be extended to fully asynchronous neuron computation where individual neurons operate independently? Section 7 suggests "Future studies could also explore asynchronous neuron computation," distinct from the current layer-wise parallel pipeline. The current method synchronizes computations at the layer level (parallel layers); it does not address the complexity of neurons updating asynchronously at different rates.

## Limitations

- Theoretical regret bounds assume idealized parallel execution; real-world hardware introduces scheduling noise and synchronization delays not captured in analysis
- Action history augmentation assumes perfect knowledge of actions during delay period; stochastic policies or exploration could violate Markovian restoration
- Skip connection architecture variants (projection-from-observation vs projection-to-action) show environment-specific effectiveness without clear unifying principle

## Confidence

- **High confidence**: Parallel computation framework and speed-up measurements (empirical GPU benchmarks directly measured)
- **Medium confidence**: Temporal skip connections reduce observational delay (strong theoretical analysis but limited ablation studies)
- **Medium confidence**: History augmentation restores Markovian properties (theoretical proof but no ablation showing necessity)
- **Low confidence**: Architecture selection (different skip types for different domains chosen empirically without systematic exploration)

## Next Checks

1. **Cross-domain architecture transfer**: Apply the projection-to-action skip variant (optimal for MinAtar) to MuJoCo tasks to test if performance degrades, validating the domain-specific architecture choice

2. **Delay-to-action correlation analysis**: Systematically vary δ from 1 to 8 steps in HalfCheetah and measure performance vs. skip connections to verify the exponential regret improvement claimed in Proposition 1

3. **Stochastic policy robustness test**: Replace deterministic action selection with stochastic policies (ε-greedy) during the delay period to verify that history augmentation still maintains Markovian properties under exploration