---
ver: rpa2
title: 'ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation'
arxiv_id: '2601.15330'
source_url: https://arxiv.org/abs/2601.15330
tags:
- icpo
- multi-turn
- rlvr
- standard
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Illocution-Calibrated Policy Optimization
  (ICPO), a novel training framework designed to address the "lost-in-conversation"
  problem in large language models during multi-turn conversations. ICPO augments
  training with underspecified prompts and conditions rewards on the user's illocutionary
  intent, encouraging the model to seek clarification when faced with ambiguity rather
  than providing overconfident, incorrect responses.
---

# ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation

## Quick Facts
- **arXiv ID:** 2601.15330
- **Source URL:** https://arxiv.org/abs/2601.15330
- **Reference count:** 0
- **Primary result:** 75% improvement in multi-turn conversation performance while preserving single-turn benchmark results

## Executive Summary
This paper introduces Illocution-Calibrated Policy Optimization (ICPO), a novel training framework designed to address the "lost-in-conversation" problem in large language models during multi-turn conversations. ICPO augments training with underspecified prompts and conditions rewards on the user's illocutionary intent, encouraging the model to seek clarification when faced with ambiguity rather than providing overconfident, incorrect responses. Experiments demonstrate that ICPO achieves a substantial 75% improvement in multi-turn conversation performance while preserving strong performance on single-turn benchmarks. The approach maintains higher response diversity and fosters appropriate humility by keeping policy entropy elevated during training, avoiding the mode collapse seen in standard reinforcement learning methods.

## Method Summary
ICPO introduces a novel training paradigm that addresses the fundamental challenge of ambiguity in multi-turn conversations by incorporating illocutionary intent into the reward structure. The method uses underspecified prompts during training to simulate real-world conversational ambiguity, and rewards the model based on its ability to correctly interpret and respond to the user's underlying communicative intent rather than just the literal meaning of utterances. During optimization, ICPO maintains higher policy entropy to encourage diverse responses and prevent the model from converging to overconfident, potentially incorrect answers. This is achieved through a calibrated reward mechanism that specifically penalizes responses that fail to acknowledge ambiguity or seek clarification when appropriate, while still allowing the model to provide confident answers when the user's intent is clear.

## Key Results
- 75% improvement in multi-turn conversation performance on MT-Bench evaluation
- Maintains strong performance on single-turn benchmarks while improving multi-turn capabilities
- Higher response diversity and avoidance of mode collapse compared to standard RLHF methods

## Why This Works (Mechanism)
ICPO works by fundamentally changing how the model interprets and responds to ambiguous inputs during training. By incorporating illocutionary intent into the reward signal, the model learns to recognize when it lacks sufficient context to provide a confident answer and is incentivized to ask clarifying questions instead. The underspecified prompts serve as realistic proxies for real-world ambiguity, training the model to handle situations where the user's true intent is not immediately clear from the literal meaning of their words. The elevated policy entropy during training prevents premature convergence to overconfident responses, allowing the model to maintain a more nuanced understanding of when to provide direct answers versus when to seek additional information.

## Foundational Learning
- **Illocutionary intent:** The underlying communicative purpose behind an utterance beyond its literal meaning. Needed to understand why users say what they say in conversations. Quick check: Can identify the difference between "Can you pass the salt?" (request) and "I wonder if you could pass the salt" (hesitant request).
- **Policy entropy in RL:** A measure of uncertainty or diversity in the model's action selection. Needed to prevent mode collapse and maintain response diversity. Quick check: Higher entropy means more varied responses; lower entropy means more deterministic behavior.
- **Underspecified prompts:** Training inputs deliberately crafted to be ambiguous or lacking context. Needed to simulate real-world conversational challenges. Quick check: A prompt like "What about it?" with no prior context would be considered underspecified.
- **Reward calibration:** The process of adjusting reward signals to better align with desired behaviors. Needed to properly incentivize the right conversational behaviors. Quick check: Rewards should encourage seeking clarification when ambiguous, not just providing any answer.
- **Mode collapse:** The phenomenon where RL training causes models to converge to a narrow set of responses. Needed to understand what ICPO is specifically preventing. Quick check: Standard RL often produces repetitive, formulaic responses over time.

## Architecture Onboarding

Component map: User prompts -> Underspecified prompt generator -> LLM policy network -> Response selector -> Reward calculator (with illocutionary intent) -> Policy optimizer

Critical path: The core optimization loop where underspecified prompts are generated, the model produces responses, rewards are calculated based on illocutionary intent alignment, and policy updates are applied while maintaining elevated entropy.

Design tradeoffs: ICPO trades some immediate response confidence for long-term conversational effectiveness. The method requires more complex reward engineering and training infrastructure compared to standard RLHF, but gains significant improvements in handling real-world conversational ambiguity.

Failure signatures: The model might fail if the underspecified prompt generator doesn't adequately capture real-world ambiguity, if the illocutionary intent reward calculation is poorly calibrated, or if entropy maintenance is too aggressive causing excessive uncertainty in clear situations.

First experiments:
1. Test ICPO on simple underspecified prompts to verify the model learns to seek clarification appropriately
2. Compare response diversity metrics between ICPO and standard RLHF on identical ambiguous inputs
3. Evaluate single-turn performance preservation while applying ICPO to ensure the multi-turn improvements don't degrade basic capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to very large model sizes remains uncertain, as experiments were conducted primarily on models up to 30B parameters
- Performance in domains beyond general conversation has not been extensively validated
- The 75% improvement metric is specific to MT-Bench evaluation and may not generalize to all conversational domains or task-oriented settings
- Reliance on underspecified prompts as a proxy for real-world ambiguity may not fully capture the complexity of user intent in diverse conversational scenarios

## Confidence

High confidence in:
- The methodological contribution of incorporating illocutionary intent into reward signals
- The empirical observation that ICPO maintains higher response diversity and avoids mode collapse compared to standard RLHF
- The claim that ICPO preserves single-turn performance while improving multi-turn capabilities based on MT-Bench results

Medium confidence in:
- The generalizability of the 75% improvement metric across different conversation types and domains
- The long-term stability of the humility-promoting effects during extended deployment

## Next Checks

1. Evaluate ICPO on task-oriented dialogue datasets to assess performance in goal-directed conversations with specific success metrics
2. Test the approach with models exceeding 30B parameters to verify scalability
3. Conduct ablation studies removing the underspecified prompt component to isolate its contribution to overall performance gains