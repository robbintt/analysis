---
ver: rpa2
title: Transformers Can Learn Connectivity in Some Graphs but Not Others
arxiv_id: '2509.22343'
source_url: https://arxiv.org/abs/2509.22343
tags:
- graphs
- graph
- grid
- connectivity
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformers can learn graph connectivity
  as a proxy for inferring transitive relations. The authors train transformer models
  on synthetic directed graphs where connectivity information is provided as training
  examples, rather than through in-context prompting.
---

# Transformers Can Learn Connectivity in Some Graphs but Not Others

## Quick Facts
- arXiv ID: 2509.22343
- Source URL: https://arxiv.org/abs/2509.22343
- Reference count: 40
- Transformers can learn grid graph connectivity but struggle with disconnected chain graphs

## Executive Summary
This paper investigates transformers' ability to learn graph connectivity as a proxy for inferring transitive relations. The authors train transformer models on synthetic directed graphs where connectivity information is provided as training examples, rather than through in-context prompting. They focus on two graph types: grid graphs (where nodes can be embedded in low-dimensional subspaces with connectivity inferable from embeddings) and disconnected chain graphs. Results show transformers excel at learning connectivity for grid graphs, especially with lower dimensionality, and scaling model size improves performance. However, transformers struggle with disconnected chain graphs, particularly when the number of chains is large. The study reveals that transformers benefit more from scaling graph size than model size when learning connectivity for graphs with many disconnected components.

## Method Summary
The authors train transformer models on synthetic directed graphs where connectivity information is provided as training examples. They focus on two graph types: grid graphs and disconnected chain graphs. Grid graphs have nodes that can be embedded in low-dimensional subspaces where connectivity is inferable from embeddings, while disconnected chain graphs consist of multiple separate chains. The training setup involves providing connectivity information directly rather than using in-context prompting, allowing the model to learn patterns from examples. The study systematically varies graph size, model size, and dimensionality to understand how these factors affect the transformers' ability to learn connectivity.

## Key Results
- Transformers excel at learning connectivity for grid graphs, especially with lower dimensionality
- Scaling model size improves performance on grid graphs but has limited effect on disconnected chains
- Transformers struggle with disconnected chain graphs, particularly when the number of chains is large
- Scaling graph size helps transformers learn connectivity for both graph types, but scaling model size is more effective for grid graphs

## Why This Works (Mechanism)
The study reveals that transformers' ability to learn graph connectivity depends on the structural properties of the graphs. For grid graphs, the low-dimensional embedding space allows transformers to infer connectivity from node positions and embeddings, making the task more tractable. The grid structure provides regular patterns that transformers can capture through attention mechanisms. In contrast, disconnected chain graphs lack these structural regularities, making it harder for transformers to learn connectivity patterns, especially as the number of chains increases. The paper suggests that the presence of multiple disconnected components creates a fundamentally different learning challenge that current transformer architectures struggle to overcome, even with scaling.

## Foundational Learning
- **Graph connectivity**: Understanding whether nodes are connected through paths in a graph - needed to establish the core learning task, check by verifying transitive closure relationships
- **Low-dimensional embeddings**: Mapping graph nodes to points in space where geometric relationships reflect graph structure - needed for grid graph analysis, check by examining embedding distances
- **Transitive relations**: Relations that extend through chains of connections (if A→B and B→C, then A→C) - needed as theoretical framework, check by verifying closure properties
- **Attention mechanisms**: How transformers weigh relationships between input elements - needed to understand learning process, check by analyzing attention patterns
- **Scaling laws**: How model and data size affect learning performance - needed for experimental design, check by plotting performance vs size

## Architecture Onboarding

**Component map**: Input nodes → Positional embeddings → Transformer layers (Multi-head attention + Feed-forward) → Output connectivity predictions

**Critical path**: Node features → Embedding layer → Self-attention computation → Feed-forward layers → Final prediction layer

**Design tradeoffs**: The study uses synthetic graphs with fixed node labels (A-Z) rather than real-world features, trading generalizability for controlled experimentation. The choice to provide connectivity information as training examples rather than in-context prompting simplifies the learning task but may not reflect practical applications.

**Failure signatures**: For disconnected chains, performance degrades significantly as chain count increases. For grid graphs, performance improves with lower dimensionality but plateaus with excessive scaling. Models fail to capture long-range connectivity in chains despite successful grid learning.

**3 first experiments**:
1. Train a small transformer on a 5x5 grid graph with 2-dimensional embeddings to establish baseline performance
2. Train the same model on 10 disconnected chains of length 5 to test chain learning capability
3. Scale the model size by 4x and retrain on both tasks to observe scaling effects

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental setup uses synthetic graphs with fixed node labels (A-Z), limiting generalizability to real-world graph data
- Focus on transitive relations as a proxy for connectivity may not capture all practical aspects of graph reasoning
- Analysis primarily focuses on structural differences without exploring alternative explanations such as attention pattern limitations

## Confidence
- Core finding (transformers learn grid better than chain connectivity): High
- Interpretation of why transformers struggle with chain graphs: Medium
- Claim that scaling graph size is more effective than scaling model size for disconnected components: Medium

## Next Checks
1. Test the models on real-world graph datasets with varying connectivity patterns to assess generalizability beyond synthetic examples
2. Investigate whether alternative attention mechanisms or architectural modifications could improve performance on disconnected chain graphs
3. Conduct ablation studies to determine whether the node labeling scheme (A-Z) significantly influences learning outcomes compared to randomized or feature-based node representations