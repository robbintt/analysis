---
ver: rpa2
title: Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question
  Answering
arxiv_id: '2510.12925'
source_url: https://arxiv.org/abs/2510.12925
tags:
- persona
- personas
- robustness
- language
- inquiry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first systematic evaluation of LLM robustness
  to inquiry personas in factual question answering. The authors propose a push-button
  robustness testing method using self-disclosed user attributes as prompts, finding
  that unaligned personas significantly degrade accuracy (e.g., up to 55% loss on
  PubMedQA) while question-aligned personas can improve performance.
---

# Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering

## Quick Facts
- arXiv ID: 2510.12925
- Source URL: https://arxiv.org/abs/2510.12925
- Reference count: 40
- Primary result: Unaligned inquiry personas degrade LLM factual QA accuracy by up to 55% on PubMedQA, while question-aligned personas can improve performance

## Executive Summary
This paper introduces the first systematic evaluation of LLM robustness to inquiry personas in factual question answering. The authors propose a push-button robustness testing method using self-disclosed user attributes as prompts, finding that unaligned personas significantly degrade accuracy while question-aligned personas can improve performance. Through thematic analysis of failure modes, they identify knowledge access alterations, ethical restrictions, role confusion, and relevance misjudgments as key issues. The study also reveals that personas influence response style, complexity, and stereotyping risk, with smaller models showing greater sensitivity. Simple objectivity guidance improves robustness across persona types without harming baseline performance.

## Method Summary
The authors evaluate LLM robustness to inquiry personas by prepending first-person user descriptions to factual questions from five QA datasets (TriviaQA, PubMedQA, TabFact, GPQA, SimpleQA). Personas are sourced from Persona Hub for unaligned conditions or generated via LLM prompting for aligned conditions (authority levels, reading levels, credulity, adversary). Each question-answer pair is tested with and without persona contexts. Accuracy is measured using dataset-specific methods, and an LLM-as-judge (Claude Sonnet 3.5 v2) evaluates additional metrics including Language Complexity, Expertise Match, and Stereotype Risk. The study uses Amazon Bedrock API to access target LLMs with default hyperparameters and tests mitigation via an objectivity-focused system prompt.

## Key Results
- Unaligned personas significantly degrade accuracy across all datasets, with up to 55% loss on PubMedQA
- Question-aligned personas can improve accuracy by up to 12.7% on SimpleQA
- Thematic analysis reveals four key failure modes: knowledge access alterations (N=868), ethical restrictions (N=65), role confusion (N=35), and relevance misjudgments
- Personas influence response style, complexity, and stereotyping risk, with smaller models showing greater sensitivity
- Simple objectivity guidance ("You are an objective question-answering assistant") improves robustness across persona types without harming baseline performance

## Why This Works (Mechanism)

### Mechanism 1: Context-Conditioned Knowledge Suppression
- Claim: LLMs suppress available factual knowledge when inquiry persona implies domain mismatch with question.
- Mechanism: Models interpret user context as scope constraints, generating refusals or limitations rather than retrieving facts when questions fall outside persona's inferred expertise.
- Core assumption: Models prioritize conversational relevance to specific user over general factual retrieval.
- Evidence anchors:
  - Section 4.3 describes knowledge access alterations with N=868 instances of claiming lack of knowledge
  - Table 4 shows retiree persona receiving refusal for medical question model answered correctly without persona
  - Corpus "Prompting Science Report 4" suggests expert personas don't robustly improve accuracy

### Mechanism 2: Feature-Based Safety Misfiring
- Claim: Safety alignment triggers false refusals when personas contain keywords associated with high-risk activities.
- Mechanism: Safety classifier assigns high weight to persona description (e.g., "skilled safecracker"), triggering refusal for benign queries.
- Core assumption: Safety training creates broad associations that fail to disentangle user identity from user intent.
- Evidence anchors:
  - Section 4.3 identifies ethical restrictions with N=65 refusals of neutral queries
  - Table 4 demonstrates "safecracker" persona receiving ethical refusal for Greek parliament question
  - Corpus "VocalBench-DF" discusses robustness to disfluency, distinct from semantic identity cues

### Mechanism 3: Role Confusion via In-Context Priming
- Claim: LLMs misinterpret first-person user descriptions as instructions to adopt that persona.
- Mechanism: Due to training on "persona prompting" data, models conflate inquirer with assistant, generating text consistent with persona's limitations.
- Core assumption: Instruction-following capabilities are brittle when input formats resemble training-time role-assignment structures.
- Evidence anchors:
  - Section 4.3 notes role and identity management failures with N=35 instances
  - Introduction explicitly contrasts with "Persona Prompting" where roles are assigned to LLM
  - Corpus "Simulating Misinformation Propagation" uses LLM personas as synthetic agents

## Foundational Learning

- Concept: **Instruction Following vs. In-Context Learning**
  - Why needed here: To understand why model follows implicit "instruction" of persona over explicit instruction to answer question
  - Quick check question: Does model treat persona description as data to process or as command to execute?

- Concept: **Prompt Sensitivity (Distractability)**
  - Why needed here: Paper treats inquiry personas as "irrelevant context" that distracts model from factual core
  - Quick check question: If we prepend random unrelated fact to prompt, does accuracy drop? (Yes, and user identity has similar or stronger effect)

- Concept: **Thematic Analysis**
  - Why needed here: Authors use qualitative method to categorize why model fails (refusals vs. role confusion) rather than just measuring accuracy drops
  - Quick check question: Can you distinguish between refusal based on "safety" vs. "relevance" in model's output?

## Architecture Onboarding

- Component map: Persona Generator/Selector -> Prompt Constructor -> Target LLM -> LLM-as-Judge
- Critical path: Designing personas is highest-leverage step; if too similar to question domain, robustness issues may not surface; if too adversarial, may trigger generic refusals
- Design tradeoffs:
  - Synthetic vs. Real Personas: Uses synthetic/sampled personas; real user data might contain subtler cues
  - Metric Granularity: LLM-as-judge allows nuanced metrics but introduces evaluator noise vs. exact string matching
- Failure signatures:
  - "Over-Apology": Responses beginning with "I apologize, but as a [persona]..." indicate Role Confusion
  - "Hallucinated Constraint": "I do not have the relevant expertise..." for general knowledge indicates Knowledge Access Alteration
  - "Moralizing Refusal": Refusing factual query because user persona is "unethical" indicates Safety Misfiring
- First 3 experiments:
  1. Baseline Robustness Check: Select 100 questions from SimpleQA; run with "No Persona" vs. "Random Unaligned Persona"; measure accuracy delta
  2. System Prompt Mitigation: Add specific system prompt; re-run Unaligned Persona condition to see if gap closes or model finds new failure modes
  3. Failure Mode Classification: Take 50 failure instances; manually classify (or use LLM-judge) into Knowledge Access, Safety, Role Confusion, or Relevance categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is real-world effectiveness of mitigation strategies (adversarial training with inquiry-persona enriched prompts, objective input classifiers, persona-stripping mechanisms) in deployed systems?
- Basis in paper: [explicit] Authors state: "Potential interventions may include adversarial training with inquiry-persona enriched prompts, filtering subjective input using objectiveness classifiers, and stripping persona cues on the input side. However, the real-world effectiveness of such methods remains uncertain."
- Why unresolved: Paper focuses on diagnosing robustness issues rather than developing/testing mitigation strategies; deployment settings inaccessible for controlled testing
- What evidence would resolve it: Comparative experiments in production environments or realistic simulation settings measuring accuracy preservation across persona types with each mitigation applied

### Open Question 2
- Question: How does LLM robustness to inquiry personas evolve over time as models adapt to user attributes, interests, and intents in longitudinal interactions?
- Basis in paper: [explicit] Authors explicitly note: "it is important to study long-term robustness when models are adapting to user information, as user attributes, interests and intents shift over time."
- Why unresolved: Current experiments use single-turn question-answer pairs rather than multi-turn or extended interaction histories
- What evidence would resolve it: Longitudinal experiments tracking accuracy across multiple sessions with persistent persona contexts and evolving user profiles

### Open Question 3
- Question: Can optimization-based adversarial approaches systematically identify persona-specific vulnerabilities that elicit incorrect factual responses?
- Basis in paper: [explicit] Authors state: "we plan to develop adversarial and optimization-based approaches to systematically identify persona-specific vulnerabilities in model responses."
- Why unresolved: Current study uses sampled and constructed personas rather than adversarially optimized ones designed to maximize failure rates
- What evidence would resolve it: Demonstrated persona constructions generated via gradient-based or search-based optimization that consistently trigger accuracy degradation beyond random unaligned personas

### Open Question 4
- Question: How does degree of specificity and granularity in persona descriptions influence magnitude of accuracy degradation and failure mode frequency?
- Basis in paper: [explicit] Authors state: "Investigating how different degrees of specificity influence model behavior is an important direction for future work" and acknowledge "there is currently no consensus in research community regarding appropriate scope or granularity of persona descriptions."
- Why unresolved: All personas in study were fixed at similar detail levels to enable controlled comparison
- What evidence would resolve it: Systematic ablation experiments varying persona length and attribute count while holding question type constant, measuring accuracy changes

## Limitations

- Evaluation relies on self-disclosed personas as prompts without examining how actual user behavior or interaction history might influence robustness
- Thematic analysis depends on LLM-as-judge evaluations that may inherit biases from judge model itself
- Study focuses on factual question answering but doesn't address how robustness issues manifest in open-ended dialogue or task-oriented contexts
- Personas were generated or sampled rather than collected from real user interactions, limiting ecological validity

## Confidence

- **High Confidence**: Empirical finding that unaligned personas significantly degrade accuracy (55% on PubMedQA) is well-supported by controlled experiments and statistical testing
- **Medium Confidence**: Identification of four failure mechanisms (knowledge access, safety misfiring, role confusion, relevance misjudgments) is supported by thematic analysis but could benefit from deeper causal investigation
- **Low Confidence**: Generalizability of findings to real-world user interactions and non-factual QA tasks remains uncertain due to synthetic nature of evaluation setup

## Next Checks

1. **Real User Validation**: Replicate experiment using actual user-submitted personas from support tickets or community forums to assess whether synthetic personas capture real-world robustness issues
2. **Causal Mechanism Isolation**: Design controlled experiments that isolate each proposed failure mechanism (e.g., test whether safety filters alone can reproduce accuracy degradation when decoupled from persona context)
3. **Cross-Domain Generalization**: Apply same evaluation methodology to open-ended dialogue tasks (e.g., customer service conversations) to determine whether persona robustness issues persist beyond factual QA