---
ver: rpa2
title: 'MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering'
arxiv_id: '2508.16357'
source_url: https://arxiv.org/abs/2508.16357
tags:
- alefisolated
- laminitial
- legal
- aleffinal
- yehmedial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MizanQA introduces a novel Arabic legal question-answering benchmark
  focused on Moroccan law, addressing the challenge of evaluating large language models
  in specialized, low-resource legal domains. The dataset comprises over 1,700 multiple-choice
  questions reflecting the linguistic and legal complexity of Moroccan law, including
  multi-answer formats and culturally specific terminology.
---

# MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering

## Quick Facts
- arXiv ID: 2508.16357
- Source URL: https://arxiv.org/abs/2508.16357
- Reference count: 4
- Models achieved only up to 58% accuracy on Moroccan legal QA benchmark

## Executive Summary
MizanQA introduces a novel Arabic legal question-answering benchmark focused on Moroccan law, addressing the challenge of evaluating large language models in specialized, low-resource legal domains. The dataset comprises over 1,700 multiple-choice questions reflecting the linguistic and legal complexity of Moroccan law, including multi-answer formats and culturally specific terminology. Experiments with leading multilingual and Arabic-focused LLMs reveal substantial performance gaps, with top models achieving only up to 58% accuracy. New evaluation metrics are proposed to handle multiple correct answers and assess confidence calibration. Results highlight the need for domain-specific, culturally grounded LLM development for legal reasoning in Arabic contexts.

## Method Summary
The MizanQA benchmark was created through a multi-stage process: (1) downloading legal documents from Moroccan government websites, (2) extracting text and images using Gemini-2.0-flash vision, (3) organizing content into batches, (4) generating multiple-choice questions using Gemini-2.0-flash, and (5) manual expert validation. The dataset contains 1,776 questions across 14 legal categories, with variable correct answers (1-10) from 2-12 total options. Zero-shot inference was performed using APIs (Groq for Llama/Allam, Gemini API for Gemini models) with specific prompt templates. Four evaluation metrics were implemented: Strict Accuracy, F1-like, PMPA, and Expected Calibration Error.

## Key Results
- Leading multilingual and Arabic-focused LLMs achieved only up to 58% accuracy on Moroccan legal QA
- Significant performance gaps across legal categories, with Family Law showing lowest performance
- High confidence calibration errors observed, with some models showing over 59% Set-Level Calibration Error despite moderate accuracy
- Novel metrics (F1-like, PMPA) revealed partial knowledge that strict accuracy missed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance degradation in LLMs is driven by linguistic complexity and domain-specific legal hybridity rather than just language identification.
- Mechanism: Moroccan law mixes Modern Standard Arabic (MSA) with Maliki jurisprudence, customary law, and French legal influences. Models trained on general MSA or English-centric legal corpora fail to map region-specific idioms (e.g., archaic expressions) to correct legal concepts, resulting in low accuracy.
- Core assumption: The performance gap is primarily due to data distribution shifts (vocabulary/concept mismatch) rather than model capacity limits.
- Evidence anchors:
  - [abstract] Mentions the dataset "draws on Modern Standard Arabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal influences."
  - [section] Page 1 notes that "statutes and regulations may use archaic or region-specific expressions that do not appear in standard Arabic corpora."
  - [corpus] DialectalArabicMMLU confirms that despite progress in MSA, "dialectal varieties remain underrepresented," supporting the difficulty of non-standard linguistic variants.
- Break condition: If models fine-tuned on parallel French/Arabic legal data show no improvement over base models, the mechanism shifts from lexical mismatch to reasoning failure.

### Mechanism 2
- Claim: Multi-answer MCQ formats expose limitations in standard evaluation metrics, requiring composite scoring to distinguish between partial knowledge and random guessing.
- Mechanism: Standard accuracy treats partial matches as total failures. By applying F1-like (Equation 2) and PMPA (Equation 3) metrics, the system penalizes false positives (incorrect options selected) while rewarding true positives, providing a granular view of model uncertainty.
- Core assumption: A high F1/PMPA score amidst low strict accuracy correlates with "usable" partial knowledge rather than hallucination.
- Evidence anchors:
  - [section] Page 2 states, "We didn’t find any instances in LLM QA literature with this setting," justifying the new metrics.
  - [section] Page 3 details the F1-like metric balancing precision and recall to handle variable option counts.
  - [corpus] No direct corpus evidence discusses multi-answer penalization mechanisms; existing benchmarks predominantly assume single-answer formats.
- Break condition: If models consistently achieve high precision but zero recall (refusing to answer), the metric focus must shift from penalization to coverage.

### Mechanism 3
- Claim: High confidence calibration errors (ECE) in legal QA indicate that models fail to recognize their own knowledge boundaries in low-resource domains.
- Mechanism: Models output high probability scores for incorrect options (hallucinations). The gap between confidence (Equation 6/8) and accuracy (Equation 5/7) widens specifically in "noisy" domains like Family Law where modern rights frameworks conflict with traditional jurisprudence.
- Core assumption: Confidence scores from the LLM logits are reliable proxies for internal epistemic uncertainty.
- Evidence anchors:
  - [abstract] Highlights "confidence calibration" as a key evaluation component.
  - [section] Table 1 shows Llama-3.3 (70b) has high accuracy (33.28%) but extremely high Set-Level Calibration Error (59.40), suggesting over-confidence in wrong answer sets.
  - [corpus] MedArabiQ and MedAraBench emphasize performance gaps in specialized Arabic domains, implying calibration issues are common in low-resource settings.
- Break condition: If post-processing calibration (e.g., temperature scaling) flattens ECE without changing rankings, the issue is softmax distribution rather than true uncertainty.

## Foundational Learning

- Concept: **Multi-Label Classification Metrics (F1-like / PMPA)**
  - Why needed here: You cannot use simple Accuracy because questions have variable correct answers (1 to 10). You must balance rewarding correct selection against penalizing wrong selection.
  - Quick check question: If a model selects 3 options for a question with 2 correct answers and 1 wrong answer, does Strict Accuracy capture the difference between this and a model that selects 0 options?

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: In legal contexts, an incorrect answer with 99% confidence is more dangerous than one with 51% confidence. You need to measure if the model "knows what it doesn't know."
  - Quick check question: A model answers 50% of questions correctly. If it is perfectly calibrated, what should the average confidence score be for the incorrect subset?

- Concept: **Legal Hybridity in NLP**
  - Why needed here: You cannot treat Moroccan legal text as standard Arabic. It contains code-switching (French) and domain-specific archaic terms that standard tokenizers may missegment.
  - Quick check question: How would a tokenizer trained solely on Modern Standard Arabic news corpora likely handle a French legal loanword transliterated into Arabic script?

## Architecture Onboarding

- Component map:
  - **Ingestion:** PDF/Image documents -> **Step 3 (Org)** -> Image batches.
  - **Extraction:** Gemini-2.0-flash (Vision) -> JSON MCQs.
  - **Validation:** Manual legal expert verification (Step 5).
  - **Execution:** Prompt Engineering (Appendix C) -> Target LLM (e.g., Llama, Gemini).
  - **Evaluation:** Compute ACC (Eq 1), F1-like (Eq 2), ECE (Eq 4).

- Critical path: The **Extraction -> Validation** loop. If the vision model hallucinates a question option during Step 4 and it is missed in Step 5, the evaluation of the target LLM is compromised.

- Design tradeoffs:
  - **Strict vs. Soft Metrics:** The paper trades off ease of interpretation (Strict Accuracy) for information density (PMPA/F1).
  - **Automation vs. Quality:** Using a multimodal LLM for extraction (Step 4) speeds up creation but introduces AI-generated artifacts that require expensive manual cleanup.

- Failure signatures:
  - **High ACC, Low ECE:** Model is correct but under-confident.
  - **Low ACC, Low ECE:** Model is wrong but admits it (safe failure).
  - **Low ACC, High ECE:** Model is wrong but confident (critical failure).
  - **Category Skew:** High performance on "Constitution" but low on "Family Code" suggests failure to integrate conflicting value systems (Western vs. Islamic).

- First 3 experiments:
  1. **Zero-Shot Baseline:** Run the prompt in Figure 5/6 on the target model to establish baseline ACC and ECE without context examples.
  2. **Metric Sensitivity Test:** Vary the penalty factor $\beta$ in PMPA (Eq 3) to see if model rankings change (does Model A beat Model B only when penalties are low?).
  3. **Error Analysis by Category:** Isolate questions from "Criminal Law" (847 items) vs. "Family Law" (66 items) to determine if performance drops correlate with dataset size or conceptual difficulty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MizanQA benchmark be effectively generalized to create a universal evaluation framework for legal LLMs across diverse Arab jurisdictions outside of Morocco?
- Basis in paper: [explicit] The authors state this work is an "initial step towards creating a universal benchmark and legal LLMs for all Arab countries" and highlight the current lack of coverage for legal systems in other Arab nations.
- Why unresolved: The current dataset is specifically tailored to the unique blend of Maliki jurisprudence, customary law, and French influences found in Morocco, which may not translate directly to other legal traditions.
- What evidence would resolve it: Successful application of the MizanQA evaluation framework—or an expanded version of it—to datasets from other Arab countries (e.g., Gulf states, Levant) showing consistent metric reliability.

### Open Question 2
- Question: How does the "fusion" of Islamic jurisprudence with modern human rights frameworks specifically contribute to the performance degradation observed in the Family Code and Criminal Law categories?
- Basis in paper: [inferred] The authors note lower performance in these areas and hypothesize it reflects challenges in integrating Islamic principles with Western concepts, but they do not isolate this variable from others like training data scarcity.
- Why unresolved: It is unclear if the drop in accuracy is due to the inherent conceptual conflict in the legal sources or simply because LLMs have been trained on less data related to these specific domains.
- What evidence would resolve it: An ablation study comparing model performance on questions purely about Islamic jurisprudence versus those integrating modern statutes, or an analysis of training data prevalence for these specific legal domains.

### Open Question 3
- Question: To what extent does the multiple-choice format limit the assessment of authentic legal reasoning capabilities compared to open-ended generative tasks?
- Basis in paper: [explicit] The authors list "Overreliance on Multiple Choice Format" as a limitation, noting it "may not fully reflect how legal professionals reason, argue, or interpret texts."
- Why unresolved: The current metrics (Accuracy, F1-like) measure option selection but do not evaluate the model's ability to generate coherent legal arguments or interpret nuanced text dynamically.
- What evidence would resolve it: A comparative study benchmarking the same models on an open-ended generative version of MizanQA using human or LLM-based evaluation of reasoning quality.

## Limitations
- Dataset creation relied on AI-assisted extraction (Gemini-2.0-flash vision) with manual validation, potentially introducing systematic biases
- Relatively small dataset size (1,776 questions) compared to established benchmarks, limiting statistical power
- Evaluation focuses exclusively on zero-shot performance, leaving fine-tuning potential unexplored
- Multiple-choice format may not fully capture authentic legal reasoning capabilities

## Confidence

- Mechanism 1 (Linguistic complexity): **Medium** - well-argued but lacks comparative ablation studies
- Mechanism 2 (Multi-answer metrics): **Medium** - novel approach but untested against alternatives
- Mechanism 3 (Calibration errors): **High** - strong quantitative evidence, though underlying assumptions about confidence reliability need validation

## Next Checks

1. **Ablation study on data source quality**: Compare model performance when evaluating only manually curated questions versus the full AI-assisted dataset to quantify the impact of the initial extraction process on final results.

2. **Cross-lingual legal reasoning test**: Evaluate whether models trained on French legal corpora but tested on Arabic Moroccan law show similar performance degradation, helping isolate whether the issue is linguistic hybridity versus general legal reasoning transfer.

3. **Confidence calibration validation**: Correlate model confidence scores with human-annotated uncertainty ratings on a subset of questions to verify whether LLM logits genuinely reflect epistemic uncertainty or are systematically miscalibrated in legal domains.