---
ver: rpa2
title: 'Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn
  Rebuttal Discussions'
arxiv_id: '2505.07920'
source_url: https://arxiv.org/abs/2505.07920
tags:
- review
- data
- dataset
- peer
- rebuttal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Re2, the largest consistency-ensured peer
  review dataset designed to address critical limitations in existing review datasets.
  Re2 includes 19,926 initial paper submissions, 70,668 review comments, and 53,818
  rebuttals from 24 conferences and 21 workshops.
---

# Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions

## Quick Facts
- arXiv ID: 2505.07920
- Source URL: https://arxiv.org/abs/2505.07920
- Reference count: 16
- Introduces the largest consistency-ensured peer review dataset with 19,926 initial submissions, 70,668 reviews, and 53,818 rebuttals

## Executive Summary
Re$^2$ is a comprehensive peer review dataset that addresses critical limitations in existing review datasets by focusing on initial paper submissions and including structured multi-turn conversation data for rebuttal and discussion stages. The dataset contains 19,926 initial paper submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops. Unlike prior datasets that often use revised submissions or lack rebuttal data, Re$^2$ guarantees all papers are initial submissions and provides rich interaction data that enables both traditional review tasks and interactive LLM-based reviewing assistants.

The dataset supports multiple research directions including acceptance prediction, score prediction, review generation, and multi-turn conversation modeling. Experimental results demonstrate that fine-tuning on Re$^2$ significantly improves model performance across these tasks, with the fine-tuned LLaMA-3.1-8B achieving state-of-the-art results in review generation (BLEU 2.50, EmbedCos 0.730) and rebuttal-discussion conversation tasks. The dataset aims to reduce reviewer burden and help authors self-evaluate manuscripts before submission, addressing key challenges in the peer review process.

## Method Summary
Re$^2$ was constructed through a systematic data collection process from 24 conferences and 21 workshops, ensuring that all papers are initial submissions rather than revised versions. The dataset includes three main components: paper submissions, review comments, and rebuttals, with a particular emphasis on capturing the full lifecycle of the peer review process including the rebuttal and discussion stages. The consistency-ensured methodology guarantees data quality and reliability, distinguishing it from previous datasets that often contained mixed data types or lacked critical interaction data.

## Key Results
- Contains 19,926 initial paper submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops
- Fine-tuned LLaMA-3.1-8B achieves state-of-the-art results in review generation (BLEU 2.50, EmbedCos 0.730)
- Significant performance improvements across multiple tasks after fine-tuning on Re$^2$
- Supports both traditional review tasks and interactive LLM-based reviewing assistants

## Why This Works (Mechanism)
The dataset's effectiveness stems from its focus on initial submissions rather than revised versions, capturing the authentic peer review process where authors receive feedback and can respond through rebuttals. The multi-turn conversation structure enables models to learn the dynamic nature of review discussions and understand how reviewers' opinions may evolve through interactions with authors. This comprehensive coverage of the full peer review lifecycle, from initial submission through rebuttal discussions, provides richer contextual information than previous datasets.

## Foundational Learning
- Peer review lifecycle understanding: Why needed - to model the complete review process from submission to final decision; Quick check - verify dataset includes all stages from initial submission through final acceptance/rejection
- Multi-turn conversation modeling: Why needed - to capture the dynamic nature of review discussions and author-reviewer interactions; Quick check - ensure conversation threads are properly structured and linked
- Initial submission vs revised paper distinction: Why needed - to study the authentic review process before author revisions; Quick check - confirm all papers are indeed initial submissions
- Review quality assessment: Why needed - to enable automated evaluation of review helpfulness and constructiveness; Quick check - implement quality metrics for review comments
- Acceptance prediction modeling: Why needed - to predict paper outcomes based on reviews and rebuttals; Quick check - validate prediction accuracy across different conference types

## Architecture Onboarding

Component map: Paper submissions -> Review comments -> Rebuttals -> Final decisions -> Multi-turn discussions

Critical path: Initial paper submission -> Review assignment -> Review generation -> Author rebuttal -> Reviewer response -> Final decision

Design tradeoffs: The dataset prioritizes comprehensiveness and authenticity over balanced representation across different conference types and research areas, potentially limiting generalizability but ensuring data quality.

Failure signatures: Models trained on this dataset may overfit to specific conference styles or reviewer patterns if not properly regularized, and may struggle with papers from domains underrepresented in the dataset.

First experiments:
1. Baseline review generation performance without fine-tuning on Re$^2$
2. Acceptance prediction accuracy using only review scores versus using full review text and rebuttal content
3. Multi-turn conversation modeling to predict review changes after author rebuttals

## Open Questions the Paper Calls Out
None

## Limitations
- The precise definition and implementation of "consistency-ensured" data collection methodology remains unclear
- The dataset's coverage of 24 conferences and 21 workshops may not be representative of all peer review practices
- Potential selection biases in which submissions, reviews, and rebuttals were included
- Temporal scope limitations may not capture evolving review practices over time

## Confidence
- Dataset construction methodology: Medium
- Size and scope claims: High
- Performance improvements from fine-tuning: Medium
- Claims about reducing reviewer burden and enabling self-evaluation: Low

## Next Checks
1. Conduct inter-annotator agreement studies to verify the consistency-ensured methodology
2. Perform domain adaptation experiments across different conference types to assess generalizability
3. Implement ablation studies to determine the relative contributions of initial submissions versus multi-turn conversation data to model performance improvements