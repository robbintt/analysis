---
ver: rpa2
title: Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and
  Complex Interactions
arxiv_id: '2502.08438'
source_url: https://arxiv.org/abs/2502.08438
tags:
- image
- sketch
- text
- object
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the novel problem of composite sketch+text
  based image retrieval (CSTBIR), where users search for objects with elusive names
  and complex interactions using both hand-drawn sketches and text descriptions. The
  authors curate a large dataset (CSTBIR) with approximately 2 million queries and
  108,000 natural scene images.
---

# Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions

## Quick Facts
- **arXiv ID:** 2502.08438
- **Source URL:** https://arxiv.org/abs/2502.08438
- **Reference count:** 8
- **Primary result:** Introduces a novel composite sketch+text retrieval problem and a model (STNET) that significantly outperforms text-only, sketch-only, and composite baselines on a curated dataset.

## Executive Summary
This paper addresses the challenge of retrieving objects with "elusive names" (lacking clear textual descriptions) and complex interactions using a composite query of a hand-drawn sketch and a text description. The authors curate the CSTBIR dataset with 2 million queries and 108,000 natural images. They propose STNET, a multimodal transformer-based model that localizes objects via sketch-guided attention and fuses sketch and text embeddings for retrieval. STNET is trained with contrastive learning plus auxiliary objectives (object detection, sketch reconstruction) and significantly outperforms baselines on multiple metrics, especially on a challenging open-category test set.

## Method Summary
The proposed STNET model uses a three-encoder architecture: a text encoder, a sketch encoder, and an image encoder (all based on CLIP/ViT). The sketch embedding is used to compute attention scores over the image patch embeddings, localizing the relevant object. The final query representation is an element-wise product of the sketch and text embeddings. The model is trained with a contrastive loss for retrieval, plus auxiliary losses for object detection and sketch reconstruction to enforce semantic alignment between sketches and images.

## Key Results
- STNET significantly outperforms text-only, sketch-only, and composite query baselines on multiple metrics (Recall@K and Median Rank).
- The model shows strong performance on a challenging open-category test set with unseen object types.
- Ablation studies confirm the importance of all three training objectives (contrastive learning, object detection, and sketch reconstruction).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sketch-guided attention localizes relevant objects in complex natural scenes, filtering out background noise.
- **Mechanism:** The model computes dot-product attention scores between the global sketch embedding and the spatial output embeddings of the image encoder. These scores weight the image features, effectively masking irrelevant regions before pooling for the final retrieval comparison.
- **Core assumption:** The visual geometry of a rough sketch correlates sufficiently with the spatial features of the target object in the natural image.
- **Evidence anchors:**
  - [Page 4] "We use the pooled output of the sketch encoder... to calculate dot-product attention over the output embeddings of the image encoder... obtain weighted values of image embeddings."
  - [Page 4] "To aid the localization of the query-relevant object... we propose the training objective of sketch-guided localization."
  - [Corpus] Related work in compositional retrieval (e.g., UNION) supports the efficacy of target representation modifications for specific queries, though specific sketch-attention mechanisms are not detailed in the provided neighbors.
- **Break condition:** If the sketch depicts an object at a scale or perspective incompatible with the ViT's patch grid (16x16), the attention mechanism may fail to isolate the object, defaulting to global features.

### Mechanism 2
- **Claim:** Multimodal fusion outperforms sequential "recognition-then-retrieval" (two-stage) pipelines by preserving ambiguous visual signals.
- **Mechanism:** Instead of forcing a discrete classification of the sketch into a text label (which discards nuance), STNET jointly embeds the sketch and complementary text. This allows the "elusive" visual features to influence the retrieval directly alongside the "easy-to-verbalize" text attributes.
- **Core assumption:** The information lost in quantizing a sketch to a class label (e.g., "mouse" vs. "bat") is critical for distinguishing the correct target image in the gallery.
- **Evidence anchors:**
  - [Page 2] "Such a two-stage method may fail when the sketch represents an object with an ambiguous name... and suffers from signal loss."
  - [Page 5] "STNET is better than the two-stage model... because the object name may not completely cover the semantics in the sketch."
- **Break condition:** If the text description contradicts the sketch or describes a different object entirely, the joint embedding space may become confused, degrading results more than a sequential approach that relies solely on the classified object.

### Mechanism 3
- **Claim:** Auxiliary reconstruction and detection objectives enforce semantic alignment between rough sketches and detailed images.
- **Mechanism:** The model is trained not just on retrieval (contrastive loss), but also to reconstruct the sketch from image features and detect bounding boxes. This forces the encoder to learn structural correspondences rather than just superficial feature correlation.
- **Core assumption:** The feature space required to reconstruct a sketch or detect an object overlaps significantly with the feature space needed for retrieval similarity.
- **Evidence anchors:**
  - [Page 4] "In addition to contrastive learning, we propose multiple training objectives... sketch-guided object detection... and sketch reconstruction."
  - [Page 6] "Removing any of the three losses leads to degradation in performance... degradation worsens when the LCLS is removed."
- **Break condition:** If the dataset contains significant noise (sketches that do not visually correspond to the labeled object), the reconstruction loss may force the model to memorize noise rather than learn generalizable alignment.

## Foundational Learning

- **Concept: Vision Transformer (ViT) Patch Embeddings**
  - **Why needed here:** The model relies on ViTs for both sketch and image encoding. Unlike CNNs, ViTs process images as sequences of patches. Understanding how positional embeddings and patch tokens work is required to grasp how the sketch-to-image attention (Mechanism 1) manipulates spatial features.
  - **Quick check question:** How does the model derive the spatial attention map for the image using the sketch's [CLS] token?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - **Why needed here:** This is the core driver of the retrieval capability. It trains the model to maximize cosine similarity between the query (sketch+text) and the correct image while minimizing it for negative samples in the batch.
  - **Quick check question:** In the context of STNET, what constitutes the "positive" pair versus the "negative" pairs during the contrastive training phase?

- **Concept: Cross-Modal / Multi-Modal Fusion**
  - **Why needed here:** The paper introduces a composite query (Sketch + Text). One must understand how distinct modalities (a rasterized sketch and a tokenized sentence) are projected into a shared latent space to form a single query representation.
  - **Quick check question:** Does STNET fuse sketch and text features before retrieving, or does it retrieve with one and refine with the other? (Answer: It fuses them via attention and element-wise product for the query representation).

## Architecture Onboarding

- **Component map:**
  - Text Encoder -> Sketch Encoder -> Image Encoder -> Dot Product Attention (Sketch over Image) -> Average Pooling -> Element-wise Product (Sketch+Text) -> Query Vector

- **Critical path:**
  1.  **Input:** Sketch $S$ + Text $T$ + Image $I$.
  2.  **Encoding:** $h^S_{CLS}$ (Sketch), $h^T_{CLS}$ (Text), $\tilde{H}^I$ (Image patches).
  3.  **Localization:** $\alpha = \text{Softmix}(\tilde{H}^I \times h^S_{CLS})$.
  4.  **Query Fusion:** $Q = h^T_{CLS} \odot h^S_{CLS}$.
  5.  **Image Representation:** $V = \text{AvgPool}(\alpha \odot \tilde{H}^I)$.
  6.  **Matching:** Cosine similarity between $Q$ and $V$.

- **Design tradeoffs:**
  - **Two-Stage vs. End-to-End:** A two-stage approach (Classify sketch -> Text query -> CLIP) is cheaper to implement using off-the-shelf models but fails on ambiguous sketches (Table 3, Two-stage performance drop). STNET is end-to-end, preserving sketch nuance but requiring complex multi-task training.
  - **Complexity:** The addition of Object Detection ($L_{OD}$) and Reconstruction ($L_{SR}$) heads adds significant computational overhead and architectural complexity compared to standard Contrastive Language-Image Pre-training.

- **Failure signatures:**
  - **High Median Rank (MdR):** Indicates the model fails to localize the object, likely caused by the sketch encoder not aligning with the image encoder's attention space.
  - **Good R@100 but bad R@10:** Suggests the model identifies the general scene context (via Text) but fails to match the specific object instance (via Sketch).

- **First 3 experiments:**
  1.  **Modality Ablation:** Run retrieval using (a) Text-only, (b) Sketch-only, and (c) Sketch+Text on the Test-1K set to reproduce the claim that the composite query outperforms individual modalities.
  2.  **Localization Visualization:** Visualize the attention map $\alpha$ (Mechanism 1) overlaid on the target image. Verify if the "attention" actually highlights the queried object or distractors.
  3.  **Zero-Shot Generalization:** Evaluate the model on the Open-Category test set to determine if the sketch encoder relies on rote memorization of training classes or learns generalizable shape primitives.

## Open Questions the Paper Calls Out
None

## Limitations
- The model's performance is tightly coupled to the quality and specificity of the hand-drawn sketch, which is inherently variable in user-generated queries.
- The generalizability of the results to real-world user queries with diverse drawing abilities and styles is not fully established.
- The model's robustness to adversarial or noisy sketches (e.g., poorly drawn, incomplete, or ambiguous sketches) is not thoroughly evaluated.

## Confidence
- **High Confidence:** The architectural design of STNET (Vision Transformer encoders, dot-product attention for localization, contrastive learning framework) is sound and well-grounded in established computer vision principles. The ablation studies in Table 3 provide strong evidence for the necessity of the proposed multi-task training objectives.
- **Medium Confidence:** The claim that sketch+text composite queries significantly outperform individual modalities is supported by the reported Recall@K and Median Rank metrics. However, the specific magnitude of improvement and its consistency across diverse query types requires further empirical validation.
- **Low Confidence:** The model's robustness to adversarial or noisy sketches is not thoroughly evaluated. The paper focuses on performance given "well-formed" queries from the dataset, leaving open questions about its failure modes under realistic user input conditions.

## Next Checks
1. **User Study with Diverse Sketches:** Conduct a user study where participants are asked to draw sketches for a set of objects with "elusive names" and ambiguous appearances. Evaluate STNET's retrieval performance on these user-generated sketches to assess its real-world robustness and identify common failure modes related to sketch quality and style.

2. **Cross-Dataset Generalization Test:** Evaluate the pre-trained STNET model on a different, external dataset of natural images with associated sketch queries (if available) or a subset of the existing dataset held out during training. This will test the model's ability to generalize beyond the specific distribution of the CSTBIR dataset.

3. **Ablation Study on Noise and Ambiguity:** Systematically introduce noise and ambiguity into the sketch input (e.g., by blurring, occluding parts of the sketch, or using sketches of visually similar objects) and measure the degradation in retrieval performance. This will quantify the model's sensitivity to imperfect input and help identify the limits of its localization mechanism.