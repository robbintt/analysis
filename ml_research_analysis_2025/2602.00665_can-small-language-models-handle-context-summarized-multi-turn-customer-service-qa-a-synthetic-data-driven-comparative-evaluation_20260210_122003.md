---
ver: rpa2
title: Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service
  QA? A Synthetic Data-Driven Comparative Evaluation
arxiv_id: '2602.00665'
source_url: https://arxiv.org/abs/2602.00665
tags:
- evaluation
- slms
- customer-service
- b-instruct
- multi-turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of fine-tuned Small Language
  Models (SLMs) for context-summarized multi-turn customer-service question answering
  (QA). To address the lack of publicly available multi-turn conversational data,
  the authors introduce a synthetic data construction pipeline that transforms single-turn
  QA pairs into structured multi-turn interactions with context summarization and
  response refinement.
---

# Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation

## Quick Facts
- **arXiv ID**: 2602.00665
- **Source URL**: https://arxiv.org/abs/2602.00665
- **Reference count**: 19
- **Primary result**: SLMs (3-4B/8B) achieve competitive performance to LLMs on context-summarized multi-turn customer-service QA across lexical, semantic, LLM-as-a-judge, and human evaluation metrics.

## Executive Summary
This paper evaluates the performance of fine-tuned Small Language Models (SLMs) for context-summarized multi-turn customer-service question answering (QA). To address the lack of publicly available multi-turn conversational data, the authors introduce a synthetic data construction pipeline that transforms single-turn QA pairs into structured multi-turn interactions with context summarization and response refinement. Nine instruction-tuned SLMs across different parameter ranges are fine-tuned using QLoRA and evaluated against three commercial LLMs using lexical and semantic metrics, LLM-as-a-judge, human evaluation, and conversation stage-based analysis. Results show that several SLMs, particularly in the 3-4B and 8B ranges, achieve competitive performance to LLMs, especially in human-likeness, tone, and late-stage resolution, demonstrating the viability of SLMs for efficient, scalable customer-service QA.

## Method Summary
The authors constructed a synthetic dataset from the Customer Service Banking Conversation Corpus by filtering conversations to 5-100 turns and splitting them into early (20%), mid (70%), and late (10%) stages. Each turn was processed using GPT-4o-mini for context summarization (max 250 tokens) and GPT-4.1 for response refinement, with OpenAI Moderation API filtering applied. Nine SLMs (1.7B-8B parameters) were fine-tuned using QLoRA with 4-bit quantization, LoRA rank=16, alpha=32, dropout=0.1, max sequence length=512, 3 epochs, and AdamW optimizer. Models were evaluated against commercial LLMs using ROUGE-L, METEOR, BERTScore, BARTScore, Cosine Similarity, LLM-as-a-judge (Claude Sonnet 4.5), human evaluation (3 annotators), and pairwise comparisons (Claude Haiku 4.5).

## Key Results
- SLMs in the 3-4B and 8B parameter ranges achieved competitive performance to commercial LLMs across all evaluation metrics
- Late-stage conversation resolution performance was particularly strong for SLMs, with Qwen-3-4B achieving the highest human evaluation scores
- Human evaluation showed SLMs outperformed commercial LLMs in tone and human-likeness dimensions, despite slightly lower semantic similarity scores

## Why This Works (Mechanism)
SLMs can achieve competitive performance on context-summarized multi-turn customer-service QA when fine-tuned on high-quality synthetic data. The synthetic data pipeline enables training without requiring large amounts of real conversational data, while context summarization preserves conversational state without overwhelming the model with history. Fine-tuning with QLoRA provides efficient parameter-efficient adaptation of pre-trained models to the customer-service domain.

## Foundational Learning
- **QLoRA fine-tuning**: Low-rank adaptation with 4-bit quantization for efficient SLM adaptation. Needed to enable fine-tuning of large models on limited hardware. Quick check: Monitor validation loss to detect overfitting.
- **Context summarization**: GPT-4o-mini generates concise conversation summaries (max 250 tokens) to preserve state. Needed to prevent context window overflow in multi-turn conversations. Quick check: Verify summary length distribution and spot-check for missing key information.
- **Response refinement**: GPT-4.1 refines agent responses for quality and consistency. Needed to ensure synthetic data quality matches real-world expectations. Quick check: Sample refined responses for coherence and domain appropriateness.

## Architecture Onboarding
- **Component map**: Single-turn QA pairs -> Multi-turn conversations -> Context summarization -> Response refinement -> Filtered dataset -> SLM fine-tuning -> Evaluation
- **Critical path**: Synthetic data construction (summarization + refinement) -> QLoRA fine-tuning -> Multi-metric evaluation
- **Design tradeoffs**: Synthetic data enables training without real conversational data but introduces quality dependency on GPT models; smaller SLMs offer efficiency but may underperform on complex reasoning tasks.
- **Failure signatures**: Context summarization truncation losing critical details; QLoRA overfitting on smallest models; reasoning tags leaking in outputs requiring post-processing.
- **Exactly 3 first experiments**: 1) Generate synthetic dataset with varying summarization quality levels and measure impact on SLM performance. 2) Fine-tune SLMs with different LoRA ranks (8, 16, 32) to optimize for validation performance. 3) Compare hallucination rates between SLMs and LLMs using retrieval-augmented generation.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data quality depends entirely on GPT-4o-mini and GPT-4.1 performance, with no ablation study on summarization impact
- Human evaluation sample size (500 per model) may be insufficient for detecting smaller performance differences
- Single banking domain limits generalizability to other customer service contexts
- LLM-as-a-judge evaluation may not capture practical deployment considerations like hallucination rates

## Confidence
- **High confidence**: SLMs in 3-4B and 8B parameter ranges can achieve competitive performance to commercial LLMs on context-summarized multi-turn customer-service QA
- **Medium confidence**: Late-stage conversation resolution performance is particularly strong for SLMs
- **Low confidence**: Claims about SLMs being "viable" for production deployment require additional validation beyond this study's scope

## Next Checks
1. **Ablation study on context summarization quality**: Generate multiple versions of the synthetic dataset with varying summarization quality using different prompting strategies or model sizes, then measure how this affects SLM performance to quantify the impact of the synthetic pipeline.

2. **Out-of-domain generalization test**: Evaluate the best-performing SLMs from this study on a different customer service domain (e.g., telecommunications or e-commerce) using the same synthetic pipeline to assess whether the observed performance advantages transfer beyond banking.

3. **Hallucination and factual consistency audit**: Implement a systematic evaluation using retrieval-augmented generation to check whether SLM responses maintain factual accuracy about customer accounts, policies, and transactions, comparing hallucination rates against commercial LLMs using the same synthetic data.