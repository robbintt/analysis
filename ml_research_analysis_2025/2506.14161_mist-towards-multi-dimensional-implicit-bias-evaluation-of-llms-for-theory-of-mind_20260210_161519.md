---
ver: rpa2
title: 'MIST: Towards Multi-dimensional Implicit BiaS Evaluation of LLMs for Theory
  of Mind'
arxiv_id: '2506.14161'
source_url: https://arxiv.org/abs/2506.14161
tags:
- bias
- arxiv
- llms
- group
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of assessing Theory of Mind (ToM)\
  \ in Large Language Models (LLMs) by detecting implicit social biases that arise\
  \ when models misapply group-level stereotypes to individual-level inferences. The\
  \ authors propose MIST, a novel framework that conceptualizes implicit bias along\
  \ three psychological dimensions\u2014Competence, Sociability, and Morality\u2014\
  using the Stereotype Content Model (SCM)."
---

# MIST: Towards Multi-dimensional Implicit BiaS Evaluation of LLMs for Theory of Mind

## Quick Facts
- arXiv ID: 2506.14161
- Source URL: https://arxiv.org/abs/2506.14161
- Reference count: 5
- Primary result: Framework detects implicit social biases in LLMs as Theory of Mind failures across three dimensions using indirect tasks.

## Executive Summary
This paper introduces MIST, a novel framework for detecting implicit social biases in Large Language Models (LLMs) by framing bias evaluation as Theory of Mind failures. The approach addresses the limitation of existing bias detection methods that rely on explicit questioning, which often triggers model avoidance mechanisms. MIST employs indirect evaluation tasks—Word Association Bias Test (WABT) and Affective Attribution Test (AAT)—to uncover latent stereotypes without triggering model refusal behaviors. The framework conceptualizes implicit bias through the Stereotype Content Model (SCM) across three psychological dimensions: Competence, Sociability, and Morality.

## Method Summary
MIST evaluates implicit biases in LLMs through two indirect tasks designed to bypass explicit bias-avoidance mechanisms. The Word Association Bias Test (WABT) presents models with group identifiers paired with attribute words, measuring bias through association patterns across Competence, Sociability, and Morality dimensions. The Affective Attribution Test (AAT) generates scenarios involving social groups and neutral objects, requiring binary classification as "Comedy" or "Tragedy" to reveal affective attribution tendencies. The framework uses a carefully constructed lexicon covering race, gender, and health dimensions, with systematic sampling to generate evaluation instances. Bias is quantified through dimension-specific scores and attribution rates, revealing multidimensional patterns that differ from traditional binary bias assessments.

## Key Results
- Models exhibit consistent positive bias in the Sociability dimension across all tested LLMs
- Bias patterns are multidimensional and divergent, not monolithic, with different models showing distinct FAR/UAR asymmetry patterns
- Emergent "Neutrality" responses in AAT suggest either internal stereotype conflicts or safety alignment effects
- MIST successfully detects subtle and pervasive stereotypes that traditional explicit methods miss

## Why This Works (Mechanism)

### Mechanism 1
Indirect task framing allows for the detection of implicit biases that are suppressed during direct inquiry. LLMs undergo safety alignment that triggers refusal or "social desirability effects" when asked explicitly about social beliefs. By framing the evaluation as an objective lexical association task (WABT) or a subjective affective judgment (AAT) rather than a belief query, the model processes the input through its standard next-token prediction pathways without activating refusal classifiers. This allows latent statistical regularities regarding social groups to surface in the output.

### Mechanism 2
Reconceptualizing bias through the Stereotype Content Model (SCM) reveals dimension-specific failures in Theory of Mind (ToM) reasoning. Rather than treating bias as a binary or single-axis problem, this mechanism maps bias onto three distinct psychological dimensions: Competence, Sociability, and Morality. When an LLM infers mental states, it may correctly apply group-level statistical regularities in one dimension while failing in another. This granular mapping prevents the "halo effect" where one positive trait masks negative biases in other areas.

### Mechanism 3
Asymmetric affective attribution tendencies expose implicit preference hierarchies between social groups. The Affective Attribution Test (AAT) forces the model to categorize scenarios involving social groups into binary emotional valences ("Comedy" vs. "Tragedy"). Bias manifests as an asymmetry: the model may disproportionately assign positive valence to advantaged groups (High FAR) or negative valence to disadvantaged groups (High UAR), but rarely both simultaneously. This divergence suggests distinct underlying mechanisms for "in-group" favoritism vs. "out-group" derogation.

## Foundational Learning

- **Concept: Stereotype Content Model (SCM)**
  - Why needed here: This is the theoretical backbone of the MIST framework. Without understanding that social perception splits into Competence, Sociability, and Morality, the bias scores generated by the system are uninterpretable.
  - Quick check question: If a model outputs high positive bias in "Sociability" but negative bias in "Competence" for a specific group, does SCM predict this is a coherent stereotype profile?

- **Concept: Implicit vs. Explicit Bias Probing**
  - Why needed here: The paper explicitly avoids direct queries. Understanding the difference between asking "Is [Group A] bad?" (Explicit) vs. "Associate [Group A] with [Word]" (Implicit) is necessary to understand why WABT and AAT are designed the way they are.
  - Quick check question: Why would a model refuse to answer a direct question about a social group but complete a word association task involving the same group?

- **Concept: Theory of Mind (ToM) in LLMs**
  - Why needed here: The paper frames bias not just as "offensive output," but as a "failure of ToM"—specifically, the inability to distinguish individual evidence from group-level statistical priors.
  - Quick check question: In the context of this paper, does a "systemic failure of ToM" mean the model cannot reason, or that it applies the wrong heuristic (group stereotype) to an individual?

## Architecture Onboarding

- **Component map:** Lexicon Constructor -> Template Engine -> Evaluator
- **Critical path:**
  1. Define Lexicon: Select group identifiers and attribute words strictly categorized by SCM dimension
  2. Generate Prompts: Inject lexicon into WABT/AAT templates
  3. Filter Validity: Discard refusals or "Neutrality" responses (AAT)
  4. Calculate Metrics: Aggregate pair counts into dimension-specific bias scores

- **Design tradeoffs:**
  - Direct vs. Indirect: Indirect tasks (AAT/WABT) reduce refusal rates but introduce ambiguity in interpretation (e.g., is "Neutrality" a bias or a safety feature?)
  - Binary vs. Continuous: AAT forces a binary "Comedy/Tragedy" choice, which simplifies evaluation but may lose nuance compared to a continuous sentiment score

- **Failure signatures:**
  - High "Neutrality" Rate: Indicates the model is avoiding the classification task entirely (e.g., LLaMa-2-70B-Chat showed 60%+ neutrality), rendering the AAT ineffective
  - Low Valid Response Count: If n is very low (e.g., DeepSeek-V3 in WABT with n=42), the statistical power of the bias score is compromised

- **First 3 experiments:**
  1. Baseline WABT: Run the WABT task on a target model using the provided race/gender lexicon to establish the base bias scores across Competence, Sociability, and Morality
  2. AAT Neutrality Check: Run the AAT task specifically to measure the percentage of "Neutrality" responses. If >20%, the model may be over-aligned for safety, requiring prompt adjustment
  3. Dimension Ablation: Isolate one SCM dimension (e.g., Morality) and test if changing the template phrasing alters the bias score significantly, checking for prompt-sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
Do the emergent neutral response patterns in the Affective Attribution Test (AAT) stem from internal stereotype conflicts or safety-oriented alignment objectives? The current study observes the neutral output behavior but cannot disentangle the underlying cognitive or algorithmic cause based solely on the output text. Ablation studies comparing base models against their instruction-tuned versions to see if neutrality correlates with the introduction or strength of safety fine-tuning would help resolve this.

### Open Question 2
Does the pervasive positive bias in the Sociability dimension originate from inherent training data skews or an inductive prior embedded in the model's design? The paper quantifies the bias but lacks a causal analysis of the training corpus or the model's architectural priors to confirm the source. Controlled experiments where models are trained on datasets explicitly filtered for sociability valence to observe if the positive bias diminishes or persists would provide evidence.

### Open Question 3
What specific architectural or training differences drive the asymmetric affective attribution tendencies (Positive Amplification vs. Negative Amplification) observed across different LLMs? The framework identifies these distinct behavioral patterns but does not investigate the model internals or training procedures that cause this divergence. Comparative analysis of reward models used in RLHF for these specific platforms to determine if they differentially penalize negative vs. positive stereotyping would help resolve this.

## Limitations
- Template phrasing variability: Only partial template examples are shown, leaving the exact indirect framing of WABT and AAT unclear
- Safety filter variability: The emergence of a "Neutrality" option in AAT suggests inconsistent model behavior across safety alignments, complicating interpretation
- Statistical robustness: DeepSeek-V3's low valid response count in WABT (n=42) raises concerns about the reliability of computed bias scores for some models

## Confidence

- **High:** The core conceptual framework linking implicit bias detection to Theory of Mind failures (SCM-based, indirect task design)
- **Medium:** The specific bias patterns observed (e.g., Sociability bias consistency, FAR/UAR asymmetry) due to potential sensitivity to prompt phrasing and sampling
- **Low:** The generalizability of results beyond the tested social dimensions and LLMs, given the limited scope of lexicon and model diversity

## Next Checks

1. **Template Ablation:** Systematically vary the phrasing of WABT/AAT prompts to test the robustness of bias score sensitivity to indirect framing
2. **Sampling Verification:** Conduct a power analysis to determine the minimum number of valid responses needed per model for reliable bias score estimation, and re-analyze results for models with low n
3. **Dimensional Transfer:** Apply the MIST framework to a new social dimension (e.g., age or religion) not covered in the original lexicon to test the method's generalizability