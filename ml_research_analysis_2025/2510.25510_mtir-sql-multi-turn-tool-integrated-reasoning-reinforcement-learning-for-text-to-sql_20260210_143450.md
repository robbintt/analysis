---
ver: rpa2
title: 'MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for
  Text-to-SQL'
arxiv_id: '2510.25510'
source_url: https://arxiv.org/abs/2510.25510
tags:
- tool
- reasoning
- multi-turn
- query
- text-to-sql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MTIR-SQL, a Multi-turn Tool-Integrated Reasoning
  reinforcement learning framework for Text-to-SQL that integrates dynamic SQL execution
  feedback into the reasoning process. The framework extends GRPO with trajectory
  filtering and removes KL regularization constraints to handle multi-turn interactions
  and improve training stability.
---

# MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL

## Quick Facts
- arXiv ID: 2510.25510
- Source URL: https://arxiv.org/abs/2510.25510
- Reference count: 37
- 64.4% accuracy on BIRD Dev set and 84.6% execution accuracy on SPIDER Dev using a 4B parameter model

## Executive Summary
MTIR-SQL introduces a Multi-turn Tool-Integrated Reasoning reinforcement learning framework for Text-to-SQL that integrates dynamic SQL execution feedback into the reasoning process. The approach extends GRPO with trajectory filtering and removes KL regularization constraints to handle multi-turn interactions and improve training stability. By enabling real-time error correction through iterative tool use and execution verification, MTIR-SQL significantly outperforms existing approaches on benchmark datasets.

## Method Summary
MTIR-SQL uses a Qwen3-4B base model with GRPO-Filter (removes KL constraint, adds trajectory filtering, multi-turn extension) for training. The framework executes SQL queries via SQLite and incorporates execution results as feedback for iterative refinement. Training uses 3-component rewards (format ±0.1, execution ±0.1, result ±1.0) with trajectory filtering to discard low-quality rollouts before policy updates. The model performs multi-turn reasoning with up to 6 tool interactions, using greedy decoding at inference.

## Key Results
- Achieves 64.4% accuracy on BIRD Dev set
- Achieves 84.6% execution accuracy on SPIDER Dev set
- Significantly outperforms existing approaches using a 4B parameter model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn execution feedback enables iterative self-correction, improving SQL generation accuracy over static single-pass approaches.
- Mechanism: The model interleaves reasoning with SQL execution—generating a query, executing it via tool call, receiving execution results (including errors), and refining based on that feedback. This repeats until a correct answer or max turns is reached.
- Core assumption: Errors are detectable from execution feedback and correctable within a small number of iterations.
- Evidence anchors:
  - [abstract] "introduces an execution-aware multi-turn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step, enabling context-sensitive query generation and progressive refinement"
  - [section 3.2] "The model dynamically incorporates database query results into the reasoning process through multi-turn execution... This multi-turn process iterates until the model returns a final answer or a maximum number of turns"
  - [corpus] ReEx-SQL (2505.12768) similarly finds execution feedback essential but treats it post-hoc; MTIR-SQL differs by integrating it during generation.
- Break condition: If execution feedback is uninformative (e.g., generic errors without detail) or max turns is too low, self-correction fails to converge.

### Mechanism 2
- Claim: Trajectory filtering mitigates training instability and reward collapse in multi-turn RL by discarding low-quality rollouts before policy updates.
- Mechanism: During rollout generation, trajectories are filtered using quality criteria (e.g., discarding rollouts with low within-group variance, invalid tool call formats, or exceeding max tries without correct answer). Only high-quality trajectories (F(x,y) > τ) are retained for training.
- Core assumption: Quality filters correctly identify trajectories that would harm training; filtering does not remove useful exploration signals.
- Evidence anchors:
  - [abstract] "enhance the GRPO algorithm by adding a trajectory filtering mechanism and removing KL loss constraints"
  - [section 3.1] "Selective Rollout Filtering: We implement a quality-aware filtering mechanism... T_filtered = {(x,y) ∈ T_rollout : F(x,y) > τ}"
  - [corpus] SimpleTIR (2509.02479) addresses multi-turn TIR instability via filtering empty rounds, providing independent validation that filtering stabilizes multi-turn RL.
- Break condition: If filters are too aggressive, they reduce sample diversity and policy learning stalls; if too permissive, instability persists.

### Mechanism 3
- Claim: Removing KL regularization allows more flexible policy updates, which is necessary when multi-turn tool interactions cause significant distribution drift from the initial model.
- Mechanism: Standard GRPO constrains policy updates with KL divergence penalty (β·KL(πθ||πref)). MTIR-SQL removes this constraint, using unconstrained optimization where the loss only includes the advantage-weighted policy ratio.
- Core assumption: Without KL constraints, the policy does not diverge catastrophically because trajectory filtering and reward signals provide sufficient regularization.
- Evidence anchors:
  - [abstract] "removes KL regularization constraints to handle turn interactions and improve training stability"
  - [section 3.1] "Unlike standard GRPO, we remove the KL divergence constraint... eliminating the traditional KL penalty term β·KL(πθ||πref)"
  - [corpus] No direct corpus comparison on KL removal specifically for Text-to-SQL; this is a relatively unexplored design choice in multi-turn TIR.
- Break condition: If reward signals are sparse or noisy, removing KL can cause policy collapse or distribution drift to irrelevant regions.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: MTIR-SQL extends GRPO; understanding baseline GRPO (group-based advantage estimation, no critic model) is prerequisite.
  - Quick check question: Can you explain how GRPO computes advantages without a learned value function?

- **Concept: Tool-Integrated Reasoning (TIR)**
  - Why needed here: The framework is built on multi-turn TIR where models invoke external tools (SQL executors) during generation.
  - Quick check question: What distinguishes single-turn tool use from multi-turn TIR in terms of training challenges?

- **Concept: Text-to-SQL Task Structure**
  - Why needed here: Understanding schema linking, SQL execution validation, and benchmark datasets (BIRD, SPIDER) contextualizes why execution feedback matters.
  - Quick check question: Why might a syntactically correct SQL query still be semantically incorrect?

## Architecture Onboarding

- **Component map:**
  Policy LLM (Qwen3-4B) -> SQL Execution Tool (SQLite) -> GRPO-Filter Trainer -> Reward Module

- **Critical path:**
  1. User question + schema → Policy LLM generates thinking + tool call
  2. Tool call parsed → SQL executed → Result/error appended to context
  3. Repeat until <answer> tag or max turns (default: 6)
  4. Full trajectory scored → Filtered → Policy updated via GRPO-Filter

- **Design tradeoffs:**
  - Max turns (1 vs 3 vs 6): More turns = more correction opportunities but higher training instability. Paper finds 3-6 optimal.
  - KL removal vs retention: Removing KL improves flexibility but risks distribution drift; trajectory filtering compensates.
  - Reward scaling: Result reward (±1.0) dominates; execution/format rewards (±0.1) provide auxiliary guidance.

- **Failure signatures:**
  - Reward collapse: Training curve shows sharp reward drop (Figure 3) — address by tightening trajectory filters
  - Empty tool results: SQL returns null repeatedly — check schema prompts and column name handling
  - Format violations: Missing <tool_call/> tags — increase format reward magnitude or add format-only pretraining

- **First 3 experiments:**
  1. Replicate single-turn baseline (Max Turns = 1) vs multi-turn (Max Turns = 3) on SPIDER Dev to validate multi-turn gain.
  2. Ablate trajectory filtering: train with vs without filtering, measure training stability (reward variance) and final accuracy.
  3. Ablate KL constraint: compare GRPO with KL (β=0.01) vs GRPO-Filter (no KL) on BIRD Dev, tracking distribution drift and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MTIR-SQL performance scale with model size beyond 4B parameters, and are there diminishing returns to the multi-turn tool-integrated reasoning approach?
- Basis in paper: [inferred] The paper demonstrates results with only a 4B parameter model, showing it matches 7B and larger models, but does not test whether MTIR benefits increase, decrease, or plateau with larger base models.
- Why unresolved: The relationship between model capacity and the value of multi-turn execution feedback is unclear—larger models may have better single-shot reasoning, potentially reducing the need for iterative correction.
- What evidence would resolve it: Experiments applying MTIR-SQL to 7B, 13B, and larger models with controlled comparisons to single-turn baselines.

### Open Question 2
- Question: What is the optimal number of maximum interaction turns (T) that balances reasoning capability with training stability in multi-turn SQL execution?
- Basis in paper: [explicit] The ablation study (Figure 4, Section 4.3) states: "excessive turns (such as 6) may also result in training instability, occasionally causing reward saturation or collapse phenomena," and notes that "Max Turns = 1 demonstrates faster convergence" but with performance gaps.
- Why unresolved: The paper empirically tests 1, 3, and 6 turns but does not provide a principled method for determining the optimal T or explain the mechanisms causing instability at higher turns.
- What evidence would resolve it: Systematic analysis of the relationship between turn count, reward distribution dynamics, and the theoretical derivation of optimal turn limits based on task complexity metrics.

### Open Question 3
- Question: To what extent does the SQLite-specific execution feedback generalize to production database systems with different SQL dialects, query optimizers, and error handling?
- Basis in paper: [inferred] The paper states: "We use SQLite as the SQL executor to obtain execution feedback" (Section 4.1) and does not test on other database engines, yet production Text-to-SQL systems must handle PostgreSQL, MySQL, and enterprise databases with different syntax and execution behaviors.
- Why unresolved: SQLite's permissive syntax and error messages may create feedback patterns that don't transfer to stricter database systems, potentially causing the model to learn SQLite-specific error correction strategies.
- What evidence would resolve it: Cross-database evaluation of MTIR-SQL models trained on SQLite but tested on PostgreSQL, MySQL, and other dialects, with analysis of error type transferability.

## Limitations

- Trajectory filtering mechanism is only partially specified with incomplete details on filtering function F(x,y) and threshold τ
- Multi-turn reasoning loop behavior with empty tool results is underspecified
- Claim of KL removal improving multi-turn TIR stability lacks direct comparative evidence within the paper

## Confidence

**High Confidence:**
- Multi-turn execution feedback improves SQL generation accuracy over static approaches
- The three-component reward structure (format ±0.1, execution ±0.1, result ±1.0) is correctly implemented and contributes to learning

**Medium Confidence:**
- Trajectory filtering mitigates training instability and reward collapse
- Removing KL regularization is necessary for multi-turn TIR stability
- The 64.4% BIRD Dev and 84.6% SPIDER Dev results are reproducible with the described methodology

**Low Confidence:**
- The exact impact of KL removal versus trajectory filtering on overall performance
- The optimal filtering threshold and its sensitivity to different dataset characteristics

## Next Checks

1. Reproduce single-turn baseline: Train MTIR-SQL with Max Turns=1 and compare to multi-turn performance (Max Turns=3) on SPIDER Dev to validate the core multi-turn advantage claim.

2. Ablate trajectory filtering: Run controlled experiments training with and without the filtering mechanism, measuring both training stability (reward variance curves) and final execution accuracy on BIRD Dev.

3. Test KL removal impact: Implement GRPO with KL constraint (β=0.01) and compare against GRPO-Filter on BIRD Dev, tracking distribution drift metrics and final accuracy to isolate the effect of KL removal.