---
ver: rpa2
title: Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted
  versus Chain of Thought Large Language Models
arxiv_id: '2602.00380'
source_url: https://arxiv.org/abs/2602.00380
tags:
- language
- turkish
- binding
- local
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tests how well large language models (LLMs) capture
  Turkish reflexive binding, focusing on the interaction between structural locality
  and discourse factors. A balanced test set of 100 Turkish sentences, each containing
  the reflexives kendi or kendisi, was constructed to contrast local versus non-local
  antecedent readings.
---

# Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models

## Quick Facts
- arXiv ID: 2602.00380
- Source URL: https://arxiv.org/abs/2602.00380
- Reference count: 22
- One-line primary result: Turkish-adapted LLM strongly prefers local antecedents in reflexive binding; Chain-of-Thought model shows balanced locality bias, indicating model adaptation outweighs reasoning supervision.

## Executive Summary
This study examines how large language models handle Turkish reflexive binding, contrasting local and long-distance antecedent preferences. Using a balanced test set of 100 Turkish sentences with reflexives kendi and kendisi, the research evaluates two models: Trendyol-LLM-7B-base-v0.1, a Turkish-adapted system, and OpenAI’s reasoning-oriented o1 Mini. Results reveal that the Turkish model strongly favors local antecedents in roughly 70% of cases, while the Chain-of-Thought model distributes its choices nearly evenly between local and non-local readings. These findings suggest that model adaptation and training data composition play a more decisive role than reasoning supervision in shaping locality bias in anaphoric dependencies.

## Method Summary
The study constructs a balanced test set of 100 Turkish sentences, each containing the reflexives kendi or kendisi, to probe local versus non-local antecedent readings. Sentence-level perplexity and forced-choice continuations are used to assess binding preferences. Two models are tested: Trendyol-LLM-7B, a Turkish-adapted LLM, and OpenAI’s o1 Mini, a reasoning-oriented model. The design contrasts structural locality with discourse factors, revealing that the Turkish model exhibits a strong locality bias, while the Chain-of-Thought model shows a more balanced approach.

## Key Results
- Trendyol-LLM-7B favors local antecedents in ~70% of reflexive binding trials.
- o1 Mini distributes choices nearly evenly between local and long-distance antecedents.
- Model adaptation and training data composition outweigh reasoning supervision in shaping locality bias.

## Why This Works (Mechanism)
The divergence in reflexive binding behavior between models is driven by differences in training data composition and specialization. The Turkish-adapted model’s strong locality bias reflects exposure to Turkish-specific syntactic patterns, while the Chain-of-Thought model’s balanced performance suggests that reasoning supervision does not inherently promote sensitivity to structural locality. Instead, the data and adaptation strategies used during training appear to be the dominant factor in shaping anaphoric resolution preferences.

## Foundational Learning
- **Turkish reflexive binding**: Critical for understanding how languages handle anaphoric dependencies; quick check: review Turkish reflexive usage rules.
- **Locality bias in anaphora**: Refers to the preference for local antecedents; quick check: examine psycholinguistic literature on binding preferences.
- **Model adaptation vs. reasoning supervision**: Distinguishes the impact of specialized training from chain-of-thought prompting; quick check: compare performance of adapted vs. general models on language-specific tasks.
- **Perplexity and forced-choice evaluation**: Standard metrics for assessing language model behavior; quick check: verify consistency across multiple Turkish models.
- **Discourse-driven binding**: Context and salience can influence antecedent choice; quick check: test models on sentences with multiple plausible antecedents.

## Architecture Onboarding
**Component map**: Input sentence -> Turkish tokenizer -> Trendyol-LLM-7B/o1 Mini -> Perplexity/forced-choice output -> Antecedent classification
**Critical path**: Sentence encoding → reflexive resolution → antecedent selection → evaluation metric
**Design tradeoffs**: Specialized adaptation improves locality sensitivity but may reduce flexibility; reasoning supervision does not guarantee improved binding behavior
**Failure signatures**: Overemphasis on local antecedents; ignoring discourse context; inconsistent binding in structurally ambiguous cases
**First experiments**:
1. Test a third Turkish model with mixed (general + Turkish) training to isolate adaptation effects
2. Expand test set with discourse-rich contexts to probe locality-discourse interaction
3. Conduct qualitative error analysis to distinguish syntactic vs. discourse-based misbindings

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on comparison of only two models; broader generalization is uncertain
- Sample size (100 sentences) and language-specificity limit cross-linguistic applicability
- Design does not fully isolate discourse factors from syntactic constraints
- Possible confounds from pretraining corpus size, fine-tuning strategies, or architectural differences

## Confidence
- High: Core finding that model adaptation and training data composition impact locality bias more than reasoning supervision
- Medium: Claims about discourse factors being overridden by locality in the Turkish model
- Low: Broader inferences about reasoning supervision’s relationship to binding behavior

## Next Checks
1. Replicate experiment with a third Turkish model with mixed training to isolate specialization effects
2. Expand test set to include discourse-rich contexts (e.g., multiple potential antecedents, varying salience)
3. Conduct qualitative error analysis to distinguish syntactic parsing failures from discourse-level misinterpretations