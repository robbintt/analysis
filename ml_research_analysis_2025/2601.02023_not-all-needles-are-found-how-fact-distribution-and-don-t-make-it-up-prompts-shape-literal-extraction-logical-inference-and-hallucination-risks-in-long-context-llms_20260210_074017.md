---
ver: rpa2
title: 'Not All Needles Are Found: How Fact Distribution and Don''t Make It Up Prompts
  Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context
  LLMs'
arxiv_id: '2601.02023'
source_url: https://arxiv.org/abs/2601.02023
tags:
- arxiv
- context
- performance
- preprint
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how fact distribution, context length, and
  anti-hallucination prompts affect literal extraction, logical inference, and hallucination
  risk in long-context LLMs. An extended needle-in-a-haystack benchmark with four
  production models (Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, Deepseek-v3.2-chat)
  tests performance across variable context lengths and nine probabilistic fact distributions,
  comparing standard and anti-hallucination prompts.
---

# Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs

## Quick Facts
- **arXiv ID:** 2601.02023
- **Source URL:** https://arxiv.org/abs/2601.02023
- **Reference count:** 40
- **Key outcome:** Anti-hallucination prompts reduce hallucinations but can trigger over-refusal, sharply lowering extraction and inference accuracy—particularly for ChatGPT-5-mini.

## Executive Summary
This study evaluates how fact distribution, context length, and anti-hallucination prompts affect literal extraction, logical inference, and hallucination risk in long-context LLMs. An extended needle-in-a-haystack benchmark with four production models (Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, Deepseek-v3.2-chat) tests performance across variable context lengths and nine probabilistic fact distributions, comparing standard and anti-hallucination prompts. Results show that longer contexts alone do not guarantee better performance; some models degrade sharply when relevant facts are diluted or clustered. Anti-hallucination prompts reduce hallucinations but can trigger over-refusal, sharply lowering extraction and inference accuracy—particularly for ChatGPT-5-mini. Models like Gemini-2.5-flash and Deepseek-v3.2-chat remain robust across distributions, while others exhibit significant positional bias or distributional collapse. Effective context length and distributional robustness are critical for reliable LLM deployment in enterprise settings.

## Method Summary
The study uses an extended needle-in-a-haystack (NIAH) benchmark with four long-context LLMs. Protocol A varies context length (10%-100% of model max) and fact depth uniformly; Protocol B tests nine probabilistic distributions scattering 10 facts across 20 segments. A narrative corpus from La Comédie Humaine is processed via "Recursive Context Contraction" to generate variable-length contexts. Each configuration runs 30-question quizzes under Standard and Anti-Hallucination prompts, graded by an independent LLM judge. Metrics include Literal Extraction accuracy, Logical Inference accuracy, and Faithfulness (hallucination avoidance). Hyperparameters are standardized (Temp=0.0, Presence Penalty=0.3).

## Key Results
- Longer contexts alone do not guarantee better performance; some models degrade sharply when relevant facts are diluted or clustered.
- Anti-hallucination prompts reduce hallucinations but can trigger over-refusal, sharply lowering extraction and inference accuracy—particularly for ChatGPT-5-mini.
- Models like Gemini-2.5-flash and Deepseek-v3.2-chat remain robust across distributions, while others exhibit significant positional bias or distributional collapse.

## Why This Works (Mechanism)

### Mechanism 1: Positional Attention Bias ("Lost-in-the-Middle")
- Claim: LLMs exhibit U-shaped retrieval accuracy where information at context boundaries is better utilized than middle-positioned content.
- Mechanism: Attention mechanisms likely distribute computational priority unevenly across sequence positions, with primacy and recency receiving higher effective attention weights. As context length scales, middle tokens compete with exponentially more distractors.
- Core assumption: Attention dilution is positional rather than semantic; the effect persists regardless of content importance.
- Evidence anchors:
  - [abstract] "Performance varies with context length and strongly interacts with how information is distributed in real-world corpora."
  - [Section 4.3] "Claude-4.5-haiku exhibits the most pronounced 'U-shaped' performance curve... accuracy drops to nearly 50% between the 20% and 60% depth intervals before recovering toward the end."
  - [corpus] Related work (Liu et al. "Lost in the Middle") confirms positional biases worsen with context length.
- Break condition: If attention mechanisms are modified to use position-agnostic scoring (e.g., retrieval-augmented attention), U-shaped curves should flatten.

### Mechanism 2: Safety Tax from Anti-Hallucination Prompting
- Claim: Explicit "Don't Make It Up" instructions reduce hallucinations but induce over-refusal, disproportionately harming extraction and inference accuracy at long contexts.
- Mechanism: Anti-hallucination prompts raise internal confidence thresholds for output generation. At long contexts, attention degradation weakens internal representations, causing legitimate evidence to fall below threshold—triggering refusal rather than correct output.
- Core assumption: Models have measurable internal confidence signals that anti-hallucination prompts calibrate upward; the paper does not directly measure this mechanism.
- Evidence anchors:
  - [Section 4.5] "ChatGPT-5-mini exhibits severe degradation in Literal Extraction and Logical Inference... the model frequently defaults to a refusal response when the needle is buried deep in the context."
  - [Section 5] "Under long-context pressure, internal representations weaken, and accurate information may fail to meet these thresholds."
  - [corpus] Related work on over-refusal (OR-Bench, Cui et al.) corroborates safety calibration trade-offs, but corpus evidence specific to long-context safety tax is limited.
- Break condition: If models expose confidence scores, over-refusal should correlate with systematically higher rejection rates at degraded attention positions.

### Mechanism 3: Distributional Collapse Under Non-Uniform Fact Placement
- Claim: Some models fail catastrophically when relevant facts are clustered (e.g., Normal distribution) rather than uniformly dispersed, even when total evidence is identical.
- Mechanism: Dense fact clusters may trigger attention sparsity mechanisms or safety filters that misinterpret concentrated evidence as noise or redundancy. Models with rigid attention allocation cannot adapt to variable information density.
- Core assumption: The failure is attention-architecture related rather than simply a token-count issue; the paper observes the phenomenon but does not isolate the internal cause.
- Evidence anchors:
  - [Section 4.6] "ChatGPT-5-mini... under 'Normal' and 'Lorentzian' distributions... its performance collapses to 0% in Literal Extraction and Logical Inference under Anti-Hallucination prompts."
  - [Section C.3] "This suggests that the model's safety filters may aggressively misinterpret dense clusters of relevant evidence as redundant noise."
  - [corpus] Corpus evidence on distributional collapse specifically is weak; related NIAH extensions focus on sequential or multiple needles but not statistical distributions.
- Break condition: If attention can dynamically upweight high-density regions, collapse should diminish; if safety filters are the cause, disabling them should recover performance.

## Foundational Learning

- **Needle-in-a-Haystack (NIAH) Testing Paradigm**
  - Why needed here: This is the core evaluation framework. Without understanding NIAH, you cannot interpret the paper's metrics or reproduce experiments.
  - Quick check question: Can you explain why single-fact NIAH tests may overestimate real-world performance compared to distributed-fact testing?

- **U-Shaped Memory / Positional Bias**
  - Why needed here: The paper's positional analysis (Figure 3) builds directly on this documented phenomenon; understanding it explains why depth matters independently of length.
  - Quick check question: If a model scores 98% at 10% depth and 95% at 90% depth, but 50% at 50% depth, what phenomenon is this?

- **Parametric vs. Contextual Knowledge**
  - Why needed here: The paper discusses models reverting to parametric priors when contextual evidence is weak or dispersed; this distinction underlies hallucination risk.
  - Quick check question: When a model answers from training data rather than provided context, which knowledge source dominates?

## Architecture Onboarding

- **Component map:**
  - Haystack generator -> Fact injector -> Prompt templates -> Evaluation loop -> Metrics calculator

- **Critical path:**
  1. Select context length (10%-100% of model max)
  2. Select fact depth or distribution
  3. Inject facts into haystack
  4. Apply prompt condition (Standard or AH)
  5. Run model with standardized hyperparameters (temp=0.0, presence_penalty=0.3)
  6. Grade responses via independent LLM judge grading (Appendix B)
  7. Compute accuracy per metric and visualize as heatmaps/radar charts

- **Design tradeoffs:**
  - Narrative corpus vs. synthetic text: More realistic but less controllable
  - 30 questions per quiz: Sufficient statistical power without excessive cost
  - LLM judge vs. human grading: Faster but introduces judge-model dependencies

- **Failure signatures:**
  - **Performance cliff**: Sharp accuracy drop at specific context thresholds (ChatGPT-5-mini at ~100k)
  - **U-shaped degradation**: Middle positions systematically worse (Claude-4.5-haiku)
  - **Over-refusal zone**: High faithfulness + low extraction under AH prompts (ChatGPT-5-mini)
  - **Distributional collapse**: Zero retrieval under central-tendency distributions

- **First 3 experiments:**
  1. **Baseline sweep**: Run Protocol A (uniform depth/length sweep) on your target model to map its failure frontiers before any optimization.
  2. **Safety tax quantification**: Compare Standard vs. AH prompts at maximum context length; calculate Δ accuracy to quantify over-refusal cost.
  3. **Distributional stress test**: Run Protocol B with at least Uniform, Normal, and Exponential distributions to check for distributional collapse before deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise magnitude of the "safety tax"—the degradation in extraction and inference accuracy—introduced by anti-hallucination prompts across different model architectures?
- Basis in paper: [explicit] The authors explicitly ask, "how much recall and inference accuracy is sacrificed for lower hallucination risk?" and define this trade-off as the "safety tax."
- Why unresolved: The study quantifies this tax for four specific models, finding it disproportionately severe for some (e.g., ChatGPT-5-mini) but negligible for others, leaving the generalized impact across the broader model landscape undefined.
- What evidence would resolve it: A cross-model benchmark that varies constraint intensities (soft warnings vs. hard refusals) to map the Pareto frontier between faithfulness and reasoning capability.

### Open Question 2
- Question: How do robust long-context LLMs compare to optimized Retrieval-Augmented Generation (RAG) pipelines in terms of cost, latency, and accuracy for enterprise-scale retrieval?
- Basis in paper: [explicit] The Discussion section states that "future benchmarking should directly compare these results, along with cost and response time, against mature RAG solutions."
- Why unresolved: The paper focuses exclusively on long-context extraction without external retrieval, leaving the practical trade-off between "paste-all" approaches and RAG architectures unmeasured.
- What evidence would resolve it: A controlled evaluation using identical corpora and query sets to measure the dollar cost, latency, and accuracy delta between Gemini-2.5-flash (long-context) and a tuned RAG implementation.

### Open Question 3
- Question: Does "distributional collapse"—where models fail when evidence is clustered—persist in specialized, high-stakes domains like legal or healthcare documentation?
- Basis in paper: [inferred] The authors note in the Limitations section that "LLM behavior may shift significantly in specialized domains... where structural and linguistic patterns differ" from the literary corpus used.
- Why unresolved: The observed fragility in models like Claude-4.5-haiku was tested only on narrative fiction; it is unknown if the dense, technical structure of legal or medical texts exacerbates or mitigates this attention failure.
- What evidence would resolve it: Replicating the probabilistic distribution protocol using domain-specific corpora (e.g., case law or clinical notes) to verify if the "lost-in-the-middle" and clustering effects remain robust.

## Limitations
- Hallucination Risk Measurement Granularity: The study uses a binary "hallucination" label (correct/incorrect) without distinguishing between confabulation types or confidence levels, limiting fine-grained risk assessment.
- Model Internal Mechanisms: The paper attributes performance patterns to attention positional bias and safety thresholds but does not directly measure attention weights or internal confidence scores, making mechanistic claims inferential rather than empirical.
- Distributional Robustness Generalizability: The nine tested distributions may not fully capture real-world information density patterns; enterprise contexts may exhibit different clustering characteristics that could trigger distributional collapse differently.

## Confidence
- **High Confidence**: Positional attention bias (U-shaped curves) and over-refusal under anti-hallucination prompts are well-documented phenomena with strong empirical support across multiple models and conditions.
- **Medium Confidence**: The proposed mechanisms explaining distributional collapse (attention sparsity vs. safety filters) are plausible but not directly validated; the paper observes the phenomenon but cannot definitively attribute causation.
- **Low Confidence**: Claims about specific internal confidence threshold calibration under AH prompts lack direct measurement; the mechanism is inferred from output patterns rather than observed.

## Next Checks
1. **Attention Weight Validation**: Run attention visualization on Claude-4.5-haiku across middle vs. boundary positions to empirically confirm positional attention bias explains the U-shaped performance curve.
2. **Confidence Score Correlation**: Compare model confidence scores (if exposed) against refusal rates under AH prompts to test whether over-refusal correlates with systematically higher rejection thresholds.
3. **Distributional Robustness Expansion**: Test additional real-world information distributions (Zipfian, power-law clustered) beyond the nine synthetic distributions to assess whether distributional collapse generalizes to enterprise data patterns.