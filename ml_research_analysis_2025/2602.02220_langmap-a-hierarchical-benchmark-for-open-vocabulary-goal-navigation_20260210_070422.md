---
ver: rpa2
title: 'LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation'
arxiv_id: '2602.02220'
source_url: https://arxiv.org/abs/2602.02220
tags:
- navigation
- object
- region
- semantic
- langmap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HieraNav, a multi-granularity open-vocabulary
  goal navigation task that unifies goals across four semantic levels: scene, room,
  region, and instance. To support this, the authors present LangMap, a large-scale
  benchmark built on real-world 3D indoor scans with comprehensive human-verified
  annotations, including region labels and discriminative descriptions at both region
  and instance levels.'
---

# LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation

## Quick Facts
- arXiv ID: 2602.02220
- Source URL: https://arxiv.org/abs/2602.02220
- Reference count: 40
- Primary result: 23.8% higher discriminative accuracy than GOAT-Bench using 4× fewer words

## Executive Summary
LangMap introduces HieraNav, a multi-granularity open-vocabulary goal navigation task that unifies goals across four semantic levels: scene, room, region, and instance. The benchmark is built on real-world 3D indoor scans with comprehensive human-verified annotations, covering 414 object categories and providing over 18K navigation tasks. Evaluations reveal that richer context and memory improve navigation success, but challenges remain for long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion. The benchmark establishes a rigorous testbed for advancing language-driven embodied navigation.

## Method Summary
LangMap is built on 36 HM3D-Sem validation scenes with human-verified annotations including region labels and discriminative descriptions at both region and instance levels. The benchmark uses the Stretch robot with RGB-D perception and odometry input. Agents navigate using MOVE FORWARD (0.25m), TURN (30°), LOOK (30°), and STOP actions within 500 steps. Success is defined as stopping within 1m of the target. The evaluation uses Success Rate (SR) and SPL metrics, with Sequence Success Rate (SeqSR) for multi-goal tasks.

## Key Results
- LangMap achieves 23.8% higher discriminative accuracy than GOAT-Bench using 4× fewer words
- Memory-based agents (MTU3D, 3D-Mem) outperform pure VLM approaches by 7.7% and 12.7% respectively
- Concise descriptions (5.3 words) are harder to ground than detailed ones (21 words), with 16.3% lower success rate
- Multi-goal completion remains challenging, with SeqSR significantly lower than per-goal SR

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Semantic Filtering
Structuring navigation goals into explicit semantic levels forces agents to switch strategies from broad exploration to fine-grained visual grounding. Scene-level queries trigger high-level exploration while instance-level queries require fine-grained visual-linguistic matching.

### Mechanism 2: Contrastive Grounding for Disambiguation
Annotation quality improves significantly when generated via a contrastive protocol where annotators must distinguish targets from hardest negatives (same-category instances) rather than describing them in isolation.

### Mechanism 3: Spatial Context Accumulation (Memory)
Navigation success in multi-goal or context-dependent tasks correlates with explicit accumulation of semantic observations over time, enabling agents to maintain state and reason about object-context relationships.

## Foundational Learning

- **Open-Vocabulary Visual Grounding:** Agents must map arbitrary natural language phrases to visual regions. Quick check: Can your visual encoder distinguish "a mug" from "a mug with a logo" without retraining?
- **Spatial-Temporal Representation (Mapping):** Understanding room and region contexts requires persistent mapping. Quick check: Does your system maintain a persistent map of room boundaries?
- **Compositional Language Understanding:** Instance-level descriptions are compositional (e.g., "bluey blanket"). Quick check: Does your language encoder parse subject-relation-object structures?

## Architecture Onboarding

- **Component map:** Perception (RGB-D Encoder + Open-Vocab Detector) -> Memory (Spatial Map/Object Graph) -> Policy (Global Planner + Local Goal Matcher)
- **Critical path:** The integrity of the Goal Matcher is crucial; the system fails if similarity functions misalign instructions like "blue cup" with "red cup"
- **Design tradeoffs:**
  - Concise vs. Detailed Descriptions: Concise (5.3 words) are realistic but harder for VLMs to ground
  - VLM vs. SLAM: Pure VLMs are fast but black-box; Modular SLAM+VLM is interpretable but computationally expensive
- **Failure signatures:**
  - Ambiguity Loop: Agent oscillates between similar objects due to unresolved discriminative features
  - Semantic Drift: Agent forgets sub-goal context in multi-goal tasks
  - Long-tail Blindness: High success on common categories but near-zero on rare ones
- **First 3 experiments:**
  1. **Sanity Check (Oracle):** Run with ground-truth object coordinates to verify navigation stack
  2. **Annotation Ablation:** Compare performance using Concise vs. Detailed descriptions
  3. **Memory Stress Test:** Evaluate memory-based agents on Multi-Goal vs. Single-Goal splits

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on 36 HM3D-Sem validation scenes creates a relatively small evaluation set
- Contrastive annotation protocol may not generalize to environments with less visual distinctiveness
- Memory-based agents show improved performance but at significant computational cost (~2.5 minutes per task)
- Task definitions assume complete semantic knowledge of the environment

## Confidence

- **High Confidence:** Hierarchical task structure and annotation methodology are well-specified and reproducible; performance trends showing memory benefits are supported
- **Medium Confidence:** 23.8% higher discriminative accuracy claim based on internal methodology requires independent verification
- **Low Confidence:** Scalability claims to truly open-vocabulary scenarios remain theoretical

## Next Checks

1. **Annotation Quality Validation:** Re-run contrastive annotation protocol with different annotator pool to verify 23.8% discriminative accuracy improvement is reproducible

2. **Memory Efficiency Benchmark:** Profile 3D-Mem and MTU3D to quantify computational overhead and identify optimization opportunities

3. **Out-of-Distribution Generalization:** Test agents trained on LangMap on held-out scenes or categories to measure true open-vocabulary performance beyond 414 annotated categories