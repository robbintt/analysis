---
ver: rpa2
title: What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language
  Models
arxiv_id: '2511.19324'
source_url: https://arxiv.org/abs/2511.19324
tags:
- retrieval
- language
- document
- translation
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how translation, multilingual dense retrieval,
  contrastive learning, and re-ranking affect cross-lingual information retrieval.
  Lexical matching via BM25 is highly ineffective, particularly for non-English and
  cross-script language pairs, while dense multilingual encoders achieve substantially
  higher recall by relying on semantic alignment rather than lexical overlap.
---

# What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models

## Quick Facts
- arXiv ID: 2511.19324
- Source URL: https://arxiv.org/abs/2511.19324
- Reference count: 40
- Primary result: Dense multilingual encoders achieve substantially higher cross-lingual recall than lexical matching by relying on semantic alignment.

## Executive Summary
This study evaluates translation, multilingual dense retrieval, contrastive learning, and re-ranking for cross-lingual information retrieval. The authors find that lexical matching via BM25 is highly ineffective for cross-script and non-English language pairs, while dense multilingual encoders achieve substantially higher recall by relying on semantic alignment rather than lexical overlap. Contrastive fine-tuning notably improves weaker encoders like XLM-R, bringing performance close to models already optimized for cross-lingual retrieval, but has little effect on already-strong encoders like Nomic. Re-ranking with cross-encoders improves results mainly in difficult language pairs and when trained with hard negatives.

## Method Summary
The authors evaluate five multilingual encoders (XLM-R, LaBSE, multilingual-E5, Nomic, mmBERT) on three benchmark CLIR datasets (CLIRMatrix, mMARCO, Large-Scale CLIR) using bi-encoder retrieval with mean pooling and cosine similarity. They apply contrastive fine-tuning at word, phrase, and query-document levels using InfoNCE loss, and cross-encoder re-ranking trained with easy versus hard negatives. Document translation via NLLB-200-distilled-1.3B is evaluated as an alternative to native multilingual retrieval. Approximate nearest neighbor search (HNSW) is tested for efficiency but found ineffective on small corpora.

## Key Results
- Dense multilingual encoders outperform lexical matching by 40-90+ percentage points in Recall@100 across all language pairs.
- Contrastive fine-tuning improves weak encoders like XLM-R from near-zero to 20-90% recall, but yields minimal gains for already-strong encoders like Nomic.
- Cross-encoder re-ranking with hard negatives boosts difficult pairs from 17% to 83% Recall@10, while easy negatives fail to improve beyond BM25 baseline.
- Document translation adds noise and latency without improving retrieval performance over native multilingual dense retrieval.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense multilingual encoders outperform lexical matching for cross-lingual retrieval by relying on semantic alignment rather than surface-form overlap.
- Mechanism: Encoders map queries and documents from different languages into a shared vector space where cosine similarity reflects semantic relatedness. This bypasses the need for shared vocabulary.
- Core assumption: The pretrained encoder has already learned reasonable cross-lingual alignment during pretraining.
- Evidence anchors:
  - [abstract] "dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation"
  - [section 4.2] BM25 Recall@100 near zero for non-English pairs; Nomic achieves 82.6–98.3% across datasets
  - [corpus] Consistent with prior work (Litschko et al., 2022) showing dense encoders outperform lexical methods in CLIR
- Break condition: Encoders with weak multilingual alignment (e.g., unmodified XLM-R at 1.0–4.3% baseline) fail to retrieve cross-lingually until explicitly fine-tuned.

### Mechanism 2
- Claim: Contrastive fine-tuning substantially improves weak encoders but yields diminishing returns for already-aligned models.
- Mechanism: Query–document level contrastive learning aligns representations by pulling relevant pairs closer and pushing irrelevant pairs apart in embedding space. Phrase-level alignment provides intermediate gains.
- Core assumption: Supervision signal quality (relevance labels, parallel data) is consistent across languages.
- Evidence anchors:
  - [abstract] "Contrastive fine-tuning notably improves weaker encoders like XLM-R, bringing performance close to models already optimized for cross-lingual retrieval, but has little effect on already-strong encoders like Nomic"
  - [section 4.2] XLM-R improves from 1.0% to 21.2% on CLIRMatrix with QD-level training; Nomic gains only ~4%
  - [corpus] Related work (Xiong et al., 2020) confirms hard negative contrastive learning improves dense retrieval
- Break condition: Word-level alignment shows negligible effect—likely because isolated word embeddings lack contextual disambiguation needed for retrieval.

### Mechanism 3
- Claim: Cross-encoder re-ranking is effective for difficult language pairs, but only when trained with hard negatives.
- Mechanism: Cross-encoders jointly encode query–document pairs, enabling fine-grained token interactions. Hard negatives (plausible but incorrect candidates) teach the model to discriminate subtle relevance distinctions.
- Core assumption: Hard negatives are available and linguistically representative of failure modes.
- Evidence anchors:
  - [abstract] "re-ranking with cross-encoders improves results mainly in difficult language pairs and when trained with hard negatives"
  - [section 4.3] XLM-R with hard negatives reaches 83.4% Recall@10 on mMARCO vs. 17.3% BM25 baseline; easy negatives fail to surpass baseline
  - [corpus] Limited corpus corroboration—prior work focuses on monolingual re-ranking (Nogueira & Cho, 2019)
- Break condition: Easy negatives provide insufficient gradient signal; re-ranking adds latency without effectiveness gains when candidate quality is already high.

## Foundational Learning

- **Bi-encoder vs. Cross-encoder architectures**
  - Why needed here: The paper uses bi-encoders for first-stage retrieval (efficiency) and cross-encoders for re-ranking (effectiveness). Understanding this tradeoff is essential for system design.
  - Quick check question: Can you explain why bi-encoders scale to millions of documents while cross-encoders do not?

- **Contrastive learning objectives (InfoNCE, in-batch negatives)**
  - Why needed here: Query–document fine-tuning uses InfoNCE loss with in-batch negatives to align representations. Understanding temperature scaling and negative sampling is critical for reproducing results.
  - Quick check question: What happens to gradient signal if all negatives in a batch are trivially easy to distinguish?

- **Multilingual embedding alignment**
  - Why needed here: Performance depends on whether the encoder has learned language-agnostic representations. The paper shows Nomic is well-aligned; XLM-R is not without fine-tuning.
  - Quick check question: How would you diagnose whether an encoder has strong cross-lingual alignment before retrieval experiments?

## Architecture Onboarding

- **Component map:**
  - First-stage retrieval: Bi-encoder (Nomic, XLM-R, LaBSE, multilingual-E5, mmBERT) → precomputed document embeddings → cosine similarity → top-100 candidates
  - Optional: BM25 lexical baseline (requires document translation for cross-lingual settings)
  - Re-ranking: Cross-encoder over top-100 candidates → relevance scores → re-ranked list
  - Optional: ANN indexing (HNSW) for large corpora—not beneficial below ~26k documents

- **Critical path:**
  1. Select encoder based on initial alignment quality (Nomic recommended for strong baseline; XLM-R for fine-tuning experiments)
  2. Precompute and cache document embeddings (mean pooling, L2-normalized, max 512 tokens)
  3. Retrieve via cosine similarity; evaluate Recall@100
  4. If first-stage is weak: apply query–document contrastive fine-tuning on domain-relevant data
  5. If difficult pairs persist: add cross-encoder re-ranking trained with hard negatives

- **Design tradeoffs:**
  - Strong pretrained encoder (Nomic) vs. fine-tuned weaker encoder (XLM-R): Nomic works out-of-box but offers limited fine-tuning gains; XLM-R requires supervision but can match performance
  - Re-ranking latency vs. accuracy: Cross-encoders add ~10× inference cost; justify only for challenging language pairs
  - Translation pipeline vs. native multilingual: Translation adds noise and latency; dense encoders make it redundant

- **Failure signatures:**
  - Near-zero recall on cross-script pairs → encoder lacks multilingual alignment; switch models or fine-tune
  - Re-ranking degrades performance → likely trained on easy negatives; acquire hard negatives from first-stage retrieval failures
  - ANN slower than exact search → corpus too small (<50k documents); use exact cosine similarity
  - Disproportionate retrieval of query-language documents → language bias in encoder; contrastive fine-tuning reduces this

- **First 3 experiments:**
  1. **Baseline retrieval comparison:** Run all five encoders on your target language pairs. Measure Recall@100. Identify whether your encoder is "strong" (Nomic-like, >80%) or "weak" (XLM-R-like, <10%).
  2. **Contrastive fine-tuning ablation:** If using a weak encoder, fine-tune at phrase-level then query–document level on parallel/relevance data. Measure gains per granularity.
  3. **Hard negative re-ranking:** For top failure language pairs, train cross-encoder with hard negatives extracted from first-stage retrieval. Compare Recall@10 and nDCG@100 against BM25 and dense baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does approximate nearest neighbour (ANN) search trade off efficiency and effectiveness in cross-lingual retrieval when applied to collections significantly larger than the 26k documents tested?
- Basis in paper: [explicit] The authors state their conclusion that ANN does not provide speed benefits "is unlikely to hold at larger scales" given the overhead of graph traversal on small datasets.
- Why unresolved: The experiments used small corpora (7k–26k docs) where exact search was faster due to optimized vectorization.
- What evidence would resolve it: Benchmarking ANN latency versus Recall@100 on multilingual corpora exceeding 1 million vectors.

### Open Question 2
- Question: How does the specific composition of hard negatives in training data systematically influence cross-encoder re-ranking performance across diverse language pairs?
- Basis in paper: [explicit] The paper notes that the analysis was "constrained by the availability of hard negatives in only two datasets," preventing a full assessment of how negative sampling interacts with multilingual alignment.
- Why unresolved: Re-ranking effectiveness varied significantly based on negative sampling (easy vs. hard), but the cross-lingual interaction was not exhaustively explored.
- What evidence would resolve it: A controlled ablation study varying hard negative ratios across a wider set of typologically distant languages.

### Open Question 3
- Question: Can current multilingual dense retrieval architectures maintain robust semantic alignment for typologically distant, low-resource languages absent from pretraining data?
- Basis in paper: [inferred] The authors acknowledge the "limited number of typologically distant and low-resource languages" in the datasets restricts the generalisability of their findings to under-represented linguistic settings.
- Why unresolved: While strong models like Nomic reduced bias, performance still correlated with linguistic similarity, suggesting potential fragility for truly low-resource pairs.
- What evidence would resolve it: Evaluation on a dataset specifically constructed with low-resource, cross-script language pairs not seen during pretraining.

## Limitations
- Hard negative selection protocol is not specified, which is critical for reproducing the re-ranking gains reported.
- Approximate nearest neighbor search was ineffective on small corpora (<26k documents) due to index overhead.
- Bias analysis lacks per-pair error diagnostics to identify systematic failure modes.

## Confidence
- **High confidence**: Dense multilingual encoders outperform lexical matching for cross-lingual retrieval (supported by consistent Recall@100 gaps across three datasets).
- **Medium confidence**: Contrastive fine-tuning benefits weaker encoders more than strong ones (mechanism is sound, but gains depend on encoder alignment quality and data quality).
- **Medium confidence**: Cross-encoder re-ranking with hard negatives improves difficult pairs (effect is demonstrated, but hard negative selection criteria are underspecified).

## Next Checks
1. **Hard negative selection experiment**: Implement multiple hard negative selection strategies (e.g., false positives from top-100, semantic similarity-based sampling) and measure their impact on re-ranking performance across language pairs.
2. **Cross-encoder latency benchmarking**: Profile cross-encoder re-ranking throughput (queries/second) on a held-out set and compare against first-stage dense retrieval to quantify the practical cost of accuracy gains.
3. **Encoder alignment diagnosis**: Before retrieval experiments, run a diagnostic task (e.g., Tatoeba sentence retrieval) to measure cross-lingual alignment strength of each encoder and correlate with downstream CLIR performance.