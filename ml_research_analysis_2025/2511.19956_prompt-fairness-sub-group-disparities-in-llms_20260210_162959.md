---
ver: rpa2
title: 'Prompt Fairness: Sub-group Disparities in LLMs'
arxiv_id: '2511.19956'
source_url: https://arxiv.org/abs/2511.19956
tags:
- prompt
- across
- demographic
- latexit
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of prompt fairness in large language
  models (LLMs), where semantically equivalent prompts phrased in the linguistic styles
  of different demographic subgroups can elicit inconsistent responses. The authors
  propose information-theoretic metrics to quantify two types of bias: subgroup sensitivity
  (variability within a subgroup) and cross-group consistency (variability across
  subgroups).'
---

# Prompt Fairness: Sub-group Disparities in LLMs

## Quick Facts
- arXiv ID: 2511.19956
- Source URL: https://arxiv.org/abs/2511.19956
- Authors: Meiyu Zhong; Noel Teku; Ravi Tandon
- Reference count: 35
- Primary result: Semantic equivalence of prompts across demographic subgroups is not preserved by LLMs, leading to measurable fairness disparities

## Executive Summary
This paper addresses prompt fairness in LLMs by quantifying how semantically equivalent prompts phrased in different demographic linguistic styles elicit inconsistent responses. The authors introduce information-theoretic metrics to measure subgroup sensitivity (variability within demographic groups) and cross-group consistency (variability across groups). Through experiments on Adult Income and BOLD datasets, they demonstrate that certain subgroups exhibit higher sensitivity and lower accuracy, with Black females showing the most pronounced disparities. The proposed mitigation strategies—majority voting across multiple prompt generations and prompt neutralization to remove demographic cues—consistently reduce these divergences while maintaining or improving overall accuracy.

## Method Summary
The method involves generating demographic-conditioned paraphrases of prompts using LLaMA-2-13B-chat-hf, then evaluating them with OpenThinker2-7B to measure response consistency. Subgroup sensitivity is quantified using conditional entropy H(Ŷ|X,t,g), while cross-group consistency is measured via symmetric KL divergence and Jensen-Shannon divergence between response distributions. Mitigation strategies include majority voting across m paraphrase variants and prompt neutralization to remove demographic cues. The pipeline embeds LLM outputs using all-mpnet-base-v2, clusters them with k-means (k=4), and computes divergence matrices to assess fairness improvements.

## Key Results
- Cross-group divergence values reach up to 0.28 before mitigation, with Black females showing highest sensitivity
- After applying neutralization and majority voting, divergences consistently decrease, with largest gap reduced to 0.22 and many falling to 0.17 or below
- Subgroup sensitivity H(Ŷ|X,t,g) drops from 0.42 to 0.23 after mitigation on Adult dataset
- Classification accuracy remains stable or improves after mitigation while response consistency increases

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Subgroup Sensitivity Quantification
- Claim: Conditional entropy H(Ŷ|X, t, g) captures variability in LLM responses to subgroup-specific prompt stylizations.
- Mechanism: Measures uncertainty in model outputs conditioned on prompts from a fixed subgroup g for the same task t, isolating stylistic sensitivity from task difficulty.
- Core assumption: Stylistic diversity within subgroups can be meaningfully captured through prompt paraphrasing by an LLM.
- Evidence anchors: Abstract states "subgroup sensitivity—the variability of responses within a subgroup"; Definition 1 formalizes H(Ŷ|X, t, g) as conditional entropy.

### Mechanism 2: Cross-Group Divergence via Distributional Distance
- Claim: Symmetric KL divergence and Jensen-Shannon divergence between response distributions P(Ŷ|t, g) and P(Ŷ|t, g') quantify behavioral disparity across demographic subgroups.
- Mechanism: Estimates probability distributions over semantic clusters of LLM outputs, then computes pairwise divergences. Large D_{g,g'} indicates systematically different outputs for same task depending on subgroup authorship style.
- Core assumption: Semantic embedding space + k-means clustering reliably discretizes response semantics into comparable distributions.
- Evidence anchors: Abstract reports "cross-group divergence values reach 0.28...after applying our neutralization and multi-generation strategy...many distances falling to 0.17 or below."

### Mechanism 3: Majority Voting Dampens Stylistic Variance
- Claim: Aggregating predictions across m paraphrase variants via majority vote reduces subgroup sensitivity by canceling style-dependent output fluctuations.
- Mechanism: Generate m stylistic variants of a prompt, collect LLM responses, then select majority prediction. Suppresses spurious variations tied to demographic markers while preserving task-consistent labels.
- Core assumption: Task-relevant signal is stable across paraphrases; noise from stylistic markers is zero-mean and cancels out.
- Evidence anchors: Abstract states "majority voting across multiple generations...improve response stability and enhance fairness"; Fig 3 shows cross-group divergence reduced from 0.16 max to ~0.09 after majority voting.

## Foundational Learning

- **Concept: Conditional Entropy H(Y|X)**
  - Why needed here: Core mathematical tool for quantifying subgroup sensitivity—measures remaining uncertainty in outputs given inputs from a specific group.
  - Quick check question: If H(Ŷ|X, t, g) = 0 for all groups g, what does that imply about the LLM's behavior?

- **Concept: f-Divergences (KL, JS, Total Variation)**
  - Why needed here: Used to measure cross-group consistency by comparing output distributions; JS divergence is symmetric and bounded, making it suitable for pairwise comparison.
  - Quick check question: Why might symmetric KL be preferred over standard KL for cross-group comparison?

- **Concept: Semantic Embedding Clustering**
  - Why needed here: The pipeline embeds LLM outputs, clusters them, and treats cluster assignments as discrete response categories for divergence computation.
  - Quick check question: What happens to divergence estimates if k-means clustering produces highly imbalanced clusters?

## Architecture Onboarding

- **Component map:** Paraphrase Generator (LLaMA-13B) → Prompt Neutralizer → Target LLM (OpenThinker-7B) → Semantic Embedder (all-mpnet-base-v2) → Clustering Module (k-means, k=4) → Divergence Computer → Majority Voting Aggregator

- **Critical path:** Input prompt → Paraphrase generation (m variants) → [Optional: Neutralization] → Target LLM inference → Embedding → Clustering → Distribution estimation → Divergence computation → Fairness assessment

- **Design tradeoffs:** More paraphrases (higher m) improves variance reduction but increases inference cost linearly; higher k (more clusters) captures finer semantic distinctions but requires more samples per subgroup for stable distribution estimates; neutralization removes bias sources but may also remove task-relevant context if over-aggressive.

- **Failure signatures:** High divergence persists after mitigation → systematic (not random) bias; neutralization likely insufficient; sensitivity varies wildly across random seeds → clustering instability; increase n_init or use deterministic embedding; refusal responses → paraphrase generator produces prompts that trigger safety filters, contaminating fairness metrics.

- **First 3 experiments:** 1) Baseline divergence measurement: Compute cross-group D_{g,g'} for all subgroup pairs on Adult dataset without mitigation; verify Black female subgroup shows highest sensitivity (paper reports 0.28). 2) Ablation on paraphrase count m: Test m ∈ {1, 3, 5, 10} with majority voting; plot sensitivity vs. m to find cost-quality inflection point. 3) Neutralization effectiveness test: Compare divergence before/after neutralization on BOLD dataset; verify reduction from ~0.16 to ≤0.09 as reported in Fig 4.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do prompt fairness disparities persist or diminish when evaluated on larger proprietary models (e.g., GPT-4, Claude 3) compared to the open-weight models used in this study? The experimental setup is restricted to specific open-source models, leaving the behavior of state-of-the-art commercial models unexplored.

- **Open Question 2:** Does the LLM-based paraphrasing pipeline generate authentic linguistic variations or stereotypical caricatures of demographic subgroups? The methodology risks eliciting heuristic-based stereotypes rather than natural, validated dialectal syntax, potentially measuring the model's reaction to artificial text rather than genuine linguistic diversity.

- **Open Question 3:** Does the application of prompt neutralization or majority voting result in a trade-off with overall task accuracy? While the paper highlights improved consistency, it does not explicitly report the change in ground-truth accuracy post-mitigation, raising questions about whether fairness gains come at accuracy costs.

- **Open Question 4:** How does subgroup sensitivity manifest in intersectional groups beyond the binary race and gender combinations studied? The analysis is limited to four subgroups despite the existence of more complex intersectional identities, potentially missing important patterns in other demographic combinations.

## Limitations

- The core metrics rely on k-means clustering of LLM outputs, with semantic granularity not validated and results potentially sensitive to clustering stability across random seeds.
- The BOLD dataset used for qualitative analysis lacks ground truth, limiting quantitative fairness claims, and the paper does not report variance across multiple runs or hyperparameter sensitivity.
- The approach's effectiveness for other protected attributes or intersectional subgroups beyond the four tested combinations remains unproven, despite the observation that Black females show the most pronounced disparities.

## Confidence

**High Confidence:** The mechanism of majority voting reducing variance is well-established and directly supported by empirical results showing cross-group divergence reduction from 0.16 to ~0.09.

**Medium Confidence:** The information-theoretic framing using conditional entropy and f-divergences is mathematically sound, but the specific implementation choices (clustering granularity, embedding space selection) lack comprehensive ablation or sensitivity analysis.

**Low Confidence:** Claims about the Prompt Neutralizer's effectiveness are based on limited qualitative examples without systematic evaluation of what stylistic features are actually being removed or whether neutralization introduces other biases.

## Next Checks

1. **Clustering Stability Test:** Run the full pipeline with 5 different random seeds for k-means initialization and report variance in cross-group divergence scores. Check if the reported improvements persist across seeds.

2. **Paraphrase Quantity Sensitivity:** Systematically vary the number of paraphrase variants m from 1 to 10 with majority voting, measuring both subgroup sensitivity reduction and computational cost to identify the optimal tradeoff point.

3. **Neutralization Ablation:** Compare cross-group divergence with three variants: (a) no neutralization, (b) prompt neutralization only, (c) paraphrase neutralization only, to isolate which mitigation strategy contributes most to the observed improvements.