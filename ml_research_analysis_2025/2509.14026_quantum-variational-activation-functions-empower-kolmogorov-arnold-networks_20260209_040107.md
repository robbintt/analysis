---
ver: rpa2
title: Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks
arxiv_id: '2509.14026'
source_url: https://arxiv.org/abs/2509.14026
tags:
- quantum
- qkan
- activation
- networks
- kolmogorov-arnold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present QKAN, a quantum-inspired variant of Kolmogorov-Arnold
  networks that replaces classical activation functions with quantum variational circuits
  called DARUANs. These are single-qubit data re-uploading circuits with trainable
  data preprocessing weights, enabling an exponentially growing frequency spectrum
  with data repetitions.
---

# Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2509.14026
- Source URL: https://arxiv.org/abs/2509.14026
- Authors: Jiun-Cheng Jiang; Morris Yu-Chao Huang; Tianlong Chen; Hsi-Sheng Goan
- Reference count: 40
- Primary result: QKAN achieves competitive accuracy with 30-70% fewer parameters than classical KANs and MLPs

## Executive Summary
QKAN introduces quantum variational circuits (DARUANs) as activation functions in Kolmogorov-Arnakov networks, replacing classical B-splines. By introducing trainable weights into data re-uploading circuits, QKAN achieves exponential frequency spectrum growth with minimal parameter increases, enabling superior parameter efficiency. The architecture demonstrates strong performance across regression, classification, and generative modeling tasks while maintaining compatibility with NISQ hardware.

## Method Summary
QKAN replaces classical KAN activation functions with single-qubit data re-uploading circuits called DARUANs. These circuits apply trainable weights to input data during encoding, enabling exponential frequency spectrum growth. The model includes a "Base Activation" residual connection and optional layer extension for improved training stability. HQKAN variants use an MLP bottleneck to compress high-dimensional inputs before processing through QKAN layers, then expand outputs.

## Key Results
- QKAN achieves competitive accuracy with 30% fewer parameters than classical KANs and MLPs on CIFAR-10
- HQKAN achieves 70% parameter reduction on CIFAR-100 while maintaining competitive accuracy
- GPT-2 models with HQKAN match MLP performance with one-third the parameters and reduced training time

## Why This Works (Mechanism)

### Mechanism 1
Trainable weights in data re-uploading circuits exponentially expand the expressible frequency spectrum. Standard circuits produce linear frequency growth (0 to r), but trainable weights create geometric growth (0 to 2^r-1) through weighted frequency combinations.

### Mechanism 2
Exponential frequency reach enables logarithmic parameter reduction relative to approximation error. Classical Fourier-KAN requires grid size proportional to target frequency, while QKAN achieves the same frequency with only logarithmic repetitions.

### Mechanism 3
Layer extension with progressive repetition increases mitigates vanishing gradients. The "Base Activation" adds residual SiLU pathways, ensuring non-zero gradient signals and stabilizing training through curriculum-learning-style complexity growth.

## Foundational Learning

- **Kolmogorov-Arnakov Networks (KAN)**: Edge-based activation networks where learnable functions replace MLP nodes. Understanding KAN structure is essential as QKAN replaces edge activations with quantum circuits.
  - Quick check: How does parameter scaling differ between standard KAN and MLP layers of same width?

- **Data Re-uploading Circuits**: Quantum circuits that sequentially encode data interleaved with trainable unitaries. This architecture generates the Fourier spectrum that serves as the activation function.
  - Quick check: Why does repeating encoding gate S(x) increase Fourier frequency range?

- **Expectation Value as Output**: Quantum circuit outputs are expectation values of observables (e.g., σ_z), bounding outputs to [-1,1]. This requires scaling for unbounded target data.
  - Quick check: How do networks scale bounded quantum outputs to match unbounded targets?

## Architecture Onboarding

- **Component map**: Input x → Linear Pre-processing (w_ℓ x) → Encoding Gate (e^(-iw_ℓxH)) → Trainable Unitary W(θ) [Repeat r times] → Measure ⟨σ_z⟩
- **Critical path**: Initialize weights and parameters → Simulate single-qubit statevector evolution → Apply post-activation scaling → Train with appropriate optimizer
- **Design tradeoffs**: Repetitions (r) vs. grid size (G) - increasing r boosts frequency range exponentially but adds circuit depth linearly; Pure QKAN vs. HQKAN - full QKAN has O(N²) edges while HQKAN uses compression for high dimensions
- **Failure signatures**: Stuck at 0 output (check base activation), poor high-frequency fitting (increase r), memory blowout (use HQKAN for high dimensions)
- **First 3 experiments**: 1D regression on sin(πx), ablation comparing fixed vs. trainable weights on Bessel function, MNIST classification with HQKAN head

## Open Questions the Paper Calls Out
The paper acknowledges limitations in scope, noting it only benchmarks against well-established models rather than state-of-the-art architectures, and defers hardware validation to future work.

## Limitations
- Theoretical advantages proven but practical optimization challenges (barren plateaus, initialization) not systematically studied
- HQKAN efficiency relies on heuristic compression ratios without theoretical bounds on expressivity trade-offs
- NISQ compatibility claimed but only validated through classical simulation

## Confidence
- **High**: QKAN achieves lower parameter counts than classical Fourier-KAN for given approximation error
- **Medium**: HQKAN improves parameter efficiency on CIFAR-100 but architectural choices may not generalize
- **Low**: GPT-2 compatibility claims based on single-task experiments without depth/data scale ablation

## Next Checks
1. Track loss and gradient norms during QKAN training to quantify barren plateau severity
2. Test QKAN on functions with frequencies beyond training range to validate generalization
3. Port small QKAN to simulator with realistic noise models and compare performance degradation