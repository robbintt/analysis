---
ver: rpa2
title: Pretraining Large Language Models with NVFP4
arxiv_id: '2509.25149'
source_url: https://arxiv.org/abs/2509.25149
tags:
- training
- nvfp4
- hadamard
- bf16
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a stable 4-bit training methodology for large
  language models using the NVFP4 format. The approach integrates Random Hadamard
  transforms to manage outliers, a two-dimensional block scaling scheme for consistent
  representations across forward and backward passes, stochastic rounding to reduce
  gradient quantization bias, and selective high-precision layers.
---

# Pretraining Large Language Models with NVFP4

## Quick Facts
- arXiv ID: 2509.25149
- Source URL: https://arxiv.org/abs/2509.25149
- Reference count: 27
- Primary result: Stable 4-bit pretraining of 12B-parameter model on 10T tokens achieving 62.58% MMLU-pro accuracy

## Executive Summary
This work presents NVFP4, a 4-bit floating point format enabling stable pretraining of large language models. The approach integrates Random Hadamard transforms for outlier management, two-dimensional block scaling for forward-backward consistency, stochastic rounding to reduce gradient quantization bias, and selective high-precision layers. A 12-billion-parameter hybrid Mamba-Transformer model was trained on 10 trillion tokens, achieving training loss and downstream task accuracies closely matching FP8 baselines. The methodology demonstrated 36% faster convergence than MXFP4 while requiring 30% less memory, establishing the first documented multi-trillion-token 4-bit pretraining run.

## Method Summary
NVFP4 uses a two-level scaling structure: an FP32 per-tensor scale combined with per-block E4M3 scales (16 elements each) for fine-grained precision. Random Hadamard transforms (16×16 tiles) are applied only to weight gradient inputs to disperse structured outliers into Gaussian-like distributions. Two-dimensional block scaling ensures consistent weight quantization between forward and backward passes, while activations use simpler one-dimensional scaling. Stochastic rounding is applied exclusively to gradients to minimize quantization bias. The methodology employs a mixed-precision strategy, keeping the first two and last eight linear blocks in BF16 (16% of layers) while quantizing the remainder to NVFP4. Training uses a warm-up schedule with learning rate decay over the final 20% of tokens, Adam optimizer, and sequence length 8192.

## Key Results
- Trained 12B-parameter hybrid Mamba-Transformer on 10T tokens with NVFP4
- Achieved 62.58% MMLU-pro accuracy, closely matching FP8 baseline
- Required 36% fewer training tokens than MXFP4 to reach equivalent performance
- Demonstrated 30% memory reduction while maintaining training stability

## Why This Works (Mechanism)

### Mechanism 1: Two-Level Microscaling with Fine-Grained Block Structure
NVFP4's hierarchical scaling enables accurate representation within FP4's limited dynamic range. The format combines per-tensor FP32 scaling with per-block E4M3 scales (16 elements vs. MXFP4's 32), providing fractional precision rather than power-of-two quantization. This dual approach maps block amax values closer to FP4's maximum (±6), preserving dynamic range that MXFP4 wastes through coarse rounding.

### Mechanism 2: Random Hadamard Transforms for Outlier Dispersion
RHT redistributes structured outliers into approximately Gaussian distributions, making them representable in narrow formats. Orthogonal Hadamard matrices (16×16 tiles) multiplied with input tensors spread outlier magnitudes across multiple elements. Applied only to Wgrad inputs because Fprop/Dgrad showed no benefit at tested scales, and applying to weights would break transform inversion.

### Mechanism 3: Two-Dimensional Block Scaling for Forward-Backward Consistency
2D scaling prevents chain rule violations caused by different quantized weight representations in forward vs. backward passes. Standard 1D scaling operates along the dot-product dimension, which transposes between forward (rows) and backward (columns), producing different quantized values. 2D scaling uses 16×16 blocks spanning both dimensions, ensuring identical quantization.

## Foundational Learning

- **Floating-point representation (E2M1, E4M3 formats)**: Understanding what values FP4 can represent (±0, ±0.5, ±1, ±1.5, ±2, ±3, ±4, ±6) and why scale factor precision matters. *Quick check: Why does E4M3 scaling preserve more dynamic range than UE8M0 power-of-two scaling?*

- **Block-wise quantization and microscaling**: The tradeoff between block size (granularity) and overhead (scale factor storage/compute). *Quick check: What happens to quantization accuracy if you increase block size from 16 to 128 elements?*

- **Chain rule in backpropagation with quantization**: Understanding why inconsistent quantization between forward and backward passes degrades training. *Quick check: If weights are quantized differently in Wgrad than Fprop, what mathematical principle is violated?*

## Architecture Onboarding

- **Component map**: Input Tensor → [Global FP32 Scale] → [Per-Block E4M3 Scale] → [Quantize to FP4] → Forward/Backward passes with 2D/1D scaling → [SR Quantize] → Weight Update (Wgrad path includes [Hadamard Transform 16×16])

- **Critical path**: Implementing 2D weight scaling correctly is the most common failure point—ensure block indices are identical in forward and backward.

- **Design tradeoffs**: 
  - Matrix size d=16 vs d=128: Smaller is faster; larger distributes outliers better but with diminishing returns
  - High-precision layers (16% vs minimal): Conservative (first 2 + last 8 blocks in BF16) ensures stability; aggressive (last 4 only) may work but less tested
  - SR on gradients only vs all tensors: SR on activations/weights increases quantization error and causes divergence

- **Failure signatures**:
  - Training divergence early: Check if all linear layers are quantized (keep final layers in BF16)
  - Loss gap widening during decay phase: Consider switching to BF16 for final 18% of training
  - Worse loss than baseline without clear cause: Verify 2D weight scaling is applied identically in both passes

- **First 3 experiments**:
  1. **Sanity check**: Train a 1.2B Transformer for 100M tokens with all components enabled. Verify loss tracks BF16 baseline within ~2% relative error.
  2. **Ablation validation**: Remove stochastic rounding, run for 500M tokens. Confirm loss degradation appears (expect ~1-2% worse relative to full method).
  3. **Scale test**: Apply methodology to your target model size, but keep first+last layers in BF16 initially. Monitor for divergence in first 1B tokens before reducing high-precision fraction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the methodology be extended to fully quantize all linear layers, including attention mechanisms and communication paths, without the current requirement for high-precision fallback layers?
- **Basis in paper:** Section 6 states future work includes "refining the methodology to quantize all linear layers without impacting convergence... and extending NVFP4 to attention and communication paths."
- **Why unresolved:** The current 12B model relies on a mixed-precision strategy, retaining 16% of linear layers (specifically the first two and final eight blocks) in BF16 to ensure stability and prevent divergence.
- **What evidence would resolve it:** A stable training run for a comparable model where 100% of linear layers are quantized to NVFP4 without divergence or significant loss degradation relative to the baseline.

### Open Question 2
- **Question:** Do distinct scaling laws govern the training efficiency gap between NVFP4 and MXFP4 across varying parameter counts and token horizons?
- **Basis in paper:** Section 5 notes that while NVFP4 required 36% fewer tokens than MXFP4 to match loss on an 8B model, "Future studies should evaluate scaling laws for these formats on different parameter counts and token horizons."
- **Why unresolved:** The observed efficiency gap (1.36T vs 1T tokens) was demonstrated only on a single 8B parameter model configuration; it is unknown if this gap remains constant, widens, or shrinks as model scale increases.
- **What evidence would resolve it:** A comparative analysis of convergence curves for both formats across a spectrum of model sizes (e.g., 1B to 100B parameters) trained on identical data mixes.

### Open Question 3
- **Question:** Is the NVFP4 training methodology applicable to architectures with high sparsity or routing dynamics, such as Mixture-of-Experts (MoE) models?
- **Basis in paper:** Section 6 explicitly lists plans to "evaluate it on larger models... and additional architectures such as mixture-of-experts."
- **Why unresolved:** The paper validates the approach using a hybrid Mamba-Transformer architecture. MoE models exhibit distinct activation patterns and gradient noise due to routing, which may interact unpredictably with the Random Hadamard transforms or stochastic rounding.
- **What evidence would resolve it:** Successful convergence results and downstream benchmark accuracy for an MoE model (e.g., a Mixtral-style architecture) trained from scratch using the NVFP4 quantization scheme.

## Limitations

- Scalability uncertainty for models beyond 12B parameters without adjusted parameters
- Implementation-dependent numerical precision claims (1.5% vs 2.5% relative error)
- Selective application of Random Hadamard Transform lacks complete theoretical justification
- Long-term stability of training runs exceeding 10T tokens remains unproven

## Confidence

**High Confidence (90-100%)**:
- NVFP4 achieves better training stability and convergence than MXFP4
- Two-level scaling structure provides measurable improvement in quantization accuracy
- Each component contributes to convergence as confirmed by ablation studies

**Medium Confidence (70-89%)**:
- NVFP4's 1.5% relative error versus MXFP4's 2.5% represents robust improvement
- 62.58% MMLU-pro accuracy claim is reliable for this specific training run
- 16-element block size represents optimal tradeoff between accuracy and overhead

**Low Confidence (0-69%)**:
- Methodology's effectiveness for models significantly larger than 12B parameters
- Claim that NVFP4 eliminates need for additional outlier control techniques
- Long-term stability of training runs exceeding 10T tokens

## Next Checks

1. **Multi-Scale Validation**: Replicate training methodology with 1.2B, 7B, and 70B parameter models using identical NVFP4 parameters. Measure training loss trajectories, convergence speed, and downstream task accuracy across all scales.

2. **Independent Implementation**: Implement NVFP4 quantization from scratch in a different framework and reproduce 12B model training on 1T tokens. Compare training loss curves and final accuracy to validate implementation independence.

3. **Ablation on Large Scale**: Train a 12B model for 5T tokens with NVFP4 but systematically remove each component (start with stochastic rounding, then 2D scaling, then RHT) while maintaining same training duration. Measure how quickly each removal degrades performance.