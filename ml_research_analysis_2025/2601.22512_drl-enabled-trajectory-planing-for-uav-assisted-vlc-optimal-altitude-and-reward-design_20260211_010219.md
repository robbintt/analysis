---
ver: rpa2
title: 'DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and
  Reward Design'
arxiv_id: '2601.22512'
source_url: https://arxiv.org/abs/2601.22512
tags:
- flight
- algorithm
- trajectory
- reward
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of three-dimensional trajectory
  planning for unmanned aerial vehicles (UAVs) in visible light communication (VLC)
  systems, aiming to minimize the UAV's flight distance while maximizing data collection
  efficiency. The authors propose an improved twin delayed deep deterministic policy
  gradient (TD3) algorithm to solve this mixed-integer non-convex optimization problem.
---

# DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design

## Quick Facts
- arXiv ID: 2601.22512
- Source URL: https://arxiv.org/abs/2601.22512
- Reference count: 14
- One-line primary result: TD3-based trajectory planning achieves 35% distance reduction and 50% faster convergence using pheromone rewards

## Executive Summary
This paper proposes an improved twin delayed deep deterministic policy gradient (TD3) algorithm for three-dimensional trajectory planning of UAVs in visible light communication (VLC) systems. The authors address the mixed-integer non-convex optimization problem of minimizing UAV flight distance while maximizing data collection efficiency. By deriving a closed-form optimal flight altitude based on VLC channel gain thresholds and introducing a pheromone-driven reward mechanism, the proposed solution demonstrates significant improvements in both trajectory efficiency and convergence speed compared to baseline methods.

## Method Summary
The authors formulate the UAV trajectory planning problem as a mixed-integer non-convex optimization, aiming to minimize flight distance while ensuring VLC channel gain constraints. They propose an improved TD3 algorithm enhanced with a pheromone-driven reward mechanism that accelerates convergence by 50%. The method derives a closed-form optimal flight altitude expression that reduces flight distance by up to 35% compared to baselines. The algorithm operates in a discrete-time manner, discretizing the UAV's trajectory into waypoints and optimizing both the sequence of ground users to visit and the corresponding waypoints.

## Key Results
- 35% reduction in UAV flight distance compared to baseline methods
- 50% acceleration in algorithm convergence speed through pheromone reward mechanism
- 22.3% performance gain in minimum flight distance compared to state-of-the-art benchmarks
- Closed-form optimal altitude derivation based on VLC channel gain threshold

## Why This Works (Mechanism)
The approach works by combining two key innovations: first, the closed-form altitude solution exploits the physics of VLC channel propagation to find the optimal flight height that balances coverage area and signal strength; second, the pheromone mechanism creates a memory-based reward system that guides the policy toward previously successful regions while avoiding redundant exploration. The TD3 algorithm's stability in continuous control problems, combined with the discrete waypoint optimization, enables efficient trajectory planning in the complex three-dimensional space.

## Foundational Learning
1. **VLC Channel Model** - Understanding Lambertian emission patterns and channel gain calculations is essential for deriving the optimal altitude formula
   - Why needed: The altitude directly affects coverage area and signal strength
   - Quick check: Verify channel gain formula with different Lambertian orders

2. **Mixed-Integer Non-Convex Optimization** - Recognizing the problem structure helps justify the DRL approach
   - Why needed: Traditional optimization methods struggle with the discrete-continuous nature
   - Quick check: Confirm problem NP-hardness through reduction arguments

3. **Twin Delayed Deep Deterministic Policy Gradient (TD3)** - Core algorithm for continuous action space optimization
   - Why needed: Provides stable learning for UAV trajectory control
   - Quick check: Validate TD3 stability through critic network updates

4. **Pheromone-based Reinforcement Learning** - Memory mechanism for accelerating convergence
   - Why needed: Addresses exploration-exploitation tradeoff in trajectory planning
   - Quick check: Compare convergence with and without pheromone mechanism

5. **Three-dimensional Path Planning** - Spatial optimization in UAV navigation
   - Why needed: Enables efficient coverage of ground users while minimizing distance
   - Quick check: Verify collision avoidance in 3D space

## Architecture Onboarding

**Component Map:**
VLC Channel Model -> TD3 Policy Network -> Pheromone Reward Mechanism -> Waypoint Optimizer -> Trajectory Generator

**Critical Path:**
1. UAV state observation from VLC channel gains
2. TD3 policy network outputs action (waypoint selection)
3. Pheromone reward calculation based on coverage history
4. Trajectory optimization using waypoints
5. Execution and state update

**Design Tradeoffs:**
- Discrete vs. continuous waypoint representation
- Pheromone decay rate balancing memory and adaptability
- Altitude optimization vs. path length minimization
- Single-agent vs. multi-agent extension complexity

**Failure Signatures:**
- Policy collapse when pheromone values become too high
- Suboptimal altitude selection under varying GU density
- Convergence issues with mobile ground users
- Performance degradation in non-ideal VLC conditions

**First 3 Experiments:**
1. Altitude optimization validation across different GU distributions
2. Convergence comparison with and without pheromone mechanism
3. Performance under varying Lambertian emission orders

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How does arbitrary device orientation or non-vertical alignment impact the accuracy of the derived closed-form optimal flight altitude?
- **Basis in paper:** [inferred] Section III-B explicitly derives the optimal altitude under the simplifying condition that "the UAV and GU are vertical downward and upward, respectively," ensuring $\cos(\phi) = \cos(\psi) = h/d$.
- **Why unresolved:** The paper does not analyze performance degradation when this alignment assumption is violated due to UAV movement or GU tilting, which creates a gap between the theoretical derivation and practical deployment.
- **What evidence would resolve it:** Simulation results analyzing flight distance minimization under random orientation errors or non-vertical irradiance/incidence angles.

### Open Question 2
- **Question:** Can the proposed single-agent TD3 framework be effectively scaled to multi-UAV scenarios requiring collision avoidance and coordination?
- **Basis in paper:** [inferred] The System Model in Section II-A specifies a setup comprising "one UAV and I GUs," and the optimization problem (P1) focuses on a single trajectory.
- **Why unresolved:** While the algorithm minimizes distance for a single agent, the pheromone mechanism and TD3 policy may require significant modification to handle the state space explosion and inter-UAV interference inherent in multi-agent systems.
- **What evidence would resolve it:** Comparative analysis of the proposed algorithm versus Multi-Agent DRL baselines in a multi-UAV network.

### Open Question 3
- **Question:** Is the pheromone-driven reward mechanism robust in dynamic environments where Ground Users (GUs) are mobile rather than static?
- **Basis in paper:** [inferred] The system model in Section II-A denotes GU locations as fixed coordinates $w_i = [x_i, y_i, 0]$, and the state space relies on updating coverage based on these static positions.
- **Why unresolved:** The pheromone update (Eq. 12) accumulates residual pheromone based on previous states; rapid user movement could render the "residual pheromone" misleading, potentially hindering convergence rather than accelerating it.
- **What evidence would resolve it:** Convergence plots and trajectory efficiency metrics generated from simulations involving mobile GUs with varying velocities.

## Limitations
- Assumes ideal VLC channel conditions without accounting for atmospheric interference or non-ideal receiver orientation
- Single-agent framework may not scale effectively to multi-UAV scenarios with collision avoidance requirements
- Pheromone mechanism performance unproven in dynamic environments with mobile ground users
- Closed-form altitude solution relies on simplifying assumptions that may not hold in practical deployments

## Confidence
- **35% distance reduction claim:** Medium confidence - lacks detailed baseline descriptions and experimental conditions
- **50% convergence acceleration:** Medium confidence - pheromone mechanism effectiveness needs validation across different network topologies
- **22.3% performance gain:** Medium confidence - requires verification under varying environmental conditions and network configurations

## Next Checks
1. Test the algorithm's performance under varying atmospheric conditions and receiver orientations
2. Validate the closed-form altitude solution across different VLC channel models
3. Evaluate the reward mechanism's effectiveness in multi-UAV scenarios with dynamic ground node movements