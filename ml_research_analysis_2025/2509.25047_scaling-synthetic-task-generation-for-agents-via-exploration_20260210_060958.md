---
ver: rpa2
title: Scaling Synthetic Task Generation for Agents via Exploration
arxiv_id: '2509.25047'
source_url: https://arxiv.org/abs/2509.25047
tags:
- task
- tasks
- agent
- action
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoPlay addresses the challenge of scaling task generation for
  post-training multimodal large language models (MLLMs) to build interactive agents
  across domains like computer use, web navigation, and robotics. The key bottleneck
  is the lack of high-quality downstream agentic task datasets that are diverse, feasible,
  and verifiable.
---

# Scaling Synthetic Task Generation for Agents via Exploration

## Quick Facts
- arXiv ID: 2509.25047
- Source URL: https://arxiv.org/abs/2509.25047
- Reference count: 40
- Primary result: AutoPlay generates 20k Android and 10k Ubuntu tasks, improving MLLM UI agents by up to 20.6% mobile and 10.9% computer-use success rates

## Executive Summary
AutoPlay addresses the critical bottleneck of scaling task generation for multimodal large language models (MLLMs) in interactive agent domains. The core challenge is creating diverse, feasible, and verifiable task datasets without relying on expensive human annotation. AutoPlay introduces a two-stage pipeline where an MLLM explorer systematically discovers environment states and functionalities, which are then used by a task generator to synthesize executable tasks. This exploration-grounded approach generates 20k tasks across 20 Android apps and 10k tasks across 13 Ubuntu apps, improving downstream agent success rates by up to 20.6% on mobile-use and 10.9% on computer-use scenarios. Additionally, combining AutoPlay tasks with MLLM verifier-based rewards enables reinforcement learning, yielding an additional 5.7% improvement.

## Method Summary
AutoPlay operates through a two-stage pipeline: exploration and task generation. In the exploration phase, an MLLM explorer agent with episodic memory performs multiple interaction turns with the environment, discovering novel states and feasible interactions. A summarizer MLLM compresses these trajectories into a memory representation. In the task generation phase, a task generator MLLM uses the exploration context plus structured task guideline prompts to synthesize diverse, executable tasks. The generated tasks are executed by an MLLM-based executor agent, and trajectories are verified by an MLLM verifier for quality filtering. This filtered data trains goal-conditioned MLLM policies via supervised finetuning or reinforcement learning with GRPO. The approach generates ~20k Android tasks and ~10k Ubuntu tasks, yielding ~8k and ~3.5k verified successful trajectories respectively.

## Key Results
- AutoPlay generates 20k tasks across 20 Android apps and 10k tasks across 13 Ubuntu apps
- Downstream MLLM UI agents achieve up to 20.6% improvement on mobile-use and 10.9% on computer-use scenarios
- RL training with verifier-based rewards yields additional 5.7% improvement beyond SFT
- Task execution success rate improves from 43.5% to 46.0% when using task guidelines

## Why This Works (Mechanism)

### Mechanism 1: Exploration-Grounded Task Synthesis
AutoPlay's exploration phase enables generation of tasks with higher feasibility and coverage than static prompting. An MLLM explorer systematically interacts with the environment over multiple rounds, summarizing discoveries into memory. This exploration context—screenshots and feasible interactions—grounds task generation in actual environment states rather than abstract descriptions.

### Mechanism 2: Task Guidelines for Diversity Control
Structured task guideline prompts steer the task generator toward diverse task categories with improved domain coverage. AutoPlay injects domain-specific guideline prompts (e.g., "Feature-Use," "Information Retrieval," "Feature-Composition") that specify desired task properties, conditioning the task generator on both exploration context and guidelines.

### Mechanism 3: Verifier-Based Quality Filtering and RL Rewards
MLLM-based trajectory verification enables scalable SFT data filtering and RL training without human annotation. A verifier MLLM takes the task instruction and executed trajectory, producing chain-of-thought reasoning and binary success/failure judgments. This signal serves dual purposes: filtering trajectories for SFT and providing binary rewards for RL training with GRPO.

## Foundational Learning

- **Partially Observable MDPs (POMDPs)**: UI environments are partially observable because agents only see screenshots, not full state. This motivates exploration—the agent must interact to discover environment dynamics and states. *Quick check: Can you explain why UI environments are "partially observable" rather than fully observable, and how this motivates the exploration phase?*

- **Goal-Conditioned Policies**: AutoPlay trains policies π(a|o, g) that condition on both observations and task goals. The generated task dataset provides goal specifications paired with initial states for training. *Quick check: How does the goal-conditioned formulation differ from standard RL, and why is it necessary for multi-task UI agents?*

- **GRPO (Group Relative Policy Optimization)**: AutoPlay uses GRPO for RL training with verifier-based binary rewards. Understanding GRPO's group-based advantage estimation is necessary to implement the RL component correctly. *Quick check: Why might GRPO be preferred over standard PPO for training with binary verifier rewards? What role does the group size hyperparameter play?*

## Architecture Onboarding

- **Component map**: Explorer MLLM → Environment → Summarizer MLLM → Memory → Task Generator MLLM → Task Dataset → Executor Agent → Trajectories → Verifier MLLM → Filtered SFT Dataset → Training

- **Critical path**: Exploration success determines task coverage → Affects downstream agent capability; Verifier accuracy determines data quality → Critical for both SFT and RL; Executor success rate determines training data yield → Higher yield = more training data

- **Design tradeoffs**: Explorer model choice (GPT-4o vs smaller models); Exploration turns (M=3 Android, M=5 Ubuntu) with diminishing returns; Verifier model choice (GPT-4o more accurate but Qwen-32B enables scalable RL due to cost)

- **Failure signatures**: Low task execution success rate (explorer discovered infeasible interactions or executor is too weak); Benchmark performance plateaus (task distribution doesn't cover benchmark types); RL instability (verifier noise or insufficient successful trajectories); App-specific failures (exploration didn't reach certain states)

- **First 3 experiments**: 1) Ablate exploration depth: vary M=1,3,5,10 on subset of apps; measure task execution success and downstream Pass@1; 2) Compare verifier models: run SFT with GPT-4o, Qwen-32B, Qwen-72B; measure downstream performance; 3) Scale RL training: from AutoPlay-7B SFT, run GRPO with 50k, 100k, 200k environment samples; measure Pass@1 on AndroidWorld

## Open Questions the Paper Calls Out

1. **Robotics adaptation**: Can AutoPlay be effectively adapted for robotics domains where state spaces are continuous and visual observations differ from structured UIs? The Introduction identifies robotics as a target domain, yet experiments are limited to Android and Ubuntu UI environments.

2. **Automated guideline generation**: How can task guideline generation be automated to better cover complex computer-use primitives like bash scripting and cross-app interactions? The method "struggles to generate tasks with cross-app interaction, bash scripting and web search," attributed to insufficient task guidelines for computer-use.

3. **Exploration bottleneck analysis**: To what extent does the capability of the exploration agent bottleneck the quality of the synthetic dataset? While implied, the quantitative relationship between exploration agent success rate and downstream utility of generated tasks hasn't been isolated.

## Limitations

- **Exploration coverage gaps**: The exploration phase fails on certain apps (Chrome, Simple Gallery Pro, VLC show 0% success), indicating the approach may not generalize to all UI environments without systematic recovery strategies.

- **Verifier reliability concerns**: The 1.5% performance gap between GPT-4o and Qwen-72B verifiers, combined with the verifier's inability to access privileged environment information, raises concerns about systematic verification biases that could propagate through both SFT and RL training.

- **Task diversity constraints**: Despite generating 20k Android and 10k Ubuntu tasks, the downstream agent performance gains are modest (20.6% max for mobile, 10.9% for computer-use), suggesting the generated task distribution may not fully align with benchmark task distributions.

## Confidence

**High Confidence**: The core mechanism of using exploration-grounded task generation to improve downstream agent performance is well-supported by ablation studies (43.5% → 46.0% task execution success with guidelines) and direct performance comparisons showing AutoPlay-72B outperforming the GPT-4o + UI-TARS policy used to collect the data.

**Medium Confidence**: Claims about the effectiveness of task guidelines and the verifier-based RL pipeline are supported by experimental results, but the underlying assumptions about guideline comprehensiveness and verifier accuracy remain partially untested. The guideline impact (26.7% → 38.2% Pass@1) is demonstrated but not deeply analyzed for guideline quality or coverage.

**Low Confidence**: The paper's claims about exploration being superior to static prompting lack direct comparison to alternative task generation methods that don't use exploration. The relative contribution of exploration versus task guidelines versus verifier quality to overall performance improvements is not systematically isolated.

## Next Checks

1. **Exploration Coverage Analysis**: Run the exploration pipeline on apps showing 0% success (Chrome, Simple Gallery Pro, VLC) with extended turn limits (M=10) and detailed logging of failure modes. Compare state coverage against apps with high success rates to identify systematic exploration limitations.

2. **Verifier Error Analysis**: Create a ground-truth dataset of 500 task trajectories by human annotation. Compare verifier predictions (both GPT-4o and Qwen-72B) against ground truth to quantify false positive/negative rates. Analyze whether verification errors correlate with specific task types or UI patterns.

3. **Task Distribution Alignment**: Extract task templates and requirements from AndroidWorld/OSWorld benchmarks. Compare the distribution of task types, complexity levels, and domain coverage between AutoPlay-generated tasks and benchmark tasks. Identify specific task categories where generated tasks under-represent benchmark distributions.