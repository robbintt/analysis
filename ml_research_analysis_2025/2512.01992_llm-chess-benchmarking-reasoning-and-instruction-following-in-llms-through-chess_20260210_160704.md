---
ver: rpa2
title: 'LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through
  Chess'
arxiv_id: '2512.01992'
source_url: https://arxiv.org/abs/2512.01992
tags:
- chess
- reasoning
- moves
- move
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM CHESS is a benchmarking framework designed to evaluate the
  reasoning and instruction-following capabilities of large language models (LLMs)
  through chess gameplay. The framework uses an agentic approach where LLMs autonomously
  interact with a chess environment by selecting actions such as viewing the board
  state, querying legal moves, or making moves in UCI format.
---

# LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess

## Quick Facts
- arXiv ID: 2512.01992
- Source URL: https://arxiv.org/abs/2512.01992
- Reference count: 40
- Primary result: Reasoning-enhanced LLMs outperform non-reasoning models in chess, with o3 (low) reaching 758 Elo rating.

## Executive Summary
LLM CHESS is a novel benchmarking framework designed to assess the reasoning and instruction-following capabilities of large language models (LLMs) by having them play chess autonomously. The framework evaluates over 50 models through an agentic approach, where LLMs interact with a chess environment by selecting actions such as viewing the board state, querying legal moves, or making moves in UCI format. Models are tested against both a random opponent and a chess engine with variable skill levels. The study reveals that only reasoning-enhanced models consistently outperform random play, with o3 (low) achieving the highest performance. The benchmark highlights significant challenges in instruction-following and robustness, even for top models, and demonstrates that scaling reasoning effort or using multiple models in parallel provides limited improvements. LLM CHESS offers a dynamic, extensible framework for evaluating LLM capabilities in complex reasoning tasks.

## Method Summary
The LLM CHESS framework evaluates LLMs through autonomous chess gameplay. Models are given a set of available actions, including viewing the board state, querying legal moves, and making moves in UCI format. Over 50 models, including reasoning-enhanced and non-reasoning models, are tested against a random opponent and a chess engine with variable skill levels. Performance is measured using Elo ratings, with o3 (low) achieving the highest score of 758 Elo. The framework emphasizes instruction-following and reasoning, revealing that reasoning-enhanced models consistently outperform others. The study also explores the impact of scaling reasoning effort and parallel model usage, finding limited improvements.

## Key Results
- Reasoning-enhanced models consistently outperform non-reasoning models in chess gameplay.
- o3 (low) achieves the highest performance with a 758 Elo rating against a chess engine.
- Instruction-following remains a significant challenge, even for top models, and scaling reasoning effort or using multiple models in parallel provides limited improvements.

## Why This Works (Mechanism)
The LLM CHESS framework works by leveraging the structured nature of chess to evaluate LLM reasoning and instruction-following capabilities. Chess provides a clear, rule-based environment where models must interpret instructions, reason about legal moves, and adapt to dynamic board states. The agentic approach allows models to autonomously interact with the chess environment, making decisions based on their understanding of the game. This setup isolates reasoning and instruction-following skills, providing a controlled yet complex task for benchmarking. The use of Elo ratings offers a standardized measure of performance, enabling comparisons across models and configurations.

## Foundational Learning
- **Chess Rules and UCI Format**: Understanding chess rules and UCI (Universal Chess Interface) format is essential for interpreting model actions and evaluating performance. Quick check: Verify model move legality using UCI format.
- **Elo Rating System**: Elo ratings provide a standardized measure of chess skill, allowing for meaningful comparisons across models. Quick check: Calculate Elo differences between models to assess performance gaps.
- **Agentic Interaction**: Models autonomously interact with the chess environment, selecting actions based on their reasoning. Quick check: Analyze action selection patterns to identify reasoning strategies.
- **Reasoning-Enhanced Models**: Models with reasoning capabilities (e.g., o3) demonstrate superior performance, highlighting the importance of reasoning in complex tasks. Quick check: Compare performance of reasoning-enhanced vs. non-reasoning models.
- **Instruction-Following**: The ability to interpret and execute instructions is critical for model success in LLM CHESS. Quick check: Evaluate model performance with varied instruction phrasing.

## Architecture Onboarding

**Component Map**: Chess Environment -> LLM Model -> Action Selection -> Move Execution -> Elo Rating Calculation

**Critical Path**: The critical path involves the model interpreting the board state, reasoning about legal moves, and executing a move. Performance is measured through Elo ratings, which depend on the model's ability to follow instructions and reason effectively.

**Design Tradeoffs**: The framework balances simplicity (using chess as a well-defined task) with complexity (requiring reasoning and instruction-following). Tradeoffs include the generalizability of results to other domains and the impact of instruction phrasing on performance.

**Failure Signatures**: Common failure modes include incorrect interpretation of board states, failure to identify legal moves, and poor instruction-following. These failures highlight limitations in reasoning and robustness.

**First Experiments**:
1. Evaluate model performance with varied instruction phrasing to quantify the impact on instruction-following.
2. Test models against human-like opponent strategies to assess robustness beyond random and fixed-engine opponents.
3. Extend the benchmark to non-chess domains requiring similar reasoning and instruction-following to assess generalizability.

## Open Questions the Paper Calls Out
- How generalizable are the results to real-world applications beyond chess?
- What is the impact of diverse opponent strategies, including human-like play patterns, on model performance?
- How does prompt engineering and instruction phrasing affect model performance and reproducibility?

## Limitations
- Results may not generalize to real-world applications beyond the well-defined chess domain.
- Evaluations are limited to a random opponent and a chess engine with fixed difficulty, potentially missing performance differences against diverse strategies.
- The impact of prompt engineering and instruction phrasing on model performance is not fully explored, affecting reproducibility and robustness assessments.

## Confidence

**High Confidence**:
- Reasoning-enhanced models consistently outperform non-reasoning models, as evidenced by Elo ratings and statistical comparisons.
- LLM CHESS offers a dynamic, extensible benchmark for evaluating LLM capabilities in complex reasoning tasks.

**Medium Confidence**:
- Instruction-following remains a significant challenge for top models, though this conclusion depends on the specific phrasing and complexity of instructions used.

## Next Checks
1. Evaluate the same models on LLM CHESS with varied opponent strategies, including human-like play patterns, to test robustness beyond random and fixed-engine opponents.
2. Systematically vary instruction phrasing and prompt engineering across model types to quantify their impact on instruction-following performance.
3. Extend the benchmark to non-chess domains requiring similar reasoning and instruction-following to assess generalizability and domain transferability.