---
ver: rpa2
title: 'RSQ: Learning from Important Tokens Leads to Better Quantized LLMs'
arxiv_id: '2503.01820'
source_url: https://arxiv.org/abs/2503.01820
tags:
- tokens
- layer
- quantization
- input
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RSQ proposes prioritizing important tokens during layer-wise quantization
  of LLMs by scaling input features based on token importance scores, followed by
  rotation to mitigate outliers and GPTQ-based quantization. Token importance is computed
  dynamically using attention concentration, which assigns higher weights to tokens
  with stronger attention scores.
---

# RSQ: Learning from Important Tokens Leads to Better Quantized LLMs

## Quick Facts
- arXiv ID: 2503.01820
- Source URL: https://arxiv.org/abs/2503.01820
- Reference count: 40
- Primary result: RSQ achieves up to 1.6% higher average accuracy than QuaRot across three model families (LLaMA3, Mistral, Qwen2.5) on ten downstream tasks.

## Executive Summary
RSQ introduces a post-training quantization method that prioritizes important tokens during layer-wise quantization of LLMs. By scaling input features based on token importance scores computed from attention concentration, followed by rotation to mitigate outliers and GPTQ-based quantization, RSQ achieves superior performance compared to state-of-the-art methods. The approach shows particular strength at lower bit precisions and on long-context benchmarks, with improvements that are robust across model sizes, calibration datasets, and quantization settings.

## Method Summary
RSQ is a three-step post-training quantization pipeline that modifies GPTQ by introducing token importance weighting. First, weights undergo orthogonal rotation using randomized Hadamard matrices to mitigate outliers. Second, token importance is computed dynamically using attention concentration (AttnCon), which assigns higher weights to tokens receiving stronger attention scores across all heads. Third, a modified GPTQ quantizer uses a Hessian matrix that incorporates importance scores (H_RSQ = 2XR²Xᵀ) to prioritize reconstruction error on high-attention tokens. The method includes data expansion that generates 8 shifted copies of calibration sequences to reduce positional bias in importance scoring.

## Key Results
- RSQ achieves up to 1.6% higher average accuracy than QuaRot across three model families (LLaMA3, Mistral, Qwen2.5) on ten downstream tasks
- Superior performance at lower bit precisions (2-bit vs 4-bit), with gains of 2.8% and 2.2% on LITM and L-Eval long-context benchmarks
- Robust improvements across model sizes from 7B to 70B parameters and various calibration datasets

## Why This Works (Mechanism)

### Mechanism 1: Attention-Based Token Importance Scaling
Weighting tokens by their attention concentration during quantization improves downstream task performance compared to uniform weighting. The method computes importance scores r_i by summing attention received by each token across all heads, then scales input features via diagonal matrix R. This modifies the Hessian used in GPTQ from H = 2XXᵀ to H_RSQ = 2XR²Xᵀ, effectively prioritizing reconstruction error on high-attention tokens. The core assumption is that tokens receiving stronger attention scores contain more critical information that should be preferentially preserved when capacity is reduced through quantization.

### Mechanism 2: Orthogonal Transformation for Outlier Mitigation
Applying random orthogonal transformations to weight matrices redistributes outlier magnitudes, improving quantization quality. The method exploits computational invariance of transformers—if RMSNorm separates layers, applying orthogonal Q to one layer and Qᵀ to the next preserves output. This transformation is applied using a randomized Hadamard matrix, transforming W_q, W_k, W_v, W_up, W_gate, lm_head as WQᵀ and W_o, W_down, embedding as QW. The core assumption is that weight outliers (parameters with unusually large magnitudes) force most weights into narrow quantization ranges, degrading performance; rotation redistributes these magnitudes more uniformly.

### Mechanism 3: Dataset Expansion for Positional Bias Mitigation
Augmenting calibration data with shifted sequences ensures tokens at all positions can receive importance during quantization. The method generates M shifted versions of each sequence by offsetting by T/M, 2T/M, ..., (M-1)T/M tokens, inserting excess tokens at the beginning. With M=8, each token eventually occupies "important" (early) positions in some augmented sample. The core assumption is that important tokens are positionally biased toward initial and final positions; without augmentation, middle-position tokens are systematically underweighted.

## Foundational Learning

- **Layer-wise Quantization & GPTQ**: Why needed here: RSQ modifies the GPTQ framework's objective function and Hessian computation; understanding the baseline algorithm is essential to see what changes. Quick check question: In GPTQ, after quantizing column q of weights, how are remaining weights updated to compensate? (Answer: Using Hessian inverse via δ = - (W_{:,q} - quant(W_{:,q})) / H^{-1}_{qq} · H^{-1}_{q,:})

- **Attention Mechanisms in Transformers**: Why needed here: Token importance is computed from attention probability maps (AttnCon method); understanding multi-head attention structure is required. Quick check question: What does A_{m,i,j} represent in the attention matrix, and why is A_{m,i,j} = 0 for j > i in autoregressive models? (Answer: Attention from token i to token j in head m; causal masking prevents attending to future tokens)

- **Computational Invariance & Orthogonal Transformations**: Why needed here: RSQ's rotation step exploits this property; without it, rotation would change model outputs and require retraining. Quick check question: Given Y = W₂W₁X, why can you apply orthogonal Q to transform to Y = (W₂Qᵀ)(QW₁)X without changing Y? (Answer: QᵀQ = I by orthogonality definition, so W₂QᵀQW₁ = W₂W₁)

## Architecture Onboarding

- **Component map**: LayerNorm → RMSNorm conversion → Rotation module (Hadamard transforms) → Importance scorer (AttnCon) → Scaling module (R matrix) → Modified GPTQ quantizer → Data expansion (M=8 shifted copies)

- **Critical path**: 1) Pre-processing: Convert LayerNorm to RMSNorm, apply rotation to all weight matrices 2) Calibration phase (per-layer loop): Forward pass with expanded calibration data, extract attention maps, compute AttnCon importance per token, build R matrix, scale input features, quantize with modified Hessian, use quantized output as input to next layer 3) Post-processing: Model is ready for inference (rotations stay baked in)

- **Design tradeoffs**: r_min hyperparameter ∈ [0.005, 0.1]: Lower values aggressively focus on important tokens; Section 5.2 shows optimal at 0.01 for AttnCon. Trade-off: too low ignores potentially useful tokens, too high reduces benefit. Expansion factor M: Paper uses M=8. Higher M improves token coverage but increases calibration time linearly. Calibration sequence length: Table 3 shows no clear trend between longer calibration sequences and long-context performance—suggests current calibration strategy may not fully exploit long sequences. Per-layer vs global importance: Paper finds keeping importance consistent across all weights within a layer works best (Section 4.3).

- **Failure signatures**: Perplexity increases at extreme token counts: Figure 2 shows perplexity worsens below ~256 tokens—model needs minimum token interactions for attention computation. Large r_min (e.g., 0.1) without rotation: Figure 9 shows scaling alone is less effective; rotation enables aggressive importance weighting. Long-context degradation: If LITM/L-Eval performance drops significantly, check that expansion factor is sufficient and calibration sequence lengths are varied. Module-specific failures: Figure 7 shows v_proj benefits most; if certain modules underperform, consider applying RSQ selectively or adjusting per-module.

- **First 3 experiments**: 1) Baseline reproduction: Quantize LLaMA3-8B-Instruct to 3-bit using RSQ with WikiText-2 (256 samples × 4096 tokens), M=8, r_min=0.01. Measure Wiki perplexity and average accuracy across the 10 tasks in Table 2. Target: perplexity ~9.04, accuracy ~65.4%. 2) Importance method ablation: Compare AttnCon vs ActNorm vs TokenSim vs First-N on Wiki perplexity, varying r_min ∈ {0.005, 0.01, 0.02, 0.05, 0.1}. Validate that AttnCon achieves lowest perplexity around r_min=0.01 as claimed. 3) Long-context stress test: Evaluate quantized model on LITM (P=1,15,30) and L-Eval (L=300,460,620) using three calibration configurations from Table 3. Check that RSQ maintains 2-3% improvement over QuaRot and that performance doesn't collapse at longer inputs.

## Open Questions the Paper Calls Out

### Open Question 1
Why does the performance improvement from importance scaling vary significantly across transformer modules, with the most pronounced gains observed specifically in the value projection (`v_proj`) layers? While the authors observe that the most significant improvement is found in `v_proj` and explicitly state they "leave a deeper exploration of this phenomenon for future work," they do not provide a mechanistic explanation for why the quantization error sensitivity is uniquely higher in this specific projection compared to query, key, or feed-forward layers.

### Open Question 2
Why is the token-scaling strategy significantly less effective when applied to weights that have not undergone orthogonal rotation (SQ) compared to the rotated setup (RSQ)? Appendix C.5 demonstrates that the optimal scaling hyperparameter ($r_{min}$) differs drastically between the rotated and unrotated settings, leading the authors to "leave further investigation of this phenomenon for future work." The paper empirically shows that scaling unrotated weights yields inferior results but does not explain the theoretical interaction between the rotation transformation and the weighted reconstruction loss.

### Open Question 3
Can calibration datasets be explicitly designed or sampled to better align with token importance distributions required for long-context tasks? In Section 5.3, the authors note that simply increasing calibration sequence length does not guarantee better long-context performance, concluding that "A more advanced strategy... is needed to further enhance performance." The current work relies on standard datasets and a simple shifting augmentation, leaving the specific alignment between calibration data importance signals and long-context evaluation requirements unexplored.

## Limitations

- Scalability concerns with attention-based importance computation that scales quadratically with sequence length
- Hyperparameter sensitivity to r_min value, which may vary across model architectures and dataset domains
- Architectural constraints requiring LayerNorm to be converted to RMSNorm for rotation to work
- Limited exploration of long-context calibration strategies beyond simple sequence expansion

## Confidence

**High confidence**: RSQ outperforms QuaRot by up to 1.6% average accuracy across three model families; token importance computed via attention concentration is more effective than alternative methods; data expansion mitigates positional bias; RSQ maintains superior performance at lower bit precisions.

**Medium confidence**: Rotation is necessary for effective importance weighting (Figure 9 shows optimal r_min shifts dramatically with/without rotation); RSQ provides consistent improvements across different model sizes; the method is robust across different calibration datasets.

**Low confidence**: RSQ's relative advantage over NUQ (7.8% vs 5.2% on L-Eval) - this comparison uses different models and settings; the specific optimal value r_min = 0.01 for AttnCon - while validated on WikiText-2, cross-domain generalizability is unclear.

## Next Checks

1. **Cross-domain robustness test**: Evaluate RSQ with AttnCon on calibration datasets from different domains (e.g., C4, BookCorpus, or domain-specific corpora) and verify that r_min = 0.01 remains optimal. Compare performance degradation when using mismatched calibration domains versus in-domain calibration.

2. **Rotation necessity ablation**: Create a controlled experiment that systematically removes rotation while keeping all other RSQ components identical. Quantify the exact performance drop on both perplexity and downstream tasks to determine whether rotation is truly necessary or if importance weighting alone provides sufficient benefit.

3. **Long-context scaling study**: Extend calibration sequences to 16k and 32k tokens with corresponding data expansion factors. Measure whether RSQ's advantage over QuaRot persists or grows at longer contexts, and whether the current M=8 expansion becomes insufficient for capturing long-range attention patterns.