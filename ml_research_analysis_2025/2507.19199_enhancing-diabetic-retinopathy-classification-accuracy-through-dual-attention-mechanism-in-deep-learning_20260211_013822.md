---
ver: rpa2
title: Enhancing Diabetic Retinopathy Classification Accuracy through Dual Attention
  Mechanism in Deep Learning
arxiv_id: '2507.19199'
source_url: https://arxiv.org/abs/2507.19199
tags:
- dataset
- attention
- classification
- accuracy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of diabetic retinopathy (DR)
  classification using imbalanced datasets. A dual attention mechanism combining Global
  Attention Block (GAB) and Category Attention Block (CAB) is integrated into three
  pre-trained backbone networks (MobileNetV3-small, EfficientNet-b0, DenseNet-169).
---

# Enhancing Diabetic Retinopathy Classification Accuracy through Dual Attention Mechanism in Deep Learning

## Quick Facts
- arXiv ID: 2507.19199
- Source URL: https://arxiv.org/abs/2507.19199
- Reference count: 12
- DenseNet-169 with dual attention achieves 83.20% accuracy on APTOS dataset

## Executive Summary
This study addresses diabetic retinopathy (DR) classification challenges using imbalanced datasets by integrating a dual attention mechanism into pre-trained backbone networks. The approach combines Global Attention Block (GAB) and Category Attention Block (CAB) to enhance feature extraction and improve classification accuracy, particularly for underrepresented DR grades. Experiments on APTOS and EYEPACS datasets demonstrate significant performance improvements, with DenseNet-169 achieving 83.20% accuracy on APTOS and EfficientNet-b0 reaching 80% on EYEPACS.

## Method Summary
The method employs a dual attention mechanism combining GAB and CAB integrated into three pre-trained backbone networks: MobileNetV3-small, EfficientNet-b0, and DenseNet-169. The GAB applies channel and spatial attention sequentially to refine global features, while CAB allocates dedicated feature channels per DR grade to mitigate class imbalance. The model processes 512×512 fundus images through the backbone, attention blocks, global average pooling, and a fully connected layer for 5-class DR grading. Training uses Adam optimizer with initial learning rate 5×10⁻³, batch size 16-32, and 40 epochs with learning rate reduction on plateau.

## Key Results
- DenseNet-169 achieves 83.20% accuracy on APTOS dataset
- EfficientNet-b0 reaches 80% accuracy on EYEPACS dataset
- MobileNetV3-small demonstrates competitive performance with only 1.6M parameters (82% accuracy on APTOS)
- The model reports F1-score of 82.0%, precision of 82.1%, sensitivity of 83.0%, specificity of 95.5%, and kappa score of 88.2%

## Why This Works (Mechanism)

### Mechanism 1
GAB improves feature selection through parallel channel-spatial attention, refining both "what" and "where" information before classification. It applies channel attention via two 1×1 convolutions after Global Average Pooling (GAP), computing channel-wise weights to suppress less informative channels. Spatial attention follows using cross-channel average pooling to highlight discriminative spatial positions. The sequential channel-then-spatial approach allows the model to first determine feature importance, then localize it.

### Mechanism 2
CAB mitigates class imbalance by allocating dedicated feature channels per DR grade, forcing the model to learn class-specific discriminative regions. It processes input features through 1×1 convolution producing kL channels (k per class for L classes). During training, dropout randomly zeros 50% of features per class, ensuring redundancy. Category-wise cross-channel pooling computes per-class attention maps, which are averaged to produce a unified attention mask. This mask element-wise multiplies the input, emphasizing regions relevant to all classes equally.

### Mechanism 3
Sequential GAB→CAB ordering preserves fine-grained lesion details while enabling category-specific refinement. GAB first extracts global contextual features independent of class labels, preserving small lesion regions. CAB then refines these features with category-aware attention, focusing on discriminative regions for each DR grade. The paper states reversing the order causes CAB to lose fine-grained details early, degrading final performance.

## Foundational Learning

- **Channel vs. Spatial Attention**
  - Why needed here: GAB combines both; understanding their distinct roles is essential for debugging attention maps. Channel attention weights feature maps (which filters fire), spatial attention weights locations (where activations matter).
  - Quick check question: Given a feature map of shape (H=32, W=32, C=512), which attention reduces spatial dimensions for weighting? (Answer: Channel attention via GAP → H,W collapsed to 1,1).

- **Class Imbalance in Multi-Class Classification**
  - Why needed here: DR datasets are severely imbalanced (e.g., 73.3% No DR, 2% Proliferative). Standard cross-entropy overfits to majority classes; CAB addresses this structurally.
  - Quick check question: Why might accuracy be misleading when 73% of samples belong to one class? (Answer: A model predicting only the majority class achieves 73% accuracy with zero minority-class recall).

- **Transfer Learning with Frozen Layers**
  - Why needed here: The backbone is pre-trained on ImageNet, frozen initially, then unfrozen for fine-tuning. Understanding when to freeze/unfreeze prevents catastrophic forgetting.
  - Quick check question: What risks arise from unfreezing all layers immediately with a small dataset? (Answer: Overfitting to small dataset, losing ImageNet-learned low-level features).

## Architecture Onboarding

- **Component map**: Backbone → Conv 1×1 → GAB (Channel + Spatial Attention) → CAB (Category Attention) → GAP → FC
- **Critical path**: 1. Preprocess: Rescale to 512×512, augment (rotation 90°/180°/270°, horizontal flip) 2. Forward pass: Backbone → GAB → CAB → GAP → FC → predictions 3. Loss: Cross-entropy with class weights optional 4. Optimization: Adam, lr=5×10⁻³ initial, reduce by 0.8× if validation stalls for 3 epochs, batch_size=16–32, epochs=40
- **Design tradeoffs**: MobileNetV3 (fastest inference, 1.6M params, 82% accuracy on APTOS) vs. DenseNet-169 (highest accuracy 83.6%, but 17M params and slower). EfficientNet-b0 offers balance (80% accuracy, 7.1M params).
- **Failure signatures**: Overfitting to majority class (high overall accuracy but F1-score for minority classes near zero), attention collapse (Grad-CAM shows uniform attention), validation loss divergence (training loss near zero but validation loss increases after epoch ~15).
- **First 3 experiments**: 1. Baseline sanity check: Train backbone without GAB/CAB on APTOS. Target: ~75–79% accuracy 2. Ablation: GAB vs. CAB: Add GAB only, then CAB only. Expect +2–3% accuracy each 3. Full model with Grad-CAM visualization: Train full GAB+CAB on APTOS, generate Grad-CAM for each DR grade. Target: 82–84% accuracy with interpretable heatmaps.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed dual attention mechanism be extended to classify specific lesion types (e.g., microaneurysms, hemorrhages) rather than solely providing a general severity grade? The current model is designed as a grading system (DR0–DR4) and does not include output nodes or loss functions for multi-label lesion classification.

### Open Question 2
To what extent can neural diffusion models generate high-quality synthetic fundus images to improve the generalization of DR grading models? The current study relies on standard geometric augmentations and existing imbalanced datasets; the utility of generative diffusion models for this specific architecture remains untested.

### Open Question 3
Does the reliance on image-level supervision limit the model's ability to accurately pinpoint minute lesion regions compared to bounding-box or pixel-level supervision? While Grad-CAM visualizations suggest the model focuses on relevant areas, the precision of these localizations is not quantitatively evaluated against ground-truth coordinates in the current study.

## Limitations
- The CAB module's category attention mechanism is ambiguously described: Equation 5 produces a scalar class score, but Equation 6 requires a spatial attention map for multiplication.
- No ablation study confirms the specific GAB→CAB ordering advantage over alternatives (CAB→GAB or parallel).
- Performance metrics are reported only on two datasets (APTOS, EyePACS) without cross-dataset generalization validation.

## Confidence
- **High**: Backbone selection and dataset preprocessing details are clearly specified.
- **Medium**: Dual attention mechanism design (GAB+CAB) is described but lacks complete mathematical specification for CAB.
- **Low**: Claims about class imbalance mitigation via CAB require external validation due to unclear implementation details.

## Next Checks
1. Implement CAB attention map generation: Resolve the mathematical ambiguity in converting scalar class scores to spatial attention maps. Verify by visualizing attention outputs during training.
2. Ablation study on attention ordering: Train models with CAB→GAB and parallel arrangements. Compare performance to confirm the claimed superiority of GAB→CAB ordering.
3. Per-class F1-score analysis: Compute F1-scores for each DR grade (DR0-DR4) on validation set. Identify if minority classes (DR3, DR4) suffer from poor recall despite overall high accuracy.