---
ver: rpa2
title: A Practical Adversarial Attack against Sequence-based Deep Learning Malware
  Classifiers
arxiv_id: '2509.11836'
source_url: https://arxiv.org/abs/2509.11836
tags:
- sequence
- sequences
- adversarial
- behavior
- behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a practical adversarial attack method against
  sequence-based deep learning malware classifiers. The approach uses a Deep Q-Network
  and a heuristic backtracking search strategy to generate perturbation sequences
  that satisfy real-world constraints.
---

# A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers

## Quick Facts
- arXiv ID: 2509.11836
- Source URL: https://arxiv.org/abs/2509.11836
- Reference count: 40
- Primary result: Adversarial attack against sequence-based malware classifiers achieves 64.4% success on AndroCT and 59.1% on ADFA-LD datasets

## Executive Summary
This paper presents a practical adversarial attack method targeting sequence-based deep learning malware classifiers. The approach combines a Deep Q-Network (DQN) with heuristic backtracking search to generate perturbation sequences that evade detection while maintaining malware functionality. A novel transformation technique maps these sequence modifications back to source code, enabling practical attacks without direct manipulation of behavior logs. The method was evaluated on public datasets, demonstrating significant evasion capabilities against various target models including LSTM, CNN, Transformer, and Autoencoder architectures.

## Method Summary
The attack methodology employs a DQN to generate perturbation sequences that modify malware behavior patterns. A heuristic backtracking search strategy guides the DQN to find effective perturbations while ensuring they satisfy real-world constraints. The key innovation is a transformation approach that maps sequence modifications back to the source code level using LLVM IR and debug information, avoiding the need for direct behavior log manipulation. This enables the generation of practical adversarial samples that can be compiled and executed while maintaining their malicious functionality. The approach was tested on AndroCT and ADFA-LD datasets against multiple deep learning models.

## Key Results
- Achieved 64.4% attack success rate on AndroCT dataset
- Achieved 59.1% attack success rate on ADFA-LD dataset
- Successfully evaded LSTM, CNN, Transformer, and Autoencoder models

## Why This Works (Mechanism)
The attack exploits the vulnerability of deep learning classifiers to adversarial examples by systematically modifying the sequence patterns that characterize malware behavior. The DQN learns to identify which perturbations most effectively reduce classification confidence while the backtracking search ensures constraints are met. The transformation from sequence space to source code enables practical implementation of the attack in real-world scenarios where malware source code is available.

## Foundational Learning
- Deep Q-Networks: Reinforcement learning approach needed for sequential decision-making in perturbation generation; quick check: understand Q-learning update rule
- Heuristic backtracking search: Constraint satisfaction technique for ensuring perturbation validity; quick check: understand constraint propagation
- LLVM IR transformation: Intermediate representation enabling source-to-sequence mapping; quick check: understand basic LLVM compilation pipeline
- Behavior sequence analysis: Understanding how malware actions are represented as sequences; quick check: review API call sequence features
- Adversarial perturbation: Small modifications that cause misclassification; quick check: understand perturbation constraints
- Sequence-to-source mapping: Technique for converting behavioral modifications back to code; quick check: understand debug information usage

## Architecture Onboarding

Component Map:
Source Code -> LLVM IR -> Sequence Representation -> DQN Perturbation -> Backtracking Search -> Modified Sequence -> Source Code Transformation -> Adversarial Sample

Critical Path:
1. Source code compilation to LLVM IR
2. Sequence extraction from IR
3. DQN-based perturbation generation
4. Backtracking search for constraint satisfaction
5. Transformation back to source code
6. Compilation of adversarial sample

Design Tradeoffs:
- Source code availability requirement vs. binary-only attacks
- Perturbation effectiveness vs. functionality preservation
- Computational cost of DQN training vs. attack success rate
- Sequence modification granularity vs. detection evasion

Failure Signatures:
- Failed compilations after source code transformation
- Loss of malware functionality in modified samples
- DQN convergence issues during training
- Backtracking search timeout on complex constraints

First Experiments:
1. Verify LLVM IR extraction from simple malware samples
2. Test basic sequence perturbation without DQN on sample dataset
3. Validate source code transformation on unmodified sequences

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can continuous learning techniques be integrated to maintain attack success rates against detection models that are frequently retrained?
- Basis in paper: [explicit] The authors state the approach may need "continual updates" to keep pace with evolving malware tactics and propose "incorporating continuous learning techniques" in future work.
- Why unresolved: The current DQN model is trained on a fixed dataset and surrogate model; it does not possess an online learning mechanism to adapt to drift in the target classifier's decision boundary.
- What evidence would resolve it: An evaluation showing the DQN agent maintaining stable evasion success rates against a target model that undergoes periodic adversarial retraining.

### Open Question 2
- Question: Does the perturbation strategy generalize effectively to more advanced or hybrid detection architectures not tested in the paper?
- Basis in paper: [explicit] The authors acknowledge that performance "might vary when applied to other advanced classifiers" and list exploring "more detection models" as a future work goal.
- Why unresolved: The experiments were limited to LSTM, CNN, Transformer, and Autoencoder models; the method's efficacy against hybrid models or attention-based classifiers with different structural constraints is unknown.
- What evidence would resolve it: Experimental results quantifying the success rate and transferability of the generated adversarial sequences against Graph Neural Networks or hybrid detection ensembles.

### Open Question 3
- Question: Can the LLVM-based code mapping technique be adapted to generate practical adversarial samples when the attacker only has access to compiled binaries?
- Basis in paper: [inferred] The paper explicitly assumes the attacker has the malware source code to compile LLVM IR and use debug information for mapping (Section 3.5), a condition that may not always hold in real-world scenarios.
- Why unresolved: Binary rewriting is significantly more complex than source-level transformation, and the automated insertion of semantically valid benign behaviors in binary code remains an unaddressed challenge.
- What evidence would resolve it: A prototype demonstrating the successful insertion of adversarial behaviors directly into binary executables without source code, verified by dynamic analysis.

## Limitations
- Requires source code availability, limiting real-world applicability
- Attack success rates remain below 65% on tested datasets
- Heuristic backtracking may struggle with longer or more complex sequences
- Functionality preservation verification is challenging and may miss subtle changes

## Confidence
- High: The technical implementation of the DQN-based perturbation generation and backtracking search is well-described and reproducible
- Medium: The success rates are accurately reported for the tested datasets and models
- Medium: The transformation approach from sequence perturbations to source code modifications is plausible but may face practical limitations
- Low: Generalization to other malware types, datasets, or model architectures beyond those tested

## Next Checks
1. Test the attack on compiled malware samples rather than source code to assess real-world applicability
2. Evaluate against additional deep learning architectures including transformers and ensemble methods
3. Conduct comprehensive functional testing of modified samples across diverse device configurations and usage scenarios