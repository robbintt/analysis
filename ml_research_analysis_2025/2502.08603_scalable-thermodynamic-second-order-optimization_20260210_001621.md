---
ver: rpa2
title: Scalable Thermodynamic Second-order Optimization
arxiv_id: '2502.08603'
source_url: https://arxiv.org/abs/2502.08603
tags:
- thermodynamic
- k-fac
- matrix
- hardware
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a scalable algorithm for employing thermodynamic
  computers to accelerate Kronecker-factured approximate curvature (K-FAC), a popular
  second-order optimizer for neural networks. The key insight is that thermodynamic
  computers can efficiently solve linear systems and invert matrices, which are computational
  bottlenecks in K-FAC.
---

# Scalable Thermodynamic Second-order Optimization

## Quick Facts
- arXiv ID: 2502.08603
- Source URL: https://arxiv.org/abs/2502.08603
- Reference count: 40
- Key outcome: Offloading K-FAC matrix inversion to thermodynamic hardware reduces per-iteration complexity from cubic to quadratic, enabling large-scale second-order optimization.

## Executive Summary
This paper proposes a method to accelerate Kronecker-factored approximate curvature (K-FAC), a second-order optimizer, by offloading its matrix inversion bottleneck to thermodynamic computers. The key insight is that thermodynamic hardware can efficiently solve linear systems and invert matrices through physical relaxation dynamics, bypassing the iterative steps of digital algorithms. By leveraging the block-diagonal structure of K-FAC, the approach scales to large neural networks, achieving theoretical speedups that grow linearly with network width. Experiments on vision and graph problems demonstrate that the benefits of second-order optimization can be preserved even under significant quantization noise from analog hardware.

## Method Summary
The method accelerates K-FAC by using thermodynamic hardware to solve linear systems and invert matrices, which are computational bottlenecks in the standard digital implementation. K-FAC approximates the Fisher matrix as a block-diagonal matrix of Kronecker products, reducing the problem size to layer width dimensions. The thermodynamic solver, modeled as a system of coupled oscillators under noise, relaxes to a steady state whose covariance encodes the matrix inverse. A conservative quantization scheme ensures positive semi-definiteness of input matrices to prevent solver instability. The algorithm maintains convergence quality despite low-precision quantization, validated through numerical experiments on ViT/ImageNet and GNN/ogbg-molpcba.

## Key Results
- Offloading K-FAC matrix inversion to thermodynamic hardware reduces per-iteration complexity from cubic ($O(n^3)$) to approximately quadratic ($O(n^2\kappa^2)$).
- Even with 8-bit quantization, the algorithm remains competitive with full-precision Adam, preserving second-order optimization benefits.
- The block-diagonal structure of K-FAC enables practical implementation on thermodynamic hardware with thousands of oscillators, scaling to large networks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offloading matrix inversion from digital to thermodynamic hardware reduces per-iteration complexity from cubic ($O(n^3)$) to approximately quadratic ($O(n^2\kappa^2)$).
- Mechanism: K-FAC's computational bottleneck is inverting Kronecker factors $A_\ell$ and $G_\ell$. Thermodynamic computers solve linear systems by allowing a coupled system of harmonic oscillators to relax to equilibrium. The covariance of the steady-state distribution of the oscillator system's state vector corresponds to the inverse matrix. This physical relaxation bypasses the iterative algorithmic steps required in digital Cholesky or SVD decompositions.
- Core assumption: The time for the physical system to thermalize scales favorably compared to digital inversion. The matrix condition number $\kappa$ does not increase so sharply with network width $n$ that it negates the complexity reduction.
- Evidence anchors:
  - [abstract] "thermodynamic computers can efficiently solve linear systems and invert matrices, which are computational bottlenecks in K-FAC."
  - [Page 7, Table 1] Compares runtime complexities: K-FAC is $O(bn^2 + n^3)$, Thermodynamic K-FAC is $O(bn^2 + n^2\kappa^2)$.
  - [Page 6-7, Section 4.2] Describes mapping the matrix to an SDE: "the second moment of this distribution is proportional to $M^{-1}$".
  - [corpus] Related work "Thermodynamic linear algebra" (cited as [9] in paper) provides foundational primitives.
- Break condition: If the condition number $\kappa$ scales as $O(n)$ or worse, the advantage over digital inversion disappears. If the hardware time constant (relaxation time) is large, constant-factor speedups are lost.

### Mechanism 2
- Claim: The block-diagonal approximation of K-FAC enables scalability to large networks by limiting matrix dimensions to layer width, making thermodynamic hardware requirements practical.
- Mechanism: Standard Natural Gradient Descent (NGD) requires inverting a massive $N \times N$ Fisher matrix (where $N$ is total parameters). This would require a thermodynamic device with billions of oscillators, which is infeasible. K-FAC approximates the Fisher matrix with a block-diagonal structure, where blocks are $n \times n$ (layer width). This reduces the required thermodynamic device size to thousands of oscillators, enabling scalable fabrication and operation.
- Core assumption: The K-FAC approximation is sufficiently accurate for the target neural network architectures and training tasks.
- Evidence anchors:
  - [Page 2, Section 1] "Crucially, the block-diagonal nature of K-FAC allows for practical implementation on thermodynamic hardware, as the matrices involved have dimension on the order of a thousand rather than a billion."
  - [Page 4, Section 3.2] Derives the K-FAC update rule using Kronecker factors.
  - [corpus] "Accelerated Training of Federated Learning via Second-Order Methods" confirms the general viability and challenges of second-order methods, though in a different domain.
- Break condition: If K-FAC's assumptions (block-diagonal Fisher, Kronecker-factored structure) are invalid for certain architectures (e.g., very deep or recurrent networks with strong inter-layer correlations), convergence will be poor regardless of hardware speed.

### Mechanism 3
- Claim: The proposed algorithm maintains convergence quality despite the low-precision quantization inherent to analog hardware.
- Mechanism: Analog hardware introduces noise from quantization (A/D conversion) and device mismatch. The method employs a "conservative quantizer" that shifts diagonal elements to guarantee input matrices remain positive semi-definite, preventing solver instability. Experiments with simulated quantization noise show that the algorithm preserves second-order benefits, remaining competitive with full-precision Adam even at 8-bit precision.
- Core assumption: Simulated quantization noise is a representative proxy for actual hardware noise sources.
- Evidence anchors:
  - [abstract] "Numerical experiments demonstrate that even under significant quantization noise, the benefits of second-order optimization can be preserved."
  - [Page 8-9, Section 4.5] Details the "diagonal-dominant quantizer" to handle input quantization.
  - [Page 10, Section 5.2] "even an 8-bit K-FAC optimizer is competitive against a full precision Adam optimizer."
  - [corpus] No direct corpus evidence on noise robustness of this specific algorithm.
- Break condition: If physical device mismatch creates non-Gaussian, structured noise not captured by the quantization model, solver outputs may be biased, degrading training.

## Foundational Learning

- Concept: **Kronecker-Factored Approximate Curvature (K-FAC)**
  - Why needed here: The entire paper is predicated on accelerating K-FAC. Without understanding that K-FAC approximates the Fisher information matrix as a block-diagonal matrix of Kronecker products ($A_\ell \otimes G_\ell$), the motivation for using thermodynamic solvers on these specific smaller matrices is unclear.
  - Quick check question: How does K-FAC's computational complexity per layer scale with layer width $n$, and what is the key approximation it makes about the Fisher matrix?

- Concept: **Stochastic Differential Equations (SDEs) and Ornstein-Uhlenbeck (OU) Processes**
  - Why needed here: The thermodynamic hardware's computational primitive is the physical dynamics of a damped system under noise, modeled by an OU process. Understanding that the stationary distribution of this process contains the desired linear algebra solution is essential for grasping how the hardware "computes."
  - Quick check question: In the context of the thermodynamic solver, what information about the matrix $M$ is contained in the covariance of the steady-state distribution of the OU process $dx = -(Mx - b)dt + \mathcal{N}(0, 2\beta^{-1}dt)$?

- Concept: **Positive Semi-Definite (PSD) Matrices**
  - Why needed here: The thermodynamic solver's physical stability requires the input matrix (coupling matrix) to be PSD. Quantization can violate this condition, which is why the paper introduces a special quantization scheme to restore PSD-ness. This constraint is central to the algorithm's error handling.
  - Quick check question: Why must the matrix loaded onto the thermodynamic solver be positive semi-definite, and what can happen if this condition is violated during quantization?

## Architecture Onboarding

- Component map:
  1. **Digital Frontend (GPU/CPU):** Runs the standard forward/backward pass of the neural network, computes gradients ($\nabla_\theta L$), and constructs the Kronecker factors ($A_\ell, G_\ell$) from activations and errors.
  2. **Interface/Digital-to-Analog Converters (DACs):** Quantizes and loads the computed Kronecker factors (or the raw activations/gradients for a more integrated design) into the analog hardware. This stage applies the "conservative quantizer" to ensure matrix definiteness.
  3. **Thermodynamic Solver (SPU):** A system of coupled, noisy oscillators (e.g., RC circuits). Its dynamics are programmed by the Kronecker factors. It equilibrates to a state whose mean and covariance encode the solution to the linear system.
  4. **Readout/Analog-to-Digital Converters (ADCs):** Samples the steady-state distribution of the solver to estimate the solution vector (or matrix inverse) with sufficient precision.
  5. **Update Computation:** Uses the solver's output to compute the natural gradient and update the network weights.

- Critical path:
  1. Compute Kronecker factors $A_\ell, G_\ell$ for each layer on the digital device.
  2. Apply conservative quantization scheme to $A_\ell, G_\ell$ to ensure they remain positive semi-definite.
  3. Load quantized matrices into the thermodynamic solver.
  4. Allow solver to equilibrate for time $\tau \approx RC$.
  5. Sample the solver's state and compute sample mean/covariance.
  6. Use the result to compute the weight update $\Delta \theta_\ell$ according to the K-FAC update rule.

- Design tradeoffs:
  - **Precision vs. Speed:** Lower bit-depth DACs/ADCs increase quantization noise, potentially harming convergence, but reduce interface cost and may be faster.
  - **Factor Computation Location:** Computing Kronecker factors digitally ($O(n^2)$ memory) vs. sending raw activations/gradients to the solver. The digital approach is simpler and supports EMA smoothing, but has higher memory overhead.
  - **Method 1 (Inversion) vs. Method 2 (Linear Systems):** The inversion method may be simpler to implement by estimating the full covariance matrix, but the linear systems method (solving $Gx = b$ for each column) avoids matrix-matrix multiplication and can be more efficient.

- Failure signatures:
  - **Non-PSD Input:** The thermodynamic system becomes unstable and fails to converge to a steady-state distribution. This is detected by observing large, diverging oscillations in the solver.
  - **Slow Thermalization:** If the matrix condition number $\kappa$ is very high, the relaxation time may exceed the allocated time slot, yielding an inaccurate solution from an out-of-equilibrium sample.
  - **High Quantization Noise:** Training loss plateaus at a higher value or diverges, directly correlated with reduced ADC/DAC bit depth.

- First 3 experiments:
  1. **Quantization Threshold Test:** Train a small CNN (e.g., ResNet on CIFAR-10) with simulated quantization on the Kronecker factors. Sweep ADC/DAC bit depth from 4 to 16 bits to find the minimum precision required to match floating-point K-FAC performance.
  2. **Scaling Validation:** Profile the per-iteration runtime for a standard digital K-FAC implementation on a ViT or GPT model while scaling model width ($n$). Compare the measured inversion time against the paper's predictions (Fig. 2) to confirm the $O(n^3)$ bottleneck and identify the cross-over point where it dominates.
  3. **End-to-End Algorithm Test on Emulated Hardware:** Implement a software emulator for the thermodynamic solver that integrates the SDE dynamics (Eq. 22) with added quantization noise. Integrate this into the K-FAC update loop and train a small model to verify that the full algorithm converges and achieves the predicted speedup over a baseline Adam optimizer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can thermodynamic devices with non-quadratic potential energies outperform second-order methods by reducing optimization iterations?
- Basis in paper: [explicit] The conclusion and Appendix A state that extending the approach to non-quadratic potentials "remains an important direction for future work."
- Why unresolved: The current work restricts the hardware mapping to quadratic potentials to approximate the K-FAC objective.
- What evidence would resolve it: A demonstration of a thermodynamic device implementing higher-order energy functions that achieves convergence in fewer steps than K-FAC.

### Open Question 2
- Question: Does the linear systems method (Method 2) provide superior efficiency over the inversion method (Method 1) on thermodynamic hardware?
- Basis in paper: [explicit] Section 4.1 notes Method 2 "could be more efficient" and avoids explicit matrix construction, but the authors used Method 1 for experiments.
- Why unresolved: The assertion that Method 2 is more efficient is an expectation that was not empirically verified in the numerical experiments.
- What evidence would resolve it: Comparative benchmarks of runtime and memory usage for both methods on a physical or simulated thermodynamic processing unit.

### Open Question 3
- Question: How does the communication latency between digital processors and thermodynamic solvers affect the theoretical speedups in practical training pipelines?
- Basis in paper: [explicit] The conclusion suggests "optimizing the interaction between digital and thermodynamic components could further improve the practicality" of the method.
- Why unresolved: The paper focuses on asymptotic complexity and simulated speedups, potentially obscuring constant-factor overheads from data transfer.
- What evidence would resolve it: End-to-end system measurements quantifying the wall-clock time spent on data transfer relative to thermodynamic computation.

## Limitations

- The projected speedups rely on the assumption that the condition number $\kappa$ of Kronecker factors scales sublinearly with layer width; if $\kappa = O(n)$ or worse, the advantage disappears.
- The conservative quantization scheme ensures positive semi-definiteness but may introduce bias not characterized in the paper.
- The analysis assumes a fixed, favorable hardware time constant (1Î¼s relaxation time), but real devices may exhibit device-to-device variation or noise sources not captured by the simulated quantization model.

## Confidence

- **High**: The fundamental mechanism of using thermodynamic computers for matrix inversion is well-established in prior work (cited as [9]). The K-FAC approximation and its computational bottleneck are well-understood. The experimental results on large-scale vision and graph problems are valid demonstrations of the algorithm's viability.
- **Medium**: The projected speedups are based on reasonable assumptions about hardware parameters and scaling laws, but rely on estimates of the condition number scaling and the exact behavior of the thermodynamic hardware under load. The noise robustness claims are supported by simulations but not by hardware measurements.
- **Low**: The long-term scaling behavior of the condition number $\kappa$ with network width is not empirically established. The generalizability to other architectures (RNNs, transformers with complex attention) where K-FAC's assumptions may break is untested.

## Next Checks

1. **Condition Number Scaling Study**: Profile the condition numbers of the Kronecker factors $A_\ell$ and $G_\ell$ across layers and model widths in the ViT architecture. Plot $\kappa$ vs. $n$ to empirically verify that it remains sublinear (e.g., $\kappa = O(n^{0.5})$) to justify the projected speedup.
2. **Physical Hardware Noise Characterization**: Build or access a prototype thermodynamic solver and measure its noise characteristics under realistic operating conditions (varying temperatures, supply voltages, fabrication variations). Compare these measurements against the simulated quantization noise model used in the paper to validate the noise robustness claims.
3. **Architecture Generalization Test**: Apply the thermodynamic K-FAC algorithm to a recurrent neural network (e.g., LSTM on Penn Treebank) or a transformer with complex attention patterns. Evaluate whether K-FAC's Kronecker approximation holds and whether the thermodynamic solver provides a speedup compared to digital K-FAC or Adam in this setting.