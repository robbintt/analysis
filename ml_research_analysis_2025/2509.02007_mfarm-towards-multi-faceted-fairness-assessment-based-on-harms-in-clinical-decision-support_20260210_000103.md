---
ver: rpa2
title: 'mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical
  Decision Support'
arxiv_id: '2509.02007'
source_url: https://arxiv.org/abs/2509.02007
tags:
- fairness
- base
- group
- score
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces mFARM, a multi-faceted fairness framework
  for auditing demographic bias in clinical LLMs across allocational, stability, and
  latent harm dimensions. It constructs two large-scale benchmarks (ED-Triage and
  Opioid Recommendation) from MIMIC-IV with 50K+ prompts spanning 12 race-gender variants
  and three context tiers.
---

# mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support

## Quick Facts
- arXiv ID: 2509.02007
- Source URL: https://arxiv.org/abs/2509.02007
- Authors: Shreyash Adappanavar; Krithi Shailya; Gokul S Krishnan; Sriraam Natarajan; Balaraman Ravindran
- Reference count: 19
- Introduces mFARM framework for multi-faceted fairness assessment in clinical LLMs across allocational, stability, and latent harm dimensions

## Executive Summary
mFARM is a comprehensive framework designed to audit demographic bias in clinical large language models (LLMs) by evaluating three harm dimensions: allocational (differential resource allocation), stability (consistent behavior across contexts), and latent (hidden adverse outcomes). The framework constructs two large-scale benchmarks from MIMIC-IV data with over 50,000 prompts spanning 12 race-gender variants across three context tiers. Five independent fairness metrics are combined into an mFARM score, complemented by a Fairness-Accuracy Balance (FAB) score. The evaluation reveals that context reduction significantly harms fairness, particularly in low-resource settings, while quantization improves fairness without degrading performance. Code and benchmarks are publicly released.

## Method Summary
mFARM evaluates clinical LLM fairness through a multi-metric approach that captures allocational, stability, and latent harms across demographic subgroups. The framework constructs synthetic prompts from real clinical data, generating 12 demographic variants (race × gender combinations) across three context tiers: full, partial, and minimal. Five fairness metrics—Mean Difference, Variance Heterogeneity, Absolute Deviation, KS Distributional, and Correlation Difference—are combined into a comprehensive mFARM score. The framework also introduces a FAB score to balance fairness and accuracy trade-offs. The benchmarks cover emergency department triage and opioid recommendation tasks, enabling systematic evaluation of demographic bias across different clinical scenarios and model configurations.

## Key Results
- Context reduction significantly degrades fairness across all harm dimensions, particularly in low-resource clinical settings
- Quantization improves fairness scores without compromising accuracy, suggesting hardware optimization can enhance equitable performance
- LoRA fine-tuning improves both accuracy and fairness compared to full fine-tuning, demonstrating efficient bias mitigation
- Absolute Deviation and Variance Heterogeneity metrics are most sensitive to demographic disparities, while KS statistic effectively captures distributional differences

## Why This Works (Mechanism)

## Foundational Learning
- **Fairness metrics computation**: Measures statistical differences in model outputs across demographic groups (why needed: to quantify bias; quick check: verify metric sensitivity to known disparities)
- **Synthetic prompt generation**: Creates controlled scenarios with varying demographic attributes (why needed: to systematically test bias across groups; quick check: validate prompt diversity and clinical relevance)
- **Context tier stratification**: Separates prompts by information completeness (why needed: to assess fairness under different clinical information scenarios; quick check: confirm tier separation preserves task difficulty)
- **HARMs framework**: Categorizes harms into allocational, stability, and latent dimensions (why needed: to comprehensively capture different types of bias impacts; quick check: validate harm categories align with clinical outcomes)
- **FAB scoring**: Balances fairness and accuracy trade-offs (why needed: to guide model selection in practical deployment; quick check: test sensitivity to different weightings)

## Architecture Onboarding

**Component Map**: Synthetic Data Generator -> Context Tiers -> Fairness Metrics -> mFARM Score -> FAB Score -> Model Evaluation

**Critical Path**: Synthetic prompt generation → context tier assignment → demographic variant creation → fairness metric computation → mFARM aggregation → FAB balance calculation → model comparison

**Design Tradeoffs**: Synthetic data provides controlled testing but may not capture all real-world complexities; multiple metrics provide comprehensive assessment but increase computational overhead; FAB score balances competing objectives but requires careful weighting

**Failure Signatures**: Context reduction causing fairness degradation indicates information incompleteness bias; high variance across demographic groups signals allocational harm; distributional shifts suggest latent harm patterns

**3 First Experiments**:
1. Evaluate baseline model fairness across all three context tiers to establish degradation patterns
2. Test quantization impact on fairness scores while monitoring accuracy retention
3. Compare LoRA vs full fine-tuning on fairness-accuracy trade-off using FAB score

## Open Questions the Paper Calls Out
None

## Limitations
- Framework relies on synthetic prompt generation which may not fully capture real-world clinical complexity
- Evaluation limited to two specific clinical tasks and four open-source models, potentially limiting generalizability
- Focuses primarily on explicit demographic attributes without addressing intersectional identities or other protected characteristics
- Real-world deployment testing needed to validate theoretical harm detection capabilities

## Confidence

**High Confidence**: Multi-metric fairness assessment approach is well-supported; context sensitivity findings are robust; quantization benefits are empirically demonstrated

**Medium Confidence**: Generalizability across clinical domains requires further validation; FAB score effectiveness needs broader testing

**Low Confidence**: Framework's ability to capture subtle latent harms in real clinical workflows remains theoretical without extensive deployment testing

## Next Checks

1. Evaluate mFARM on additional clinical tasks (e.g., diagnostic imaging interpretation, treatment planning) to assess generalizability across medical specialties

2. Conduct longitudinal studies to monitor fairness stability over time as models encounter diverse patient populations in real clinical settings

3. Extend framework to include intersectional demographic attributes and test effectiveness in capturing compound biases affecting minority subgroups