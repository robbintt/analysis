---
ver: rpa2
title: Privacy Auditing Synthetic Data Release through Local Likelihood Attacks
arxiv_id: '2508.21146'
source_url: https://arxiv.org/abs/2508.21146
tags:
- data
- synthetic
- gen-lra
- privacy
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Generative Likelihood Ratio Attack (Gen-LRA),
  a membership inference attack for auditing privacy leakage in synthetic tabular
  data release. The method exploits overfitting in generative models by measuring
  the influence of a test observation on the likelihood ratio of synthetic data under
  surrogate density estimators trained with and without that observation.
---

# Privacy Auditing Synthetic Data Release through Local Likelihood Attacks

## Quick Facts
- arXiv ID: 2508.21146
- Source URL: https://arxiv.org/abs/2508.21146
- Reference count: 40
- This paper proposes Generative Likelihood Ratio Attack (Gen-LRA), a membership inference attack for auditing privacy leakage in synthetic tabular data release, which consistently outperforms competing attacks across diverse datasets and nine generative model architectures.

## Executive Summary
This paper introduces Gen-LRA, a membership inference attack framework for auditing privacy leakage in synthetic tabular data generated by various models including CTGAN, TVAE, Tab-DDPM, and Bayesian Networks. The method measures the influence of a test observation on the likelihood ratio of synthetic data under surrogate density estimators trained with and without that observation, exploiting overfitting in generative models. Evaluated across 15 datasets and nine generative architectures, Gen-LRA consistently achieves superior AUC-ROC performance, particularly at low false positive rates, demonstrating its effectiveness for characterizing privacy-utility tradeoffs in synthetic data generation.

## Method Summary
Gen-LRA operates by encoding heterogeneous tabular data ordinally, then fitting Gaussian Kernel Density Estimators (KDE) with Silverman's Rule bandwidth on a reference dataset R and R ∪ {x*} for each test point x*. The attack computes the log-likelihood ratio of the k-nearest neighbors in synthetic data S under these two KDEs, using this influence score to infer membership. The method localizes the attack to k-nearest neighbors in S to amplify the membership signal, and evaluates performance using AUC-ROC and TPR@FPR metrics. The framework assumes access to a reference dataset R independent of the training data, making it suitable for release auditing scenarios.

## Key Results
- Gen-LRA achieves an average rank of 1 across nine generative model architectures in AUC-ROC comparisons
- The attack demonstrates superior detection of privacy leakage, especially at low false positive rates (FPR ∈ {0.001, 0.01})
- Gaussian KDE outperforms deep learning-based density estimators (e.g., BNAF) for this task, particularly in identifying extreme leakage in models like Bayesian Network and Tab-DDPM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Measuring influence of a test observation on synthetic data likelihood exposes overfitting-based privacy leakage.
- Mechanism: If x* was in training data and the generator overfit, adding x* to a surrogate density estimator trained on R should increase the estimated likelihood of synthetic data S. The attack computes Î(x*; R, S) = log p̂(S | R ∪ {x*}) − log p̂(S | R) as the scoring function.
- Core assumption: Generative models overfit to training points, and this overfitting is reflected as elevated density in specific regions of S.
- Evidence anchors:
  - [abstract] "measuring the influence of a test observation on the likelihood ratio of synthetic data under surrogate density estimators trained with and without that observation"
  - [Section 3.2] Eq. 3 formalizes the influence function as the log-likelihood difference
  - [corpus] Synth-MIA (arXiv:2509.18014) confirms MIAs as the principal framework for auditing synthetic tabular data privacy
- Break condition: If generative model does not overfit (e.g., strong differential privacy guarantees), likelihood ratio approaches zero; attack degrades to random guessing.

### Mechanism 2
- Claim: Localization to k-nearest neighbors amplifies membership signal compared to global likelihood.
- Mechanism: Global likelihood over all of S dilutes subtle local overfitting signals. By computing the influence score over only the k-nearest neighbors in S to x*, the attack targets the region where overfitting manifests.
- Core assumption: Overfitting is inherently localized—training points influence nearby regions of the synthetic distribution more than distant ones.
- Evidence anchors:
  - [Section 3.3] "we localize Gen-LRA by only considering the k-nearest elements in S to x* in our estimation"
  - [Appendix D.2, Table 4] Shows k=N (global) yields AUC-ROC = 0.500 across all models, while k=1 to k=10 achieves meaningfully higher AUC
  - [corpus] Ensembling MIAs paper (arXiv:2509.05350) notes diverse methods exploit different leakage signals; localization is one such signal
- Break condition: If k is too large, signal degrades; if k is too small, estimator variance increases. Paper finds k=10 robust across experiments.

### Mechanism 3
- Claim: Gaussian KDE outperforms deep density estimators for detecting single-point influence on local regions.
- Mechanism: KDE fits locally via bandwidth parameter, making it sensitive to density changes from adding one point. Deep learning estimators (e.g., BNAF) learn global distributions with many hyperparameters, potentially smoothing over local changes.
- Core assumption: The influence of a single point on a density estimate is better captured by methods with explicit local fitting behavior.
- Evidence anchors:
  - [Section 6.3, Figure 3] KDE outperforms BNAF for both Gen-LRA and DOMIAS; BNAF fails to identify extreme leakage in Bayesian Network and Tab-DDPM
  - [Section 3.3] "KDEs...achieve state of the art results"
  - [corpus] No direct corpus contradiction; related work does not specifically compare KDE vs. deep density estimators for this task
- Break condition: On very high-dimensional data, KDE's curse of dimensionality may degrade performance; alternative encoding (PCA, VAE latent space) may be required (see Appendix D.1).

## Foundational Learning

- **Influence Functions (Robust Statistics)**
  - Why needed here: Gen-LRA is fundamentally an empirical influence function measuring how adding x* changes the density estimate over S. Without this concept, the scoring function appears arbitrary.
  - Quick check question: If you add a point to a dataset and re-fit an estimator, what does a large change in the estimate imply about that point's relationship to the original data?

- **Likelihood Ratio Testing**
  - Why needed here: The attack score is a log-likelihood ratio. Understanding why ratios are invariant to invertible transformations (Theorem 3.1) and how they concentrate evidence is essential.
  - Quick check question: Why compare likelihoods under two hypotheses (x* ∈ T vs. x* ∉ T) rather than using a single density estimate?

- **Threat Models in Privacy Auditing**
  - Why needed here: Gen-LRA operates under a "No-Box" threat model—no model access, only synthetic data S and reference R. This constrains what attacks are realistic for release auditing.
  - Quick check question: If you had white-box access to the generator, what additional attack surfaces would be available? Why does the paper intentionally exclude them?

## Architecture Onboarding

- **Component map:**
  - Input: Test observation x*, synthetic dataset S, reference dataset R
  - Surrogate density estimators: Fit KDE on R and R ∪ {x*}
  - Localization: Find k-nearest neighbors in S to x*
  - Scoring: Compute log-likelihood ratio over localized S using both estimators
  - Decision: Compare score to threshold γ; if Î > γ, predict membership

- **Critical path:**
  1. Encode data ordinally (ordinal encoding for categorical variables, scaled numeric)
  2. Fit KDE on R with Silverman's rule bandwidth
  3. For each test point, fit KDE on R ∪ {x*}, retrieve k-nearest S neighbors, compute influence score
  4. Aggregate scores and evaluate via AUC-ROC or TPR@FPR

- **Design tradeoffs:**
  - k (locality): Lower k increases sensitivity but raises variance; paper uses k=10 as default
  - Density estimator: KDE is fast and effective locally; deep estimators may scale better to high dimensions but underperform empirically
  - Encoding: Ordinal generally works; PCA and VAE latent space are alternatives (Appendix D.1 shows no dominant strategy)
  - Assumption: Requires reference dataset R independent of training set; may not always be available

- **Failure signatures:**
  - AUC-ROC ≈ 0.5: Either model has no overfitting (e.g., DP-trained), or attack misconfigured (k too large, encoding issues)
  - High variance across seeds: Small sample sizes (N=250) or heterogeneous datasets with mixed-type features
  - KDE fitting failures: Categorical variables not properly encoded; bandwidth estimation fails on sparse regions

- **First 3 experiments:**
  1. Replicate Tab-DDPM results from Table 1 on a single dataset (e.g., Adult) with k=10, ordinal encoding, and Silverman KDE; verify AUC-ROC in ~0.60 range
  2. Ablate k across {1, 5, 10, 20, N} to confirm localization effect; expect k=N to yield AUC=0.5 and k=1-10 to show elevated AUC
  3. Compare KDE vs. BNAF density estimation on a model with known high leakage (Bayesian Network); verify KDE better captures extreme leakage per Figure 3

## Open Questions the Paper Calls Out

- **Open Question 1**: Do modern deep density estimation methodologies outperform Gaussian KDE for Gen-LRA specifically on high-dimensional tabular datasets?
  - Basis in paper: [explicit] The conclusion explicitly states, "Exploring emerging density estimation methodologies would likely yield better empirical performance, especially on high dimensional datasets."
  - Why unresolved: The experiments demonstrate that Gaussian KDE outperforms deep learning methods like BNAF on the tested benchmarks, but the authors note that KDE generally struggles with high-dimensional data and heterogeneous encodings.
  - What evidence would resolve it: A benchmark comparison on datasets with significantly higher feature dimensions using advanced estimators (e.g., normalizing flows or score-based models), showing improved AUC-ROC over the KDE baseline.

- **Open Question 2**: Can the likelihood influence signal utilized by Gen-LRA be reverse-engineered to create interpretability techniques that explain model overfitting?
  - Basis in paper: [explicit] Section 7 suggests that "research into developing adversarial techniques to better understand model overfitting in general could also lead to important interpretability techniques."
  - Why unresolved: The paper focuses on the detection of privacy leakage but does not explore the mechanistic interpretation of that leakage to explain why a model overfits to specific training observations.
  - What evidence would resolve it: A methodological framework that correlates high Gen-LRA scores with specific generative model weights or training data characteristics, providing explanatory power beyond binary membership inference.

- **Open Question 3**: Is there a theoretically grounded or adaptive strategy for determining the localization parameter k and KDE bandwidth that eliminates the need for empirical tuning?
  - Basis in paper: [inferred] The conclusion lists the requirement of hyperparameters for localization and density estimation as a limitation.
  - Why unresolved: While the authors find that performance is robust to small values of k and use a standard heuristic (Silverman's Rule), they do not propose a method to optimize these parameters automatically for diverse datasets.
  - What evidence would resolve it: An algorithm that dynamically sets k based on the local density of the test observation x* or dataset dimensionality, achieving comparable or superior TPR@FPR without manual search.

## Limitations

- Performance on heterogeneous datasets with many categorical features remains unclear, as KDE performance degrades with dimensionality and categorical encoding strategies are not standardized across implementations.
- Scalability to larger datasets (N > 10,000) is untested; fitting N+1 density estimators becomes computationally prohibitive.
- The paper assumes reference data R is independent of training data T, but in practice obtaining such independent reference sets may be difficult for proprietary datasets.

## Confidence

- **High Confidence**: The core mechanism of measuring likelihood ratio influence for detecting overfitting-based membership leakage is theoretically sound and empirically validated across multiple architectures.
- **Medium Confidence**: KDE consistently outperforms deep density estimators for this task, though this may be architecture-specific and not generalizable to all synthetic data scenarios.
- **Medium Confidence**: Localization to k-nearest neighbors consistently improves performance, though the optimal k value (k=10) may vary by dataset and model architecture.

## Next Checks

1. Test Gen-LRA on a high-dimensional heterogeneous dataset (e.g., Covertype with mixed features) to verify KDE scaling and encoding robustness.
2. Evaluate attack performance when synthetic data contains only partial leakage (moderate overfitting) to test detection limits.
3. Compare Gen-LRA performance using alternative density estimators (Normalizing Flows with different architectures) to confirm KDE superiority is not implementation-dependent.