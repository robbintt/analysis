---
ver: rpa2
title: 'Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework'
arxiv_id: '2509.24127'
source_url: https://arxiv.org/abs/2509.24127
tags:
- agent
- data
- claims
- evaluation
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a modular AI agent architecture that enables
  non-technical users to query enterprise data warehouses through natural language,
  while providing transparent and auditable reasoning. The system uses a multi-layered
  interpretability framework to decompose decisions into traceable steps, integrates
  automated evaluation against golden standards, and quantifies statistical deviations
  from normative behavior.
---

# Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework

## Quick Facts
- arXiv ID: 2509.24127
- Source URL: https://arxiv.org/abs/2509.24127
- Reference count: 1
- Non-technical users can query enterprise data warehouses via natural language while receiving transparent, auditable reasoning

## Executive Summary
This paper presents a modular AI agent architecture that enables non-technical users to query enterprise data warehouses through natural language, while providing transparent and auditable reasoning. The system uses a multi-layered interpretability framework to decompose decisions into traceable steps, integrates automated evaluation against golden standards, and quantifies statistical deviations from normative behavior. Applied to an insurance claims dataset, the agent successfully retrieves precise information, explains outlier detection using specific business rules and supporting statistics, and detects potential geographic bias. Evaluation results show moderate accuracy (mean 3.4/5 for outlier detection and completeness, 2.6/5 for factual accuracy) with high variability, no system failures, and effective transparency.

## Method Summary
The framework employs the Agent Development Kit (ADK) with Gemini-2.5-Flash LLM to process natural language queries into secure BigQuery operations. A 4-layer interpretability framework (Input Feature Analysis → Rule Activation → Decision Pathway → Confidence Scoring) forces the LLM to externalize intermediate reasoning steps. The Business Rule Engine applies four explicit rules: diagnosis-procedure mismatch, costs exceeding 3x average, geographic restrictions, and suspicious patterns. A Statistical Comparator module quantifies deviations from historical distributions. The system was tested on synthetic telehealth insurance claims data with 12 fields, using 10 evaluation cases with automated scoring against golden reference answers.

## Key Results
- Outlier Detection accuracy: mean 3.4/5 (SD 2.07)
- Factual Accuracy: mean 2.6/5 (SD 2.07)
- Zero system failures across all test cases
- 14.7 second average latency per query
- High variability in performance metrics suggests inconsistent reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured reasoning decomposition produces auditable decision trails.
- Mechanism: The 4-layer interpretability framework forces the LLM to externalize intermediate reasoning steps rather than emit opaque conclusions. Each layer maps to explicit evidence (activated business rules, statistical deviations).
- Core assumption: LLMs can reliably articulate their reasoning process when constrained by structured output templates; this articulation faithfully reflects actual decision logic.
- Evidence anchors:
  - [abstract] "multi-layered reasoning framework that explains the 'why' behind every decision, allowing for full interpretability by tracing conclusions through specific, activated business rules and data points"
  - [section] Figure 4 demonstrates claim CLM_10668 analysis: rule violation identified ("Unusual Diagnosis-Procedure Combinations"), quantified (9.95% frequency), and confidence assigned.
- Break condition: If reasoning layers become pro forma (template-filling without genuine causal linkage), auditability degrades to theatrical compliance.

### Mechanism 2
- Claim: Automated evaluation against golden standards enables regression detection and iterative improvement.
- Mechanism: A curated dataset of prompt-reference pairs grounds objective scoring across multiple metrics. The evaluation framework serializes results per experiment, creating longitudinal performance tracking.
- Core assumption: Golden reference answers adequately cover the domain's edge cases; automated scoring correlates with human judgment quality.
- Evidence anchors:
  - [abstract] "automated evaluation framework... enables performance benchmarking by objectively measuring agent performance against golden standards"
  - [section] Figure 10 shows evaluation results: mean Outlier Detection 3.4/5 (SD 2.07), Factual Accuracy 2.6/5 (SD 2.07), zero failure rate.
- Break condition: If evaluation dataset lacks diversity or reference answers become stale, metrics track proxy-gaming rather than genuine capability.

### Mechanism 3
- Claim: Statistical context module grounds conclusions in quantitative deviation rather than threshold heuristics alone.
- Mechanism: The Statistical Comparator retrieves aggregated historical distributions and computes individual claim deviations, providing evidentiary support for outlier classifications.
- Core assumption: Historical distributions represent valid normative baselines; statistical deviation correlates with genuine anomaly rather than distribution shift.
- Evidence anchors:
  - [abstract] "statistical context module, which quantifies deviations from normative behavior, ensuring all conclusions are supported by quantitative evidence"
  - [section] Figure 4: CLM_10668 amount ($61.41) calculated as 67.47% below Emergency Consult average ($296.25); diagnosis frequency (9.95%) cited.
- Break condition: If underlying data distributions shift (concept drift), normative comparisons generate misleading confidence.

## Foundational Learning

- Concept: **Chain-of-thought prompting**
  - Why needed here: The multi-layered reasoning framework extends CoT principles—breaking decisions into intermediate steps before conclusions.
  - Quick check question: Can you explain why forcing an LLM to state its reasoning steps before its answer might improve both accuracy and interpretability?

- Concept: **Parameterized SQL query construction**
  - Why needed here: The Safe Query Builder translates natural language to secure, executable SQL while preventing injection and enforcing role-limited access.
  - Quick check question: Why is string concatenation for SQL queries dangerous, and how does parameterization mitigate this?

- Concept: **Golden dataset evaluation methodology**
  - Why needed here: The framework's quality assurance depends on curated prompt-reference pairs with objective scoring rubrics.
  - Quick check question: What failure modes might emerge if a golden dataset covers common cases but misses edge cases?

## Architecture Onboarding

- Component map:
  - Agent Reasoning Engine (Gemini-2.5-Flash) -> Business Rule Engine -> Statistical Comparator -> BigQuery Toolset
  - Input Validation Layer -> Safe Query Builder -> ADK Framework (session management, runner orchestration)

- Critical path:
  1. User submits natural language query
  2. Session manager initializes conversation context
  3. Agent interprets intent, generates SQL via Safe Query Builder
  4. BigQuery executes, returns structured results
  5. Business Rule Engine + Statistical Comparator analyze results
  6. 4-layer reasoning generates human-auditable response
  7. Response validation checks for required sections, PII

- Design tradeoffs:
  - Rule-based outlier detection vs. ML anomaly detection: Current system uses explicit rules (interpretable but may miss novel fraud patterns); future work proposes ML complement
  - Human-in-the-loop vs. automation: System flags for review rather than auto-adjudicating (ethical safety at cost of throughput)
  - Evaluation complexity vs. coverage: 10 test cases in study—tractable but potentially insufficient for production coverage

- Failure signatures:
  - High metric variability (SD 2.07 across metrics): Agent performs inconsistently across query types
  - Low Tool Trajectory score (0.11/1): Agent fails to invoke expected tools in correct sequence
  - Factual fabrication: Figure 11 shows agent generating numerical data not present in reference
  - Moderate factual accuracy (2.6/5): System produces incorrect information in ~48% of evaluated dimensions

- First 3 experiments:
  1. Extend evaluation dataset from 10 to 50+ cases with stratified coverage of all business rules; measure whether variability (SD) decreases with broader test coverage.
  2. Add tool-use validation gate: Before final response, verify expected tools were invoked; log trajectory deviations for analysis.
  3. Introduce adversarial test cases (edge-case diagnosis-procedure pairs, boundary claim amounts) to stress-test statistical comparator accuracy and rule coverage gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can machine learning models be effectively integrated to complement the rule-based engine in detecting emerging fraud schemes that static rules miss?
- Basis in paper: [explicit] Section 4.5 explicitly states future work will focus on "integrating machine learning models for anomaly detection to complement the rule-based system."
- Why unresolved: The current architecture relies solely on static business rules, which the authors note may fail to identify "sophisticated fraud schemes."
- What evidence would resolve it: Performance benchmarking of a hybrid (ML + rule-based) system against the current baseline using adversarial or novel fraud datasets.

### Open Question 2
- Question: What specific architectural or prompting interventions are required to reduce the high variability and low factual accuracy observed in the evaluation results?
- Basis in paper: [inferred] The results show a mean factual accuracy of only 2.6/5 with a high standard deviation (2.07) across metrics (Figure 10).
- Why unresolved: The paper establishes the framework's feasibility but does not address the causes of inconsistent performance across different query types.
- What evidence would resolve it: Ablation studies testing different reasoning strategies or model sizes that demonstrate a statistically significant reduction in variance and higher mean scores.

### Open Question 3
- Question: How does the framework perform when deployed on noisy, real-world enterprise data compared to the synthetic dataset used in this proof-of-concept?
- Basis in paper: [inferred] The method relies on a "synthetic health insurance claims data" generator (Table 1), whereas the introduction claims applicability to "complex enterprise data warehouses."
- Why unresolved: Synthetic data lacks the noise, missing values, and schema inconsistencies typical of real production environments.
- What evidence would resolve it: Evaluation metrics (latency, accuracy) generated from applying the agent to a sanitized but real corporate dataset.

## Limitations
- Absence of publicly available code, prompts, and complete evaluation dataset blocks reproducibility verification
- High variability in evaluation metrics (SD = 2.07) suggests unreliable performance across diverse queries
- Small sample size (10 test cases) limits generalizability to production scenarios
- Reliance on synthetic data prevents assessment of real-world noise and schema inconsistencies

## Confidence
- High confidence in the architectural validity of the multi-layered interpretability framework and its potential for producing auditable reasoning trails
- Medium confidence in the automated evaluation methodology, though concerns about LLM-as-judge biases (per Zheng et al.) warrant caution
- Medium confidence in statistical grounding claims, limited by lack of corpus validation for the specific implementation
- Low confidence in production readiness given small sample size, high metric variability, and absence of real-world deployment data

## Next Checks
1. **Extend evaluation breadth**: Expand from 10 to 50+ test cases with stratified coverage across all business rules and geographic regions; measure whether performance variability decreases with broader test coverage
2. **Implement adversarial testing**: Create edge-case scenarios targeting diagnosis-procedure combinations, boundary claim amounts, and geographic restrictions to identify rule coverage gaps and statistical comparator limitations
3. **Establish tool-use validation**: Add automated verification that required tools (list_table_ids, execute_sql) are invoked in correct sequence before final response generation; log trajectory deviations for systematic analysis