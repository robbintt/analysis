---
ver: rpa2
title: An Explainable Two Stage Deep Learning Framework for Pericoronitis Assessment
  in Panoramic Radiographs Using YOLOv8 and ResNet-50
arxiv_id: '2601.08401'
source_url: https://arxiv.org/abs/2601.08401
tags:
- pericoronitis
- panoramic
- classification
- clinical
- dental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a two-stage deep learning pipeline to automate
  pericoronitis detection in panoramic radiographs. The first stage used YOLOv8 to
  detect third molars and classify their anatomical features, while the second stage
  employed a ResNet-50 model to classify pericoronitis presence.
---

# An Explainable Two Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50

## Quick Facts
- arXiv ID: 2601.08401
- Source URL: https://arxiv.org/abs/2601.08401
- Reference count: 25
- Primary result: 92% precision and 92.5% mAP50 for YOLOv8 third molar detection; 88% and 86% F1-scores for ResNet-50 pericoronitis classification; 84% radiologist alignment with Grad-CAM explanations

## Executive Summary
This study presents a two-stage deep learning pipeline for automating pericoronitis detection in panoramic radiographs. The first stage employs YOLOv8 to detect third molars and classify their anatomical positions, while the second stage uses a ResNet-50 model to classify pericoronitis presence. Grad-CAM is integrated for visual interpretability, enabling clinicians to verify model decisions. The system achieves strong performance metrics and demonstrates potential for AI-assisted radiographic assessment with integrated explainability features.

## Method Summary
The framework uses a two-stage approach: YOLOv8 first detects third molars and classifies their anatomical positions (quadrant and Winter's angulation) in full panoramic images (832×832), then ResNet-50 classifies pericoronitis presence in extracted ROIs (224×224 via center crop). The pipeline was trained on 1,190 OPGs for localization and 1,550 cropped third molar ROIs for classification. Grad-CAM applied to ResNet-50's final convolutional layer provides visual explanations. Data augmentation included mosaic, flips, and rotation for YOLOv8; only flips and rotation for ResNet-50 (brightness/contrast avoided to preserve subtle features).

## Key Results
- YOLOv8 achieved 92% precision and 92.5% mAP50 for third molar detection
- ResNet-50 classifier attained F1-scores of 88% for normal cases and 86% for pericoronitis
- Grad-CAM visualizations aligned with radiologist diagnostic impressions 84% of the time
- AUC for pericoronitis classification reached 0.94

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Task Decomposition for Specialized Optimization
Separating third molar localization from pathological classification allows each model to specialize, potentially improving overall diagnostic accuracy compared to single-stage approaches. YOLOv8 handles spatial localization and anatomical feature extraction, while ResNet-50 specializes in detecting subtle radiographic features of pericoronitis. This division mirrors clinical workflow where anatomical identification precedes pathological assessment.

### Mechanism 2: Grad-CAM as Clinical Trust and Verification Interface
Gradient-weighted Class Activation Mapping provides clinically meaningful visualizations that align with expert diagnostic reasoning, supporting model interpretability. Grad-CAM computes gradients flowing into ResNet-50's final convolutional layer to produce coarse localization maps highlighting regions most influential for the predicted class.

### Mechanism 3: ROI Cropping Preserves Critical Spatial Context While Enabling Focused Analysis
Extracting detected third molar regions and classifying them at standardized resolution improves pericoronitis detection by focusing computational attention on diagnostically relevant anatomy. This provides higher effective resolution on the region of interest while preserving spatial relationships critical for identifying pericoronal changes.

## Foundational Learning

- **YOLO Object Detection Architecture**
  - Why needed here: Understanding how YOLOv8 performs simultaneous bounding box regression and multi-class classification for third molar detection and anatomical categorization.
  - Quick check question: Can you explain how YOLO generates spatial predictions in a single forward pass, and what mAP50 measures?

- **Residual Networks and Skip Connections**
  - Why needed here: ResNet-50's skip connections enable training of deeper networks for medical image classification where subtle features must be preserved across layers.
  - Quick check question: What optimization problem do skip connections solve, and why is the final convolutional layer important for Grad-CAM?

- **Gradient-Based Explainability (Grad-CAM)**
  - Why needed here: Understanding how gradient flow generates attention maps is critical for interpreting model decisions and validating clinical relevance.
  - Quick check question: Which gradients does Grad-CAM use, and why might highlighted regions sometimes misrepresent the model's true reasoning pathway?

## Architecture Onboarding

- **Component map:**
  Input: Panoramic radiographs → grayscale, resized to 832×832
  Stage 1: YOLOv8 → bounding boxes + anatomical classifications (quadrant: UR/UL/LL/LR; Winter's classification: vertical/mesioangular/horizontal/distoangular)
  Stage 2: ROI extraction → center crop → resize to 224×224 → ResNet-50 → binary classification (pericoronitis/normal)
  Explainability: Grad-CAM on ResNet-50 final conv layer → heatmap overlay

- **Critical path:**
  1. Image preprocessing validation (format, resolution, artifact check)
  2. YOLOv8 inference → verify bounding boxes cover third molars
  3. ROI extraction quality check (cropping alignment, no truncation)
  4. ResNet-50 classification → probability threshold calibration
  5. Grad-CAM generation → visual sanity check against radiographic features

- **Design tradeoffs:**
  - Brightness/contrast augmentation excluded to preserve subtle radiographic features (trades robustness to acquisition variation for feature fidelity)
  - Binary classification cannot distinguish acute vs. chronic pericoronitis (clinical symptom data absent)
  - Multi-center public dataset provides diversity but introduces acquisition heterogeneity
  - Stage 1 recall (0.70) indicates some third molars may be missed—acceptable for screening but not replacement

- **Failure signatures:**
  - Stage 1 low recall: Third molars not detected → no Stage 2 analysis → silent failures
  - Grad-CAM highlighting non-pathological regions → potential spurious correlation learned
  - Poor performance on images with severe cone-beam artifacts (excluded from training)
  - Disagreement between Grad-CAM and radiologist assessment in ambiguous/borderline cases

- **First 3 experiments:**
  1. **Stage 1 validation**: Run YOLOv8 on held-out OPGs, compute precision/recall/mAP50 against paper targets (P=0.92, mAP50=0.925, R=0.70). Identify failure modes for missed detections.
  2. **Stage 2 classifier probe**: Test ResNet-50 on independently cropped third molar ROIs. Verify F1-scores (0.88 normal, 0.86 pericoronitis) and AUC (0.94). Generate Grad-CAM overlays and compare with paper figures for qualitative alignment.
  3. **End-to-end integration test**: Process 20-30 full OPGs through complete pipeline. Measure latency, identify Stage 1→Stage 2 handoff failures, and have a radiologist assess Grad-CAM alignment (target: ~84% agreement).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of clinical symptom data and metadata improve the model's ability to distinguish between acute and chronic phases of pericoronitis?
- Basis in paper: The authors state in the Discussion that a key limitation is the "absence of clinical symptom data," which restricts the model to identifying radiographic correlates and prevents it from distinguishing between acute and chronic phases. They propose the "inclusion of clinical metadata" in future work.
- Why unresolved: The current study utilized retrospective radiographic data without associated clinical records, making it impossible to correlate radiographic features with symptomatology or disease chronicity.
- What evidence would resolve it: A study training the model on a dataset that includes both radiographic images and corresponding clinical symptom data, demonstrating validated classification performance for acute versus chronic conditions.

### Open Question 2
- Question: Can the binary classification framework be successfully extended into a multi-class system that differentiates varying severities of inflammation (e.g., mild, moderate, severe)?
- Basis in paper: In the Discussion, the authors suggest that "the current binary classification... can be extended into a fine-grained, multi-class diagnostic framework that differentiates varying severities of inflammation."
- Why unresolved: The current dataset and model architecture were designed solely for binary classification (healthy vs. pericoronitis), and the authors have not yet validated the model's sensitivity to graded levels of severity.
- What evidence would resolve it: Experimental results from a modified model trained on annotated data labeled by inflammation severity, showing high granularity in classification metrics.

### Open Question 3
- Question: Does integrating an unsupervised active guardrail mechanism enhance anomaly detection and model reliability?
- Basis in paper: In the Methods section (3.1), the authors note that "Future work proposes integrating an unsupervised active guardrail mechanism to enhance anomaly detection and model reliability."
- Why unresolved: The proposed guardrail is mentioned as a future direction inspired by safety mechanisms in large language models but has not yet been implemented or tested within the current dental imaging pipeline.
- What evidence would resolve it: Implementation details and performance metrics (e.g., outlier detection rates) showing that the guardrail effectively identifies out-of-distribution samples or anomalies without degrading classification performance.

### Open Question 4
- Question: Can the proposed pipeline be optimized for real-time inference on resource-constrained edge devices without significant loss of diagnostic accuracy?
- Basis in paper: The Conclusion identifies "YOLOv8's hardware demands" as a practical deployment challenge and suggests the model "can be optimized for real-time inference on edge devices using techniques such as quantization and model pruning."
- Why unresolved: The current implementation relies on standard deep learning architectures which can be computationally expensive; the trade-off between optimization techniques (like pruning) and the reported 86-88% F1-scores has not been quantified.
- What evidence would resolve it: Benchmarking results showing inference speed, memory usage, and accuracy metrics (F1-score, AUC) of the pruned/quantized model running on specific edge hardware.

## Limitations

- The binary classification framework cannot distinguish between acute and chronic pericoronitis due to absence of clinical symptom data
- Stage 1 recall of 0.70 means approximately 30% of third molars may be missed, creating silent failure modes
- The exclusion of brightness/contrast augmentation may reduce robustness to real-world acquisition variability
- Complete reproduction requires unspecified YOLOv8 configuration details, ResNet-50 architectural modifications, and ROI extraction protocols

## Confidence

- **High Confidence**: YOLOv8 performance metrics (precision=0.92, mAP50=0.925) and ResNet-50 classification performance (F1: 0.88 normal, 0.86 pericoronitis, AUC=0.94) are well-specified and verifiable
- **Medium Confidence**: The 84% radiologist alignment with Grad-CAM visualizations is reported but lacks details on assessment methodology and sample size
- **Low Confidence**: Complete reproduction requires YOLOv8 configuration details, ResNet-50 architectural modifications, and ROI extraction protocols that are not provided

## Next Checks

1. **Stage 1 Localization Verification**: Train YOLOv8 on the specified public datasets with mosaic/flip/rotation augmentation. Evaluate on held-out data to confirm mAP50 ≥ 0.925 and precision ≥ 0.92. Identify failure modes for third molars that are not detected.

2. **Stage 2 Classification Validation**: Train ResNet-50 on independently annotated ROIs with flip/rotation augmentation only. Verify F1-scores (0.88 for normal, 0.86 for pericoronitis) and AUC (0.94). Generate Grad-CAM overlays and compare with paper figures for qualitative alignment.

3. **End-to-End Clinical Workflow Assessment**: Process 20-30 full OPGs through the complete pipeline. Measure inference latency, identify Stage 1→Stage 2 handoff failures, and have radiologists assess Grad-CAM alignment against their diagnostic impressions, targeting ~84% agreement.