---
ver: rpa2
title: 'Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction'
arxiv_id: '2511.11770'
source_url: https://arxiv.org/abs/2511.11770
tags:
- agent
- query
- arxiv
- policy
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an RL-based iterative agent for SPARQL query
  construction that learns to refine queries using execution feedback. The method
  employs a 3B-parameter LLM trained via GRPO without supervised fine-tuning, guided
  by a structured think-query loop.
---

# Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction

## Quick Facts
- arXiv ID: 2511.11770
- Source URL: https://arxiv.org/abs/2511.11770
- Reference count: 40
- Primary result: 49.7% accuracy on single-answer subset of LC-QuAD 2.0, 17.5pp improvement over best baseline

## Executive Summary
This paper introduces an RL-based iterative agent for SPARQL query construction that learns to refine queries using execution feedback. The method employs a 3B-parameter LLM trained via GRPO without supervised fine-tuning, guided by a structured think-query loop. On a curated single-answer subset of LC-QuAD 2.0, the agent achieves 49.7% accuracy post-entity-linking, a 17.5-point improvement over the strongest iterative zero-shot baseline. Further analysis shows RL-driven capability complemented by deliberative reasoning as a cognitive scaffold, enabling robust multi-hop KGQA.

## Method Summary
The approach frames SPARQL generation as an MDP where the agent iteratively constructs queries, executes them against Wikidata, and refines based on feedback. A 3B-parameter Qwen2.5-Instruct model is fine-tuned using Group Relative Policy Optimization (GRPO) with QLoRA adapters. The agent operates in a think-query loop, emitting deliberative reasoning before generating SPARQL. Training uses a composite reward function penalizing syntax errors, execution failures, and incorrect answers while rewarding successful query completion. The method achieves 49.7% accuracy on a filtered LC-QuAD 2.0 subset after 11.5 hours of training on a single H100 GPU.

## Key Results
- 49.7% accuracy on single-answer subset of LC-QuAD 2.0, compared to 32.2% for best iterative baseline
- RL-driven learning shows layered acquisition: executability rate saturates before accuracy improves
- Deliberative reasoning agent achieves 49.7% accuracy versus 48.1% for reactive agent
- 17.5-point improvement over strongest iterative zero-shot baseline

## Why This Works (Mechanism)

### Mechanism 1: Error Recovery Through Execution Feedback
RL allows a compact model to learn error recovery strategies for SPARQL generation that static prompting cannot replicate. The agent operates in a loop: generates a query, executes it against the KG, and receives structured feedback (e.g., syntax errors, empty results). GRPO optimizes the policy by rewarding successful outcomes, forcing the model to internalize query language syntax and semantics through trial and error. The feedback signal from the execution environment is sufficiently informative for credit assignment across multi-step trajectories.

### Mechanism 2: Deliberative Reasoning as Cognitive Scaffold
Explicit deliberative reasoning (`<think >` blocks) acts as a "cognitive scaffold" that regularizes the policy, improving precision over purely reactive agents. Forcing the model to emit thought processes before taking action constrains the output space and appears to stabilize policy gradient updates by grounding query generation in high-level plans. The base model has sufficient inherent instruction-following capability to utilize thought tokens effectively during RL training.

### Mechanism 3: Layered Learning Through Composite Reward
A composite reward structure drives a "layered learning" process where syntactic validity is acquired before semantic reasoning. The reward function heavily penalizes structural invalidity and execution errors (-1 for invalid trajectories), creating a strong gradient that prioritizes learning executable SPARQL syntax first, establishing a foundation upon which semantic reasoning is later optimized. The penalty weights are correctly balanced to prevent the agent from becoming too conservative or too reckless.

## Foundational Learning

- **Concept**: Markov Decision Process (MDP) in Text Domains
  - Why needed here: The paper frames query construction not as a translation task, but as a sequential decision problem. Understanding state, action, and transition is required to grasp why RL is applicable.
  - Quick check question: Can you identify what constitutes the "State" and the "Transition Function" in the paper's defined MDP?

- **Concept**: Group Relative Policy Optimization (GRPO)
  - Why needed here: This is the specific RL algorithm used to bypass the need for a learned value function (critic). It compares group samples to determine advantage, which is critical for understanding training efficiency.
  - Quick check question: How does GRPO determine which actions are "good" without a separate value network estimating future rewards?

- **Concept**: SPARQL & Knowledge Graph Schema
  - Why needed here: The agent's action space is valid SPARQL syntax. Without understanding the rigidity of formal query languages (triples, prefixes, filters), one cannot appreciate why one-shot generation fails and why execution feedback is necessary.
  - Quick check question: Why does a "syntax error" in an MDP context result in a distinct penalty compared to a "logic error" in this system?

## Architecture Onboarding

- **Component map**: Agent Policy -> Environment (qEndpoint) -> Orchestrator -> Rollout Generation -> Reward Calculation -> Policy Update
- **Critical path**: The training pipeline relies on the Rollout Generation -> Reward Calculation -> Policy Update loop. The most fragile component is the environment execution wrapper; if the SPARQL endpoint hangs or returns unstructured errors, the reward signal becomes noisy, breaking the GRPO logic.
- **Design tradeoffs**:
  - No SFT: The authors skipped imitation learning to test if RL alone can teach the skill, reducing data curation cost but potentially increasing training time.
  - 3B Model: A smaller model is cheaper to fine-tune (11.5 hours on 1x H100) but may lack world knowledge for complex entity disambiguation without external retrieval.
- **Failure signatures**:
  - Syntactic Flailing: The model generates the same malformed query repeatedly until the turn limit.
  - Reward Hacking: The model discovers ways to generate "valid" queries that always return some result but do not answer the question.
  - Context Overflow: Large XML/JSON results fill the context window, pushing the original question out of context.
- **First 3 experiments**:
  1. Sanity Check: Run base Qwen2.5-3B model in zero-shot setup to confirm baseline failure rate (~47% executability).
  2. Environment Validation: Manually execute the Reward Calculation module on 10 curated trajectories to verify the composite reward formula.
  3. Ablation Run: Train "Reactive" agent (no `<think >` block) for 100 steps against "Deliberative" agent to observe immediate effects on loss curves.

## Open Questions the Paper Calls Out

- Does incorporating Supervised Fine-Tuning (SFT) prior to Reinforcement Learning significantly reduce sample complexity compared to the pure RL approach?
- How does the agent's performance degrade when shifting from gold-standard entity linking to an end-to-end setting with a learned entity linker?
- Can the policy learned for SPARQL construction transfer effectively to other structured query domains, such as Text-to-SQL, without retraining from scratch?
- Is the iterative refinement policy robust to the "false negative" feedback caused by incomplete Knowledge Graphs?

## Limitations
- Results are reported on a filtered, single-answer subset of LC-QuAD 2.0, which may not reflect performance on complex, multi-answer queries.
- The 17.5-point improvement over baselines is significant but the deliberative reasoning agent only shows a 1.6-point accuracy gain over the reactive agent.
- The reward function's sensitivity to penalty weights is not fully explored, raising concerns about potential reward hacking or overly conservative query generation.

## Confidence
- **High**: Core mechanism (RL enables error recovery through execution feedback) given 17.5-point improvement and clear training dynamics.
- **Medium**: Deliberative reasoning claim as the 1.6-point accuracy difference may not justify added complexity in all scenarios.
- **Low**: Generalization claims beyond curated dataset as no cross-dataset validation is presented.

## Next Checks
1. **Reward Function Sensitivity Analysis**: Systematically vary penalty weights (0.1 per error, 0.2 for wrong answer) to identify breaking points where the agent either stops generating queries or exploits the reward structure.

2. **Cross-Dataset Generalization**: Evaluate the trained agent on a different KGQA dataset (e.g., MetaQA or WebQSP) without fine-tuning to assess transfer capability.

3. **SFT vs. RL Ablation**: Train an identical architecture using only supervised fine-tuning on the same data to quantify the marginal benefit of RL over imitation learning for SPARQL generation.