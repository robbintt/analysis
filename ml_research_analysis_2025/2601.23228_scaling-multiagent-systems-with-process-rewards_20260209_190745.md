---
ver: rpa2
title: Scaling Multiagent Systems with Process Rewards
arxiv_id: '2601.23228'
source_url: https://arxiv.org/abs/2601.23228
tags:
- agent
- training
- coach
- multiagent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAPPA introduces per-action process rewards from AI feedback to
  address credit assignment and sample efficiency in multiagent systems. By having
  a coach LLM evaluate each agent's action based on role, inputs, and environment
  feedback, MAPPA provides dense supervision without ground truth labels.
---

# Scaling Multiagent Systems with Process Rewards

## Quick Facts
- arXiv ID: 2601.23228
- Source URL: https://arxiv.org/abs/2601.23228
- Authors: Ed Li; Junyu Ren; Cat Yan
- Reference count: 40
- Primary result: MAPPA achieves +5.0-17.5pp improvements on AIME and +7.8-17.2pp on AMC math problems using per-action process rewards from AI feedback

## Executive Summary
MAPPA introduces per-action process rewards from AI feedback to address credit assignment and sample efficiency in multiagent systems. By having a coach LLM evaluate each agent's action based on role, inputs, and environment feedback, MAPPA provides dense supervision without ground truth labels. The method uses REINFORCE++ with global batch normalization to handle state divergence in heterogeneous pipelines. On competition math problems, MAPPA achieves +5.0-17.5pp improvements on AIME and +7.8-17.2pp on AMC. For data analysis tasks, it improves success rate by +16.7pp while quality metrics improve by up to 47%.

## Method Summary
MAPPA trains multiagent systems by assigning per-action process rewards from an external coach LLM rather than relying on sparse outcome signals. Each agent action receives a 0-10 score based on role appropriateness, input context, action taken, and tool execution results. The system uses REINFORCE++ with global advantage normalization across all experiences, handling state divergence where traditional GRPO fails. Training employs KL penalty regularization, PPO clipping, and asynchronous coach evaluation overlapped with rollouts. The approach works with or without ground truth labels, using AI feedback as the primary supervision signal.

## Key Results
- MathChat: +5.0-17.5pp improvements on AIME problems over baselines
- MathChat: +7.8-17.2pp improvements on AMC problems
- DSBench: +16.7pp improvement in success rate with up to 47% improvement in quality metrics
- Process rewards enable learning from 1/10th the samples required by outcome-only RL

## Why This Works (Mechanism)

### Mechanism 1
Dense per-action process rewards enable learning when outcome-only signals would fail. A coach LLM evaluates each agent action on a 0-10 scale based on role, input context, action taken, and tool execution results. This converts a single sparse trajectory-level signal into N×A training signals (N agents × A actions per trajectory), dramatically improving sample efficiency. Core assumption: The coach model can accurately assess action quality without ground truth labels by reasoning about role appropriateness and causal responsibility. Evidence anchors: [abstract] "Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout." [Section 2.2] "Crucially, MAPPA can work with or without ground truth/verifier that most other alternatives RL approaches such as RLVR require." Break condition: If coach evaluation quality degrades significantly for domains outside its training distribution, process rewards may misguide learning.

### Mechanism 2
Global batch normalization handles state divergence in sequential multiagent pipelines where GRPO fails. Each agent's input depends on upstream stochastic outputs, so rollouts from the same initial prompt produce different intermediate states. REINFORCE++ normalizes advantages across all experiences globally rather than per-prompt, preventing ill-defined within-group comparisons. Core assumption: Advantage normalization across heterogeneous states produces stable gradient estimates despite varying trajectory lengths and agent roles. Evidence anchors: [Section 2.3] "GRPO normalizes advantages within groups of samples sharing the same prompt, assuming identical input states. This assumption breaks in end-to-end multiagent training—each agent's input depends on upstream agents' stochastic outputs." [Section E.1] "Consider two rollouts of the same math problem: the Problem Solver may generate different reasoning chains, causing the Code Executor to observe different inputs despite originating from the same initial prompt." Break condition: If trajectories become so diverse that global normalization washes out meaningful signal, per-agent normalization with proper state grouping may be needed.

### Mechanism 3
Context-aware coach evaluation performs implicit credit assignment across agent dependencies. The coach observes tool outputs and error messages invisible to agents, enabling causal attribution. When a downstream agent encounters a missing artifact error, the coach assigns low scores to the upstream agent responsible for producing that artifact, not the agent that correctly reported the issue. Core assumption: The coach can reliably infer causal responsibility from error messages and file system state. Evidence anchors: [Section 2.2] "if a downstream agent encounters a missing file error, the coach assigns low scores to the upstream agent that should have produced it, not the downstream agent that correctly reported the issue." [Appendix D.4] Example shows coach identifying that Analyst failure was caused by upstream Modeler not saving model.pkl. Break condition: If error messages are ambiguous or multiple agents contribute to failure, credit assignment becomes noisy.

## Foundational Learning

- **REINFORCE with baseline/advantage normalization**: MAPPA uses REINFORCE++ with global advantage normalization. Understanding why policy gradient methods need variance reduction through baselines is essential. Quick check: Why does subtracting a baseline from returns not bias the gradient estimate?

- **Credit assignment in multiagent RL**: The core problem MAPPA solves. Without per-action rewards, blame for failure propagates incorrectly across agents. Quick check: In a sequential pipeline where agent B fails because agent A produced bad output, how would outcome-only RL incorrectly penalize agent B?

- **Process vs. outcome rewards**: MAPPA's key innovation is dense process rewards from AI feedback rather than sparse outcome signals. Quick check: What is the sample efficiency advantage of process rewards over outcome rewards for a 10-step trajectory?

## Architecture Onboarding

- **Component map**: Rollout coordinator -> Agent workers (vLLM engines + DeepSpeed actors) -> Coach evaluator (Gemini 2.5) -> Experience router -> Weight synchronizer
- **Critical path**: 1) Shard prompts across workers → parallel multiagent rollouts, 2) Coach evaluates each action (0–10 scale) during/after rollout, 3) Compute KL-penalized rewards and undiscounted return-to-go advantages, 4) Global advantage normalization across all agents/experiences, 5) PPO-clipped policy gradient per agent, 6) Broadcast updated weights to vLLM engines
- **Design tradeoffs**: REINFORCE++ vs. GRPO (handles state divergence but requires more samples), Undiscounted returns (γ=1) (all actions weighted equally), External coach vs. trainable reward model (immediate signal vs. adaptability), Co-located models (reduces memory 3–5× but requires careful GPU management)
- **Failure signatures**: NCCL deadlock during gradient sync (variable turn counts across workers), Coach bias toward regression/classification (systematic scoring differences), Reward hacking (coach scores improve while task success stagnates), Specialization collapse (regression in one task type while another improves)
- **First 3 experiments**: 1) Single-agent baseline (verify coach evaluation correlates with ground truth), 2) Ablate coach context (remove tool execution results, measure credit assignment degradation), 3) Sweep KL penalty β (test β ∈ {0.001, 0.01, 0.1} for stable policy updates)

## Open Questions the Paper Calls Out

### Open Question 1
Can a trainable coach that co-evolves with agents avoid degenerate equilibria without external supervision? Basis in paper: [explicit] Section 4.4 states: "Whether a fully self-contained system—where agents and coaches co-evolve without external supervision—can avoid degenerate equilibria remains an open question." Why unresolved: The current coach is stateless and cannot detect emergent imbalances in its own scoring behavior (e.g., systematic preference for regression tasks over classification). What evidence would resolve it: Training experiments where coach and agents update jointly, measuring whether success rates and quality metrics improve across all task types without external meta-evaluation.

### Open Question 2
Would reward backpropagation (top-down outcome-aware decomposition) outperform bottom-up process rewards for credit assignment? Basis in paper: [explicit] Section 4.4 and Appendix F propose reward backpropagation as future work: "given a specific outcome, trace backward to identify which agent made which mistake." Why unresolved: Current process rewards critique everything that could be improved without knowing which improvements actually matter for final outcomes. What evidence would resolve it: Comparative experiments on DSBench-style pipelines measuring whether backpropagated rewards reduce reward hacking and improve attribution accuracy relative to per-action coach scores.

### Open Question 3
Can ensembling multiple coach models (e.g., Claude, Gemini, GPT-4) reduce variance and mitigate task-type biases in training signals? Basis in paper: [explicit] Section 4.4 identifies verbosity and self-enhancement biases in LLM-as-judge and suggests: "potential mitigation strategies including ensembling multiple coach models." Why unresolved: Single-coach experiments revealed systematic scoring differences (regression +0.51 to +1.80 over classification) that led to unintended specialization. What evidence would resolve it: Training runs with multi-model coach ensembles, comparing score variance across task types and measuring whether classification/regression success rates remain balanced.

## Limitations
- External coach evaluation introduces potential bias and cannot adapt to evolving agent behaviors
- Single training seed limits statistical confidence in reported improvements
- Synchronous weight updates across agents may not scale to larger agent counts
- Coach prompt engineering quality significantly impacts training signal quality but is only partially specified

## Confidence

**High**: Process rewards improve sample efficiency for complex multiagent tasks (grounded in established RL principles)
**Medium**: MAPPA's +5.0-17.5pp AIME improvements over baselines (single seed, compute-limited evaluation)
**Medium**: Global batch normalization resolves state divergence better than GRPO in multiagent settings (theoretical justification stronger than empirical validation)
**Low**: Coach evaluation quality matches human judgment across diverse domains (no human evaluation reported)

## Next Checks

1. **Ablation on coach context**: Remove tool execution results from coach evaluation and measure degradation in credit assignment accuracy using synthetic failure scenarios
2. **Statistical significance testing**: Run 3 additional training seeds on MathChat to establish confidence intervals for reported improvements
3. **Coach bias analysis**: Systematically vary the relative proportion of regression vs. classification tasks in training data to measure specialization effects predicted in Section 4.1