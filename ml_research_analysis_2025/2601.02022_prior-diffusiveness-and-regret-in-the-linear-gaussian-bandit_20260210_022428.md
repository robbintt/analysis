---
ver: rpa2
title: Prior Diffusiveness and Regret in the Linear-Gaussian Bandit
arxiv_id: '2601.02022'
source_url: https://arxiv.org/abs/2601.02022
tags:
- regret
- bound
- lemma
- prior
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a sharp analysis of Thompson sampling for the
  linear-Gaussian bandit, showing that the regret exhibits an additive decoupling
  between a prior-dependent "burn-in" term and the standard minimax rate. Specifically,
  the authors prove that Thompson sampling achieves $\tilde{O}(\sigma d \sqrt{T} +
  d r \sqrt{\mathrm{Tr}(\Sigma0)})$ Bayesian regret, where the first term scales with
  the observation noise $\sigma$ and the second term depends on the prior diffusiveness
  captured by $\mathrm{Tr}(\Sigma0)$.
---

# Prior Diffusiveness and Regret in the Linear-Gaussian Bandit

## Quick Facts
- arXiv ID: 2601.02022
- Source URL: https://arxiv.org/abs/2601.02022
- Authors: Yifan Zhu; John C. Duchi; Benjamin Van Roy
- Reference count: 40
- Primary result: Thompson sampling achieves $\tilde{O}(\sigma d \sqrt{T} + d r \sqrt{\mathrm{Tr}(\Sigma_0)})$ Bayesian regret for linear-Gaussian bandits

## Executive Summary
This paper provides a sharp analysis of Thompson sampling for the linear-Gaussian bandit problem, demonstrating that the Bayesian regret exhibits an additive decoupling between a prior-dependent "burn-in" term and the standard minimax rate. The authors prove that Thompson sampling achieves $\tilde{O}(\sigma d \sqrt{T} + d r \sqrt{\mathrm{Tr}(\Sigma_0)})$ regret, where the first term scales with observation noise and the second depends on prior diffusiveness. This improves upon previous bounds that showed a multiplicative dependence on these terms. The analysis introduces a novel "elliptical potential" lemma and establishes matching lower bounds showing the burn-in term is unavoidable.

## Method Summary
The paper analyzes Thompson sampling for linear-Gaussian bandits by developing a new technical tool called the "elliptical potential" lemma. This allows the authors to decompose the regret into two distinct components: a burn-in phase where the algorithm explores due to prior uncertainty, and a steady-state phase matching the minimax optimal rate. The analysis carefully tracks how posterior concentration affects the sampling distribution over time, enabling precise characterization of when the burn-in phase ends and the algorithm transitions to optimal performance.

## Key Results
- Thompson sampling achieves $\tilde{O}(\sigma d \sqrt{T} + d r \sqrt{\mathrm{Tr}(\Sigma_0)})$ Bayesian regret
- Regret shows additive decoupling between burn-in term ($d r \sqrt{\mathrm{Tr}(\Sigma_0)}$) and minimax term ($\sigma d \sqrt{T}$)
- Matching lower bounds prove the burn-in term is unavoidable
- Results extend to strongly log-concave priors and non-Gaussian noise distributions

## Why This Works (Mechanism)
The key mechanism is the precise characterization of how posterior concentration interacts with Thompson sampling's exploration. As observations accumulate, the posterior variance decreases, eventually overcoming the prior diffusiveness. The elliptical potential lemma quantifies this transition, showing that once enough information is gathered (proportional to $\mathrm{Tr}(\Sigma_0)$), the algorithm behaves as if initialized with a tight prior and achieves the minimax rate. The additive decomposition emerges because the burn-in phase contributes a fixed cost independent of the steady-state regret rate.

## Foundational Learning

**Elliptical Potential Lemma**: Needed to bound the cumulative posterior variance reduction over time. Quick check: Verify the lemma applies to general positive definite matrices beyond the specific construction used in the proof.

**Bayesian Regret Analysis**: Required for establishing performance guarantees under posterior sampling. Quick check: Confirm the analysis correctly handles the correlation between sampled parameters and actual rewards across time steps.

**Posterior Concentration**: Essential for characterizing when the burn-in phase ends. Quick check: Validate the rate of posterior concentration matches theoretical predictions in numerical experiments.

## Architecture Onboarding

Component map: Prior $\Sigma_0$ -> Thompson Sampling -> Posterior Updates -> Regret Decomposition -> Elliptical Potential Lemma -> Final Bound

Critical path: The elliptical potential lemma is the critical component, as it bridges the gap between posterior updates and regret bounds by quantifying cumulative variance reduction.

Design tradeoffs: The analysis sacrifices generality (linear-Gaussian setting) for precision (sharp bounds with matching lower bounds). This enables the additive decomposition but limits direct applicability to non-linear or non-Gaussian settings.

Failure signatures: If the prior diffusiveness is too extreme (large $\mathrm{Tr}(\Sigma_0)$), the burn-in term may dominate, making the algorithm inefficient. The analysis shows this is unavoidable, distinguishing between inherent limitations and algorithmic deficiencies.

First experiments:
1. Verify the regret decomposition by plotting burn-in and steady-state components separately on synthetic data
2. Test sensitivity to prior specification by varying $\mathrm{Tr}(\Sigma_0)$ while holding other parameters fixed
3. Compare Thompson sampling's regret to the theoretical bound across different problem scales

## Open Questions the Paper Calls Out
None

## Limitations
- Results heavily rely on the linear-Gaussian setting, with extensions to other settings being more complex
- Assumes known, well-specified prior distribution which may not hold in practical applications
- Bound's dependence on prior trace may be conservative for informative priors in specific directions

## Confidence
High: Core technical contributions are mathematically sound with matching lower bounds
Medium: Extensions to non-Gaussian settings require additional assumptions
Low: Practical implications under prior misspecification not fully characterized

## Next Checks
1. Empirical validation on synthetic and real-world datasets to verify theoretical bounds and assess impact of prior specification on regret performance
2. Extension of analysis to high-dimensional settings where $d$ scales with $T$, examining interplay between dimensionality and regret scaling
3. Investigation of algorithm behavior under misspecified priors or prior uncertainty, including robustness to prior misspecification and potential adaptive prior calibration techniques