---
ver: rpa2
title: 'The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against
  Llm Jailbreaks and Prompt Injections'
arxiv_id: '2510.09023'
source_url: https://arxiv.org/abs/2510.09023
tags:
- attacks
- attack
- defenses
- defense
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that current evaluations of LLM defenses are\
  \ unreliable because they rely on static attacks or weak optimization methods. Instead,\
  \ the authors propose adaptive attackers that explicitly modify their strategy to\
  \ counter the defense\u2019s design while using considerable computational resources."
---

# The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections

## Quick Facts
- arXiv ID: 2510.09023
- Source URL: https://arxiv.org/abs/2510.09023
- Reference count: 40
- Primary result: Adaptive attacks achieve >90% success against 12 recent defenses that claimed near-zero attack success rates

## Executive Summary
This paper demonstrates that current evaluations of LLM defenses are fundamentally unreliable because they rely on static attacks or weak optimization methods. The authors propose adaptive attackers that explicitly modify their strategy to counter the defense's design while using considerable computational resources. They implement four general adaptive attack frameworks—gradient-based, reinforcement learning, search-based, and human red-teaming—and systematically apply them to 12 recent defenses, including prompting, training, filtering, and secret-knowledge approaches. The results show that most defenses, which reported near-zero attack success rates, are bypassed with over 90% success by adaptive attacks, and human red-teaming succeeds on all cases. The authors conclude that future defense work must incorporate strong, adaptive evaluations to make reliable claims of robustness.

## Method Summary
The authors develop a general adaptive attack framework based on a PSSU loop (Propose-Score-Select-Update) that can be instantiated with different optimization techniques. They implement four specific attack methods: gradient-based attacks using token embeddings, reinforcement learning with reward hacking prevention, search-based methods using genetic algorithms, and human red-teaming with collective creativity. These attacks are systematically applied to 12 recent defenses across four categories: prompting defenses (prompt engineering), training defenses (adversarial training), filtering defenses (input/output filtering), and secret-knowledge defenses (access control). The evaluation uses multiple benchmarks including HarmBench, AgentDojo, and human evaluation, with attacks ranging from simple single-shot queries to multi-query optimization processes.

## Key Results
- Adaptive attacks bypass 11 of 12 defenses with over 90% success rates, despite those defenses claiming near-zero attack success
- Human red-teaming achieved 100% success rate across all defense categories, outperforming automated methods
- Filtering and prompting defenses were particularly vulnerable, with some defenses showing no meaningful robustness against adaptive attacks
- The attacks succeeded by finding semantically distinct inputs that bypassed the defense's decision boundaries

## Why This Works (Mechanism)

### Mechanism 1: The Adaptive PSSU Optimization Loop
- **Claim:** Conditional on the attacker having query access and a feedback signal, an iterative optimization loop significantly increases attack success rates against non-adaptive defenses.
- **Mechanism:** The paper formalizes a "PSSU" loop: **Propose** (generate candidates via mutation or gradient), **Score** (evaluate success using a judge or log-probability), **Select** (filter high-performers), and **Update** (refine strategy). This allows the attack to "learn" the defense's blind spots, such as specific semantic structures that bypass filters.
- **Core assumption:** The defense provides a discernible signal (e.g., a rejection, a confidence score, or a specific output format) that the attacker can use as a loss function or reward.
- **Evidence anchors:**
  - [Abstract]: "By systematically tuning and scaling general optimization techniques... we bypass 12 recent defenses."
  - [Section 4]: Describes the general adaptive attack framework and the four steps (Propose, Score, Select, Update).
  - [Corpus]: Related work "Adaptive Attacks Break Defenses..." supports the efficacy of tailored attacks against specific defense mechanisms.

### Mechanism 2: Bypassing Static Generalization via Out-of-Distribution Triggers
- **Claim:** Defenses evaluated primarily on static datasets likely overfit to those specific attack vectors and fail against semantically distinct or "creative" inputs.
- **Mechanism:** Defenses like filtering or adversarial training often create decision boundaries around known bad patterns. Adaptive attacks explore the "gaps" in these boundaries by generating inputs that are statistically or semantically distinct from the training data (e.g., redefining terms, using conditional logic), causing the defense to misclassify the input as benign.
- **Core assumption:** The defense model has limited capacity to generalize safety rules to all possible semantic variations of a harmful request.
- **Evidence anchors:**
  - [Section 6 (Lesson #1)]: "Defenses that merely report near-zero attack success rates on public benchmarks are often among the easiest to break once novel attacks are attempted."
  - [Section 5.4]: Shows how triggers using "policy" language bypassed detectors because they "are not obviously harmful."
  - [Corpus]: "Metaphor-based Jailbreaking..." suggests semantic obfuscation is a consistent bypass vector across modalities.

### Mechanism 3: Hybrid Exploration of Discrete Prompt Space
- **Claim:** Combining automated search efficiency with human semantic creativity yields higher success rates than either method alone, particularly against complex or agentic defenses.
- **Mechanism:** Automated search (e.g., genetic algorithms) efficiently traverses the discrete token space but can get stuck in local minima or low-perplexity traps. Human red-teaming provides high-level semantic restructuring (e.g., role-playing, logical framing) that jumps the attack to a new region of the search space.
- **Core assumption:** Effective prompt attacks require both syntactic optimization (handled by machines) and semantic coherence/trickery (handled by humans).
- **Evidence anchors:**
  - [Figure 1]: Shows Human red-teaming achieved 100% success rate across all defense categories.
  - [Section 4]: "Human red-teaming relies on human creativity... often outperforming automated methods when defenses are dynamic."
  - [Corpus]: Weak or missing specific corpus link for *why* the combination works best, inferred primarily from the paper's results comparison.

## Foundational Learning

- **Concept: Adaptive vs. Static Evaluation**
  - **Why needed here:** The core thesis is that static evaluations are misleading. You must understand that "robustness" is a moving target defined by the attacker's budget and adaptability.
  - **Quick check question:** If a defense blocks 100% of attacks in a fixed dataset (e.g., HarmBench), is it robust? (Answer: Not necessarily; it may have overfit to the dataset.)

- **Concept: Threat Models (Whitebox vs. Blackbox)**
  - **Why needed here:** The paper deploys different attack strategies (Gradient-based vs. Search-based) depending on the attacker's access level (gradients vs. generation only).
  - **Quick check question:** Which attack method is most suitable if you only have API access to output text? (Answer: Search-based or RL-based methods.)

- **Concept: Reward Hacking**
  - **Why needed here:** Section C.1 highlights that automated attacks might maximize the "score" without actually bypassing the defense (e.g., repeating a string to lower perplexity).
  - **Quick check question:** Why is defining the "Score" function in an RL attack critical? (Answer: To prevent the model from finding a trivial solution that scores high but fails the actual objective.)

## Architecture Onboarding

- **Component map:** Proposer -> Target System -> Scorer -> Updater
- **Critical path:** The flow from `Proposer` -> `Target System` -> `Scorer`. If the `Scorer` is noisy or the `Target` has low utility, the entire loop fails to converge.
- **Design tradeoffs:**
  - **Compute vs. Reliability:** Gradient-based attacks are fast but brittle on discrete tokens; Search-based methods are robust but computationally expensive.
  - **Automation vs. Human Effort:** Humans are slower but find "logical" bypasses; Automation is faster but prone to reward hacking.
- **Failure signatures:**
  - **Stagnation:** ASR plateaus early; the attack converges to a local minimum (e.g., repeating "Ignore instructions").
  - **High False Positives:** The defense blocks benign user requests (utility loss), visible in Table 7 (e.g., Protect AI lowering utility).
- **First 3 experiments:**
  1. **Baseline Search Attack:** Run the Search-based method (Section A.3) against an undefended model on AgentDojo to establish the upper bound of success.
  2. **Defense Evasion:** Apply the Search-based method against a specific defense (e.g., Spotlighting) to see if ASR drops or if the defense simply lowers utility.
  3. **Reward Hacking Check:** Run an RL-based attack on a defense like StruQ and manually inspect the "successful" triggers to ensure they aren't just repeating the target string artificially.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive attacks be optimized to maintain high success rates with significantly lower computational efficiency?
- Basis in paper: [explicit] Section 3 states, "In the future, we believe it would be interesting to investigate to what extent it is possible to achieve strong attacks like the ones we will present here more efficiently."
- Why unresolved: The current evaluation assumes an attacker with "considerable resources," leaving the trade-off between attack cost and success rate undefined.
- What evidence would resolve it: A study identifying the minimum computational budget required to achieve >90% ASR on the defended models.

### Open Question 2
- Question: How can automated scoring functions be designed to resist "reward hacking" while accurately proxying true attack success?
- Basis in paper: [inferred] Appendix C.1 details instances where RL-based attacks maximized the numerical score without actually bypassing the defense (reward hacking), forcing the authors to manually verify results.
- Why unresolved: Proxy metrics (e.g., perplexity or classifier logits) are themselves vulnerable to adversarial optimization, creating a gap between the score and actual safety violation.
- What evidence would resolve it: The development of a scoring mechanism that correlates 1:1 with manual human judgment across diverse attack strategies without being exploitable.

### Open Question 3
- Question: Can automated search methods be enhanced to match the query efficiency and creativity of collective human red-teaming?
- Basis in paper: [inferred] Section F.2 notes that in a head-to-head comparison, collective human red-teaming achieved a 100% success rate, whereas the automated search attack plateaued at 69%.
- Why unresolved: Automated mutators may lack the semantic reasoning to escape local optima that human attackers navigate intuitively.
- What evidence would resolve it: An automated agent that achieves parity with human success rates using a comparable or lower number of queries.

## Limitations

- Evaluation setting constraints: The study assumes attacker access to query outputs or feedback signals, which may not reflect all real-world deployment scenarios.
- Computational resource asymmetry: The paper acknowledges but does not quantify the resource disparity between defenders and attackers.
- Generalization boundaries: The paper does not establish whether defenses that succeed against adaptive attacks would also succeed against entirely novel attack strategies.

## Confidence

**High confidence:** The core finding that static defenses evaluated on fixed benchmarks are systematically bypassed by adaptive attacks. This is supported by consistent results across 12 defenses and multiple attack methods, with quantitative evidence showing 90%+ success rates versus claimed near-zero rates.

**Medium confidence:** The claim that human red-teaming is the most reliable attack method. While humans achieved 100% success, this is a small sample (12 defenses) and the evaluation may be biased by the specific human participants and their expertise.

**Medium confidence:** The assertion that defenses primarily fail by either blocking benign requests (utility loss) or being bypassed entirely. This interpretation is reasonable but the paper does not provide comprehensive utility metrics across all defenses.

## Next Checks

1. **Resource-Constrained Attacker Test:** Replicate the adaptive attack framework with computational budgets 10×, 100×, and 1000× lower than reported. Measure whether ASR degrades predictably with budget constraints, establishing practical attack thresholds.

2. **Feedback-Obscured Defense Evaluation:** Design defenses that provide minimal or obfuscated feedback (e.g., random delays, ambiguous error messages) and test whether adaptive attacks can still optimize effectively. This would validate whether the PSSU loop requires clear feedback signals.

3. **Cross-Domain Generalization Study:** Apply the successful attack templates from one domain (e.g., text-to-image models) to defenses in another domain (e.g., code generation or reasoning tasks). This would test whether the adaptive strategies transfer across semantic domains or are domain-specific.