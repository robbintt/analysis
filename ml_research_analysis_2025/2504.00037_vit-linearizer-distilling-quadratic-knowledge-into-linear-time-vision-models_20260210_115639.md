---
ver: rpa2
title: 'ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models'
arxiv_id: '2504.00037'
source_url: https://arxiv.org/abs/2504.00037
tags:
- arxiv
- teacher
- student
- vision
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ViT-Linearizer, a cross-architecture distillation\
  \ framework that transfers the rich representations of Vision Transformers (ViTs)\
  \ into linear-time, recurrent-style models like Adventurer with Mamba-2 token mixers.\
  \ The approach employs activation matching\u2014an intermediate constraint aligning\
  \ token-wise dependencies between teacher and student\u2014and masked prediction,\
  \ which requires the student to reconstruct teacher representations for unseen tokens."
---

# ViT-Linearizer: Distilling Quadratic Knowledge into Linear-Time Vision Models

## Quick Facts
- arXiv ID: 2504.00037
- Source URL: https://arxiv.org/abs/2504.00037
- Authors: Guoyizhe Wei; Rama Chellappa
- Reference count: 40
- Achieves 84.3% top-1 accuracy on ImageNet with linear-time models

## Executive Summary
This paper introduces ViT-Linearizer, a cross-architecture distillation framework that transfers rich Vision Transformer (ViT) representations into linear-time, recurrent-style models like Adventurer with Mamba-2 token mixers. The approach employs activation matching—an intermediate constraint aligning token-wise dependencies between teacher and student—and masked prediction, requiring the student to reconstruct teacher representations for unseen tokens. The method achieves competitive 84.3% top-1 accuracy on ImageNet while providing significant speedups for high-resolution tasks (4.2× faster on Cityscapes semantic segmentation). It also advances the state of the art for Mamba-based architectures, boosting Adventurer-Large from 83.4% to 85.0% on ImageNet.

## Method Summary
ViT-Linearizer uses a two-stage distillation process: activation matching constrains the student model to learn similar token-wise dependencies as the ViT teacher, while masked prediction forces the student to reconstruct teacher representations for tokens masked during training. This combination allows linear-time recurrent models to capture quadratic ViT knowledge efficiently. The framework is particularly effective for architectures using selective state space models like Mamba-2, which provide linear complexity through recurrent processing while maintaining competitive accuracy.

## Key Results
- Achieves 84.3% top-1 accuracy on ImageNet with base-sized linear-time model
- Improves Adventurer-Large from 83.4% to 85.0% on ImageNet, setting new state-of-the-art for Mamba-based architectures
- Provides 4.2× speedup on Cityscapes semantic segmentation for high-resolution tasks
- Demonstrates competitive accuracy with quadratic ViTs while maintaining linear computational complexity

## Why This Works (Mechanism)
The framework works by transferring quadratic ViT knowledge into linear-time architectures through two complementary constraints. Activation matching captures token-wise dependencies by aligning intermediate representations between teacher and student, while masked prediction ensures the student learns to reconstruct teacher representations for unseen tokens. This dual approach allows recurrent models to capture the rich contextual information typically associated with quadratic attention mechanisms, enabling linear-time models to match ViT accuracy without quadratic computational costs.

## Foundational Learning
- **Cross-architecture knowledge distillation**: Why needed - to transfer knowledge between structurally different models; Quick check - verify teacher-student architecture compatibility
- **Activation matching**: Why needed - to align intermediate representations between different architectures; Quick check - confirm token-wise dependency alignment
- **Masked prediction**: Why needed - to force student to reconstruct teacher knowledge for unseen tokens; Quick check - validate reconstruction accuracy
- **Linear-time recurrent architectures**: Why needed - to reduce computational complexity from quadratic to linear; Quick check - verify computational complexity claims
- **Selective state space models**: Why needed - to enable efficient recurrent processing in vision models; Quick check - confirm Mamba-2 implementation correctness

## Architecture Onboarding
- **Component map**: ViT teacher -> Activation matching module -> Masked prediction module -> Linear-time student (Adventurer with Mamba-2)
- **Critical path**: Forward pass through ViT teacher → Compute activation matching loss → Forward pass through student → Compute masked prediction loss → Backpropagation through student
- **Design tradeoffs**: Fidelity vs efficiency (activation matching may constrain student representations) vs computational cost of additional constraints
- **Failure signatures**: Poor performance indicates activation matching may be too restrictive or masked prediction not properly aligned with teacher representations
- **First experiments**: 1) Baseline distillation without activation matching, 2) Baseline with only activation matching, 3) Full framework with both components

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability beyond Mamba-based architectures to other linear-time models
- Architectural coupling from activation matching may constrain student's ability to develop alternative efficient representations
- Speedup claims depend on specific hardware and implementation details not fully specified

## Confidence
- **High confidence**: ImageNet accuracy improvements for Adventurer-Large (83.4% → 85.0%) and base model (84.3% top-1)
- **Medium confidence**: Speedup claims on high-resolution tasks (4.2× on Cityscapes)
- **Medium confidence**: The assertion that linear-complexity models can "match" ViTs in accuracy

## Next Checks
1. Test the distillation framework on alternative linear-time architectures beyond Mamba-based models (e.g., ConvNext, MLP-Mixer variants)
2. Conduct ablation studies isolating the contribution of activation matching versus masked prediction
3. Evaluate model robustness to distribution shifts and out-of-distribution data