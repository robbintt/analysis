---
ver: rpa2
title: Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations
arxiv_id: '2506.09067'
source_url: https://arxiv.org/abs/2506.09067
tags:
- demonstrations
- medical
- clinical
- safety
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the underexplored safety challenges in generative
  medical vision-language models (Med-VLMs), which risk over-defense (refusing benign
  queries) when defending against harmful ones. The authors propose a novel inference-time
  defense strategy using synthetic clinical demonstrations for in-context learning.
---

# Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations

## Quick Facts
- arXiv ID: 2506.09067
- Source URL: https://arxiv.org/abs/2506.09067
- Authors: Zhiyu Xue; Reza Abbasi-Asl; Ramtin Pedarsani
- Reference count: 40
- Primary result: Synthetic demonstrations reduce attack success rates on harmful medical queries by up to 77% while mitigating over-defense on benign queries through mixed demonstration strategies

## Executive Summary
This paper introduces a novel inference-time defense strategy for Medical Vision-Language Models (Med-VLMs) using synthetic clinical demonstrations for in-context learning. The method generates two types of demonstrations—Benign-Affirmative (B-A) and Harmful-Refusal (H-R)—via large language models without requiring medical expert input. By combining these demonstrations with adjustable mixing ratios, the approach effectively balances safety against harmful queries with performance on benign clinical questions. Experiments on the O2M benchmark demonstrate significant reductions in attack success rates while maintaining utility, addressing the critical over-defense problem common in safety mechanisms.

## Method Summary
The method employs in-context learning with synthetically generated demonstrations to steer Med-VLM behavior without parameter updates. It generates B-A demonstrations (benign queries with affirmative answers) using aligned LLMs like GPT-4o, and H-R demonstrations (harmful queries with refusal responses) using unaligned LLMs like WizardLM-13B. These demonstrations are combined in varying ratios (α) and orders to create mixed prompts that precede the actual input query. The approach requires no fine-tuning, instead leveraging the model's ability to learn from demonstration context. Demonstration pools are organized by modality and anatomical region, with budgets (n) ranging from 2 to 16 demonstrations. The method is evaluated against multiple attack types including PGD, GCG, and template-based attacks on the O2M benchmark.

## Key Results
- H-R demonstrations reduce ASR on harmful queries by up to 77% (72.58% → 17.88% for Llava-Med-v1 at n=2)
- Mixed demonstration strategy with α=0.5-0.75 effectively mitigates over-defense while maintaining safety
- H-R demonstrations increase attack loss for optimization-based attacks, making convergence harder
- Random mixing of demonstrations strikes optimal balance between robustness and safety (moderate ASR and RR)
- Over-defense persists under few-shot budgets (<4 demonstrations) but is alleviated with larger n

## Why This Works (Mechanism)

### Mechanism 1: In-Context Behavioral Steering via Synthetic Demonstrations
- **Claim:** Providing synthetically generated Harmful-Refusal (H-R) demonstrations in the prompt context reduces Attack Success Rate (ASR) on harmful clinical queries without requiring fine-tuning.
- **Mechanism:** The Med-VLM learns refusal patterns from H-R demonstrations through in-context learning (ICL), which activates implicit behavioral priors rather than updating parameters. The model mimics the refusal response patterns shown in demonstrations when encountering similar harmful query structures.
- **Core assumption:** The Med-VLM has sufficient ICL capability to generalize refusal behavior from synthetic examples to novel harmful queries, and the synthetic demonstrations adequately represent the distribution of harmful clinical queries.
- **Evidence anchors:** [abstract]: "our defense strategy based on synthetic clinical demonstrations enhances model safety without significantly compromising performance"; [section 3.3]: "the H-R demonstrations teach the model to recognize and reject unsafe requests"; [section 4.2]: "H-R demonstrations effectively reduce ASR, particularly in Llava-Med-v1 (72.58 → 17.88 at n = 2)"; [corpus]: Weak direct evidence; HSCR paper addresses modality misalignment but not ICL-based safety
- **Break condition:** If the Med-VLM lacks sufficient ICL capability (smaller models), or if synthetic demonstrations fail to cover novel harmful query patterns, the mechanism degrades.

### Mechanism 2: Loss Landscape Modification Against Optimization-Based Attacks
- **Claim:** H-R demonstrations increase the attack loss for optimization-based jailbreak attacks (PGD, GCG), making adversarial perturbation harder to converge.
- **Mechanism:** The presence of H-R demonstrations in the context shifts the model's internal representations, causing the gradient-based attack to require higher loss values at each step. This increases computational cost and reduces attack success probability within fixed iteration budgets.
- **Core assumption:** The attack loss landscape is meaningfully altered by the demonstration context, and this alteration generalizes across different image-query pairs.
- **Evidence anchors:** [section 4.3]: "H-R Demonstration changes the loss landscape, which makes it harder to be attacked"; [section 4.3, Fig. 5 description]: "the attack loss for utilizing H-R demonstrations is always higher than baselines"; [corpus]: No direct corpus evidence for this specific mechanism
- **Break condition:** If attackers can adaptively optimize demonstrations (many-shot jailbreaking) or use significantly larger iteration budgets, the protective effect diminishes.

### Mechanism 3: Mixed Demonstration Balance for Safety-Utility Tradeoff
- **Claim:** Combining H-R and B-A demonstrations with a mixing ratio α allows fine-grained control between defense (low ASR) and utility (low Refusal Rate on benign queries).
- **Mechanism:** B-A demonstrations reinforce affirmative response patterns, counterbalancing the over-defense tendency induced by H-R demonstrations. When demonstrations are uniformly interleaved (random mixing), the model learns to discriminate query types rather than applying blanket refusal.
- **Core assumption:** The Med-VLM can simultaneously learn both refusal and affirmative patterns from the same context, and the mixing order affects learning dynamics.
- **Evidence anchors:** [section 4.2]: "The mixing ratio α allows fine-grained control over the trade-off between defense and over-defense"; [section 4.3, Fig. 6]: "Mix 3 [random mixing] strikes a balance between robustness and safety, offering moderate ASR and RR"; [corpus]: OmniV-Med addresses modality integration but not safety-utility tradeoff mechanisms
- **Break condition:** If demonstration budget n is too small (<4), mixing cannot overcome over-defense; if query ambiguity is high (edge cases between harmful and benign), discrimination fails.

## Foundational Learning

- **In-Context Learning (ICL):**
  - **Why needed here:** The entire defense strategy relies on ICL to transfer safety behavior from synthetic demonstrations without parameter updates. Understanding how demonstrations influence model output distributions is essential for tuning mixing ratios.
  - **Quick check question:** Given a prompt with 4 H-R demonstrations, can you predict whether the model will refuse a novel harmful query? What factors might cause this to fail?

- **Jailbreak Attacks (PGD, GCG, Template-based):**
  - **Why needed here:** The method claims defense against visual (PGD), textual (GCG), and template-based (AIM, RS) attacks. Understanding attack mechanisms clarifies why loss landscape modification helps against optimization attacks but not necessarily template attacks.
  - **Quick check question:** Why would H-R demonstrations affect PGD attack convergence but potentially not a well-crafted template attack like AIM?

- **Over-Defense Problem:**
  - **Why needed here:** The paper explicitly addresses over-defense as a key failure mode where safety mechanisms cause models to reject benign clinical queries. Recognizing over-defense symptoms is critical for tuning α.
  - **Quick check question:** If a Med-VLM refuses a query like "What does this CT scan suggest about the lungs?", is this over-defense or appropriate safety behavior? How would you diagnose the cause?

## Architecture Onboarding

- **Component map:** Synthetic Demonstration Generator -> Demonstration Pool -> Mixing Module -> Inference Pipeline -> Evaluation Layer
- **Critical path:**
  1. Generate synthetic demonstrations offline (one-time cost)
  2. At inference, sample demonstrations matching input modality/organ
  3. Apply mixing strategy with configured α
  4. Construct prompt with demonstrations + input
  5. Pass to Med-VLM and evaluate response
- **Design tradeoffs:**
  - **Demonstration budget (n):** Higher n reduces over-defense but increases context length and latency
  - **Mixing ratio (α):** Higher α improves safety but risks over-defense; optimal range appears to be 0.5-0.75 based on trade-off curves
  - **Mixing method:** Random mixing (Mix 3) balances ASR and RR; B-A-first (Mix 1) prioritizes utility; H-R-first (Mix 2) prioritizes safety
- **Failure signatures:**
  - **High ASR with H-R demonstrations:** Insufficient ICL capability or demonstration-query mismatch
  - **High RR on benign queries:** Over-defense from high α with low n (<4)
  - **Persistent vulnerability to template attacks:** ICL-based defense may not generalize to prompt engineering attacks; consider additional guardrails
- **First 3 experiments:**
  1. **Baseline calibration:** Run Med-VLM on O2M benchmark without demonstrations to establish baseline ASR and RR for your target model
  2. **H-R demonstration scaling:** Test ASR and RR with H-R demonstrations at n = 2, 4, 8, 16 to identify over-defense threshold
  3. **Mixing ratio sweep:** With n = 4, test α = [0, 0.25, 0.5, 0.75, 1.0] to map the safety-utility tradeoff curve for your deployment context

## Open Questions the Paper Calls Out
None

## Limitations
- Over-defense persists under few-shot budgets (<4 demonstrations) even with mixed demonstrations
- Effectiveness against template-based attacks (AIM, RS) shows low confidence due to limited evaluation
- Performance depends heavily on demonstration quality and representativeness, generated without medical expert oversight
- Method requires larger demonstration budgets to fully mitigate over-defense, increasing latency

## Confidence
- **High confidence:** H-R demonstrations significantly reduce attack success rates (ASR reduction from 72.58% to 17.88% at n=2)
- **Medium confidence:** Mixed demonstration strategy effectively mitigates over-defense while maintaining safety
- **Low confidence:** Defense effectiveness against template-based attacks due to limited evaluation

## Next Checks
1. Test over-defense persistence by measuring refusal rates on benign queries across varying demonstration budgets (n=2, 4, 8, 16) with pure H-R demonstrations
2. Evaluate template attack vulnerability by testing AIM and RS attacks on the same O2M benchmark to quantify defense effectiveness against prompt engineering
3. Assess demonstration quality sensitivity by comparing performance when using GPT-4o vs smaller aligned LLMs for generating benign-affirmative demonstrations