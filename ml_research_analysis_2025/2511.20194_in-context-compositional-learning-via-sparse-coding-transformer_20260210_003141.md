---
ver: rpa2
title: In-Context Compositional Learning via Sparse Coding Transformer
arxiv_id: '2511.20194'
source_url: https://arxiv.org/abs/2511.20194
tags:
- compositional
- attention
- coefficients
- tasks
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of in-context compositional
  learning in Transformers, where models must infer and apply compositional rules
  from context examples to solve target problems. The authors propose a reformulation
  of the attention mechanism inspired by sparse coding, reinterpreting it as a mapping
  of inputs to outputs via projections onto two learned dictionaries: an encoding
  dictionary and a decoding dictionary.'
---

# In-Context Compositional Learning via Sparse Coding Transformer

## Quick Facts
- arXiv ID: 2511.20194
- Source URL: https://arxiv.org/abs/2511.20194
- Authors: Wei Chen; Jingxi Yu; Zichen Miao; Qiang Qiu
- Reference count: 40
- Primary result: Sparse coding reformulation of attention achieves up to 82.7% accuracy on S-RAVEN vs 65.1% baseline

## Executive Summary
This paper addresses the challenge of in-context compositional learning in Transformers, where models must infer and apply compositional rules from context examples to solve target problems. The authors propose a reformulation of the attention mechanism inspired by sparse coding, reinterpreting it as a mapping of inputs to outputs via projections onto two learned dictionaries: an encoding dictionary and a decoding dictionary. By enforcing sparsity on the coefficients obtained from the encoding dictionary, the model explicitly captures compositional structure. To transfer these rules, they estimate target task coefficients as a linear combination of context task coefficients.

## Method Summary
The proposed method reformulates multi-head attention as a sparse coding operation by factorizing it into encoding and decoding dictionaries. Standard attention σ(Xϕ(X))ψ(X) is replaced with prox(Xϕ(X))ψ(X), where prox is a soft-thresholding operator that promotes sparsity. The encoding dictionary ϕ(X) decomposes inputs into sparse coefficients, while the decoding dictionary ψ(X) reconstructs outputs from these coefficients. For in-context learning, target task coefficients are estimated as linear combinations of context coefficients, enabling compositional rule transfer without observing the target input. This approach explicitly captures compositional structure while maintaining computational efficiency.

## Key Results
- On S-RAVEN dataset: 82.7% accuracy vs 65.1% for baseline
- On RA-VEN dataset: Maintains over 30% samples with PSNR above 40 vs baseline nearly drops to zero
- Achieves 90-98% sparsity while maintaining high accuracy across tasks

## Why This Works (Mechanism)

### Mechanism 1: Sparse Coefficient Representation
- Claim: Imposing sparsity on attention coefficients enables explicit representation of compositional rules by preserving informative components while suppressing redundant interactions.
- Mechanism: Replace softmax normalization with soft-thresholding prox(x) = sign(x) ⊙ max(|x| - ξ, 0), which shrinks small values toward zero, yielding sparse coefficient matrices α = σ(Xϕ(X)).
- Core assumption: Compositional rules are encoded in sparse, structured coefficient patterns rather than dense attention distributions.

### Mechanism 2: Dual Dictionary Factorization
- Claim: Reinterpreting attention as projections onto encoding and decoding dictionaries separates the decomposition and reconstruction phases, enabling explicit compositional structure capture.
- Mechanism: Factorize MHA as σ(Xϕ(X))ψ(X) where ϕ(X) generates the encoding dictionary (decomposing input into coefficients) and ψ(X) generates the decoding dictionary (reconstructing output from coefficients).
- Core assumption: The encoding-decoding separation provides better inductive bias for compositional tasks than coupled query-key-value projections.

### Mechanism 3: Linear Coefficient Transfer
- Claim: Target task coefficients can be estimated as linear combinations of context coefficients, enabling compositional rule transfer without observing the target input.
- Mechanism: Update αL ← αL + Σᵢλᵢαᵢ where λᵢ are learned parameters. Since prox(0) = 0, without this transfer the target output would be zero; the linear combination transfers compositional patterns from context.
- Core assumption: Target coefficients lie in the span of context coefficients when compositional rules are shared.

## Foundational Learning

- **Sparse coding / Dictionary learning**
  - Why needed here: Core mathematical framework. Must understand how signals decompose into sparse combinations of dictionary atoms.
  - Quick check question: Given a dictionary D ∈ R^(m×d) with m > d, can you explain why sparse coefficients provide more interpretable structure than dense ones?

- **Soft-thresholding operator**
  - Why needed here: Replaces softmax as the sparsity-promoting nonlinearity. Understanding prox(x) = sign(x)⊙max(|x|-ξ, 0) is essential.
  - Quick check question: What happens to prox(x) when ξ > |x|? When x > ξ?

- **In-context learning formulation**
  - Why needed here: Problem framing. Model must predict target from context demonstrations without weight updates.
  - Quick check question: How does in-context compositional learning differ from standard supervised learning when test tasks involve R_test ∉ span(R_train)?

## Architecture Onboarding

- **Component map:** Input X → [Encoding Dictionary ϕ(X)] → Coefficients Xϕ(X) → [Sparsity prox(·)] → Sparse α → [Coefficient Transfer g(α) for target] → [Decoding Dictionary ψ(X)] → Output Z
- **Critical path:**
  1. Implement soft-thresholding with learnable threshold ξ (initialization matters)
  2. Implement coefficient transfer g(α) with learnable λ parameters (L-1 per layer)
  3. Verify: when XL=0, output ZL should be zero before coefficient transfer, nonzero after
- **Design tradeoffs:**
  - Threshold ξ: Controls sparsity (Table 1 shows 0.003→18.53% sparse, 0.3→99.38% sparse). Higher sparsity may lose information.
  - Basis function design: ReLU on ϕ/ψ increases accuracy (Table 2: 71.7%→73.6%) at cost of complexity.
  - Number of λ parameters: Scales with context length L-1, but remains small relative to full model parameters.
- **Failure signatures:**
  - Blurry outputs (high PSNR variance): Indicates insufficient sparsity or coefficient transfer not learning meaningful λ values.
  - Zero target outputs: Coefficient transfer disabled or λ not initialized/trained properly.
  - Performance collapse on novel rules: Dictionary atoms may not generalize; check if training rules cover sufficient primitive diversity.
- **First 3 experiments:**
  1. Reproduce synthetic toy experiment (Figure 3): Single-layer, 8×8 binary images, train on rule (A,∅,∅,B)→(B,∅,∅,C)→(C,∅,∅,A), test on (∅,A,B,∅) rule. Compare PSNR with baseline.
  2. Ablate sparsity alone (without coefficient transfer): Replace softmax with prox but set λ=0. Measure degradation on RAVEN.
  3. Sweep threshold ξ on S-RAVEN: Plot accuracy vs. sparsity level to find operating regime (Table 1 suggests 90-98% sparsity).

## Open Questions the Paper Calls Out
- **Integration with large-scale models**: Application to large pre-trained models (e.g., LLMs) without causing instability or catastrophic forgetting remains unexplored.
- **Generative task applicability**: Performance on translation, summarization, or image generation tasks has not been evaluated.
- **Robustness to incomplete context**: Performance when context examples provide incomplete coverage of dictionary atoms (violating Assumption 9.3) is not analyzed.

## Limitations
- Evaluation restricted to synthetic compositional datasets (S-RAVEN, RA-VEN) rather than real-world tasks
- Strong assumptions about coefficient span may not hold for tasks with novel primitives
- Additional computational overhead from dictionary learning and soft-thresholding operations

## Confidence
- **High Confidence**: Sparse coefficient representation improves compositional generalization on tested synthetic datasets; dual dictionary factorization provides explicit compositional structure capture; coefficient transfer mechanism successfully enables compositional rule application to unseen inputs
- **Medium Confidence**: The proposed method's advantage generalizes beyond the specific synthetic domains tested; the operating point for sparsity threshold (90-98% sparsity) represents an optimal balance for all compositional tasks; the soft-thresholding operator consistently outperforms softmax across different compositional learning scenarios
- **Low Confidence**: Performance on real-world compositional tasks (e.g., language, complex vision); scalability to longer sequences and larger dictionaries; robustness to noisy or ambiguous context demonstrations

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate the method on a real-world compositional task such as SCAN dataset for compositional language understanding or CLEVR for visual reasoning. Compare performance against standard Transformers and specialized compositional models.
2. **Dictionary Capacity Analysis**: Systematically vary the dictionary size (number of atoms) and measure the trade-off between compositional accuracy and dictionary capacity. Identify whether the method scales effectively as the complexity of compositional rules increases.
3. **Noise Robustness Evaluation**: Introduce varying levels of noise or ambiguity in context demonstrations and measure how performance degrades compared to the baseline. This validates whether the sparse representation is robust to imperfect demonstrations, a critical requirement for practical in-context learning.