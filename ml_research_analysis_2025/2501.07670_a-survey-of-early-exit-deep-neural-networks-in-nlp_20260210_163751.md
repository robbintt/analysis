---
ver: rpa2
title: A Survey of Early Exit Deep Neural Networks in NLP
arxiv_id: '2501.07670'
source_url: https://arxiv.org/abs/2501.07670
tags:
- exit
- layer
- early
- arxiv
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys Early Exit Deep Neural Networks (EEDNNs) in
  NLP, focusing on how they enable adaptive inference by allowing simpler samples
  to exit early from intermediate layers, thereby reducing computational overhead.
  EEDNNs attach classifiers at various layers, offering benefits like faster inference,
  input adaptiveness, robustness, and interpretability.
---

# A Survey of Early Exit Deep Neural Networks in NLP

## Quick Facts
- arXiv ID: 2501.07670
- Source URL: https://arxiv.org/abs/2501.07670
- Authors: Divya Jyoti Bajpai; Manjesh Kumar Hanawal
- Reference count: 20
- Key outcome: Surveys EEDNNs in NLP, showing how they reduce computational overhead by allowing simpler samples to exit early while maintaining accuracy.

## Executive Summary
This survey comprehensively examines Early Exit Deep Neural Networks (EEDNNs) for natural language processing tasks. EEDNNs attach classifiers at multiple intermediate layers of transformer models, enabling adaptive inference where samples exit when confidence thresholds are met. The paper systematically categorizes training strategies, exit criteria, confidence metrics, and applications across text classification, NLI, translation, and vision-language tasks. It identifies critical challenges including "fake confidence" where models exit early with incorrect predictions, domain shift affecting static thresholds, and the computational overhead of exit classifiers for generation tasks.

## Method Summary
EEDNNs modify transformer architectures by attaching classification heads to intermediate layers. The survey distinguishes between separate training (fine-tune backbone, then freeze and train exits) and joint training (simultaneous optimization using weighted loss). During inference, samples pass through layers sequentially until a confidence metric (max probability, entropy, or patience-based consistency) exceeds a threshold, at which point they exit early. The method reduces computational overhead by 40-80% on average while maintaining accuracy, particularly benefiting simpler samples that exit at initial layers.

## Key Results
- EEDNNs achieve 40-80% computational reduction by enabling early exits for ~80% of "confident" samples
- Separate training preserves backbone performance while joint training risks final-layer degradation
- Dynamic threshold adaptation (MAB-based) addresses domain shift but requires infrastructure assumptions
- "Fake confidence" remains an unsolved problem where high-confidence early exits produce incorrect predictions

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Based Early Exit
- **Claim**: EEDNNs reduce computational overhead by allowing samples to exit early when confidence thresholds are met at intermediate layers.
- **Mechanism**: Classifiers attached at multiple intermediate layers produce predictions; when confidence score Ci ≥ threshold α, the sample exits without traversing deeper layers. Approximately 80% of samples ("confident" category) achieve sufficient confidence at initial layers.
- **Core assumption**: Simpler samples achieve high prediction confidence at shallower layers, while complex samples require deeper processing.
- **Evidence anchors**:
  - [abstract]: "simpler samples are classified using the initial layers of the DNN, thereby accelerating the overall inference process"
  - [section 2]: Figure 2 shows ~80% of SST-2 samples labeled 'confident' exhibit high confidence at initial layers
  - [corpus]: "Know What You Don't Know" paper validates confidence-based selective prediction for early exit DNNs
- **Break condition**: When samples exhibit "fake confidence" (high confidence on wrong class at shallow layers), leading to mispredictions that the survey explicitly identifies as an unsolved problem (Section 7.3).

### Mechanism 2: Joint Training Optimization
- **Claim**: Joint training of exit classifiers and backbone simultaneously optimizes both intermediate and final layer performance.
- **Mechanism**: Loss function L = Σ(wi*Li) combines weighted losses from all exit classifiers; weights can be layer-indexed (wi = i) or learned via sigmoid transformation (DynExit).
- **Core assumption**: Deeper layers should receive more emphasis, and intermediate classifiers provide gradient signals that benefit the backbone through regularization.
- **Evidence anchors**:
  - [section 3.2]: "the loss function is: L = Σ wi Li. This method simultaneously finetunes the backbone and learns the classifier weights"
  - [section 2]: "attaching classifiers at multiple layers... also offer more direct gradient signals for backpropagation, particularly from shallower layers"
  - [corpus]: "Confidence-gated training" paper explicitly addresses gradient interference where deeper classifiers dominate optimization
- **Break condition**: When gradient interference causes deeper classifiers to dominate optimization, degrading early exit performance—a known issue that motivated alternate training strategies like BERxiT.

### Mechanism 3: Patience-Based Exit Criteria
- **Claim**: Patience-based exit criteria (multiple consecutive classifiers agreeing) provides more robust predictions than single-classifier confidence.
- **Mechanism**: Instead of relying on a single classifier's confidence, PABEE requires predictions from several consecutive classifiers to remain consistent before exiting.
- **Core assumption**: Consistent predictions across layers indicate genuine model confidence rather than noise or adversarial perturbations.
- **Evidence anchors**:
  - [section 3.3]: "PABEE takes a different approach by defining confidence based on prediction consistency across multiple exit classifiers"
  - [section 2.1]: "advantage of this method is that it reduces the chances of adversarial attacks as its predictions are based on multiple classifier's output"
  - [corpus]: SQUAD paper proposes ensemble quorum decisions to address calibration issues in single-threshold approaches
- **Break condition**: When intermediate classifiers systematically disagree due to architectural issues or when syntactic vs. semantic processing stages produce fundamentally different representations.

## Foundational Learning

- **Concept: Confidence Calibration and Entropy vs. Max-Probability**
  - Why needed here: Understanding how classifier confidence relates to actual accuracy is essential for threshold selection. DeeBERT uses entropy while CeeBERT uses max-probability—knowing when each works better is critical.
  - Quick check question: For a 3-class problem with output [0.5, 0.3, 0.2] vs. [0.5, 0.49, 0.01], which would entropy-based confidence rank higher and why might this matter?

- **Concept: Multi-Armed Bandit Formulation**
  - Why needed here: Dynamic threshold adaptation under domain shift (CeeBERT, UCBEE) is framed as an exploration-exploitation problem where actions are threshold values.
  - Quick check question: In the CeeBERT formulation, how would you design a reward function that trades off confidence against latency when you observe neither the true label nor computational cost at test time?

- **Concept: Knowledge Distillation for Exit Training**
  - Why needed here: FastBERT, LeeBERT, and CapEEN use distillation from final layer to improve intermediate classifiers without modifying the backbone.
  - Quick check question: Why might soft labels (probability distributions) from the final layer help early exit classifiers more than hard labels alone, particularly for samples near decision boundaries?

## Architecture Onboarding

- **Component map**: Input tokenization → embedding layer → sequential transformer layers → at each exit: hidden state → classifier → confidence score → threshold check → return prediction or continue
- **Critical path**:
  1. Input tokenization → embedding layer
  2. Sequential processing through transformer layers
  3. At each exit layer: Hidden state → classifier → confidence score
  4. If confidence ≥ threshold: Return prediction immediately
  5. Else: Continue to next layer
  6. Final layer always produces prediction if no early exit

- **Design tradeoffs**:
  - Exit placement density vs. parameter overhead (LLaMA with exit at every layer would add 4B parameters—exceeding model size)
  - Linear vs. complex exits: Single-layer exits add ~0.1ms but may lack capacity for complex tasks like generation
  - Static vs. dynamic thresholds: Static fails under domain shift; dynamic requires online learning infrastructure and assumes reward signal availability
  - Separate vs. joint training: Separate preserves backbone; joint improves exits but risks final-layer degradation

- **Failure signatures**:
  - Fake confidence: High confidence on wrong class at shallow layers (Figure 2, Section 7.3)—no proven solution
  - Domain shift collapse: Confidence distribution changes invalidate static thresholds
  - Vocabulary explosion: For generation tasks, exit classifiers with full vocabulary (HCN) become larger than backbone layers
  - Overhead dominance: Exit computation exceeds savings from early stopping

- **First 3 experiments**:
  1. Replicate Figure 2 confidence patterns on your dataset: Plot average confidence on true class across layers; categorize samples as confident/confused/fake-confidence to quantify the "fake confidence" problem before selecting exit strategy.
  2. Training strategy ablation on text classification: Compare separate vs. joint training on BERT-base with SST-2; measure (a) early exit accuracy at each layer, (b) final layer degradation, (c) wall-clock speedup. Use patience-based exiting with k=3.
  3. Domain shift robustness test: Train on source domain (IMDB reviews), test on target domain (Yelp reviews) with static threshold from validation. Then implement UCB-based threshold adaptation following CeeBERT and measure iterations to recover efficiency-accuracy trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can early exit architectures detect and mitigate "fake confidence" where intermediate classifiers assign high probabilities to incorrect classes?
- Basis: [explicit] The authors identify "fake confidence" samples in Section 7.3 as a critical issue leading to early mispredictions.
- Why unresolved: Standard confidence metrics (e.g., entropy, max probability) often fail to distinguish between genuine certainty and false certainty in shallow layers.
- What evidence would resolve it: Calibration metrics or uncertainty measures specifically designed for intermediate layers that successfully flag high-confidence errors before the exit decision is made.

### Open Question 2
- Question: What constitutes the optimal placement strategy for exit classifiers in large-scale models (e.g., LLaMA) to balance parameter efficiency and accuracy?
- Basis: [explicit] Section 7.1 notes that standard per-layer placement creates prohibitive parameter overhead (e.g., 4B extra parameters for OPT 2.7B) and necessitates strategic spacing.
- Why unresolved: Current research lacks a standardized method for determining which specific layers provide diminishing returns versus necessary semantic depth.
- What evidence would resolve it: An automated framework identifying layer saturation points that minimizes performance degradation while maximizing computational savings.

### Open Question 3
- Question: How can formal risk management be integrated into early exit decisions to prevent confidently wrong predictions?
- Basis: [explicit] Section 7.2 highlights that EEDNNs face amplified risks as multiple classifiers can yield incorrect predictions, a problem currently lacking sufficient insight.
- Why unresolved: Existing thresholding focuses primarily on the efficiency-accuracy trade-off rather than safety or minimizing the risk of catastrophic misclassification.
- What evidence would resolve it: A risk-controlled inference mechanism that formally bounds the error probability based on threshold selection.

## Limitations

- **Fake confidence problem**: No proven solutions exist for samples exiting early with high confidence but incorrect predictions, particularly problematic for OOD detection
- **Domain shift vulnerability**: Static thresholds fail when confidence distributions change across domains, requiring dynamic adaptation infrastructure
- **Parameter overhead for generation**: Exit classifiers with full vocabulary (HCN) can exceed backbone size, negating computational savings

## Confidence

- **High confidence**: Core mechanism of confidence-based early exit reducing computational overhead is well-supported by multiple papers and empirical validation (Section 2, Figure 2)
- **Medium confidence**: Training strategy tradeoffs (separate vs. joint) are theoretically sound but empirical comparisons show mixed results depending on task complexity and dataset characteristics
- **Low confidence**: Dynamic threshold adaptation effectiveness varies significantly across domains, with some papers showing successful MAB-based approaches while others report instability without proper reward signal design

## Next Checks

1. **Fake Confidence Validation**: Replicate Figure 2 confidence patterns on your dataset, categorizing samples as confident/confused/fake-confidence. Quantify the prevalence of fake confidence before selecting exit strategy.
2. **Training Strategy Ablation**: Compare separate vs. joint training on BERT-base with SST-2, measuring early exit accuracy, final layer degradation, and wall-clock speedup using patience-based exiting (k=3).
3. **Domain Shift Robustness**: Train on source domain (IMDB), test on target domain (Yelp) with static threshold, then implement UCB-based threshold adaptation following CeeBERT and measure iterations to recover efficiency-accuracy trade-off.