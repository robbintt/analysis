---
ver: rpa2
title: 'XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization'
arxiv_id: '2508.10395'
source_url: https://arxiv.org/abs/2508.10395
tags:
- memory
- cache
- quantization
- which
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "XQuant addresses the memory bottleneck in large language model\
  \ inference by quantizing and caching layer input activations (X) instead of the\
  \ standard Key-Value (KV) cache. This approach provides a 2\xD7 memory savings compared\
  \ to KV cache quantization, as only one tensor per layer needs to be stored."
---

# XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization

## Quick Facts
- **arXiv ID**: 2508.10395
- **Source URL**: https://arxiv.org/abs/2508.10395
- **Reference count**: 40
- **Primary result**: Up to 12.5× memory savings with <0.1 perplexity degradation by quantizing layer input activations (X) instead of KV cache

## Executive Summary
XQuant addresses the memory bottleneck in large language model inference by quantizing and caching layer input activations (X) instead of the standard Key-Value (KV) cache. This approach provides a 2× memory savings compared to KV cache quantization, as only one tensor per layer needs to be stored. By rematerializing the Keys and Values on-the-fly during inference, XQuant achieves up to 7.7× memory savings with less than 0.1 perplexity degradation compared to the FP16 baseline. Building on the observation that X values are similar across layers, XQuant-CL further exploits cross-layer similarity in X embeddings for extreme compression. Using standard asymmetric uniform quantization, XQuant-CL attains up to 10× memory savings with only 0.01 perplexity degradation and 12.5× memory savings with 0.1 perplexity degradation relative to the FP16 baseline.

## Method Summary
XQuant quantizes and caches the input activations (X) to each layer instead of the KV cache, enabling 2× memory savings for MHA models and up to 12.5× savings for the cross-layer variant (XQuant-CL). The method rematerializes Keys and Values during inference by multiplying the cached X with the weight matrices. For Grouped Query Attention (GQA) models, XQuant applies SVD to the projection matrices offline, down-projects X to a lower-dimensional latent space, and quantizes the latent representations. XQuant-CL exploits cross-layer similarity in X embeddings by quantizing deltas between successive layers, maintaining an accumulator for rematerialization.

## Key Results
- 2× memory savings compared to KV cache quantization for standard XQuant
- Up to 7.7× memory savings with <0.1 perplexity degradation for XQuant
- Up to 12.5× memory savings with 0.1 perplexity degradation for XQuant-CL
- Outperforms state-of-the-art KV cache quantization methods while maintaining near-FP16 accuracy

## Why This Works (Mechanism)
XQuant works by exploiting the observation that layer input activations (X) are highly correlated across layers, making them amenable to aggressive quantization. By caching only X and rematerializing K and V on-the-fly, the method reduces memory requirements by half compared to traditional KV cache quantization. The cross-layer variant (XQuant-CL) further exploits the similarity between successive X embeddings by quantizing their deltas, enabling extreme compression ratios. For GQA models, SVD-based dimensionality reduction in the latent space allows efficient quantization while preserving the essential information needed for attention computation.

## Foundational Learning
- **Asymmetric uniform quantization**: A quantization scheme where the quantized values are not necessarily symmetric around zero, allowing better representation of data with asymmetric distributions. Why needed: X values often have asymmetric distributions that benefit from asymmetric quantization. Quick check: Verify quantized values span the full range of the original data with minimal clipping.
- **Group size in quantization**: The number of consecutive elements that share the same quantization parameters (scale and zero-point). Why needed: Larger group sizes reduce memory overhead for storing quantization parameters. Quick check: Confirm group size of 128 is used as specified.
- **Singular value decomposition (SVD)**: A matrix factorization technique that decomposes a matrix into three components: U, Σ, and V^T. Why needed: Enables dimensionality reduction for GQA models by projecting X into a lower-dimensional latent space. Quick check: Verify SVD is applied to Wk and Wv offline as specified.
- **Cross-layer delta quantization**: A compression technique that quantizes the difference between successive layer inputs rather than the absolute values. Why needed: Exploits the similarity between successive X embeddings for extreme compression. Quick check: Confirm delta computation and accumulation mechanism is implemented correctly.
- **Per-channel vs per-token quantization**: Different strategies for assigning quantization parameters to different dimensions of the data. Why needed: Per-channel for Keys/XUk and per-token for Values/XUv optimize the trade-off between accuracy and memory. Quick check: Verify the correct quantization strategy is applied to each tensor type.
- **Rematerialization**: The process of recomputing intermediate values during inference instead of storing them. Why needed: Enables the memory savings of XQuant by computing K and V on-the-fly from cached X. Quick check: Confirm K and V are correctly rematerialized during attention computation.

## Architecture Onboarding

### Component Map
Input -> X Quantization -> Cached X -> Rematerialization (K,V) -> Attention -> Output
                    ↓
            Delta Computation -> Delta Quantization -> XQuant-CL

### Critical Path
1. Input preprocessing and X extraction
2. X quantization and caching
3. K and V rematerialization during attention
4. Delta computation and quantization for XQuant-CL
5. Accumulator maintenance for cross-layer compression

### Design Tradeoffs
- Memory vs. Compute: XQuant trades additional compute for significant memory savings by rematerializing K and V
- Precision vs. Compression: Higher compression ratios (XQuant-CL) require careful precision management to maintain accuracy
- Per-channel vs. Per-token quantization: Different strategies for different tensor types optimize the accuracy-memory tradeoff

### Failure Signatures
- High perplexity (>1.0 degradation): Likely issues with quantization parameters, group size, or residual token handling
- Insufficient memory savings: May indicate caching both X and KV instead of only X
- Latency increase: Rematerialization overhead may be too high for the target hardware

### First Experiments
1. Implement standard XQuant for MHA models and verify 2× memory savings and perplexity parity with FP16 baseline
2. Extend XQuant to GQA models with SVD-based dimensionality reduction and verify the memory-accuracy tradeoff curve
3. Implement XQuant-CL and measure perplexity degradation at different compression ratios to establish the limits of cross-layer compression

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but raises several important considerations in the Limitations section, including the impact of rematerialization on actual wall-clock latency and throughput, the potential for calibration-free quantization using weight matrix analysis, and the applicability of XQuant-CL to architectures without standard residual connections.

## Limitations
- Rematerialization requires additional compute operations which may increase latency on particular hardware platforms
- The method relies on the residual stream structure of Transformers, potentially limiting applicability to other architectures
- Exact implementation details for SVD truncation and RoPE handling are not fully specified

## Confidence
- **High Confidence**: The 2× memory savings claim for standard XQuant over KV quantization is straightforward given that only X needs to be stored instead of separate K and V caches. The per-token vs per-channel quantization distinction for MHA vs GQA models is clearly specified.
- **Medium Confidence**: The perplexity degradation measurements (0.01 at 10× compression, 0.1 at 12.5× compression) are methodologically sound but depend critically on the unspecified SVD truncation and RoPE handling details.
- **Low Confidence**: The exact implementation of the cross-layer compression (XQuant-CL) accumulator mechanism and its precision trade-offs during sequential delta summation.

## Next Checks
1. Implement and benchmark the SVD-based dimensionality reduction for GQA models with different truncation thresholds to verify the claimed memory-latency trade-off curve.
2. Measure perplexity degradation when varying the group size parameter (currently fixed at 128) to establish the sensitivity of XQuant-CL performance to this hyperparameter.
3. Profile the actual memory bandwidth savings during generation by instrumenting a KV cache replacement with XQuant's quantized X cache and measuring cache-miss rates and rematerialization compute overhead.