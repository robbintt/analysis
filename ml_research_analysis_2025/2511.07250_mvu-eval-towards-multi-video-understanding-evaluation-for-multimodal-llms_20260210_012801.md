---
ver: rpa2
title: 'MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs'
arxiv_id: '2511.07250'
source_url: https://arxiv.org/abs/2511.07250
tags:
- video
- arxiv
- wang
- zhang
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MVU-Eval, the first comprehensive benchmark
  designed to evaluate Multi-Video Understanding (MVU) capabilities of Multimodal
  Large Language Models (MLLMs). Unlike existing benchmarks that focus on single-video
  analysis, MVU-Eval addresses the critical need for models to process and reason
  across multiple video sources simultaneously.
---

# MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs

## Quick Facts
- arXiv ID: 2511.07250
- Source URL: https://arxiv.org/abs/2511.07250
- Reference count: 23
- Multi-video understanding remains a significant challenge for current MLLMs, with top models achieving only ~56% accuracy on this benchmark.

## Executive Summary
MVU-Eval introduces the first comprehensive benchmark for evaluating multi-video understanding capabilities of multimodal large language models. The benchmark contains 1,824 question-answer pairs spanning 4,959 videos from diverse domains, requiring models to reason across multiple video sources simultaneously. Extensive evaluation of 27 state-of-the-art models reveals significant performance gaps, with even the best model achieving only 56.6% accuracy. The study identifies critical challenges including cross-video alignment, spatial reasoning, and temporal understanding, highlighting the need for new architectural approaches to handle multiple asynchronous video streams effectively.

## Method Summary
The benchmark evaluates models using zero-shot multiple-choice QA across 1,824 questions and 4,959 videos, with an average of 4.7 videos per question. Models receive 32 frames per video (longer side resized to 720px) and must output a single letter answer. The evaluation includes eight core competencies: object recognition, spatial understanding, counting, comparison, knowledge-intensive reasoning, in-context learning, retrieval-augmented generation, and temporal reasoning. The benchmark is publicly available on GitHub and includes specific prompt templates for different task types.

## Key Results
- Top-performing model (Gemini 2.5 Pro) achieves only 56.6% accuracy
- Most open-source models score below 50% accuracy
- Performance degrades significantly when processing more than 64 frames or exceeding 720p resolution
- Native multi-video input formats outperform merged-video and multi-image alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Processing multiple videos concurrently demands scaling both context length and model capacity, but gains plateau when visual inputs exceed an optimal complexity threshold.
- Mechanism: Larger model capacity supports more complex cross-video reasoning and alignment, while increased frame counts and higher resolutions provide necessary temporal and spatial detail. However, excessive visual tokens can overwhelm the model's fusion mechanisms, leading to performance degradation.
- Core assumption: Failure at higher complexity thresholds is due to architectural limitations in attention or fusion mechanisms, not lack of pre-training data.
- Evidence anchors:
  - [abstract] "larger models generally perform better, that performance degrades with increasing numbers of videos..."
  - [section 4.2, Figure 7] "VideoLLaVA3 generally exhibits improved performance with an increasing number of frames. However... we observe a performance degradation due to excessive input tokens."
  - [corpus] Corpus signals on multi-video understanding show this is a recognized, unsolved challenge.

### Mechanism 2
- Claim: Multi-video understanding requires reasoning over content that cannot be retrieved from any single video alone, making cross-video alignment and fusion critical.
- Mechanism: The model must identify corresponding entities or temporal states across different video streams and integrate this information to answer a query.
- Core assumption: Current architectures treat multiple videos as a longer sequence of frames rather than as distinct, related streams.
- Evidence anchors:
  - [abstract] "current MLLMs struggle with cross-video alignment, spatial reasoning, and temporal understanding."
  - [section 3.1] "each question in MVU-Eval requires cross-video integration, demanding not just accurate perception but contextual synthesis of temporal and spatial relationships across disparate visual sequences."

### Mechanism 3
- Claim: Input format and modality representation significantly impact a model's ability to perform multi-video reasoning.
- Mechanism: Providing multiple frames per video (preserving temporal dynamics) and indicating distinct video boundaries allows better temporal and cross-video representations than using single frames or merging videos.
- Evidence anchors:
  - [section 4.2, Table 4] Multi-video input (47.5%) is far superior to single-video (24.9%) or multi-image (34.6%) input.
  - [section 4.2, Table 5] Native multi-video input for Qwen2.5-VL-7B (51.9%) outperforms both merged-video (44.6%) and multi-image per video (45.2%) formats.

## Foundational Learning

- Concept: **Cross-Modal Alignment**
  - Why needed here: Multi-video understanding fundamentally requires aligning visual features from different video streams to a common conceptual space before reasoning can occur.
  - Quick check question: Can you explain how a model would identify that "the car in Video 1" is the same vehicle type as "the car in Video 2" for a spatial understanding task?

- Concept: **Token Budget and Context Window**
  - Why needed here: The paper highlights performance degradation when input tokens become excessive. Understanding this constraint is key to designing efficient multi-video systems.
  - Quick check question: Given a model with a 128K token context window and a video that produces 500 tokens per frame, what is the maximum number of frames you could theoretically process from 4 videos, and what trade-off does this calculation ignore?

- Concept: **Zero-Shot Evaluation**
  - Why needed here: The MVU-Eval benchmark is evaluated in a zero-shot setting. This means models are not fine-tuned on the benchmark, testing their general pre-trained capabilities.
  - Quick check question: Why is zero-shot accuracy a more rigorous measure of a model's general multi-video understanding ability than fine-tuned accuracy?

## Architecture Onboarding

- Component map: Vision Encoder -> Vision-Language Adapter -> Core LLM -> Answer Generation
- Critical path: Frame Extraction from all videos -> Vision Encoding (per frame) -> Token Projection -> Concatenation of all tokens -> Core LLM Processing -> Answer Generation
- Design tradeoffs:
  - More frames vs. Token Limit: Increasing frames improves temporal understanding but consumes the context window
  - Higher Resolution vs. Compute: Higher resolution improves detail recognition but increases token count quadratically
  - Native Multi-Video vs. Merged Input: Native support is better than merging videos as it may preserve distinct video boundaries
- Failure signatures:
  - Format Leakage: The model answers correctly without looking at the video
  - Instruction Following Failure: The model generates free-form descriptive text instead of selecting an option
  - Single-Video Bias: The model's answer is based on a dominant or random single video
  - Context Overflow: Model performance drops sharply when visual tokens exceed a critical threshold
- First 3 experiments:
  1. Baseline Single-Video Performance: Run your model on a subset of MVU-Eval tasks by providing only one video per question
  2. Ablate Input Format: Test the model using native multi-video input, merged video input, and multi-image input
  3. Vary Frame Count and Resolution: For a fixed multi-video QA task, vary the number of frames and resolution to identify the optimal point before performance degrades

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can architectures be designed to align frames across asynchronous multi-video inputs that lack shared temporal boundaries?
- Basis in paper: [explicit] Section 6 lists "Cross-video Visual Alignment" as a future direction, noting most MVU-Eval videos are asynchronous and lack shared timelines.
- Why unresolved: Current MLLMs assume temporally aligned inputs; no mechanism exists for inferring relative offsets between unrelated video streams.
- What evidence would resolve it: A model achieving strong performance on MVU-Eval tasks requiring temporal synchronization without manual alignment cues.

### Open Question 2
- Question: What fusion strategies enable scalable processing of 10+ videos without token-limit degradation?
- Basis in paper: [explicit] Section 6 identifies "Scalable Multi-Modal Fusion for High-Cardinality Inputs"; Figure 7 shows performance drops when frames exceed 64 or resolution exceeds 720p.
- Why unresolved: Attention mechanisms scale quadratically with token count; current compression methods underperform on multi-video tasks.
- What evidence would resolve it: A method maintaining >50% accuracy on 10-video MVU-Eval subsets with bounded memory/compute costs.

### Open Question 3
- Question: How can models robustly identify anchor objects across videos from diverse domains (indoor, outdoor, gaming, AIGC)?
- Basis in paper: [explicit] Section 6 calls for "Cross-video Spatial Understanding" via anchor-point identification; Section 4.2 notes failures in spatial reasoning across multi-camera autonomous driving scenes.
- Why unresolved: Domain shifts cause feature misalignment; Figure 8 shows models confuse spatial relations across camera angles.
- What evidence would resolve it: Consistent performance (>70% accuracy) on Spatial Understanding tasks spanning all 8 MVU-Eval video domains.

### Open Question 4
- Question: What training paradigms would improve instruction-following for multi-video QA without sacrificing reasoning capability?
- Basis in paper: [inferred] Appendix C.1 documents models ignoring prompts and generating free-form text; Section 4.1 notes this causes evaluation failures.
- Why unresolved: Video-LLMs optimized for description may lack fine-grained instruction tuning for multi-choice, multi-video formats.
- What evidence would resolve it: Near-zero format-violation rates coupled with improved accuracy on knowledge-intensive subtasks after targeted instruction tuning.

## Limitations

- Dataset design constraint: Fixed 32 frames per video may not capture optimal temporal resolution for all task types
- Quality control reliance: Multiple annotators and automated checks may not catch all forms of implicit information leakage
- Multiple-choice format: While enabling standardized evaluation, may not fully capture complexity of open-ended reasoning
- Zero-shot focus: Does not address how fine-tuning or instruction tuning might improve results

## Confidence

**High Confidence**: Performance degradation with increasing videos and cross-video alignment challenges are well-supported by empirical results across 27 models and multiple input formats.

**Medium Confidence**: Larger models generally perform better, but the relationship is not perfectly linear and depends heavily on other factors like input format and task type.

**Low Confidence**: The assertion that current architectures fundamentally cannot handle cross-video alignment due to attention or fusion limitations is speculative.

## Next Checks

1. **Temporal Resolution Sensitivity Analysis**: Systematically vary the number of frames per video (e.g., 8, 16, 32, 64) for each task type to identify whether the 32-frame standard is optimal.

2. **Cross-Video Attention Analysis**: Implement attention visualization and analysis for top-performing models to empirically verify whether attention mechanisms are actually attending across video boundaries.

3. **Open-Ended Response Validation**: Convert a subset of multiple-choice questions to open-ended format and evaluate whether models can generate correct answers without option constraints.