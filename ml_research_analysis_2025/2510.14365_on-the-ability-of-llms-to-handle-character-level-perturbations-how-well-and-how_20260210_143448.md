---
ver: rpa2
title: 'On the Ability of LLMs to Handle Character-Level Perturbations: How Well and
  How?'
arxiv_id: '2510.14365'
source_url: https://arxiv.org/abs/2510.14365
tags:
- problem
- characters
- noise
- llms
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well and how large language models
  (LLMs) handle frequent, structured character-level perturbations. The authors propose
  UCC-Inj, a method that inserts invisible Unicode control characters after each input
  character to disrupt LLM processing while preserving human readability.
---

# On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?

## Quick Facts
- **arXiv ID**: 2510.14365
- **Source URL**: https://arxiv.org/abs/2510.14365
- **Reference count**: 40
- **Primary result**: Despite severe tokenization disruption, large LLMs maintain substantial performance on reasoning tasks under character-level noise, with Qwen3-30B achieving 88.5% accuracy on GSM8K under 1-Inj (vs. 96.3% baseline).

## Executive Summary
This study investigates how well and through what mechanisms large language models (LLMs) handle frequent, structured character-level perturbations. The authors propose UCC-Inj, a method that inserts invisible Unicode control characters after each input character to disrupt LLM processing while preserving human readability. Tested on GSM8K (mathematical reasoning) and Luogu Official Contest (coding problems), the approach reveals surprising robustness: even under severe noise levels, models like DeepSeek-Reasoner and Qwen3-30B maintain notable performance. The study systematically evaluates factors including model family/size, problem length/language, and noise characteristics, finding that larger models are more robust, Chinese problems are easier to process than English ones, and visible noise causes greater degradation than invisible noise. Analysis reveals that successful denoising involves both high-level explicit problem rewriting in chain-of-thought and implicit low-level denoising mechanisms, with the latter likely involving attention patterns that focus on clean characters.

## Method Summary
The study introduces UCC-Inj, which injects invisible Unicode Variation Selectors (VS) after each character in the input text. The method tests models across perturbation levels (0-Inj to 3-Inj, representing number of VS characters inserted per input character) and noise set sizes (1 to 256 different VS characters). Experiments use GSM8K for mathematical reasoning and Luogu Official Contest for coding problems, evaluating multiple model families including DeepSeek, Qwen3/Qwen2.5, and Llama3/3.1 across various sizes. The evaluation measures accuracy degradation under different perturbation scenarios and analyzes the mechanisms through which models recover from character-level corruption.

## Key Results
- Under 1-Inj perturbation, Qwen3-30B achieves 88.5% accuracy on GSM8K (96.3% baseline), demonstrating substantial robustness to character-level noise
- Larger models (>14B parameters) show <10% accuracy degradation under character-wise tokenization, while smaller models fail catastrophically
- Chinese problems exhibit higher robustness to perturbation than English problems, with Qwen3-32B achieving 93% accuracy under character-wise tokenization
- Visible noise causes greater performance degradation than invisible noise, and randomized noise insertion patterns are more effective than fixed patterns
- Models employ two denoising strategies: explicit problem rewriting in chain-of-thought (higher success rate) and implicit attention-based filtering of noise characters

## Why This Works (Mechanism)

### Mechanism 1: Character-Wise Tokenization Robustness
Larger LLMs maintain comprehension when inputs are forced into single-character tokens, even though this pattern is uncommon during pretraining. The model's embeddings and attention layers can aggregate character-level information into semantically meaningful representations across layers, without requiring explicit reconstruction of normal token sequences.

### Mechanism 2: Explicit Problem Rewriting in CoT
Models that explicitly rewrite the clean problem during chain-of-thought show higher success rates on perturbed inputs. The model first outputs a denoised version of the problem in its response, then uses this cleaner representation to guide subsequent reasoning steps.

### Mechanism 3: Implicit Attention-Based Denoising
Some attention heads learn to predominantly attend to clean characters while filtering noise tokens. During forward passes, certain heads develop higher attention weights for informative characters versus inserted noise characters, enabling semantic recovery without explicit character-by-character processing.

## Foundational Learning

**Subword Tokenization (BPE)**
Why needed: UCC-Inj works by disrupting normal tokenization, forcing character-level splits. Understanding why "the" normally becomes one token but becomes [t, ?, h, ?, e, ?] under perturbation is essential.
Quick check: If a tokenizer's vocabulary contains "the" as a single token, what happens when Unicode Variation Selectors are inserted between each character?

**Unicode Variation Selectors**
Why needed: These are the invisible control characters used in UCC-Inj. They're non-printing but copied as text, making them practical for anti-cheating while preserving human readability.
Quick check: Why would invisible characters be more effective than visible ones (like asterisks) for preserving user experience, yet potentially less effective for disrupting LLM processing?

**Attention Patterns and Head Specialization**
Why needed: The paper's mechanistic analysis relies on interpreting which tokens attention heads focus on. Understanding that different heads may serve different functions is critical for the implicit denoising hypothesis.
Quick check: If some heads attend primarily to clean characters, what would you expect to see when visualizing attention weights for a noised input versus a clean input?

## Architecture Onboarding

**Component map**: Tokenizer -> Embedding Layer -> Attention Layers -> Generation/CoT -> Output

**Critical path**:
1. Perturbed input â†’ Tokenizer produces fragmented sequence
2. Embeddings represent each character and noise token
3. Attention layers (selectively) aggregate information across positions
4. If model adopts explicit rewriting: CoT outputs clean problem first
5. Reasoning proceeds on recovered (explicit or implicit) representation
6. Final answer generated

**Design tradeoffs**:
- Model size vs. robustness: Larger models (>14B) show <10% degradation under character-wise tokenization; smaller models fail catastrophically
- Noise type: Invisible noise (VS chars) is stealthier but easier for models to handle than visible noise (asterisks, letters)
- Noise complexity: Larger noise sets (256 vs. 1) and randomized insertion counts increase attack effectiveness
- Language: Chinese characters carry more information per character, making them more robust to perturbation than English

**Failure signatures**:
- Accuracy drops sharply with longer noised inputs (even with same underlying problem)
- Models may fail to attempt explicit rewriting (Llama: 5% vs. Qwen: 67%)
- 3+ noise chars per input char causes near-complete failure for most models
- Visible noise that merges with adjacent tokens (e.g., inserting 'A') can cause worse degradation than invisible noise

**First 3 experiments**:
1. Apply 1-Inj UCC-Inj to a small test set (e.g., 100 GSM8K problems) and measure accuracy drop for your target model
2. Compare accuracy with and without instructions to "remove meaningless characters first"
3. Test 1-Inj, 2-Inj, 3-Inj with different VS set sizes (1, 16, 256) to map the robustness boundary

## Open Questions the Paper Calls Out

**Open Question 1**: How does the robustness of LLMs to character-level noise manifest in tasks requiring long-context reasoning or multimodal processing?
Basis: The evaluation is limited to code and math tasks; other tasks may exhibit different robustness patterns.
Why unresolved: The study restricts its scope to specific high-level tasks, leaving generalization to other complex domains unverified.
Evidence needed: Empirical results from evaluating models under UCC-Inj on long-context benchmarks and multimodal datasets.

**Open Question 2**: Do alternative character-level perturbation strategies, such as character deletion or substitution, degrade LLM performance more effectively than the insertion methods evaluated?
Basis: The study focuses on noise insertion and doesn't cover the full spectrum of character-level corruptions.
Why unresolved: While insertion creates a specific signal-to-noise challenge, deletions or substitutions fundamentally alter semantic content, potentially bypassing observed denoising mechanisms.
Evidence needed: Comparative analysis of model accuracy on datasets perturbed by character deletion and substitution attacks versus UCC-Inj.

**Open Question 3**: What specific training dynamics or data distributions during pretraining enable the emergence of robustness to character-level perturbations?
Basis: The paper identifies the need to analyze how robustness emerges through pretraining or finetuning.
Why unresolved: The paper identifies the robustness phenomenon and hypothesizes about attention mechanisms but doesn't explain how these capabilities are acquired during training.
Evidence needed: Probing intermediate checkpoints of models during pretraining to track development of noise robustness, or training ablations on data with varying noise profiles.

## Limitations
- Evaluation limited to mathematical reasoning and coding problems, lacking validation on domains requiring character-level precision
- Mechanistic claims about implicit denoising are observational rather than experimentally validated with ablation studies
- Missing implementation details for critical experiments, particularly how character-wise tokenization was enforced
- Security claims lack real-world testing in actual exam scenarios with varied question types and formats

## Confidence

**High Confidence**:
- Larger models (>14B parameters) show greater robustness to character-level perturbations than smaller models
- Chinese problems are processed more robustly than English problems under perturbation
- Explicit instruction to remove noise characters improves accuracy for models capable of explicit rewriting

**Medium Confidence**:
- LLMs can maintain comprehension under character-wise tokenization (supported by GSM8K but not extensively validated across domains)
- Attention-based implicit denoising occurs in some models (observational evidence but lacks causal proof)
- 3+ noise characters per input character causes near-complete failure (supported by experiments but may vary with tokenizer behavior)

**Low Confidence**:
- UCC-Inj is "sufficiently strong" for real-world anti-cheating applications (limited to controlled test scenarios)
- The implicit denoising mechanism is the primary pathway for robustness (hypothesis without ablation or intervention studies)
- Performance patterns generalize to tasks requiring character-level precision (unsupported extrapolation from reasoning tasks)

## Next Checks

**Validation Check 1**: Test the same perturbation methods on datasets requiring character-level precision (chemical compound names, mathematical expressions with subscripts/superscripts, or programming problems with case-sensitive syntax) to validate whether observed robustness generalizes beyond mathematical reasoning.

**Validation Check 2**: Design an experiment that systematically ablates or modifies attention heads identified as attending primarily to clean characters. If these heads are causally responsible for denoising, their removal should cause disproportionate performance degradation on noised inputs compared to clean inputs.

**Validation Check 3**: Implement UCC-Inj in an actual exam setting with varied question types (multiple choice, short answer, code, mathematical proofs) and test with both API-accessible models and local deployments, measuring not just accuracy but also student experience, detection rates, and potential circumvention strategies.