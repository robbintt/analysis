---
ver: rpa2
title: 'Synth It Like KITTI: Synthetic Data Generation for Object Detection in Driving
  Scenarios'
arxiv_id: '2502.15076'
source_url: https://arxiv.org/abs/2502.15076
tags:
- data
- object
- carla
- kitti
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a synthetic data generation pipeline for 3D
  object detection on LiDAR point clouds using the CARLA simulator. The authors address
  the domain gap between synthetic and real-world data by implementing realistic sensor
  modeling, including intensity-based raydrop effects, dual-optical-center LiDAR configuration,
  and domain randomization strategies.
---

# Synth It Like KITTI: Synthetic Data Generation for Object Detection in Driving Scenarios

## Quick Facts
- arXiv ID: 2502.15076
- Source URL: https://arxiv.org/abs/2502.15076
- Reference count: 40
- This work presents a synthetic data generation pipeline for 3D object detection on LiDAR point clouds using the CARLA simulator.

## Executive Summary
This paper presents a synthetic data generation pipeline for 3D object detection on LiDAR point clouds using the CARLA simulator. The authors address the domain gap between synthetic and real-world data by implementing realistic sensor modeling, including intensity-based raydrop effects, dual-optical-center LiDAR configuration, and domain randomization strategies. They demonstrate strong generalization capabilities to the KITTI dataset with a Voxel-R-CNN detector, achieving 91.98% 3D AP on Easy, 81.39% on Moderate, and 78.76% on Hard categories when trained on synthetic data and tested on KITTI.

## Method Summary
The pipeline uses CARLA 0.9.15 to generate synthetic driving scenarios with realistic sensor modeling. Key modifications include: dual-optical-center LiDAR configuration mimicking Velodyne HDL-64E's two emitter blocks with different vertical FOVs; intensity-based raydrop filtering that removes points below calculated reflectivity thresholds; and bounding box shrinking to align with KITTI's "tight" annotation bias. The post-processing applies a linear shrinking formula to CARLA's ground truth boxes, applies Gaussian noise (µ=0, σ=0.1), and filters points using the intensity model B(x,y)=r1×G(x,y)+r2 with r1∈[0.25,0.5], r2∈[0.2,0.3]; I=B×(N·L)^n, n∈[0,8]. Voxel-R-CNN via OpenPCDet is used for evaluation.

## Key Results
- 91.98% 3D AP on Easy, 81.39% on Moderate, and 78.76% on Hard categories when trained on synthetic data and tested on KITTI
- Fine-tuning with 400 real samples nearly matches the baseline (91.51%/81.52%/79.0%)
- Using the full KITTI training set slightly surpasses synthetic-only performance (91.98%/82.61%/80.89%)
- Significantly outperforms previous synthetic-to-real transfer methods

## Why This Works (Mechanism)

### Mechanism 1: Labeling Bias Alignment via Bounding Box Shrinking
Real-world labels often bound only visible points (occlusion-aware), resulting in smaller boxes than true object volume. The authors apply a linear shrinking formula based on the difference between bounding box and point extents to mimic this "human labeling bias." This alignment is crucial for achieving high detection precision.

### Mechanism 2: Non-Uniform Sampling via Dual-Optical-Center Modeling
The real Velodyne HDL-64E uses two separate emitter blocks with different vertical FOVs, creating specific non-uniform point distribution (denser at range). Replicating this non-uniformity ensures the neural network receives correct point density patterns during training.

### Mechanism 3: Physics-Guided Raydrop Filtering
Real sensors fail to return signals for low-reflectivity surfaces or oblique angles. The authors model intensity using surface normals and color, then drop points falling below a threshold. This pattern of "missing points" is a stronger geometric cue for the network than absolute intensity values.

## Foundational Learning

- **Concept: Sim2Real Domain Gap**
  - Why needed here: The core problem is that models trained on "perfect" synthetic data fail on noisy real data
  - Quick check question: Why would a model trained on complete 3D meshes fail to detect a real car partially occluded by a glass window?

- **Concept: Voxel-based Object Detection (Voxel-R-CNN)**
  - Why needed here: The paper uses Voxel-R-CNN as the evaluator, which quantizes points into a 3D grid
  - Quick check question: How does converting a sparse point cloud to a dense voxel grid affect the model's sensitivity to small sensor position errors?

- **Concept: LiDAR Sensor Physics**
  - Why needed here: To understand modifications (dual centers, raydrop), one must understand LiDAR's specific angular resolution and reflectivity dependencies
  - Quick check question: Why does the intensity of a LiDAR return depend on the angle of incidence relative to the surface normal?

## Architecture Onboarding

- **Component map:** CARLA Simulator -> Custom LiDAR Client -> Post-Processor (Intensity Model, Raydrop Filter, Label Shrinker) -> Detector (Voxel-R-CNN)

- **Critical path:** The correct implementation of the Dual Sensor setup and the Bounding Box Shrinking logic

- **Design tradeoffs:**
  - Intensity Features vs. Raydrop: Using simulated intensity as input degrades real-world performance, while using it only to trigger raydrop improves it
  - Geometry vs. Labeling: High-fidelity geometry is useful, but correct label size (bias modeling) was found to be more critical for the KITTI benchmark

- **Failure signatures:**
  - High "Moderate/Hard" Error: Likely due to incorrect bounding box sizing or lack of sensor randomization
  - Low Performance on Distant Objects: Likely due to using a single uniform LiDAR vertical FOV instead of dual-sensor configuration

- **First 3 experiments:**
  1. Baseline Validation: Train Voxel-R-CNN on "First Hit" data (default CARLA) vs. "Strongest Hit" (with transparency)
  2. Label Ablation: Train on "Strongest*" (original boxes) vs. "Strongest" (shrunk boxes)
  3. Fine-Tuning Check: Take best synthetic model and fine-tune with 400 real KITTI samples

## Open Questions the Paper Calls Out

- How does the synthetic data pipeline affect point-based detection architectures compared to the voxel-based methods evaluated?
- How can LiDAR intensity simulation be improved to prevent the observed performance degradation when transferring to real data?
- Is it possible to create a single synthetic dataset that generalizes effectively to arbitrary real-world datasets without specific tuning for labeling biases?

## Limitations

- Relies on CARLA's UE4-based physics, which may not perfectly match real LiDAR sensor noise profiles
- Bounding box shrinking approach may not generalize to datasets with different annotation conventions
- Intensity model uses heuristic equations that approximate real-world reflectivity behavior but are not physically calibrated to specific sensor parameters

## Confidence

- **High confidence**: The Sim2Real performance improvements and fine-tuning results are well-validated against KITTI benchmarks
- **Medium confidence**: The specific sensor modeling parameters produce measurable gains, though their exact impact may vary with detector architecture
- **Medium confidence**: The labeling bias mechanism is empirically validated, but the generalizability to other datasets remains untested

## Next Checks

1. Validate bounding box shrinking on a dataset with known loose annotation conventions to test generalizability
2. Compare performance when training Voxel-R-CNN vs point-based detectors to isolate the impact of dual-optical-center modeling
3. Test whether the intensity-based raydrop mechanism transfers to different LiDAR models (e.g., mechanical vs solid-state sensors)