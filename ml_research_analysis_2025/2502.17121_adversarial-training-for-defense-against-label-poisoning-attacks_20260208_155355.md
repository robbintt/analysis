---
ver: rpa2
title: Adversarial Training for Defense Against Label Poisoning Attacks
arxiv_id: '2502.17121'
source_url: https://arxiv.org/abs/2502.17121
tags:
- floral
- accuracy
- adversarial
- dadv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLORAL introduces a novel adversarial training strategy using kernel
  SVMs to defend against label poisoning attacks. It formulates the problem as a non-zero-sum
  Stackelberg game between an attacker and a model, leveraging bilevel optimization
  to identify and poison the most influential training points.
---

# Adversarial Training for Defense Against Label Poisoning Attacks

## Quick Facts
- arXiv ID: 2502.17121
- Source URL: https://arxiv.org/abs/2502.17121
- Reference count: 40
- Key outcome: FLORAL achieves 20-30% higher robust accuracy than baselines against label poisoning attacks

## Executive Summary
FLORAL introduces a novel adversarial training strategy using kernel SVMs to defend against label poisoning attacks. It formulates the problem as a non-zero-sum Stackelberg game between an attacker and a model, leveraging bilevel optimization to identify and poison the most influential training points. The method employs projected gradient descent for adversarial training and scales via a fixed-point iteration projection scheme. Theoretical analysis proves local asymptotic stability, and empirical evaluations on Moon, IMDB, and MNIST datasets demonstrate that FLORAL consistently achieves higher robust accuracy than baselines, even under increasing attack budgets.

## Method Summary
FLORAL casts adversarial training as a non-zero-sum Stackelberg game where an attacker strategically poisons influential training labels while the model learns to recover. The attacker uses dual variables (λ) from kernel SVMs to identify high-influence support vectors, then randomly selects k points from the top-B candidates to flip labels. The model performs iterative PGD updates with a fixed-point iteration projection scheme to maintain constraints. This dynamic process not only defends against attacks but also sanitizes 25-35% of existing poisoned labels, achieving higher robust accuracy across synthetic and real datasets.

## Key Results
- FLORAL achieves 20-30% higher robust accuracy than baselines on Moon, IMDB, and MNIST datasets
- Empirical sanitization effect recovers 25-35% of initially poisoned labels during training
- Integration with RoBERTa and neural networks validates generalizability beyond kernel SVMs

## Why This Works (Mechanism)

### Mechanism 1: Non-Zero-Sum Stackelberg Game Formulation
A non-zero-sum Stackelberg game formulation more accurately captures the asymmetry between attacker and defender objectives than zero-sum minimax approaches. The attacker maximizes influence on the decision boundary (measured by dual variables λ), while the model minimizes classification loss under adversarial labels. This separation allows the model to recover from attacks that zero-sum formulations would not anticipate, as the attacker's true objective (maximizing influence) differs from simply maximizing training loss.

### Mechanism 2: Influence-Based Label Selection via Dual Variables
Dual variables (λ) from the SVM formulation provide a direct measure of training point influence on the decision boundary, enabling efficient identification of high-impact poisoning targets. Support vectors with high λ values have greater influence on the decision boundary. The attacker uses a randomized top-k rule—selecting k points from the top-B candidates—to inject randomness that avoids local optima while respecting budget constraints.

### Mechanism 3: Iterative Adversarial Training with Dynamic Label Sanitization
Iteratively training on evolving adversarial configurations both proactively reduces model sensitivity to label attacks and empirically sanitizes a portion of existing poisoned labels. Each round generates a new adversarial dataset via the randomized top-k rule, then performs PGD to update model parameters. This dynamic process encourages convergence to a smoother decision boundary. Empirically, FLORAL recovers 25-35% of initially poisoned labels, contributing to improved robust accuracy.

## Foundational Learning

### Concept 1: Bilevel Optimization
**Why needed here**: FLORAL's core formulation structures defense as nested optimization: an inner problem (attacker identifying influential points) nested within an outer problem (model learning robust parameters).
**Quick check question**: Explain why a standard minimax formulation (Eq. 3) fails for label poisoning, and how bilevel optimization addresses the "misaligned objectives" and "combinatorial explosion" problems.

### Concept 2: Kernel SVM Dual Formulation
**Why needed here**: The defense interprets dual variables (λ) as influence measures and computes gradients ∇_λ D(f_λ; D) = Q̃λ - 1, requiring understanding of the dual SVM formulation.
**Quick check question**: Given a kernel SVM with RBF kernel, what does a high λ_i value indicate about training point x_i's role in the decision boundary?

### Concept 3: Projected Gradient Descent with Equality and Box Constraints
**Why needed here**: FLORAL uses PGD to update λ while maintaining constraints (0 ≤ λ ≤ C, ỹ^T λ = 0), requiring understanding of projection operators and scalable implementations.
**Quick check question**: How would you project a vector z onto the feasible set {λ | 0 ≤ λ ≤ C, y^T λ = 0}? Why does Algorithm 2's fixed-point iteration provide a scalable approximation?

## Architecture Onboarding

### Component Map
1. **Attacker Module** -> Identifies top-B support vectors by λ values
2. **Adversarial Dataset Generator** -> Flips labels of selected k points
3. **Gradient Computer** -> Calculates ∇_λ D = Q̃λ - 1
4. **Projection Module** -> Enforces constraints via fixed-point iteration
5. **SVM Classifier** -> Kernel-based with RBF kernel, parametrized by λ and bias b

### Critical Path
1. Initialize λ^0 = 0, load D^0
2. Warm-up: 1 round with dummy attack to initialize λ values
3. For each round t = 1...T:
   - Identify top-B support vectors by λ^{t-1}
   - Randomly select k points → flip labels → D^t
   - Compute gradient ∇_λ D(f_λ^{t-1}; D^t)
   - PGD step: z^t = λ^{t-1} - η∇_λ D
   - Project: λ^t = PROX_{S(ỹ^t)}(z^t) via Algorithm 2
4. Return f_λ^T

### Design Tradeoffs
- **Attacker budget B during training vs. convergence speed**: Higher B improves robustness but may slow convergence
- **k (flips per round) vs. realism**: Small k (e.g., 1-5%) is more realistic; larger k accelerates training
- **Kernel parameters (C, γ)**: Larger C increases flexibility (risk of overfitting); γ controls RBF kernel width
- **Projection method**: Exact QP vs. fixed-point iteration (Algorithm 2 trades numerical precision for scalability)

### Failure Signatures
1. **Accuracy collapse on clean data**: May indicate over-regularization or inappropriate (C, γ)
2. **No improvement over vanilla SVM**: Check warm-up period sufficiency; B may be too low
3. **Divergent λ values**: Reduce learning rate η; verify constraint satisfaction in projection

### First 3 Experiments
1. **Reproduce Moon results (Table 1)**: Train FLORAL with C=10, γ=1, B=2k for k∈{5,10,25}, compare robust accuracy at 5%, 10%, 25% poisoning against vanilla SVM, NN, LN-SVM, Curie
2. **Ablation on attacker budget B**: Fix poisoning at 10%, vary B∈{5,10,25,50,125} to characterize sensitivity
3. **NN integration on MNIST-1vs7**: Extract last-layer embeddings from 2-layer MLP, apply FLORAL, compare robust accuracy against standard NN and NN-PGD

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can the FLORAL bilevel optimization framework be extended to allow for end-to-end robust fine-tuning of foundation models, where feature extractors are updated concurrently with the SVM classifier?
**Basis in paper**: [explicit] The Conclusion section lists "integrating our approach to robust fine-tuning of foundation models for supervised downstream tasks" as a primary direction for future research.
**Why unresolved**: The current implementation applies FLORAL to fixed embeddings (e.g., pre-extracted RoBERTa features) or utilizes the SVM as a static head, leaving the robustness of the backbone's representation learning unexplored.
**What evidence would resolve it**: A modification of the algorithm to compute gradients with respect to the neural network weights during the Stackelberg game, along with empirical results on NLP or vision transformers showing robust performance after full fine-tuning.

### Open Question 2
**Question**: How does the FLORAL defense mechanism mechanistically alter the distribution of the most influential training points (support vectors) compared to standard training?
**Basis in paper**: [explicit] The Conclusion states that a "detailed analysis of how FLORAL alters the most influential training points for model predictions... could provide interesting insights."
**Why unresolved**: While the paper observes that the overlap of influential points decreases in adversarial settings (Appendix E.2), it does not provide a theoretical or comprehensive empirical characterization of how the support vectors shift during the convergence to the Stackelberg equilibrium.
**What evidence would resolve it**: A theoretical analysis linking the support vector indices of the equilibrium solution $\hat{\lambda}$ to the clean data distribution, or a quantitative study tracking the trajectory of specific training instances' influence throughout the adversarial training rounds.

### Open Question 3
**Question**: How does FLORAL's robustness hold up against an adaptive attacker who optimizes label flips specifically to counter the Stackelberg equilibrium established by the defense?
**Basis in paper**: [inferred] The "Limitations" section notes that defense strategies may fail due to their "non-adaptive nature" and that the current method relies on a specific white-box attacker formulation.
**Why unresolved**: The current threat model assumes an attacker solving a fixed inner optimization problem; if the attacker is aware of the defense strategy, they might employ a strategy that breaks the stability assumptions or forces the model to diverge.
**What evidence would resolve it**: Empirical evaluations using an adaptive attacker who incorporates the defender's reaction into their optimization (e.g., a look-ahead attack) to test if the equilibrium remains stable or if the robust accuracy collapses.

## Limitations

- **Limited theoretical guarantees**: Analysis is limited to local asymptotic stability without addressing global convergence or optimality
- **Empirical focus**: Strongest claims rely on empirical evaluations across three datasets, with theoretical advantages over zero-sum alternatives remaining primarily theoretical
- **Assumption sensitivity**: Influence-based mechanism assumes dual variables reliably identify critical points, which may break down under severe non-separability or extreme label noise

## Confidence

- **High Confidence**: The iterative adversarial training mechanism (Mechanism 3) is well-supported by empirical evidence showing 25-35% label sanitization and consistent robust accuracy improvements across datasets.
- **Medium Confidence**: The non-zero-sum Stackelberg game formulation (Mechanism 1) provides theoretical justification, but the practical advantage over simpler formulations needs more direct comparison.
- **Medium Confidence**: The influence-based selection via dual variables (Mechanism 2) is plausible given SVM theory, but lacks direct empirical validation of the λ-to-influence relationship under attack.

## Next Checks

1. **Zero-sum vs Non-zero-sum Comparison**: Implement and compare FLORAL against a zero-sum minimax variant to empirically validate the claimed advantage of the Stackelberg formulation.
2. **Influence Function Validation**: Systematically measure the correlation between dual variables and actual decision boundary influence under various poisoning levels to verify Mechanism 2's core assumption.
3. **Convergence Analysis**: Conduct experiments varying attacker budget B and learning rate η to characterize convergence stability and identify regimes where the iterative process fails.