---
ver: rpa2
title: 'XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language
  Models on Extremist Content'
arxiv_id: '2506.00973'
source_url: https://arxiv.org/abs/2506.00973
tags:
- content
- extremist
- safety
- severity
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "XGUARD introduces a graded benchmark and evaluation framework\
  \ for assessing the severity of extremist content generated by LLMs. Unlike binary\
  \ safety evaluations, it uses 3,840 real-world prompts and a five-level danger scale\
  \ (0\u20134) to measure both frequency and severity of harmful outputs."
---

# XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content

## Quick Facts
- **arXiv ID:** 2506.00973
- **Source URL:** https://arxiv.org/abs/2506.00973
- **Reference count:** 28
- **Primary result:** Introduces a graded benchmark using 3,840 real-world prompts and a five-level danger scale to measure both frequency and severity of extremist content generated by LLMs

## Executive Summary
XGUARD introduces a novel graded benchmark and evaluation framework for assessing safety failures of large language models when exposed to extremist content. Unlike traditional binary safety evaluations, XGUARD employs a five-level danger scale (0-4) to measure both the frequency and severity of harmful outputs. The framework uses 3,840 real-world prompts and an Attack Severity Curve (ASC) to visualize vulnerabilities and compare defenses. Experiments on six LLMs and two lightweight defenses show that ICE significantly outperforms SFT at reducing attack success rate (ASR), with models like Gemma and DeepSeek demonstrating high vulnerability while Llama3 shows strong resistance.

## Method Summary
XGUARD uses a comprehensive evaluation framework that moves beyond binary safety assessments by introducing a graded danger scale for measuring extremist content generation. The system employs 3,840 real-world prompts sourced from Reddit and evaluates six different LLMs using both supervised fine-tuning (SFT) and input-level conditional editing (ICE) defenses. The framework generates Attack Severity Curves (ASC) to visualize how different models and defenses perform across varying threat levels. The benchmark measures both frequency and severity of harmful outputs, providing a more nuanced understanding of LLM safety vulnerabilities than traditional pass/fail metrics.

## Key Results
- ICE defense reduces ASR more effectively than SFT across all tested models
- Gemma and DeepSeek show highest vulnerability to extremist content generation
- Llama3 demonstrates strongest resistance to safety failures
- Attack Severity Curves reveal varying vulnerabilities across different threat levels

## Why This Works (Mechanism)
XGUARD works by introducing a graded evaluation framework that captures the nuanced nature of safety failures rather than treating them as binary outcomes. The five-level danger scale allows for more granular measurement of harmful content severity, while the Attack Severity Curve provides visual representation of vulnerabilities across different threat levels. The use of real-world prompts from Reddit ensures ecological validity, and the comparison between SFT and ICE defenses demonstrates the effectiveness of input-level interventions over traditional fine-tuning approaches.

## Foundational Learning
- **Graded Danger Scale (0-4):** Categorizes severity of extremist content from benign to highly dangerous; needed to capture nuanced safety failures beyond binary pass/fail metrics; quick check: verify inter-rater reliability of severity scoring
- **Attack Severity Curve (ASC):** Visualizes model vulnerability across different threat levels; needed to compare models and defenses comprehensively; quick check: ensure consistent scaling across all model evaluations
- **Input-Level Conditional Editing (ICE):** Defense mechanism that modifies inputs before model processing; needed to reduce ASR more effectively than traditional SFT; quick check: measure reduction in ASR before/after ICE application
- **Real-World Prompt Collection:** 3,840 prompts sourced from Reddit; needed to ensure ecological validity of safety evaluation; quick check: verify prompt diversity and representativeness

## Architecture Onboarding
- **Component Map:** Reddit prompts -> LLM model -> Defense mechanism (SFT/ICE) -> Severity scoring -> Attack Severity Curve
- **Critical Path:** Prompt generation → Model response → Defense application → Human/automated severity scoring → ASC visualization
- **Design Tradeoffs:** Real-world prompts provide ecological validity but may introduce noise; graded scale captures nuance but requires subjective scoring; lightweight defenses are efficient but may not address all vulnerabilities
- **Failure Signatures:** High ASR with low severity scores indicates models generate harmful content but with limited impact; high ASR with high severity indicates dangerous vulnerabilities requiring immediate attention
- **First Experiments:** 1) Compare ASC performance across all six LLMs without defenses, 2) Evaluate ICE vs SFT effectiveness on highest-vulnerability models, 3) Test transferability of benchmark to different types of harmful content

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Human rater subjectivity in severity scoring without reported inter-rater reliability metrics
- Five-level danger scale may face challenges in consistent application across cultural contexts
- Results focused specifically on extremist content may not generalize to other harmful content types

## Confidence
- **ICE effectiveness claim:** High confidence - experimental methodology appears sound with clear results
- **Generalizability to other domains:** Medium confidence - results specific to extremist content may not transfer to other harmful content types
- **Benchmark framework robustness:** Medium confidence - while innovative, relies on human scoring which introduces potential subjectivity

## Next Checks
1. Conduct inter-rater reliability analysis (e.g., Krippendorff's alpha) to quantify consistency in severity scoring across human annotators
2. Test the benchmark framework on a broader range of harmful content types (e.g., self-harm, misinformation) to assess domain transferability
3. Implement and evaluate additional defense mechanisms beyond SFT and ICE to compare effectiveness across different intervention approaches