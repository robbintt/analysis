---
ver: rpa2
title: 'Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical
  LLMs'
arxiv_id: '2510.12255'
source_url: https://arxiv.org/abs/2510.12255
tags:
- context
- answer
- medical
- system
- interventions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedQA-Followup, a framework for evaluating
  multi-turn robustness in medical large language models (LLMs). While existing benchmarks
  focus on single-turn performance under ideal conditions, this work systematically
  examines how models behave when their initial answers are challenged across conversational
  turns.
---

# Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs

## Quick Facts
- **arXiv ID:** 2510.12255
- **Source URL:** https://arxiv.org/abs/2510.12255
- **Reference count:** 40
- **Key outcome:** Multi-turn robustness framework reveals medical LLMs drop from 91.2% to 13.5% accuracy when answers are challenged

## Executive Summary
This paper introduces MedQA-Followup, a framework for evaluating multi-turn robustness in medical large language models. While existing benchmarks focus on single-turn performance under ideal conditions, this work systematically examines how models behave when their initial answers are challenged across conversational turns. The authors introduce a taxonomy distinguishing shallow robustness (resistance to misleading initial context) from deep robustness (maintaining accuracy when answers are challenged), and an indirect-direct axis separating subtle contextual framing from explicit suggestions. Using controlled interventions on the MedQA dataset, the study evaluates five state-of-the-art LLMs and reveals dramatic vulnerabilities in multi-turn settings, with accuracy drops from 91.2% to as low as 13.5% for Claude Sonnet 4 when subjected to context-based interventions.

## Method Summary
The study evaluates multi-turn robustness using the MedQA-Followup framework on 1,273 USMLE-style medical questions. Five state-of-the-art LLMs are tested: GPT-4.1, GPT-4.1 mini, Claude Sonnet 4, MedGemma 27B, and MedGemma 4B. The evaluation distinguishes shallow robustness (single-turn performance with misleading context) from deep robustness (multi-turn performance after initial answer commitment). Four intervention categories are applied: rethink (control), inc_letter (direct suggestions), context (indirect manipulations), and compounding (sequential interventions). Context strings are generated using GPT-4.1 with 4-10 sentence templates from Appendix F. Accuracy is calculated with deterministic decoding (temperature=0) and output format "Final Answer: (X)".

## Key Results
- Accuracy drops from 91.2% to as low as 13.5% for Claude Sonnet 4 under context-based interventions
- Indirect context manipulations cause larger accuracy degradation than explicit wrong-answer suggestions across models
- 85% of compounding intervention combinations exhibit sub-additive effects, preventing worst-case amplification
- Step 2&3 (clinical) domains show higher vulnerability to context-based attacks than Step 1 (basic science) domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Indirect context manipulations cause larger accuracy degradation than explicit wrong-answer suggestions in multi-turn medical Q&A.
- **Mechanism:** Context-based interventions (RAG-style, misleading clinical framing) integrate with the model's reasoning chain as *plausible evidence*, bypassing the "disagreement detection" that explicit suggestions trigger. Models are trained to be helpful and context-aware; when context appears authoritative and medically coherent, it overrides prior reasoning without triggering rejection responses that overt bias attempts activate.
- **Core assumption:** Models have stronger learned defenses against explicit persuasion attempts than against context that appears to be additional clinical information.
- **Evidence anchors:**
  - [abstract] "Counterintuitively, indirect, context-based interventions are often more harmful than direct suggestions, yielding larger accuracy drops across models"
  - [section] Table 2 shows RAG-style context causing −85.2% relative change for Claude vs. −24.3% for social proof prior; GPT-4.1 shows −48.3% vs. −0.5%
  - [corpus] JMedEthicBench and MOCHA similarly find multi-turn attacks more effective than single-turn, supporting conversational drift as a general vulnerability class
- **Break condition:** If models are explicitly trained/finetuned to treat all new conversational context with skepticism proportional to answer confidence, indirect manipulation advantage diminishes.

### Mechanism 2
- **Claim:** Multi-turn robustness degrades dramatically while single-turn (shallow) robustness remains strong in current medical LLMs.
- **Mechanism:** In single-turn settings, misleading context is evaluated *before* the model commits. The model can weigh all information simultaneously. In multi-turn settings, the model has already produced reasoning and a final answer; follow-up context forces *re-evaluation* after cognitive commitment. Models exhibit "reasoning inertia"—once a path is justified, additional context is assimilated into the existing frame rather than triggering independent re-analysis.
- **Core assumption:** Sequential context presentation after answer commitment exploits different cognitive/mechanistic pathways than simultaneous context presentation.
- **Evidence anchors:**
  - [abstract] "While models perform reasonably well under shallow perturbations, they exhibit severe vulnerabilities in multi-turn settings, with accuracy dropping from 91.2% to as low as 13.5%"
  - [section] Figure 2 and Table 2: single-turn inc_letter avg change −1.0% to −17.3% vs. multi-turn context avg change −32.8% to −50.0%
  - [corpus] Pattern Enhanced Multi-Turn Jailbreaking and RedTWIZ confirm multi-turn adversarial success across domains, suggesting sequential commitment-then-challenge is a general attack surface
- **Break condition:** If follow-up turns are processed with independent reasoning chains (no access to prior reasoning), multi-turn vulnerability approaches single-turn levels.

### Mechanism 3
- **Claim:** Compounding interventions show predominantly sub-additive effects, limiting worst-case exploitation.
- **Mechanism:** Models saturate in their malleability—once an answer has shifted once, subsequent interventions compete for the same "influence budget" rather than stacking linearly. Some interventions may even trigger partial recovery as the model detects inconsistency in the accumulated manipulation signals.
- **Core assumption:** Models have implicit limits on answer-changing behavior per conversation; manipulation signals compete rather than compound.
- **Evidence anchors:**
  - [abstract] "85% of combinations exhibit sub-additive effects (85% of combinations), preventing worst-case amplification"
  - [section] Figure 5 shows most compounding combinations below the additive-expected line; MedGemma 4B shows recovery patterns
  - [corpus] Weak direct evidence in corpus for compounding sub-additivity specifically—this mechanism is less explored in prior work
- **Break condition:** If interventions are designed to target *different* reasoning stages (e.g., one challenges evidence, another challenges interpretation, a third challenges confidence), additivity may increase.

## Foundational Learning

- **Concept:** Shallow vs. Deep Robustness distinction
  - **Why needed here:** The paper's core taxonomy; without this, you cannot interpret why single-turn benchmark performance is insufficient for deployment confidence.
  - **Quick check question:** If a model scores 95% on MedQA baseline and 93% with single-turn distractors, what does this tell you about its multi-turn clinical reliability? (Answer: Very little—deep robustness is a separate axis.)

- **Concept:** Indirect vs. Direct intervention mechanisms
  - **Why needed here:** Explains why "helpful" context injection is more dangerous than explicit persuasion; critical for understanding RAG system vulnerabilities.
  - **Quick check question:** Why might framing the same wrong-answer signal as "retrieved evidence" work better than "a colleague thinks"? (Answer: Models trained for RAG-assistance may weight retrieved context more heavily than opinion framing.)

- **Concept:** Conversational commitment and reasoning inertia
  - **Why needed here:** Underlies why multi-turn is fundamentally different from single-turn with equivalent total information.
  - **Quick check question:** In Q→A→I(Q)→A', what psychological/mechanistic property of the model changes between A and A' that doesn't exist in I(Q)→A'? (Answer: Post-commitment re-evaluation vs. pre-commitment synthesis.)

## Architecture Onboarding

- **Component map:** MedQA-Followup framework -> 4 intervention categories -> Context generation pipeline -> Multi-turn conversation manager -> Accuracy extraction
- **Critical path:**
  1. Establish baseline on MedQA (1,273 questions, 15 USMLE domains)
  2. Apply single-turn interventions from BiasMedQA and KGGD for comparison
  3. Apply follow-up interventions after model commits to initial answer
  4. For compounding: layer additional interventions sequentially
  5. Calculate relative accuracy change: (intervention_accuracy - baseline) / baseline
- **Design tradeoffs:**
  - Context generation via GPT-4.1 introduces potential contamination; manual generation would be cleaner but unscalable
  - Temperature=0 ensures reproducibility but may underestimate real-world variability
  - Multiple-choice format enables automated evaluation but may not generalize to open-ended clinical dialogue
- **Failure signatures:**
  - Extreme answer flipping on RAG-style context (>80% relative drop) indicates high susceptibility to authoritative-sounding context
  - Recovery on compounding interventions (esp. MedGemma 4B pattern) suggests intervention competition rather than linear stacking
  - Step 2&3 (clinical) > Step 1 (basic science) vulnerability indicates context-based attacks are more effective on applied reasoning
- **First 3 experiments:**
  1. Replicate the context-length ablation (Figure 3) on your target model to calibrate vulnerability before deployment
  2. Test your specific RAG retrieval pipeline framing against the paper's RAG-style context template to measure real-system vulnerability
  3. Run compounding interventions on your highest-risk clinical domain (per Appendix D.2 heatmaps, likely Social Sciences/Ethics if applicable) to establish worst-case bounds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adversarial training or confidence-weighted resistance mechanisms mitigate deep robustness vulnerabilities in medical LLMs without compromising baseline accuracy?
- **Basis in paper:** [explicit] The authors explicitly call for future work to "explore adversarial training on multi-turn dialogues, confidence-weighted resistance... and safeguards."
- **Why unresolved:** The study evaluates existing models to expose vulnerabilities but does not test proposed defensive strategies.
- **What evidence would resolve it:** Benchmarking models fine-tuned with multi-turn adversarial examples on MedQA-Followup to measure improvements in deep robustness relative to baseline performance.

### Open Question 2
- **Question:** Do the deep robustness failures observed in multiple-choice formats extend to open-ended clinical dialogue?
- **Basis in paper:** [inferred] The Ethics Statement notes the multiple-choice framework "may not fully capture the complexities of open-ended clinical dialogue."
- **Why unresolved:** MedQA-Followup relies on discrete option selection; it is unclear if context manipulation similarly degrades performance in unconstrained text generation.
- **What evidence would resolve it:** Applying multi-turn interventions to open-ended benchmarks to assess if indirect context causes similar rates of diagnostic error.

### Open Question 3
- **Question:** Why do indirect context manipulations yield larger accuracy drops than direct suggestions in current medical LLMs?
- **Basis in paper:** [explicit] The authors highlight the "counterintuitive" finding that "indirect, context-based interventions are often more harmful than direct suggestions."
- **Why unresolved:** The paper quantifies this effect but does not explain the mechanisms allowing implicit framing to override reasoning more effectively than explicit social pressure.
- **What evidence would resolve it:** Mechanistic analysis of attention patterns to identify how "RAG style context" alters reasoning pathways compared to "Authority prior" prompts.

## Limitations
- Reliance on GPT-4.1 for context generation introduces potential contamination between evaluator and target models
- Multiple-choice format may not generalize to open-ended clinical dialogue where answer space is less constrained
- The 85% sub-additive compounding effect may be an artifact of specific intervention types tested rather than a fundamental model property

## Confidence

- **High Confidence:** The shallow vs. deep robustness distinction and the relative performance ordering between models under different intervention types
- **Medium Confidence:** The mechanism explaining why indirect context manipulations are more effective than direct suggestions
- **Low Confidence:** The generalizability of the 85% sub-additive compounding effect to other domains or more sophisticated intervention designs

## Next Checks

1. **Cross-Model Consistency Test:** Run the same evaluation pipeline on an additional medical LLM not included in the original study (e.g., Gemini or LLaMA-based medical model) to verify whether the shallow/deep robustness pattern holds across model architectures.

2. **Context Generation Independence Test:** Replace the GPT-4.1 context generator with either manual human-generated contexts or a different model family entirely to determine whether the observed vulnerabilities are specific to the generation method or represent fundamental model weaknesses.

3. **Open-Ended Extension Test:** Adapt the MedQA-Followup framework to a small set of open-ended medical questions and evaluate whether the same patterns of indirect manipulation superiority and multi-turn vulnerability persist when answers are not constrained to multiple-choice options.