---
ver: rpa2
title: 'LiMuon: Light and Fast Muon Optimizer for Large Models'
arxiv_id: '2509.14562'
source_url: https://arxiv.org/abs/2509.14562
tags:
- have
- limuon
- algorithm
- muon
- optimizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LiMuon, a memory-efficient variant of the Muon
  optimizer designed for training large-scale models. LiMuon incorporates momentum-based
  variance reduction and randomized Singular Value Decomposition (SVD) to reduce memory
  usage while maintaining competitive performance.
---

# LiMuon: Light and Fast Muon Optimizer for Large Models

## Quick Facts
- arXiv ID: 2509.14562
- Source URL: https://arxiv.org/abs/2509.14562
- Reference count: 40
- Key outcome: LiMuon achieves O(ϵ⁻³) sample complexity with memory reduction via low-rank momentum compression, outperforming AdamW, Lion, Muon, Muon++, and SUMO on DistilGPT2 and ViT models.

## Executive Summary
LiMuon is a memory-efficient variant of the Muon optimizer designed for training large-scale models. It combines momentum-based variance reduction (STORM) with randomized Singular Value Decomposition (SVD) to reduce memory usage while maintaining competitive performance. The algorithm achieves a sample complexity of O(ϵ⁻³) for finding an ϵ-stationary point under both Lipschitz smooth and generalized smooth conditions, improving upon the O(ϵ⁻⁴) complexity of existing Muon variants. Empirical evaluations demonstrate that LiMuon achieves better perplexity and accuracy scores compared to baseline optimizers while using less memory.

## Method Summary
LiMuon implements a momentum-based variance reduced optimizer with orthogonalized gradient updates. The method uses STORM-style momentum correction: M_{t+1} = ∇f(W_{t+1}; ξ_{t+1}) + (1-β_{t+1})(M_t - ∇f(W_t; ξ_{t+1})), which reduces sample complexity from O(ϵ⁻⁴) to O(ϵ⁻³). For memory efficiency, it replaces full momentum storage with low-rank factors via Randomized SVD, reducing memory from O(mn) to O((m+n)ĥr). The algorithm works in two modes: Option #1 uses full momentum with standard SVD, while Option #2 uses memory-efficient low-rank approximation. Weight updates follow W^{t+1} = W^t - η_t U_t V_t^T from the SVD decomposition of the momentum matrix.

## Key Results
- Sample complexity improves from O(ϵ⁻⁴) to O(ϵ⁻³) for finding ϵ-stationary points
- DistilGPT2 perplexity: LiMuon (Option #1) achieves 142.85 vs Muon++ at 150.32
- Memory usage reduced by 5-10% compared to full Muon variants
- ViT model accuracy improves from 65.38% (AdamW) to 66.56% (LiMuon)

## Why This Works (Mechanism)

### Mechanism 1: Variance Reduction via Momentum Correction (STORM)
The STORM estimator reduces sample complexity by maintaining a lower-variance estimate of the true gradient using single samples. Instead of standard momentum, it uses a recursive bias-correction term that theoretically speeds up convergence relative to sample count compared to methods requiring large batches.

### Mechanism 2: Low-Rank Momentum Compression (Randomized SVD)
Replacing full-momentum storage with low-rank factors via Randomized SVD reduces memory from O(mn) to O((m+n)ĥr) while preserving convergence. The algorithm projects the momentum matrix onto a random subspace, computes a small SVD, and stores the truncated factors, assuming the momentum signal is effectively low-rank.

### Mechanism 3: Convergence under Generalized Smoothness
LiMuon guarantees convergence to an ϵ-stationary point even when the loss landscape violates strict Lipschitz smoothness. The analysis extends to (L_0, L_1)-smoothness, where the gradient Lipschitz constant scales with the gradient norm, preventing exploding gradients through spectral constraints.

## Foundational Learning

- **Concept: Stochastic First-Order Oracle (SFO) Complexity**
  - Why needed here: LiMuon's primary theoretical contribution is proving a tighter SFO complexity (O(ϵ⁻³)) than standard Muon (O(ϵ⁻⁴). Understanding this distinguishes "faster convergence" (time) from "sample efficiency" (data steps).
  - Quick check question: Why does reducing the dependency from ϵ⁻⁴ to ϵ⁻³ theoretically allow for faster training on large datasets without increasing batch size?

- **Concept: Nuclear Norm Stationarity**
  - Why needed here: The paper optimizes for the nuclear norm of the gradient (∥∇f(W)∥_*) rather than just the Frobenius norm, which is the natural metric for matrix-structured parameters in Muon.
  - Quick check question: How does the nuclear norm relate to the singular values of a matrix, and why is it a stricter measure of stationarity for matrix optimization than the Frobenius norm?

- **Concept: Randomized Linear Algebra (RSVD)**
  - Why needed here: The memory efficiency of Option #2 relies entirely on the properties of RSVD. You must understand the trade-off between the target rank ĥr and the oversampling parameter s to implement it effectively.
  - Quick check question: In RSVD, what is the role of the oversampling parameter s, and what happens to the approximation error if s is set too low?

## Architecture Onboarding

- **Component map:** Input weights W → Gradient calculation → STORM momentum update → RSVD projection (Option #2) or Standard SVD (Option #1) → Orthogonalization → Weight update W_{t+1} = W_t - η U V^T → Store low-rank factors (Option #2) or full momentum (Option #1)

- **Critical path:** The RSVD projection (Line 8, Algorithm 2). If the random projection matrix Ω generation or the QR decomposition of Y is inefficient, it bottlenecks the iteration speed. The rank parameter ĥr directly controls memory vs. error.

- **Design tradeoffs:**
  - Option #1 vs. Option #2: Option #1 offers slightly better perplexity (142.85 vs 249.79 on DistilGPT2) but uses higher memory (~6.30GB vs ~6.05GB). Option #2 is strictly for memory-constrained environments.
  - Rank ĥr selection: A small ĥr saves memory but increases the approximation error γ. If γ violates the condition γ ≤ β/(16√r(1-β)), convergence is not guaranteed.

- **Failure signatures:**
  - Memory OOM: Likely RSVD rank ĥr is too high, or Option #1 is used on very large layers.
  - Divergence: If β (momentum) is too high relative to the variance σ, or if the generalized smoothness condition is violated by an aggressive learning rate η.
  - Slow Convergence: If RSVD oversampling s is too small, leading to poor orthogonalization of the update direction.

- **First 3 experiments:**
  1. **Baseline Comparison:** Train DistilGPT2 on WikiText-103 using LiMuon (Option #2, ĥr=10) vs. AdamW. Plot validation perplexity against peak allocated GPU memory to verify the trade-off.
  2. **Rank Ablation:** Run LiMuon on a ViT model with varying ĥr ∈ {4, 8, 16, 32}. Identify the "knee" where increasing rank yields diminishing returns in accuracy.
  3. **Convergence Rate Verification:** Plot log(∥∇f(W_t)∥_*) vs. iterations. Check if the slope aligns with the theoretical O(T^{-1/3}) decay rate compared to standard Muon.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on generalized smooth condition and bounded variance assumptions that may not hold exactly for modern LLMs with highly non-convex objectives
- Memory-efficiency claims are empirically supported but the theoretical rank bound (Assumption 5) lacks explicit verification on real momentum matrices
- Claims about outperforming Lion on accuracy need more ablation on regularization strength

## Confidence
- **High**: STORM variance-reduction mechanism and its O(ϵ⁻³) sample complexity guarantee
- **Medium**: Randomized SVD compression effectiveness, dependent on uncontrolled spectral decay properties  
- **Low**: Claims about outperforming Lion on accuracy without sufficient ablation studies

## Next Checks
1. **Spectral Decay Test**: Measure the singular value spectrum of momentum matrices across training epochs for DistilGPT2 to verify Assumption 5 holds empirically.
2. **Rank Sensitivity Analysis**: Systematically sweep ĥr from 4 to 64 on a small ViT variant, plotting both memory usage and validation accuracy to identify the optimal trade-off curve.
3. **Convergence Rate Verification**: Plot log(∥∇f(W_t)∥_*) vs. iterations on a convex toy problem with known L_0, L_1 to confirm the theoretical O(T^{-1/3}) slope.