---
ver: rpa2
title: 'RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports'
arxiv_id: '2503.09358'
source_url: https://arxiv.org/abs/2503.09358
tags:
- clinical
- reports
- data
- standardization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RetSTA addresses the challenge of standardizing clinical fundus
  image reports by constructing a bilingual terminology and fine-tuning two large
  language models, RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, built with data augmentation,
  demonstrates strong performance but is limited in disease coverage.
---

# RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports

## Quick Facts
- arXiv ID: 2503.09358
- Source URL: https://arxiv.org/abs/2503.09358
- Reference count: 30
- Primary result: RetSTA-7B achieves BLEU-1 of 92.69, BLEU-4 of 89.66, METEOR of 86.04, and ROUGE-L of 90.08 on English report standardization

## Executive Summary
RetSTA addresses the challenge of standardizing clinical fundus image reports by constructing a bilingual terminology and fine-tuning two large language models, RetSTA-7B-Zero and RetSTA-7B. The approach leverages domain-specific terminology grounding, controlled data augmentation, and iterative self-distillation to achieve state-of-the-art performance on report-level standardization tasks. The system demonstrates strong bilingual generalization across Chinese and English clinical reports while maintaining high precision in mapping non-standard expressions to standardized forms.

## Method Summary
RetSTA constructs a bilingual standard terminology from ICD-11, SNOMED-CT, PPP guidelines, and 451,956 real clinical reports, comprising 362 standardized terms. The method employs a two-stage fine-tuning approach: RetSTA-7B-Zero is trained on 10,000 augmented samples from 2,000 human-annotated pairs using syntactic noise injection, semantic perturbation, and synonym replacement. RetSTA-7B further improves performance by integrating 60,000 bilingual pairs generated through self-distillation, filtering, and translation with DeepSeek-V3. Both models use Qwen2.5-7B-Instruct with LoRA fine-tuning on RTX 4090 GPUs.

## Key Results
- RetSTA-7B-Zero achieves BLEU-1 of 87.11, BLEU-4 of 83.96, METEOR of 84.71, and ROUGE-L of 86.51 on augmented data
- RetSTA-7B outperforms 72B models (Qwen2.5-72B, GLM-4-Plus) on English standardization with BLEU-1 of 92.69, BLEU-4 of 89.66, METEOR of 86.04, ROUGE-L of 90.08
- On Chinese reports, RetSTA-7B reaches BLEU-1 of 93.35, BLEU-4 of 89.83, METEOR of 94.19, and ROUGE-L of 94.30

## Why This Works (Mechanism)

### Mechanism 1: Domain-specific terminology grounding
- Claim: Domain-specific terminology grounding enables accurate standardization by constraining the output space to clinically valid expressions.
- Core assumption: The 362-term terminology sufficiently represents the distribution of clinical expressions encountered in practice.
- Evidence: Terminology constructed from ICD-11, SNOMED-CT, PPP guidelines, and 451,956 clinical reports; weak direct corpus evidence.

### Mechanism 2: Controlled data augmentation
- Claim: Controlled data augmentation simulating clinical noise patterns enables effective learning from limited labeled data.
- Core assumption: Synthetic noise distributions approximate real clinical report imperfections.
- Evidence: Augmentation expands 2,000 samples to 10,000; augmented samples achieve BLEU-1 of 87.11 vs. 87.49 for non-augmented.

### Mechanism 3: Iterative self-distillation
- Claim: Iterative self-distillation with cross-lingual data expansion improves coverage of long-tail clinical scenarios without additional human labeling.
- Core assumption: Filtering criteria successfully remove incorrect standardizations.
- Evidence: RetSTA-7B trained on 60,037 bilingual pairs from RetSTA-7B-Zero generation and translation; no direct corpus evidence on self-distillation pipelines.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed: Enables fine-tuning on consumer GPUs while preserving base model capabilities. Quick check: Can you explain why LoRA modifies only low-rank decomposition matrices rather than the full weight matrix, and how this affects catastrophic forgetting?

- **BLEU/ROUGE/METEOR metrics**: Why needed: These metrics measure n-gram similarity but may miss semantic errors. Quick check: Why might high BLEU scores fail to capture semantic errors in medical text standardization (e.g., swapping "left" for "right" eye)?

- **Bilingual transfer and cross-lingual consistency**: Why needed: The paper demonstrates that incorporating English data does not degrade Chinese performance. Quick check: What mechanisms allow a single model to maintain independent performance across languages without interference?

## Architecture Onboarding

- **Component map**: Terminology Module (362-term vocabulary) → Data Augmentation Pipeline (syntactic noise → semantic perturbation → synonym replacement) → RetSTA-7B-Zero (Qwen2.5-7B-Instruct + LoRA) → Self-Distillation Filter (rule-based conflict detection + similarity deduplication) → Translation Module (DeepSeek-V3) → RetSTA-7B (Qwen2.5-7B-Instruct + LoRA)

- **Critical path**: 1) Manual terminology construction → 2) Initial 2,000 human-annotated pairs → 3) Augmentation to 10,000 → RetSTA-7B-Zero training → 4) Batch inference on 60,000 reports → filtering → translation → 5) RetSTA-7B training on 60,037 bilingual pairs

- **Design tradeoffs**: 7B vs. larger models shows domain-specific fine-tuning matters more than scale; self-distillation trades label quality for data volume; bilingual joint training shows no performance degradation but also no improvement.

- **Failure signatures**: Language leakage from base model, over-standardization of rare conditions, translation artifacts from DeepSeek-V3.

- **First 3 experiments**: 1) Terminology ablation: train without 362-term vocabulary → 2) Augmentation pattern analysis: evaluate on noise-type categorized reports → 3) Error propagation audit: manual review of filtered RetSTA-7B-Zero outputs.

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does integrating RetSTA-7B improve downstream text-image foundation models and multimodal LLMs? [explicit] The paper plans to leverage RetSTA-7B for multimodal training but hasn't tested this application.

- **Open Question 2**: Can the RetSTA standardization paradigm be successfully transferred to medical domains other than ophthalmology? [explicit] The authors intend to extend beyond ophthalmology but haven't demonstrated cross-domain applicability.

- **Open Question 3**: Do the high NLG metric scores translate to improved clinical validity and utility for medical professionals? [inferred] The paper relies on automated metrics without clinical expert validation of standardized outputs.

## Limitations

- Terminology coverage is limited to 362 terms, constraining ability to handle long-tail clinical scenarios and rare disease variants.
- Self-distillation approach may propagate systematic errors that pass filtering criteria, amplifying biases from RetSTA-7B-Zero.
- Evaluation metrics measure surface similarity rather than clinical correctness, potentially missing critical semantic errors like eye laterality swaps.

## Confidence

- **High Confidence**: Core performance claims well-supported by experimental results across multiple model variants and baselines.
- **Medium Confidence**: Bilingual transfer claims rely on positive joint training results, but cross-lingual benefits appear limited.
- **Low Confidence**: Generalization to real-world deployment untested; lacks external dataset evaluation and clinical expert validation.

## Next Checks

1. **Terminology Coverage Expansion Test**: Analyze 1,000 clinical reports from diverse hospital systems to identify distribution of terms not covered by current 362-term vocabulary and measure frequency of standardization failures.

2. **Clinical Expert Validation Study**: Engage 3-5 ophthalmologists to review 200 standardized reports, measuring inter-rater agreement and identifying systematic error patterns missed by automated metrics.

3. **Cross-Institutional Generalization Benchmark**: Evaluate RetSTA-7B on 500 reports from hospital systems not represented in training data to quantify real-world robustness and performance degradation.