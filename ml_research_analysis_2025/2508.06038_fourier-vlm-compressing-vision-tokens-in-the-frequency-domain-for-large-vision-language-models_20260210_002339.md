---
ver: rpa2
title: 'Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language
  Models'
arxiv_id: '2508.06038'
source_url: https://arxiv.org/abs/2508.06038
tags:
- vision
- tokens
- visual
- image
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational overhead caused by the large
  number of vision tokens in Vision-Language Models (VLMs). The authors propose Fourier-VLM,
  a method that compresses visual representations in the frequency domain using a
  low-pass filter applied via two-dimensional Discrete Cosine Transform (DCT).
---

# Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2508.06038
- Source URL: https://arxiv.org/abs/2508.06038
- Reference count: 40
- This paper proposes a frequency-domain compression method that achieves up to 83.8% FLOPs reduction while maintaining competitive VQA performance.

## Executive Summary
This paper addresses the computational overhead caused by the large number of vision tokens in Vision-Language Models (VLMs). The authors propose Fourier-VLM, a method that compresses visual representations in the frequency domain using a low-pass filter applied via two-dimensional Discrete Cosine Transform (DCT). The DCT is efficiently computed using FFT with O(n log n) complexity, requiring no additional parameters. Fourier-VLM achieves competitive performance across various image-based benchmarks while reducing inference FLOPs by up to 83.8% and boosting generation speed by 31.2% compared to LLaVA-v1.5, demonstrating superior efficiency and practicality.

## Method Summary
Fourier-VLM introduces the Frequency Feature Compressor (FFC), a parameter-free module inserted after the vision encoder that applies 2D-DCT to spatial features, truncates high-frequency coefficients, and reconstructs compressed tokens via 2D-iDCT. The method leverages FFT for O(N log N) DCT computation and requires no additional learnable parameters. Training occurs in two stages: first aligning the projector MLP to compressed features on LAION-CC-SBU data, then finetuning projector and LLM with LoRA on mixed instruction data. The approach achieves 75-93% vision token reduction while maintaining VQA accuracy within 3-4% of baseline.

## Key Results
- Achieves up to 83.8% reduction in inference FLOPs while maintaining competitive VQA performance
- Demonstrates 31.2% faster generation speed compared to LLaVA-v1.5 on A100
- Shows minimal performance degradation (3.87% absolute drop) even at extreme 93.75% compression (36 tokens)
- Maintains competitive accuracy on VQAv2 (64.6%), GQA (64.9%), and SciQA (59.3%) benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Vision encoder outputs exhibit energy concentration in low-frequency components, enabling selective high-frequency pruning with minimal semantic loss. The vision encoder transforms raw pixels into feature representations where structural and semantic information predominantly resides in low-frequency coefficients. Core assumption: semantic content required for VLM tasks is preserved in low-frequency components while high-frequency components contain redundant or task-irrelevant information.

### Mechanism 2
FFT-based DCT provides O(N log N) compression without learnable parameters, avoiding optimization overhead and architectural modifications. The type-II DCT is computed via FFT by rearranging the input sequence, applying FFT, then multiplying by cosine/sine modulation terms. Core assumption: computational overhead of DCT/iDCT is negligible compared to LLM attention costs over long visual token sequences.

### Mechanism 3
Frequency truncation followed by iDCT reconstruction maintains spatial token structure, allowing seamless integration with existing projector and LLM components. After 2D-DCT, only top-left C×C frequency coefficients are retained, and 2D-iDCT reconstructs spatial features at reduced resolution C×C. Core assumption: projector can adapt to compressed feature distributions through finetuning without requiring architectural changes.

## Foundational Learning

- **Discrete Cosine Transform (DCT) and Energy Compaction**
  - Why needed here: DCT is the mathematical foundation of Fourier-VLM. Understanding why natural images concentrate energy in low frequencies explains why truncation works.
  - Quick check question: Given a 4×4 feature map with DCT coefficients, which coefficients would you retain to preserve 75% of signal energy with only 25% of tokens?

- **Vision Token Redundancy in VLMs**
  - Why needed here: The paper's central hypothesis is that vision tokens are more redundant than language tokens. This motivates compression as a viable strategy.
  - Quick check question: Why might a 336×336 image need 576 vision tokens in LLaVA, and what happens to attention complexity if we reduce to 64 tokens?

- **FFT-based DCT Implementation**
  - Why needed here: Practical deployment requires understanding how O(N log N) DCT is implemented, as this determines real-world latency.
  - Quick check question: Convert a length-8 sequence to DCT coefficients using the FFT-based method: what is the intermediate signal fed to FFT?

## Architecture Onboarding

- **Component map**: Input Image → Vision Encoder (CLIP ViT-L/336) → FFC Module → Projector (2-layer MLP) → LLM (Vicuna)
- **Critical path**: Vision encoder produces H_v ∈ R^{576×1024}, FFC reshapes to 24×24×1024, applies 2D-DCT, truncates to C×C×1024, 2D-iDCT reconstructs spatial features, flattened tokens pass through projector to LLM
- **Design tradeoffs**: Compression ratio vs. performance (144 tokens averages 64.6% accuracy, 36 tokens drops to 62.1%), static vs. adaptive compression (Fourier-VLM uses fixed C), training cost (requires full two-stage training)
- **Failure signatures**: Fine-detail hallucination (heart-shaped sign misclassified as "balloon" or "box"), position encoding drift (objects shift position in generated descriptions), no training-free mode (direct application to pretrained LLaVA without finetuning produces degraded outputs)
- **First 3 experiments**:
  1. Ablation on compression ratio: Train Fourier-LLaVA-7B with C² ∈ {256, 144, 64, 36} tokens; evaluate VQAv2, GQA, SciQA to reproduce Table 1 tradeoffs
  2. Latency profiling: Measure TTFT and KV cache on A100 with batch size 1-8 for vanilla vs. Fourier-LLaVA at 144 tokens; verify 31.2% speedup claim
  3. Cross-architecture transfer: Apply FFC to different vision encoder (e.g., SigLIP, EVA-CLIP) without retraining vision encoder; measure performance gap to assess generalization claims

## Open Questions the Paper Calls Out
1. How can frequency-domain compression strategies be effectively adapted to handle temporal redundancy in video-language models?
2. Would adaptive or content-aware frequency thresholds outperform the fixed grid truncation (C × C) used in the current Frequency Feature Compressor?
3. Can the semantic hallucinations observed at high compression ratios be mitigated without requiring full model finetuning?

## Limitations
- Spatial localization degradation at high compression (3-4% performance drop at 93.75% compression with qualitative errors in object identification)
- Energy concentration assumption may not generalize to specialized domains like medical imaging where high-frequency components carry critical information
- Training requirement prevents deployment as drop-in replacement for pretrained models

## Confidence
- **High Confidence**: FFT-based DCT implementation achieving O(N log N) complexity is mathematically sound; energy concentration phenomenon is observable; two-stage training procedure is correctly specified
- **Medium Confidence**: 83.8% FLOPs reduction and 31.2% speedup claims depend on specific hardware configurations; cross-architecture generalization is asserted but not validated; performance drop at extreme compression is measured but may vary
- **Low Confidence**: Assumption that frequency-based compression works equally well across all vision encoder architectures and image domains; "superior efficiency and practicality" claims without comparison to alternative strategies; parameter-free design advantages without empirical validation

## Next Checks
1. **Frequency Distribution Analysis Across Domains**: Apply 2D-DCT to vision encoder outputs from diverse image domains (medical imaging, satellite, microscopy, natural images) and quantify energy concentration in low-frequency components to validate core assumption.
2. **Latency-Amplitude Profiling**: Implement FFC and profile inference latency (TTFT, KV cache operations) on A100 across batch sizes 1-32 for compression ratios 25-100% to identify token count threshold where DCT overhead becomes negligible.
3. **Spatial Reasoning Benchmark Evaluation**: Evaluate Fourier-LLaVA-7B at various compression ratios (144, 64, 36 tokens) on spatial reasoning benchmarks specifically designed to test object localization and positional awareness to quantify degradation in tasks requiring precise spatial relationships.