---
ver: rpa2
title: 'Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep Reinforcement
  Learning'
arxiv_id: '2502.00802'
source_url: https://arxiv.org/abs/2502.00802
tags:
- learning
- fgsf
- trace
- network
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fisher-Guided Selective Forgetting (FGSF) addresses the primacy
  bias in deep reinforcement learning by leveraging the Fisher Information Matrix
  to selectively modify network weights during training. The method identifies characteristic
  memorization and reorganization phases in the FIM trace evolution and applies periodic
  weight scrubbing to prevent early experiences from dominating the learning process.
---

# Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2502.00802
- **Source URL:** https://arxiv.org/abs/2502.00802
- **Reference count:** 40
- **One-line result:** Up to 50% improvement in mean return on complex DeepMind Control Suite tasks using Fisher-guided weight scrubbing

## Executive Summary
FGSF addresses primacy bias in deep reinforcement learning by leveraging the Fisher Information Matrix to selectively modify network weights during training. The method identifies characteristic memorization and reorganization phases in the FIM trace evolution and applies periodic weight scrubbing to prevent early experiences from dominating the learning process. Across DeepMind Control Suite environments, FGSF achieves up to 50% improvement in mean return compared to baseline SAC in complex tasks like Humanoid, demonstrating superior performance and stability while maintaining efficient learning dynamics.

## Method Summary
FGSF is a wrapper around standard Soft Actor-Critic optimization that computes the empirical Fisher Information Matrix on each training batch using EKFAC approximation, then applies structured noise injection to weights every 10 optimization steps. The scrubbing formula: S(w) = w + (λσ²)^(1/4) × F^(-1/4) × ε where ε ~ N(0, I), λ = 5×10^(-7), and F is computed via NNGeometry library. The method can be applied to full networks or critic-only, with critic-only often matching full network performance while being computationally cheaper.

## Key Results
- 50% improvement in mean return on Humanoid (136.6 ± 14.4 vs 68.5 ± 21.9)
- Superior learning stability with lower variance across random seeds
- 15-20% computational overhead from FIM calculation
- Critic-only scrubbing achieves comparable performance to full network scrubbing

## Why This Works (Mechanism)

### Mechanism 1: FIM-Trace Characterization of Learning Phases
The Primacy Bias manifests as a specific "Memorization-to-Reorganization" pattern in the Fisher Information Matrix trace, where early training shows a sharp spike in Tr(F) followed by decline. FGSF regulates this magnitude, preventing the network from entering a low-plasticity state where early experiences are "locked" in. The FIM trace directly reflects susceptibility to overfitting early data rather than being a benign byproduct of standard learning.

### Mechanism 2: Structured Noise Injection via Fisher Geometry
FGSF scales noise by the inverse fourth root of the Fisher matrix, selectively perturbing weights based on their sensitivity. Unlike isotropic Gaussian noise that perturbs weights uniformly, this Fisher-guided approach "scrubs" information in a way that minimizes damage to retained knowledge while maximizing forgetting of early data influence. The empirical Fisher Matrix calculated on a batch sufficiently approximates the true curvature to guide the noise injection effectively.

### Mechanism 3: Critic-Specific Plasticity Regulation
The Critic network is the primary victim of Primacy Bias, with an order-of-magnitude difference in Tr(F) between Critic and Actor. Applying FGSF specifically to the Critic stabilizes the TD-targets used to train the Actor, preventing the propagation of biased value estimates. The Actor's learning dynamics are sufficiently corrected by stabilizing the Critic alone, without needing direct intervention on Actor weights.

## Foundational Learning

- **Fisher Information Matrix (FIM)**
  - Why needed: The core diagnostic and operational tool of the paper. Without understanding FIM as a measure of parameter sensitivity/curvature, the "Memorization Phase" and "Scrubbing" concepts are opaque.
  - Quick check: Can you explain why a high FIM trace magnitude suggests a network is sensitive to specific data points (memorization) rather than generalizing?

- **Primacy Bias in DRL**
  - Why needed: This is the specific failure mode being solved. Understanding that off-policy RL tends to "lock" onto early, random exploration data is crucial for diagnosing when to apply FGSF.
  - Quick check: Why does the Replay Buffer exacerbate Primacy Bias compared to on-policy methods?

- **Machine Unlearning (Scrubbing)**
  - Why needed: FGSF adapts theoretical frameworks from the "unlearning" literature. Understanding the goal of "scrubbing" weights to remove data influence helps explain the specific mathematical formulation of the noise injection.
  - Quick check: How does the "Forgetting Lagrangian" differ from standard weight decay or regularization?

## Architecture Onboarding

- **Component map:** SAC Base Agent -> FGSF Module -> FIM Calculator (EKFAC) -> Injector -> Weights
- **Critical path:** Standard Optimization Step (SGD/Adam on batch) → Checkpoint: Is step % F == 0? → Compute Empirical FIM on current batch → Sample Gaussian Noise ε → Apply Fisher-scaled noise to weights
- **Design tradeoffs:**
  - Scrubbing Frequency (F) vs. Forgetting Coefficient (λ): High frequency requires lower λ to avoid instability. Default F=10
  - Computation vs. Performance: FGSF adds 15-20% training overhead due to FIM calculation
  - Full vs. Critic-Only: Critic-only scrubbing is computationally cheaper and often more stable, but Full scrubbing may squeeze out marginal performance gains in complex environments
- **Failure signatures:**
  - Divergence/NaNs: λ is too high (too much noise)
  - No Improvement: λ is too low (ineffective forgetting) or task is too simple where PB isn't the bottleneck
  - Memory OOM: FIM calculation on large networks without efficient approximation
- **First 3 experiments:**
  1. Baseline Comparison: Run SAC vs. FGSF (default λ=5e-7) on Humanoid to verify ~50% performance gain
  2. Ablation (Critic vs. Full): Isolate the mechanism by running FGSF on Critic-only vs. Full Network
  3. Geometry Validation: Compare FGSF vs. "Gaussian Noise Injection" baseline to confirm Fisher-guided structure is necessary

## Open Questions the Paper Calls Out

- Can FGSF be effectively applied to transfer learning scenarios to mitigate the overfitting of agents to source tasks?
- How does FGSF interact with optimization algorithms that naturally utilize the Fisher Information Matrix, such as natural gradient descent?
- Can the Fisher Information Matrix be leveraged for more targeted weight interventions than the current noise injection approach?

## Limitations

- Hyperparameter sensitivity: The method shows inconsistent performance on Acrobot, suggesting limited generalization across task complexities
- Computational overhead: FGSF adds 15-20% training overhead due to EKFAC FIM approximation calculations
- Geometry assumption: The claim that Fisher-guided noise is fundamentally superior to isotropic noise lacks rigorous ablation studies beyond a single baseline comparison

## Confidence

- **High Confidence:** The empirical performance improvements on DeepMind Control Suite environments (50% gain on Humanoid) are well-documented and reproducible
- **Medium Confidence:** The FIM-trace characterization of memorization-to-reorganization phases is supported by presented evidence but requires deeper theoretical grounding
- **Low Confidence:** The claim that Fisher-guided noise injection is fundamentally superior to isotropic noise due to geometry preservation lacks rigorous ablation studies

## Next Checks

1. **Mechanism Isolation:** Run FGSF with FIM-trace computation but no weight perturbation to verify that FIM spikes correlate with performance degradation in baseline SAC
2. **Geometry Ablation:** Compare FGSF against structured noise injection using alternative geometric priors (e.g., Hessian diagonal approximation) to isolate whether Fisher geometry is essential
3. **Off-Policy Generalization:** Test FGSF on non-Actor-Critic DRL algorithms (DQN, Rainbow) to determine if the critic-specific mechanism is truly algorithm-dependent or represents a broader principle