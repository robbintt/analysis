---
ver: rpa2
title: Off Policy Lyapunov Stability in Reinforcement Learning
arxiv_id: '2509.09863'
source_url: https://arxiv.org/abs/2509.09863
tags:
- lyapunov
- function
- learning
- control
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning stable control policies
  in reinforcement learning (RL) for robotics systems. Traditional RL lacks stability
  guarantees, and while recent approaches learn Lyapunov functions alongside control
  policies, they are sample-inefficient due to their on-policy nature.
---

# Off Policy Lyapunov Stability in Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.09863
- Source URL: https://arxiv.org/abs/2509.09863
- Reference count: 28
- Primary result: Novel off-policy method for learning Lyapunov-stable control policies, demonstrated on pendulum and quadrotor tasks with improved sample efficiency

## Executive Summary
This paper addresses the challenge of learning stable control policies in reinforcement learning for robotics systems. Traditional RL lacks stability guarantees, and while recent approaches learn Lyapunov functions alongside control policies, they are sample-inefficient due to their on-policy nature. The authors propose a novel method to learn Lyapunov functions off-policy by extending existing Lyapunov risk formulations to account for off-policy data. The key innovation is learning a neural Lyapunov function that depends on both state and action, then using the expectation over actions under the current policy to verify Lyapunov conditions. This approach is incorporated into Soft Actor Critic (SAC) and Proximal Policy Optimization (PPO) algorithms to provide stability certificates while maintaining data efficiency.

## Method Summary
The authors extend Lyapunov risk formulations to learn Lyapunov functions off-policy by conditioning the Lyapunov function on both state and action. The Lyapunov network $L_\eta(s,a)$ is trained using transitions from a replay buffer, with the Lie derivative calculated using a modified finite-difference that evaluates the current policy's action at the next state. The method is incorporated into SAC and PPO algorithms through augmented loss functions that penalize positive Lyapunov derivatives. This enables stability certification while maintaining the data efficiency benefits of off-policy learning.

## Key Results
- LSAC achieved highest reward with fewest training steps on Pendulum-v1, stabilizing pendulum closest to equilibrium with minimal noise
- LPPO was more sample-efficient than PPO on quadrotor trajectory tracking while achieving similar maximum rewards
- Learned Lyapunov functions showed significantly fewer violations of the decreasing condition compared to baseline methods
- Both algorithms maintained stability certificates while improving sample efficiency over on-policy alternatives

## Why This Works (Mechanism)

### Mechanism 1
Conditioning the Lyapunov function on both state and action allows stability verification using off-policy data distributions. Traditional Lyapunov functions $L(s)$ depend only on state. The authors propose learning $L_\eta(s, a)$ (a state-action Lyapunov function). By taking the expectation $\mathbb{E}_{a \sim \pi}[L_\eta(s, a)]$, the system evaluates the stability of the current policy using transitions $(s, a, s')$ generated by older policies (off-policy data). This decouples the data generation policy from the evaluation policy. Core assumption: The expectation over actions accurately approximates the true Lyapunov condition for the closed-loop system under the current policy.

### Mechanism 2
A modified finite-difference Lie derivative enables the "Lyapunov Risk" to be calculated on off-policy transitions. Standard on-policy methods approximate the Lie derivative $\dot{L}$ using $(L(s') - L(s)) / \Delta t$. For off-policy data, the action $a$ leading to $s'$ was not generated by the current policy. The authors re-define the derivative term to measure the change between the actual value at $s$ and the projected value at $s'$ under the current policy: $(L_\eta(s', \pi(s')) - L_\eta(s, a)) / \Delta t$. This penalizes the network if the policy's predicted next state increases the Lyapunov function. Core assumption: The transition dynamics $s \to s'$ are consistent regardless of the policy that generated the action.

### Mechanism 3
Penalizing positive Lyapunov derivatives in the policy loss guides the agent toward stable equilibria without requiring hard constraints. The method augments standard RL losses (SAC/PPO) with a term $\beta \cdot \max(0, L_{f, \Delta t} L_\eta + \mu)$. This acts as a soft constraint: if the policy update suggests an action that increases the Lyapunov function (instability), the loss penalizes it. The hyperparameter $\mu$ enforces a minimum decay rate, encouraging aggressive stabilization. Core assumption: The base RL algorithm (SAC/PPO) has sufficient gradient signal to satisfy both the reward maximization and the stability constraint simultaneously.

## Foundational Learning

- **Concept:** Lyapunov Stability Theory (Direct Method)
  - **Why needed here:** This paper is fundamentally about enforcing Lyapunov conditions (positive definite function, negative semi-definite derivative) via neural networks. You cannot interpret the loss function or the "stability certificates" without understanding what a Lyapunov function represents mathematically.
  - **Quick check question:** Given a function $V(x)$, what two conditions must it satisfy to prove a system is stable at the origin?

- **Concept:** Off-Policy vs. On-Policy Reinforcement Learning
  - **Why needed here:** The central value proposition of this paper is bringing Lyapunov stability (previously on-policy) to off-policy algorithms (SAC). Understanding the difference in data reuse and sample efficiency is required to evaluate the claims.
  - **Quick check question:** Why is an off-policy algorithm generally more sample-efficient than an on-policy algorithm?

- **Concept:** Soft Actor-Critic (SAC) and Entropy Regularization
  - **Why needed here:** The proposed LSAC builds directly on SAC. The interaction between the Lyapunov constraint and the entropy bonus (exploration vs. stability) is a key dynamic in the experimental results.
  - **Quick check question:** In SAC, what role does the entropy term play, and how might a "stability constraint" conflict with it?

## Architecture Onboarding

- **Component map:** Replay Buffer $D$ -> Lyapunov Network $L_\eta(s,a)$ -> Lyapunov Loss Module -> Policy $\pi_\phi$ -> Augmented Policy Loss -> Base RL Algorithm (SAC/PPO)

- **Critical path:**
  1. Sample batch $(s, a, s')$ from Replay Buffer $D$.
  2. Lyapunov Update: Compute target $L(s', \pi(s'))$ and derivative. Backprop $J_L(\eta)$ to train the Lyapunov Network.
  3. Policy Update: Sample new action $a_{new} \sim \pi(s)$. Compute Lyapunov derivative for $a_{new}$. Add penalty to standard RL loss.
  4. Certification: Monitor $J_L(\eta)$; if near zero, the system is theoretically stable.

- **Design tradeoffs:**
  - Stability vs. Optimality: The hyperparameter $\mu$ (min decay rate) forces the system to decay error faster but may limit the agent's ability to perform exploratory or aggressive maneuvers required for high rewards.
  - Bias vs. Efficiency: The paper notes that off-policy data introduces bias because $L_\eta$ depends on the current controller, but old data came from previous controllers. The method ignores this bias for efficiency.
  - Constraint Softness: Using a soft penalty ($\beta$) rather than a hard projection makes optimization easier but provides weaker guarantees than constrained policy optimization.

- **Failure signatures:**
  - Lyapunov Loss Divergence: If $J_L(\eta)$ does not converge to zero, the stability certificate is invalid.
  - Excessive Conservatism: The agent converges to a local minimum near the initial state, refusing to move toward the goal to keep $L$ low.
  - High Violation Rate: Red dots in Fig 3 (violations) dominate the trajectory, indicating the learned $L_\eta$ is not a valid Lyapunov function for the learned policy.

- **First 3 experiments:**
  1. Pendulum-v1 Sanity Check: Train LSAC vs. SAC. Plot the Lyapunov Loss alongside Reward to verify the "certificate" correlates with stability.
  2. Hyperparameter Sweep ($\beta$): Test the sensitivity of the stability penalty weight. If $\beta=0$, you have SAC; if $\beta \to \infty$, does the agent freeze?
  3. Violation Counting: Run the trained policy on Pendulum. Count the frequency of positive Lie derivatives (red dots in Fig 3) to quantitatively verify if "Almost Lyapunov" conditions hold.

## Open Questions the Paper Calls Out

- **Question:** Can the proposed off-policy Lyapunov control methods successfully transfer from simulation to physical robotic hardware without significant performance degradation?
  - **Basis:** The authors state in the Limitations section that "the algorithms presented have yet to be tested in physical environments" and identify this as "an important consideration for future work."
  - **Why unresolved:** Simulation environments often fail to capture the full complexity of physical dynamics (e.g., friction, sensor noise, delays), which may violate the learned Lyapunov conditions or destabilize the policy.
  - **What evidence would resolve it:** Demonstration of LSAC or LPPO successfully stabilizing a physical robot (e.g., a quadrotor or pendulum) with metrics comparing tracking error and stability violations against the simulation results.

- **Question:** Can formal theoretical stability guarantees be established for the proposed learning algorithms?
  - **Basis:** Section 6 notes that the work "currently lacks theoretical support" and states that "Developing stability guarantees for the proposed algorithms is an important area for future work."
  - **Why unresolved:** The paper relies on empirical minimization of a Lyapunov risk loss, which satisfies conditions on sampled data but does not constitute a mathematical proof that the system dynamics will remain stable during the entire learning process.
  - **What evidence would resolve it:** Mathematical proofs demonstrating that the policy and Lyapunov network updates converge such that the system state asymptotically stabilizes from a theoretical perspective.

- **Question:** What is the quantitative impact of off-policy data bias on the convergence and accuracy of the learned Lyapunov function?
  - **Basis:** The authors acknowledge that "there is bias in the data collected from previous control policies" and admit they "do not analyze the impact of the bias itself," suggesting further work could compare this against importance sampling.
  - **Why unresolved:** While the expectation over actions mitigates some issues, the distribution shift between the current policy and the replay buffer data may introduce errors in the Lie derivative approximation that are not yet quantified.
  - **What evidence would resolve it:** A comparative study analyzing the error in the Lyapunov decrease condition when training on pure off-policy data versus on-policy data, potentially utilizing importance sampling to correct the distribution mismatch.

## Limitations
- Lack of theoretical stability guarantees for the proposed algorithms
- Missing implementation details including network architectures and hyperparameters
- Limited experimental scope with only two tasks and no comparison against other stability-aware methods
- Unquantified impact of off-policy data bias on Lyapunov function accuracy

## Confidence

- **High confidence:** The core mechanism of conditioning Lyapunov functions on state-action pairs and using policy expectations is technically sound and well-explained. The finite-difference Lie derivative modification for off-policy learning is clearly derived.
- **Medium confidence:** The experimental results show promising improvements in sample efficiency and stability, but the limited scope and missing implementation details prevent full verification. The Lyapunov loss convergence appears to support the method's theoretical foundations.
- **Low confidence:** Claims about robustness to distribution shift and generalization to more complex robotics tasks remain speculative without additional experiments.

## Next Checks

1. **Reproduce the Lyapunov loss convergence** on Pendulum-v1 by implementing LSAC with the same hyperparameters, monitoring whether $J_L(\eta)$ approaches zero as shown in Figure 2b.

2. **Conduct a systematic hyperparameter sweep** for β to determine if the stability constraint can be disabled (β=0) without loss of performance, and whether very high β causes over-conservatism.

3. **Quantify Lyapunov violations across all 10 seeds** and compare violation rates between LSAC and baselines to verify the claimed reduction in decreasing condition violations.