---
ver: rpa2
title: On the Alignment Between Supervised and Self-Supervised Contrastive Learning
arxiv_id: '2510.08852'
source_url: https://arxiv.org/abs/2510.08852
tags:
- nscl
- learning
- training
- epochs
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the alignment between self-supervised contrastive
  learning (CL) and negatives-only supervised contrastive learning (NSCL) during training.
  While prior work showed CL loss approximates NSCL loss asymptotically, this work
  asks whether their representations remain coupled throughout optimization.
---

# On the Alignment Between Supervised and Self-Supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2510.08852
- Source URL: https://arxiv.org/abs/2510.08852
- Authors: Achleshwar Luthra; Priyadarsi Mishra; Tomer Galanti
- Reference count: 40
- Primary result: CL and NSCL representations remain coupled during training with high-probability bounds on CKA/RSA similarity despite exponential parameter divergence

## Executive Summary
This paper bridges self-supervised contrastive learning (CL) and supervised contrastive learning (NSCL) by showing their representations stay aligned throughout training, not just in the loss function. While prior work showed CL loss approximates NSCL loss asymptotically, this work proves the induced similarity matrices remain close under shared randomness, yielding high-probability bounds on representational similarity metrics like CKA and RSA. The key insight is that despite exponential parameter divergence, the similarity-space dynamics have an "instability rate" of only 1/(2τ²B), which is negligible for typical batch sizes. Experiments confirm alignment strengthens with more classes, higher temperatures, and appropriate batch size scaling.

## Method Summary
The paper theoretically analyzes the training dynamics of CL and NSCL under shared randomness (identical initialization, batches, and augmentations). It establishes high-probability bounds on the Frobenius norm gap between their similarity matrices using concentration inequalities for batch composition and Hessian-based Lipschitz stability. The theory is validated through empirical studies on CIFAR-100 and Mini-ImageNet using ResNet-50 encoders with 2-layer MLP projection heads, tracking CKA/RSA alignment and weight-space distances throughout training.

## Key Results
- Similarity matrices induced by CL and NSCL remain close under shared randomness, yielding high-probability bounds on representational similarity metrics like CKA and RSA
- Alignment strengthens with more classes, higher temperatures, and depends on batch size with appropriate learning rate scaling
- NSCL aligns more closely with CL than other supervised methods like cross-entropy
- Parameter-space divergence can grow exponentially while similarity-space remains tightly coupled

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CL and NSCL representations remain coupled during training despite exponential parameter divergence.
- **Mechanism:** The similarity-space update dynamics have an "instability rate" of only 1/(2τ²B), which is negligible for typical batch sizes (B ~ 10²–10³). This contrasts with parameter space where instability scales with loss smoothness β. The exponential growth factor exp(1/(2τ²B) · Σηₜ) accumulates slowly, while the prefactor (1/τ√B)(Σηₜ)ΔC,δ(B;τ) shrinks with larger C and B. The combined effect bounds the Frobenius gap between CL and NSCL similarity matrices, which transfers directly to centered Gram matrices used in CKA.
- **Core assumption:** Bounded similarity values [-1,1] (cosine similarity on normalized embeddings); shared randomness across training runs (same initialization, batches, augmentations).
- **Evidence anchors:**
  - [abstract] "similarity matrices induced by CL and NSCL remain close under shared randomness, yielding high-probability bounds on representational similarity metrics like CKA and RSA"
  - [section 4.1] "the exponential factor is moderated by the 1/2τ²B term in the exponent. Unlike parameter space, where the growth rate scales with β, the 'instability rate' in similarity space is only 1/2τ²B"
  - [corpus] Luthra et al. (2025) establish the loss-level CL–NSCL approximation that this paper extends to representations.
- **Break condition:** Violations of bounded similarity; extremely long training horizons where the exponential factor dominates; very small batch sizes (B < 10) or very low temperatures (τ << 0.1) that amplify sensitivity.

### Mechanism 2
- **Claim:** High-probability batch composition guarantees control the CL–NSCL reweighting gap.
- **Mechanism:** For class-balanced data with C classes, each anchor's denominator has expected proportion (1 - 1/C) negatives. Hoeffding's inequality bounds the deviation ϵB,δ = √((1/2B)log(TB/δ)). On this high-probability event, the ratio of positive-to-negative softmax mass is bounded by ΔC,δ(B;τ) = 2e^(2/τ)(1/C + ϵB,δ)/(1 - 1/C - ϵB,δ). This directly bounds the total variation distance between CL and NSCL softmax distributions, which propagates to gradient differences.
- **Core assumption:** Class-balanced training data; i.i.d. batch sampling with replacement; temperature τ > 0.
- **Evidence anchors:**
  - [section D.2] "with probability at least 1-δ, for every step t and every anchor i, |Dⁿᵉᵍᵢ| ≥ B(1 - 1/C - ϵB,δ)"
  - [corollary 3] "Z⁰ᵢ/Zⁿᵉᵍᵢ ≤ e^(2/τ)(1/C + ϵB,δ)/(1 - 1/C - ϵB,δ) = (1/2)ΔC,δ(B;τ)"
  - [corpus] Corpus lacks direct evidence for this specific concentration argument.
- **Break condition:** Severely class-imbalanced data; very small batch sizes that break concentration; C so small that (1 - 1/C - ϵB,δ) ≤ 0.

### Mechanism 3
- **Claim:** Temperature-controlled softmax Hessian bounds provide Lipschitz stability for similarity gradients.
- **Mechanism:** The batch loss Hessian in similarity space is block-diagonal across anchors with spectral norm ≤ 1/(2τ²B). This 1/(2τ²B)-Lipschitzness means that when CL and NSCL similarity matrices differ by εF in Frobenius norm, their gradients differ by at most (1/2τ²B)·εF. Combined with the reweighting gap (Mechanism 2), this yields the per-step recurrence Dₜ₊₁ ≤ (1 + ηₜ/(2τ²B))Dₜ + ηₜ·(ΔC,δ/(τ√B)), which integrates to the coupled bound.
- **Core assumption:** Temperature τ > 0; bounded logits in [-1, 1] enabling softmax bounds.
- **Evidence anchors:**
  - [lemma 2] "‖∇²ℓBₜ(Σ)‖₂→₂ ≤ 1/(2τ²B)"
  - [section 4.1 proof idea] "a stability term from the dependence on the current similarities, controlled by the 1/(2τ²B)-Lipschitzness of the batch-gradient map"
  - [corpus] Corpus provides no contradictory evidence but lacks independent confirmation of this Hessian analysis.
- **Break condition:** Temperature approaching zero (τ → 0) which makes Hessian unbounded; non-cosine similarity functions violating the [-1,1] bound.

## Foundational Learning

- **Concept:** InfoNCE loss structure (anchor-positive vs negatives in denominator)
  - **Why needed here:** CL and NSCL differ only in whether same-class samples are excluded from the denominator. Understanding this structural difference is essential for grasping why they can produce similar representations despite different explicit supervision.
  - **Quick check question:** Given a batch with B samples from C classes, what is the maximum number of same-class entries that could appear in an anchor's denominator under uniform class distribution?

- **Concept:** CKA (Centered Kernel Alignment) and RSA (Representational Similarity Analysis) as invariance-aware metrics
  - **Why needed here:** The paper's main result is that CKA and RSA remain high between CL and NSCL. These metrics measure similarity structure rather than parameter values, which is why they can detect alignment when weight-space distances fail.
  - **Quick check question:** If two models have identical CKA=1.0 but different parameter norms, what does this imply about their representations?

- **Concept:** Temperature τ as a softmax sharpness control
  - **Why needed here:** Temperature appears throughout the theoretical bounds and empirical results. Lower τ sharpens the softmax (more selective), higher τ softens it (more uniform). The theory predicts alignment strengthens with higher τ—counterintuitively, "softer" objectives align better.
  - **Quick check question:** If τ is increased from 0.1 to 1.0, what happens to the ratio of the largest to smallest softmax weight for logits in [-1,1]?

## Architecture Onboarding

- **Component map:**
  - Encoder backbone: ResNet-50 (standard, any backbone works as theory is architecture-agnostic)
  - Projection head: 2-layer MLP (2048→2048→128), kept for both CL and NSCL
  - Loss heads: CL uses full denominator; NSCL filters same-class samples per-anchor before computing softmax
  - Alignment probes: CKA/RSA computed on frozen representations from a held-out reference set

- **Critical path:**
  1. Initialize both CL and NSCL models identically (same seed, same weights)
  2. For each training step: sample identical batch + augmentations for both models
  3. CL computes loss with all 2B-2 negatives; NSCL masks same-class entries before softmax
  4. Track similarity matrices Σᴄʟₜ and Σɴsᴄʟₜ on fixed reference set every N epochs
  5. Compute CKA/RSA and weight-space distances at checkpoints

- **Design tradeoffs:**
  - **Higher temperature (τ → 1.0):** Stronger alignment guarantees, but may reduce downstream discrimination (softer training signal)
  - **Larger batch size:** Reduces batch-composition variance ϵB,δ, but may require different learning rate scaling (O(B) scaling harms alignment; O(√B) or lower helps)
  - **More training classes:** Tightens ΔC,δ, improving bounds, but requires dataset with sufficient class diversity

- **Failure signatures:**
  - CKA declining after initial epochs: likely learning rate too high or temperature too low for the batch size
  - Weight gap growing while CKA stays high: expected behavior (parameters diverge, representations stay aligned)
  - CKA and RSA both dropping to ~0.5 or below: check that batch sampling is truly shared between runs

- **First 3 experiments:**
  1. **Validation run on CIFAR-100 with τ∈{0.1, 0.5, 1.0}:** Train CL and NSCL pairs with shared randomness for 300 epochs. Plot CKA over time. Confirm that τ=1.0 yields highest alignment. Verify weight-space divergence grows while CKA stays >0.85.
  2. **Class ablation on Mini-ImageNet:** Train on random C'∈{10, 25, 50, 100} class subsets. Plot final CKA vs C'. Confirm monotonic increase with class count as theory predicts.
  3. **Batch size scaling with controlled learning rates:** Train with B∈{256, 512, 1024} under four LR schedules (O(B), O(√B), O(B^0.25), O(1)). Identify which schedule yields alignment that increases with B vs decreases.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis requires class-balanced data and moderate batch sizes; highly imbalanced datasets or extremely large C may break the concentration bounds
- Temperature regime sensitivity: the theory assumes τ is not too small; as τ→0 the softmax becomes unstable and the Hessian bounds degrade
- Architecture-agnostic but empirical verification limited to ResNet-50 with 2-layer MLP projection heads; transfer to transformers untested
- Single run dynamics: the bound assumes shared randomness (same initialization, batches, augmentations); tiny variations could affect bound tightness

## Confidence
- **High confidence:** The loss-level CL ≈ NSCL approximation (foundation of the paper) is established by prior work (Luthra et al., 2025) and the batch-composition concentration arguments are standard Hoeffding applications
- **Medium confidence:** The similarity-space Lipschitz bounds and the recurrence solution for Dₜ are mathematically sound but rely on maintaining the concentration events over long training horizons; practical violations are plausible
- **Medium confidence:** Empirical results showing monotonic alignment improvements with τ, C, and appropriate B are consistent with theory but depend on controlled training conditions (shared randomness, fixed learning rates)

## Next Checks
1. **Class imbalance robustness:** Train CL/NSCL pairs on CIFAR-100 with varying class imbalance ratios (e.g., 10%, 50%, 90% balanced). Measure whether CKA still remains high under NSCL and whether the theory's concentration bounds become vacuous.
2. **Extreme temperature sweep:** Train with τ∈{0.01, 0.03, 0.1, 0.3, 1.0, 3.0}. Plot alignment vs. τ alongside downstream accuracy to identify the operational regime where the "higher τ improves alignment" prediction breaks down.
3. **Batch size with adaptive learning rates:** Following the paper's finding that O(B) LR scaling hurts alignment, test LR schedules of the form η₀·B⁻α for α∈{0.25, 0.5, 0.75}. Determine which α yields alignment that increases with B while maintaining convergence speed.