---
ver: rpa2
title: Multi-Task Learning for Robot Perception with Imbalanced Data
arxiv_id: '2602.01899'
source_url: https://arxiv.org/abs/2602.01899
tags:
- depth
- labels
- semantic
- segmentation
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of training multi-task deep learning
  models in scenarios where ground truth labels are imbalanced or missing for certain
  tasks. It proposes using a multi-modal teacher network to generate pseudo-labels
  for tasks lacking ground truth, by leveraging available labels from other tasks
  as additional input.
---

# Multi-Task Learning for Robot Perception with Imbalanced Data

## Quick Facts
- arXiv ID: 2602.01899
- Source URL: https://arxiv.org/abs/2602.01899
- Authors: Ozgur Erkent
- Reference count: 0
- Primary result: Proposed method improves semantic segmentation mIoU from 22.1% to 25.3% and reduces depth RMSE from 0.790 to 0.653 on NYUDv2 dataset with imbalanced labels

## Executive Summary
This paper addresses the challenge of training multi-task deep learning models when ground truth labels are imbalanced or missing for certain tasks. The proposed approach uses a multi-modal teacher network to generate pseudo-labels for tasks lacking ground truth by leveraging available labels from other tasks as additional input modalities. A student network is then trained using these pseudo-labels, achieving improved performance compared to standard imbalanced data handling methods. Experiments on semantic segmentation and depth estimation tasks using NYUDv2 and Cityscapes datasets demonstrate the effectiveness of the approach, particularly in scenarios with limited ground truth availability.

## Method Summary
The method employs a teacher-student architecture where a multi-modal teacher network (trained with both RGB and depth/segmentation ground truth) generates pseudo-labels for the under-labeled task. The student network, which only receives RGB input, is trained using both the original ground truth labels and the generated pseudo-labels. The teacher uses an HRNet_w18 backbone with feature fusion modules to combine modalities at different scales, while the student uses a standard HRNet_w18 backbone. Pseudo-labels are filtered using a confidence threshold (τ=0.9) to exclude low-quality predictions. The asymmetric task relationship is leveraged where depth estimation improves semantic segmentation, but the reverse does not hold.

## Key Results
- On NYUDv2 dataset: mIoU improved from 22.1% to 25.3% and RMSE reduced from 0.790 to 0.653 using pseudo-labels
- On Cityscapes with limited training samples (300 images): mIoU improved from 52.83% to 54.06%
- Asymmetric task relationship confirmed: depth estimation improves semantic segmentation via teacher network, but not vice versa
- Confidence threshold filtering (τ=0.9) effectively removes noisy pseudo-labels from student training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal teacher networks can generate high-quality pseudo-labels for tasks lacking ground truth by leveraging available labels from related tasks as additional input modalities.
- Mechanism: The teacher network receives both RGB input and ground truth from the well-labeled task during training. This extra modality provides structural guidance when predicting the missing-task labels, improving pseudo-label accuracy compared to uni-modal approaches.
- Core assumption: The tasks share common sub-functions in their underlying representations, such that information from one task constrains the solution space for the other.
- Evidence anchors: Table 3 shows UMMT-PS achieving 25.3 mIoU vs 22.1 for baseline, with RMSE dropping from 0.790 to 0.653.

### Mechanism 2
- Claim: Task relationships can be asymmetric—depth estimation improves semantic segmentation, but segmentation does not improve depth estimation.
- Mechanism: The sub-functions learned for depth estimation largely overlap with those needed for segmentation, but segmentation includes additional task-specific features not useful for depth. When depth is input to the teacher, it provides relevant constraints for segmentation prediction; the reverse provides less useful information.
- Core assumption: The feature space can be decomposed into shared and task-specific sub-functions, with unequal overlap between task pairs.
- Evidence anchors: Table 4 shows UMMT-PD performs worse: 23.9 mIoU vs 25.3, and RMSE increases to 0.882 from 0.653.

### Mechanism 3
- Claim: Pseudo-label effectiveness depends on a confidence threshold filter to exclude low-quality predictions from student training.
- Mechanism: The teacher outputs pass through a softmax layer; only predictions with confidence above threshold τ=0.9 are retained as pseudo-labels. This filters uncertain predictions that would otherwise inject label noise.
- Core assumption: Teacher confidence correlates with pseudo-label correctness, and high-confidence predictions are sufficiently accurate for student learning.
- Evidence anchors: "We used the labels which have a value of larger than a threshold of τ=0.9 on the last softmax layer of DNN."

## Foundational Learning

- Concept: Multi-Task Learning with Shared Representations
  - Why needed here: The entire method assumes tasks can share a backbone and benefit from joint feature learning.
  - Quick check question: Can you explain why training two tasks together might require fewer samples per task than training them separately?

- Concept: Knowledge Distillation / Pseudo-Labeling
  - Why needed here: The method's core innovation is using a multi-modal teacher to supervise a uni-modal student via generated labels.
  - Quick check question: What happens to student performance if teacher pseudo-labels have systematic bias rather than random noise?

- Concept: Feature Pyramid Networks and Multi-Scale Processing
  - Why needed here: The architecture uses FPN at 4 scales with feature propagation and distillation modules.
  - Quick check question: Why might semantic segmentation benefit from features at multiple spatial resolutions?

## Architecture Onboarding

- Component map: RGB -> Backbone (HRNet_w18) -> FFM -> FPN (4 scales: 1/4, 1/8, 1/16, 1/32) -> FPM -> MMDM -> FA -> Task Predictions
- Critical path: Train multi-modal teacher with RGB + depth GT → Generate pseudo-segmentation labels with τ=0.9 threshold → Train uni-modal student with RGB only using GT + pseudo-labels
- Design tradeoffs:
  - Threshold τ=0.9 balances pseudo-label quantity vs quality
  - Loss weight α=1 (balanced between segmentation and depth) set via cross-validation
  - Multi-modal teacher requires paired multi-modal data during training (not inference)
  - Uni-modal student enables deployment without depth sensors
- Failure signatures:
  - Pseudo-labels degrading student performance: Check teacher calibration, lower τ, or verify task relationship
  - Depth estimation not improving despite pseudo-labels: Expected per Condition 2—depth→segmentation helps, not reverse
  - Large dataset showing minimal improvement: Diminishing returns when original training data is sufficient
- First 3 experiments:
  1. Reproduce ablation on NYUDv2 subset (500 depth, 250 segmentation labels): Compare UMMT vs UMMT-PS to validate setup
  2. Test asymmetric task relationship: Train MMMT-S to generate pseudo-depth labels, then train student—verify performance does NOT improve
  3. Sweep confidence threshold τ ∈ {0.7, 0.8, 0.9, 0.95} on validation set to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed multi-modal teacher-student framework effectively scale to and manage imbalances across three or more distinct perception tasks simultaneously?
- Basis in paper: The conclusion explicitly states: "As part of the future work, the analyses of more than two tasks can be performed and the empirical evaluations related to several tasks can be performed" (Page 12).
- Why unresolved: The current study limits its scope to two-task scenarios, leaving the interactions and computational complexity of handling multiple missing label sets in larger task groups unexplored.
- What evidence would resolve it: Empirical results from training a unified network on a dataset with >2 tasks where labels are missing non-uniformly across the tasks.

### Open Question 2
- Question: Can the theoretical relationship between task sub-functions (Conditions 1, 2, and 3) be quantified a priori to predict task transferability without requiring empirical training?
- Basis in paper: The paper notes that "a detailed theoretical background is not provided on how we can determine the tasks that support each other" and mentions that finding an upper bound for the multi-task problem is "intractable" (Page 3-4).
- Why unresolved: The determination of whether tasks share enough "common sub-functions" to benefit from the method is currently derived through observation and experimental trial (Condition 2), rather than a pre-calculable metric.
- What evidence would resolve it: A theoretical metric or analytical method that predicts the success of the pseudo-labeling approach for arbitrary task pairs before the expensive training of the teacher network is undertaken.

### Open Question 3
- Question: How does noise in the "excessive" task labels (e.g., noisy depth maps) impact the fidelity of the generated pseudo-labels and the stability of the student network?
- Basis in paper: In the Cityscapes experiments, the authors note that the depth obtained from the disparity map has "strong noise" and that "low accuracies on depth estimation are probably related to this issue" (Page 11-12).
- Why unresolved: The paper demonstrates improved performance but does not explicitly isolate or analyze the propagation of error when the privileged input modality (Teacher's input) contains significant systematic noise compared to the cleaner NYUDv2 data.
- What evidence would resolve it: An ablation study comparing teacher performance using clean synthetic depth versus noisy measured depth, measuring the signal-to-noise ratio of the resulting pseudo-labels.

## Limitations
- The asymmetric task relationship claim relies on limited experimental evidence from one dataset without extensive validation across diverse task pairs
- The pseudo-label confidence threshold τ=0.9 is treated as fixed without sensitivity analysis for different imbalance ratios
- Performance on extremely imbalanced datasets (e.g., 10:1 ratio) is not evaluated, leaving uncertainty about method robustness
- The method requires paired multi-modal data during teacher training, limiting applicability to datasets with complete multi-modal coverage

## Confidence
- **High confidence**: The multi-modal teacher generates pseudo-labels that improve student performance compared to training without them (mIoU 22.1% → 25.3% on NYUDv2, RMSE 0.790 → 0.653)
- **Medium confidence**: The asymmetric task relationship (depth→segmentation helps, segmentation→depth does not) based on limited experimental evidence
- **Low confidence**: The claim that pseudo-labels provide benefit across all imbalance ratios without quantitative analysis of threshold sensitivity or extreme imbalance scenarios

## Next Checks
1. **Generalize asymmetric benefit**: Test whether the depth→segmentation improvement holds on Cityscapes with segmentation pseudo-labels from depth, and test with additional task pairs to verify Condition 2 applies broadly
2. **Threshold sensitivity analysis**: Systematically vary τ from 0.7 to 0.99 and measure trade-offs between pseudo-label quantity and quality on validation sets to identify optimal thresholds for different imbalance ratios
3. **Extreme imbalance robustness**: Evaluate performance when ground truth labels are available for only 5-10% of one task (compared to current 50%) to identify when pseudo-labels become ineffective or harmful