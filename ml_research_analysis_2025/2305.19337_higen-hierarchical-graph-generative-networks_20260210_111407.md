---
ver: rpa2
title: 'HiGen: Hierarchical Graph Generative Networks'
arxiv_id: '2305.19337'
source_url: https://arxiv.org/abs/2305.19337
tags:
- graph
- graphs
- generation
- node
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HiGen, a hierarchical graph generative network
  designed to capture the multi-scale structure of real-world graphs. HiGen generates
  graphs in a coarse-to-fine fashion, producing communities in parallel at each level
  and predicting cross-edges between communities using separate neural networks.
---

# HiGen: Hierarchical Graph Generative Networks

## Quick Facts
- **arXiv ID:** 2305.19337
- **Source URL:** https://arxiv.org/abs/2305.19337
- **Reference count:** 40
- **Primary result:** HiGen achieves state-of-the-art performance in hierarchical graph generation with MMD scores competitive with flat generative models while capturing multi-scale structure

## Executive Summary
This paper presents HiGen, a hierarchical graph generative network designed to capture the multi-scale structure of real-world graphs. HiGen generates graphs in a coarse-to-fine fashion, producing communities in parallel at each level and predicting cross-edges between communities using separate neural networks. This modular approach enables scalable generation of large and complex graphs. The model employs a multinomial distribution to model edge weights in the hierarchical graph and derives a recursive factorization for this distribution, allowing for autoregressive generation of integer-valued edge weights. Empirical studies on various benchmark datasets demonstrate that HiGen achieves state-of-the-art performance in terms of graph quality across multiple metrics.

## Method Summary
HiGen generates graphs through a coarse-to-fine hierarchical approach, factorizing the joint distribution of a graph recursively across levels. The method uses Louvain algorithm to create a hierarchical graph structure as preprocessing, then employs GraphGPS for parent graph embeddings and attentive GNNs for community generation. Edge weights are modeled using a multinomial distribution with recursive factorization into binomial and multinomial components. The model generates communities in parallel while cross-edges between communities are predicted subsequently using a separate transformer-based architecture. Training uses negative log-likelihood loss with mixture models for parameter estimation.

## Key Results
- HiGen achieves state-of-the-art performance on graph generation benchmarks with MMD scores competitive with flat generative models
- The hierarchical approach successfully captures multi-scale structure while maintaining scalability
- Empirical results show superior performance on degree distribution, clustering coefficient, 4-node orbits, and Laplacian spectra metrics
- The model demonstrates effectiveness across multiple datasets including SBM, Protein, Enzyme, Ego, and Point Cloud graphs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating graphs via a coarse-to-fine hierarchy improves scalability and captures global structure better than flat generation, provided the graph exhibits community structure.
- **Mechanism:** The model factorizes the graph likelihood $p(G)$ recursively by levels ($L_0 \to L_L$). It first generates a coarse "parent" graph, then conditions the generation of finer "child" communities on this parent. This constrains the local generation space using global context.
- **Core assumption:** The graph's topology can be meaningfully reduced to a hierarchy where high-level nodes (super-nodes) effectively summarize low-level connectivity.
- **Evidence anchors:** [abstract] "successively generates the graph sub-structures in a coarse-to-fine fashion." [Section 3] Eq. 1 factorizes the joint distribution $p(G) = \prod p(G^l | G^{l-1})$.

### Mechanism 2
- **Claim:** Decomposing the graph into communities and bipartites enables parallel generation while maintaining inter-community consistency.
- **Mechanism:** HiGen separates the generation of intra-community edges (dense, local) from inter-community edges (bipartites). Given the parent graph, the model assumes communities are conditionally independent, allowing them to be generated in parallel. Cross-edges are predicted subsequently using a separate GNN.
- **Core assumption:** Conditional independence holds: $p(C_i | G^{l-1})$ is independent of $p(C_j | G^{l-1})$, and inter-community dependencies are sufficiently encoded in the parent graph.
- **Evidence anchors:** [abstract] "generates communities in parallel, followed by the prediction of cross-edges between communities." [Section 3] Theorem 3.1 proves conditional independence via Multinomial properties.

### Mechanism 3
- **Claim:** Modeling edge weights via a recursive "stick-breaking" process allows for efficient integer-weighted graph generation.
- **Mechanism:** Instead of predicting edges one-by-one (quadratic cost), the model distributes a "pool" of edge weights (determined by the parent node) to nodes in chunks (rows of the adjacency matrix). It breaks a Multinomial distribution into a product of Binomial (row weight) and Multinomial (edge distribution) distributions.
- **Core assumption:** The total edge weight (degree budget) of a community is strictly defined by its parent super-node.
- **Evidence anchors:** [abstract] "model the output distribution... with a multinomial distribution and derive a recursive factorization." [Section 3.1] Theorem 3.2 derives the factorization $\text{Mu} \to \text{Bi} \times \text{Mu}$.

## Foundational Learning

- **Concept:** **Community Detection / Graph Partitioning (Louvain)**
  - **Why needed here:** HiGen is not end-to-end in its structure discovery; it relies on a pre-processing step to define the hierarchy. You cannot implement the data loader without understanding how the input graph is split into $C_i$ and $B_{ij}$.
  - **Quick check question:** Given a graph $G$, does the Louvain algorithm maximize modularity or min-cut, and does it produce a hierarchy (dendrogram) or a flat partition?

- **Concept:** **Graph Message Passing (GNNs) vs. Graph Transformers (GraphGPS)**
  - **Why needed here:** The paper uses distinct architectures for different tasks: Attentive GNNs for local community generation and GraphGPS (Transformer) for global bipartite prediction. Understanding the difference in complexity ($O(n)$ vs $O(n^2)$) and receptive fields is critical.
  - **Quick check question:** Why would the authors choose a Transformer (Global Attention) for the bipartite generation task instead of a standard Message Passing GNN?

- **Concept:** **Autoregressive Generation**
  - **Why needed here:** While HiGen is hierarchical, the generation *within* a community is autoregressive. You must understand the sequential nature of the "stick-breaking" process to debug sampling loops.
  - **Quick check question:** In the context of Theorem 3.2, what happens to the "remaining stick" $r_t$ if the Binomial sample $v_t$ consumes all remaining probability mass?

## Architecture Onboarding

- **Component map:** Louvain preprocessing -> Hierarchical Graph construction -> GraphGPS encoder for parent graphs -> Attentive GNN + MLPs for community generation -> Transformer (GraphGPS) for bipartite prediction
- **Critical path:** The **Stick-Breaking Sampling Loop** (Algorithm 2, lines 10-26). This is where the integer weights are sampled. Errors here typically result in graph statistics mismatch (e.g., wrong average degree) or disconnected components.
- **Design tradeoffs:**
  - **Parallelism vs. Memory:** The model generates communities in parallel, but the augmented graph for bipartite generation ($\hat{G}^l$) can grow quadratically ($O(n_c^2)$) with cluster size. The paper notes using sub-sampling (HiGen-s) for large "Point Cloud" datasets to mitigate this.
  - **Fixed vs. Learned Hierarchy:** The hierarchy is fixed by the training data partitioning (Louvain). The model learns to reverse this specific partitioning, meaning it may struggle to generate graphs with a different community structure than the training set.
- **Failure signatures:**
  - **Mode Collapse:** Generated communities are identical regardless of parent conditioning.
  - **Disconnection:** The model generates valid communities but places zero weight on bipartite edges, resulting in a graph of isolated clusters.
  - **Spectral Mismatch:** High "Spec." error in Table 1 usually indicates the bipartite decoder is failing to capture global connectivity patterns.
- **First 3 experiments:**
  1. **Partitioning Ablation:** Train HiGen on SBM graphs using (a) Louvain partitioning vs. (b) Random partitioning. Verify that performance drops significantly without valid hierarchical structure.
  2. **Degree Distribution Check:** Generate samples and plot the degree distribution against the training set. If the "stick-breaking" is implemented correctly, the curves should overlap; divergence implies an error in the Multinomial/Binomial sampling logic.
  3. **Scalability Benchmark:** Measure memory consumption during bipartite generation as graph size increases. Verify if OOM occurs at the predicted quadratic rate for the standard model vs. the linear rate for the sub-sampled variant (HiGen-s).

## Open Questions the Paper Calls Out

- **Question:** Can the cardinality potential model improve the modeling of leaf-level binary edge weights compared to the current mixture of multinomials?
- **Basis in paper:** [explicit] Page 7, Footnote 2 suggests that "a possible extension to this work could be using the cardinality potential model... to model the edge weight at the leaf level."
- **Why unresolved:** The current model uses a standard softmax or multi-hot activation for leaf weights, which may not perfectly constrain the sum of binary edges to match the parent edge weight.
- **What evidence would resolve it:** Empirical comparison of Maximum Mean Discrepancy (MMD) scores between the standard HiGen and a cardinality-potential-based variant on datasets with strict binary edges.

## Limitations

- The hierarchical preprocessing via Louvain is fixed and not learned, creating a dependency on training data structure
- Limited ablation studies on the necessity of the hierarchy for different graph types
- Computational bottlenecks during bipartite generation are only partially resolved
- Key training hyperparameters remain unspecified, hindering reproducibility

## Confidence

- **High confidence** in the mechanism descriptions and theoretical derivations, particularly the recursive factorization theorems
- **Medium confidence** in empirical performance claims due to limited transparency in training details (batch size, epochs, sampling parameters)
- **Medium confidence** in scalability claims, as the quadratic growth in bipartite generation candidates is acknowledged but only partially addressed through sub-sampling

## Next Checks

1. Test HiGen on graphs without community structure (Erdos-Renyi) to validate the break condition for Mechanism 1
2. Implement the stick-breaking sampling loop independently to verify Theorem 3.2's factorization works as described
3. Compare HiGen's performance with and without hierarchical preprocessing on SBM graphs to quantify the value added by the coarse-to-fine approach