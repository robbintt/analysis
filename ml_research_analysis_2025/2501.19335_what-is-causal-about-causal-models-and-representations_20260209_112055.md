---
ver: rpa2
title: What is causal about causal models and representations?
arxiv_id: '2501.19335'
source_url: https://arxiv.org/abs/2501.19335
tags:
- causal
- interventions
- page
- intervention
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between causal models
  and real-world data-generating processes. The authors introduce a formal framework
  that explicitly links causal Bayesian networks (CBNs) to data-generating processes
  through representations and interpretations of actions as interventions.
---

# What is causal about causal models and representations?

## Quick Facts
- **arXiv ID:** 2501.19335
- **Source URL:** https://arxiv.org/abs/2501.19335
- **Reference count:** 40
- **Primary result:** The natural interpretation of actions as interventions (IntC) is circular and unfalsifiable; non-circular interpretations (IntP, IntS, IntK) enable model falsification under specific conditions.

## Executive Summary
This paper investigates the relationship between causal Bayesian networks (CBNs) and real-world data-generating processes by examining how actions should be interpreted as interventions. The authors prove that the seemingly natural interpretation of actions as interventions (IntC) is circular, making causal models trivially valid and impossible to falsify. They demonstrate that no interpretation can simultaneously satisfy four intuitive desiderata while being non-circular. The paper then examines alternative non-circular interpretations that may violate some desiderata but enable falsification of causal models under specific conditions, with implications for causal representation learning, causal discovery, and causal abstraction.

## Method Summary
The authors introduce a formal framework linking causal Bayesian networks to data-generating processes through representations and interpretations of actions as interventions. They define a causal representation as a function mapping low-level features to high-level variables, and specify different interpretations (IntC, IntP, IntS, IntK) for how actions relate to model interventions. The methodology involves proving mathematical impossibility results about interpretation desiderata, demonstrating non-circular interpretations that enable falsification, and examining implications for causal abstraction and representation learning. The framework distinguishes between emulation (mathematical compatibility) and interventional validity (predictive correctness).

## Key Results
- The IntC interpretation (actions as interventions matching model predictions) is circular, making every compatible CBN trivially interventionally valid and unfalsifiable.
- No interpretation can simultaneously satisfy four intuitive desiderata (D0-D4) while being non-circular, establishing an impossibility result.
- Non-circular interpretations (IntP, IntS, IntK) enable falsification of causal models under specific conditions, such as imperfect interventions or single-node interventions.
- Identifiability in causal representation learning does not imply interventional validity; a representation can be mathematically emulated by a CBN without the CBN being an interventionally valid model of the world.
- Causal abstraction rests on an infinite regress without explicit interpretations of actions.

## Why This Works (Mechanism)

### Mechanism 1: The Circularity of Natural Interpretations
- **Claim:** The intuitive mapping of actions to model interventions (IntC) renders causal models unfalsifiable because it creates a tautology between the definition of the action and the model's prediction.
- **Mechanism:** Under interpretation IntC, an action is defined as an intervention $d$ if and only if the resulting distribution matches the model's prediction for $d$. Because the condition for being an intervention includes the outcome, the model cannot fail; it is trivially valid for any compatible Causal Bayesian Network (CBN).
- **Core assumption:** The researcher implicitly uses the model's own predictions to identify which real-world actions count as valid interventions.
- **Evidence anchors:**
  - [abstract]: "We prove that the seemingly natural interpretation of actions as interventions is circular... no action yields empirical data that could possibly falsify such a model."
  - [Section 3]: "Corollary 3.3. IntC is circular... every compatible CBN C is an I−IntC valid model of Z*."
  - [corpus]: Corpus papers discuss causal interventions generally [arXiv:2511.04638], but do not address this specific circularity theorem.
- **Break condition:** If the interpretation of an action as an intervention does *not* depend on the downstream conditional distributions (violating Desiderata D1/D2), the circularity is broken.

### Mechanism 2: Falsification via Imperfect or Multi-node Interventions (IntP/IntS)
- **Claim:** Falsification becomes possible if one adopts non-circular interpretations that restrict how actions map to interventions, specifically by allowing imperfect interventions (IntP) or forbidding multi-node interpretations (IntS).
- **Mechanism:** By decoupling the definition of the intervention from the full interventional distribution, the model makes risky predictions. For example, IntP allows an action to be interpreted as a perfect intervention on a node even if downstream mechanisms change; if the outcome differs from the model, the model is falsified.
- **Core assumption:** The system admits "imperfect" actions (where non-intervened nodes change) or "multi-node" actions that can be mistaken for single-node ones.
- **Evidence anchors:**
  - [abstract]: "Instead, we examine non-circular interpretations that may violate some desiderata and show how this may in turn enable the falsification of causal models."
  - [Section 5.1]: "If we insist that all actions correspond to perfect interventions, then it becomes possible to falsify a causal model."
  - [corpus]: Evidence is weak in provided corpus; papers focus on effect estimation [arXiv:2501.15973] rather than the logical conditions of falsifiability.
- **Break condition:** If all real-world actions perfectly align with the modular independencies of the causal graph (violating the conditions for IntP/IntS falsification), the model cannot be falsified via these specific interpretations.

### Mechanism 3: The Gap Between Emulation and Validity
- **Claim:** A representation can be mathematically emulated by a Causal Bayesian Network (CBN) without the CBN being an "interventionally valid" model of the world.
- **Mechanism:** Emulation merely requires that for every action in a set, there exists *some* intervention in the model that reproduces the distribution. Validity requires that specific, hypothesized interventions (mapped via an interpretation) predict the outcomes of *future* actions. Identifiability in causal representation learning ensures emulation, not validity.
- **Core assumption:** The set of observed actions/environments does not fully constrain the interpretation of future, unseen actions.
- **Evidence anchors:**
  - [Section 6.1.1]: "Identifiability does not imply interventional validity... eg(E)−IntS can be an invalid model... even if eg(a) were a single-node intervention for all a in E."
  - [Section 2.1]: Definition 2.5 (Emulation) vs Definition 2.7 (Validity).
  - [corpus]: "How Rules Represent Causal Knowledge" [arXiv:2507.05088] supports the distinction between descriptive and interventional knowledge, aligning with this separation.
- **Break condition:** If the interpretation is circular (IntC), the gap disappears because validity collapses into emulation.

## Foundational Learning

- **Concept: Causal Bayesian Network (CBN)**
  - **Why needed here:** The paper deconstructs the CBN not just as a graph, but as a collection of Markov kernels that predict interventional distributions. Without this baseline, the distinction between "emulation" (math) and "validity" (world) is unclear.
  - **Quick check question:** Can you explain why a CBN is considered a "concise mathematical model of several distributions" rather than just a single joint distribution?

- **Concept: Interventional Validity vs. Compatibility**
  - **Why needed here:** This is the central definition of the paper. Compatibility means the model matches observational data. Validity means the model correctly predicts the outcome of specific actions interpreted as specific interventions.
  - **Quick check question:** If a model correctly predicts the observational distribution, is it automatically interventionally valid? (Answer: No, per the paper's argument on circularity).

- **Concept: Desiderata for Interpretations (D0-D4)**
  - **Why needed here:** The impossibility result rests on these logical properties (e.g., D1: "If it behaves like an intervention, it is that intervention"). Understanding these is required to see why avoiding circularity requires sacrificing intuitive properties.
  - **Quick check question:** Which desideratum states that interpreting an action as an intervention shouldn't depend on what other interventions are in the set?

## Architecture Onboarding

- **Component map:**
  - Data-Generating Process (DGP) -> Representation (Z*) -> Model (CBN) -> Interpretation (Int) -> Validator

- **Critical path:**
  1.  Define the Representation (Z*).
  2.  **Explicitly choose an Interpretation (IntP, IntS, or IntK).** Do not proceed with implicit IntC.
  3.  Test Interventional Validity by checking if real-world actions satisfy the interpretation constraints and match model predictions.

- **Design tradeoffs:**
  - **Interpretation Choice:** Choosing **IntC** (natural) provides no falsifiability but requires no assumptions about action mechanics. Choosing **IntP/IntS** allows falsifiability but risks rejecting a valid model if the real-world actions are "messy" (e.g., imperfect or multi-node).
  - **Abstraction Level:** Coarse representations (e.g., Total Cholesterol) are easier to model but may fail interventional validity because they aggregate distinct low-level mechanisms (LDL vs HDL) with different causal effects (Example 5.3).

- **Failure signatures:**
  - **"Trivial Validation":** If every model you test passes validation, you are likely using the circular IntC interpretation.
  - **"Spurious Falsification":** If a model is rejected because an action changed a non-intervened node, verify if you are using IntS/IntP too strictly for messy real-world data.
  - **"Identifiability-Validity Gap":** In Causal Representation Learning, successfully recovering the ground-truth graph from data does not guarantee the model will predict the effect of a *new* policy/action correctly.

- **First 3 experiments:**
  1.  **Audit for Circularity:** Review existing causal models in your system. Does the definition of "intervention" rely on observing the post-intervention distribution to confirm the intervention occurred? If so, re-label as IntC and flag as unfalsifiable.
  2.  **IntP Stress Test:** Construct a synthetic dataset where actions are "imperfect" interventions (change target node + side effects). Test if an IntP-based validator successfully rejects a model that assumes perfect modularity.
  3.  **Abstraction Validity Check:** Take a known valid low-level model (e.g., LDL → Heart Disease). Create a coarser representation (Total Cholesterol). Show that while a CBN can be compatible with the coarse data, it fails IntS validity when specific interventions (changing LDL ratios) are applied.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the precise formulation of a learning objective for causal representations as defined in Definition 6.3, and under what conditions can such representations be learned from data?
- **Basis in paper:** [explicit] "an important question for future research is the precise formulation of a learning objective for causal representations in the sense of Definition 6.3"
- **Why unresolved:** The paper introduces a new definition of causal representations based on interventional validity, but does not develop methods for learning such representations from observational or interventional data.
- **What evidence would resolve it:** A formal learning objective function and identifiability results showing when causal representations can be uniquely recovered from observed distributions across environments.

### Open Question 2
- **Question:** What precise properties should causal representations satisfy beyond interventional validity, and how should one choose between multiple valid representations?
- **Basis in paper:** [explicit] "What precise properties to require of a causal representation and what it may be used for, is in our opinion a neglected question... how to choose between representations and which properties to impose is a pressing question for future research"
- **Why unresolved:** The paper shows many transformations can yield causal representations (e.g., the trivial representation h(X*) = 0), suggesting additional criteria are needed.
- **What evidence would resolve it:** Formal criteria for comparing causal representations and theoretical or empirical analysis of which properties enable useful downstream tasks.

### Open Question 3
- **Question:** What constitutes an appropriate complexity measure K for actions when using the IntK interpretation, and can such measures be derived from domain knowledge rather than circularly from the causal model itself?
- **Basis in paper:** [inferred] The paper introduces IntK requiring an external complexity measure but notes this "is rife with difficulties" and that defining complexity via the model's interventions would be circular.
- **Why unresolved:** IntK enables falsifiability while avoiding the restrictiveness of IntP or IntS, but the paper provides no principled method for constructing the required complexity measure.
- **What evidence would resolve it:** Formal specification of a complexity measure grounded in physical constraints, information-theoretic principles, or domain-specific knowledge that preserves IntK validity properties.

### Open Question 4
- **Question:** Do there exist useful interpretations that violate only Desideratum D3 (interpretations should not depend on the intervention set I), and what practical advantages might they offer?
- **Basis in paper:** [explicit] "We leave it for future work to investigate if there exist interesting interpretations that may violate D3."
- **Why unresolved:** The paper constructs a proof-of-concept interpretation (InteI,f) that violates D3 but suggests it may not be practically useful.
- **What evidence would resolve it:** Discovery of an interpretation violating D3 that enables novel falsification strategies or handles scenarios where available interventions depend on modeling context.

## Limitations
- The framework assumes perfect knowledge of the representation and data-generating process, which is rarely available in practice.
- The non-circular interpretations require explicit assumptions about action mechanics that may not be empirically verifiable in all domains.
- The theoretical arguments about abstraction and representation learning require empirical validation across diverse domains.

## Confidence
- **High Confidence:** The circularity of IntC interpretation (Corollary 3.3) and the impossibility result (Proposition 4.1) - these are formal mathematical proofs with clear logical structure.
- **Medium Confidence:** The practical applicability of non-circular interpretations (IntP, IntS, IntK) - while theoretically sound, their real-world utility depends on domain-specific assumptions about action mechanisms.
- **Medium Confidence:** The implications for causal representation learning and abstraction - the theoretical arguments are sound, but empirical validation across diverse domains is needed.

## Next Checks
1. **Empirical Falsification Test:** Implement the IntP interpretation on a real-world dataset with known causal structure (e.g., health interventions with biomarkers). Verify that IntP successfully falsifies models when interventions have unintended side effects, while IntC cannot.
2. **Representation Validity Audit:** Take existing causal models from the literature and systematically evaluate whether they rely on IntC interpretation. For models that claim to represent abstractions (e.g., total cholesterol), test whether they satisfy IntS validity under realistic intervention scenarios.
3. **Causal Abstraction Stress Test:** Construct a controlled experiment where a valid low-level causal model is systematically coarsened into higher-level representations. Measure the gap between emulation (mathematical compatibility) and interventional validity across multiple abstraction levels, quantifying when and why abstraction fails.