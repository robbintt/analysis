---
ver: rpa2
title: Faster Predictive Coding Networks via Better Initialization
arxiv_id: '2601.20895'
source_url: https://arxiv.org/abs/2601.20895
tags:
- training
- initialization
- learning
- coding
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of predictive coding networks
  (PCNs) during training due to their iterative nature. The core contribution is a
  new initialization method called "stream-aligned average initialization" that preserves
  the iterative progress made on previous training samples.
---

# Faster Predictive Coding Networks via Better Initialization

## Quick Facts
- arXiv ID: 2601.20895
- Source URL: https://arxiv.org/abs/2601.20895
- Authors: Luca Pinchetti; Simon Frieder; Thomas Lukasiewicz; Tommaso Salvatori
- Reference count: 40
- One-line primary result: New initialization methods significantly accelerate predictive coding networks, achieving up to 10% better accuracy than state-of-the-art methods while requiring up to 10× fewer inference steps.

## Executive Summary
This paper addresses the inefficiency of predictive coding networks (PCNs) during training by introducing two novel initialization methods that dramatically reduce the number of inference steps required. The first method, stream-aligned average initialization, leverages class-aligned batching to average neuron values across similar samples, providing a better starting point for iterative energy minimization. The second method uses Hopfield networks to create a memory-based initialization for unsupervised learning tasks. Experimental results show these methods can achieve state-of-the-art performance while requiring significantly fewer Sequential Matrix Multiplications (SMMs) than previous approaches.

## Method Summary
The paper proposes two initialization strategies for PCNs: I_avg for supervised learning and I_mem for unsupervised learning. I_avg works by grouping training samples by class, averaging their converged hidden states from the previous batch, and using this class-specific average as the starting point for the next batch. This approach exploits the assumption that consecutive minibatches share similar optimal hidden states. I_mem uses Hopfield layers to store and retrieve patterns of observation-hidden state pairs, allowing the network to initialize new samples based on previously seen similar patterns. Both methods aim to reduce the number of inference steps T required for convergence compared to standard initialization methods.

## Key Results
- PC-Iavg outperforms I_fw by up to 10% in accuracy on AlexNet architecture
- I_avg requires up to 10× fewer SMMs than previous state-of-the-art methods
- I_mem achieves high-quality image reconstructions at just T=3 inference steps
- Stream-aligned batching is essential for I_avg to function properly
- Both methods significantly reduce training time while maintaining or improving final accuracy

## Why This Works (Mechanism)

### Mechanism 1: State Preservation as Warm Starting
Predictive coding networks normally reset neuron values every batch, forcing convergence from scratch. By preserving the hidden state from the previous batch (h^(0,b+1) = h^(T,b)), the network starts closer to the equilibrium point, reducing required inference steps. This works because consecutive minibatches typically share similar optimal hidden states under small weight updates and distribution shifts.

### Mechanism 2: Stream-Aligned Averaging (Supervised)
Instead of random batching, data is fed in class-specific streams. The initialization I_avg uses the average of converged hidden states for each class to initialize the next batch. This creates a stable signal that propagates faster than error signals in standard initialization, allowing training with fewer than L inference steps (T < L).

### Mechanism 3: Hopfield-Based State Retrieval (Unsupervised)
Since labels are unavailable for alignment in unsupervised tasks, continuous Hopfield layers store patterns of (observation, hidden state) pairs. When a new observation arrives, the Hopfield network retrieves the most similar pattern and uses its hidden state as initialization, effectively "guessing" the convergence point via pattern matching rather than iterative settling.

## Foundational Learning

- **Concept:** Predictive Coding Energy Minimization
  - Why needed here: PC minimizes local prediction error iteratively rather than using global gradients directly
  - Quick check question: Does the network update weights immediately after seeing data, or wait for T inference steps? (Answer: It waits for T steps to settle neuron values first)

- **Concept:** Sequential Matrix Multiplications (SMMs)
  - Why needed here: This is the efficiency metric used; PC is theoretically parallel but sequentially deep due to inference steps
  - Quick check question: If T=20 and Layers=5, why is PC slower than BP despite parallel layers? (Answer: The iterative loop counts as sequential steps)

- **Concept:** Forward vs. Null Initialization
  - Why needed here: Forward (I_fw) resets state and wastes energy information; Null (I_null) preserves it as starting point for improvements
  - Quick check question: Why is Forward initialization (I_fw) considered inefficient for training in this paper? (Answer: It resets the state, discarding the "memory" of where the energy minimum was on the previous batch)

## Architecture Onboarding

- **Component map:** Input -> Layers h_0...h_L -> Weights W -> Predictions μ
- **Critical path:** The inference loop (Eq. 3) is the bottleneck; new initialization methods intervene at t=0, setting h^(0) closer to h^(T)
- **Design tradeoffs:**
  - Batching: Random shuffling breaks I_avg and I_null; must use stream-aligned or sorted batching
  - Memory: I_avg requires no extra params; I_mem adds Hopfield parameters
  - Generality: I_avg is superior for classification but cannot be used for unsupervised generation; I_mem is required there
- **Failure signatures:**
  - Spiking/Unstable Loss: Occurs if T is too low (<L) with standard initialization; should disappear with I_avg
  - Memory Overfitting: If Hopfield patterns saturate or learning rate is too high in I_mem
- **First 3 experiments:**
  1. Baseline Sanity Check: Replicate Figure 2 on FashionMNIST. Compare Random vs. Zero vs. Forward init at T=5, 20, 100 to verify initialization affects convergence speed
  2. Stream Alignment Ablation: Implement I_avg on 5-layer MLP. Compare "Random Shuffle" vs. "Class-Aligned" batching. Verify I_avg fails or performs poorly when batches are shuffled randomly
  3. Inference Step Reduction: Train PC-I_avg with T < L (e.g., T=3 on 5-layer net). Check if accuracy remains stable compared to standard requirement of T ≈ 5L

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the analytical lower bound (T_min) for the number of inference steps required to train a predictive coding network?
- Basis in paper: Section 3.1 states "establishing an analytical lower bound T_min for the number of inference steps required to train a PCN is currently an open problem"
- Why unresolved: While the paper empirically demonstrates that fewer steps are possible with better initialization, a formal theoretical limit for convergence remains undefined
- What evidence would resolve it: A mathematical proof determining the minimum necessary iterations for energy convergence given specific network depth and initialization

### Open Question 2
- Question: Can the proposed initialization techniques be generalized to other energy-based learning algorithms like Equilibrium Propagation?
- Basis in paper: The conclusion identifies "generaliz[ing] our results to other energy-based learning methods, such as equilibrium propagation" as an "interesting future direction"
- Why unresolved: The methods are designed for Predictive Coding (PC) dynamics; it is unclear if the memory-based or stream-aligned approaches transfer to the distinct energy landscapes of other algorithms
- What evidence would resolve it: Experiments applying I_mem and I_avg to Equilibrium Propagation benchmarks, showing similar efficiency gains

### Open Question 3
- Question: How effectively do these initialization methods scale to complex architectures such as Graph Neural Networks (GNNs) and Transformers?
- Basis in paper: The conclusion explicitly calls for "extending this method to more complex tasks already tackled in the predictive coding literature, such as graph neural networks and transformer models"
- Why unresolved: The paper primarily tests MLPs and AlexNet; Proposition 3.2 suggests forward initialization fails on non-DAGs, raising questions about initialization in cyclic or attention-heavy topologies
- What evidence would resolve it: Benchmarking results on large-scale graph or language tasks comparing PC initialization against standard backpropagation

## Limitations

- The stream-aligned averaging method requires class-grouped batching, limiting applicability to regression tasks or scenarios where class labels are unavailable
- The Hopfield-based initialization may face capacity limitations with high-dimensional hidden states and larger datasets
- The methods assume minibatch temporal coherence that may not hold in highly shuffled or non-stationary data distributions
- Scalability to complex architectures like Transformers and GNNs remains unexplored and potentially problematic

## Confidence

- **High Confidence:** The basic mechanism of state preservation (I_null) reducing inference steps is well-supported by both theoretical arguments and empirical results across multiple datasets
- **Medium Confidence:** The stream-aligned averaging approach shows consistent improvements in controlled experiments, but its dependency on class-aligned batching introduces practical constraints
- **Low Confidence:** The Hopfield-based initialization for unsupervised learning, while promising on the specific reconstruction task tested, lacks extensive validation across different unsupervised learning scenarios and larger-scale problems

## Next Checks

1. **Generalization to Regression Tasks:** Test the initialization methods on a regression dataset (e.g., UCI regression problems) where class alignment is not applicable, to verify if the state preservation mechanisms still provide benefits without explicit class grouping

2. **Temporal Coherence Analysis:** Systematically vary the batch shuffling strategy (fully shuffled vs. temporal coherence vs. random) across multiple datasets to quantify the actual dependency on temporal correlations between consecutive minibatches

3. **Scalability Assessment:** Implement the Hopfield-based initialization on a larger-scale unsupervised learning task (e.g., CIFAR-10 reconstruction or generative modeling) to evaluate memory capacity constraints and computational overhead as hidden state dimensions increase