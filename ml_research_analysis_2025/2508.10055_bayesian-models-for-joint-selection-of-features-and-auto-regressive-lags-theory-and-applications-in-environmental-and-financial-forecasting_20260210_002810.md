---
ver: rpa2
title: 'Bayesian Models for Joint Selection of Features and Auto-Regressive Lags:
  Theory and Applications in Environmental and Financial Forecasting'
arxiv_id: '2508.10055'
source_url: https://arxiv.org/abs/2508.10055
tags:
- variables
- table
- selection
- partial
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a Bayesian framework for joint feature and auto-regressive
  lag selection in linear regression with autocorrelated errors, applicable to environmental
  and financial forecasting. The method uses hierarchical Bayesian models with spike-and-slab
  priors to simultaneously select relevant covariates and lagged error terms.
---

# Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting

## Quick Facts
- **arXiv ID**: 2508.10055
- **Source URL**: https://arxiv.org/abs/2508.10055
- **Reference count**: 40
- **Primary result**: Joint feature and auto-regressive lag selection in linear regression with autocorrelated errors using hierarchical Bayesian models with spike-and-slab priors

## Executive Summary
This paper develops a Bayesian framework for simultaneous selection of relevant covariates and lagged error terms in time series regression models with autocorrelated errors. The approach employs hierarchical Bayesian models with spike-and-slab priors to identify both important features and auto-regressive lags. A two-stage MCMC algorithm is introduced to efficiently sample from the posterior distribution by separating variable inclusion indicators from model parameters. Theoretical analysis establishes posterior selection consistency even as the number of predictors grows exponentially with sample size. The method is validated through simulations and applied to groundwater depth prediction and S&P 500 log returns modeling, demonstrating superior performance in variable selection accuracy and predictive capability compared to existing approaches.

## Method Summary
The proposed method uses hierarchical Bayesian models with spike-and-slab priors to jointly select features and auto-regressive lags. The framework models the relationship between predictors and response while accounting for autocorrelated errors through lagged terms. A key innovation is the two-stage MCMC algorithm that first samples variable inclusion indicators (which predictors and lags to include) and then samples the corresponding model parameters. This separation addresses computational challenges in high-dimensional settings. The spike-and-slab prior structure allows for exact selection by assigning positive probability to both including and excluding each variable/lag term. Theoretical analysis proves posterior selection consistency under conditions where the number of candidate predictors can grow exponentially with sample size, ensuring the method's reliability in modern high-dimensional contexts.

## Key Results
- The method achieves lower mean squared prediction error (MSPE) compared to existing approaches in both simulated and real-world datasets
- Substantially improved true model component identification, correctly detecting relevant features and auto-regressive lags
- Greater robustness to autocorrelated noise structures, maintaining performance across different correlation patterns
- Demonstrated practical utility in environmental forecasting (groundwater depth prediction) and financial modeling (S&P 500 log returns)

## Why This Works (Mechanism)
The spike-and-slab prior structure provides exact selection capability by assigning positive mass to both including and excluding each variable, enabling precise identification of relevant predictors and lags. The two-stage MCMC algorithm separates the computationally intensive variable selection from parameter estimation, improving efficiency and allowing the method to scale to higher dimensions. The hierarchical Bayesian framework naturally incorporates uncertainty in both model structure and parameters, leading to more robust inference. Theoretical consistency results ensure that as sample size increases, the posterior probability concentrates on the true model even when the number of candidate predictors grows exponentially, providing strong guarantees for high-dimensional applications.

## Foundational Learning
- **Spike-and-slab priors**: Why needed - provide exact selection by allowing both inclusion and exclusion of variables; Quick check - verify that prior includes point mass at zero for exclusion and continuous distribution for inclusion
- **Posterior selection consistency**: Why needed - ensures the method identifies true model as sample size grows; Quick check - confirm theoretical conditions (irrepresentable condition, beta-min condition) are satisfied
- **Two-stage MCMC algorithm**: Why needed - separates variable selection from parameter estimation to improve computational efficiency; Quick check - verify convergence diagnostics for both stages independently
- **Hierarchical Bayesian modeling**: Why needed - allows proper uncertainty quantification for both model structure and parameters; Quick check - examine posterior distributions of hyperparameters
- **Autocorrelated error structures**: Why needed - common in time series data, ignoring them leads to biased inference; Quick check - test residuals for remaining autocorrelation after model fitting
- **High-dimensional asymptotics**: Why needed - modern applications often have p >> n; Quick check - verify theoretical results hold when p grows exponentially with n

## Architecture Onboarding

**Component Map**: Spike-and-slab priors -> Two-stage MCMC (selection stage -> estimation stage) -> Hierarchical Bayesian framework -> Theoretical consistency analysis

**Critical Path**: Prior specification → MCMC sampling → Posterior inference → Model selection → Prediction

**Design Tradeoffs**: Exact selection capability (spike-and-slab) vs computational efficiency (two-stage algorithm) vs theoretical guarantees (consistency under exponential growth of p)

**Failure Signatures**: Poor mixing in MCMC chains indicates issues with prior specification or model misspecification; High false positive/negative rates suggest inadequate spike-and-slab tuning; Degraded performance with p >> n may indicate computational limitations of the two-stage approach

**First Experiments**: 1) Test convergence diagnostics on simulated data with known structure; 2) Compare variable selection accuracy against LASSO and SCAD methods; 3) Evaluate predictive performance across different autocorrelation patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Computational efficiency may degrade significantly when applied to very high-dimensional datasets with complex autocorrelation structures
- The assumption of linear relationships between predictors and responses may not capture nonlinear dependencies common in environmental and financial data
- Performance in non-stationary time series contexts remains unclear, as the method assumes stationarity

## Confidence
- **High confidence** in theoretical selection consistency results under stated assumptions
- **Medium confidence** in simulation results, given controlled experimental conditions
- **Medium confidence** in real-world applications, as environmental and financial systems contain unmodeled complexities
- **Low confidence** in computational scalability claims without extensive testing on larger datasets

## Next Checks
1. Test the algorithm's performance on synthetic datasets with increasing dimensionality (p >> n) to evaluate scalability limits and computational efficiency
2. Apply the method to non-stationary time series datasets to assess robustness when model assumptions are violated
3. Compare predictive performance against modern machine learning approaches (e.g., gradient boosting, deep learning) on the same real-world datasets to establish practical advantages