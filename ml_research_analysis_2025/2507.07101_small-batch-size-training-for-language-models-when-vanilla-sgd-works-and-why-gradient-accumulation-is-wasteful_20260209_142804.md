---
ver: rpa2
title: 'Small Batch Size Training for Language Models: When Vanilla SGD Works, and
  Why Gradient Accumulation Is Wasteful'
arxiv_id: '2507.07101'
source_url: https://arxiv.org/abs/2507.07101
tags:
- batch
- size
- sizes
- small
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that small batch sizes (including batch\
  \ size 1) are stable and perform on par with or better than large batch sizes for\
  \ language model training when using proper hyperparameter scaling. The authors\
  \ propose scaling the Adam optimizer's \u03B22 decay parameter such that its token\
  \ half-life remains constant across batch sizes, which eliminates the performance\
  \ gap observed in previous work."
---

# Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful

## Quick Facts
- arXiv ID: 2507.07101
- Source URL: https://arxiv.org/abs/2507.07101
- Authors: Martin Marek; Sanae Lotfi; Aditya Somasundaram; Andrew Gordon Wilson; Micah Goldblum
- Reference count: 40
- Primary result: Small batch sizes (including batch size 1) are stable and perform on par with or better than large batch sizes for language model training when using proper hyperparameter scaling, particularly scaling Adam's β₂ decay parameter to maintain constant token half-life across batch sizes.

## Executive Summary
This paper challenges the conventional wisdom that large batch sizes are necessary for stable and efficient language model training. The authors demonstrate that small batch sizes, including batch size 1, can achieve comparable or superior performance when using properly scaled hyperparameters, specifically by maintaining a constant token half-life for Adam's second moment estimate rather than a fixed decay rate. The key insight is that small batch training enables stable optimization with simpler algorithms like vanilla SGD, eliminating the need for sophisticated optimizers and gradient accumulation techniques. The paper also introduces a memory-performance optimized fine-tuning approach using Adafactor with small batch sizes, achieving results comparable to full fine-tuning while maintaining memory requirements similar to LoRA.

## Method Summary
The authors propose scaling the Adam optimizer's β₂ parameter such that its token half-life remains constant across batch sizes, using the formula β₂* = β₂^(B*/B) where B* is the reference batch size. They train decoder-only transformer language models on FineWeb-Edu and C4 datasets using batch sizes ranging from 1 to 4096, comparing SGD, Adam, AdamW, Adafactor, and Muon optimizers. The experimental setup includes GPT-2 tokenizer with 50257 vocabulary, rotary embeddings, QK-normalization, RMSNorm, GELU activations, and Chinchilla scaling (20 tokens per active parameter). Training uses linear warmup for 5% of steps followed by cosine decay (pretraining) or constant learning rate (fine-tuning).

## Key Results
- Small batch sizes with scaled β₂ parameters achieve comparable or better performance than large batch sizes across all tested batch sizes
- Vanilla SGD becomes competitive with Adam at small batch sizes, eliminating the need for sophisticated optimizers
- Adafactor combined with small batch sizes provides optimal memory-performance tradeoff for fine-tuning, matching full fine-tuning results while using memory similar to LoRA
- Gradient accumulation is wasteful for most practitioners, as small batch sizes maximize throughput without requiring accumulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling β₂ to maintain constant token half-life enables stable training at small batch sizes with Adam
- Mechanism: Adam's second moment is an exponential moving average of squared gradients. When batch size decreases, each step processes fewer tokens, so fixed β₂ causes the moment estimate to average over fewer total tokens than intended. Adjusting β₂ upward restores the intended averaging timescale: β₂* = β₂^(B*/B)
- Core assumption: The appropriate timescale for second-moment estimation should depend on tokens observed, not optimizer steps
- Evidence: Section 4.3 Figure 4 shows fixing t₂ across batch sizes provides strong performance while fixing β₂ leads to suboptimal results at small batches

### Mechanism 2
- Claim: Small batch sizes reduce the necessity of sophisticated optimizers because they require smaller learning rates and thus make shorter-range predictions about the loss surface
- Mechanism: For fixed compute budget, smaller batches take more steps with smaller learning rates. Shorter steps mean the optimizer must predict the loss landscape only near current parameters, which is easier than making long-range predictions required by large-batch training with high learning rates
- Core assumption: The difficulty of the optimization prediction problem scales with step size
- Evidence: Section 4.1 Figure 1a shows optimizer performance gaps shrink at small batch sizes; SGD becomes competitive

### Mechanism 3
- Claim: Momentum is less necessary at small batch sizes because smaller steps reduce oscillations on ill-conditioned loss surfaces
- Mechanism: Momentum dampens oscillations caused by overshooting along high-curvature directions during large steps. With small learning rates (as in small-batch training), the optimizer does not overshoot, so there are no oscillations to dampen
- Core assumption: Oscillations primarily arise from step size relative to curvature, not from gradient noise
- Evidence: Section 4.1 states that when the optimizer takes small steps, parameter updates do not overshoot the minimum across short axes

## Foundational Learning

- Concept: **Exponential Moving Average (EMA) half-life**
  - Why needed here: The paper's core contribution reformulates β₂ scaling in terms of token half-life rather than step-based decay rate. Understanding how β relates to half-life is essential to implement the scaling rule
  - Quick check question: Given β=0.9 and 100 optimizer steps per half-life, how many tokens does one half-life span if batch size is 64 and sequence length is 512?

- Concept: **Adam optimizer moments (m_t, v_t) and their role**
  - Why needed here: The paper specifically adjusts β₂ while keeping β₁ fixed. Understanding why the second moment (gradient magnitude scaling) matters more than the first moment (direction smoothing) for batch size adaptation is important
  - Quick check question: Why might the second moment estimate be more sensitive to batch size than the first moment?

- Concept: **Gradient accumulation vs. effective batch size**
  - Why needed here: The paper recommends against gradient accumulation for most practitioners. Understanding the memory/throughput tradeoff is critical for practical decisions
  - Quick check question: If gradient accumulation increases effective batch size without changing optimizer steps, why might it be "wasteful" per the paper's title?

## Architecture Onboarding

- Component map:
  1. Optimizer selection: SGD (no state), Adafactor (O(d₁+d₂) state), Adam (O(d₁×d₂) state), Muon (matrix orthogonalization)
  2. β₂ scaling module: Converts target t₂ (token half-life) to β₂ given batch size and sequence length
  3. Batch size selector: Identifies smallest batch size that maintains target throughput/MFU
  4. Precision management: Stochastic rounding for bfloat16 weights with float32 optimizer state (Adafactor)

- Critical path:
  1. Determine minimum batch size that achieves acceptable MFU on target hardware (start with ~several hundred tokens per device)
  2. Compute β₂ from target t₂ (e.g., 10M tokens) using: β₂ = 0.5^(B×T/t₂)
  3. Set β₁=0.9, tune learning rate (scales ~logarithmically, not √B, per Section 4.3)
  4. If memory-constrained, switch from Adam to Adafactor; use stochastic rounding if weights are bfloat16

- Design tradeoffs:
  - Small batch + SGD: Zero optimizer state, minimal memory, but requires tuning and may have lower MFU
  - Small batch + Adafactor: Sublinear state, strong performance-memory tradeoff for fine-tuning
  - Large batch + Adam: Higher MFU, but requires careful hyperparameter tuning and more memory
  - Gradient accumulation: Increases effective batch size without throughput gain; primarily useful for multi-device training where inter-replica bandwidth is the bottleneck

- Failure signatures:
  - Loss spikes or instability at small batch sizes with default β₂ values (fix: apply β₂ scaling)
  - Performance degradation when using bfloat16 weights with deterministic rounding (fix: stochastic rounding)
  - Poor MFU with batch sizes below hardware-optimal threshold (fix: increase batch size until compute-bound)

- First 3 experiments:
  1. Validate β₂ scaling: Train a small model (e.g., 30M params) at batch sizes 1, 64, and 1024 with t₂=10M tokens. Confirm comparable loss across batch sizes.
  2. SGD baseline: At batch size 1, compare vanilla SGD (no momentum) against Adam with scaled β₂ on a held-out validation set.
  3. Memory-performance sweep: For fine-tuning, benchmark Adam (batch=16, requires grad accumulation), Adam (batch=1, scaled β₂), Adafactor (batch=1), and LoRA on memory usage and final performance.

## Open Questions the Paper Calls Out

- **Question**: How do these findings regarding small-batch optimization interact with batch size schedules (e.g., dynamically increasing batch size during training)?
  - Basis: Section 5 explicitly asks this question
  - Why unresolved: The paper focuses on static batch sizes throughout training, whereas common practice often involves adjusting batch sizes as training progresses
  - What evidence would resolve it: Experiments applying the proposed β₂ scaling rules to training runs that utilize dynamic batch size schedules

- **Question**: Can weight precision be further reduced to 8 or even 4 bits during small-batch fine-tuning while maintaining performance?
  - Basis: Section 5 explicitly asks this question
  - Why unresolved: The paper validates bfloat16 weights with stochastic rounding but does not test extreme quantization (sub-8-bit)
  - What evidence would resolve it: Ablation studies evaluating performance of small-batch Adafactor setup when model weights are quantized to 4-bit or 8-bit precision

- **Question**: Why does fixing the second-moment half-life (t₂) in tokens generalize effectively across different model and data scales?
  - Basis: Section 5 explicitly asks this question
  - Why unresolved: The paper empirically derives and validates the scaling rule but lacks theoretical derivation explaining why token-based half-life preserves optimizer dynamics
  - What evidence would resolve it: A theoretical framework linking variance of gradient estimators to token-based half-life

- **Question**: Can novel optimizers with small state sizes be specifically designed to maximize the benefits of the small batch size regime?
  - Basis: Section 5 explicitly asks this question
  - Why unresolved: The paper tests existing optimizers but does not propose a new algorithm explicitly constructed to exploit small-batch training stability
  - What evidence would resolve it: Derivation and testing of a new optimizer algorithm that minimizes memory state while optimizing for small-batch gradient noise characteristics

## Limitations

- The optimal token half-life parameter (t₂ = 10M tokens) may be task-dependent and not universally optimal across all architectures and domains
- The paper does not investigate long-term stability over multi-year or trillion-token scales, which is relevant for foundation model development
- The recommendation to use smallest batch size that maximizes throughput may not fully account for multi-device training scenarios where gradient accumulation has different implications for inter-device communication costs

## Confidence

- **High confidence**: The empirical demonstration that small batch sizes with properly scaled β₂ parameters achieve comparable or superior performance to large batch sizes
- **Medium confidence**: The claim that small batch sizes eliminate the need for sophisticated optimizers like Adam in favor of vanilla SGD
- **Medium confidence**: The recommendation to use the smallest batch size that maximizes throughput rather than gradient accumulation

## Next Checks

1. **Cross-architecture validation**: Implement and test the β₂ scaling methodology with β₂ = 0.9999 (t₂ = 10M tokens) on a vision transformer (e.g., ViT) trained on ImageNet. Compare performance against baseline Adam (β₂ = 0.999) at batch size 1024 to validate the generality of the token-half-life approach beyond language models.

2. **Task diversity experiment**: Apply the small batch size methodology to a sequence modeling task with different loss landscape characteristics, such as time series forecasting or molecular property prediction. This would test whether the claimed robustness to hyperparameter choices extends to non-language domains.

3. **Multi-device training benchmark**: Evaluate the memory-performance tradeoff of Adafactor with small batch sizes against gradient accumulation strategies in a multi-GPU setting. Measure not just memory usage and final performance, but also communication overhead and training stability across devices to validate the "wasteful" claim in distributed contexts.