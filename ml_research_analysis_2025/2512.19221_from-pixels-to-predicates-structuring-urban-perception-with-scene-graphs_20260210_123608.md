---
ver: rpa2
title: From Pixels to Predicates Structuring urban perception with scene graphs
arxiv_id: '2512.19221'
source_url: https://arxiv.org/abs/2512.19221
tags:
- perception
- urban
- scene
- graph
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a pipeline that transforms street view imagery
  into structured scene graphs to predict urban perception. It uses an open-set Panoptic
  Scene Graph model to extract object-predicate-object triplets, learns compact embeddings
  via a heterogeneous graph autoencoder, and feeds these into a neural network to
  predict six perceptual indicators.
---

# From Pixels to Predicates Structuring urban perception with scene graphs

## Quick Facts
- arXiv ID: 2512.19221
- Source URL: https://arxiv.org/abs/2512.19221
- Reference count: 2
- One-line primary result: Graph-based urban perception prediction improves accuracy by 26% over image-only baselines while maintaining strong cross-city generalization.

## Executive Summary
This paper presents a pipeline that transforms street view imagery into structured scene graphs to predict urban perception. The approach uses OpenPSG to extract object-predicate-object triplets, learns compact embeddings via a heterogeneous graph autoencoder, and feeds these into a neural network to predict six perceptual indicators. The graph-based approach improves prediction accuracy by an average of 26% over image-only baselines and maintains strong cross-city generalization. The structured representation also reveals relational patterns linked to perception, such as graffiti-on-wall and car-parked-on-sidewalk, offering interpretability for urban analytics.

## Method Summary
The pipeline extracts scene graphs from street view imagery using OpenPSG, encodes node and edge text with Sentence-BERT, and learns 128-dimensional scene embeddings via GraphMAE. These embeddings are used for pairwise comparison learning with a Bradley-Terry formulation to predict six urban perception indicators. The method is trained on Place Pulse 2.0 data and evaluated for cross-city generalization on Tokyo and Amsterdam datasets.

## Key Results
- GraphMAE achieves 0.87 average accuracy across six perceptual dimensions vs. 0.73 for CNN, 0.71 for ViT, and 0.63 for CLIP
- Cross-city test shows only 5.6% accuracy drop and 3.5% AUC reduction when moving from Place Pulse to Tokyo-Amsterdam
- Graph-based approach improves prediction accuracy by an average of 26% over image-only baselines

## Why This Works (Mechanism)

### Mechanism 1: Relational Triplets Capture Perceptual Cues
OpenPSG parses street view imagery into scene graphs where nodes = urban entities and edges = semantic relations. Sentence-BERT encodes these into 384-dim vectors before graph construction. Human perception depends on inter-object relationships, not merely object presence. Limited evidence; PRISM-0 paper addresses open-vocabulary SGG but focuses on long-tail predicate bias, not urban perception specifically. Break condition: If OpenPSG fails to detect domain-specific urban objects, triplet extraction degrades.

### Mechanism 2: Masked Graph Autoencoding for Generalization
GraphMAE masks portions of input graph, forcing the model to learn scene-level representations that generalize across cities. The decoder is discarded after training; encoder produces scene embeddings for downstream prediction. Cross-city test shows only 5.6% accuracy drop on Tokyo-Amsterdam vs. Place Pulse training data. Weak corpus evidence; neighbor papers focus on DSGs for robotics/indoor scenes, not urban perception transfer. Break condition: If scene graphs lack sufficient relational diversity, masking provides weak supervision signal.

### Mechanism 3: Pairwise Learning for Robust Score Calibration
Weight-shared MLP processes paired scene embeddings to compute P(left > right) = σ(s_left - s_right), trained via cross-entropy on true pairwise labels. Bradley-Terry formulation captures latent perceptual scores from crowdsourced pairwise comparisons. GraphMAE achieves 0.87 average accuracy vs. 0.73 CNN, 0.71 ViT, 0.63 CLIP. No corpus evidence for Bradley-Terry in urban perception; mechanism is standard in ranking literature. Break condition: If annotation quality is poor, score calibration fails regardless of embedding quality.

## Foundational Learning

- **Scene Graphs**
  - Why needed here: Core representation replacing pixel features. Without understanding nodes/edges/triplets, you cannot debug triplet extraction or interpret why certain relations predict perception scores.
  - Quick check question: Given an image of "bicycle locked to fence," what triplets should OpenPSG extract?

- **Masked Autoencoding for Graphs**
  - Why needed here: GraphMAE is the embedding engine. Understanding masking/reconstruction clarifies why embeddings generalize and what happens when graphs are sparse.
  - Quick check question: If a scene graph has 3 nodes and 2 edges, what portion should GraphMAE mask to maintain reconstruction signal?

- **Pairwise Learning (Bradley-Terry)**
  - Why needed here: Training objective differs from classification/regression. Understanding σ(s_i - s_j) clarifies why the model outputs relative scores, not absolute values.
  - Quick check question: How does the model convert pairwise preferences into a single "safety" score for a new image?

## Architecture Onboarding

- **Component map**: Image → OpenPSG triplets → Sentence-BERT embeddings → GraphMAE encoder → 128-dim embedding → MLP → score
- **Critical path**: The GraphMAE encoder is the only component trained end-to-end with perception labels
- **Design tradeoffs**:
  - OpenPSG vs. closed-vocabulary SGG: OpenPSG handles region-specific urban elements but may hallucinate relations on unseen street furniture
  - 128-dim vs. higher embeddings: Paper uses 128-dim; lower dims risk losing relational nuance, higher dims may overfit to training cities
  - Frozen vs. fine-tuned Sentence-BERT: Paper freezes; fine-tuning could improve urban-specific semantics but requires labeled data
- **Failure signatures**:
  - Low accuracy on "wealth" dimension specifically → check if OpenPSG misses material/condition cues
  - Cross-city performance collapse → scene graphs may lack relational diversity
  - Repetitive triplets in high-scoring scenes → model may conflate graph density with positive perception
- **First 3 experiments**:
  1. Manually inspect 50 random images; verify OpenPSG triplets match ground-truth relations
  2. Compare top-20 most frequent triplets in Place Pulse vs. Tokyo-Amsterdam
  3. Train GraphMAE with 64/128/256-dim embeddings; plot accuracy vs. dimension

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does domain-specific fine-tuning of the scene parser significantly enhance the capture of nuanced relational patterns compared to the generic pre-trained OpenPSG model?
- **Basis in paper:** The Conclusion states that "OpenPSG-based scene parsing could be further improved through domain-specific fine-tuning to better capture relational pattern nuances."
- **Why unresolved:** The current implementation utilizes generic pre-trained weights without adaptation to specific urban contexts.
- **What evidence would resolve it:** Comparative analysis of triplet extraction accuracy and downstream perception scores between generic and fine-tuned models.

### Open Question 2
- **Question:** Does the observed cross-city generalization remain robust when evaluated on a statistically representative global sample?
- **Basis in paper:** The authors note that "the cross-city evaluation was based on a limited subset, and expanding it with more representative samples would improve statistical reliability."
- **Why unresolved:** Current generalization test is limited to Tokyo and Amsterdam, which may not represent full diversity of global urban forms.
- **What evidence would resolve it:** Evaluation across broader dataset including cities with distinct morphologies and cultural contexts.

### Open Question 3
- **Question:** Can the sequential pipeline be optimized into an end-to-end architecture without sacrificing the interpretability of the intermediate scene graphs?
- **Basis in paper:** The methodology employs a three-stage pipeline with frozen backbones for feature extraction.
- **Why unresolved:** It is unclear if "frozen" extraction introduces information bottlenecks that could be resolved by joint optimization.
- **What evidence would resolve it:** Comparison of prediction accuracy and gradient flow between current disconnected approach and jointly trained model.

## Limitations
- OpenPSG performance on domain-specific urban elements remains unquantified
- GraphMAE training details (masking ratio, encoder depth, learning rate) are unspecified
- The claim that 128-dim embeddings are optimal is not validated

## Confidence
- High: Graph-based approach outperforms image-only baselines by ~26% average accuracy
- Medium: Cross-city generalization holds with only 5.6% accuracy drop; triplet patterns correlate with perception
- Low: Bradley-Terry formulation is standard in ranking literature but unvalidated for urban perception specifically

## Next Checks
1. Inspect 50 random OpenPSG outputs; measure triplet precision/recall against ground-truth urban relations
2. Analyze top-20 most frequent triplets in Place Pulse vs. Tokyo/Amsterdam; identify missing relations in cross-city test
3. Train GraphMAE with 64/128/256-dim embeddings; plot accuracy vs. dimension across all six perceptual dimensions