---
ver: rpa2
title: 'My Answer Is NOT ''Fair'': Mitigating Social Bias in Vision-Language Models
  via Fair and Biased Residuals'
arxiv_id: '2505.23798'
source_url: https://arxiv.org/abs/2505.23798
tags:
- bias
- ours
- social
- fair
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses social bias in large vision-language models
  (VLMs), focusing on their generative responses and confidence levels. It introduces
  a new evaluation method using multiple-choice selection tasks on the PAIRS and SocialCounterfactuals
  datasets, with carefully designed prompts to elicit models' opinions on gender and
  race.
---

# My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals

## Quick Facts
- arXiv ID: 2505.23798
- Source URL: https://arxiv.org/abs/2505.23798
- Authors: Jian Lan; Yifei Fu; Udo Schlegel; Gengyuan Zhang; Tanveer Hannan; Haokun Chen; Thomas Seidl
- Reference count: 40
- One-line primary result: Post-hoc orthogonal projection of fair/biased residuals in last 10 layers improves fairness scores (e.g., 0.68→0.84 on gender) and confidence calibration while maintaining VQA performance.

## Executive Summary
This paper addresses social bias in vision-language models by introducing a novel post-hoc debiasing method that manipulates residuals during inference. The authors develop a multiple-choice selection task to evaluate bias in both responses and confidence levels, revealing that state-of-the-art VLMs exhibit miscalibrated confidence toward specific social groups. Their method extracts fair and biased residual vectors from hidden layers and uses orthogonal projection matrices to amplify fairness-associated residuals while ablating bias-associated ones, achieving significant improvements in fairness without requiring model retraining.

## Method Summary
The approach involves extracting residuals (Δ₁ for self-attention, Δ₂ for FFN) from the last 10 layers of a VLM. For each residual, the method classifies it as fair or biased by passing it through the final layer and observing the output. Mean fair and biased vectors are computed per layer, then orthogonal projection matrices are constructed. During inference, residuals are modified via projection: first removing the bias component (v_debias = v - λ₁·P_biased·v), then amplifying the fair component (v_new = λ₂·P_fair·v_debias). The method uses λ₁=λ₂=0.2 and is training-free and model-agnostic.

## Key Results
- Fairness scores increase significantly (e.g., from 0.68 to 0.84 on LLaVA-NeXT-13B for gender classification)
- KL divergence decreases substantially (e.g., from 0.209 to 0.096), indicating better confidence calibration
- Method outperforms training-based approaches (DPO+KL) on stronger models while maintaining VQA accuracy
- Layer-wise analysis reveals fairness fluctuations, with some intermediate layers showing higher fairness than the final layer

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Post-hoc orthogonal projection of layer residuals can amplify fairness-associated signals while ablating bias-associated signals during inference.
- **Mechanism:** For each layer in the last N layers, the method extracts two residual types (Δ₁, Δ₂), classifies them as fair or biased based on their effect on output, computes mean fair/biased vectors (v_fair, v_biased), then constructs orthogonal projection matrices. The debiased output is: v_debias = v_i - P_biased·v_i, followed by amplification via P_fair with weighting parameters λ₁ and λ₂.
- **Core assumption:** Fairness and bias components in residual space are approximately linearly separable and can be manipulated via orthogonal projection without destroying model capabilities.

### Mechanism 2
- **Claim:** Fairness levels fluctuate across hidden layers rather than monotonically improving, with intermediate layers often fairer than the final layer.
- **Mechanism:** The authors probe each layer's output directly by passing it through the final normalization and LM head. They observe that the last 10 layers show substantial fluctuations, with a "sudden drop" in fairness at the final layer.
- **Core assumption:** Fairness-relevant representations are not strictly built up hierarchically; later layers may inject bias or overfit to biased training correlations.

### Mechanism 3
- **Claim:** Asking models directly about social categories (rather than classifying individual images) reveals latent biases in both responses and confidence distributions.
- **Mechanism:** The MCS task uses prompts like "In our society, which gender are more likely to do a better job as [occupation]?" with candidate options (Male, Female, Both...). Models are instructed to treat images as reference, not strong evidence, exposing background knowledge biases.
- **Core assumption:** Social bias is "background knowledge" stored in models; single-image classification tasks underestimate bias because responses are constrained to the specific sample.

## Foundational Learning

- **Concept:** Residual connections in transformers
  - **Why needed here:** The method manipulates Δ₁ (self-attention residual) and Δ₂ (FFN residual) independently. Understanding how residuals accumulate information across layers is essential.
  - **Quick check question:** Given a transformer layer with two residual connections, which residual carries more semantic vs. positional information?

- **Concept:** Orthogonal projection matrices
  - **Why needed here:** The debiasing operation projects hidden states onto the orthogonal complement of the bias direction (v_biased) and optionally onto the fair direction (v_fair).
  - **Quick check question:** If P = v(v^T v)^{-1}v^T is a projection onto v, what is the projection that removes the component along v?

- **Concept:** Confidence calibration and KL divergence
  - **Why needed here:** The paper evaluates not just response fairness but confidence calibration (KL divergence from uniform 0.5/0.5 target). Miscalibrated confidence toward specific groups is a key failure mode.
  - **Quick check question:** A model outputs P(male)=0.7, P(female)=0.3 for a fairness question. What is the KL divergence from the ideal fair distribution?

## Architecture Onboarding

- **Component map:** Input pipeline (MCS prompt + image reference) -> Residual extraction module (hooks into layers -10 to -1) -> Classification module (passes residuals through final layer) -> Vector aggregation (computes mean fair/biased vectors) -> Projection matrices (constructs P_fair and P_biased) -> Inference modifier (applies v_debias and fair amplification)

- **Critical path:** 1) Run forward pass on calibration dataset to collect residuals 2) Classify residuals → compute v_fair, v_biased 3) At inference: for each target layer, apply projection before passing to next layer

- **Design tradeoffs:**
  - **Post-hoc vs. training:** Post-hoc is training-free but requires calibration data; DPO+KL training may outperform on weaker models but requires fine-tuning infrastructure
  - **Layer selection:** Starting from layer with highest fairness (variable per model) vs. fixed last-N layers
  - **Weight tuning (λ₁, λ₂):** Higher values improve fairness but may degrade fluency; 0.2 works empirically

- **Failure signatures:**
  - **Fluency collapse:** If λ₁ or λ₂ too small, model outputs degenerate ("Fair fair." or "genders are no not non-equal")
  - **Generalization gap:** Vectors extracted from one dataset may not transfer if bias patterns differ
  - **VQA degradation:** Post-hoc method shows minor VQA accuracy drop (0.1-0.3%)

- **First 3 experiments:**
  1. Reproduce layer-wise fairness curve: Run fairness probe on layers -10 to -1 for PAIRS gender; verify fluctuation pattern and identify highest-fairness layer
  2. Ablation on λ values: Test λ₁ ∈ {0.1, 0.2, 0.3} and λ₂ ∈ {0.1, 0.2, 0.3}; measure fairness score and fluency
  3. Cross-dataset transfer: Extract residual vectors from SCF, apply to PAIRS; compare fairness improvement vs. in-domain extraction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the post-hoc residual projection method generalize to social categories beyond binary gender (male/female) and binary race (black/white), such as non-binary gender identities or additional racial groups (e.g., Asian, Hispanic)?
- **Basis in paper:** The authors state they did not expand gender and race to a wider range mainly because the PAIRS dataset does not provide such labels.
- **Why unresolved:** The datasets used only contain binary gender and limited race categories, so the method's effectiveness on broader social categories remains untested.
- **What evidence would resolve it:** Experiments on datasets containing diverse gender identities and multiple racial/ethnic groups, applying the same residual extraction and projection methodology.

### Open Question 2
- Question: What are the conditions under which the training-based DPO+KL strategy outperforms the post-hoc method, and can these insights improve the post-hoc approach?
- **Basis in paper:** The paper notes that for weaker models (LLaVA-1.5-13B, Qwen2-VL-7B), the post-hoc method still helps reach the highest fairness scores consistently, but does not outperform the proposed DPO+KL training strategy, suggesting model capability influences method effectiveness.
- **Why unresolved:** The paper speculates that stronger models can "teach itself" calibration through generation direction control, but does not systematically investigate what model properties determine which method works better.
- **What evidence would resolve it:** Controlled experiments varying model size, architecture, and pre-training data while comparing post-hoc vs. training methods, with analysis of intermediate representations.

### Open Question 3
- **Question:** Does manipulating residuals at the last 10 layers sufficiently capture bias-related information, or would extending the intervention to earlier layers improve debiasing effectiveness?
- **Basis in paper:** The choice of last 10 layers is pragmatic but not theoretically motivated; fairness fluctuations may occur at earlier layers that remain unexplored.
- **Why unresolved:** The choice of last 10 layers is pragmatic but not theoretically motivated; fairness fluctuations may occur at earlier layers that remain unexplored.
- **What evidence would resolve it:** Systematic evaluation of debiasing performance when applying the method across different layer ranges (e.g., layers 1-10, 11-20, etc.) with computational cost analysis.

## Limitations

- The method requires calibration data to extract fair/biased residual vectors, adding preprocessing overhead
- Performance may degrade when applied to social categories beyond binary gender and race due to dataset limitations
- Layer selection (last 10 layers) is pragmatic but not theoretically optimal for all models

## Confidence

- **High confidence**: The post-hoc projection method works as described (measured improvements in fairness scores and KL divergence are statistically significant across multiple models and datasets)
- **Medium confidence**: The layer-wise fairness fluctuation finding is reproducible on PAIRS but generalizability to other domains/tasks is unknown
- **Low confidence**: The orthogonal projection in residual space is the optimal debiasing approach; alternative methods (e.g., contrastive learning on residuals) might perform better

## Next Checks

1. **Cross-dataset robustness**: Apply residual vectors extracted from SCF to PAIRS and vice versa; measure degradation in fairness improvement to assess domain transfer limits
2. **Alternative debiasing methods**: Compare orthogonal projection against a contrastive loss trained on residuals (similar to TriCon-Fair [11]) to evaluate if the post-hoc approach is truly optimal
3. **Layer selection sensitivity**: Systematically vary the starting layer l_h (highest fairness layer) across models and evaluate whether fixed "last 10 layers" is suboptimal for some architectures