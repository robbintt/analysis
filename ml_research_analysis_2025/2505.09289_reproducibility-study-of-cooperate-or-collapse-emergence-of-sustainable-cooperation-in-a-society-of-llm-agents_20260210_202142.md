---
ver: rpa2
title: 'Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable
  Cooperation in a Society of LLM Agents"'
arxiv_id: '2505.09289'
source_url: https://arxiv.org/abs/2505.09289
tags:
- arxiv
- scenario
- agents
- behavior
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reproduced and extended the GovSim framework, which
  evaluates LLM cooperation in resource-sharing scenarios. By replicating experiments,
  we confirmed that only the largest models (GPT-4-turbo and GPT-4o) achieved sustainable
  cooperation without the universalization principle, while smaller models failed.
---

# Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"

## Quick Facts
- **arXiv ID**: 2505.09289
- **Source URL**: https://arxiv.org/abs/2505.09289
- **Reference count**: 40
- **Primary result**: Only largest models (GPT-4-turbo, GPT-4o) achieved sustainable cooperation without universalization; smaller models required universalization prompts to cooperate sustainably

## Executive Summary
This reproducibility study confirms the original findings that model size critically determines cooperation sustainability in resource-sharing scenarios. Only the largest models (GPT-4-turbo and GPT-4o) achieved sustainable cooperation without intervention, while smaller models failed without the universalization principle. The universalization principle significantly improved cooperation among smaller models, and the introduction of DeepSeek-V3 demonstrated comparable performance to GPT-4-turbo. Testing revealed no significant behavioral differences between English and Japanese instructions, and the inverse "trash" scenario showed agents cooperated better when removing harmful resources, suggesting loss aversion effects. The study also demonstrated that high-performing models can influence lower-performing ones toward sustainable behavior in heterogeneous multi-agent systems.

## Method Summary
The study replicated the GovSim framework using OpenAI's Chat Completions API to evaluate LLM cooperation in resource-sharing scenarios. Experiments involved 5 agents per simulation, each receiving identical scenario descriptions and acting independently through harvesting and discussion phases. The study tested multiple models including GPT-4-turbo, GPT-4o, GPT-4o-mini, Llama-2 variants, and DeepSeek-V3, with and without the universalization principle prompt. Additional experiments tested Japanese instructions versus English and the inverse "trash" scenario where agents cooperated by removing harmful resources rather than harvesting beneficial ones. Heterogeneous agent configurations were tested to examine influence dynamics between high and low-performing models.

## Key Results
- Universalization principle improved cooperation rates among smaller models that otherwise collapsed
- DeepSeek-V3 achieved comparable cooperation performance to GPT-4-turbo despite lower active parameters
- Japanese instructions showed no significant behavioral differences compared to English
- Agents cooperated better in "trash" scenario (removing harmful resources) than in default resource harvesting
- High-performing models influenced lower-performing ones toward sustainable behavior in heterogeneous systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The universalization principle improves cooperation rates in smaller models that otherwise collapse
- Mechanism: Prompting agents with "Given the current situation, if everyone takes more than f(t), the shared resources will decrease next month" triggers consideration of collective consequences, enabling long-term reasoning that smaller models fail to develop autonomously
- Core assumption: The prompt activates dormant reasoning capabilities rather than introducing new ones; models must have sufficient baseline comprehension of conditional statements
- Evidence anchors:
  - [abstract] "The universalization principle significantly improved cooperation among smaller models"
  - [section 4.1] "GPT-4o-mini fails to pass the test in the default scenario, surviving only 1 month. However, the universalization principle can improve its performance, making it maximize the survival time to 12 months"
  - [corpus] Weak direct corpus support for universalization mechanism specifically; related work focuses on reputation systems and repeated interactions as cooperation enablers
- Break condition: Very small models (<7B parameters in tested set) showed minimal improvement even with universalization; Llama-2-7B and Llama-2-13B still collapsed with 1-month survival

### Mechanism 2
- Claim: High-performing models influence lower-performing models toward cooperative behavior through natural language communication
- Mechanism: Larger models (DeepSeek-V3, GPT-4-turbo) propose concrete sustainable limits in discussion phase; smaller models verbally commit and adjust behavior accordingly, creating norm convergence
- Core assumption: Influence flows asymmetrically from high-to-low performers due to reasoning quality rather than rhetorical style; requires shared language understanding
- Evidence anchors:
  - [abstract] "In heterogeneous multi-agent systems, high-performing models influenced lower-performing ones toward sustainable behavior"
  - [Figure 1] DeepSeek-V3 agent says "Maybe we should agree on a sustainable limit for everyone, like 10 tons each"; GPT-4o-mini responds "I'm willing to stick to 10 tons next month if everyone else does the same"
  - [corpus] "Reputation as a Solution to Cooperation Collapse in LLM-based MASs" suggests reputation mechanisms complement communication; "Super-additive Cooperation" explores inter-group rivalry effects
- Break condition: Influence insufficient when low-performing agents form majority (4:1 ratio of GPT-4o-mini to DeepSeek-V3 still collapsed); first-month overconsumption can deplete resources before communication occurs

### Mechanism 3
- Claim: Framing resources as harmful ("public bad") rather than beneficial ("public good") produces higher cooperation rates through loss aversion
- Mechanism: Mathematically equivalent scenarios produce different behaviors; agents cooperate more readily when removing toxic resources than when harvesting beneficial ones, possibly triggering risk-averse responses
- Core assumption: Training data contains stronger cultural patterns for sharing undesirable tasks (household chores) than for restraining consumption of desirable resources
- Evidence anchors:
  - [abstract] "The inverse 'trash' scenario revealed agents cooperated better when removing harmful resources, suggesting loss aversion effects"
  - [section 4.2] "A striking contrast is that, while most models failed the sustainability test in the default setting, nearly all succeeded in the trash scenario"
  - [corpus] Weak corpus support for framing effects specifically in LLM cooperation; mechanism extrapolated from behavioral economics literature cited in paper (Schmidt & Zank, 2005)
- Break condition: Behavior more erratic in trash scenario with "sudden reductions" in cooperation, suggesting mechanism produces stability at cost of predictability

## Foundational Learning

- Concept: **Tragedy of the Commons dynamics**
  - Why needed here: GovSim implements Hardin's framework where individual rationality (maximize extraction) conflicts with collective sustainability; without this, the universalization principle's purpose is unclear
  - Quick check question: If a resource grows by 2x each month (capped at 100) and 5 agents each extract 20 units from 100, will the system survive to month 3?

- Concept: **Sustainability threshold f(t)**
  - Why needed here: The universalization prompt references this threshold explicitly; understanding that f(t) represents maximum sustainable extraction is prerequisite for interpreting results
  - Quick check question: With 50 units remaining and 2x growth rate, what is the maximum total extraction that allows full resource recovery next month?

- Concept: **Mixture-of-Experts (MoE) architecture**
  - Why needed here: DeepSeek-V3 uses MoE (671B total parameters, 37B active) which explains its efficiency-performance profile compared to dense models like GPT-4-turbo
  - Quick check question: Why might an MoE model achieve similar cooperation performance to dense models while using fewer active parameters?

## Architecture Onboarding

- Component map:
  - **Environment**: Manages h(t) (resource amount), f(t) (sustainability threshold), growth rate (2x), collapse threshold (C=5)
  - **Agent instances**: 5 LLM instances per simulation; each receives identical scenario description and acts independently
  - **Harvesting phase**: Simultaneous private action submission → execution → public revelation
  - **Discussion phase**: Free-form natural language exchange (max 10 steps)
  - **Manager**: Announces monthly harvest results to all agents (configurable strategy)

- Critical path:
  1. Initialize: 100 resource units, 5 agents, T=12 months
  2. Month loop: Harvest (concurrent) → Announce results → Discussion → Resource regeneration
  3. Collapse check: h(t) < 5 terminates simulation
  4. Metrics: Survival rate/time, efficiency (gain/max_possible), over-usage (excess extraction)

- Design tradeoffs:
  - Concurrent vs sequential harvesting: Concurrent prevents first-mover advantage but eliminates reactive strategies
  - Homogeneous vs heterogeneous agents: Heterogeneous enables influence studies but introduces confounds from model-specific behaviors
  - Universalization prompt simplicity vs nuance: Current prompt is minimal; richer prompts might overwhelm smaller models

- Failure signatures:
  - Immediate collapse (1-2 months): First harvest >70% of available resource; indicates no cooperation reasoning
  - Unstable cooperation (erratic extraction): Agents alternate between sustainable and overconsumption; suggests weak norm internalization
  - Discussion without action change: Verbal commitment ("I'll stick to 10 tons") followed by overconsumption; communication decoupled from decision-making

- First 3 experiments:
  1. **Baseline reproduction**: Run GPT-4o-mini in default fishery (expect 1-month survival), then with universalization (expect 12-month survival) to validate your setup matches the paper
  2. **Influence ratio test**: Test 3:2 vs 2:3 ratio of DeepSeek-V3 to GPT-4o-mini to find minimum threshold for influence success (paper shows 4:1 works, 1:4 fails)
  3. **Framing validation**: Run identical model (e.g., Llama-3-8B) in fishery vs trash scenarios; if survival time increases in trash, confirms loss aversion mechanism is reproducible

## Open Questions the Paper Calls Out
None

## Limitations
- Universalization principle effectiveness appears constrained by model capacity thresholds, with models below 7B parameters showing minimal improvement
- Influence mechanism reliability depends heavily on agent ratios, with minority high-performing agents unable to sustain cooperation when vastly outnumbered
- Loss aversion framing effect introduces behavioral unpredictability that may complicate real-world applications

## Confidence
- **High Confidence**: Universalization principle effectiveness for medium-sized models (7B-70B parameters); DeepSeek-V3 performance matching GPT-4-turbo; baseline cooperation failure in small models
- **Medium Confidence**: Influence mechanism thresholds and ratio dependencies; loss aversion effects across scenarios; Japanese vs English instruction equivalence
- **Low Confidence**: Exact parameter thresholds for universalization success; long-term stability of influenced cooperation; generalizability to non-English languages

## Next Checks
1. **Parameter Threshold Mapping**: Systematically test universalization effectiveness across discrete parameter sizes (1B, 3B, 7B, 13B, 33B, 70B, 175B) to identify precise capacity thresholds where prompts become effective

2. **Minority Influence Stress Test**: Evaluate influence mechanism robustness by testing extreme ratios (1:9, 2:8, 3:7, 4:6, 5:5) of high-to-low performing agents across multiple model combinations to determine minimum viable ratios for sustainable cooperation

3. **Cross-Lingual Generalization**: Replicate core experiments (universalization, influence, loss aversion) using non-English instructions (e.g., Mandarin, Spanish, Arabic) to validate whether observed cooperation patterns transfer across linguistic and cultural boundaries