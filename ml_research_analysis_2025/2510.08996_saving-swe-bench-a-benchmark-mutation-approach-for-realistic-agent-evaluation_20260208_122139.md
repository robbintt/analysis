---
ver: rpa2
title: 'Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation'
arxiv_id: '2510.08996'
source_url: https://arxiv.org/abs/2510.08996
tags:
- agent
- swe-bench
- benchmark
- user
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the mismatch between how developers interact
  with chat-based coding agents and how current software engineering benchmarks, like
  SWE-Bench Verified, are structured. The authors analyze real-world developer communication
  patterns using telemetry data and identify distinct templates for bug reporting.
---

# Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation

## Quick Facts
- **arXiv ID**: 2510.08996
- **Source URL**: https://arxiv.org/abs/2510.08996
- **Reference count**: 34
- **Primary result**: Existing benchmarks overestimate agent capabilities by 20-50% for public benchmarks and 10-16% for internal benchmarks

## Executive Summary
This paper addresses the fundamental mismatch between how developers interact with chat-based coding agents and how current software engineering benchmarks like SWE-Bench Verified are structured. Through analysis of real-world developer communication patterns using telemetry data, the authors identify distinct templates for bug reporting and introduce a benchmark mutation methodology that transforms formal GitHub issue descriptions into realistic user queries matching these patterns. When evaluating the OpenHands agent on mutated versions of SWE-Bench Verified and other benchmarks, they find that existing evaluations significantly overestimate agent capabilities, establishing a new paradigm for evaluating interactive chat-based software engineering agents.

## Method Summary
The methodology transforms formal GitHub issue descriptions from bug-fixing benchmarks into realistic, informal user queries by extracting communication templates from real developer telemetry data. The process involves: (1) collecting 10,000 user queries from ~6k users to identify 11 bug-reporting patterns; (2) categorizing queries into 10 task types using LLM-based labeling with human validation; (3) mutating benchmark problems using LLM with problem description, code patch, and extracted templates; (4) evaluating the OpenHands agent on both baseline and mutated versions using multiple models (GPT-4.1, Claude Sonnet 3.7, Claude Sonnet 4) in isolated Docker containers with 100-step limits. The mutation preserves technical validity while changing the communication style from formal GitHub issues to realistic chat interactions.

## Key Results
- Existing benchmarks overestimate agent capabilities by >50% over baseline performance for public benchmarks
- Internal benchmarks show smaller performance gaps (~10-16%) compared to public benchmarks (20-50%)
- Agent reasoning burden increases with information withholding, showing +11.6% to +13.6% more steps but -9.4% to -16.2% fewer tokens
- SWE-Bench C# shows smaller degradation (2-5%) compared to SWE-Bench Verified (13-16%)

## Why This Works (Mechanism)

### Mechanism 1: Query Distribution Shift from Formal to Informal
- Claim: Agents trained/evaluated on formal GitHub issue descriptions overperform when tested on realistic user queries
- Mechanism: Formal benchmarks provide 100+ word problem statements with reproduction code, test cases, and environment details. Real user queries average 10-30 words and primarily contain error stacks and file paths. This information asymmetry forces agents to work harder to localize and diagnose issues.
- Core assumption: The templates extracted from ~6k users generalize to broader developer populations
- Evidence anchors:
  - [abstract] "existing benchmarks significantly overestimate agent capabilities for some models by >50% over baseline performance for public benchmarks"
  - [Section 3.1.2] Figure 5 shows word count distributions; Figure 6 shows that user queries rarely contain reproduction code (present in ~80% of GitHub issues but <20% of user queries)
  - [corpus] Weak direct support; neighbor papers address contamination but not query style mismatch
- Break condition: If agents are specifically trained on short, informal queries, the gap would narrow significantly

### Mechanism 2: Public Benchmark Contamination and Overfitting
- Claim: Public benchmarks conflate genuine problem-solving with memorization; private benchmarks reveal smaller performance gaps
- Mechanism: SWE-Bench Verified's public availability allows models to implicitly memorize problem-solution patterns. When tested on mutated variants (same technical content, different phrasing), agents can't rely on pattern matching and must reason from first principles.
- Core assumption: The mutation preserves problem difficulty while removing memorization cues
- Evidence anchors:
  - [abstract] "overestimate agent capabilities... ~10-16% for our internal benchmark" (vs. 20-50% for public)
  - [Section 5.1.2] "smaller absolute performance drops (2-5% versus 13-16% in SWE-Bench Verified)"
  - [corpus] "Does SWE-Bench-Verified Test Agent Ability or Model Memory?" directly examines this contamination hypothesis
- Break condition: If mutated queries inadvertently change problem difficulty (not just phrasing), the gap could reflect harder problems, not overfitting

### Mechanism 3: Increased Reasoning Burden from Information Withholding
- Claim: Withholding contextual information increases agent reasoning steps but can paradoxically decrease token usage
- Mechanism: Agents compensate for missing details by taking more exploratory steps (searching code, running diagnostics). However, each step becomes less verbose because the agent has less context to reference in its reasoning traces.
- Core assumption: Step count reflects genuine reasoning effort, not agent implementation quirks
- Evidence anchors:
  - [Section 5.2.1] "agent has to deliberate for longer or search for relevant code... however, there is a drop in the token usage... because the agent has fewer details to work with — each step in the trajectory becomes less verbose"
  - [Table 1a] SWE-Bench Verified: steps +11.6% to +13.6%, but tokens -9.4% to -16.2%
  - [corpus] No direct corpus support for this step/token inverse relationship
- Break condition: If the evaluation harness grants partial credit for verbose incorrect attempts, token reduction could mask performance degradation

## Foundational Learning

- Concept: **Benchmark Contamination in LLM Evaluation**
  - Why needed here: The paper's central claim hinges on distinguishing genuine capability from memorization; without understanding contamination, you can't interpret the 20-50% performance gap
  - Quick check question: If a model achieves 90% on a benchmark but only 50% on semantically equivalent reformulations, what does this suggest about the original score?

- Concept: **Distribution Shift in Evaluation Design**
  - Why needed here: The mutation methodology works by shifting the query distribution from GitHub-issue-style to chat-style; understanding this shift is essential for designing mutation templates that remain valid
  - Quick check question: What features of a GitHub issue (file paths, stack traces, reproduction code) are most critical to preserve during mutation?

- Concept: **Telemetry-Driven Template Extraction**
  - Why needed here: The 11 communication templates (e.g., "Paste Error/Stack Trace Only") are derived from real user behavior; these templates determine what mutations are valid
  - Quick check question: If telemetry from one IDE agent differs significantly from another (e.g., different UI affordances), would the same templates generalize?

## Architecture Onboarding

- Component map:
  1. **Telemetry Collector** -> samples 10k queries from ~6k users
  2. **Categorization Module** -> LLM-based labeling into 10 task types
  3. **Template Extractor** -> LLM identifies 11 bug-reporting patterns
  4. **Mutation Engine** -> transforms benchmark problems using templates + patch context
  5. **Evaluation Harness** -> runs OpenHands agent in Docker containers with 100-step limit

- Critical path:
  1. Telemetry must be representative of target user population
  2. Template extraction must capture diverse communication styles
  3. Mutation must preserve technical validity (assumed, not verified)
  4. Evaluation harness must support multi-language (Python, C#, TypeScript)

- Design tradeoffs:
  - **Realism vs. Validity**: More aggressive mutations (e.g., removing all file paths) increase realism but risk creating unsolvable problems
  - **LLM-based vs. Human Validation**: LLM extraction scales but may introduce systematic biases; human validation catches errors but doesn't scale
  - **Single-turn vs. Multi-turn Evaluation**: Current design tests single queries; real chat involves clarifying questions (acknowledged limitation in Section 6)

- Failure signatures:
  1. **Mutation invalidates problem**: Agent produces correct solution that differs from ground-truth patch → false negative
  2. **Template over-representation**: If "Paste Error Only" dominates, mutations may lose too much context
  3. **Language-specific gaps**: C# results show smaller degradation → could indicate either less overfitting or lower baseline capability

- First 3 experiments:
  1. **Validate mutation difficulty preservation**: Run OpenHands on original vs. mutated problems with ground-truth patches hidden; verify that solvable problems remain solvable by expert humans
  2. **Cross-agent generalization**: Test Claude Code and VSCode Agent on mutated benchmarks to determine if the gap is agent-specific or universal
  3. **Multi-turn interaction simulation**: Allow agents to ask clarifying questions after the initial mutated query; measure how much performance recovers with realistic follow-up exchanges

## Open Questions the Paper Calls Out
None

## Limitations
- The mutation methodology lacks human validation for technical correctness, creating potential false negatives when mutated problems become unsolvable
- The 11 communication templates derived from telemetry data from ~6k users may not generalize to broader developer populations across different programming languages and environments
- Current evaluation design tests single-turn interactions, not reflecting the multi-turn clarifying exchanges that occur in real chat-based agent usage

## Confidence
- **High Confidence**: Distribution shift evidence showing real user queries are significantly shorter and less detailed than GitHub issues is well-supported by telemetry data (Figures 5-6)
- **Medium Confidence**: Performance degradation metrics (20-50% for public, 10-16% for internal) are measured but the exact contribution of memorization versus problem difficulty changes is uncertain
- **Low Confidence**: The inverse relationship between step count and token usage (more steps, fewer tokens) lacks theoretical grounding and direct empirical support

## Next Checks
1. **Human Validation of Mutation Quality**: Have expert developers solve both original and mutated problems without access to ground-truth patches to verify that the mutation preserves problem difficulty and doesn't create unsolvable instances
2. **Cross-Agent Generalization Study**: Evaluate multiple chat-based coding agents (Claude Code, GitHub Copilot Chat, etc.) on the same mutated benchmarks to determine whether the 20-50% performance gap is agent-specific or represents a universal benchmark calibration issue
3. **Multi-turn Interaction Evaluation**: Extend the evaluation harness to support clarifying question exchanges after the initial mutated query, measuring how much performance recovers with realistic multi-turn interactions that better reflect actual usage patterns