---
ver: rpa2
title: A Convex formulation for linear discriminant analysis
arxiv_id: '2503.13623'
source_url: https://arxiv.org/abs/2503.13623
tags:
- data
- class
- convexlda
- dimension
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ConvexLDA, a convex formulation of linear discriminant
  analysis (LDA) that addresses key limitations of Fisher's LDA. Unlike Fisher's LDA,
  ConvexLDA does not require matrix inversion and avoids singularity issues in high-dimensional
  settings.
---

# A Convex formulation for linear discriminant analysis

## Quick Facts
- arXiv ID: 2503.13623
- Source URL: https://arxiv.org/abs/2503.13613
- Reference count: 40
- This paper presents ConvexLDA, a convex formulation of linear discriminant analysis (LDA) that addresses key limitations of Fisher's LDA.

## Executive Summary
This paper introduces ConvexLDA, a novel convex formulation of linear discriminant analysis that overcomes fundamental limitations of Fisher's LDA, particularly the need for matrix inversion and singularity issues in high-dimensional settings. The method reformulates LDA as a multi-objective optimization problem that simultaneously minimizes sample-to-centroid distances and maximizes class separation through log-determinant optimization. By leveraging convex optimization techniques, ConvexLDA ensures global optimality and improved scalability while maintaining computational efficiency through gradient-based optimization.

## Method Summary
ConvexLDA reformulates linear discriminant analysis as a convex optimization problem by combining two key objectives: minimizing distances between samples and their class centroids while maximizing between-class separation using log-determinant regularization. The method avoids matrix inversion entirely, eliminating singularity issues common in high-dimensional data. The optimization problem is solved using gradient-based methods, making it computationally efficient and scalable to large datasets. The trade-off between the two objectives is controlled by a regularization parameter, allowing flexibility in balancing classification accuracy with class separation.

## Key Results
- ConvexLDA achieves higher classification accuracy than state-of-the-art LDA variants across multiple datasets
- In high-dimensional biological datasets, ConvexLDA outperforms competing methods by 20-25% margins
- The method effectively mitigates class collapse issues while maintaining global optimality guarantees

## Why This Works (Mechanism)
ConvexLDA works by reformulating LDA as a convex optimization problem that simultaneously optimizes two complementary objectives. The first objective minimizes the distance between samples and their respective class centroids, promoting tight intra-class clustering. The second objective maximizes the log-determinant of the between-class scatter matrix, which promotes large class separation while maintaining convexity. This dual-objective formulation eliminates the need for matrix inversion, avoiding singularity issues in high-dimensional settings. The convexity ensures that any local minimum is also a global minimum, providing theoretical guarantees for the optimization process. The regularization parameter allows flexible control over the trade-off between these competing objectives.

## Foundational Learning
The foundational learning concept in ConvexLDA relies on the convex relaxation of the traditional LDA objective. By expressing the discriminant analysis problem as a convex combination of sample-to-centroid distance minimization and log-determinant-based class separation maximization, the method ensures that the learning process converges to a globally optimal solution. This approach leverages the theoretical properties of convex optimization, where the feasible region is a convex set and the objective function is convex, guaranteeing that gradient-based optimization methods will find the global optimum rather than getting trapped in local minima.

## Architecture Onboarding
ConvexLDA's architecture can be understood as a two-component optimization framework. The first component handles the intra-class compactness through Euclidean distance minimization between samples and their class centroids. The second component ensures inter-class separability through log-determinant maximization of the between-class scatter matrix. These components are combined using a regularization parameter that controls their relative importance. The architecture is designed to be modular, allowing for easy adaptation to different problem settings. The gradient-based optimization approach makes the architecture computationally efficient and scalable to large datasets, while the convex nature ensures reliable convergence properties.

## Open Questions the Paper Calls Out
None

## Limitations
- No specific datasets are named or analyzed, making independent verification impossible
- The claimed 20-25% performance margins lack supporting details about baseline methods and statistical significance
- Reliance on gradient-based optimization for a non-smooth log-determinant term raises practical implementation concerns
- The regularization parameter's optimal selection strategy is not discussed
- No theoretical analysis of computational complexity or convergence rates is provided
- The method's behavior in extremely high-dimensional settings (where the number of features exceeds the number of samples) is not explored

## Confidence
The primary limitations center on the lack of empirical validation for theoretical claims. While the theoretical framework appears sound, the absence of numerical experiments or convergence analysis leaves benefits unverified in practice. The computational efficiency claims also lack supporting runtime analysis. The paper presents an interesting theoretical approach to LDA reformulation, but without empirical evidence or detailed implementation specifications, the practical utility remains uncertain.

## Next Checks
1. Implement ConvexLDA on standard benchmark datasets (e.g., UCI repository, MNIST) and compare performance against established LDA variants with proper statistical testing
2. Analyze computational complexity and runtime performance across varying dataset sizes and dimensions to verify scalability claims
3. Conduct sensitivity analysis on the trade-off parameter balancing the two objective terms to determine robustness to hyperparameter selection
4. Test the method's behavior in high-dimensional settings where features exceed samples to validate singularity avoidance claims
5. Evaluate the non-smooth log-determinant optimization through numerical experiments to assess practical convergence properties