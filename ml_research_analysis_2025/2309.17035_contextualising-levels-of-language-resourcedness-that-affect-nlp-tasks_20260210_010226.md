---
ver: rpa2
title: Contextualising Levels of Language Resourcedness that affect NLP tasks
arxiv_id: '2309.17035'
source_url: https://arxiv.org/abs/2309.17035
tags:
- language
- languages
- resources
- south
- african
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of characterising language resourcedness
  in NLP, which is typically oversimplified into binary "low" vs "high" resource categories.
  The authors propose a more nuanced 5-point scale (Very LRL, LRL, RL, HRL, Very HRL)
  based on 11 contextual dimensions beyond just counting tools and corpora.
---

# Contextualising Levels of Language Resourcedness that affect NLP tasks

## Quick Facts
- **arXiv ID:** 2309.17035
- **Source URL:** https://arxiv.org/abs/2309.17035
- **Reference count:** 0
- **Primary result:** Proposes a 5-level scale (Very LRL, LRL, RL, HRL, Very HRL) based on 11 contextual dimensions to characterize language resourcedness in NLP.

## Executive Summary
This paper addresses the oversimplification of language resourcedness in NLP, which is typically reduced to a binary "low" vs. "high" resource classification. The authors propose a more nuanced framework using a 5-point scale based on 11 contextual dimensions including grammar documentation, corpus availability, HLT tools, researcher communities, funding, education, government policy, and dictionary availability. The approach was operationalized by classifying South African official languages and other African and global languages, demonstrating that languages like isiNdebele are Very LRL while English is the only Very HRL language. This framework provides a more comprehensive understanding of language resourcedness that can inform language policy and research planning.

## Method Summary
The authors developed a rule-based framework for classifying languages into five resourcedness levels by assessing them against 11 contextual dimensions. These dimensions were mapped to qualitative thresholds (e.g., corpus size, existence of tools, number of researchers) and languages were classified based on which features they satisfied. The classification process involves auditing a language against each dimension, mapping findings to feature criteria, and assigning a level based on the pattern of satisfied features. The framework was applied to South African official languages and compared with other African and global languages to validate its effectiveness.

## Key Results
- The 11-dimensional framework successfully classified South African languages: isiNdebele as Very LRL, isiXhosa as LRL, Afrikaans and South African English as RL
- Afrikaans and South African English were classified as Resourced (RL), while Dutch, German, French, and Mandarin were classified as High Resourced (HRL)
- English was identified as the only Very High Resourced (Very HRL) language in the study
- The LRL-to-RL transition was identified as the steepest development curve and primary barrier for language survival in the digital age

## Why This Works (Mechanism)

### Mechanism 1: Multidimensional Gap Analysis
If "resourcedness" is decomposed from a binary label into a vector of 11 distinct dimensions (e.g., grammar, funding, policy), specific systemic bottlenecks hindering NLP progress become visible that would otherwise be obscured. The framework replaces the monolithic "low resource" label with a structured assessment across technical (corpora, tools), human (researchers, teachers), and societal (policy, funding) axes. By scoring these independently, it prevents the "masking" of deficits where a language might have data (corpora) but lacks the human infrastructure (linguists) to utilize it, forcing targeted interventions.

### Mechanism 2: The "LRL-to-RL" Barrier Effect
The transition from Low Resourced (LRL) to Resourced (RL) represents the steepest part of the development curve, acting as the primary filter for language survival in the digital age. The paper posits a non-linear progression where "crawling out" of the LRL category requires solving a "chicken-and-egg" problem of simultaneous dependencies (creating tools requires data which requires funding which requires policy). Once a language reaches RL, infrastructure exists that accelerates future growth (the "system is rolling").

### Mechanism 3: Contextual Validity Constraints
NLP project success is constrained by the "lived reality" of the language's ecosystem; purely technical solutions (e.g., "just get translators") fail when they ignore sociopolitical dimensions like standardization and education. The framework enforces a reality check by including dimensions like "Limited education" and "Government policy." If a language lacks a standard orthography or a pipeline of trained linguists, technical data collection efforts will produce noisy, unusable artifacts, causing project failure.

## Foundational Learning

- **Concept: Basic Language Resource Kit (BLARK)**
  - **Why needed here:** The paper explicitly contrasts its approach with BLARK (a minimal set of resources like corpora/lexicons). Understanding BLARK is necessary to see how the authors expand it from a technical checklist to a sociotechnical matrix.
  - **Quick check question:** Why is checking for a "spellchecker" (a BLARK component) insufficient to determine if a language is "Resourced" according to this paper?

- **Concept: The Dependency Chain in NLP**
  - **Why needed here:** The mechanism relies on the idea that you cannot just "build tools" without "grammar" and "people". Recognizing this dependency stack is crucial for the "Critical Path" analysis.
  - **Quick check question:** If a language has a large corpus but no computational grammar, why might a parser project fail?

- **Concept: Orthography and Standardization**
  - **Why needed here:** The paper cites issues with OCR and spellcheckers failing due to language peculiarities or lack of standard resources. Understanding that NLP requires stable written standards helps explain why "Very LRL" languages struggle with digitization.
  - **Quick check question:** How does the lack of a monolingual dictionary signal a deeper problem than just missing a data file?

## Architecture Onboarding

- **Component map:** Input: Target Language & Context → Assessment Engine: 11-Dimension Matrix → Classifier: 5-Level Scale → Strategic Output: Intervention Plan
- **Critical path:**
  1. **Inventory:** Audit the language against the 11 dimensions (don't just count papers; assess "lived reality" like funding streams and education policy)
  2. **Classify:** Map the results to the 5-level scale (e.g., Is it "Very LRL" or "LRL"?)
  3. **Gap Analysis:** Identify the largest gap (likely the LRL->RL transition)
  4. **Intervention:** Prioritize "Basic HLTs" or "Corpus Building" only if "Education/People" exist to support them
- **Design tradeoffs:**
  - **Granularity vs. Operationalization:** The 11 dimensions provide high fidelity but make automated classification difficult compared to simpler "count papers" methods
  - **Expert-based vs. Data-driven:** The paper relies on expert assessment (Subjective) rather than purely quantitative metrics (Objective), which is richer but harder to scale
- **Failure signatures:**
  - **The "Zombie" Tool:** A spellchecker exists but is incompatible with current OS versions or was built on outdated grammar, effectively a "Very LRL" condition masquerading as "LRL"
  - **The "Paper" Policy:** High score on Government Policy but low score on Implementation/Funding, resulting in no actual resources
- **First 3 experiments:**
  1. **Static Validation:** Apply the matrix to a known "Resourced" language (e.g., English) and a known "Very LRL" (e.g., isiNdebele) to confirm the sensitivity of the 11 dimensions
  2. **Ablation Study:** Attempt to classify a language using only technical dimensions vs. the full matrix to quantify the impact of the "Contextual" dimensions
  3. **Intervention Simulation:** For a specific LRL (e.g., isiXhosa), simulate adding resources to specific dimensions to see if it shifts the classification level, identifying the lowest-cost path to "RL"

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does language endangerment correlate with the proposed levels of digital resourcedness?
- **Basis in paper:** [explicit] The authors state, "How exactly language endangerment and resourcedness interrelate is yet to be investigated in more detail," noting that current evidence suggests they do not always align.
- **Why unresolved:** The paper focuses on defining resourcedness dimensions rather than mapping them against language vitality scales.
- **What evidence would resolve it:** A comparative study mapping the 5-level resourcedness scale against established language endangerment frameworks for a diverse set of languages.

### Open Question 2
- **Question:** Can quantitative thresholds be established for the 11 dimensions to standardize language categorization?
- **Basis in paper:** [explicit] The authors note that "More research is needed to refine some of the values for the features to enable a more robust categorization," admitting that current boundaries are fuzzy.
- **Why unresolved:** The current framework relies on qualitative, expert-based assessment rather than hard numerical cut-offs.
- **What evidence would resolve it:** A dataset quantifying metrics for all 11 dimensions across multiple languages that validates a statistically significant separation between the five proposed categories.

### Open Question 3
- **Question:** Which specific interventions (technological vs. policy) most effectively transition a language from Low-Resourced to Resourced?
- **Basis in paper:** [explicit] The conclusion invites research to investigate "what low-hanging technology and policy development can be targeted for each... to strategise to obtain the most realistic and effective output."
- **Why unresolved:** The paper establishes the classification matrix but does not evaluate the efficacy of specific remediation strategies.
- **What evidence would resolve it:** Longitudinal studies tracking the impact of targeted funding or policy changes on the resourcedness level of pilot languages.

## Limitations
- The 11-dimensional framework introduces significant methodological complexity that creates both opportunities and challenges
- The framework relies on expert judgment for dimension assessment, raising questions about reproducibility and potential cultural or regional biases
- The claim that the LRL-to-RL transition represents the steepest development curve is based on the authors' experience rather than empirical validation
- The framework assumes static conditions, though language resourcedness may change rapidly with technological advances or policy shifts

## Confidence

- **High Confidence**: The identification of the oversimplification problem in current binary LRL/HRL classification; the general structure of expanding beyond technical metrics to include sociotechnical dimensions; the observation that isiNdebele, isiXhosa, and Afrikaans fit the proposed classification
- **Medium Confidence**: The specific delineation of the 11 dimensions as orthogonal and comprehensive; the claim that the LRL-to-RL transition is uniquely challenging; the assertion that policy and funding dimensions are critical barriers
- **Low Confidence**: The precise quantitative boundaries between categories for qualitative dimensions; the generalizability of the framework to non-African language contexts without modification; the claim that modern foundation models won't significantly alter dependency relationships

## Next Checks

1. **Cross-cultural validation**: Apply the 11-dimensional assessment to languages from different linguistic families and geographical regions (e.g., Indigenous languages of the Americas, Pacific languages) to test the framework's generalizability beyond African contexts.

2. **Temporal stability assessment**: Track a set of languages classified as LRL or RL over a 2-3 year period, monitoring how changes in specific dimensions (particularly funding, policy, and tool development) correlate with movement between categories.

3. **Automated classification feasibility**: Develop and test a semi-automated pipeline using publicly available databases (e.g., ELRA, LDC, institutional repositories, government education statistics) to populate the 11 dimensions, measuring inter-rater reliability between automated and expert assessments.