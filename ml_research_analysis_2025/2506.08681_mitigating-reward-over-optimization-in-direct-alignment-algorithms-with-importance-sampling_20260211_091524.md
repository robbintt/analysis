---
ver: rpa2
title: Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance
  Sampling
arxiv_id: '2506.08681'
source_url: https://arxiv.org/abs/2506.08681
tags:
- policy
- reward
- preference
- daas
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the over-optimization problem in Direct Preference
  Optimization (DPO), where the model drifts away from the reference policy during
  training, leading to degraded performance. The authors propose Importance Sampling
  DPO (IS-DPO), which multiplies the DPO objective by an importance ratio that accounts
  for the reference policy distribution.
---

# Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling

## Quick Facts
- arXiv ID: 2506.08681
- Source URL: https://arxiv.org/abs/2506.08681
- Reference count: 19
- Key result: IS-DPO outperforms other methods in mitigating reward over-optimization with up to 92% win rate in summarization tasks

## Executive Summary
This paper addresses the over-optimization problem in Direct Preference Optimization (DPO), where models drift away from the reference policy during training, leading to degraded performance. The authors propose Importance Sampling DPO (IS-DPO), which incorporates an importance ratio into the DPO objective to account for the reference policy distribution. By clipping the importance ratio, IS-DPO effectively mitigates reward over-optimization while maintaining robustness to early convergence issues. Experimental results on TL;DR summarization and instruction-following tasks demonstrate significant improvements over baseline methods, with up to 92% win rate in summarization and 74% GPT-4 win rate in dialogue tasks.

## Method Summary
The paper introduces Importance Sampling DPO (IS-DPO) to address reward over-optimization in DPO algorithms. IS-DPO modifies the standard DPO objective by multiplying it with an importance ratio that accounts for the reference policy distribution. This ratio is clipped to prevent high variance during training. The approach effectively constrains the model to stay closer to the reference policy while still optimizing for the desired rewards. The method is evaluated on TL;DR summarization and instruction-following tasks using Llama models of various sizes (7B and 33B parameters).

## Key Results
- IS-DPO achieves up to 92% win rate in summarization tasks compared to baseline methods
- In dialogue tasks, IS-DPO demonstrates 74% GPT-4 win rate while maintaining lower KL divergence to reference models
- The method shows robustness to early convergence issues observed in other approaches
- IS-DPO significantly outperforms other DPO variants in mitigating reward over-optimization

## Why This Works (Mechanism)
IS-DPO works by incorporating importance sampling into the DPO objective, which accounts for the discrepancy between the current policy and the reference policy. By multiplying the DPO loss with the importance ratio and clipping it, the method effectively regularizes the optimization process to prevent excessive deviation from the reference policy. This mechanism ensures that the model doesn't over-optimize for the reward signal at the cost of drifting too far from the original behavior, addressing a fundamental limitation of standard DPO approaches.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A method for aligning language models using preference data without reinforcement learning; needed to understand the baseline approach being improved
- **Importance Sampling**: A technique for estimating expectations under a target distribution using samples from a different distribution; critical for understanding how IS-DPO modifies the objective
- **KL Divergence**: A measure of how one probability distribution diverges from a second, reference probability distribution; important for evaluating how much the model drifts from the reference policy
- **Reward Over-optimization**: The phenomenon where models excessively optimize for reward signals at the expense of other important behaviors; the core problem being addressed
- **Clipping Techniques**: Methods for bounding values to prevent numerical instability; essential for understanding how IS-DPO controls variance

## Architecture Onboarding

**Component Map**: Reference Model -> Importance Ratio Calculator -> Clipped IS-DPO Objective -> Updated Model

**Critical Path**: The importance ratio calculation and clipping mechanism forms the critical path, as these operations directly control the optimization dynamics and prevent over-optimization.

**Design Tradeoffs**: The main tradeoff is between the strength of regularization (controlled by the clipping threshold) and the model's ability to optimize for rewards. Lower thresholds provide stronger regularization but may limit performance gains, while higher thresholds risk over-optimization.

**Failure Signatures**: Common failure modes include:
- Insufficient clipping leading to high variance and unstable training
- Excessive clipping preventing meaningful optimization
- Incorrect importance ratio estimation causing poor alignment

**First Experiments**:
1. Verify importance ratio calculation matches theoretical expectations on simple distributions
2. Test clipping behavior across different threshold values with synthetic data
3. Compare KL divergence trajectories between IS-DPO and standard DPO during training

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation:
- Long-term stability of IS-DPO under varying hyperparameter settings
- Sensitivity to the clipping threshold and its interaction with learning rates and batch sizes
- Generalization to other domains beyond TL;DR summarization and dialogue tasks
- Performance comparison with non-DPO alignment methods like RLHF and SLiC

## Limitations
- Experiments focus on specific datasets (TL;DR and dialogue) and model sizes (7B, 33B parameters), limiting generalization claims
- Comparison primarily against other DPO variants rather than alternative alignment approaches
- Limited ablation studies on critical hyperparameters like the clipping threshold
- Does not explore the interaction between clipping thresholds, learning rates, and batch sizes

## Confidence

| Claim | Confidence |
|-------|------------|
| Core technical contribution is sound | High |
| Empirical results demonstrate effectiveness | Medium |
| Generalizability across domains and scales | Low |
| Improvements are specific to importance sampling mechanism | Medium |

## Next Checks

1. Conduct extensive hyperparameter sensitivity analysis across different clipping thresholds, learning rates, and batch sizes to identify robust operating regimes.

2. Evaluate IS-DPO on additional datasets and model scales (e.g., 70B parameter models) to test generalization beyond the current experimental scope.

3. Compare against non-DPO alignment methods (RLHF, SLiC) to better contextualize the improvements and determine if they are specific to the importance sampling approach.