---
ver: rpa2
title: 'LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics
  Olympiads?'
arxiv_id: '2510.09595'
source_url: https://arxiv.org/abs/2510.09595
tags:
- informatics
- olympiad
- reasoning
- performance
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LiveOIBench, a new benchmark for evaluating
  large language models (LLMs) on competitive programming problems. The benchmark
  addresses limitations in existing coding benchmarks by curating 403 expert-designed
  Olympiad-level problems with an average of 60 private test cases each, sourced directly
  from official Informatics Olympiads.
---

# LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?

## Quick Facts
- arXiv ID: 2510.09595
- Source URL: https://arxiv.org/abs/2510.09595
- Reference count: 40
- Primary result: GPT-5 achieves 81.76th percentile against human contestants but falls short of top performers

## Executive Summary
LiveOIBench is a new benchmark for evaluating large language models on competitive programming problems from official Informatics Olympiads. The benchmark addresses limitations in existing coding benchmarks by curating 403 expert-designed problems with extensive private test cases and direct comparison against human contestant performance. The authors benchmark 34 models and find that while current models like GPT-5 achieve high percentiles, they still struggle with complex algorithms requiring creative reasoning, particularly dynamic programming and tree problems.

## Method Summary
The benchmark curates 403 problems from 72 contests across 14 Olympiads (2023-2025), each with ~60 private test cases and subtask rubrics. For evaluation, models generate 8 C++ solutions per problem, and the highest-scoring solution is selected. Performance is measured against human contestant score distributions using metrics including Pass@8, relative score percentage, and human percentile rankings. The evaluation judge supports batch, interactive, and output-verified problem types, with careful attention to contamination risks through date cutoffs and private test case verification.

## Key Results
- GPT-5 achieves 81.76th percentile against human contestants, outperforming 80% of competitors but not reaching top human performers
- Among open-weight models, GPT-OSS-120B reaches 60th percentile
- Performance degrades significantly for dynamic programming (46.88% pass rate for GPT-5) and tree problems (37.50% pass rate)
- Models show algorithm-specific weaknesses, struggling particularly with problems requiring creative state design and hierarchical invariants
- Stronger reasoning models allocate proportionally more tokens to structured analysis and planning while reducing unnecessary exploration

## Why This Works (Mechanism)

### Mechanism 1: Strategic Reasoning Token Allocation
High-performing models prioritize structured analysis and planning over exploratory search. Stronger reasoning models allocate proportionally more tokens to problem understanding and algorithmic analysis, while maintaining stable (not excessive) exploration levels. This prevents inefficient pivoting and "underthinking." If increased planning tokens show no predictive power for correctness when controlling for total reasoning budget and model capability, the mechanism is likely epiphenomenal.

### Mechanism 2: Algorithm-Complexity Gating
Performance degrades systematically for algorithms requiring hierarchical state design or creative invariants. Models succeed on tasks requiring procedural knowledge (implementation, sorting, prefix sums) but fail when solutions demand novel state-space construction or compositional reasoning that cannot be retrieved from training distributions. If curriculum fine-tuning on synthetic DP/tree problems closes the gap without architectural changes, the mechanism may be data-coverage rather than reasoning-capacity limited.

### Mechanism 3: Inference-Time Compute Scaling via Sequential Reasoning
Increasing reasoning token budget yields larger performance gains than parameter scaling for complex algorithmic tasks. Sequential scaling (extended chain-of-thought) allows smaller models to approach larger-model performance by enabling deeper multi-step analysis, backtracking, and verification before committing to code. If token-scaling gains plateau completely before reaching larger-model performance ceilings on held-out problem classes, sequential scaling may have fundamental limits.

## Foundational Learning

- **Concept: Informatics Olympiad problem structure (subtasks, test cases, partial credit)**
  - Why needed here: Unlike standard coding benchmarks, OI problems have graded subtasks with explicit scoring rubrics. Understanding this structure is essential for interpreting percentile rankings and medal thresholds.
  - Quick check question: If a model scores 60/100 points on a problem with 5 subtasks worth [10, 15, 20, 25, 30] points each, which subtasks did it likely solve?

- **Concept: Competitive programming algorithm taxonomy**
  - Why needed here: The paper's analysis relies on algorithm tags (DP, greedy, graph traversal, etc.) to diagnose model weaknesses. You need to recognize what makes DP harder than sorting to interpret results.
  - Quick check question: Why would a model that handles graph traversal well struggle with tree problems, given that trees are a subset of graphs?

- **Concept: Inference-time compute paradigms (parallel vs. sequential scaling)**
  - Why needed here: The paper evaluates both Pass@k (parallel sampling) and reasoning-budget scaling (sequential). These represent fundamentally different strategies for using compute at test time.
  - Quick check question: If you have 100K tokens of compute budget, how would you allocate it differently for a model with strong sampling diversity vs. one with strong single-attempt reasoning?

## Architecture Onboarding

- **Component map:** PDF→Markdown conversion (Marker) → LLM-assisted verification (Gemini-2.0-Flash) → Problem ingestion → Evaluation judge → Human baseline (Codeforces matching) → Model interface (prompt template → 8-sample C++ generation → Best-score selection)

- **Critical path:** Verify test cases by running official solutions through judge → Confirm subtask scoring sums to total points → For interactive problems, validate grader + solution pairing before including in benchmark → Exclude output-only problems (no algorithmic solution required)

- **Design tradeoffs:** C++ only for evaluation (sacrifices language-coverage for execution efficiency; paper shows C++ outperforms Python/Java due to competitive programming's time/memory constraints) → Pass@8 with best-score selection (balances compute cost against coverage; parallel scaling shows diminishing returns beyond k=4) → 2023+ date cutoff (reduces contamination risk but excludes classic IOI problems with extensive solution literature)

- **Failure signatures:** High runtime errors despite correct algorithm (aggressive optimization patterns in strong models) → Compilation errors in cautious models (declining to generate when reasoning budget insufficient) → False positives on problems with <60 test cases (paper notes prior benchmarks have ~50% false-positive rates)

- **First 3 experiments:** Establish baseline with open-weight model: Run GPT-OSS-120B on 10-problem subset across difficulty divisions (D1-D4) to validate your evaluation pipeline against reported percentiles → Ablate reasoning budget: Test same model at low/medium/high token limits on DP-heavy vs. implementation-heavy problems to reproduce the algorithm-specific scaling pattern → Contamination spot-check: Pick 5 problems from Q1 2023 (before most model cutoffs) and 5 from Q4 2024; verify no temporal performance gap that would indicate memorization

## Open Questions the Paper Calls Out

### Open Question 1
Can curriculum-driven fine-tuning using synthetic datasets of complex graph, tree, and dynamic programming problems enable models to internalize the hierarchical invariants currently missing in their reasoning? Current models rely heavily on procedural knowledge from training and fail to spontaneously generate the creative insights needed for complex structural tasks. Significant improvement in Pass@k rates for "DP," "Tree," and "Segment Tree" tags after the proposed fine-tuning intervention would resolve this.

### Open Question 2
How can the allocation of reasoning tokens be optimized to maximize structured planning and analysis while minimizing unnecessary exploration and pivoting? It is unclear if explicit training constraints or reward shaping are required to prevent models from "underthinking" or wasting compute on dead-ends. A training method that dynamically adjusts reasoning behaviors based on problem difficulty, resulting in higher accuracy with lower token usage, would resolve this.

### Open Question 3
Does incorporating fine-grained reward signals for efficiency and memory management during reinforcement learning reduce the runtime errors caused by aggressive optimization patterns? Current RL approaches predominantly use binary solution correctness as the sole reward, neglecting execution robustness. A model trained with efficiency rewards demonstrating a statistically significant reduction in runtime errors without a loss in problem-solving capability would resolve this.

### Open Question 4
What is the isolated contribution of reinforcement learning versus distillation in developing algorithmic reasoning capabilities compared to parameter scaling? Model families differ in architecture and pretraining data, making it hard to isolate the specific causal impact of the training strategy. Controlled ablation studies comparing RL-only, distillation-only, and combined approaches on the same base architecture would resolve this.

## Limitations
- Narrow focus on C++-based competitive programming limits generalizability to broader coding tasks and real-world software engineering
- Contamination risk remains non-zero despite 2023+ cutoff due to potential exposure through programming forums or derivative datasets
- Human baseline construction relies on Codeforces profile matching, which may introduce bias if not all contestants maintain public profiles

## Confidence
**High Confidence**: Benchmark construction methodology and core finding that GPT-5 achieves 81.76th percentile against humans while remaining below top competitors. Algorithm-specific performance degradation patterns are consistently observed.

**Medium Confidence**: Mechanism linking token allocation to success and inference-time scaling results. These findings require further validation across different model families to establish causal relationships.

**Low Confidence**: Contamination assessment and generalizability to non-competitive programming domains due to reliance on date cutoffs and exclusive focus on C++.

## Next Checks
1. **Cross-Language Generalization**: Replicate the benchmark with Python solutions to test whether algorithm-specific performance gaps persist across languages, isolating whether C++ optimization patterns contribute to observed failures.

2. **Algorithm Transfer Learning**: Fine-tune a strong reasoning model on synthetic DP and tree problems with explicit state-space annotations, then evaluate on held-out LiveOIBench problems to determine whether data coverage or fundamental reasoning limitations drive the performance gap.

3. **Token Allocation Ablation**: Conduct controlled experiments varying reasoning token budgets while holding model parameters constant, specifically measuring the marginal utility of planning vs. exploration tokens on problems requiring creative state design to validate the proposed mechanism.