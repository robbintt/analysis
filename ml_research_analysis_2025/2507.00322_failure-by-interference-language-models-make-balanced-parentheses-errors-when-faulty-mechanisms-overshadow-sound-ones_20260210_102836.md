---
ver: rpa2
title: 'Failure by Interference: Language Models Make Balanced Parentheses Errors
  When Faulty Mechanisms Overshadow Sound Ones'
arxiv_id: '2507.00322'
source_url: https://arxiv.org/abs/2507.00322
tags:
- paren
- heads
- attention
- gpt-2
- components
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why language models (LMs) make errors on
  simple syntactic tasks like generating balanced parentheses, and proposes a method
  to mitigate these errors. The study reveals that LMs rely on multiple components
  (attention heads and feed-forward neurons) with varying levels of reliability to
  perform such tasks.
---

# Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones

## Quick Facts
- **arXiv ID**: 2507.00322
- **Source URL**: https://arxiv.org/abs/2507.00322
- **Reference count**: 40
- **Primary result**: RASTEER boosts balanced parentheses accuracy from 0% to 100% on some models by amplifying reliable components and suppressing faulty ones

## Executive Summary
This paper investigates why language models make errors on simple syntactic tasks like generating balanced parentheses. The authors propose that errors occur when "faulty mechanisms" (components promoting incorrect tokens) overshadow "sound mechanisms" (components consistently promoting correct tokens). They introduce RASTEER, a steering method that ranks model components by reliability and amplifies the contribution of more reliable ones to the final logits. RASTEER substantially improves performance on balanced parentheses tasks, boosting accuracy from 0% to around 100% for some models without impairing general coding ability. It also generalizes to arithmetic reasoning tasks, achieving performance gains of up to 20%.

## Method Summary
RASTEER analyzes transformer components (attention heads and feed-forward neurons) using a "logit lens" approach to identify which components reliably promote correct tokens. Components are ranked by generalizability (number of sub-tasks with high accuracy) and F1-score (promotion of target tokens). The method then amplifies the activations of top-ranked components by a multiplier α during inference. For balanced parentheses, components are pre-filtered based on whether they promote closing-paren tokens in their top/bottom logits. The approach is evaluated on synthetic datasets with 4 sub-tasks (1-4 closing parentheses) and arithmetic reasoning, using accuracy and HumanEval pass@1 as metrics.

## Key Results
- RASTEER boosts balanced parentheses accuracy from 0% to 100% on some models
- Steering attention heads is far more effective than FF neurons due to "noisy promotion" issues
- RASTEER preserves general coding ability on HumanEval while improving task-specific performance
- Performance gains up to 20% on arithmetic reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Sound vs. Faulty Component Interference
Errors in balanced parentheses tasks occur when the contribution of "faulty" model components outweighs the contribution of "sound" components. LMs use a collection of components that independently contribute to the final logit. "Sound" mechanisms generalize across sub-tasks and promote the correct token, while "faulty" mechanisms introduce noise by promoting incorrect tokens. The final prediction fails if the aggregate signal from faulty components is stronger than the signal from sound ones.

### Mechanism 2: Noisy Promotion via Low-Selectivity Components
Many model components, especially FF neurons, exhibit low selectivity, promoting both correct and distractor tokens simultaneously. This "noisy promotion" means a component cannot cleanly isolate the correct output, increasing the chance that incorrect tokens accumulate enough logit strength to win. Components often have both the correct token and incorrect tokens in their top-k or bottom-k logits after projection to vocabulary space.

### Mechanism 3: Inference-Time Steering via Component Re-weighting
A model's accuracy can be substantially improved at inference time by identifying reliable components and amplifying their contribution to the final logit. RASTEER ranks components based on reliability metrics like F1-score and scales the activation of top-ranked components by a multiplier α > 1 before they are added to the residual stream. This boosts the signal from "sound" mechanisms, allowing them to overcome the noise from "faulty" ones without changing model weights.

## Foundational Learning

**Concept: Residual Stream and Additive Composition in Transformers**
- Why needed here: The core explanation of "interference" and the RASTEER method both depend on understanding that transformer layers update a central "residual stream" through additive contributions from attention heads and FF layers.
- Quick check question: In a standard transformer, how does the output of layer L relate to its input? (Answer: It's the input plus the output of the attention and FF sub-layers: r_L = r_{L-1} + Attention + FF)

**Concept: Logit Lens / Projecting Activations to Vocabulary**
- Why needed here: The method for analyzing components relies on taking an internal component's activation vector and projecting it directly to the vocabulary using the unembedding matrix W_U to see which tokens it promotes.
- Quick check question: If you have an activation vector h from an attention head at the last token position, how would you inspect which vocabulary tokens it is likely to promote? (Answer: Multiply it by the unembedding matrix W_U and inspect the top values in the resulting logit vector)

**Concept: Precision vs. Recall in Component Analysis**
- Why needed here: The paper uses these metrics to rank components. Understanding the distinction is critical for interpreting the results.
- Quick check question: For a specific component, what does it mean if it has 100% recall but 25% precision on a 4-class balanced parentheses task? (Answer: It promotes the correct token for every input, but when it promotes a specific token it's only correct 1 in 4 times)

## Architecture Onboarding

**Component map:**
- Inputs: Synthetic code prompts requiring closing parentheses
- Transformer Layers: Stacked layers, each with Multi-Head Self-Attention (MHSA) and Feed-Forward (FF) sub-layers
- Attention Heads (H_{ℓ,h}): Indexed by layer ℓ and head h, analyzed at last token position
- FF Neurons: Decomposed as m^ℓ_i v^ℓ_i, analyzed at last token position
- Logit Lens: Operation of taking component activation h_c and computing h_c^T W_U
- Output: Final logits from last token's residual stream: logits = r_n^L W_U

**Critical path:**
1. Identify Candidate Components: Use Logit Lens to pre-filter components by projecting to vocabulary space
2. Analyze & Rank Components: Run on training set, measure correctness (Algorithm 1) and promotion (Algorithm 2), rank by generalizability then F1-score
3. Apply Steering (RASTEER): Select top-k ranked components, scale their activations by multiplier α ∈ [1.1, 2.0] before adding to residual stream

**Design tradeoffs:**
- Top-down (RASTEER) vs. Bottom-up (Circuit Analysis): Top-down approach is more practical for steering than full circuit reverse-engineering
- Steering Attention Heads vs. FF Neurons: Steering attention heads is far more effective due to "noisy promotion" issues with FF neurons
- Metric for Ranking: F1-score outperforms precision and recall alone
- Scope of Steering: Steering across all token positions is safer and more effective than position-specific steering

**Failure signatures:**
- Performance Collapse: Promoting too many components (>20 heads) or non-final-layer circuit components can cause performance to drop dramatically
- RASTEER Failure on Small Models (GPT-2 Small): Even RASTEER fails to fully solve the task on smallest models for hardest sub-task
- No Improvement from Circuit Baseline: Steering circuit-identified heads can fail to improve performance, especially in smaller models

**First 3 experiments:**
1. Reproduce the Logit Lens Analysis: Take a pre-trained GPT-2 or Llama model, extract a head's output activation at last token, project to vocabulary space, and print top-5/bottom-5 tokens to verify parentheses tokens
2. Implement and Test the Promotion/Correctness Algorithms: Implement Algorithm 1 and 2, compute accuracy, recall, and precision for a small set of heads
3. Implement Basic RASTEER Steering: Add a forward hook to scale the output of the top-ranked head by a factor α=1.5, run inference on test set, and observe change in accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models employ non-additive mechanisms, such as active noise suppression, that contribute to syntactic correctness, and can these be leveraged for steering?
- Basis in paper: Section 7 states that the assumption of a simple additive motif "may overlook non-additive mechanisms—such as those that actively suppress noise"
- Why unresolved: RASTEER assumes components contribute additively to the residual stream. If the model actively suppresses noise via complex interactions, the current steering method ignores a potential source of improvement
- What evidence would resolve it: Discovery and successful manipulation of components whose primary function is the active inhibition of incorrect tokens

### Open Question 2
- Question: What is the specific mechanistic factor preventing RASTEER from improving GPT-2 Small's performance on the four-parentheses task?
- Basis in paper: In Section 5.4.2, the authors conclude the failure is "not due to a weakness in its noisy promotion, but rather an unexplored factor"
- Why unresolved: The authors ruled out quantity of accurate attention heads and severity of noisy promotion as causes
- What evidence would resolve it: A causal analysis identifying a specific layer or sub-layer interaction in GPT-2 Small that dampens the effect of amplified attention heads

### Open Question 3
- Question: Can learned or adaptive strategies for ranking components and setting scaling multipliers outperform the fixed heuristics used in RASTEER?
- Basis in paper: The authors note in Section 7 that the method "relies on simple heuristics: components are ranked using recall, precision, and F1-score, and promoted via fixed scalar multipliers"
- Why unresolved: The current method uses static ranking and a fixed search range for multipliers
- What evidence would resolve it: Experiments demonstrating that a gradient-based or reinforcement learning optimization of steering parameters achieves higher accuracy than the current grid-search approach

## Limitations

- The analysis is confined to synthetic tasks with simple, well-defined ground truth, limiting generalizability to real-world code generation
- The F1-score threshold of τ=0.5 for identifying reliable components is empirically chosen without theoretical justification
- Performance gains are highly model-dependent—some models (e.g., GPT-2 Small) remain resistant even with RASTEER

## Confidence

- **High Confidence**: The existence of component-level noise and interference as an explanation for balanced parentheses errors. The logit lens analysis and F1-ranking method are reproducible and yield consistent results.
- **Medium Confidence**: The generalizability of RASTEER to other tasks beyond balanced parentheses, particularly to arithmetic reasoning where gains are smaller (up to 20%) and the mechanism may differ.
- **Low Confidence**: The claim that RASTEER preserves general coding capability without degradation, given that excessive steering (>20 heads) causes collapse.

## Next Checks

1. **Robustness to F1 Threshold**: Systematically vary the τ threshold used in Algorithm 2 (e.g., 0.3, 0.5, 0.7) and measure the impact on RASTEER's effectiveness across different model sizes to determine if the choice is critical or robust.

2. **Cross-Task Transfer Analysis**: Apply RASTEER-trained component rankings from balanced parentheses to arithmetic reasoning (and vice versa) to test whether reliable components are task-specific or have broader syntactic capability.

3. **Layer-by-Layer Steering Impact**: Instead of steering only final-layer components, implement and compare steering at different depths (early, middle, late layers) to determine whether the "overshadowing" mechanism operates primarily in final layers or is distributed throughout the network.