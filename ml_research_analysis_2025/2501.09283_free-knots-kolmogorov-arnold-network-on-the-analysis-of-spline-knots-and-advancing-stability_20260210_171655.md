---
ver: rpa2
title: 'Free-Knots Kolmogorov-Arnold Network: On the Analysis of Spline Knots and
  Advancing Stability'
arxiv_id: '2501.09283'
source_url: https://arxiv.org/abs/2501.09283
tags:
- knots
- grid
- spline
- function
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitations of Kolmogorov-Arnold Networks
  (KANs), specifically their poor training stability and heavy trainable parameter
  load. The authors analyze KANs through the lens of spline knots, deriving upper
  and lower bounds for the number of knots in B-spline-based KANs.
---

# Free-Knots Kolmogorov-Arnold Network: On the Analysis of Spline Knots and Advancing Stability

## Quick Facts
- **arXiv ID:** 2501.09283
- **Source URL:** https://arxiv.org/abs/2501.09283
- **Reference count:** 10
- **Primary result:** FR-KAN enhances KAN performance while reducing trainable parameters to match MLPs, demonstrating superior stability and accuracy across 8 datasets.

## Executive Summary
This work addresses the limitations of Kolmogorov-Arnold Networks (KANs), specifically their poor training stability and heavy trainable parameter load. The authors analyze KANs through the lens of spline knots, deriving upper and lower bounds for the number of knots in B-spline-based KANs. They propose a novel Free-Knots KAN (FR-KAN) that enhances performance while reducing trainable parameters to match standard Multi-Layer Perceptrons (MLPs). The method introduces a new training strategy to ensure C2 continuity of the learnable spline, resulting in smoother activation and improved training stability. Evaluated on 8 datasets spanning image, text, time series, multimodal, and function approximation tasks, FR-KAN demonstrates superior performance compared to MLP and original KAN, showcasing its feasibility and effectiveness.

## Method Summary
FR-KAN modifies the original KAN architecture by introducing learnable grid positions for B-splines, grouping neurons to share these grids, and adding second-derivative regularization to enforce C2 continuity. The learnable grid positions are implemented by adding a bias vector to each grid point and sorting to maintain order, allowing each neuron to develop unique knots. Neuron groups share the same grid to reduce parameters, while linear combination weights remain ungrouped to preserve the Kolmogorov-Arnold Theorem's requirements. The training loss includes a term penalizing large second derivatives of the spline combination weights, which smooths the activation and improves stability.

## Key Results
- FR-KAN reduces trainable parameters to match MLP scale while maintaining or improving accuracy across diverse datasets
- The method ensures C2 continuity of the learnable spline, resulting in smoother activation and improved training stability
- Evaluated on 8 datasets (image, text, time series, multimodal, function approximation), FR-KAN demonstrates superior performance compared to both MLP and original KAN

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learnable grid positions increase the upper bound on unique spline knots, enhancing expressive power without adding neurons.
- **Mechanism:** A learnable bias vector $b_g$ is added to each grid position and then sorted to maintain $g_1 < g_2 < \cdots < g_j$. This allows each neuron to develop knots at different positions rather than sharing fixed grid points across all neurons.
- **Core assumption:** Non-uniform knot placement better fits finite real-world observations than uniform grids; unique knots across neurons compound expressively across layers.
- **Evidence anchors:**
  - [abstract] "analyze the behavior of KANs through the lens of spline knots and derive the lower and upper bound"
  - [section 5.2] Theorem 5.1 provides the upper bound $N_k \leq h(G+K) + \sum_{l=1}^{L} hG(G-1)$ for free-knot KAN
  - [section 7, Figure 3] FR-KAN learns sharper, more accurate activation for Feynman equation I.6.2 with less oscillation than original KAN
  - [corpus] Weak direct support; related works (PRKAN, Sprecher Networks) pursue parameter reduction via different architectural choices, not via free knots
- **Break condition:** If sorted grid positions collapse (multiple $b_g$ values converge), the mechanism degenerates to fixed-grid KAN; sorting gradient flow may also introduce optimization instability.

### Mechanism 2
- **Claim:** Second-derivative regularization on spline combination weights enforces $C^2$ continuity, reducing oscillation and improving training stability.
- **Mechanism:** The loss adds $\lambda \sum_{i=0}^{n_i} \sum_{j=0}^{G} \frac{\partial^2 c_{j,i}}{\partial^2 x}$, explicitly penalizing inflection points while preserving activation values. Combined with extended grid range (e.g., [-10, 10] instead of [-1, 1]), this prevents grid clustering and reduces rapid gradient variation.
- **Core assumption:** Large second derivatives correlate with training instability and NaN loss; B-spline oscillation at inflection points is the primary cause of KAN instability.
- **Evidence anchors:**
  - [abstract] "introduce new a training strategy to ensure $C^2$ continuity... resulting in smoother activation"
  - [section 5.3] Figure 4 shows training converges stably at extended ranges while [-1, 1] range produces NaN loss
  - [corpus] No direct corpus evidence for this specific regularization; other KAN variants address stability via alternative basis functions (RBF, Wavelet) rather than derivative penalties
- **Break condition:** Excessive $\lambda$ may over-smooth activations, underfitting sharp features; the derivative is computed w.r.t. input $x$, not grid position, so effectiveness depends on input distribution alignment with grid.

### Mechanism 3
- **Claim:** Neuron grouping and weight sharing reduce trainable parameters from $O(d_{in} \times d_{out} \times (G+K+1))$ to $O(d_{in} \times d_{out} + h(G+K))$, matching MLP scale.
- **Mechanism:** Neurons are divided into $h$ groups sharing the same grid $G$; spline combination weights $A_b$ are shared with shortcut SiLU weights $A_s$, denoted as single matrix $A$. Linear combination weights remain ungrouped to preserve variety per Kolmogorov-Arnold Theorem.
- **Core assumption:** Shared grids within groups do not critically reduce model capacity; linear combination weights $A$ provide sufficient per-neuron variation.
- **Evidence anchors:**
  - [abstract] "reducing the number of trainable parameters to match the trainable parameter scale of standard Multi-Layer Perceptrons (MLPs)"
  - [section 5.1] Equation 5 shows final architecture; parameter counts derived explicitly
  - [section 6.2, Figure 2] FR-KAN achieves comparable or better accuracy than MLP at similar parameter counts across CIFAR, MNIST, STL10
  - [corpus] PRKAN and Sprecher Networks also target parameter reduction but use different strategies (shared basis functions, constructive forms)
- **Break condition:** Small $h$ (few groups) may overly constrain expressivity; performance gains shown primarily at moderate widths—if $d_{in}, d_{out}$ grow while $h$ stays fixed, capacity bottleneck may emerge.

## Foundational Learning

- **Concept: B-spline knots, order, and grid**
  - **Why needed here:** The entire paper's theoretical contribution (Theorem 4.3, 5.1) is expressed in terms of grid size $G$, spline order $K$, and knot bounds. Without this, the "free knots" innovation is opaque.
  - **Quick check question:** Given a B-spline with grid size $G=10$ and order $K=3$, how many basis functions and knots exist on a single layer? (Answer: $G+1$ basis functions, $G+K$ knots)

- **Concept: Kolmogorov-Arnold Representation Theorem (KAT)**
  - **Why needed here:** FR-KAN's architecture preserves per-neuron linear combination weights $A$ (ungrouped) specifically to honor KAT's requirement that complex multivariate functions decompose into superpositions of univariate functions.
  - **Quick check question:** How does KAT differ from the Universal Approximation Theorem for MLPs? (Answer: KAT decomposes functions into sums of univariate functions; UAT uses compositions of fixed nonlinearities)

- **Concept: Spline continuity ($C^0$, $C^1$, $C^2$)**
  - **Why needed here:** The paper claims $C^2$ continuity via second-derivative regularization; understanding why second derivatives matter for smoothness is essential to evaluate this claim.
  - **Quick check question:** What does $C^2$ continuity require that $C^1$ does not? (Answer: Continuous second derivative; no sudden changes in curvature/inflection)

## Architecture Onboarding

- **Component map:** Input → LayerNorm → Neuron Groups (each group shares grid $G^*$) → B-spline basis evaluation (De Boor recursion) → Weighted sum via shared $A$ → SiLU shortcut → Output
- **Critical path:**
  1. Initialize uniform grid $G$ in range [a, b] (default: [-10, 10])
  2. Sample learnable bias $b_g$ and compute sorted $G^*$
  3. Forward pass: evaluate B-spline bases at input points using $G^*$
  4. Apply grouped sharing: same $G^*$ for all neurons in group $h$
  5. Compute loss with second-derivative regularization
  6. Backprop through $b_g$, $c_j$, and $A$
- **Design tradeoffs:**
  - **Grid range [-10, 10] vs. [-1, 1]:** Wider range prevents clustering and NaN loss but may reduce resolution on narrow input distributions.
  - **Group count $h$:** Higher $h$ increases parameters and expressivity; lower $h$ improves efficiency but risks capacity loss.
  - **Regularization $\lambda$:** Larger $\lambda$ smooths training but may underfit sharp functions; paper does not specify optimal value.
- **Failure signatures:**
  - **NaN loss during training:** Grid range too narrow; inputs falling outside [a, b]; second-derivative explosion.
  - **Grid position collapse:** Multiple $b_g$ values converge; sorting creates ties; degenerates to fixed-grid behavior.
  - **Underfitting on sharp functions:** $\lambda$ too large; $h$ too small; grid range misaligned with input scale.
  - **High variance across runs:** Paper notes original KAN shows higher variance; FR-KAN should reduce this, but small $h$ may reintroduce instability.
- **First 3 experiments:**
  1. **Replicate function approximation (Feynman I.6.2):** Train FR-KAN with $G=10, K=3, h=1$, grid range [-10, 10]; compare RMSE and activation shape against original KAN; verify oscillation reduction per Figure 3.
  2. **Ablate grid range:** Train on CIFAR10 with ranges [-1,1], [-3,3], [-10,10]; monitor for NaN loss, convergence speed, and final accuracy to validate Figure 4 claims.
  3. **Ablate grouping factor $h$:** Fix width 128, vary $h \in \{1, 2, 4, 8\}$; plot parameter count vs. accuracy to confirm parameter-performance tradeoff matches Figure 2 behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the computational graph of B-splines be modified to reduce the overhead of recursive evaluation during backpropagation?
- **Basis in paper:** [explicit] The authors state, "future research can focus on reducing computational graph from recursive round of B-spline."
- **Why unresolved:** B-splines are inherently recursive, creating deep computational graphs that slow down training compared to kernels with closed-form expressions.
- **What evidence would resolve it:** Development of a non-recursive approximation or custom CUDA kernel that maintains $C^2$ continuity while lowering computational complexity.

### Open Question 2
- **Question:** Is there a theoretical grounding for the optimal grid range initialization, or can it be learned adaptively?
- **Basis in paper:** [inferred] The authors switch from $[-1, 1]$ to $[-10, 10]$ to solve instability, noting the original range is "problematic" but offering only empirical validation for the new range.
- **Why unresolved:** A fixed large range may not be optimal for all data distributions, and manual tuning lacks theoretical justification.
- **What evidence would resolve it:** An analysis showing the relationship between input data distribution and the necessary grid span, or the introduction of a learnable range parameter.

### Open Question 3
- **Question:** How sensitive is FR-KAN's performance to the choice of neuron group size ($h$) relative to the grid size ($G$)?
- **Basis in paper:** [inferred] The method uses neuron grouping to reduce parameters, but acknowledges this forces neurons to share activations, potentially limiting the "variety" required by the Kolmogorov-Arnold Theorem.
- **Why unresolved:** The trade-off between parameter efficiency (small $h$) and expressive power (large $h$) is not thoroughly ablated.
- **What evidence would resolve it:** Experiments varying $h$ independently of total parameter count to identify if a "capacity cliff" exists where grouping stifles learning.

## Limitations

- **Limited empirical scope:** While evaluated on 8 datasets spanning image, text, time series, multimodal, and function approximation tasks, the breadth of real-world applications tested remains narrow.
- **Hyperparameter sensitivity:** The paper does not specify optimal values for the regularization coefficient λ, the grouping factor h, or the grid range in most experiments.
- **Sorting gradient flow:** The mechanism of sorting learnable grid positions is described but not deeply analyzed for gradient stability or convergence properties.

## Confidence

- **High confidence:** The theoretical analysis of B-spline knot bounds (Theorem 4.3, 5.1) is rigorous and well-grounded. The parameter reduction mechanism via grouping is clearly explained and empirically validated.
- **Medium confidence:** The claim of improved training stability via second-derivative regularization is supported by Figure 4 (showing NaN loss at narrow grid ranges), but lacks direct comparison to other KAN variants or detailed ablation of λ.
- **Medium confidence:** The expressiveness gains from free knots are demonstrated on function approximation tasks, but the general advantage over other KAN variants (e.g., PRKAN, Sprecher Networks) is not directly tested.

## Next Checks

1. **Ablate regularization strength:** Systematically vary λ across a range (e.g., 0.01, 0.1, 1.0) on a challenging function approximation task (e.g., Feynman I.6.2) and measure both smoothness and accuracy to identify optimal trade-offs.
2. **Stress test sorting stability:** Train FR-KAN with varying numbers of neurons per group and monitor the distribution of $b_g$ values over training epochs; check for collapse or clustering that could indicate gradient instability.
3. **Direct KAN variant comparison:** Benchmark FR-KAN against PRKAN and Sprecher Networks on the same set of datasets (especially CIFAR10 and function approximation) to isolate the contribution of free knots versus other architectural innovations.