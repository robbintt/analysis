---
ver: rpa2
title: 'A Transformer-based survival model for prediction of all-cause mortality in
  heart failure patients: a multi-cohort study'
arxiv_id: '2503.12317'
source_url: https://arxiv.org/abs/2503.12317
tags:
- trisk
- maggic-ehr
- risk
- validation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRisk is a Transformer-based AI model that predicts 36-month mortality
  in heart failure patients by analyzing longitudinal electronic health records. Trained
  on 304,152 UK patients and validated on 99,382 independent patients, it achieved
  a concordance index of 0.845, significantly outperforming the MAGGIC-EHR model (0.728).
---

# A Transformer-based survival model for prediction of all-cause mortality in heart failure patients: a multi-cohort study

## Quick Facts
- arXiv ID: 2503.12317
- Source URL: https://arxiv.org/abs/2503.12317
- Reference count: 0
- TRisk achieves C-index 0.845 on UK validation data and 0.802 on US transfer learning for 36-month mortality prediction in heart failure patients

## Executive Summary
TRisk is a Transformer-based AI model that predicts 36-month mortality in heart failure patients by analyzing longitudinal electronic health records. Trained on 304,152 UK patients and validated on 99,382 independent patients, it achieved a concordance index of 0.845, significantly outperforming the MAGGIC-EHR model (0.728). The model demonstrated consistent performance across age, sex, and baseline characteristics, with reduced bias compared to conventional models. Transfer learning enabled successful adaptation to US hospital data (C-index 0.802). Explainability analysis revealed TRisk captured established risk factors while identifying underappreciated predictors like cancers and hepatic failure, with persistent prognostic value even a decade after cancer diagnosis.

## Method Summary
TRisk uses a Transformer encoder (6 layers, 6 heads, 150 hidden size) with three parallel embeddings for encounter codes, age at encounter, and visit number, concatenated with tanh nonlinearity. The model processes longitudinal EHR sequences up to 512 encounters per patient, using ICD-10 diagnoses, BNF/VTM medications, and OPCS procedures. It employs a SODEN (Survival ODE Network) framework for flexible hazard modeling with 48 monthly time bins and explicit calibration regularization (λ=2.0). The model was trained on CPRD Aurum data (304,152 derivation, 99,382 validation) and adapted to MIMIC-IV via transfer learning, achieving C-index of 0.845 and 0.802 respectively.

## Key Results
- C-index of 0.845 on UK validation cohort (99,382 patients) vs. 0.728 for MAGGIC-EHR model
- C-index of 0.802 on US transfer learning validation (MIMIC-IV) vs. 0.5 for random initialization
- Reduced bias across age, sex, and baseline characteristics compared to conventional models
- Identified cancers and hepatic failure as underappreciated risk factors with prognostic value persisting even a decade after cancer diagnosis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer architecture captures temporal dependencies in longitudinal EHR that conventional models miss.
- Mechanism: Self-attention learns which prior clinical encounters (diagnoses, medications, procedures) predict mortality, weighting their relative importance regardless of temporal distance. Unlike Cox models assuming fixed predictor relationships, attention dynamically adjusts feature importance per patient context.
- Core assumption: Medical history encodes prognostic signals that are recoverable through learned attention patterns.
- Evidence anchors: [abstract] "analysing temporal patient journeys – including diagnoses, medications, and procedures – up till baseline"; [Page 4] "Relative ordering and patient age at each encounter are also provided to the model, thereby providing rich longitudinal annotations"
- Break condition: If attention weights show no meaningful clinical patterns or if temporal sequence shuffling doesn't degrade performance, the mechanism is not operating as claimed.

### Mechanism 2
- Claim: SODEN (Survival ODE Network) framework provides more flexible hazard modeling than proportional hazards assumptions.
- Mechanism: Rather than assuming fixed hazard ratios over time, SODEN models the survival function through neural ODEs, allowing the baseline hazard to be learned non-parametrically. This accommodates non-proportional hazards common in complex multimorbidity.
- Core assumption: Heart failure mortality risk does not follow proportional hazards across all patient subgroups.
- Evidence anchors: [Page 21] "the SODEN framework alternatively poses MLE as a differential-equation constrained optimisation objective"; [Page 21] "more flexible (i.e., absent of strong structural assumptions of the shape of the survival/hazard distribution)"
- Break condition: If performance gains disappear when comparing against a well-tuned DeepSurv or other neural survival model without ODE formulation, the SODEN-specific contribution is questionable.

### Mechanism 3
- Claim: Transfer learning enables cross-healthcare-system adaptation despite coding vocabulary and population differences.
- Mechanism: Pre-training on large UK dataset learns general representations of clinical trajectories; fine-tuning on smaller US dataset adapts to local coding conventions (ICD-9→ICD-10 mapping, NDC→BNF/VTM mapping) and population characteristics.
- Core assumption: Prognostic patterns in heart failure are partially universal across healthcare systems.
- Evidence anchors: [Page 6] "transfer learning yielded a C-index of 0.802 (0.789, 0.816)" vs. 0.5 for random initialization on US data alone; [Page 23] Details code mapping across vocabularies for transfer
- Break condition: If transfer learning degrades performance vs. training from scratch on sufficient local data, or if domain shift is too large, mechanism fails.

## Foundational Learning

- Concept: **Self-attention and positional encoding**
  - Why needed here: TRisk uses attention to weight clinical encounters; positional encoding preserves sequence order. Without understanding this, you cannot debug why certain encounters receive high attention.
  - Quick check question: If you permute the order of a patient's encounters, what should happen to model output? (Answer: Should change substantially if positional encoding is working.)

- Concept: **Censored data in survival analysis**
  - Why needed here: Many patients haven't died by study end; the model must learn from incomplete outcomes. Standard classification approaches fail here.
  - Quick check question: Why can't you treat survival prediction as binary classification at 36 months? (Answer: Censored patients may die after 36 months—you don't know, and treating them as "survived" biases the model.)

- Concept: **Concordance index (C-index)**
  - Why needed here: Primary evaluation metric. Measures pairwise ranking correctness, not absolute probability accuracy.
  - Quick check question: A model with C-index 0.845 correctly ranks what fraction of comparable patient pairs? (Answer: ~84.5% of pairs where you can observe who died first.)

## Architecture Onboarding

- Component map: Raw EHR codes → vocabulary mapping (ICD-10, BNF/VTM, OPCS) → embedding → attention → [PRED] token representation → SODEN → survival probability at each time bin

- Critical path: Raw EHR codes → vocabulary mapping (ICD-10, BNF/VTM, OPCS) → embedding → attention → [PRED] token representation → SODEN → survival probability at each time bin

- Design tradeoffs:
  - Large hidden size (150) and 6 layers chosen for expressiveness vs. overfitting risk mitigated by 300K+ training samples
  - High dropout (0.3 hidden, 0.4 attention) reflects noise in EHR data
  - Maximum sequence length 512 balances capturing long histories vs. computational cost

- Failure signatures:
  - C-index ~0.5 on transfer data without fine-tuning indicates domain shift too large
  - Calibration curves systematically above/below diagonal indicate miscalibration
  - Attention weights concentrated on single encounter suggest attention collapse

- First 3 experiments:
  1. **Ablate temporal encoding**: Shuffle encounter order in validation data; expect C-index drop if temporal modeling matters.
  2. **Attention visualization**: Extract top-attended encounters for high-risk vs. low-risk patients; verify clinical plausibility against explainability results (cancers, hepatic failure should rank high).
  3. **Transfer boundary test**: Vary fine-tuning dataset size (1%, 5%, 10%, 20% of US data) to identify minimum data for effective transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the persistent prognostic value of cancer diagnoses observed even a decade before baseline reflect direct effects of the cancer itself or long-term toxicity from cancer therapies?
- Basis in paper: [explicit] The authors state: "This finding could reflect either direct effects of cancer or of cancer therapies - a distinction that warrants future research but aligns with current cardio-oncology literature."
- Why unresolved: The current study design cannot disentangle cancer-specific effects from treatment-related cardiotoxicity using EHR data alone.
- What evidence would resolve it: Linkage with detailed oncology treatment records (chemotherapy regimens, radiation fields) enabling stratified analysis by treatment exposure.

### Open Question 2
- Question: Would prospective clinical integration of TRisk into heart failure care pathways improve patient outcomes compared to standard risk assessment?
- Basis in paper: [inferred] The discussion describes potential clinical benefits throughout the HF care pathway, including prioritising medication over transplantation and enabling timely palliative care referrals, but no prospective trial data are presented.
- Why unresolved: Retrospective validation cannot establish whether acting on TRisk predictions changes management decisions or patient outcomes.
- What evidence would resolve it: Randomised controlled trial randomising practices or health systems to TRisk-guided vs. standard care, with outcomes including mortality, appropriate referrals, and healthcare utilisation.

### Open Question 3
- Question: What specific cohort or health system characteristics drive the performance difference between UK (C-index 0.845) and USA (C-index 0.802) validation cohorts?
- Basis in paper: [inferred] The transfer learning approach reduced performance by ~0.04 in C-index. While the paper demonstrates successful transfer, it does not decompose whether this reflects population differences, coding practices, care setting (primary vs. hospital-based), or other factors.
- Why unresolved: The analysis pools multiple sources of variation without isolating individual contributions to performance degradation.
- What evidence would resolve it: Systematic ablation study varying one dimension at a time (e.g., UK primary care vs. UK hospital data; same coding system across different populations) to quantify each factor's contribution.

### Open Question 4
- Question: Would incorporating left ventricular ejection fraction measurements into TRisk further improve discrimination beyond the current model?
- Basis in paper: [inferred] The MAGGIC-EHR benchmark lacked LVEF data, and the paper acknowledges this as a limitation. TRisk achieved superior performance despite not using LVEF, but it remains unknown whether adding this specialised test would provide incremental value.
- Why unresolved: LVEF was unavailable in the routine EHR data used; its potential contribution to TRisk was not tested.
- What evidence would resolve it: Validation in a cohort with linked echocardiography data, comparing TRisk with and without LVEF as an additional input feature.

## Limitations

- Performance claims based on internal validation splits rather than truly external cohorts; US validation uses subset of MIMIC-IV data rather than independent hospital system
- Explainability analysis relies on integrated gradients which provides correlation but not causal inference for "underappreciated" risk factors
- Model cannot distinguish whether persistent cancer prognostic value reflects cancer itself vs. long-term treatment toxicity

## Confidence

- **High confidence**: Transformer architecture achieving C-index 0.845 on UK validation data; transfer learning working (0.802 C-index on US data); superior performance to MAGGIC-EHR baseline
- **Medium confidence**: Clinical interpretability of attention patterns; generalizability to truly external healthcare systems; temporal persistence findings
- **Low confidence**: Causal interpretation of underappreciated risk factors; absolute performance difference attributable to Transformer vs. broader feature engineering

## Next Checks

1. **External validation on truly independent US health system data**: Test TRisk on EHR from a different US health system than MIMIC-IV to assess generalizability beyond transfer learning from UK to one US dataset.

2. **Architecture ablation study**: Compare TRisk against identical survival model using CNN or RNN backbone to isolate Transformer-specific contribution to the C-index improvement.

3. **Temporal validation stability**: Evaluate model performance when trained on older data (2005-2015) and tested on more recent data (2016-2020) to assess temporal robustness and potential dataset shift over time.