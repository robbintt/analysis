---
ver: rpa2
title: 'Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for
  Invoice Processing'
arxiv_id: '2509.04469'
source_url: https://arxiv.org/abs/2509.04469
tags:
- native
- document
- processing
- docling
- gemini-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks eight multi-modal large language models
  from three families (GPT-5, Gemini 2.5, and Gemma 3) on three diverse invoice datasets
  using zero-shot prompting. The study compares two processing strategies: direct
  image processing using multi-modal capabilities and a structured parsing approach
  that converts documents to markdown first using Docling.'
---

# Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing

## Quick Facts
- arXiv ID: 2509.04469
- Source URL: https://arxiv.org/abs/2509.04469
- Reference count: 40
- Eight multi-modal LLMs consistently outperform Docling-based text parsing on invoice extraction across three datasets

## Executive Summary
This paper benchmarks eight multi-modal large language models on three invoice datasets, comparing direct image processing against structured markdown conversion using Docling. Native image processing consistently outperforms the structured approach, with Gemini 2.5 Pro achieving the highest overall accuracy. The study identifies IBAN extraction as particularly challenging due to OCR-related character confusion. The benchmark provides insights for selecting appropriate models and processing strategies for automated document systems.

## Method Summary
The study compares eight multi-modal LLMs (GPT-5, Gemini 2.5, Gemma 3) on three invoice datasets using zero-shot prompting. Two processing strategies are evaluated: direct image processing using native multi-modal capabilities and structured parsing that converts documents to markdown first using Docling. Both approaches aim to extract invoice fields into JSON format, which is then compared to ground truth annotations using exact-match accuracy after minimal normalization.

## Key Results
- Native image processing outperforms Docling-based structured parsing across all models and datasets
- Gemini 2.5 Pro achieves highest accuracy: 87.46% on scanned receipts, 96.50% on clean invoices, 92.71% on scanned invoices
- Smaller models like gemma-3-4b-it struggle with direct image analysis but match larger models with Docling
- IBAN fields present particular challenges due to OCR-related character confusion (0 vs O vs U)

## Why This Works (Mechanism)

### Mechanism 1: Visual Context Preservation
Direct image processing outperforms structured parsing because multi-modal models leverage spatial layout and visual relationships that are lost during markdown conversion. Native image input allows the model to simultaneously process text, layout, and spatial cues, while Docling conversion flattens this into linear markdown, discarding positional information that helps distinguish header fields from line items.

### Mechanism 2: Pipeline Bottleneck Effect
The Docling-to-markdown conversion creates an upstream error bottleneck that limits downstream LLM reasoning regardless of model capability. Docling's OCR and layout analysis errors propagate into the markdown, compressing all models—regardless of size or architecture—into similar performance bands.

### Mechanism 3: Alphanumeric Pattern Confusion Under Visual Noise
Unstructured alphanumeric fields (e.g., IBANs) remain difficult even for state-of-the-art multi-modal models due to visual similarity between characters. IBANs lack semantic context and rely on precise character recognition, making them particularly vulnerable to OCR-related errors.

## Foundational Learning

- **Concept: Zero-Shot Prompting for Structured Extraction**
  - Why needed here: All experiments use zero-shot prompting to elicit JSON outputs without task-specific training. Understanding how to design prompts that produce consistent schemas is critical for reproducibility.
  - Quick check question: Can you write a prompt that asks an LLM to extract invoice fields into a specific JSON schema without providing examples?

- **Concept: Multi-Modal Tokenization and Resolution**
  - Why needed here: Native image processing requires the model to map visual regions to textual tokens. The performance gap suggests this internal resolution is more reliable than external OCR for complex layouts.
  - Quick check question: How does a vision-language model convert an image patch into tokens that can be processed alongside text?

- **Concept: Error Cascade in Multi-Stage Pipelines**
  - Why needed here: The Docling → LLM pipeline demonstrates how upstream errors compound. Recognizing where information loss occurs helps diagnose whether to blame the parser or the model.
  - Quick check question: If a markdown table is misaligned during conversion, can a downstream LLM detect and correct the error?

## Architecture Onboarding

- **Component map:** Invoice images → Strategy A (Native): Image → Multi-modal LLM → JSON OR Strategy B (Docling): Image → Docling OCR/markdown → Text-based LLM → JSON → Evaluation: Normalized field matching against ground truth

- **Critical path:**
  1. Dataset selection and annotation verification (authors corrected inconsistent labels in inv-cdip)
  2. Prompt design for each dataset's schema
  3. Strategy selection (native vs. Docling)
  4. Model inference with zero-shot prompting
  5. Post-processing: minimal normalization (dates, whitespace), exact match evaluation

- **Design tradeoffs:**
  - Native processing: Higher accuracy, preserves visual context, but may have higher latency/cost and requires robust multi-modal models
  - Docling processing: Lower accuracy, introduces bottleneck, but standardizes input format and may work with smaller text-only models
  - Model size: Larger models outperform smaller variants, but gemma-3-12b-it offers competitive cost-performance balance

- **Failure signatures:**
  - IBAN field accuracy drops below 50% even for top models—indicates alphanumeric confusion
  - Small open-source models (gemma-3-4b-it) collapse on clean invoices with native processing (45.69%) but match larger models with Docling (84.59%)—suggests vision capabilities are undertrained
  - Docling compresses all models to 84-85% on clean invoices—sign of upstream bottleneck

- **First 3 experiments:**
  1. Replicate the native vs. Docling comparison on a held-out invoice sample from your domain to confirm the gap persists.
  2. Test IBAN-specific post-processing: extract raw strings, apply checksum validation, and compare correction rates across models.
  3. Ablate model size within one family (e.g., Gemini Flash vs. Flash-Lite vs. Pro) on your noisiest document class to identify the cost-accuracy frontier.

## Open Questions the Paper Calls Out

1. **PDF vs. Image Performance:** How does LLM performance on invoice extraction differ when processing native digital PDFs compared to scanned document images? The study was restricted to image-based datasets, lacking a benchmark for documents with native text layers.

2. **Fine-Tuned Model Comparison:** Can fine-tuning specialized layout models (e.g., LayoutLM, LiLT) outperform the zero-shot capabilities of general-purpose multi-modal LLMs? The current benchmark focused exclusively on zero-shot prompting for general LLMs, excluding models that require fine-tuning.

3. **Training Data Memorization:** To what extent does training data memorization inflate the accuracy scores of proprietary models on open-source invoice datasets? The authors acknowledge it's impossible to verify if high performance results from generalization or memorization without access to proprietary training logs.

4. **IBAN Extraction Improvements:** What specific architectural or prompting improvements are required to resolve OCR confusion in unstructured alphanumeric fields like IBANs? The paper identifies the failure mode but doesn't test methods to mitigate it.

## Limitations

- Limited model diversity with only eight models from three families tested
- Dataset representativeness restricted to specific invoice types and quality levels
- Zero-shot methodology constraints that don't explore few-shot or fine-tuned approaches
- Exact-match accuracy metric may not reflect real-world requirements for approximate matches

## Confidence

- **High Confidence:** Native image processing consistently outperforms Docling-based structured parsing across all models and datasets
- **Medium Confidence:** IBAN extraction challenges due to OCR-related character confusion, though corpus validation is limited
- **Low Confidence:** gemma-3-12b-it provides best cost-performance balance based on single data point without systematic cost analysis

## Next Checks

1. Apply the same benchmark methodology to at least two additional invoice datasets from different domains to verify native processing advantage persists across document types.

2. Implement post-processing validation using IBAN checksum algorithms and measure reduction in false positives/negatives compared to pure processing methods.

3. Design hybrid pipeline experiments combining external OCR with configurable quality followed by multi-modal LLM processing to identify optimal preprocessing accuracy threshold.