---
ver: rpa2
title: 'NGA: Non-autoregressive Generative Auction with Global Externalities for Advertising
  Systems'
arxiv_id: '2506.05685'
source_url: https://arxiv.org/abs/2506.05685
tags:
- auction
- wang
- advertising
- externalities
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NGA (Non-autoregressive Generative Auction with global externalities)
  addresses key limitations in generative auction frameworks for online advertising
  by explicitly modeling global externalities, employing a non-autoregressive and
  constraint-based decoding mechanism, and using a parallel multi-tower evaluator
  for efficient reward and payment computation. The proposed approach significantly
  outperforms state-of-the-art baselines, achieving a 2.2% increase in RPM, 1.8% increase
  in CTR, and 6.4% increase in CVR in offline experiments.
---

# NGA: Non-autoregressive Generative Auction with Global Externalities for Advertising Systems

## Quick Facts
- arXiv ID: 2506.05685
- Source URL: https://arxiv.org/abs/2506.05685
- Reference count: 33
- Primary result: 2.2% RPM increase, 1.8% CTR increase, 6.4% CVR increase offline; 1.9% RPM, 1.6% CTR, 0.8% CVR online

## Executive Summary
NGA introduces a non-autoregressive generative auction framework that explicitly models global externalities in online advertising by jointly capturing relationships among ads and effects of adjacent organic content. The approach addresses key limitations of prior auctions by using a dual-encoder architecture with cross-attention to model cross-content externalities, a non-autoregressive constrained decoding mechanism for efficient parallel generation, and a parallel multi-tower evaluator for unified reward and payment computation. The framework achieves significant performance improvements over state-of-the-art baselines while maintaining real-time inference capabilities.

## Method Summary
NGA operates in a two-stage process: first training an Evaluator to predict pCTR, pCVR, and payment ratios using a shared list encoder and three parallel MLP towers, then freezing the Evaluator to train a Generator via policy gradient. The Generator uses an Item Encoder and Position Encoder with cross-attention to produce an allocation probability matrix, which is decoded using per-position constrained decoding to satisfy business rules. The framework jointly processes 30 candidate ads and 20 organic items to select 10 slots, optimizing for RPM and order volume while maintaining incentive compatibility and individual rationality through regret penalties.

## Key Results
- Offline experiments: 2.2% RPM increase, 1.8% CTR increase, 6.4% CVR increase over state-of-the-art baselines
- Online A/B testing: 1.9% RPM increase, 1.6% CTR increase, 0.8% CVR increase with 6.5% response time reduction
- Maintains incentive compatibility (IC regret Ψ close to zero) and individual rationality (IR) properties
- Demonstrates consistent improvements across key business metrics in industrial LBS platform environment

## Why This Works (Mechanism)

### Mechanism 1: Global Context Embedding via Cross-Attention
Integrating organic content embeddings into allocation allows modeling cross-content externalities, addressing "evaluation-before-ranking" limitations. The dual-encoder architecture uses cross-attention between item and position embeddings to adjust placement scores based on neighboring organic content rather than scoring ads in isolation.

### Mechanism 2: Non-Autoregressive Constrained Decoding
Decoupling sequence generation from dependency modeling enables parallel decoding, reducing inference latency compared to autoregressive approaches. NGA computes an allocation probability matrix in a single pass using per-position constrained decoding with business rules, approximating optimal lists through argmax selection rather than sequential conditional probability modeling.

### Mechanism 3: Parallel Multi-Tower Reward Estimation
Replacing serial evaluation with parallel multi-tower architecture enables unified list-wise reward and payment computation without computational bottlenecks. The Evaluator uses a shared List Encoder followed by three parallel MLP towers to simultaneously predict pCTR, pCVR, and payment ratios, bypassing iterative evaluation loops.

## Foundational Learning

- **Auction Externalities**: Why needed - NGA's value proposition is modeling "global externalities" where ad slot value is enhanced or diminished by surrounding ads and organic content. Quick check - Explain why GSP auction fails to capture ad-organic item interactions.

- **Non-Autoregressive Generation**: Why needed - NGA claims efficiency gains by moving away from autoregressive models. Quick check - How does per-position constrained decoding ensure valid outputs without sequential masking?

- **Incentive Compatibility (IC) & Individual Rationality (IR)**: Why needed - Framework must maintain economic properties while optimizing revenue. Quick check - In loss function, what does λᵢcrgtᵢ represent and why is it necessary for valid auction mechanism?

## Architecture Onboarding

- **Component map**: Item Encoder + Position Encoder -> Alloc Prob Matrix (Z) -> Constrained Decoder -> Candidate Lists; List Encoder -> Tower 1: pCTR, Tower 2: pCVR, Tower 3: Pay Ratio -> Aggregated Reward (R)
- **Critical path**: Transition from soft allocation matrix to hard constrained decoding differentiates from standard generative models; hidden score (xᵀt) must align with bid/CTR metrics for coherent list selection
- **Design tradeoffs**: Global vs. local context sacrifices sequential dependency modeling for 6.5% latency reduction; constraint handling hard-codes business rules rather than learning to avoid violations
- **Failure signatures**: Empty/trivial lists if constraints too strict; position bias mismatch if Position Encoder embeddings don't generalize to new slot configurations
- **First 3 experiments**: 1) Ablate organic items to quantify global vs local externalities lift, 2) Profile latency vs AR baseline as candidate set scales, 3) Validate IC regret is close to zero on held-out test set

## Open Questions the Paper Calls Out
- Generalization to advertising channels beyond LBS like search or social feed environments
- Extension to support more complex business constraints than current per-position rules
- Impact on performance if fixed internal order for organic content is relaxed

## Limitations
- Experiments limited to single commercial LBS platform with unique spatial/user-behavior characteristics
- Constraint specification opacity with no sensitivity analysis provided
- Payment mechanism verification lacks formal proof or extensive validation across market conditions

## Confidence
- **High confidence**: Non-autoregressive architecture achieving 6.5% latency reduction; parallel multi-tower evaluator design; two-stage training procedure
- **Medium confidence**: 2.2% RPM improvement claim; global externalities mechanism providing meaningful lift; policy gradient stability
- **Low confidence**: Generalization to different platform architectures; performance with different candidate set sizes; IC regret values across market regimes

## Next Checks
1. Compute IC regret cross-validation on held-out test sets across different market segments (high-value vs low-value ad categories)
2. Systematically remove organic items from candidate set in offline experiments to quantify externality modeling contribution
3. Run controlled experiments varying constraint strictness to identify threshold where performance gains diminish