---
ver: rpa2
title: 'Veila: Panoramic LiDAR Generation from a Monocular RGB Image'
arxiv_id: '2508.03690'
source_url: https://arxiv.org/abs/2508.03690
tags:
- lidar
- diffusion
- semantic
- generation
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Veila, the first diffusion framework for
  generating panoramic LiDAR scenes from a monocular RGB image. It addresses three
  key challenges: spatially varying reliability of semantic and depth cues from RGB,
  modality gaps between RGB appearance and LiDAR geometry, and maintaining structural
  coherence across the panoramic LiDAR.'
---

# Veila: Panoramic LiDAR Generation from a Monocular RGB Image

## Quick Facts
- arXiv ID: 2508.03690
- Source URL: https://arxiv.org/abs/2508.03690
- Authors: Youquan Liu; Lingdong Kong; Weidong Yang; Ao Liang; Jianxiong Gao; Yang Wu; Xiang Xu; Xin Li; Linfeng Li; Runnan Chen; Ben Fei
- Reference count: 12
- Primary result: Introduces the first diffusion framework for generating panoramic LiDAR scenes from a monocular RGB image, achieving state-of-the-art performance in fidelity and cross-modal consistency.

## Executive Summary
This paper introduces Veila, the first diffusion framework for generating panoramic LiDAR scenes from a monocular RGB image. It addresses three key challenges: spatially varying reliability of semantic and depth cues from RGB, modality gaps between RGB appearance and LiDAR geometry, and maintaining structural coherence across the panoramic LiDAR. Veila introduces three core components: a Confidence-Aware Conditioning Mechanism (CACM) that adaptively fuses semantic and depth features based on local reliability; a Geometric Cross-Modal Alignment (GCMA) module that leverages epipolar geometry for robust RGB-LiDAR alignment under noisy diffusion; and a Panoramic Feature Coherence (PFC) strategy that enforces global structural consistency across the LiDAR panorama. The method is evaluated on nuScenes, SemanticKITTI, and a newly proposed KITTI-Weather benchmark, achieving state-of-the-art performance in fidelity and cross-modal consistency. It also demonstrates improved downstream LiDAR semantic segmentation through generative data augmentation.

## Method Summary
Veila is a conditional diffusion model that generates panoramic LiDAR range images (depth + intensity) from a monocular RGB image. The model uses a U-Net backbone with three novel components: CACM fuses frozen semantic and depth encoder features using learned confidence weights; GCMA projects sampled 3D rays to RGB via epipolar geometry and retrieves features through cross-attention; PFC adds global self-attention at the deepest layer to ensure panoramic coherence. The model is trained with standard DDPM objectives on datasets like SemanticKITTI and nuScenes, with evaluation using FRD, FPD, JSD, MMD metrics and proposed cross-modal consistency measures.

## Key Results
- Achieves state-of-the-art performance in Fréchet Point Cloud Distance (FPD) and Fréchet Range Image Distance (FRD) compared to existing LiDAR generation methods
- Demonstrates improved downstream LiDAR semantic segmentation performance through generative data augmentation
- Introduces the KITTI-Weather benchmark for evaluating LiDAR generation under adverse weather conditions

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Reliability Weighting (CACM)
The Confidence-Aware Conditioning Mechanism calculates spatial confidence scores for semantic and depth features independently, normalizing these scores to weight the feature maps before fusion. This prioritizes semantic cues in textured regions and depth cues in geometric areas. The core assumption is that local feature statistics correlate directly with ground-truth reliability. If confidence estimators hallucinate high confidence in corrupt areas, the fusion will amplify noise.

### Mechanism 2: Geometry-Guided Feature Retrieval (GCMA)
The Geometric Cross-Modal Alignment module decouples alignment from noisy 3D coordinates by using fixed LiDAR ray directions and epipolar geometry to sample features from the RGB image at multiple depths along the ray. This aggregates them with depth-aware weights. The assumption is that camera extrinsic and intrinsic matrices are accurate and constant. If camera-LiDAR extrinsics are mis-calibrated, epipolar constraints will retrieve features from incorrect RGB regions, causing structural misalignment.

### Mechanism 3: Global Context Propagation (PFC)
The Panoramic Feature Coherence strategy injects global self-attention at the deepest U-Net bottleneck, allowing features from the observed front-view to attend to unobserved rear-view features. This enforces global structural consistency without explicit supervision for the rear. The assumption is that scene semantics and geometry are spatially correlated, allowing front-view context to logically inform rear-view generation. In highly dynamic or disjoint scenes, propagating context globally may cause semantic bleeding or unrealistic merging of separate structures.

## Foundational Learning

- **Concept: Range View Representation**
  - Why needed: Veila generates LiDAR as 2D range images rather than raw point clouds to leverage efficient 2D U-Net diffusion architectures
  - Quick check: How does spherical projection map a 3D point $(x, y, z)$ to the 2D range image coordinates $(u, v)$?

- **Concept: Epipolar Geometry**
  - Why needed: Essential for GCMA to understand how a 3D ray corresponds to a line in the 2D image plane for debugging feature retrieval
  - Quick check: Why does the GCMA module sample along the ray direction rather than projecting a single estimated point?

- **Concept: Conditional Diffusion (DDPM)**
  - Why needed: The underlying generative engine is a U-Net denoising network conditioned on RGB features
  - Quick check: In the training objective (Eq. 3), what does the network $\epsilon_\theta$ predict, and how do $I$ and $t$ condition this prediction?

## Architecture Onboarding

- **Component map:** Frozen Semantic ($E_s$) and Depth ($E_d$) Encoders -> CACM (Adaptive Fusion) -> Standard Diffusion U-Net (Down/Up blocks) -> GCMA (Cross-Attention) using Ray projection -> PFC (Global Self-Attention) at bottleneck -> Denoised Range Image -> Spherical Unprojection -> Point Cloud

- **Critical path:** The GCMA module is the most complex integration point. You must verify that the ray-to-RGB projection (Eq. 2 + Eq. 7) correctly indexes the RGB feature map $F^{(i)}$ before passing features to cross-attention.

- **Design tradeoffs:** PFC vs. Memory - global self-attention has quadratic memory cost, so it's placed only at the deepest layer to manage VRAM. Frozen Encoders - $E_s$ and $E_d$ are frozen to prevent overfitting on limited LiDAR data, trading off domain-specific adaptability for general feature richness.

- **Failure signatures:** "Ghost" Objects caused by misalignment in GCMA (bad extrinsics), resulting in LiDAR points that don't match RGB objects. Rear-View Disconnect if PFC fails or is removed, causing rear point cloud to look realistic locally but fail to connect geometrically with the front. Conditional Ignorance if CACM weights are uniform, leading to smooth but semantically wrong shapes.

- **First 3 experiments:**
  1. **Sanity Check (Overfit):** Train on a single scene. The model should reconstruct the exact range image with zero noise to verify U-Net and GCMA capacity.
  2. **Ablation on CACM:** Run inference using only semantic features vs. only depth features vs. CACM fusion to visualize spatial reliability handling.
  3. **Rear-View Consistency:** Generate a full panorama and quantitatively compare FPD of the front half vs. rear half to verify PFC propagation.

## Open Questions the Paper Calls Out

- Can Veila maintain its cross-modal alignment and structural fidelity when applied to real-world adverse weather scenarios, rather than simulated benchmarks? The authors note that current datasets contain limited RGB-LiDAR pairs under adverse weather conditions, making real-world applicability unverified.

- Does joint fine-tuning of the semantic and depth encoders improve generation quality, or is the risk of overfitting on limited LiDAR data too high? The paper freezes encoders to mitigate overfitting but doesn't explore if partially fine-tuned encoders could better bridge domain gaps.

- How robust is the GCMA module to significant changes in sensor calibration parameters or LiDAR vertical field-of-view at inference time? The module relies heavily on fixed camera matrices and LiDAR spherical projection formulas, suggesting dependency on static sensor geometries not tested across varying configurations.

## Limitations

- The effectiveness of confidence weighting depends on whether learned confidence scores correlate with actual feature reliability - this correlation is assumed but not empirically validated beyond qualitative examples.
- The epipolar geometry approach in GCMA is principled but its robustness to high diffusion noise and calibration errors needs more systematic evaluation.
- The evaluation focuses on synthetic weather variations rather than true sensor degradation from fog, rain, or snow.

## Confidence

- **High Confidence:** The fundamental problem framing (RGB-LiDAR modality gap) and architectural integration of CACM, GCMA, and PFC components are well-justified and technically sound.
- **Medium Confidence:** The effectiveness of the confidence weighting scheme depends on correlation between learned confidence scores and actual feature reliability.
- **Medium Confidence:** The epipolar geometry approach in GCMA is principled but needs more systematic evaluation of robustness to noise and calibration errors.

## Next Checks

1. **Calibration Sensitivity Analysis:** Systematically evaluate Veila's performance as camera-LiDAR extrinsic error increases from 1cm to 10cm in translation and rotation, quantifying the breakdown point for each component.

2. **Cross-Dataset Generalization:** Train on SemanticKITTI and test on nuScenes (or vice versa) without fine-tuning to assess whether learned reliability weighting and cross-modal alignment generalize across different sensor configurations.

3. **Dynamic Scene Robustness:** Create test scenarios with moving objects that appear in RGB but not in the corresponding LiDAR frame, and evaluate whether Veila hallucinates phantom objects or appropriately handles temporal misalignment.