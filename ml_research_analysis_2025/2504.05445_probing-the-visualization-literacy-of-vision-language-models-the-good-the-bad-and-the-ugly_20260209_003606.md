---
ver: rpa2
title: 'Probing the Visualization Literacy of Vision Language Models: the Good, the
  Bad, and the Ugly'
arxiv_id: '2504.05445'
source_url: https://arxiv.org/abs/2504.05445
tags:
- chart
- reasoning
- vlms
- visualization
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the visualization literacy of Vision Language
  Models (VLMs) by adapting Attention-Guided Grad-CAM (AG-CAM) to visualize their
  internal reasoning when answering chart questions. The study compares four open-source
  models (ChartGemma, Janus 1B/7B, and LLaVA) and two closed-source models (GPT-4o,
  Gemini) on chart QA tasks using mini-VLAT benchmarks.
---

# Probing the Visualization Literacy of Vision Language Models: the Good, the Bad, and the Ugly

## Quick Facts
- arXiv ID: 2504.05445
- Source URL: https://arxiv.org/abs/2504.05445
- Reference count: 40
- Key outcome: This paper adapts Attention-Guided Grad-CAM to visualize VLM reasoning on chart QA tasks, revealing that chart-specific fine-tuning improves spatial and semantic reasoning, with ChartGemma outperforming other open-source models and matching larger closed-source models.

## Executive Summary
This paper introduces a visualization technique called Attention-Guided Grad-CAM (AG-CAM) to probe how Vision Language Models (VLMs) reason about charts and visualizations. The authors adapt AG-CAM to work with VLMs by computing attention weights weighted by gradients from the model's output logits, creating attention-saliency maps that show which image regions and text tokens most influence the model's response. The study compares four open-source VLMs (ChartGemma, Janus 1B/7B, LLaVA) and two closed-source models (GPT-4o, Gemini) on chart QA tasks using mini-VLAT benchmarks. ChartGemma, a 3B parameter model fine-tuned specifically for chart question-answering, demonstrated superior performance among open-source models and matched the capabilities of much larger closed-source models, suggesting that domain-specific fine-tuning is more important than raw model size for visualization literacy.

## Method Summary
The authors adapted Attention-Guided Grad-CAM (AG-CAM) to visualize VLM reasoning on chart question-answering tasks. The method computes attention weights from self-attention layers, then weights these by gradients from a backward pass that aggregates token-level logits (y = Σ max logits). This creates attention-saliency maps showing which input features most influence model responses. The study evaluated four open-source VLMs (ChartGemma, Janus 1B/7B, LLaVA) and two closed-source models (GPT-4o, Gemini) on mini-VLAT benchmarks containing 12 chart types and 12 questions. ChartGemma was specifically fine-tuned for chart QA tasks, while other models were general-purpose VLMs with vision capabilities.

## Key Results
- ChartGemma, a 3B parameter VLM fine-tuned for chart QA, outperformed other open-source models and matched the performance of much larger closed-source models like GPT-4o and Gemini
- VLMs exhibit spatial reasoning by accurately localizing key chart features and semantic reasoning by associating visual elements with corresponding data values and query tokens
- VLM reasoning can align with human reasoning patterns, particularly when models are fine-tuned on chart QA tasks
- Early-fusion VLMs show focused attention on chart elements when fine-tuned, while general-purpose VLMs tend to focus more on text elements like axis labels and legends

## Why This Works (Mechanism)

### Mechanism 1: Attention-Guided Gradient Weighting for QA
- Claim: Weighting self-attention matrices by gradients reveals which input features most influence the model's response
- Mechanism: The method computes attention weights F for each layer and head, then multiplies them by gradients from a backward pass that aggregates token-level logits (Eq. 3: y = Σ max logits). The element-wise product with ReLU-gradients yields an attention-saliency map localizing important regions
- Core assumption: Gradient magnitude correlates with feature importance for the generated output
- Evidence anchors: The abstract states the method visualizes "influence and importance of input features (image and text) on model responses"; section 4.1.1 explains treating response generation as multiple classification tasks

### Mechanism 2: Early-Fusion Self-Attention for Cross-Modal Binding
- Claim: Concatenating adapted image embeddings with text tokens enables self-attention layers to learn relationships between visual and textual elements
- Mechanism: Vision encoder processes image patches into embeddings; an adapter aligns these to the language model's input space. The fused sequence enters transformer blocks where self-attention computes weighted relationships across all tokens
- Core assumption: Self-attention on a unified token sequence suffices for cross-modal reasoning
- Evidence anchors: The abstract notes VLMs "exhibit spatial reasoning by accurately localizing key chart features"; section 3.1.2 explains how self-attention layers can learn relationships between text and image tokens

### Mechanism 3: Domain-Specific Fine-Tuning Strengthens Feature Binding
- Claim: Fine-tuning on chart QA tasks improves the alignment between query tokens and relevant visual encodings
- Mechanism: ChartGemma (3B), fine-tuned on chart QA, shows stronger associations between query terms and chart elements compared to general-purpose VLMs of similar size
- Core assumption: The fine-tuning dataset contains sufficient diversity and quality to teach proper visual-textual alignment
- Evidence anchors: The abstract states ChartGemma "outperformed other open-source models"; section 5.1.2 notes ChartGemma "focuses more on visual encodings" due to its fine-tuning

## Foundational Learning

- Concept: Vision Transformer (ViT) Patch Embeddings
  - Why needed here: Understanding how images become token-like sequences is essential for interpreting AG-CAM outputs at the patch level
  - Quick check question: Given a 224×224 image with 16×16 patches, how many patch tokens does ViT produce?

- Concept: Self-Attention as Dynamic Weighted Routing
  - Why needed here: AG-CAM builds on attention to show information flow; you must understand what attention matrices represent
  - Quick check question: In a 12-token sequence, what is the shape of the attention weight matrix for one head?

- Concept: Gradient-Based Attribution Methods
  - Why needed here: AG-CAM extends Grad-CAM by weighting attention with gradients; grasping the intuition prevents misinterpreting saliency maps
  - Quick check question: Why does multiplying attention by gradients (vs. attention alone) better indicate influence on the output?

## Architecture Onboarding

- Component map: Vision Encoder (SigLIP/CLIP) -> Adapter -> Language Model (Gemma/LLaVA) -> Tokenizer -> AG-CAM Module
- Critical path: 1) Preprocess image → resize, split into patches → flatten 2) Vision encoder forward pass → raw image embeddings 3) Adapter forward pass → adapted image embeddings 4) Tokenize query → text token embeddings 5) Concatenate adapted image + text tokens → fused input 6) LM forward (multi-layer self-attention) → logits 7) Compute y = Σ max logits; backward pass → gradients 8) For each layer [start, end]: compute Lq = Σ F ⊙ ReLU(∂y/∂F) 9) Normalize, reshape to image dimensions, overlay as heatmap
- Design tradeoffs: Softmax vs. sigmoid normalization (softmax gives focused localization; sigmoid finds more correlations but introduces noise on charts); layer aggregation (summing captures distributed reasoning; multiplication can dilute signal); early vs. deep fusion (early fusion is simpler and more efficient but lacks explicit cross-attention)
- Failure signatures: Diffuse saliency with correct answer (model relies on prior knowledge rather than visual evidence); strong text focus, weak encoding focus (insufficient cross-modal binding); correct localization but wrong answer (extraction or reasoning error); multi-step reasoning failure (saliency scattered across irrelevant regions)
- First 3 experiments: 1) Reproduce ChartGemma AG-CAM on Mini-VLAT Q2 (bar chart) and verify that "Japan" token attends to the correct bar 2) Compare softmax vs. sigmoid normalization on a pie chart (Q5) 3) Run the same query with modified prompts on line/area charts to observe attention-saliency shifts

## Open Questions the Paper Calls Out

- Can the AG-CAM methodology be adapted to visualize internal reasoning in deep fusion VLM architectures? The authors note comparing early fusion and deep fusion models was "intractable in the scope of this work" because DF models "operate in different ways and would require different treatments."
- How can the interpretation of attention-saliency maps be standardized or quantified to move beyond subjective analysis? The authors state that interpreting these maps "can be more of an art than a science" and acknowledge that "defining and especially quantifying 'reasoning' remains an open problem."
- How does VLM visualization literacy generalize to complex, real-world chart scenarios beyond the standardized VLAT test suite? The authors suggest "future research could incorporate more complex, real-world chart scenarios" as the current test suite "may not fully capture the nuances."

## Limitations

- The study uses a limited benchmark (mini-VLAT) that may not generalize to real-world visualization literacy scenarios
- The AG-CAM method's adaptation from classification to QA tasks assumes summed token logits adequately represent multi-word responses without validation
- The comparison conflates model size, architecture, and training regime differences, making it difficult to isolate performance drivers

## Confidence

- **High Confidence**: ChartGemma's superior performance on open-source models and its ability to match larger closed-source models in visualization literacy tasks
- **Medium Confidence**: The general pattern that VLMs exhibit both spatial and semantic reasoning as revealed by AG-CAM
- **Low Confidence**: The claim that VLM reasoning can fully align with human reasoning patterns

## Next Checks

1. Systematically compare AG-CAM outputs using different gradient aggregation methods (summed token logits vs. individual token gradients vs. sequence-level gradients) on the same chart QA tasks to determine which approach most accurately reflects feature importance.

2. Design chart QA queries that require combining information from multiple visual encodings to test whether AG-CAM reveals genuine cross-modal reasoning or superficial pattern matching.

3. Conduct a controlled experiment varying query formulations on identical chart images to quantify how much AG-CAM visualizations and model accuracy depend on prompt structure rather than visual understanding.