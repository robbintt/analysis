---
ver: rpa2
title: Sampling and Loss Weights in Multi-Domain Training
arxiv_id: '2511.06913'
source_url: https://arxiv.org/abs/2511.06913
tags:
- weights
- sampling
- domain
- data
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work studies two distinct types of domain weighting in multi-domain
  training: loss weights and sampling weights. Loss weights adjust how much each domain
  contributes to the empirical risk, reducing the generalization gap, while sampling
  weights determine how often data from each domain is sampled during optimization,
  reducing gradient variance.'
---

# Sampling and Loss Weights in Multi-Domain Training

## Quick Facts
- arXiv ID: 2511.06913
- Source URL: https://arxiv.org/abs/2511.06913
- Authors: Mahdi Salmani; Pratik Worah; Meisam Razaviyayn; Vahab Mirrokni
- Reference count: 40
- Primary result: Domain weighting operates in two dimensions—sampling weights reduce gradient variance while loss weights improve generalization; combining them yields complementary benefits.

## Executive Summary
This work systematically disentangles two distinct types of domain weighting in multi-domain training: loss weights and sampling weights. While previous work often conflated these, the authors show they address fundamentally different challenges. Loss weights optimize the generalization gap by appropriately weighting domains based on noise characteristics, following from generalized least squares theory. Sampling weights accelerate optimization by reducing gradient variance through adaptive data sampling. Through theoretical analysis and extensive experiments on linear, logistic, and neural network models, the paper demonstrates that both weight types independently improve performance, with further gains when combined. This reveals domain weighting as inherently two-dimensional rather than a single scalar problem.

## Method Summary
The method introduces two complementary weighting mechanisms for multi-domain training. For loss weights, it proposes One-shot FGLS (Feasible Generalized Least Squares) which periodically estimates domain-specific noise levels on a held-out set and updates weights inversely proportional to these estimates (w_i ∝ 1/σ̂²_i). For sampling weights, it implements Variance Aware (VA) sampling that estimates per-domain gradient variance and allocates batch samples proportionally to π_i·w_i·v_i to minimize gradient estimator variance. The approach requires periodic estimation phases where domain characteristics are measured, followed by weight updates that inform both the data loader (for sampling) and loss aggregator (for weighting). A warm-up period is used before weight updates begin to ensure stable initial estimates.

## Key Results
- Theoretical derivation shows optimal loss weights follow inverse-variance scaling from generalized least squares
- Sampling weights optimized via variance reduction in stochastic optimization provably accelerate convergence
- Experiments demonstrate both mechanisms independently improve performance across linear regression, logistic regression, and neural networks
- Combined use of both weight types provides complementary benefits beyond either alone
- Effectiveness depends on domain heterogeneity—VA sampling shows minimal benefit when domains have similar inputs

## Why This Works (Mechanism)

### Mechanism 1: Loss Weights Minimize Generalization Error via Noise Inverse-Variance Scaling
- **Claim:** Assigning loss weights inversely proportional to domain label noise (w_i ∝ 1/σ²_i) reduces the generalization gap.
- **Mechanism:** In linear regression, this approach equates to Generalized Least Squares (GLS), which minimizes the variance of the estimator. The paper generalizes this via ERMA (ERM Aware Weighting), which dynamically updates weights based on observed loss variances, effectively down-weighting noisy or unreliable domains to stabilize the learning objective.
- **Core assumption:** Domains exhibit heteroskedasticity (different noise levels) and the "validation" subset used for weight estimation is sufficiently independent of the training gradients.
- **Evidence anchors:**
  - [abstract] "loss weights improve generalization by appropriately weighting domains with different noise characteristics."
  - [section 3.1] Corollary 3.2 derives optimal weights w*ᵢ ∝ 1/σ²ᵢ from the Aitken theorem.
  - [corpus] Weak direct support; neighbor papers focus on sampling or fusion, not specifically on decoupled loss weighting for noise reduction.
- **Break condition:** If domain noise is uniform or label noise is correlated with feature utility (rather than being purely random noise), inverse-variance scaling may discard useful signal.

### Mechanism 2: Sampling Weights Accelerate Convergence via Gradient Variance Reduction
- **Claim:** Allocating more samples to domains with higher gradient variance improves SGD convergence rates.
- **Mechanism:** The Variance Aware (VA) sampling strategy optimizes the batch composition to minimize the mean squared error of the gradient estimator. By solving a constrained optimization (Eq. 13), the method assigns batch sizes bᵢ ∝ πᵢwᵢvᵢ, where vᵢ is the per-domain gradient variance. This reduces stochastic noise in the optimization path.
- **Core assumption:** The primary bottleneck to convergence is gradient variance rather than saddle points or poor initialization, and gradient variance can be reliably estimated periodically.
- **Evidence anchors:**
  - [abstract] "sampling weights reduce variance in gradient estimation, improving optimization convergence."
  - [section 4] Eq. 16 derives the optimal sampling allocation using Lagrange multipliers to minimize estimator variance.
  - [corpus] "DIDS" discusses domain sampling but frames it around "domain impact" rather than explicit variance reduction mechanics.
- **Break condition:** If domain inputs are highly similar (low variance difference), the overhead of dynamic sampling estimation outweighs convergence gains (observed in MNIST experiments).

### Mechanism 3: Orthogonal Decomposition of Domain Influence
- **Claim:** Separating weights into "sampling" and "loss" components provides complementary benefits that a single scalar weight cannot capture.
- **Mechanism:** Sampling weights act on the *efficiency* of the optimization trajectory (variance), while loss weights act on the *correctness* of the objective surface (generalization). By tuning them independently, one can simultaneously speed up training and improve final model accuracy.
- **Core assumption:** The dimensions of "optimization difficulty" (variance) and "data quality" (noise) are sufficiently independent across domains.
- **Evidence anchors:**
  - [abstract] "validates... that both weight types independently improve performance and that their combination provides complementary benefits."
  - [section 5.1] Figure 1 shows distinct behaviors: One-shot FGLS converges to optimal loss weights while VA adjusts sampling, sometimes in opposite directions depending on data scale (Cᵢ).
  - [corpus] Weak support; related work typically treats domain weighting as a single scalar (e.g., DoReMi, DoGE).
- **Break condition:** If computational constraints prevent maintaining two distinct weighting systems, or if batch size is fixed and small, the coupling between sampling and loss may re-emerge.

## Foundational Learning

- **Concept: Generalized Least Squares (GLS) & Heteroskedasticity**
  - **Why needed here:** The paper derives its theoretical justification for loss weights directly from GLS (Aitken's Theorem). Understanding that standard OLS is suboptimal when noise variances differ (σᵢ² ≠ σⱼ²) is essential to grasp why loss reweighting works.
  - **Quick check question:** If domain A has label variance 0.1 and domain B has 10.0, how should their loss weights relate ideally? (Answer: Domain A should have weight ≈ 100× higher).

- **Concept: Stochastic Gradient Variance**
  - **Why needed here:** The core contribution of sampling weights is minimizing the variance of the gradient estimator (gₜ). One must understand that high variance in gradient estimation slows convergence in SGD to see the value of "Variance Aware" sampling.
  - **Quick check question:** Why does sampling more data points from a "high variance" domain reduce the total gradient error? (Answer: Averaging over more samples reduces the standard error of the mean gradient for that domain).

- **Concept: Generalization Gap**
  - **Why needed here:** The paper distinguishes between optimizing the training objective and closing the gap to test performance. Loss weights are theoretically anchored in reducing this specific gap, distinct from just speeding up training.
  - **Quick check question:** Does reducing training loss variance automatically reduce the generalization gap? (Answer: No, this paper argues loss weights handle generalization while sampling weights handle training variance).

## Architecture Onboarding

- **Component map:** Data Loader -> Modified for non-uniform domain sampling (VA Sampler) -> Loss Aggregator -> Applies domain weights (wᵢ) -> Weight Estimator (Controller) -> Periodically computes domain losses/variances -> Updates sampler/aggregator configs

- **Critical path:**
  1. Initialize uniform weights
  2. Run warm-up steps to stabilize initial gradients/residuals
  3. Estimate: Periodically compute domain-specific gradient variance (for sampling) and loss residuals (for loss weights)
  4. Update: Apply One-shot FGLS or ERMA updates to loss weights; apply Variance Aware updates to sampling probabilities
  5. Train: Construct batches using updated sampling probs; scale losses using updated loss weights

- **Design tradeoffs:**
  - **Estimation Overhead:** Computing gradient variance requires extra forward/backward passes or dedicated estimation batches. The paper suggests periodic estimation (every T₁ steps) to mitigate this.
  - **Data Splitting:** The "One-shot" method requires a split between training and estimation data. In large-scale pretraining (data seen once), one might use the "current" batch for estimation before training on it, risking slight overfitting if not careful.

- **Failure signatures:**
  - **Collapsed Sampling:** If gradient variance estimates are noisy, sampling weights might oscillate or collapse to a single domain.
  - **Noisy Weight Divergence:** In early training, losses are high for everyone; updating loss weights too early (before warm-up) might incorrectly down-weight hard-but-important domains.
  - **Similarity Neutralization:** In datasets like MNIST (cited in paper), VA sampling showed little benefit because domains were too similar; expect this mechanism to fail on homogeneous datasets.

- **First 3 experiments:**
  1. **Synthetic Heteroskedastic Regression:** Create a 2-domain linear regression task where Domain A has low label noise and Domain B has high label noise. Verify that ERMA/FGLS automatically down-weights Domain B and achieves lower MSE than uniform weighting.
  2. **Ablation on Orthogonality:** Train a model using *only* VA sampling vs. *only* ERMA loss weights vs. *both*. Plot convergence speed (steps to loss threshold) vs. final validation accuracy to confirm they address different metrics.
  3. **Sensitivity to Estimation Batch Size:** Test how often weights must be updated. Compare updating every step vs. every 100 steps to quantify the trade-off between compute overhead and convergence stability.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can ERMA be extended to handle dependent samples in autoregressive language model training?
  - **Basis in paper:** [explicit] "In contrast, this assumption does not hold in the training of autoregressive language models, where samples are inherently dependent. Extending ERMA to handle such cases would therefore be an important avenue for future work."
  - **Why unresolved:** ERMA's current formulation assumes independent samples within and across domains for variance estimation and weight updates.
  - **What evidence would resolve it:** A modified ERMA algorithm that accounts for sequential dependencies, validated on autoregressive language modeling tasks with correlated training examples.

- **Open Question 2:** Can variance-aware sampling be improved to remain effective when domains have highly similar input distributions?
  - **Basis in paper:** [explicit] "In this setup, ERMA achieves the best results... while VA appears to be ineffective. We attribute this to the high similarity of data inputs in both the clean and noisy groups, which makes the difference in gradient variance insignificant."
  - **Why unresolved:** When gradient variance is similar across domains, VA sampling provides little benefit over uniform sampling.
  - **What evidence would resolve it:** A modified sampling strategy that incorporates additional domain characteristics beyond gradient variance, demonstrating improvements on tasks with homogeneous input distributions.

- **Open Question 3:** How can the population mixture proportions π be optimally determined rather than assumed given?
  - **Basis in paper:** [explicit] "Motivated by this line of research, we turn our attention to the other two types of weights, assuming that the population mixture proportions πᵢ are given."
  - **Why unresolved:** The paper deliberately excludes the problem of determining target domain importance, focusing instead on optimization and generalization weights.
  - **What evidence would resolve it:** An integrated framework that jointly learns population weights alongside sampling and loss weights, with theoretical guarantees.

## Limitations
- Weight estimation frequency not specified, creating computational overhead uncertainty
- Data splitting requirement limits applicability to one-pass large-scale pretraining
- Heteroskedasticity assumption may fail when noise correlates with feature utility
- Effectiveness depends on domain heterogeneity; minimal benefit when domains are similar

## Confidence
- **High Confidence:** Core theoretical insight that loss weights and sampling weights play orthogonal roles (generalization vs. optimization) is well-supported by synthetic experiments and linear regression analysis
- **Medium Confidence:** Empirical gains in logistic regression and neural network experiments are demonstrated but magnitude is modest and context-dependent
- **Low Confidence:** Paper does not address potential interactions between loss weights and sampling weights beyond their independence claim

## Next Checks
1. **Estimation Overhead Benchmark:** Implement both FGLS and VA with varying update frequencies (every 10, 50, 100 steps) on a multi-domain classification task. Measure the trade-off between convergence speed, final accuracy, and wall-clock time to determine optimal update intervals.

2. **Domain Similarity Stress Test:** Create a controlled experiment with domains of varying similarity (e.g., same task, different styles) and test whether VA sampling provides benefit when gradient variances are similar. Compare against a baseline that uses uniform sampling with domain-specific learning rates.

3. **Noise-Utility Correlation:** Design a synthetic regression task where high-noise domains also have higher signal-to-noise ratio features. Test whether inverse-variance loss weighting still improves generalization or if it discards useful information, validating the heteroskedasticity assumption.