---
ver: rpa2
title: Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task
  in WMT 2024
arxiv_id: '2512.15226'
source_url: https://arxiv.org/abs/2512.15226
tags:
- translation
- language
- arxiv
- data
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the Yes-MT team's submissions for the WMT 2024
  Low-Resource Indic Language Translation Shared Task, focusing on translating between
  English and Assamese, Mizo, Khasi, and Manipuri. The team explored various approaches
  including fine-tuning pre-trained models (mT5, IndicBart, IndicTrans2), using LLMs
  (Llama 3, Mixtral) with zero-shot and few-shot prompting, LoRA fine-tuning of LLMs,
  and training Transformer models from scratch.
---

# Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024

## Quick Facts
- arXiv ID: 2512.15226
- Source URL: https://arxiv.org/abs/2512.15226
- Reference count: 4
- Primary result: Multilingual fine-tuning of mT5 and IndicBart outperformed monolingual approaches by 1.3-4.7 ChrF points

## Executive Summary
The Yes-MT team participated in the WMT 2024 Low-Resource Indic Language Translation Shared Task, focusing on translating between English and four low-resource languages: Assamese, Mizo, Khasi, and Manipuri. The team explored multiple approaches including fine-tuning pre-trained multilingual models (mT5, IndicBart, IndicTrans2), leveraging large language models (Llama 3, Mixtral) through zero-shot and few-shot prompting, applying LoRA fine-tuning to LLMs, and training Transformer models from scratch. The experiments revealed that multilingual fine-tuning consistently outperformed monolingual approaches across all language pairs, with ChrF score improvements ranging from 1.3 to 4.7 points. The LLM-based approaches, particularly when fine-tuned with LoRA, demonstrated significant potential for low-resource translation tasks.

## Method Summary
The Yes-MT team employed a comprehensive set of approaches for the low-resource translation task. They fine-tuned pre-trained models including mT5, IndicBart, and IndicTrans2 using both monolingual and multilingual strategies. For LLM-based approaches, they utilized Llama 3 and Mixtral models with zero-shot and few-shot prompting techniques. They also implemented LoRA fine-tuning on LLMs to adapt them to the specific translation tasks. Additionally, they trained Transformer models from scratch as baseline systems. The multilingual fine-tuning approach involved training models on data from multiple language pairs simultaneously, which proved more effective than training separate models for each language pair. The contrastive systems leveraged more sophisticated LLM-based approaches compared to the primary Transformer models trained from scratch.

## Key Results
- Multilingual fine-tuning of mT5 and IndicBart consistently outperformed monolingual fine-tuning across all language pairs
- LLM-based approaches with LoRA fine-tuning showed significant improvement over baseline Transformer models
- Contrastive systems achieved ChrF scores ranging from 0.3541 to 0.6518, substantially higher than primary systems (0.1102-0.1282)
- Fine-tuning strategies provided 1.3-4.7 point improvements in ChrF scores compared to monolingual approaches

## Why This Works (Mechanism)
The success of multilingual fine-tuning stems from the transfer of knowledge across related languages and the ability to leverage shared linguistic structures. Pre-trained models like mT5 and IndicBart have already learned general language understanding patterns, which can be adapted more effectively when exposed to multiple related languages simultaneously. This approach helps mitigate data sparsity issues inherent in low-resource languages by allowing models to generalize from higher-resource languages. The LLM-based approaches benefit from their larger capacity to capture complex linguistic patterns and contextual relationships, while LoRA fine-tuning enables efficient adaptation without full model retraining. The combination of these techniques allows models to better handle the unique challenges of low-resource language translation, including limited parallel corpora and linguistic divergence from high-resource languages.

## Foundational Learning
- **Low-resource language translation**: Translation between languages with limited parallel corpora; needed to address data scarcity in Assamese, Mizo, Khasi, and Manipuri
- **Multilingual fine-tuning**: Training models on multiple language pairs simultaneously; helps leverage shared linguistic patterns and mitigates data sparsity
- **LoRA fine-tuning**: Parameter-efficient fine-tuning using low-rank adaptation; enables efficient model adaptation without full retraining
- **Few-shot prompting**: Providing examples in prompts to guide LLM behavior; useful for adapting LLMs to translation tasks with minimal examples
- **ChrF metric**: Character n-gram F-score for translation quality evaluation; particularly suitable for morphologically rich and low-resource languages
- **Contrastive learning**: Training multiple systems with different approaches for comparison; helps identify optimal strategies for specific tasks

## Architecture Onboarding
- **Component map**: Pre-trained models (mT5/IndicBart/LLMs) -> Fine-tuning (monolingual/multilingual/LoRA) -> Translation generation -> ChrF evaluation
- **Critical path**: Data preparation -> Model fine-tuning -> Inference generation -> Quality evaluation
- **Design tradeoffs**: Primary systems used simpler Transformer models from scratch vs. contrastive systems using sophisticated LLM-based approaches with LoRA fine-tuning
- **Failure signatures**: Poor performance on language-specific morphological patterns, inability to handle code-switching, degradation when test data differs from training domain
- **First experiments**: 1) Fine-tune mT5 with multilingual data vs. monolingual data, 2) Apply LoRA to Llama 3 with few-shot prompts, 3) Compare ChrF scores across primary and contrastive systems

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions for future research, but the limitations section suggests several areas that warrant investigation, including the need for human evaluation, exploration of additional evaluation metrics beyond ChrF, and analysis of computational resource requirements for different approaches.

## Limitations
- Lack of human evaluation to validate automatic metric results
- Unspecified data sources raising concerns about domain mismatch
- Limited evaluation scope using only ChrF scores without other metrics like BLEU or TER
- No discussion of computational resource requirements for different approaches
- Uneven comparison between primary Transformer models and more sophisticated contrastive LLM-based systems

## Confidence
- **High confidence**: Multilingual fine-tuning of pre-trained models (mT5, IndicBart) outperforms monolingual approaches - well-supported across all language pairs
- **Medium confidence**: LLM-based approaches show significant potential for low-resource translation - promising results but limited systematic comparison
- **Low confidence**: Superiority of contrastive systems over primary systems - weakened by significant differences in approach complexity and resource requirements

## Next Checks
1. Conduct human evaluation studies comparing translations from primary Transformer models, fine-tuned LLMs, and contrastive systems to validate ChrF score findings
2. Perform detailed ablation studies isolating the effects of data augmentation, LoRA fine-tuning, and few-shot prompting on translation quality
3. Evaluate all systems using additional metrics (BLEU, TER) and assess computational resource requirements to provide a more comprehensive comparison of approach feasibility