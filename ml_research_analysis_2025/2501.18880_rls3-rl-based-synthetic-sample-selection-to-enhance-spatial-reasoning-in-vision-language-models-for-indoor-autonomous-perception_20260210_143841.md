---
ver: rpa2
title: 'RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in
  Vision-Language Models for Indoor Autonomous Perception'
arxiv_id: '2501.18880'
source_url: https://arxiv.org/abs/2501.18880
tags:
- data
- fine-tuning
- spatial
- agent
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RLS3 introduces a framework that uses reinforcement learning (RL)\
  \ to guide the generation of synthetic data for fine-tuning vision-language models\
  \ (VLMs) on spatial reasoning tasks. An RL agent manipulates objects in Unity-based\
  \ indoor environments to create challenging spatial scenarios, with the VLM\u2019\
  s performance providing feedback to improve data quality and relevance."
---

# RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception

## Quick Facts
- arXiv ID: 2501.18880
- Source URL: https://arxiv.org/abs/2501.18880
- Reference count: 40
- RL-guided synthetic data generation improves spatial reasoning in VLMs for indoor perception

## Executive Summary
RLS3 introduces a framework that uses reinforcement learning (RL) to guide the generation of synthetic data for fine-tuning vision-language models (VLMs) on spatial reasoning tasks. An RL agent manipulates objects in Unity-based indoor environments to create challenging spatial scenarios, with the VLM's performance providing feedback to improve data quality and relevance. Experiments with PaliGemma and CLIP models show that RLS3 improves spatial reasoning scores, especially for terms like "left," "right," "above," and "behind." The RL-guided approach generates data more efficiently than random sampling, achieving higher performance with fewer samples. This framework enhances VLM robustness in complex spatial reasoning tasks through intelligent synthetic data generation.

## Method Summary
RLS3 operates through an iterative co-optimization loop where an RL agent manipulates objects in Unity-based indoor environments to generate synthetic spatial reasoning samples. The RL agent, trained with Soft Actor-Critic (SAC), receives rewards based on both physical feasibility (intrinsic) and VLM performance (extrinsic). The VLM's difficulty with generated samples inversely determines the reward, creating an adversarial co-training dynamic. The framework generates data in episodes, fine-tunes the VLM on collected batches, and repeats the process. Spatial reasoning is evaluated using a rubric-based scoring system for PaliGemma and contrastive loss accuracy for CLIP models.

## Key Results
- RLS3 achieves higher spatial reasoning scores than random sampling with fewer total samples
- Performance improvements are most pronounced for spatial terms "left," "right," "above," and "behind"
- The RL agent learns to generate more challenging spatial configurations that expose VLM weaknesses
- Iterative refinement with early stopping provides sample-efficient fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Co-optimization Loop
- Claim: RL agent learns to generate samples that expose VLM weaknesses, which are then used for targeted fine-tuning.
- Mechanism: VLM inference performance is inverted to form the RL reward (J₂ = (6 - S_PG)² for PaliGemma), so poor VLM performance yields high reward. This creates adversarial pressure for the RL agent to discover challenging spatial configurations.
- Core assumption: VLM failures on synthetic data reflect learnable reasoning gaps rather than unfixable architectural limitations or pure distribution shift.

### Mechanism 2: Feasibility-Constrained Sample Generation
- Claim: Built-in physical validity constraints reduce noise and ensure learnable spatial configurations.
- Mechanism: Intrinsic reward r(s,a) = +1 for valid placements (no overlaps, within bounds), -1 for invalid. Unity collision detection enforces physical plausibility before samples reach VLM.
- Core assumption: Physically plausible configurations produce spatial relationships that generalize beyond the simulator.

### Mechanism 3: Iterative Refinement with Early Stopping
- Claim: Multiple fine-tuning iterations with validation monitoring achieve better sample efficiency than single-shot training.
- Mechanism: Generate E episodes → compile batch → fine-tune VLM → updated VLM provides new reward landscape → repeat. Early stopping triggered when validation plateaus for specified iterations.
- Core assumption: The VLM's changing weakness distribution creates a learnable non-stationary reward signal for RL.

## Foundational Learning

### Concept: Soft Actor-Critic (SAC)
- Why needed here: Off-policy RL algorithm used to train the sample generation agent with entropy regularization for exploration.
- Quick check question: Why might SAC's entropy bonus help discover diverse spatial configurations rather than exploiting a single hard pattern?

### Concept: Contrastive Vision-Language Learning
- Why needed here: CLIP-style training aligns image and text embeddings; understanding L_I→T and L_T→I (Eq. 1) is essential for interpreting VLM feedback signals.
- Quick check question: How does bidirectional contrastive loss create different gradient signals than unidirectional alignment?

### Concept: Spatial Relation Taxonomy
- Why needed here: The angle-based prompt generator maps 3D positions to discrete spatial terms (8 horizontal regions, 3 vertical), directly shaping the learning objective.
- Quick check question: Why might "left"/"right" terms be learned faster than "above"/"below" given typical indoor scene geometry?

## Architecture Onboarding

### Component Map:
Unity Environment -> RL Agent (SAC) -> Prompt Generator -> VLM -> Scheduler

### Critical Path:
RL action → Unity renders + metadata → Prompt generation → VLM inference → Compute J₂ reward → SAC update → After E episodes: compile batch (size E×T₀×η) → VLM fine-tuning for K epochs/steps → Clear batch → Next iteration

### Design Tradeoffs:
- **Feedback frequency (E)**: More episodes = richer RL signal but slower VLM updates
- **VLM reward scaling (β=10)**: Balances intrinsic feasibility vs extrinsic VLM weakness signal
- **Pre-training (100k steps)**: RL learns feasibility before full RLS3 loop activates
- **Sampling rate (η=0.5)**: Trades batch diversity against storage/processing

### Failure Signatures:
- **High invalid sample rate**: RL hasn't learned feasibility → extend pre-training or tighten action bounds
- **VLM plateaus immediately**: RL not generating informative samples → check entropy coefficient, reduce β
- **Severe term imbalance**: Environment bias (e.g., mostly horizontal relations) → modify scene layouts or adjust prompt generation thresholds
- **Loss spikes without convergence**: Batch distribution too volatile → increase E or reduce η

### First 3 Experiments:
1. **Baseline validation**: Replicate SAC vs random agent comparison on PaliGemma with T₀=200, E=20, tracking test score vs cumulative samples to confirm sample efficiency gains.
2. **Reward scaling ablation**: Test β ∈ {1, 5, 10, 20} while monitoring ratio of valid samples and per-iteration score improvement to identify optimal balance.
3. **Per-term analysis**: Log cumulative counts and per-term scores for each spatial relation to quantify which terms benefit most from RL-guided sampling and diagnose environment biases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the spatial reasoning improvement achieved via RLS3 in the Unity simulator transfer to real-world indoor environments?
- Basis in paper: The authors explicitly identify the "challenge of sim2real transfer which in itself necessitates further investigation, perhaps through the direct integration of domain adaptation techniques."
- Why unresolved: The paper evaluates performance exclusively on a static test set generated within the separate Unity environment, lacking validation on physical world data.
- What evidence would resolve it: Benchmarking the fine-tuned VLMs on real-world spatial reasoning datasets to measure the performance gap between synthetic and physical domains.

### Open Question 2
- Question: Does replacing the batch-wise VLM feedback with finer-grained, sample-level rewards improve the efficiency of the RL agent's data selection?
- Basis in paper: The conclusion suggests that "finer-grained signals would make the reward less sparse and more precisely identified informative samples" as a key direction to explore.
- Why unresolved: The current methodology provides a single extrinsic reward signal ($J_2$) only at the end of an episode (batch), which makes credit assignment for individual "hard" samples ambiguous.
- What evidence would resolve it: A comparative ablation study showing that per-sample rewards lead to faster convergence or higher peak performance than the current batch-based reward mechanism.

### Open Question 3
- Question: Can the RLS3 framework scale to support the fine-tuning of significantly larger or more sophisticated VLMs, such as LLaVA or Grounding DINO?
- Basis in paper: The authors state that future work includes adjusting the framework for "multi-device communication" to support "more sophisticated models, such as LLaVA and Grounding DINO."
- Why unresolved: The experiments are limited to PaliGemma and CLIP; the computational overhead of iteratively inferencing and fine-tuning larger models within the RL loop is unknown.
- What evidence would resolve it: A demonstration of RLS3 training a large-scale VLM (e.g., LLaVA) with an analysis of the computational cost and training stability.

## Limitations
- Simulator-to-real transfer gap remains unverified without real-world validation
- RL agent interpretability is limited - unclear what specific spatial configurations are discovered
- VLM reward signal stability depends on assumption that synthetic failures reflect genuine reasoning gaps

## Confidence
- **High**: Sample efficiency improvements and iterative refinement mechanism are well-supported by experimental results
- **Medium**: Spatial reasoning improvements for specific terms are supported, but relative improvements and environment biases need further investigation
- **Low**: Claims about generalization beyond simulator lack validation on real-world data or alternative synthetic environments

## Next Checks
1. **Cross-Environment Generalization Test**: Evaluate the fine-tuned VLMs on a second synthetic environment with different layouts, object distributions, and camera configurations to verify that improvements transfer beyond the training simulator.

2. **Real-World Transfer Validation**: Test the spatial reasoning performance on real indoor images (e.g., from Matterport3D or similar datasets) to quantify the simulator-to-real gap and validate generalization claims.

3. **Adversarial Sample Analysis**: Conduct systematic analysis of the most challenging samples generated by the RL agent to identify patterns in spatial configurations, verify physical plausibility, and assess whether the agent discovers meaningful edge cases or exploits simulator-specific artifacts.