---
ver: rpa2
title: Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series
  Forecasting
arxiv_id: '2510.21491'
source_url: https://arxiv.org/abs/2510.21491
tags:
- learning
- data
- tasks
- forecasting
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces the first benchmarking framework for evaluating\
  \ catastrophic forgetting (CF) mitigation methods in federated continual time series\
  \ forecasting. Using the Beijing Multi-site Air Quality dataset across 12 decentralized\
  \ clients, the authors systematically compare five core CL strategies\u2014Replay,\
  \ Elastic Weight Consolidation (EWC), Online EWC, Learning without Forgetting (LwF),\
  \ and Synaptic Intelligence (SI)\u2014integrated within a federated learning pipeline."
---

# Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2510.21491
- **Source URL**: https://arxiv.org/abs/2510.21491
- **Reference count**: 26
- **Primary result**: Introduces first benchmarking framework for CF mitigation in federated time series forecasting using Beijing Air Quality dataset across 12 decentralized clients.

## Executive Summary
This study presents the first comprehensive benchmarking framework for evaluating catastrophic forgetting (CF) mitigation methods in federated continual time series forecasting. Using the Beijing Multi-site Air Quality dataset across 12 decentralized clients, the authors systematically compare five core CL strategies—Replay, Elastic Weight Consolidation (EWC), Online EWC, Learning without Forgetting (LwF), and Synaptic Intelligence (SI)—integrated within a federated learning pipeline. The framework employs LSTM models with lagged sequence generation and seasonal task partitioning to simulate real-world non-i.i.d. time series environments. Evaluation metrics include average forgetting, plasticity, and average performance, revealing that replay-based methods consistently achieve the best trade-off between forgetting mitigation and plasticity, though at higher computational cost.

## Method Summary
The authors develop a federated continual learning framework using the Beijing Multi-site Air Quality dataset with 12 monitoring stations as decentralized clients. The approach involves pre-training a global LSTM model on initial data (T0) using FedAvg, followed by sequential learning on 11 seasonal tasks. Five CL mitigation methods are evaluated: Replay (with k-means clustering for sample selection), EWC, Online EWC, LwF, and SI. The framework uses lagged sequence generation (12 past steps to predict 6 future steps) and temporal train-test splits. Hyperparameters are tuned via grid search for each target variable (Temperature, PM2.5, Wind Speed). Performance is evaluated using Average Forgetting (AF), Average Plasticity (AP), and Average Performance (AvgPerf) metrics across 5 random seeds.

## Key Results
- Replay achieves the best trade-off with lowest forgetting rates and high plasticity but highest computational cost
- EWC and O-EWC provide strong stability but lower plasticity, resulting in lowest adaptability and overall performance
- LwF and SI offer balanced alternatives with good adaptability due to knowledge distillation and accumulated gradient mechanisms
- The study highlights the need for tailored CL strategies in federated time series forecasting

## Why This Works (Mechanism)

### Mechanism 1: Experience Replay
- **Claim**: Storing and interleaving representative samples from previous tasks with current task data significantly reduces catastrophic forgetting while maintaining plasticity.
- **Mechanism**: Clustering-based strategy (k-means centroids) selects representative samples. During training on new task, loss function combines current task data with replayed samples (`L_replay = L_current + λ_replay · L_buffer`).
- **Core assumption**: Small, representative subset of past data can sufficiently approximate original task distribution and is accessible under memory/privacy constraints.
- **Evidence anchors**: [abstract] "Results show Replay achieves the best trade-off with lowest forgetting rates and high plasticity but highest computational cost."
- **Break condition**: Privacy policies strictly forbid storing any raw past data; memory constraints limit buffer size below representative threshold.

### Mechanism 2: Parameter Regularization (EWC/SI)
- **Claim**: Penalizing changes to parameters deemed important for previous tasks stabilizes prior knowledge at cost of reduced adaptability.
- **Mechanism**: Methods estimate parameter importance (Fisher Information Matrix or accumulated gradient contributions) and add quadratic penalty to loss (`L_EWC = Σ F_i(θ_i - θ*_i)²`).
- **Core assumption**: Importance estimation accurately reflects parameter relevance across tasks; rigid constraints do not critically harm new task learning.
- **Evidence anchors**: [abstract] "LwF and SI provide balanced alternatives, while EWC shows stability at the cost of adaptability."
- **Break condition**: Importance estimation is inaccurate (non-stationary data shifts parameter importance) or regularization strength is too high.

### Mechanism 3: Knowledge Distillation (LwF)
- **Claim**: Aligning outputs of current model with frozen "teacher" model from previous task maintains prior functional behavior, offering data-free stability compromise.
- **Mechanism**: Snapshot of model saved as teacher after each task. During new task training, distillation loss forces student to mimic teacher's outputs on new task's inputs.
- **Core assumption**: New task's input distribution overlaps sufficiently with previous tasks for distillation signal to be meaningful.
- **Evidence anchors**: [abstract] Mentions LwF as "balanced alternative."
- **Break condition**: Significant distribution shift between tasks causes teacher's outputs on new data to be irrelevant or misleading.

## Foundational Learning

**Concept: Catastrophic Forgetting (CF)**
- **Why needed here**: This is the central problem the entire paper addresses. Understanding that sequential learning on new tasks degrades performance on old ones is the core motivation for all methods tested.
- **Quick check question**: Can you explain why a model that learns task B after task A might fail at task A, even if it performed well initially?

**Concept: The Stability-Plasticity Dilemma**
- **Why needed here**: This is the core tradeoff being benchmarked. Engineers must understand that effective CL methods are not just about preventing forgetting (stability) but also about continuing to learn (plasticity).
- **Quick check question**: If a model has high stability but low plasticity after learning several tasks, what is the likely symptom?

**Concept: Federated Learning (FL) & Non-IID Data**
- **Why needed here**: The experiments occur in a federated setting with decentralized, heterogeneous data. This adds complexity to CF (e.g., local forgetting affects global model).
- **Quick check question**: Why might a global model aggregated from clients who have each learned different local tasks suffer from forgetting?

## Architecture Onboarding

**Component map:**
Base Model (Offline Phase) -> Online/Continual Phase -> CL Mitigation Modules -> Server Aggregation -> Evaluation Matrix

**Critical path:**
1. Set up federated simulation with K=12 clients, each assigned a station from Beijing Air Quality dataset
2. Implement Base Model training loop: 500 rounds of FedAvg on T0 data to get starting global model
3. Implement Continual Learning loop: For each new task (T1-T11), each client applies selected CL method, trains locally for 1 epoch/30 rounds, sends updated parameters to server, server aggregates via FedAvg and distributes new global model
4. After all tasks, run evaluation: Test final model M_N on ALL task test sets to populate performance matrix P and compute AF, AP, AvgPerf

**Design tradeoffs:**
- **Replay**: Highest retention & plasticity, highest computational cost and potential privacy issues
- **EWC/SI**: Good stability, lower compute than replay, but can over-constrain and hurt plasticity. Hyperparameter (λ) sensitivity is high
- **LwF**: No data storage, good balance, but assumes task distribution overlap. Fails if tasks are vastly different
- **Naive**: Highest plasticity, lowest compute, but severe forgetting (lower bound for retention)

**Failure signatures:**
- **High Forgetting (High AF)**: Model performance on early tasks degrades severely after training on later tasks. Check if CL method is active or λ is too low
- **Low Plasticity (High AP)**: Model performance on new tasks is poor. Check if regularization (λ) is too high or replay buffer is overwhelming new data
- **Unstable Training**: Loss diverges. Check learning rates or normalization consistency across federated rounds

**First 3 experiments:**
1. **Reproduce Naive vs. Static Baseline**: Train static model on T0 only and naive CL model (no mitigation) on T1-T11. Confirm naive model has higher AP but also shows significant AF, validating basic CL setup
2. **Tune Replay Buffer**: Implement Replay with small buffer (5% of data). Evaluate AF and AP. Sweep buffer size (10%, 15%, 20%) to observe tradeoff on plasticity
3. **Compare EWC Variants**: Implement and compare standard EWC vs. Online EWC. Measure both AF (stability) and AP (plasticity) to confirm O-EWC provides better adaptability while maintaining stability

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question**: Can adaptive replay strategies, which dynamically adjust buffer sizes based on task difficulty or data drift, optimize the stability-plasticity trade-off more effectively than fixed sampling ratios?
- **Basis in paper**: [explicit] The authors state in Section IV.C that future research could explore adaptive replay strategies where buffer size is dynamically adjusted rather than maximizing fixed ratios
- **Why unresolved**: Current study relies on fixed, target-specific buffer ratios (15-25%) determined via grid search, which may be suboptimal as data evolves
- **What evidence would resolve it**: Comparative analysis showing drift-aware buffer allocation policy outperforms static buffers in both Average Forgetting and Plasticity metrics

**Open Question 2**
- **Question**: Can replay-free methods such as latent replay or lightweight generative surrogates match the performance of Experience Replay without violating federated privacy constraints?
- **Basis in paper**: [explicit] The conclusion notes that Replay achieves the best trade-off but raises scalability and privacy concerns, explicitly calling for development of replay-free methods or generative surrogates
- **Why unresolved**: Replay currently requires storing raw data, which conflicts with decentralized, privacy-preserving nature of Federated Learning
- **What evidence would resolve it**: Empirical results demonstrating generative or latent approach achieves Average Forgetting rates comparable to Replay (near zero) while strictly utilizing local memory

**Open Question 3**
- **Question**: Can meta-learning or sensitivity analysis automate the selection of Continual Learning hyperparameters (λ) to eliminate need for target-specific grid searches?
- **Basis in paper**: [explicit] The conclusion identifies "automation of hyperparameter selection through principled sensitivity analysis or meta-learning" as necessary research avenue
- **Why unresolved**: Current experiments relied on manual grid searches to find distinct optimal hyperparameters for each forecasting target
- **What evidence would resolve it**: Framework that dynamically tunes regularization weights during training and maintains stability without manual intervention across diverse time series targets

## Limitations

- Model architecture details (layers, hidden size, dropout) are not specified, requiring assumptions for reproduction
- Optimizer configuration (learning rate, batch size, optimizer choice) is unspecified, relying on common defaults
- Replay buffer specifics (absolute size, k-means clustering parameters) are not detailed, only relative ratios are given
- Client participation strategy and FedAvg weighting scheme are not explicitly stated

## Confidence

**High Confidence**: The core claim that replay-based methods offer the best tradeoff between forgetting mitigation and plasticity is strongly supported by results (lowest AF, high AP, Table II)

**Medium Confidence**: The assertion that EWC and O-EWC provide stability at cost of plasticity is supported, but exact mechanisms of this tradeoff in federated setting are not fully explored in corpus

**Low Confidence**: The claim that LwF offers a "balanced alternative" is weakly supported by corpus, which lacks direct evidence for LwF efficacy in federated time series specifically

## Next Checks

1. **Reproduce Core Tradeoff**: Implement baseline (Naive) and Replay methods. Confirm Replay achieves lower Average Forgetting (AF) than Naive, demonstrating fundamental benefit of mitigation

2. **Hyperparameter Sensitivity Analysis**: Systematically sweep regularization parameter (λ) for EWC and replay buffer ratio for Replay. Document how changes affect stability-plasticity tradeoff (AF vs. AP)

3. **FedAvg Configuration Impact**: Run experiments with different FedAvg configurations (partial client participation vs. all clients, uniform vs. data-weighted averaging). Assess how federated learning choices influence performance and forgetting rates of different CL methods