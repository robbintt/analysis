---
ver: rpa2
title: A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container
  Stowage Planning Problem
arxiv_id: '2510.02589'
source_url: https://arxiv.org/abs/2510.02589
tags:
- container
- crane
- scenario
- stowage
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark of five deep reinforcement
  learning algorithms (DQN, QR-DQN, A2C, PPO, TRPO) for the Container Stowage Planning
  Problem (CSPP). The authors develop the Stowage Planning Gym Environment (SPGE)
  to evaluate these algorithms across eight progressively complex scenarios, varying
  vessel/yard sizes, container counts, types, and number of cranes.
---

# A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem

## Quick Facts
- arXiv ID: 2510.02589
- Source URL: https://arxiv.org/abs/2510.02589
- Reference count: 40
- This paper benchmarks five deep RL algorithms (DQN, QR-DQN, A2C, PPO, TRPO) on Container Stowage Planning Problem using SPGE environment

## Executive Summary
This paper presents a comprehensive benchmark of five deep reinforcement learning algorithms on the Container Stowage Planning Problem (CSPP). The authors develop the Stowage Planning Gym Environment (SPGE) to evaluate these algorithms across eight progressively complex scenarios with varying vessel/yard sizes, container counts, types, and number of cranes. The study reveals significant performance differences between algorithm types, with policy gradient methods (PPO, TRPO) demonstrating superior performance over value-based methods (DQN, QR-DQN) and vanilla actor-critic (A2C) as problem complexity increases.

## Method Summary
The study benchmarks five deep RL algorithms (DQN, QR-DQN, A2C, PPO, TRPO) using Stable-Baselines3 implementations on a custom Stowage Planning Gym Environment (SPGE). The environment simulates container loading operations with varying complexity levels, including single-crane (SPGE), multi-crane single-agent (SPGE-MC), and multi-agent (SPAEC) formulations. Action masking is integrated to filter invalid actions, and experiments are conducted across eight scenarios with 10-30 repetitions each. The primary metrics are shifter counts and operation time, with the reward function based on negative shifter counts.

## Key Results
- All algorithms perform comparably on simple problems but effectiveness diverges significantly as complexity increases
- A2C and value-based methods (DQN, QR-DQN) struggle with complex scenarios and converge to suboptimal policies
- PPO and TRPO demonstrate superior performance, with TRPO consistently achieving the best results, particularly in most challenging settings
- For multi-crane scenarios, the single-agent formulation (SPGE-MC) tends to perform better in reducing shifters during algorithm training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy gradient methods with trust regions (TRPO, PPO) outperform value-based methods (DQN, QR-DQN) and vanilla actor-critic (A2C) as CSPP complexity increases.
- Mechanism: In complex stowage scenarios, the credit assignment problem becomes harder—a container selection's true impact on shifters may not manifest for many timesteps. TRPO/PPO use Generalized Advantage Estimation (GAE) which balances bias-variance in value estimation, while A2C's fixed 5-step returns may miss long-term consequences. TRPO's trust region constraint prevents destabilizing policy updates during training on combinatorial action spaces.
- Core assumption: The performance gap stems from advantage estimation quality rather than hyperparameter tuning; the 5-step return horizon for A2C is insufficient for complex CSPP credit assignment.
- Evidence anchors:
  - [Section 5.4]: "A2C consistently yielded the poorest results... the true value of a single action may not be reflected solely by the immediate shifter, whose long-term impact might only emerge far beyond 5 steps."
  - [Section 5.4]: "TRPO consistently achieving the best performance in the most challenging settings" with ~193 shifters in 200-container scenarios vs higher for others.
  - [corpus]: Weak direct evidence—neighbor papers focus on stowage planning applications rather than algorithmic comparisons.
- Break condition: If A2C with GAE and tuned n-step still underperforms, the mechanism may instead relate to trust region stability rather than credit assignment alone.

### Mechanism 2
- Claim: Single-agent formulation (SPGE-MC) for multi-crane scheduling tends to achieve lower shifter counts than multi-agent formulation (SPAEC) during training.
- Mechanism: SPGE-MC allows the agent free choice among available cranes at each step, enabling global optimization. SPAEC's predefined crane activation order imposes structural constraints that may prevent discovering optimal crane-container pairings. Shifter-based rewards are denser than operation-time rewards, making shifter reduction easier to prioritize.
- Core assumption: The constraint on crane selection order in SPAEC is the primary driver of performance differences; agents can effectively learn crane assignment when given explicit control.
- Evidence anchors:
  - [Section 5.4]: "Single-agent setup in SPGE-MC has more flexibility in crane selection when multiple cranes are available... which might be helpful for agent to learn a globally optimal policy."
  - [Table 4]: TRPO in Scenario 8 shows 87.7 shifters (SPGE-MC) vs 89.3 (SPAEC), statistically significant difference.
  - [corpus]: No direct corpus evidence on single vs multi-agent crane formulations.
- Break condition: If SPAEC with learned crane selection priority matches SPGE-MC performance, the constraint mechanism is less important than observation/action space design.

### Mechanism 3
- Claim: Action masking accelerates convergence and improves scalability in constrained CSPP environments.
- Mechanism: Rather than penalizing invalid actions (which wastes exploration budget), action masking filters infeasible actions at the network level, ensuring the policy only explores valid container-slot assignments that satisfy physical and operational constraints.
- Core assumption: Invalid action penalties create sparse reward signal issues that significantly slow learning in complex scenarios.
- Evidence anchors:
  - [Section 5.2]: "Action masking dramatically accelerates convergence and improves scalability compared to naïve penalty-based approaches."
  - [Section 5.2]: "Filters out invalid actions at the policy or Q-network level, preventing the agent from exploring futile actions."
  - [corpus]: Neighbor paper (2502.12756) mentions challenges with "action space feasibility... dependent on corresponding state" but doesn't directly compare masking approaches.
- Break condition: If masked DQN still converges slowly relative to unmasked PPO, masking alone is insufficient—algorithm choice remains primary driver.

## Foundational Learning

- Concept: **Markov Decision Process (MDP) formulation for combinatorial optimization**
  - Why needed here: CSPP is framed as sequential container selection where each state transition represents a partial loading plan. Understanding that the problem decomposes into state (vessel/yard configuration), action (container selection), and reward (negative shifters) is prerequisite to interpreting algorithm behavior.
  - Quick check question: Can you explain why the shifter count is a delayed rather than immediate reward signal in this formulation?

- Concept: **Value-based vs policy-based RL distinction**
  - Why needed here: The benchmark's central finding is that these algorithm families behave differently under complexity scaling. Value-based methods (DQN) learn Q(s,a) estimates; policy-based methods (PPO, TRPO) directly optimize the policy. The critic quality hypothesis depends on understanding this distinction.
  - Quick check question: Why would a 5-step return horizon cause more problems for A2C than for PPO with GAE?

- Concept: **Multi-agent vs single-agent problem formulation**
  - Why needed here: The SPGE-MC vs SPAEC comparison tests whether centralized control outperforms distributed control for crane scheduling. Understanding observation/action space differences is essential for interpreting the results.
  - Quick check question: What structural constraint does SPAEC impose that SPGE-MC does not?

## Architecture Onboarding

- Component map: State encoding (flattened slot attributes + crane status) -> Action masking layer -> Policy/Q-network -> Valid action selection -> Environment step -> Reward calculation (shifters incurred)
- Critical path: State encoding (flattened slot attributes + crane status) -> Action masking layer -> Policy/Q-network -> Valid action selection -> Environment step -> Reward calculation (shifters incurred)
- Design tradeoffs:
  - Slot-based abstraction vs detailed physics: Simplifies constraints but may miss real-world hull geometry effects (acknowledged in conclusion)
  - Single centralized agent vs distributed: SPGE-MC offers flexibility but scales action space (|containers| × |cranes|); SPAEC scales agents but constrains coordination
  - Dense shifter rewards vs sparse feasibility: Current design optimizes shifters well; operation time may conflict at extreme optimization levels
- Failure signatures:
  - A2C converging to high shifter counts in complex scenarios -> n-step horizon too short for credit assignment
  - DQN/QR-DQN suboptimal in small-scale complex problems -> value estimation struggles with combinatorial branching
  - Large variance across training runs -> increase repetitions (paper uses 10-30) or investigate seed sensitivity
- First 3 experiments:
  1. Reproduce Scenario 1 (simple) with all five algorithms to validate environment setup matches paper's baseline (~12-28 shifters range)
  2. Ablate action masking by comparing masked vs penalty-based invalid action handling on Scenario 3 to quantify convergence acceleration
  3. Test A2C with GAE and extended n-step (e.g., 15-20 steps) on Scenario 5 to evaluate whether credit assignment mechanism explains performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of realistic vessel features, such as hull structures, hatches, and specialized zones, affect the performance divergence between policy-based (TRPO/PPO) and value-based algorithms in CSPP?
- Basis in paper: [explicit] The conclusion states future work involves "implementing more realistic vessel hull structures, adding features like hatches and specialized zones."
- Why unresolved: The current SPGE environment abstracts vessels as 3D grids of slots, simplifying physical constraints.
- What evidence would resolve it: Benchmarking the algorithms in an updated SPGE that incorporates these structural constraints and comparing the convergence rates.

### Open Question 2
- Question: Does replacing the abstracted operation time model with a sophisticated real-world model alter the effectiveness of the single-agent formulation in minimizing shifters and operation duration?
- Basis in paper: [explicit] The conclusion suggests "developing more sophisticated models for operation time to reflect more aspects in real-world operations."
- Why unresolved: The current study uses a simplified time mechanism (global clock $t$ and availability vector $\tau$), which may not capture complex real-world dynamics.
- What evidence would resolve it: Experiments comparing agent performance using the current abstract model versus a high-fidelity simulation of crane kinetics and yard traffic.

### Open Question 3
- Question: Can A2C achieve performance parity with PPO and TRPO in complex CSPP scenarios by utilizing Generalized Advantage Estimation (GAE) or optimized n-step returns?
- Basis in paper: [inferred] The results section hypothesizes that A2C's poor performance stems from its inability to use GAE and the bias introduced by a fixed 5-step return in long-horizon problems.
- Why unresolved: The study utilized standard A2C; the impact of integrating GAE or tuning the n-step parameter specifically for the "shifter" reward structure was not tested.
- What evidence would resolve it: Ablation studies varying the n-step parameter and implementing GAE for A2C within the complex Scenario 5 and 8 settings.

## Limitations
- The study relies on the correctness of SPGE environment's constraint implementation, particularly sequencer logic and action masking integration
- Lack of explicit neural network architectures and hyperparameter specifications creates significant reproduction variance
- The paper acknowledges that slot-based vessel abstraction may miss real-world hull geometry effects

## Confidence

- High confidence: PPO and TRPO outperform other algorithms on complex scenarios; action masking improves convergence
- Medium confidence: A2C's underperformance is primarily due to 5-step return horizon limitations; single-agent formulation outperforms multi-agent for crane scheduling
- Low confidence: The relative performance rankings are robust to hyperparameter variations; the specific neural network architectures don't significantly impact conclusions

## Next Checks

1. Implement A2C with GAE and extended n-step returns (15-20 steps) on Scenario 5 to verify whether credit assignment quality explains the performance gap
2. Compare masked vs penalty-based invalid action handling on Scenario 3 to quantify the convergence acceleration from action masking
3. Reproduce Scenario 1 (simple) with all five algorithms to establish baseline performance and validate environment implementation correctness