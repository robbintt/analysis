---
ver: rpa2
title: Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial
  Attacks
arxiv_id: '2511.18562'
source_url: https://arxiv.org/abs/2511.18562
tags:
- test
- accuracy
- adversarial
- training
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes how adversarial perturbations during calibration\
  \ affect split conformal prediction (CP) validity and efficiency. The key method\
  \ idea is to inject adversarial attacks into the calibration set with strength \u03F5\
  cal and study the resulting coverage guarantees under test-time attacks \u03F5test."
---

# Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks

## Quick Facts
- **arXiv ID**: 2511.18562
- **Source URL**: https://arxiv.org/abs/2511.18562
- **Reference count**: 0
- **Primary result**: Calibration-time adversarial attacks (ε_cal) monotonically control CP coverage and create tolerance bands against test-time attacks

## Executive Summary
This paper analyzes how adversarial perturbations during calibration affect split conformal prediction validity and efficiency. The key insight is that injecting adversarial attacks into the calibration set creates a monotonic relationship between calibration attack strength and prediction coverage on perturbed test data. By carefully selecting ε_cal, one can maintain target coverage within a tolerance band across a range of test-time attack strengths. The paper also demonstrates that adversarial training of the underlying classifier further improves efficiency by reducing prediction set size while maintaining robustness.

## Method Summary
The method modifies split conformal prediction by applying adversarial attacks to the calibration set with strength ε_cal before computing nonconformity scores. The key steps are: (1) train model with optional adversarial training at strength ε_train, (2) calibrate Split CP on adversarially perturbed calibration data using HPS score S(x, y) = 1 − f_y(x), and (3) test with ℓ∞-FGSM attacks at varying strengths ε_test. The calibration set size is 20% of total data, with models trained on CIFAR-10/100, MNIST, and TinyImageNet using ResNet-50d and ViT architectures.

## Key Results
- Coverage varies monotonically with ε_cal, enabling predictable control over robustness to test-time attacks
- Target coverage can be maintained within 88%-92% tolerance band over contiguous range of test-time attacks by appropriate ε_cal selection
- Adversarial training reduces prediction set size by increasing confidence on true labels under the uniform-residual assumption
- Experiments confirm theoretical findings across multiple datasets and architectures

## Why This Works (Mechanism)

### Mechanism 1: Calibration-Attack Modulation of Coverage
Injecting adversarial perturbations into the calibration set inflates nonconformity scores, systematically shifting the conformal threshold Q to expand prediction sets at test time. This creates a monotonic relationship between ε_cal and coverage under smoothness assumptions.

### Mechanism 2: Tolerance-Band Robustness
The coverage error bound is linear in both ε_test and ε_cal. By solving for when deviation is less than β, the paper defines a valid interval for ε_test around ε_cal. Increasing ε_cal shifts this robust interval toward stronger attack levels.

### Mechanism 3: Adversarial Training Efficiency
Adversarial training increases model confidence on true labels. Under the uniform-residual assumption, this increased true-label mass reduces the probability that incorrect classes exceed the conformal threshold, tightening prediction sets.

## Foundational Learning

- **Concept: Split Conformal Prediction (Split CP)**
  - Why needed here: Base architecture modified to handle distribution shift from adversarial attacks
  - Quick check question: Why does Split CP require a held-out calibration set separate from the training set?

- **Concept: Nonconformity Scores (HPS Score)**
  - Why needed here: Paper analyzes how adversarial attacks shift score distribution (1 - f_y(x))
  - Quick check question: How does HPS score 1 - f_y(x) change if adversarial attack lowers probability of true class?

- **Concept: Exchangeability vs. Distribution Shift**
  - Why needed here: Standard CP relies on exchangeability; adversarial attacks break this assumption
  - Quick check question: Why does applying perturbation to test but not calibration set break exchangeability?

## Architecture Onboarding

- **Component map**: Model Training -> Calibration (with ε_cal attack) -> Inference (with ε_test attack)
- **Critical path**: Selecting ε_cal as the knob trading off coverage levels against robustness to test-time attack magnitudes
- **Design tradeoffs**:
  - High ε_cal increases robustness but may cause over-coverage on clean data
  - Adversarial training reduces set size but requires longer training and may reduce standard accuracy
- **Failure signatures**:
  - Coverage Collapse: Coverage drops when ε_test significantly exceeds ε_cal
  - Set Explosion: Large prediction sets when clean training used on heavily perturbed test data
- **First 3 experiments**:
  1. Verify Monotonicity: Fix ε_test=8/255, sweep ε_cal from 0 to 16/255, plot coverage vs ε_cal
  2. Verify Tolerance Band: Fix ε_cal=8/255, sweep ε_test, confirm coverage stays within 88%-92% band
  3. Verify Efficiency: Train Clean vs Adversarial models, run CP with same ε_cal, plot average prediction set size

## Open Questions the Paper Calls Out

- How do theoretical guarantees generalize to stronger multi-step attacks (PGD, CW) vs single-step FGSM?
- Can analysis extend to Adaptive Prediction Set (APS) nonconformity score?
- How does method perform under combined distribution shifts (adversarial + natural covariate shift)?
- How sensitive are tolerance band guarantees to calibration set size in low-data regimes?

## Limitations

- Theoretical guarantees rely on smoothness and bounded gradient norm assumptions that may not hold for deep networks under strong attacks
- Computational cost scales with adversarial training iterations and multiple ε_cal sweeps
- The Uniform-Residual Assumption for adversarial training efficiency lacks empirical validation across datasets

## Confidence

- **High Confidence**: Monotonicity of coverage with respect to ε_cal - directly observable in experiments
- **Medium Confidence**: Tolerance-band robustness property - depends on bounded concentration constants that may vary
- **Medium Confidence**: Adversarial training efficiency gains - assumes uniform residual distribution which may not hold

## Next Checks

1. **Gradient Norm Stability**: Measure ||∇f|| across different ε_train values to verify boundedness assumption for Theorem 2
2. **Uniform-Residual Validation**: Empirically test probability distribution across incorrect classes under adversarial training
3. **Cross-Dataset Transfer**: Apply tolerance-band methodology to a dataset outside original study (e.g., ImageNet-1K)