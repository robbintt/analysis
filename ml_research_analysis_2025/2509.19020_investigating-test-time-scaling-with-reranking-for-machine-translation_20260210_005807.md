---
ver: rpa2
title: Investigating Test-Time Scaling with Reranking for Machine Translation
arxiv_id: '2509.19020'
source_url: https://arxiv.org/abs/2509.19020
tags:
- translation
- scaling
- quality
- machine
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically explores Test-Time Scaling (TTS) for
  machine translation by generating multiple translation candidates and selecting
  the best using quality estimation. Experiments cover six high-resource and one low-resource
  language pair, five model sizes (3B-72B), and up to 1024 candidates.
---

# Investigating Test-Time Scaling with Reranking for Machine Translation

## Quick Facts
- arXiv ID: 2509.19020
- Source URL: https://arxiv.org/abs/2509.19020
- Reference count: 10
- Key outcome: TTS consistently improves translation quality in high-resource settings; small models at large N can match or surpass larger models at N=1; under fixed compute budgets larger models are more efficient; TTS can degrade quality in low-resource settings due to code-switching errors.

## Executive Summary
This paper systematically explores Test-Time Scaling (TTS) for machine translation by generating multiple translation candidates and selecting the best using quality estimation. Experiments cover six high-resource and one low-resource language pair, five model sizes (3B-72B), and up to 1024 candidates. TTS consistently improves translation quality in high-resource settings, with small models at large N matching or surpassing larger models at N=1, confirmed by human evaluation. Under fixed compute budgets, larger models are generally more compute-efficient, though a 14B model at N≈8-16 can approach 72B performance at lower cost. In low-resource settings, TTS can degrade quality due to code-switching errors that fool evaluation metrics. TTS-MT emerges as a strong baseline outperforming recent inference-scaling and RLHF approaches.

## Method Summary
The paper implements Test-Time Scaling for machine translation by generating N translation candidates using Qwen2.5 Instruct models (3B-72B) with temperature=1.0 and top-p=0.95, then selecting the highest-scoring candidate via the KIWI22 Quality Estimation model. The framework operates in a zero-shot setting without fine-tuning on parallel data. For each source sentence, N candidates are generated, scored by KIWI22, and the argmax is selected as the final translation. The method is evaluated across seven language pairs from the WMT24 general MT benchmark, with N ranging from 1 to 1024, and quality assessed using both reference-free QE (KIWI22) and reference-based metrics (BLEU, ChrF++, COMET22, Remedy, MetricX, XCOMET).

## Key Results
- TTS consistently improves translation quality in high-resource settings across all model sizes
- Small models (14B) at large N (8-16) can match or exceed the quality of much larger models (72B) at N=1
- Under fixed compute budgets, larger models are generally more compute-efficient
- TTS can degrade quality in low-resource settings due to code-switching errors that fool evaluation metrics
- TTS-MT emerges as a strong baseline outperforming recent inference-scaling and RLHF approaches

## Why This Works (Mechanism)

### Mechanism 1: Best-of-N Selection via Quality Estimation
The process exploits variance in stochastic decoding by generating multiple independent candidates and using a reference-free Quality Estimation model to identify the best one. While a single sample might contain errors, increasing N raises the probability that at least one candidate lands closer to the optimal translation. The QE model acts as an external verifier to identify this "best" candidate without ground truth.

### Mechanism 2: Compute-Memory Trade-off (Small Model, Large N)
Smaller models augmented with large inference budgets can match or exceed the quality of much larger models, allowing trading GPU memory for latency/compute. A 14B model requires significantly less memory than a 72B model, and by repeating inference, the smaller model explores the solution space more thoroughly than a single pass of the larger model.

### Mechanism 3: Metric Interference and Blind Spots
TTS can degrade quality in low-resource settings because QE models can be "fooled" by severe code-switching or hallucinations, assigning high scores to poor translations. In low-resource pairs, models may output text containing Chinese characters or other languages, and current metrics may assign these anomalous outputs high confidence scores, selecting them over correct translations.

## Foundational Learning

- **Concept: Quality Estimation (QE) vs. Reference-Based Metrics**
  - Why needed here: The core of this paper relies on using QE (reference-free) for selection during inference, but reference-based metrics (BLEU, COMET) for evaluation. Confusing the two leads to invalid experimental loops.
  - Quick check question: If I use a QE model to pick the best translation, which specific metric families must I avoid using for the final score reporting to ensure validity?

- **Concept: TFLOPs Accounting for Transformers**
  - Why needed here: To understand the "Efficiency" claims, one must grasp how compute scales with parameters vs. sequence length and candidate count.
  - Quick check question: In the formula C_total ≈ 2N_θ(P + N_cand T), which variable represents the linear scaling of inference cost with the number of candidates?

- **Concept: Zero-Shot Translation**
  - Why needed here: The authors do not fine-tune the Qwen models on parallel data, defining the baseline capability before TTS is applied.
  - Quick check question: Does the "Best-of-N" mechanism require the base model to be fine-tuned on the target language pair to work, or does it leverage the model's pre-training distribution?

## Architecture Onboarding

- **Component map:** Generator (Qwen2.5 Instruct) -> Sampler (vLLM with temp=1.0, top-p=0.95) -> Scorer (KIWI22 QE) -> Selector (Argmax)

- **Critical path:** Batch Generation (primary bottleneck) -> Scoring -> Selection

- **Design tradeoffs:** VRAM vs. Latency (use 14B + TTS for ~28GB vs. 72B + N=1 for ~114GB); Metric Reliability (TTS relies heavily on Scorer)

- **Failure signatures:** Code-switching in low-resource settings (look for mixed scripts with high KIWI scores); Diminishing Returns (metrics plateau after N > 16)

- **First 3 experiments:**
  1. Sanity Check: Replicate N={1, 8, 32} scaling curve on En-De using 7B model to verify pipeline
  2. Failure Mode Analysis: Run N=64 on En-Is and inspect selected translations for code-switching artifacts
  3. Efficiency Boundary: Measure latency of 14B at N=16 vs. 72B at N=1; calculate quality gain per second

## Open Questions the Paper Calls Out

- **Open Question 1:** How can TTS be adapted to prevent quality degradation in low-resource translation caused by code-switching?
  - Basis: Authors state "Future work should also explore... strategies to mitigate low-resource regressions" and show QE models assign high scores to code-switched outputs
  - Why unresolved: Current QE models reward fluent but hallucinated code-switched text, causing performance to drop as N increases
  - What evidence would resolve it: Developing a selection method that penalizes code-switching, restoring positive correlation between N and quality

- **Open Question 2:** Do TTS scaling laws generalize to model architectures other than Qwen2.5?
  - Basis: Limitations note experiments focus solely on Qwen2.5 family and results may not generalize
  - Why unresolved: Specific scaling curves may be artifacts of Qwen architecture or training data
  - What evidence would resolve it: Replicating Best-of-N framework on diverse decoder-only architectures (Llama, Mistral)

- **Open Question 3:** How can systems dynamically determine optimal allocation between model size and N?
  - Basis: Authors call for "adaptive approaches that jointly optimize model size and inference-time budget"
  - Why unresolved: Paper analyzes trade-offs statically, unclear how to best allocate compute on per-instance basis
  - What evidence would resolve it: Algorithm that adaptively selects N or model size based on input difficulty

## Limitations

- Metric selection and interference concerns due to using KIWI22 for selection but multiple reference-based metrics for evaluation
- Computational cost analysis doesn't account for practical considerations like batch processing or distributed inference
- Single low-resource pair (English-Icelandic) may not represent full spectrum of low-resource scenarios
- No systematic method provided for determining optimal N value for given model size and language pair

## Confidence

- **High Confidence:** TTS consistently improves translation quality in high-resource settings, with small models at large N matching or surpassing larger models at N=1
- **Medium Confidence:** Under fixed compute budgets, larger models are generally more compute-efficient than smaller models with large N
- **Low Confidence:** TTS can degrade quality in low-resource settings due to metric blind spots

## Next Checks

1. Multi-Metric Validation: Repeat main experiments using different QE models for selection (COMET2, DeText) to verify scaling behavior consistency across reference-free scoring approaches, particularly for low-resource English-Icelandic pair

2. Cross-Lingual Generalization: Test TTS approach on additional low-resource language pairs (English-Gujarati, English-Swahili) to determine whether code-switching failure mode is generalizable or specific to English-Icelandic case

3. Dynamic N Optimization: Implement and evaluate a method for dynamically determining optimal N value based on source sentence characteristics (length, complexity, domain) rather than using fixed N across all inputs