---
ver: rpa2
title: 'Query Circuits: Explaining How Language Models Answer User Prompts'
arxiv_id: '2509.24808'
source_url: https://arxiv.org/abs/2509.24808
tags:
- query
- circuit
- circuits
- edges
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of explaining why language models
  produce specific outputs for individual input queries. While prior work has identified
  global capability circuits (e.g., for indirect object identification), these do
  not explain model behavior on specific prompts.
---

# Query Circuits: Explaining How Language Models Answer User Prompts

## Quick Facts
- arXiv ID: 2509.24808
- Source URL: https://arxiv.org/abs/2509.24808
- Reference count: 40
- Key outcome: Query circuits directly trace information flow within models to explain specific input-output mappings, with extremely sparse circuits (1.3% edges) recovering 60% performance on MMLU questions.

## Executive Summary
Language models' outputs are typically explained through global capability circuits, but these don't account for specific input variations. This paper introduces query circuits—sparse subgraphs within a model that directly map a specific input to its output. The authors develop Normalized Deviation Faithfulness (NDF) to evaluate circuit faithfulness, addressing instability in existing metrics. They propose Best-of-N sampling with paraphrases to overcome gradient noise and combinatorial effects in edge scoring. Across benchmarks including IOI, arithmetic, MMLU, and ARC Challenge, experiments demonstrate that query circuits can be extremely sparse yet recover substantial model performance, offering a practical path toward faithful, scalable explanations of how language models process individual inputs.

## Method Summary
The method constructs query circuits by first generating paraphrases of a given input query, then computing edge importance scores using Edge Attribution Patching with Integrated Gradients (EAP-IG). For each paraphrase, a separate edge score matrix is computed, and circuits are constructed via greedy selection of top-scoring edges. The Best-of-N (BoN) approach selects the highest-faithfulness circuit across all paraphrases. Faithfulness is evaluated using Normalized Deviation Faithfulness (NDF), which measures the circuit's performance relative to the full model and corrupted baseline. Variants include interpolated BoN (iBoN) and BoN with Constraint-adaptive Score Matrix (BoN-CSM) for improved efficiency and performance.

## Key Results
- Query circuits can be extremely sparse while recovering substantial performance: a circuit covering only 1.3% of model connections recovered about 60% of performance on MMLU questions
- Best-of-N sampling with paraphrases consistently outperforms single-query edge scoring, particularly for complex queries where gradient noise is problematic
- NDF provides stable, bounded evaluation (0-1) compared to NFS, which can fluctuate wildly on complex queries
- Across IOI, arithmetic, MMLU, and ARC Challenge benchmarks, query circuits demonstrate practical feasibility for explaining model behavior on specific inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edge attribution via integrated gradients approximates indirect effects for transformer edges
- Mechanism: EAP-IG computes edge importance scores by integrating gradients along a path from corrupted to clean inputs, enabling parallel scoring of all edges in fixed forward passes
- Core assumption: Gradient-based attribution approximates causal indirect effects; gradients are meaningful proxies for edge importance despite noise
- Evidence anchors: Abstract states the paper develops sampling-based methods building on attribution patching; Section 2.2.1 describes integrated gradients formulation and computational advantage over edge activation patching methods

### Mechanism 2
- Claim: Best-of-N sampling leverages paraphrases to overcome gradient noise and combinatorial effects
- Mechanism: Generate p paraphrases of the original query, compute separate edge score matrices for each, construct circuits from each matrix, and select the circuit with highest faithfulness
- Core assumption: Paraphrases preserve critical information flow while perturbing noise in edge scores; at least one paraphrase-derived circuit will achieve high faithfulness
- Evidence anchors: Abstract proposes Best-of-N sampling to efficiently identify faithful query circuits using paraphrases; Section 5.1 shows circuits from paraphrases can succeed when original query fails

### Mechanism 3
- Claim: Normalized Deviation Faithfulness (NDF) provides stable, bounded evaluation for query circuits
- Mechanism: NDF normalizes circuit-model performance deviation by the model's clean-corrupted gap, clips to [0,1], and is symmetric around model performance
- Core assumption: The clean-corrupted performance gap is non-trivial and meaningful; deviation is an appropriate faithfulness measure
- Evidence anchors: Abstract introduces NDF as robust metric bounded in [0,1]; Section 4 defines NDF and demonstrates improved stability vs NFS on MMLU queries

## Foundational Learning

- Concept: Transformer Circuits (Residual Stream + Attention/MLP Edges)
  - Why needed here: Understanding nodes (attention heads, MLPs) and edges (residual connections) is prerequisite to interpreting edge score matrices and constructing circuits
  - Quick check question: Can you trace how layer-5 attention head 3's output contributes to layer-8 MLP input?

- Concept: Activation/Attribution Patching and Indirect Effects
  - Why needed here: The paper relies on edge attribution patching (EAP-IG) to compute edge importance; understanding causal intervention via corruption is essential
  - Quick check question: Explain the computational difference between edge activation patching (ACDC) and edge attribution patching (EAP-IG) in forward passes required

- Concept: Evaluation Metrics for Circuit Faithfulness (NFS vs NDF)
  - Why needed here: The paper critiques NFS and proposes NDF; understanding why NFS can be unbounded helps evaluate circuits robustly
  - Quick check question: For a circuit slightly outperforming the full model on a query, what would NFS vs NDF report?

## Architecture Onboarding

- Component map: Input query → Token embeddings → Edge Scoring (EAP-IG) → Edge score matrix → Circuit Construction (Greedy) → Query circuit
- Critical path: 1) Generate paraphrases of query (if using BoN) 2) For each (query, paraphrase), compute edge score matrix S via EAP-IG 3) Construct candidate circuits at various edge budgets using greedy selection 4) Evaluate each circuit with NDF; for BoN, select highest-NDF circuit per budget 5) Optional: Use iBoN or BoN-CSM to interpolate or re-rank edges across budgets efficiently
- Design tradeoffs: EAP-IG step size (m=20); Number of paraphrases (p=9); Circuit construction: Greedy vs Dijkstra-like; Metric choice: NFS for toy tasks, NDF for complex queries
- Failure signatures: NDF near 0 for all circuits (check corrupted query design); NFS fluctuating outside [0,1] (switch to NDF); BoN not improving over baseline (paraphrases may not preserve critical edges); Needing >50% edges for NDF>0.5 (edge scoring unreliable)
- First 3 experiments: 1) Replicate Figure 2a on MMLU: compute NFS and NDF for circuits of varying sizes on 3 queries to observe NFS instability vs NDF stability 2) Implement EAP-IG edge scoring with m=20 on GPT-2 Small for IOI; compare greedy vs Dijkstra-like construction at N=1000 edges 3) Run BoN (p=9 paraphrases) on one MMLU category; plot NDF vs edge budget comparing to single-query and averaging baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can circuit discovery methods be adapted to handle multi-token generation tasks where attribution must span multiple sequential forward passes?
- Basis in paper: [explicit] The authors note in Appendix D that "this work focuses on queries whose outputs are single tokens" and attributing components for "multi-token generations is complex, and no existing studies have fully addressed this challenge."
- Why unresolved: Current attribution metrics (like EAP-IG) and faithfulness scores assume a single prediction step (e.g., the logit of one token)
- What evidence would resolve it: A modified discovery algorithm that successfully traces information flow across a sequence of generated tokens while maintaining faithfulness

### Open Question 2
- Question: Can circuit discovery algorithms be improved to efficiently capture combinatorial interactions between edges rather than relying on additive indirect effects?
- Basis in paper: [explicit] Appendix D identifies the "fundamental limitation of using indirect effects as edge scores: the neglect of combinatorial interactions among edges."
- Why unresolved: Fully accounting for these interactions is currently NP-hard, leading methods to use linear approximations that may miss critical circuit logic
- What evidence would resolve it: A method that incorporates interaction effects (e.g., Shapley values) without exponential runtime costs, yielding higher faithfulness scores

### Open Question 3
- Question: How can automated interpretability techniques be integrated to explain the function of polysemantic nodes within query circuits?
- Basis in paper: [explicit] The paper states in Appendix D that "raw model components are more polysemantic and thus hard to explain," requiring an automated method to interpret identified edges
- Why unresolved: Unlike surrogate models (e.g., SAEs) which yield monosemantic features, internal model components lack direct semantic labels, making manual investigation unscalable
- What evidence would resolve it: A pipeline that automatically generates human-readable descriptions for specific nodes (e.g., attention heads) within a discovered query circuit

## Limitations

- Gradient-based attribution faithfulness: The assumption that gradients reliably approximate causal indirect effects for transformer edges remains empirically unverified for query circuits, with noise and combinatorial effects potentially causing failures
- Paraphrase effectiveness: Success of Best-of-N sampling hinges on paraphrases preserving critical information flow while perturbing noise, but no corpus evidence confirms this mechanism
- Generalization across tasks: While results span multiple benchmarks, the extreme sparsity achieved (1.3% edges for 60% performance) may not generalize to more complex reasoning tasks or larger models

## Confidence

- High confidence: NDF provides more stable evaluation than NFS for query circuits (Section 4 evidence with Table 1 and Figure 3 showing NFS instability on MMLU)
- Medium confidence: Best-of-N sampling improves circuit discovery over single-query baseline (Figure 6 and 7 show consistent improvements, though magnitude varies by task)
- Medium confidence: Query circuits can be extremely sparse yet recover substantial performance (Figure 9 shows consistent sparsity patterns across tasks, but generalization to new domains unclear)

## Next Checks

1. **Gradient attribution validation**: Implement edge activation patching (ACDC) on a subset of MMLU queries to compare edge rankings with EAP-IG; measure correlation in top-k edges selected and resulting circuit performance
2. **Paraphrase perturbation analysis**: For one MMLU category, analyze edge score matrix variance across paraphrases; identify which edge groups show highest variance and correlate with circuit success/failure
3. **Scaling experiment**: Apply query circuit discovery to Llama-3.2-1B-Instruct on ARC Challenge; compare edge budget requirements and NDF scores to GPT-2 Small results to assess model size scaling effects