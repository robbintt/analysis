---
ver: rpa2
title: Understanding Silent Data Corruption in LLM Training
arxiv_id: '2502.12340'
source_url: https://arxiv.org/abs/2502.12340
tags:
- node
- sdcs
- training
- unhealthy
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates silent data corruption (SDC) in large language
  model (LLM) training by analyzing real-world unhealthy hardware nodes exhibiting
  SDCs. The authors develop a methodology to isolate SDC impacts at three levels:
  submodule computation, optimizer step gradients, and accumulated model quality over
  training.'
---

# Understanding Silent Data Corruption in LLM Training

## Quick Facts
- arXiv ID: 2502.12340
- Source URL: https://arxiv.org/abs/2502.12340
- Reference count: 40
- Primary result: SDC perturbations on gradients are small relative to true gradients but accumulate to cause models to converge to different optima with drifting parameters and occasional loss spikes during fine-tuning.

## Executive Summary
This work investigates silent data corruption (SDC) in large language model (LLM) training by analyzing real-world unhealthy hardware nodes exhibiting SDCs. The authors develop a methodology to isolate SDC impacts at three levels: submodule computation, optimizer step gradients, and accumulated model quality over training. They implement synchronization mechanisms that overwrite intermediate results from healthy nodes to measure SDC effects precisely. Their results show that while SDC perturbations on gradients are relatively small, they can cause models to converge to different optima with drifting parameters and occasionally trigger loss spikes during fine-tuning, potentially corrupting model weights.

## Method Summary
The authors investigate SDC impacts by pairing unhealthy nodes (identified by production fleet management) with healthy nodes running identical training workloads. They use three synchronization mechanisms: lock-step parallelism to isolate submodule computation errors, parameter synchronization to measure per-step gradient perturbations, and full training without synchronization to observe accumulated impacts. The methodology employs deterministic training with XLA, tensor parallelism with ZeRO-1 and sequence parallelism, and monitors mismatch frequency, gradient differences, parameter drift, and loss behavior across pre-training and fine-tuning tasks.

## Key Results
- SDC-induced gradient noise is small (WCNTS ratio of 5.1%) relative to true gradients but accumulates to cause parameter drift over time
- Model weights on unhealthy nodes incrementally drift away from healthy nodes, pushing optimization trajectories onto different paths
- SDC impacts are closely related to loss surface sharpness, with sharp regions more susceptible to catastrophic loss spikes and weight corruption

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent hardware defects cause non-uniform, localized silent data corruption (SDC) in Transformer submodule computations (attention/FFN) during training.
- Mechanism: Hardware faults (e.g., from manufacturing or stress) produce incorrect floating-point results without explicit failure signals. These errors appear as mismatched tensor values in forward/backward passes, with frequency and severity varying significantly across different unhealthy nodes.
- Core assumption: The 15 unhealthy nodes identified by production fleet management are representative of real-world SDC behavior in large-scale training clusters.
- Evidence anchors:
  - [abstract] "hardware produces incorrect computations without explicit failure signals."
  - [Section 4.2] "SDCs do not occur uniformly during training... spikes of mismatch frequency sometimes occur... impact of SDCs on computation varies on different unhealthy nodes."
  - [corpus] The paper "A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults" addresses predicting SDCs from circuit-level defects, supporting the hardware origin, but lacks direct evidence for LLM training patterns.
- Break condition: If hardware were perfectly reliable or if error-correcting codes (ECC) caught all compute-level errors, this mechanism would not operate.

### Mechanism 2
- Claim: Single-step SDC-induced gradient noise is small relative to true gradients, but cumulative errors cause model parameters to drift toward different optima over time.
- Mechanism: An SDC event perturbs a submodule output → error propagates via backpropagation → creates a small noise component in gradients for that step → the optimizer applies a slightly perturbed update → over hundreds or thousands of steps, this leads to significant parameter divergence, even while training loss appears normal.
- Core assumption: The optimization landscape allows small, persistent perturbations to alter the final convergence point without immediately destabilizing training.
- Evidence anchors:
  - [abstract] "SDCs can lead models to converge to different optima with different weights."
  - [Section 5.2] "Worst Case Noise-to-Signal (WCNTS) ratios... 5.1%... SDC-induced noise in the gradients is relatively small relative to the ground-truth gradients."
  - [Section 6.2] "model weights on unhealthy nodes incrementally drift away from those on the healthy node... pushing the optimization trajectory onto a different path."
  - [corpus] No direct corpus evidence for this specific drift mechanism in LLMs.
- Break condition: If SDC-induced perturbations were large enough to cause immediate loss divergence or NaN values, or if the optimizer's learning rate was so low that noise was effectively averaged out.

### Mechanism 3
- Claim: The severity of SDC impact on model quality is mediated by the sharpness of the loss surface; in sharp regions, small perturbations can trigger sudden loss spikes and catastrophic model corruption.
- Mechanism: An SDC introduces a small parameter perturbation. In a "flat" region of the loss landscape, this has minimal effect on the loss. In a "sharp" region, the same perturbation causes a disproportionately large increase in loss (a spike), which can destabilize subsequent training steps and fully corrupt model weights.
- Core assumption: The observed correlation between loss spikes and SDCs is causal and not confounded by other factors like data order or hyperparameter sensitivity.
- Evidence anchors:
  - [abstract] "SDCs... can cause spikes in the training loss."
  - [Section 6.2] "rates at which the parameter differences increase are similar... suggesting that the rate of parameter drift is more likely to be driven by the sharp loss surface."
  - [Section 7.2] "impact of SDCs on model quality is closely related to the sharpness of the loss surface."
  - [corpus] No direct corpus evidence for this loss surface mediation hypothesis.
- Break condition: If the model converged exclusively to flat minima where loss is robust to small perturbations, or if robust training techniques (e.g., Sharpness-Aware Minimization) were used to flatten the landscape.

## Foundational Learning

- Concept: **Silent Data Corruption (SDC)**
  - Why needed here: This is the core hardware failure mode being investigated. One must grasp that SDCs are "silent" errors—incorrect computations that do not trigger a system crash or error flag, making them particularly insidious in long-running training jobs.
  - Quick check question: How does an SDC differ from a hardware failure that causes a node to crash or reboot?

- Concept: **Tensor Parallelism (TP)**
  - Why needed here: The paper's experimental design isolates SDC effects to a single node using TP, where each accelerator holds a shard of the model layers. This setup is critical for understanding how SDCs can corrupt specific parts of a model.
  - Quick check question: Why is TP the chosen parallelism strategy for this study's node-level analysis, as opposed to data parallelism?

- Concept: **Loss Surface Sharpness**
  - Why needed here: The paper posits a link between SDC severity and the geometry of the loss landscape. Understanding the difference between "sharp" and "flat" minima is essential for interpreting the findings on parameter drift and loss spikes.
  - Quick check question: According to the paper, is a model more vulnerable to SDC-induced corruption when it is in a sharp or a flat region of the loss landscape?

## Architecture Onboarding

- Component map:
  - Production Fleet Management Flow -> Unhealthy Node Flagging -> Unhealthy/Healthy Node Pair Formation -> Lock-step Parallelism / Parameter Synchronization -> SDC Impact Analysis

- Critical path:
  1. A node is flagged as **unhealthy** by the fleet management flow after failing a deterministic stress test.
  2. The unhealthy node is paired with a known-healthy node.
  3. Identical LLM training jobs are launched on both nodes.
  4. To investigate RQ1 (submodule impact), **lock-step parallelism** is used to compare and overwrite intermediate tensors from each Transformer layer.
  5. To investigate RQ2 (gradient impact), **parameter synchronization** is used to reset model weights after each optimizer step, allowing for per-step gradient comparison.
  6. To investigate RQ3 (accumulated impact), both synchronizations are disabled, and the training loss and parameter drift are monitored over time.

- Design tradeoffs:
  - **Measurement Fidelity vs. SDC Occurrence:** The synchronization mechanisms add communication overhead, which reduces accelerator utilization. This, in turn, appears to *lower the frequency* of SDCs, creating a tradeoff where finer-grained measurement may hide the natural rate of corruption.
  - **Real-World Validity vs. Reproducibility:** Using real unhealthy nodes provides crucial real-world data, but SDCs are non-deterministic and the supply of such nodes is limited, making experiments difficult to replicate at scale.

- Failure signatures:
  - **Silent Parameter Drift:** The most dangerous signature. Training loss appears identical to a healthy run, but model parameters slowly diverge, resulting in a different final model with potentially different behavior.
  - **Catastrophic Loss Spike:** A sudden, sharp increase in loss, potentially leading to `NaN` or `Inf` values. This is more common in fine-tuning and can fully corrupt the model.
  - **Submodule Tensor Mismatch:** The atomic signature of an SDC, measured as a non-zero "mismatch frequency" or "mismatch severity" when comparing tensor outputs between healthy and unhealthy nodes.

- First 3 experiments:
  1. **Validate Determinism:** Run the same training workload on two known-healthy nodes to confirm that results are bit-wise identical, ensuring that any observed differences on unhealthy nodes are due to SDC and not software non-determinism.
  2. **Submodule Characterization (RQ1):** On an unhealthy/healthy node pair, enable lock-step parallelism. Train for a few thousand steps and log the mismatch frequency and severity for attention and FFN layers. Analyze if SDCs are uniform or spiky.
  3. **Accumulated Impact Test (RQ3):** On an unhealthy/healthy node pair, disable all synchronization. Run a short pre-training or fine-tuning job. Monitor training loss for spikes and compare final model weights to the healthy baseline to measure parameter drift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can methods that reduce loss surface sharpness (e.g., sharpness-aware minimization) reduce parameter divergence and loss spikes caused by SDCs during LLM training?
- Basis in paper: [explicit] Section 7.2 states: "Future work could examine whether methods that reduce the sharpness of the loss surface or avoid optimization towards sharp regions can help reduce the divergence of model parameters and loss spikes caused by SDCs."
- Why unresolved: The paper identifies a correlation between SDC impact and loss surface sharpness but does not experimentally test mitigation strategies.
- What evidence would resolve it: Controlled experiments comparing training with and without sharpness-reducing methods on unhealthy nodes, measuring parameter drift and loss spike frequency.

### Open Question 2
- Question: Can a shadow data-parallel replica reliably detect SDCs during gradient all-reduce with acceptable overhead?
- Basis in paper: [explicit] Section 7.2 proposes: "One possible approach is using an additional shadow data-parallel replica during training... The gradients from the shadow data-parallel replica and the target data-parallel rank can be compared during gradient all-reduce."
- Why unresolved: The paper only hypothesizes this detection approach aligned with Gemini's SDC scanners but does not implement or evaluate it.
- What evidence would resolve it: Implementation and benchmarking of shadow replica detection, reporting precision, recall, latency, and compute overhead.

### Open Question 3
- Question: How do SDC impacts propagate in large-scale multi-node training with combined data, tensor, and pipeline parallelism?
- Basis in paper: [explicit] Section 9 (Limitations): "our study focuses on training with tensor parallelism only on a single node... we could not run large-scale training with more complex parallelism strategies."
- Why unresolved: Resource constraints limited experiments to single-node tensor parallelism; SDC behavior in distributed settings with inter-node communication remains uncharacterized.
- What evidence would resolve it: Experiments running the same healthy-unhealthy node methodology with full 3D parallelism across multi-node clusters.

### Open Question 4
- Question: Can algorithm-based fault tolerance (ABFT) be extended to reliably detect SDCs in low-precision datatypes (bf16, fp8) used in modern LLM training?
- Basis in paper: [inferred] Appendix D shows ABFT fails at bf16 due to floating-point error bound assumptions; the paper notes "more work in error analysis is needed to extend ABFT methods properly to low precision datatypes."
- Why unresolved: ABFT theoretical thresholds assume fp32 unit-roundoff; bf16/fp8 violate these assumptions, causing false positives on healthy nodes.
- What evidence would resolve it: Derivation of revised ABFT thresholds for low-precision formats and empirical validation of detection precision/recall.

## Limitations

- Limited to 15 real-world unhealthy nodes, constraining statistical power and generalizability
- Synchronization overhead may suppress natural SDC occurrence rates, potentially biasing measurements
- Focus on single-node tensor parallelism doesn't characterize SDC behavior in distributed multi-node training
- Non-deterministic SDC nature makes it difficult to reproduce specific failure patterns

## Confidence

- **High Confidence:** The characterization of SDC as silent, non-crashing compute errors that produce mismatched tensor values between healthy and unhealthy nodes
- **Medium Confidence:** The claim that SDC-induced gradient noise is small relative to true gradients but accumulates to cause parameter drift
- **Medium Confidence:** The relationship between loss surface sharpness and SDC severity

## Next Checks

1. **Reproduce Determinism Baseline:** Before investigating unhealthy nodes, verify that two known-healthy nodes produce bit-wise identical outputs under the same training configuration. This establishes that any observed differences are due to SDC rather than software non-determinism.

2. **Validate Submodule Isolation:** Use the lock-step synchronization mechanism to measure mismatch frequency and severity for attention and FFN layers across multiple unhealthy/healthy node pairs. Confirm whether SDCs show the non-uniform, spiky behavior described, and whether patterns are consistent across different unhealthy nodes.

3. **Test Loss Surface Hypothesis:** For a single unhealthy node, monitor training loss and parameter drift during both flat and sharp regions of the loss landscape. Compare the rate of parameter divergence and frequency of loss spikes between these regions to empirically validate the sharpness-mediated SDC impact hypothesis.