---
ver: rpa2
title: 'The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or
  Both? A Two-Stage Dynamic View'
arxiv_id: '2510.04028'
source_url: https://arxiv.org/abs/2510.04028
tags:
- arxiv
- preprint
- training
- policy
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study resolves the debate on whether reinforcement learning
  with verifiable rewards (RLVR) expands or shrinks large language models' (LLMs)
  reasoning capabilities by revealing a two-stage probability mass dynamic. Initially,
  RLVR favors exploitation, leading to capability boundary shrinkage, but prolonged
  training enables exploration and genuine capability expansion.
---

# The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View

## Quick Facts
- arXiv ID: 2510.04028
- Source URL: https://arxiv.org/abs/2510.04028
- Reference count: 40
- This study resolves the debate on whether reinforcement learning with verifiable rewards (RLVR) expands or shrinks large language models' (LLMs) reasoning capabilities by revealing a two-stage probability mass dynamic.

## Executive Summary
This paper resolves the long-standing debate about whether RLVR expands or shrinks LLMs' reasoning capabilities by revealing a two-stage probability mass dynamic. Initially, RLVR favors exploitation, leading to capability boundary shrinkage, but prolonged training enables exploration and genuine capability expansion. Theoretically and empirically, the authors show that both phenomena occur at different training phases. Guided by these insights, they propose using only relative negative gradients during training, implemented in GRPO-N and GSPO-N. Experiments demonstrate that these methods achieve competitive and stable performance improvements while largely preserving the base model's diversity, validating the potential for prolonged training to enhance reasoning capabilities.

## Method Summary
The paper compares standard GRPO/GSPO with modified variants (GRPO-N/GSPO-N) that use only relative negative gradients. The core change in "-N" variants is retaining only gradients where group-relative advantage Â < 0, masking out positive-advantage updates. Group size is fixed at G=8 per prompt, with importance sampling, KL penalty (1e-3), clip ratio (0.2), entropy bonus (1e-4), learning rate (1e-6), batch size (256), max length (4096), and sampling temperature (1.0). The training uses MATH dataset and evaluates on AMC 2023, AIME 2024/2025, ARC-c, and MMLU-Pro using Pass@k metrics for k ∈ {1,2,4,8,16,32,64,128,256}.

## Key Results
- GRPO-N and GSPO-N variants preserve entropy significantly better than standard GRPO while maintaining competitive Pass@1 performance
- Prolonged training with GRPO-N enables capability boundary expansion, reversing the shrinkage observed in early training stages
- Entropy preservation through negative gradient filtering is critical for sustained reasoning capability improvement at large k values

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Probability Mass Dynamic
RLVR exhibits two distinct phases—initial capability boundary shrinkage (exploitation) followed by potential expansion (exploration) with prolonged training. In exploitation, high-reward tokens dominate and their probability approaches 1, preventing exploration of optimal alternatives. During exploration, when high-reward tokens saturate (1−π→0), occasionally sampled optimal tokens with positive advantage estimates gain probability mass from formerly high-reward tokens. This bidirectional logit update rule depends on both advantage estimates and current policy distribution.

### Mechanism 2: Relative Negative Gradients Preserve Diversity
Using only relative negative gradients (GRPO-N, GSPO-N) achieves competitive performance while preserving base model diversity, enabling prolonged training. When relative advantage Â_i,t < 0, only the negative gradient term is applied, avoiding reinforcement of entire error-prone trajectories while maintaining policy entropy. This prevents early entropy collapse that typically occurs with standard GRPO, allowing continued exploration of the reasoning space.

### Mechanism 3: Bidirectional Logit Update Rule
For sampled action v with positive advantage: z_v increases by ηÂ(v)(1−π(v)); for all unsampled u≠v: z_u decreases by ηÂ(v)π(u). This creates competitive probability mass reallocation where increases come at the expense of all other tokens proportionally to their current probabilities. The update depends bidirectionally on the sampled action's advantage and all other tokens' probabilities.

## Foundational Learning

- Concept: Softmax Policy Gradient
  - Why needed here: The entire theoretical framework relies on understanding how logits z map to probabilities π via Softmax, and how gradients ∇_z log π(v) flow to update logits bidirectionally.
  - Quick check question: Given logits [2.0, 1.0, 0.1] and a sampled action with advantage +0.5, what happens to the probability of the highest-logit unsampled token?

- Concept: Group Relative Advantage (GRPO-style)
  - Why needed here: The two-stage dynamic only emerges when advantages are computed relative to a group of samples, creating the transition from reinforcing known-good tokens to discovering optimal ones.
  - Quick check question: If you sample 8 responses with rewards [1,1,1,1,0,0,0,0], what are the relative advantages for correct vs. incorrect responses?

- Concept: Probability Mass Reallocation Over Search Trees
  - Why needed here: The paper conceptualizes reasoning as tree search with O(V^T) complexity; capability boundary expansion requires reallocating probability mass from high-reward-but-suboptimal paths to optimal-but-underexplored paths.
  - Quick check question: Why does saturation of high-reward tokens (1−π→0) enable the exploration stage rather than preventing further learning?

## Architecture Onboarding

- Component map:
  Policy Model π_θ -> Group Sampler (G=8) -> Binary Reward Function r(x,y) -> Group Relative Advantage Estimator Â_i -> Gradient Filter (I(Â_i,t < 0)) -> Optimizer with clipping (ε=0.2), KL penalty (β=10^−3), entropy bonus (10^−4)

- Critical path:
  1. Sample G responses per prompt from π_θ at temperature τ=1.0
  2. Compute binary rewards and group-relative advantages
  3. For -N variants: retain only gradients where Â < 0
  4. Apply bidirectional logit updates per Lemma 1 across all tokens
  5. Monitor held-out entropy and Pass@k to detect exploitation→exploration transition

- Design tradeoffs:
  1. Group size G: Larger G stabilizes advantage estimation but increases compute; G=8 used in experiments
  2. Training duration: Early stopping risks permanent shrinkage; prolonged training enables expansion but requires stability mechanisms
  3. Standard GRPO vs. GRPO-N: Standard achieves competitive Pass@1 but collapses entropy; -N variants preserve diversity, critical for Pass@k at large k
  4. Temperature: Higher sampling temperature (τ>1) increases exploration during rollout, aiding discovery of optimal tokens

- Failure signatures:
  1. Entropy collapse on test set: Model stuck in exploitation stage, overfitting to known solutions
  2. Pass@1 improves but Pass@k (k≥16) degrades vs. base model: Classic shrinkage—capability boundary narrowed
  3. Gradient norm explosion: Advantage estimates poorly scaled; check reward normalization
  4. Repetitive errors in trajectories (e.g., TypeError loops in Figure 3): Positive gradients reinforcing mistake-containing paths

- First 3 experiments:
  1. Reproduce the 3-action toy example (Algorithm 1) with rewards [0.85, 1.0, 0] and varying initial probabilities to visually confirm the two-stage probability dynamic
  2. Train GRPO vs. GRPO-N on MATH-500 for 40+ steps, plotting entropy curves on a held-out split to verify that -N variants maintain/increase entropy while standard GRPO collapses
  3. Evaluate Pass@k for k∈{1,2,4,8,16,32,64,128,256} at checkpoints {10, 20, 30, 40} steps to identify when shrinkage reverses to expansion

## Open Questions the Paper Calls Out

- Question: How can efficient algorithms be designed to achieve fine-grained probability mass allocation that optimizes the transition from exploitation to exploration?
  - Basis in paper: [explicit] The authors explicitly state in the Discussion that "further studies are required on... how to design efficient algorithms for fine-grained probability mass allocation."
  - Why unresolved: While the paper demonstrates that probability mass dynamics drive capability changes, the proposed solution (GRPO-N) relies on a relative negative gradient heuristic rather than a direct, optimized control mechanism for mass allocation.
  - What evidence would resolve it: The development of an optimization algorithm that actively manages probability mass distribution across the search tree, demonstrating superior boundary expansion compared to implicit methods like GRPO-N.

- Question: Which specific architectural or pre-training characteristics of base models make them most conducive to capability boundary expansion during the RL stage?
  - Basis in paper: [explicit] The Discussion calls for research into "what kind of base models are more conducive to capability boundary expansion during the RL stage."
  - Why unresolved: The empirical results show different absolute performance ceilings for Qwen2.5 and Llama-3.2, but the study does not isolate the factors (e.g., initial entropy, pre-training data diversity) that determine a model's potential for expansion.
  - What evidence would resolve it: A comparative analysis of diverse base models linking specific pre-training attributes to the magnitude of capability expansion observed during prolonged training.

- Question: Where is the ultimate ceiling of boundary exploration in RLVR, and does performance eventually plateau or degrade?
  - Basis in paper: [explicit] The authors highlight the need to determine "where the ceiling of boundary exploration lies."
  - Why unresolved: The paper validates that prolonged training leads to expansion, but it remains unclear if this expansion is theoretically unbounded or if it eventually hits a hard limit imposed by the model's capacity or data constraints.
  - What evidence would resolve it: Long-duration training curves that identify a convergence point or "collapse" threshold where further optimization steps no longer yield novel reasoning strategies.

## Limitations

- The two-stage dynamic theory depends on specific conditions (high-reward token saturation) that may not reliably occur across diverse problem distributions
- The framework assumes softmax policies and binary rewards, limiting generalizability to non-verifiable tasks or continuous reward settings
- The empirical evidence for genuine capability expansion beyond base model capabilities is suggestive but not conclusive

## Confidence

- **High confidence**: The mathematical derivation of the bidirectional logit update rule (Lemma 1) and its dependence on current policy distribution and advantage estimates
- **Medium confidence**: The empirical demonstration that GRPO-N variants preserve entropy and achieve competitive Pass@k while standard GRPO collapses
- **Medium confidence**: The claim that capability boundary shrinkage occurs during initial exploitation
- **Low confidence**: The assertion that prolonged training reliably enables capability boundary expansion

## Next Checks

1. **Test the transition trigger**: Design an experiment varying the reward distribution and initial policy to verify when the exploration stage begins. Specifically, train on a controlled synthetic task where you can precisely control when high-reward tokens saturate, and measure whether the transition to exploration occurs exactly when predicted by the theory (when 1−π→0 for high-reward tokens).

2. **Validate the negative gradient hypothesis**: Run ablation studies on GRPO-N variants that apply negative gradients selectively—only to sampled tokens, only to unsampled tokens, or with different thresholds for Â < 0. Compare entropy preservation and Pass@k performance to isolate which aspects of the negative gradient mechanism are critical for the observed benefits.

3. **Test across reward types**: Implement GRPO-N with non-binary rewards (e.g., partial credit scoring) and evaluate whether the two-stage dynamic and entropy preservation still hold. This would validate whether the framework generalizes beyond verifiable binary reward settings.