---
ver: rpa2
title: 'DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play'
arxiv_id: '2511.13186'
source_url: https://arxiv.org/abs/2511.13186
tags:
- learning
- difffp
- policy
- diffusion
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffFP introduces a fictitious play framework that learns robust
  multi-modal policies in continuous-action zero-sum games by leveraging diffusion
  models. The method approximates best responses through iterative self-play, where
  a diffusion policy is trained to represent diverse strategies against an evolving
  average opponent strategy.
---

# DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play

## Quick Facts
- **arXiv ID:** 2511.13186
- **Source URL:** https://arxiv.org/abs/2511.13186
- **Reference count:** 40
- **Primary result:** Diffusion-based fictitious play learns robust multi-modal policies in continuous-action zero-sum games, achieving up to 3× faster convergence and 30× higher success rates compared to RL-based baselines.

## Executive Summary
DiffFP introduces a fictitious play framework that learns robust multi-modal policies in continuous-action zero-sum games by leveraging diffusion models. The method approximates best responses through iterative self-play, where a diffusion policy is trained to represent diverse strategies against an evolving average opponent strategy. This approach addresses the limitations of unimodal RL policies, which tend to overfit to recent opponent behaviors. Empirical results show that DiffFP achieves up to 3× faster convergence and 30× higher success rates compared to RL-based baselines, converging toward ε-Nash equilibria in continuous-space games. The learned policies demonstrate robustness to unseen opponents across complex multi-agent environments, including racing and multi-particle zero-sum games.

## Method Summary
DiffFP implements fictitious play using diffusion policies as approximate best-response learners in continuous zero-sum games. The framework trains a conditional diffusion model to generate actions conditioned on observations, using a denoising objective combined with Q-guided action refinement. During each fictitious play iteration, policies are trained against a weighted historical opponent pool, with the average strategy updated incrementally. The method employs double Q-learning for critic training and uses action gradients to refine diffusion samples before storage in the replay buffer. Centralized training occurs with decentralized execution, targeting ε-Nash equilibrium convergence through iterative best-response approximation.

## Key Results
- Achieves up to 3× faster convergence toward ε-Nash equilibria compared to RL-based baselines
- Demonstrates 30× higher success rates in continuous zero-sum games
- Shows robustness to unseen opponents across complex multi-agent environments including racing and multi-particle zero-sum games

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diffusion policies capture multimodal best responses that unimodal Gaussian policies cannot, reducing strategic exploitation.
- **Mechanism:** The diffusion model's denoising process (Equations 5–9) learns a score function over the action distribution conditioned on observations. This allows the policy to represent stochastic mixtures over diverse actions—e.g., both attacking left and defending right—as separate modes rather than collapsing to their mean. During FP, this prevents the cyclic forgetting observed when unimodal policies overfit to recent opponent behaviors.
- **Core assumption:** The optimal policy in continuous zero-sum games is genuinely multimodal; if the true best response is unimodal, diffusion adds computational overhead without benefit.
- **Evidence anchors:**
  - [abstract]: "achieves up to 3× faster convergence and 30× higher success rates"
  - [section IV-B]: "Traditional unimodal policies (e.g., Gaussian actors) tend to collapse to a single dominant mode, whereas diffusion policies naturally support multi-modal action distributions"
  - [corpus]: Weak direct evidence—neighbor papers focus on self-play broadly but not diffusion policy representations.
- **Break condition:** If exploitability curves show no improvement over Gaussian policies in simple unimodal games, the mechanism is not providing benefit.

### Mechanism 2
- **Claim:** Fictitious Play with explicit average strategy modeling stabilizes training by reducing non-stationarity in multi-agent learning.
- **Mechanism:** Rather than training against the most recent opponent (which creates a moving target), DiffFP samples from a historical pool weighted by FP update coefficients (Algorithm 1, line 5–6). This approximates playing against the empirical distribution π̄ = Σₖ αₖπₖ, providing a stationary learning target for best-response computation.
- **Core assumption:** The game admits approximate Nash equilibria reachable through FP dynamics; non-transitive games with persistent cycles may not converge.
- **Evidence anchors:**
  - [abstract]: "converging toward ε-Nash equilibria in continuous-space games"
  - [section III]: References generalized weakened fictitious play [21] allowing approximate best responses
  - [corpus]: SPIRAL paper validates self-play incentives but doesn't directly address FP stability mechanisms.
- **Break condition:** If exploitability oscillates without decreasing over FP iterations, the averaging is insufficient to stabilize dynamics.

### Mechanism 3
- **Claim:** Q-guided action refinement improves sample efficiency by directing diffusion samples toward higher-value regions.
- **Mechanism:** After sampling actions from the diffusion policy, Alg. 2 (lines 16–18) performs N gradient ascent steps: ãⁿ⁺¹ ← ãⁿ + η∇ₐQ(s, a). This locally optimizes sampled actions before storage in the replay buffer, combining the expressiveness of generative modeling with directed Q-improvement.
- **Core assumption:** The Q-function is sufficiently accurate to guide action refinement; noisy Q-estimates in non-stationary multi-agent settings may misguide refinement.
- **Evidence anchors:**
  - [section IV-B]: "Policy improvement is achieved through action gradients that refine actions sampled from the replay buffer"
  - [section V]: QSM baseline suffers instability from Q-value reliance, suggesting this assumption is non-trivial
  - [corpus]: No direct corpus evidence for this specific Q-diffusion integration.
- **Break condition:** If increasing refinement steps N degrades performance or increases variance, Q-guidance is introducing noise rather than signal.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** The policy is implemented as a conditional diffusion model—you must understand the forward/reverse process (Equations 5–6), noise schedules, and the denoising objective (Equation 7).
  - **Quick check question:** Can you explain why the reverse process requires learning to predict the noise ε rather than directly predicting the clean action?

- **Concept: Fictitious Play and Nash Equilibrium**
  - **Why needed here:** DiffFP is built on game-theoretic FP; you need to understand best responses, empirical distributions, and why ε-Nash is the convergence target.
  - **Quick check question:** In rock-paper-scissors, why does FP eventually converge to (⅓, ⅓, ⅓) despite players only playing pure strategies at each iteration?

- **Concept: Actor-Critic RL with Double Q-Learning**
  - **Why needed here:** The critic uses double Q-learning (Equation 8) to reduce overestimation, and policy improvement uses Q-gradients.
  - **Quick check question:** Why does Double Q-learning use two Q-networks with minimum over their estimates?

## Architecture Onboarding

- **Component map:** Opponent Policy Pool -> Diffusion Actor (πθ) -> Double Critics (Qψ₁, Qψ₂) -> Replay Buffer D -> FP Controller -> Average Strategy π̄

- **Critical path:**
  1. Initialize policies randomly; set up noise schedule {αₜ}
  2. For each FP iteration k: sample opponent from historical pool
  3. Collect experience via diffusion policy sampling (Eq. 9)
  4. Train critics on TD loss; train actor on denoising loss + Q-guided refinement
  5. Update average strategy: πₖ₊₁ ← (k/k+1)πₖ + (1/k+1)πᴮᴿ
  6. Evaluate exploitability periodically; check convergence

- **Design tradeoffs:**
  - **Diffusion steps T:** More steps = higher-quality samples but slower inference. Paper uses T=100 implicitly; ablation needed.
  - **Refinement steps N:** More Q-ascent steps improve actions but add compute and risk Q-overfitting.
  - **FP iteration budget:** Undertrained best responses (small M in Alg. 2) populate pool with weak policies, degrading average strategy.

- **Failure signatures:**
  - **Exploitability oscillates or diverges:** Likely Q-function instability or insufficient best-response training (increase M)
  - **Policies collapse to single mode:** Diffusion may not be learning multimodal distribution—check noise schedule and network capacity
  - **High collision rates in racing:** Reward shaping may be misaligned with strategic objectives; collision penalty may be too weak
  - **Slow convergence vs. baselines:** Sampling overhead from diffusion may dominate; consider fewer diffusion steps

- **First 3 experiments:**
  1. **Sanity check on simple game:** Run DiffFP on a 1D continuous zero-sum game with known Nash equilibrium. Verify exploitability decreases monotonically and converges near zero.
  2. **Ablation on diffusion steps:** Compare T ∈ {10, 50, 100} on racing task. Measure tradeoff between sample quality (success rate) and inference time.
  3. **Robustness to opponent diversity:** Train on racing with single opponent type; evaluate against 3–5 unseen opponents. Compare crash rates and rewards vs. SAC-FP baseline to validate multimodal generalization claims.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can replacing the DDPM backbone with flow-matching techniques enable efficient one-step denoising without compromising the quality of the multi-modal policy?
- **Basis in paper:** [explicit] The authors identify replacing the DDPM backbone with flow-matching as a promising direction to improve training and inference efficiency.
- **Why unresolved:** DDPMs require iterative denoising steps which are computationally expensive; it is unclear if faster flow-matching approximations preserve the robustness of the learned equilibrium.
- **Evidence:** A comparison of convergence speed and exploitability scores between standard DiffFP and a flow-matching variant in the Racing and MPE environments.

### Open Question 2
- **Question:** How does the reliance on one-step utility rewards affect the learning of long-horizon strategic value?
- **Basis in paper:** [explicit] The Limitations section notes that the reward function is based on one-step utilities (e.g., track progress), which may overlook long-horizon strategic value.
- **Why unresolved:** Focusing on immediate rewards might cause the agent to miss complex, delayed strategies required in more intricate game settings.
- **Evidence:** Evaluation of DiffFP performance in environments specifically designed to require multi-step planning or sparse rewards, comparing strategic depth against baseline methods.

### Open Question 3
- **Question:** Can the framework be extended to explicitly control behavioral styles (e.g., aggressive vs. defensive) to enhance safety and interpretability?
- **Basis in paper:** [explicit] The authors state they do not explicitly control the agent's style, which limits behavioral diversity and safety in critical systems.
- **Why unresolved:** The current model learns diverse strategies implicitly, but lacks a mechanism to enforce specific safety constraints or tactical modes upon deployment.
- **Evidence:** Introducing a conditional variable for behavioral style in the diffusion policy and measuring the success rate of specific tactical maneuvers (e.g., defensive blocking) versus standard reward maximization.

## Limitations
- Diffusion-based policy representation adds computational overhead that may not benefit unimodal games
- Q-guided refinement depends on accurate Q-function estimates in non-stationary settings
- FP convergence guarantees require game-specific conditions that may not hold for all continuous zero-sum games

## Confidence
- **High confidence**: Diffusion policies can represent multimodal action distributions (mechanistic understanding clear)
- **Medium confidence**: FP with diffusion actors converges faster than RL baselines (results shown but hyperparameters critical)
- **Low confidence**: 30× success rate improvement generalizes beyond tested racing environments (single domain evaluation)

## Next Checks
1. Test exploitability convergence on a 1D continuous zero-sum game with known Nash to verify FP dynamics work as claimed
2. Ablate diffusion steps T on racing task to quantify sample quality vs. inference time tradeoff
3. Evaluate trained policies against diverse unseen opponents to validate multimodal generalization claims