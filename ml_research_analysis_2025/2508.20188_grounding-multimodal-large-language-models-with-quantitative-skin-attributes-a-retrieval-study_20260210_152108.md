---
ver: rpa2
title: 'Grounding Multimodal Large Language Models with Quantitative Skin Attributes:
  A Retrieval Study'
arxiv_id: '2508.20188'
source_url: https://arxiv.org/abs/2508.20188
tags:
- image
- images
- attributes
- lesion
- area
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the need for improved interpretability in AI
  models for skin cancer diagnosis by grounding Multimodal Large Language Models (MLLMs)
  in quantitative skin lesion attributes. The approach involves fine-tuning the Qwen
  2 VL model to predict numerical attributes (e.g., lesion area, border irregularity)
  from dermoscopic images using the SLICE-3D dataset.
---

# Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study

## Quick Facts
- **arXiv ID**: 2508.20188
- **Source URL**: https://arxiv.org/abs/2508.20188
- **Reference count**: 24
- **Primary result**: Fine-tuned Qwen 2 VL achieves R² 0.71-0.96 on 16 skin lesion attributes and enables attribute-specific image retrieval

## Executive Summary
This paper addresses the interpretability challenge in AI-assisted skin cancer diagnosis by grounding Multimodal Large Language Models (MLLMs) in quantitative skin lesion attributes. The authors fine-tune Qwen 2 VL to predict numerical attributes (e.g., lesion area, border irregularity) from dermoscopic images using the SLICE-3D dataset. They demonstrate that the resulting embeddings enable attribute-specific content-based image retrieval, where retrieved images not only visually resemble the query but also match in specific quantitative attributes. This approach provides evidence that MLLM embedding spaces can be grounded in clinically meaningful quantitative features, offering a pathway toward more interpretable AI-assisted dermatological diagnosis.

## Method Summary
The approach involves fine-tuning Qwen 2 VL (7.6B decoder + 365M vision encoder) using LoRA adaptation to predict 16 quantitative skin lesion attributes from dermoscopic images. The model is trained on tuples of (image, attribute question, numerical value) using next-token prediction. For inference, two embedding types are extracted: image-only embeddings (averaging all image token outputs) and image-text embeddings (final token after processing image + question). The image-text embeddings enable attribute-specific retrieval by allowing the same image to be retrieved differently depending on which attribute is queried. Retrieval is performed using cosine similarity between query embeddings and a database of training image embeddings.

## Key Results
- Fine-tuned model achieves R² scores of 0.71-0.96 across 16 skin lesion attributes
- Image-text embeddings outperform image-only embeddings for attribute-specific retrieval
- Hierarchical retrieval with b=200 effectively retrieves images matching specific attributes
- MONET baseline is outperformed by the proposed approach in attribute difference percentiles

## Why This Works (Mechanism)

### Mechanism 1: Supervised Attribute Prediction Grounds Embedding Space
Fine-tuning an MLLM to predict numerical skin lesion attributes from images grounds its embedding space in clinically meaningful quantitative features. The model learns representations that encode clinically relevant concepts rather than just pixel-level patterns. The training uses next-token prediction to generate numerical values, forcing the vision encoder and decoder to extract attribute-specific visual features.

### Mechanism 2: Final-Token Embedding Captures Cross-Modal Aggregation
The final token embedding in the decoder sequence captures attribute-specific information by aggregating from both image and question tokens through self-attention. This representation has "seen" all image patches and all question tokens, creating an embedding specialized to the specific attribute queried.

### Mechanism 3: Composed Search via Decoder Fusion Outperforms Separate Embedding Spaces
Using the MLLM decoder to fuse image and text embeddings enables attribute-specific retrieval that CLIP-like dual-encoder models cannot achieve at inference time. This approach allows dynamic specialization: the same image can be retrieved differently depending on which attribute is queried, without maintaining separate databases for each attribute.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)** - Needed to efficiently fine-tune large models; quick check: Why apply LoRA to both vision encoder and decoder?
- **Concept: Vision Transformer (ViT) Patch Tokenization** - Images are converted to patches becoming tokens; quick check: How does patch size affect NI and embedding extraction?
- **Concept: R² Score Interpretation for Clinical Use** - R²=0.71-0.96 reported; quick check: What fraction of variance is unexplained at R²=0.71?

## Architecture Onboarding

**Component map:**
Image (I) → Vision Encoder g (ViT, 365M) → Patch embeddings [NI × d]
↓
Question tokens Q(a) → Token embedding ψ → Text embeddings [k × d]
↓
Concatenate → [NI + k, d]
↓
Decoder f (Qwen2, 7.6B with LoRA) → Token features [m × d]
↓
┌─────────────────┬─────────────────────┐
↓                 ↓                     ↓
Avg image tokens   Final token         Prediction head h
(h_im)             (h_im,a)            → logits for next token
↓                 ↓
General visual    Attribute-specific
similarity        retrieval

**Critical path:**
1. Load pre-trained Qwen 2 VL 7B (7.6B decoder + 365M encoder)
2. Prepare SLICE-3D dataset: sample W=5 attributes per image, format as (image, Q(a), value) tuples
3. Apply LoRA (rank=8) to both encoder and decoder
4. Train for 1 epoch, batch size 16, cosine LR schedule starting 1e-4
5. For inference: extract embeddings using h_im or h_im,a
6. Build retrieval database B(im) or B(im,a) from training images
7. Query with new image, compute cosine similarity, return top-k

**Design tradeoffs:**
- W (attributes per image): Higher = richer training signal, but more forward passes per image
- Hierarchical retrieval parameter b: Paper uses b=200; higher = more accurate but slower
- LoRA rank: Rank=8 balances expressiveness and efficiency
- Embedding type: Image-only is fast (one database) but less attribute-specific

**Failure signatures:**
- Low R² on specific attributes: May indicate poor annotation quality or visually indeterminate attributes
- Image-only retrieval matches image-text performance: Question conditioning not working
- Large train-test R² gap: Patient stratification failed, model memorizing patient-specific features

**First 3 experiments:**
1. **Baseline attribute prediction**: Train with W=5, LoRA rank=8 for 1 epoch. Evaluate R² per attribute on test set.
2. **Retrieval ablation study**: Compare image-only, image-text, hierarchical, untuned Qwen, MONET baseline.
3. **Patient-stratified generalization**: Split by patient (802 train / 240 test). Further split test by demographic or imaging device.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the image-text embedding approach be extended to effectively retrieve images that match a query across multiple quantitative attributes simultaneously?
- **Basis in paper**: The Conclusion states that the efficacy of attribute-specific retrieval "provides evidence for the feasibility of multi-attribute retrieval," and suggests implementing it by including questions about multiple attributes in the text input.

### Open Question 2
- **Question**: Does grounding MLLMs in quantitative attributes improve the model's ability to generate faithful natural language reasoning for diagnosis?
- **Basis in paper**: The Introduction posits that MLLMs offer a path to interpretability via "reasoning for diagnosis in natural language," but the experiments focus solely on regression performance and embedding retrieval.

### Open Question 3
- **Question**: Does attribute-specific retrieval provide tangible benefits in clinical diagnostic accuracy or workflow efficiency compared to standard visual search?
- **Basis in paper**: The paper frames the research as a solution to the "black box" problem to help clinicians, but the evaluation relies on technical metrics rather than clinical utility.

## Limitations

- SLICE-3D dataset composition and generalization to different imaging conditions/institutions is unclear
- Evaluation limited to same dataset used for training, without external validation
- Computational overhead of on-the-fly embedding generation may limit real-time clinical deployment
- No direct validation that grounding improves actual diagnostic accuracy or clinical decision-making

## Confidence

**High Confidence**: Core mechanism of using fine-tuned MLLM embeddings for attribute-specific retrieval is well-supported by R² scores (0.71-0.96) and retrieval performance metrics.

**Medium Confidence**: Clinical utility and generalization claims are moderately supported; evaluation limited to single dataset without external validation.

**Low Confidence**: Claims about scalability and practicality for clinical deployment lack supporting evidence; no computational cost analysis or user study data provided.

## Next Checks

1. **External Dataset Validation**: Test the fine-tuned model and retrieval system on an independent dermoscopic image dataset (e.g., ISIC Archive) to assess generalization across different imaging conditions and patient demographics.

2. **Clinical Utility Assessment**: Conduct a user study with dermatologists comparing the attribute-specific retrieval system against standard visual search and existing interpretable AI tools, measuring diagnostic accuracy and user preference.

3. **Computational Efficiency Analysis**: Profile the inference latency and memory requirements for on-the-fly embedding generation during attribute-specific retrieval, comparing against pre-computed embedding approaches.