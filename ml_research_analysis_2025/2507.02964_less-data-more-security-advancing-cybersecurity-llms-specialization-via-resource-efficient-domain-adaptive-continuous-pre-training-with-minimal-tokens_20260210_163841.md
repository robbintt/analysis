---
ver: rpa2
title: 'Less Data, More Security: Advancing Cybersecurity LLMs Specialization via
  Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens'
arxiv_id: '2507.02964'
source_url: https://arxiv.org/abs/2507.02964
tags:
- cybersecurity
- domain
- while
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Domain-Adaptive Continuous Pretraining
  (DAP) as a methodology for enhancing cybersecurity understanding in pretrained LLMs
  while preserving general language capabilities. The authors systematically adapted
  three decoder-based architectures (Llama-3.1-8B, DeepSeek-R1-Distill-Qwen-14B, and
  Llama-3.3-70B-Instruct) using a curated 126-million-word cybersecurity corpus.
---

# Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens

## Quick Facts
- **arXiv ID**: 2507.02964
- **Source URL**: https://arxiv.org/abs/2507.02964
- **Reference count**: 40
- **Key outcome**: Cybersecurity domain-adaptive pretraining with 118.8M tokens achieved state-of-the-art accuracy on three benchmarks, outperforming models trained on 2.77B tokens

## Executive Summary
This paper investigates Domain-Adaptive Continuous Pretraining (DAP) as a methodology for enhancing cybersecurity understanding in pretrained LLMs while preserving general language capabilities. The authors systematically adapted three decoder-based architectures using a curated 126-million-word cybersecurity corpus with conservative training parameters and distributed FSDP training. Evaluation across three cybersecurity benchmarks demonstrates consistent improvements post-adaptation, with the 70B model achieving state-of-the-art accuracies of 0.718, 0.933, and 0.864, respectively, while using substantially smaller datasets than prior approaches.

## Method Summary
The methodology employs Domain-Adaptive Continuous Pretraining on decoder-only models using a curated cybersecurity corpus from standards, academic literature, and technical books. Training uses frozen embedding layers, small learning rates (≈1×10⁻⁶), limited epochs (2-3), and distributed FSDP across GPUs. The approach balances domain specialization with knowledge preservation through conservative hyperparameter choices, achieving competitive performance with significantly fewer tokens than traditional pretraining approaches.

## Key Results
- Llama-3.3-70B-Ins-DAP achieved state-of-the-art accuracies of 0.718, 0.933, and 0.864 on CTI-MCQ, CyberMetric, and SecEval benchmarks respectively
- Competitive performance achieved using substantially smaller datasets (118.8M versus 2.77B tokens) than prior approaches
- All three model sizes (8B, 14B, 70B) showed consistent improvement across benchmarks after domain adaptation

## Why This Works (Mechanism)

### Mechanism 1: Conservative Hyperparameter Strategy for Knowledge Preservation
Small learning rates and limited training epochs enable domain knowledge acquisition while preserving pre-trained general language capabilities. Conservative learning rate (≈1×10⁻⁶) with cosine scheduling and warmup, combined with only 2-3 epochs, allows gradual weight adjustments that incorporate domain-specific patterns without overwriting existing knowledge representations. If learning rate exceeds ≈1×10⁻⁵ or training extends beyond 3-4 epochs, catastrophic forgetting of general language capabilities likely occurs.

### Mechanism 2: Curated Domain Corpus for Efficient Specialization
High-quality, authoritative cybersecurity content enables effective domain adaptation with substantially fewer tokens than large-scale pretraining approaches. Systematically curated corpus from standards (NIST, ISO), academic literature (arXiv), and technical books provides dense domain-specific terminology and contextual patterns, achieving approximately 20× efficiency over larger uncurated datasets. If corpus lacks domain breadth or contains substantial noise/irrelevant content, specialization efficiency degrades significantly.

### Mechanism 3: Embedding Layer Freezing for Vocabulary Stability
Freezing embedding layers during DAP preserves foundational token representations while allowing higher transformer layers to adapt to domain patterns. Pre-trained tokenizers and embeddings capture general language semantics; freezing prevents vocabulary fragmentation and maintains stable token-level semantics while attention layers above learn cybersecurity-specific contextual relationships. If domain introduces substantial novel terminology not represented in pre-trained vocabulary, frozen embeddings may limit adaptation effectiveness.

## Foundational Learning

- **Concept: Causal Language Modeling (CLM)**
  - Why needed here: DAP uses unsupervised next-token prediction to learn cybersecurity terminology and patterns without requiring labeled training data
  - Quick check question: Can you explain why decoder-only models with CLM suit autoregressive generation tasks compared to encoder-only masked language modeling?

- **Concept: Catastrophic Forgetting**
  - Why needed here: The central challenge DAP addresses is acquiring new domain knowledge without degrading or erasing pre-trained general language capabilities
  - Quick check question: What happens if you train a pre-trained LLM on specialized data using standard learning rates for many epochs?

- **Concept: Fully Sharded Data Parallel (FSDP)**
  - Why needed here: Training 8B-70B parameter models requires distributing model weights, gradients, and optimizer states across multiple GPUs to fit in memory
  - Quick check question: How does FSDP differ from standard data parallelism when model weights exceed single-GPU memory capacity?

## Architecture Onboarding

- **Component map:** PDF/text extraction → JSON conversion → RegEx cleaning → JSONL formatting → Tokenization (1024-2048 tokens) → DistributedSampler/DataLoader → Pre-trained model initialization → Frozen embeddings → FSDP wrapping → AdamW optimizer (lr≈1e-6) → Cosine scheduler with warmup → Gradient accumulation → Checkpoint persistence → Evaluation pipeline (benchmark loading → Inference endpoint → Accuracy/Jaccard Index scoring → Semantic similarity analysis)

- **Critical path:**
  1. Curate domain corpus from authoritative sources (standards, papers, books)
  2. Preprocess to JSONL with noise removal and context-preserving segmentation
  3. Initialize pre-trained model with frozen embedding layer
  4. Configure FSDP with conservative hyperparameters (lr≈1e-6, 2-3 epochs)
  5. Monitor perplexity during training as domain acquisition signal
  6. Evaluate on domain benchmarks comparing base vs. adapted performance

- **Design tradeoffs:**
  - Data size vs. quality: 118.8M curated tokens outperformed 2.77B tokens from Llama-Primus-Base on all benchmarks
  - Model scale vs. compute cost: 70B achieved best results (0.933 CyberMetric) but required ~$23K total and 8×H100s; 14B offered better cost-efficiency
  - Context length vs. memory: Limited to 1024-2048 tokens to fit GPU memory; longer contexts could improve semantic understanding but require larger instances

- **Failure signatures:**
  - Performance regression on certain benchmarks (e.g., Llama-3.1-8B showed minimal SecEval improvement with only 1M tokens—insufficient data for model capacity)
  - High perplexity plateau indicating domain knowledge not being acquired (check data quality or increase dataset size)
  - General capability degradation on non-cyber tasks (learning rate too aggressive or epochs too many—catastrophic forgetting)

- **First 3 experiments:**
  1. Validate pipeline with smaller model (e.g., 3B parameters) on 1M token subset before committing resources to larger models
  2. Compare 8B model performance across dataset sizes (1M vs 50M tokens) to identify minimum viable dataset for given model capacity
  3. Run semantic similarity analysis (cosine between model explanations and ground truth) to verify contextual understanding beyond answer accuracy alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does cybersecurity domain-adaptive continuous pretraining cause catastrophic forgetting of general language capabilities?
- Basis in paper: Future investigations should prioritize this evaluation dimension to validate that cybersecurity knowledge acquisition occurs without compromising core linguistic competencies essential for general-purpose deployment scenarios.
- Why unresolved: Resource constraints prevented comprehensive benchmarking of baseline and DAP variants across linguistic evaluation frameworks such as SuperGLUE.
- What evidence would resolve it: Benchmarking all six model variants (three baseline, three domain-adapted) on general-purpose linguistic tasks to quantify preservation of fundamental language comprehension.

### Open Question 2
- Question: What is the optimal relationship between model parameter count and domain-adaptation dataset size for cybersecurity specialization?
- Basis in paper: The 8B model showed minimal SecEval improvement with only 1M tokens, while larger models received 50-100× more data; the authors note this underscores the critical relationship between model scale and dataset size.
- Why unresolved: The study used uneven dataset allocations across model sizes, confounding systematic analysis of this relationship.
- What evidence would resolve it: Controlled experiments varying dataset sizes systematically across fixed model architectures to identify optimal token-to-parameter ratios.

### Open Question 3
- Question: Can knowledge distillation during domain adaptation mitigate overfitting and catastrophic forgetting while improving cybersecurity knowledge acquisition?
- Basis in paper: Training methodology refinements such as knowledge distillation could optimize the domain adaptation process, potentially mitigating issues like overfitting and catastrophic forgetting, though resource constraints prevented exploration of these techniques.
- Why unresolved: The authors used only standard DAP without distillation-based loss functions that reference original model weights.
- What evidence would resolve it: Comparative experiments using teacher-student architectures with distillation losses during cybersecurity domain adaptation, evaluated on both domain-specific and general benchmarks.

## Limitations
- **Data provenance and reproducibility**: The paper specifies cybersecurity corpus categories but does not provide specific document lists, URLs, or extraction methods, creating a fundamental barrier to faithful reproduction
- **Generalization scope**: Results show consistent improvement across three cybersecurity benchmarks, but these represent specific knowledge domains within cybersecurity, not broader cybersecurity challenges
- **Evaluation methodology**: While reporting accuracy and Jaccard Index scores, the paper lacks comparison to specialized cybersecurity models on non-cybersecurity tasks, making it difficult to assess true knowledge preservation

## Confidence

**High confidence**: Domain-Adaptive Continuous Pretraining with conservative hyperparameters effectively preserves general language capabilities while acquiring domain knowledge, well-supported by systematic experiments across three model architectures and multiple dataset sizes.

**Medium confidence**: The 20× efficiency gain (118.8M vs 2.77B tokens) achieving comparable or better performance is plausible given curated corpus quality, but lacks direct ablation studies comparing different curation strategies.

**Low confidence**: The assertion that frozen embeddings are optimal for all domain adaptation scenarios. While effective here, the paper provides no evidence that this strategy works for domains with substantially different vocabularies or tokenization requirements.

## Next Checks
1. **Ablation study on corpus curation strategy**: Systematically compare the current curated corpus against alternative approaches (random web scraping, synthetic data generation, mixed-domain corpora) using identical training procedures to quantify the actual efficiency gain and identify optimal curation methods for different cybersecurity subdomains.

2. **Cross-domain capability assessment**: Evaluate adapted models on general NLP benchmarks (MMLU, BBH, HellaSwag) to quantify knowledge preservation guarantees. Measure performance degradation relative to base models and identify the learning rate/epoch threshold where catastrophic forgetting begins across different model scales.

3. **Semantic understanding validation**: Conduct human evaluation of model explanations on cybersecurity concepts to verify that performance improvements reflect genuine contextual understanding rather than pattern matching. Compare cosine similarity distributions between model-generated and ground-truth explanations across all benchmarks to quantify reasoning quality.