---
ver: rpa2
title: Combining Causal Models for More Accurate Abstractions of Neural Networks
arxiv_id: '2503.11429'
source_url: https://arxiv.org/abs/2503.11429
tags:
- causal
- input
- variables
- inputs
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of quantifying and improving the
  faithfulness of mechanistic interpretability hypotheses about neural networks. While
  causal abstraction provides a rigorous framework for aligning neural network behavior
  with high-level algorithmic models, these abstractions often fail to fully capture
  the network's true computational process.
---

# Combining Causal Models for More Accurate Abstractions of Neural Networks

## Quick Facts
- arXiv ID: 2503.11429
- Source URL: https://arxiv.org/abs/2503.11429
- Reference count: 37
- Primary result: Combined causal models achieve 75% input space coverage at 100% faithfulness on boolean logic task, versus 50% for individual models

## Executive Summary
This paper addresses the problem of quantifying and improving the faithfulness of mechanistic interpretability hypotheses about neural networks. While causal abstraction provides a rigorous framework for aligning neural network behavior with high-level algorithmic models, these abstractions often fail to fully capture the network's true computational process. The authors propose a solution: combining multiple simple causal models into a single, more expressive model that activates different computational processes based on the input provided. This combined model can more faithfully represent the network's behavior by accounting for the fact that networks may use different strategies for different inputs.

## Method Summary
The core method involves constructing evaluation graphs for each candidate causal model, where nodes represent inputs and edges are weighted by interchange intervention accuracy. These graphs are then used to greedily partition the input space, assigning inputs to different models while maintaining a specified faithfulness threshold. The method is demonstrated on GPT-2-small fine-tuned on arithmetic and boolean logic tasks, showing that combined models provide stronger interpretability hypotheses at high faithfulness levels compared to individual models. The approach uses Distributed Alignment Search (DAS) to align high-level causal variables to low-level neural subspaces, then constructs weighted graphs based on interchange intervention accuracy between input pairs.

## Key Results
- Combined models achieved 75% input space coverage at 100% faithfulness on boolean logic task versus 50% or lower for individual models
- For arithmetic task, combined models achieved 60% coverage at faithfulness 0.9-1.0, compared to 20-40% for individual models
- Clear trade-off demonstrated between hypothesis strength (proportion of inputs assigned to non-trivial models) and faithfulness (interchange intervention accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluation graphs quantifiably capture how well each candidate causal model explains pairwise input relationships.
- Mechanism: For each candidate model, construct a weighted graph where nodes are inputs and edge weights are {0, 0.5, 1} based on bidirectional interchange intervention accuracy. High-weight edges indicate inputs that the model faithfully explains together.
- Core assumption: Interchange intervention accuracy on input pairs is a sufficient proxy for whether a model faithfully represents the network's computation for those inputs.
- Break condition: If interchange interventions don't reflect meaningful causal structure (e.g., models are polysemantic or interventions corrupt unrelated features), edge weights become noise and partition quality degrades.

### Mechanism 2
- Claim: Greedy partitioning of input space across models maximizes explained inputs while respecting faithfulness thresholds.
- Mechanism: Sort nodes by degree in each evaluation graph, iteratively add nodes to subgraphs while maintaining IIA ≥ λ, select the model with largest valid subgraph, remove those nodes, repeat. Unassigned inputs default to the trivial model.
- Core assumption: A greedy approach finds sufficiently good partitions; optimal partitioning is not required for practical gains.
- Break condition: When models have overlapping but incompatible strengths across inputs (non-disjoint optimal coverage), greedy selection may leave significant input regions suboptimally explained.

### Mechanism 3
- Claim: Combined causal models with input-conditional mechanism activation more faithfully represent networks that use different strategies for different inputs.
- Mechanism: Define piecewise mechanisms F*X(v*) = Fj_X(ProjVj(v*)) if input ∈ ∆j, else ∅. This allows the combined model to express hypotheses like "the network sums X+Y first for small X, but uses direct summation for large X."
- Core assumption: Neural networks may employ input-dependent computational strategies that cannot be captured by a single uniform causal model.
- Break condition: If the network uses a single uniform algorithm (no input-dependent branching), combined models add complexity without benefit and may overfit to noise in IIA estimates.

## Foundational Learning

- Concept: **Interchange Intervention Accuracy (IIA)**
  - Why needed here: IIA is the core faithfulness metric; without understanding it, you cannot interpret the strength-faithfulness trade-off curves.
  - Quick check question: Given a base input b and source input s, what does IIA = 0.7 mean for a high-level model H aligned to network L?

- Concept: **Distributed Alignment Search (DAS)**
  - Why needed here: DAS learns the rotation matrices R that align high-level variables to low-level neural subspaces. The combined model approach inherits these alignments.
  - Quick check question: Why does DAS optimize an orthogonal matrix R rather than directly intervening on individual neurons?

- Concept: **Constructive Causal Abstraction**
  - Why needed here: Provides the theoretical grounding for when a high-level model is an abstraction of a low-level network via alignment maps ⟨Π, π⟩.
  - Quick check question: What is the difference between exact transformation and approximate transformation in this framework?

## Architecture Onboarding

- Component map: Task Setup -> Network Fine-tuning -> DAS Training -> Evaluation Graph Construction -> Greedy Partitioning -> Combined Model Assembly
- Critical path: DAS alignment quality → Evaluation graph edge weights → Partition boundaries → Combined model strength at given λ. Poor DAS alignment propagates as noisy graphs, leading to suboptimal partitions.
- Design tradeoffs:
  - **Subspace dimension k**: Higher k (64→256) captures more information but risks overfitting. Paper finds stable results across {64, 128, 256}.
  - **Faithfulness threshold λ**: Higher λ (0.9→1.0) yields weaker hypotheses (more inputs assigned to trivial model); lower λ yields stronger but less trustworthy explanations.
  - **Number of candidate models**: More models increase expressivity but also search complexity and risk of spurious alignments.
- Failure signatures:
  - Low IIA across all models at a layer → network computation not captured by any candidate; need new hypotheses
  - Asymmetric IIA (e.g., MXY more accurate than MYZ) → indicates causal attention asymmetry in transformer, not a failure
  - Sharp drops in IIA between adjacent layers → computational transition zone; may need finer-grained models
- First 3 experiments:
  1. Replicate arithmetic task (X+Y+Z) on GPT-2-small; verify Figure 1 IIA curves for MXY, MZ, MXYZ across layers 0-11 with k=256.
  2. Run Algorithm 2 on layer 7 with λ = {1.0, 0.95, 0.9, 0.8}; confirm combined model strength exceeds individual models at λ ≥ 0.9 (Figure 2d).
  3. Extend to a new task (e.g., two-digit addition X+Y where X, Y ∈ [10, 99]); define candidate models (ones-place-first, tens-place-first, direct) and evaluate whether combined models provide similar faithfulness-strength improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework of combining causal models scale effectively to larger architectures and more complex, real-world tasks?
- Basis in paper: [explicit] The conclusion states the framework "could be extended to more complex tasks" beyond the toy tasks used for validation.
- Why unresolved: The study only validates the method on GPT-2 small fine-tuned on simple arithmetic and boolean logic tasks.
- What evidence would resolve it: Successful application of the method to larger foundation models (e.g., 7B+ parameters) on complex reasoning or linguistic benchmarks without manual model specification.

### Open Question 2
- Question: Can non-greedy optimization objectives significantly improve the partitioning of the input space compared to the current greedy approach?
- Basis in paper: [explicit] The authors note that the method could be "enhanced with more sophisticated optimization objectives for constructing input space partitions."
- Why unresolved: The current greedy algorithm selects inputs based on node degree, which may result in suboptimal local maxima for the faithfulness-strength trade-off.
- What evidence would resolve it: A comparative study showing that global optimization techniques yield higher input space coverage at equivalent faithfulness levels.

### Open Question 3
- Question: How can this approach be integrated with other mechanistic interpretability techniques to refine hypothesis generation?
- Basis in paper: [explicit] The authors suggest the framework could be "integrated with other mechanistic interpretability methods."
- Why unresolved: The current method relies on manually defining candidate causal models, which limits scalability; combining it with discovery tools is an open problem.
- What evidence would resolve it: A pipeline that automatically generates candidate causal models (e.g., using sparse autoencoders) and then combines them using the paper's evaluation graph method.

## Limitations
- The method relies heavily on interchange intervention accuracy as the sole faithfulness metric, which may not capture meaningful causal relationships if IIA measurements are noisy or artifacts of polysemantic features
- The greedy partitioning algorithm may not find globally optimal model assignments, potentially leaving significant input regions suboptimally explained
- Combined models introduce additional complexity that may be harder to interpret than simpler individual models

## Confidence
- **High confidence**: The core methodology of combining causal models through evaluation graph partitioning is well-defined and reproducible. The arithmetic and boolean logic task results demonstrate the approach works as described.
- **Medium confidence**: The claim that combined models provide more accurate abstractions of neural network behavior is supported by the experimental results but requires broader validation across different network architectures and tasks.
- **Low confidence**: The paper does not extensively validate whether the combined models capture meaningful computational strategies versus statistical artifacts in the interchange intervention measurements.

## Next Checks
1. **Cross-architecture validation**: Apply the combined model approach to GPT-2-medium and GPT-3-small on the same tasks to verify that the faithfulness-strength improvements generalize beyond GPT-2-small.
2. **Sensitivity analysis**: Systematically vary the subspace dimension k (e.g., test k=64, 128, 256, 512) and evaluate how this affects both individual model faithfulness and combined model performance to understand robustness to hyperparameter choices.
3. **Alternative faithfulness metrics**: Compare IIA-based partitioning results with alternative faithfulness metrics (e.g., causal abstraction distance or randomized interventions) to validate that the observed improvements are not artifacts of the specific IIA measurement approach.