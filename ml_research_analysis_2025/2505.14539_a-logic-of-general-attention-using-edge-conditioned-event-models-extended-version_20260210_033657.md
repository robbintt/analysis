---
ver: rpa2
title: A Logic of General Attention Using Edge-Conditioned Event Models (Extended
  Version)
arxiv_id: '2505.14539'
source_url: https://arxiv.org/abs/2505.14539
tags:
- event
- attention
- definition
- update
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first general logic of attention that
  overcomes key limitations of existing approaches. The authors present edge-conditioned
  event models that generalize both standard event models and generalized arrow updates,
  achieving exponential succinctness compared to standard models while maintaining
  equal expressivity.
---

# A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)

## Quick Facts
- arXiv ID: 2505.14539
- Source URL: https://arxiv.org/abs/2505.14539
- Authors: Gaia Belardinelli; Thomas Bolander; Sebastian Watzl
- Reference count: 12
- Primary result: First general logic of attention that treats attention as a modality allowing agents to attend to arbitrary formulas, not just atomic propositions

## Executive Summary
This paper introduces the first general logic of attention that overcomes key limitations of existing approaches. The authors present edge-conditioned event models that generalize both standard event models and generalized arrow updates, achieving exponential succinctness compared to standard models while maintaining equal expressivity. By treating attention as a modality, the framework allows agents to attend to arbitrary formulas, not just atomic propositions, enabling modeling of complex attention scenarios such as social learning and attentional biases. The paper provides sound and complete axiomatizations and introduces attention principles (closure properties) that govern the behavior of the attention modality.

## Method Summary
The method involves defining edge-conditioned event models with logical conditions on accessibility edges rather than explicit event preconditions, constructing transformations between standard and edge-conditioned models to prove update equivalence, and implementing product updates that check edge conditions against agent attention functions. The approach defines languages for epistemic logic (LEL), propositional attention (LPA), and general attention (LGA), and provides transformations T1 and T'1 to verify that edge-conditioned and standard event models yield equivalent results. The method demonstrates exponential succinctness by showing that attention models require O(|Ag|) events versus ≥2^|Ag| for equivalent standard models.

## Key Results
- Exponential succinctness: Edge-conditioned event models require O(|Ag|) events versus ≥2^|Ag| for equivalent standard models
- Equal expressivity: The edge-conditioned approach maintains the same expressive power as standard event models
- General attention modality: Agents can attend to arbitrary formulas, including beliefs about other agents' attention
- Sound and complete axiomatization: The framework provides formal axioms governing attention behavior

## Why This Works (Mechanism)

### Mechanism 1: Edge-Conditioned Succinctness
Encoding attention constraints as conditions on accessibility edges rather than explicit event preconditions yields exponentially smaller state spaces. Standard DEL requires creating a unique event for every combination of what agents might attend to (e.g., 2^n events for n atoms), while edge-conditioning moves these constraints to the accessibility relation. This allows a single event to represent multiple perceptual outcomes depending on which edge condition is satisfied during the transition.

### Mechanism 2: Attention as a Learning Filter
Attention functions as a dynamic filter that selectively gates belief updates without altering the underlying truth of the revelation. The framework defines an "attention set" A_a(w) for each agent. During a product update with a revelation Γ, the mechanism checks if an edge condition (e:φ, f:ψ) matches the agent's attention. If agent a does not attend to formula φ, the "Inertia" condition prevents the agent from accessing the event where φ is true.

### Mechanism 3: Higher-Order Attentional Biases
Treating attention as a recursive modality (rather than a propositional atom) allows the system to model biases where agents ignore not just facts, but the beliefs or attention of others. The logic extends syntax to A_a φ where φ can be any formula, including belief formulas B_b ψ or other attention formulas A_b ψ. This enables "social attention," where an agent's epistemic state changes only if they are attending to the source or the topic.

## Foundational Learning

- **Concept: Product Update (DEL)**
  - Why needed: This is the engine of the paper. You must understand how a Kripke model (static state) is combined with an Event Model (action) to produce a new Kripke model.
  - Quick check: If an event e has precondition p, and the world w does not satisfy p, does (w, e) exist in the updated model?

- **Concept: Accessibility Relations (R_a)**
  - Why needed: These relations define what an agent considers possible. The paper modifies these edges to be "conditioned" rather than static.
  - Quick check: In an epistemic model, if agent a has an edge from w to v, what does v represent to a at w?

- **Concept: Modality vs. Atom**
  - Why needed: The paper shifts from A_a p (an atom/prop) to A_a φ (a modality). Understanding this distinction is key to grasping the "General Attention" contribution.
  - Quick check: Can a propositional atom contain nested beliefs (e.g., p = B_b q)? Can a modal formula?

## Architecture Onboarding

- **Component map**: Kripke Model (W, R, V) + Attention Function A → Edge-Conditioned Event Model (E, Q, pre) → Product Update (⊗) → Updated Kripke Model

- **Critical path**: Defining the Revelation (Γ) → Instantiating Event Model R(Γ) → Applying Update to State → Observing Belief Change

- **Design tradeoffs**:
  - Succinctness vs. Evaluation: The edge-conditioned approach minimizes memory/state size but requires evaluating logical formulas (A_a φ) at runtime for every edge traversal
  - Expressivity vs. Complexity: Allowing arbitrary formulas in attention sets permits modeling social bias but complicates the satisfiability checks required for the update

- **Failure signatures**:
  - State Explosion: If you revert to standard event models for compatibility, you lose the exponential succinctness, leading to memory overload
  - Logical Omniscience: If you treat attention as standard awareness logic without dynamic filtering, agents will update beliefs about unattended formulas, violating the "Inertia" requirement

- **First 3 experiments**:
  1. Replicate Figure 3 vs Figure 2: Implement a standard event model for p ∧ q and the edge-conditioned version. Measure node count to verify succinctness.
  2. Propositional Bias Test: Run the "Hiring Committee" example. Verify that the AI agent updates its belief about the human's bias (B_b A_a p ∧ ¬B_b A_a q) correctly after the CV reveal.
  3. Social Attention Stress Test: Construct a scenario where Agent A attends to Agent B's utterance, but Agent B is attending to a third fact. Check if Agent A learns that B attended to the fact, even if A didn't attend to the fact itself.

## Open Questions the Paper Calls Out

### Open Question 1
How can the logic of general attention be extended to incorporate capacity constraints and formula ordering? The current semantics assumes agents attend to all formulas in their attention set without resource limitations or prioritization mechanisms. An extension would need a priority function over formulas and a sound and complete axiomatization for this resource-sensitive logic.

### Open Question 2
Can the logic of general attention be adapted to plausibility models to allow agents to recover from false beliefs? The current Kripke-based framework inherits a limitation where agents cannot unlearn false beliefs. Adapting to plausibility models would require a definition of product update that preserves belief consistency and allows for belief revision.

### Open Question 3
What is the precise formal relationship between the dynamic logic of general attention and the static logic of general awareness? While similarities are noted, the paper does not provide a formal reduction or translation between the dynamic attention modality and the static awareness modality. A formal translation mapping general attention formulas to awareness logic formulas would resolve this.

## Limitations
- The exponential succinctness claim relies on worst-case theoretical bounds rather than empirical measurements across varied scenarios
- The proof assumes specific configurations of attention formulas and doesn't address how edge-condition complexity scales with formula depth
- The framework assumes agents maintain stable attention sets that are evaluated at the moment of the event, not accounting for dynamic attention shifts during updates

## Confidence

- **High confidence** in the formal definition of edge-conditioned event models and the update mechanism
- **Medium confidence** in the exponential succinctness claim due to reliance on worst-case analysis
- **Medium confidence** in the attention principles based on their theoretical soundness but limited empirical validation
- **Low confidence** in claims about practical performance without computational complexity analysis of edge formula evaluation

## Next Checks

1. **Computational complexity benchmark**: Implement both standard and edge-conditioned event models for varying formula depths and agent counts, measuring actual runtime and memory usage to validate the claimed exponential savings.

2. **Edge formula evaluation analysis**: Characterize the computational cost of evaluating complex formulas (containing nested beliefs, quantifiers) on edges versus the memory cost of explicit event generation.

3. **Expressive equivalence stress test**: Construct scenarios with deeply nested attention formulas (e.g., A_a B_b A_c B_d p) to verify that edge-conditioning maintains correct behavior across all formula structures.