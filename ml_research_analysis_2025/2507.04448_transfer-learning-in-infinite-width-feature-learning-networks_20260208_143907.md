---
ver: rpa2
title: Transfer Learning in Infinite Width Feature Learning Networks
arxiv_id: '2507.04448'
source_url: https://arxiv.org/abs/2507.04448
tags:
- learning
- networks
- task
- target
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a theoretical framework for transfer learning
  in infinitely wide neural networks where both pretraining (source) and downstream
  (target) tasks operate in a feature learning regime. The authors analyze both Bayesian
  networks sampled from posterior distributions and gradient flow training with weight
  decay, tracking how representations evolve in both source and target tasks.
---

# Transfer Learning in Infinite Width Feature Learning Networks

## Quick Facts
- arXiv ID: 2507.04448
- Source URL: https://arxiv.org/abs/2507.04448
- Reference count: 40
- Key outcome: Develops theoretical framework for transfer learning in infinitely wide neural networks where both pretraining and downstream tasks operate in a feature learning regime, predicting adaptive kernel machines controlled by elastic weight coupling.

## Executive Summary
This work develops a theoretical framework for transfer learning in infinitely wide neural networks operating in a feature learning regime. The authors analyze both Bayesian networks sampled from posterior distributions and gradient flow training with weight decay, tracking how representations evolve in both source and target tasks. Their theory predicts that transfer learning leads to adaptive kernel machines whose kernels depend on features learned from both source and target tasks, controlled by an elastic weight coupling parameter. The framework accurately predicts generalization improvements from transfer learning across different settings and reveals rich phenomenology in how feature learning strength, dataset size, and task alignment interact to determine transfer learning utility.

## Method Summary
The authors develop a theoretical framework for transfer learning using two-layer MLPs in mean-field/µP parameterization, with both Bayesian and gradient flow formulations. The method involves two stages: source pretraining on task T₁ with feature strength γ̅₀ and weight decay λ, followed by target training on task T₂ with feature strength γ₀, elastic coupling δ to source weights, and weight decay λ. The theory predicts adaptive kernel evolution through saddle point equations (Bayesian) or DMFT stochastic processes (gradient flow), which are validated against Langevin dynamics and real CIFAR-10 experiments.

## Key Results
- Transfer learning creates adaptive kernel predictors whose kernels depend on features learned from both source and target tasks, controlled by elastic weight coupling parameter δ
- Transfer learning benefits are maximized when source and target tasks share structure (α > 0), but strong target feature learning can compensate for misalignment when target data is sufficient
- Transfer learning utility is most pronounced when target sample size is small; benefits diminish as target data increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning creates adaptive kernel predictors whose kernels depend on features learned from both source and target tasks, controlled by an elastic weight coupling parameter.
- Mechanism: The elastic penalty δ²‖W - W̄‖² in the log-prior couples target weights to source weights. This shifts the preactivation distribution p(h|h̄) toward the source field distribution on target data, producing kernels that blend source-learned structure with target-specific adaptation.
- Core assumption: The mean-field/µP parameterization preserves Θ(N⁰) feature learning effects at infinite width, and the free energy concentrates around its average over source weight posteriors.
- Evidence anchors:
  - [abstract] "Reuse of features during transfer learning is controlled by an elastic weight coupling which controls the reliance of the network on features learned during training on the source task."
  - [section 2.2, Eq. 3-5] The Bayesian posterior includes the elastic penalty; the resulting kernel Φ emerges from a min-max optimization involving both source fields h̄ and target dual kernel Φ̂.
  - [corpus] Related work on adaptive kernels (arXiv:2502.07998) supports the kernel machine formulation in feature-learning regimes, though specific transfer learning mechanisms are unique to this paper.
- Break condition: If δ → 0, transfer effects vanish (random initialization). If δ → ∞ with weak target feature learning (γ₀ → 0), the network cannot adapt to target-specific structure.

### Mechanism 2
- Claim: Transfer learning benefits are maximized when source and target tasks share structure, but strong target feature learning can compensate for misalignment when target data is sufficient.
- Mechanism: Task alignment α (overlap between source and target label-generating functions) determines how much source features help. When α > 0, the source kernel provides useful priors. When α ≈ 0, transfer has negligible effect since the target must learn orthogonal directions.
- Core assumption: Task similarity can be quantified through label or feature alignment metrics that are preserved across the transfer.
- Evidence anchors:
  - [section 3.1] "When α = 0 and the source and target task vectors are orthogonal, transfer learning has no effect on the test loss."
  - [Figure 2a] Test loss decreases monotonically with δ when α > 0, but shows no improvement when α = 0.
  - [corpus] Corpus lacks direct replications of this task alignment mechanism in infinite-width transfer settings.
- Break condition: Orthogonal tasks (α = 0) yield no transfer benefit. Transfer from higher-complexity to lower-complexity tasks can be harmful if elastic coupling prevents discarding spurious features.

### Mechanism 3
- Claim: Transfer learning utility is most pronounced when target sample size is small; benefits diminish as target data increases.
- Mechanism: With limited target data P₂, the network relies on source priors (large δ) to overcome data scarcity. As P₂ grows, target feature learning can independently discover useful representations, reducing dependence on transfer.
- Core assumption: Source pretraining data P₁ ≥ P₂, and source representations contain transferable structure relevant to the target.
- Evidence anchors:
  - [section 1] "Transfer learning benefits are most pronounced when target task sample size is small."
  - [Figure 6c] Transfer gains (δ > 0) are large at small P₂ but diminish as P₂ increases.
  - [corpus] Scaling law literature (arXiv:2505.22491) notes infinite-width limits may not fully capture finite-width behavior, suggesting caution in extrapolation.
- Break condition: If target data is abundant and source task is dissimilar or more complex, transfer may hurt performance compared to learning from scratch.

## Foundational Learning

- Concept: Mean-field/µP parameterization (scaling network output by γ₀⁻¹N and learning rate by η₀γ₀²N)
  - Why needed here: This scaling preserves feature learning at infinite width, enabling analysis of representation evolution that would collapse to kernel regression under standard NTK parameterization.
  - Quick check question: Can you explain why NTK parameterization leads to "lazy" learning (static kernels) while µP enables feature adaptation?

- Concept: Saddle point equations from free energy minimization
  - Why needed here: The infinite-width limit yields deterministic kernel dynamics governed by saddle point conditions ∂F/∂Φ̂ = 0 and ∂F/∂Φ = 0, which must be solved self-consistently.
  - Quick check question: What happens to the dual kernel Φ̂ in the lazy limit γ₀ → 0?

- Concept: Franz-Parisi potential / elastic coupling in Bayesian inference
  - Why needed here: The transfer learning setup uses an elastic penalty δ²‖W - W̄‖² to constrain target weights near source weights, analogous to constraining posteriors to metastable states in spin glass theory.
  - Quick check question: How does the preactivation distribution p(h|h̄) change as δ increases from 0 to ∞?

## Architecture Onboarding

- Component map:
  - Source network: Two-layer MLP (W̄ ∈ R^{N×D}, w̄ ∈ R^N) trained on T₁ = {x̄_μ, ȳ_μ} with feature strength γ̅₀
  - Target network: Same architecture (W, w) trained on T₂ with elastic coupling δ to W̄ and feature strength γ₀
  - Adaptive kernel Φ ∈ R^{P₂×P₂}: Emerges from solving min-max optimization; depends on both tasks
  - Dual kernel Φ̂: Conjugate variable encoding target label influence

- Critical path:
  1. Pre-train source network on T₁ using gradient flow with weight decay (or sample from Bayesian posterior)
  2. Extract source preactivations h̄ on target data T₂
  3. Initialize target network with W(0) = W̄, random w
  4. Train target with elastic penalty δ‖W - W̄‖² + weight decay λ
  5. Solve saddle point equations (Bayesian) or integrate DMFT equations (gradient flow) to predict kernels and generalization

- Design tradeoffs:
  - High δ vs. low δ: High δ preserves source features but limits target adaptation; low δ enables fresh feature learning but discards transfer benefits
  - High γ₀ vs. low γ₀: High γ₀ enables strong feature learning but may require more data/steps; low γ₀ is stable but kernel-like
  - Source complexity vs. target complexity: Transferring from simpler to complex tasks generally helps; complex to simple can hurt if δ prevents feature pruning

- Failure signatures:
  - Test loss plateaus higher than random initialization → δ too large for dissimilar/complex source task
  - No improvement from transfer despite large δ → α ≈ 0 (orthogonal tasks)
  - Training instability → γ₀ too large relative to dataset size
  - Theory-experiment mismatch at moderate width → finite-width corrections needed

- First 3 experiments:
  1. Linear regression with controlled task alignment: Generate y = w·x and ȳ = β·x with β·w = α. Vary α ∈ {0, 0.3, 0.6, 0.9} and δ ∈ {0, 1, 10, 100}. Verify test loss matches saddle point predictions.
  2. Hermite polynomial transfer (low-to-high and high-to-low): Pre-train on He_k(β·x) with k=1,2 and transfer to He_m with m>k and m<k. Compare grokking-like dynamics with theory.
  3. CIFAR-10 class subset transfer: Pre-train on classes {0,1}, transfer to {8,9}. Sweep γ₀ and δ, measure preactivation distribution broadening and kernel task alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal number of hidden layers to preserve during transfer learning depend on the depth of the source network and the similarity between source and target tasks?
- Basis in paper: [explicit] The authors state: "Future work could explore how representation learning in deeper networks enable transfer learning. Specifically, it could be interesting to study what number of hidden layers should be preserved during transfer learning."
- Why unresolved: The current theory is limited to two-layer networks, and extending to deeper architectures requires understanding how representations evolve across multiple hidden layers during both pretraining and fine-tuning.
- What evidence would resolve it: Theoretical extension of the DMFT or Bayesian framework to deeper networks, plus experiments showing how test loss varies with the number of transferred layers for different task similarity levels.

### Open Question 2
- Question: Can the theoretical equations be reformulated with dataset averaging to achieve computational complexity independent of dataset size for simple target functions?
- Basis in paper: [explicit] The authors note: "Our theory currently does not average over the dataset so the theoretical equations scale with the number of datapoints. Future work could attempt to average over data for simple target functions such as the linear or polynomial tasks."
- Why unresolved: Current saddle point equations involve P×P kernel matrices, limiting scalability; averaging over data distributions would require techniques from statistical mechanics of disordered systems.
- What evidence would resolve it: Derivation of closed-form expressions for generalization error that depend only on statistical properties of the data distribution (e.g., spectral properties) rather than individual samples.

### Open Question 3
- Question: How does low-rank fine-tuning (e.g., LoRA) compare to full parameter fine-tuning in terms of the adaptive feature kernels and transfer learning effectiveness?
- Basis in paper: [explicit] The authors state: "While our current theory describes fine tuning every parameter in the model with gradient descent, future work could explore a theory for low-rank fine tuning methods, which are becoming increasingly popular for large pretrained models."
- Why unresolved: Low-rank methods constrain weight updates to a subspace, potentially altering the kernel evolution dynamics captured by the current DMFT framework.
- What evidence would resolve it: Extension of the theoretical framework incorporating low-rank constraints, plus comparative experiments measuring generalization gaps between full and low-rank fine-tuning across varying target sample sizes.

## Limitations
- The infinite-width limit with mean-field parameterization may not fully capture finite-width effects observed in practice, particularly for moderate network sizes (N < 10,000)
- The theory assumes Gaussian data and controlled task similarity through alignment parameters, which may not reflect the complex correlations and hierarchical structures present in real-world datasets
- The elastic weight coupling mechanism requires careful hyperparameter tuning in practice, and the optimal δ value is highly task-dependent

## Confidence

**High confidence**: The core mechanism of adaptive kernel formation through elastic coupling (Mechanism 1) is mathematically rigorous and supported by both theoretical analysis and experimental validation in controlled settings.

**Medium confidence**: The task alignment effects (Mechanism 2) are well-predicted by theory in synthetic settings, but real-world task similarity is more nuanced and may not be fully captured by simple alignment metrics.

**Medium confidence**: The sample size dependency (Mechanism 3) is demonstrated experimentally, but the theory assumes source pretraining data is sufficient (P₁ ≥ P₂), which may not hold in all practical scenarios.

## Next Checks

1. **Finite-width validation**: Systematically test the theory's predictions across a range of network widths (N = 100, 1,000, 10,000, 100,000) on synthetic linear regression tasks to quantify where finite-width corrections become significant and identify scaling laws for the critical width needed for accurate predictions.

2. **Real-world task transfer**: Extend validation to realistic transfer scenarios such as ImageNet pretraining followed by fine-tuning on medical imaging datasets, measuring both the predicted kernel evolution and actual generalization improvements, while tracking whether the elastic coupling parameter correlates with optimal transfer performance.

3. **Distribution shift robustness**: Test transfer learning under controlled distribution shifts (e.g., covariate shift, label shift) to determine how well the theory predicts transfer utility when source and target data have different marginal distributions, which would extend the current framework beyond the assumed covariate alignment.