---
ver: rpa2
title: Masked Omics Modeling for Multimodal Representation Learning across Histopathology
  and Molecular Profiles
arxiv_id: '2508.00969'
source_url: https://arxiv.org/abs/2508.00969
tags:
- omics
- learning
- multimodal
- data
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MORPHEUS introduces the first self-supervised learning framework
  for multimodal integration of histopathology images and molecular omics profiles
  in cancer. It uses a novel masked omics modeling objective where the model learns
  to reconstruct randomly masked portions of omics data (RNA, DNA methylation, and
  copy number variation) from unmasked omics features and whole-slide images.
---

# Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles

## Quick Facts
- arXiv ID: 2508.00969
- Source URL: https://arxiv.org/abs/2508.00969
- Authors: Lucas Robinet; Ahmad Berjaoui; Elizabeth Cohen-Jonathan Moyal
- Reference count: 40
- Primary result: First self-supervised framework for integrating histopathology images and molecular omics profiles in cancer

## Executive Summary
MORPHEUS introduces a novel self-supervised learning framework for multimodal integration of histopathology images and molecular omics profiles in cancer. The method employs a masked omics modeling objective where the model learns to reconstruct randomly masked portions of omics data (RNA, DNA methylation, and copy number variation) from unmasked omics features and whole-slide images. Pre-trained on 4,718 pan-cancer samples from TCGA, MORPHEUS achieves significant performance improvements over supervised and self-supervised baselines across multiple tasks, demonstrating strong capabilities in biomarker classification, few-shot learning for brain tumor subtyping, and flexible omics reconstruction.

## Method Summary
MORPHEUS uses a shared transformer architecture with modality-specific tokenization strategies for multimodal integration. For histology, it employs a Perceiver-based aggregation of image patches, while omics data is processed through pathway- or chromosome-based grouping. The core innovation is the masked omics modeling objective, where random portions of omics features are masked and the model learns to reconstruct them using both unmasked omics data and whole-slide images as context. The model was pre-trained on 4,718 pan-cancer samples and evaluated across multiple downstream tasks including biomarker classification, tumor subtyping, and omics reconstruction.

## Key Results
- Achieves 6.3% relative improvement in biomarker classification (AUC up to 97.3%) over supervised and self-supervised baselines
- Demonstrates strong few-shot learning capabilities with 11.1-12.0% AUC gains in brain tumor subtyping
- Enables flexible omics reconstruction with median correlations of 0.46-0.74 across RNA, DNA methylation, and copy number variation profiles

## Why This Works (Mechanism)
The framework leverages the complementary information available in histopathology images and molecular profiles to create rich, cross-modal representations. By masking portions of omics data and reconstructing them from both omics context and histology, the model learns to identify meaningful associations between tissue morphology and molecular patterns. The shared transformer architecture with modality-specific tokenization allows for effective integration while preserving the unique characteristics of each data type.

## Foundational Learning
- **Masked language modeling**: Why needed - provides the conceptual foundation for the masked omics modeling objective; Quick check - ensures understanding of how masking and reconstruction drive representation learning
- **Transformer architectures**: Why needed - enables effective multimodal integration through attention mechanisms; Quick check - verify understanding of self-attention and cross-modal attention operations
- **Perceiver models**: Why needed - handles the quadratic complexity of attention over large image patches; Quick check - confirm understanding of cross-attention to latent arrays
- **Multi-omics integration**: Why needed - combines complementary information from different molecular modalities; Quick check - validate understanding of how pathway-based grouping preserves biological meaning
- **Self-supervised learning**: Why needed - enables pre-training without labeled data; Quick check - ensure grasp of contrastive and reconstruction-based objectives

## Architecture Onboarding

**Component map**: Whole-slide images -> Perceiver aggregation -> Token embeddings -> Shared transformer; Omics profiles -> Pathway/chromosome grouping -> Token embeddings -> Shared transformer; Shared transformer -> Cross-modal attention -> Masked omics reconstruction

**Critical path**: The model processes histology patches through Perceiver aggregation to create compact image representations, while omics data is grouped by biological pathways or chromosomes. Both modalities are tokenized and fed into a shared transformer architecture where cross-modal attention enables information exchange. The critical path for learning involves the masked omics reconstruction objective, where the model must use both modalities to predict masked portions of omics data.

**Design tradeoffs**: The choice of Perceiver over standard transformers reduces computational complexity from quadratic to linear in the number of image patches, enabling processing of whole-slide images. However, this comes at the cost of some representational capacity. The pathway-based grouping of omics data preserves biological meaning but may miss interactions between pathways.

**Failure signatures**: Poor performance on rare cancer types or non-tumor tissues would indicate limitations in generalization. Computational bottlenecks during inference could signal insufficient optimization for production environments. Inability to reconstruct certain omics modalities would suggest modality-specific weaknesses in the cross-modal attention mechanisms.

**First experiments**: 
1. Ablation study removing the histology input to assess its contribution to omics reconstruction
2. Test reconstruction performance on held-out omics modalities not seen during pre-training
3. Evaluate cross-modal attention weights to identify which histopathology patterns drive molecular predictions

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability constraints due to reliance on TCGA data with potential batch effects and demographic biases
- Substantial computational requirements limiting accessibility for smaller research groups
- Incomplete interpretability of complex interactions between histopathology patterns and molecular pathways

## Confidence

**High confidence**: Masked omics modeling methodology, transformer architecture design, and performance improvements over baselines with appropriate statistical validation.

**Medium confidence**: Biological interpretability claims requiring independent experimental validation, and few-shot learning capabilities tested on limited brain tumor subtypes.

**Low confidence**: Long-term clinical utility and performance on external datasets outside TCGA remain speculative without validation.

## Next Checks
1. External dataset validation using independent multi-omics datasets (CPATC, independent pathology archives) to assess generalizability
2. Temporal stability assessment on longitudinal samples to evaluate representation stability across disease progression
3. Clinical workflow integration pilot study to assess practical utility, computational efficiency, and pathologist acceptance