---
ver: rpa2
title: 'UQLegalAI@COLIEE2025: Advancing Legal Case Retrieval with Large Language Models
  and Graph Neural Networks'
arxiv_id: '2505.20743'
source_url: https://arxiv.org/abs/2505.20743
tags:
- case
- legal
- retrieval
- cases
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CaseLink, a method that achieved second place
  in Task 1 of COLIEE 2025, a legal case retrieval competition. The approach addresses
  the challenge of identifying relevant legal precedents from large case databases
  by leveraging case connectivity through a Global Case Graph (GCG) that captures
  Case-Case, Case-Charge, and Charge-Charge relationships.
---

# UQLegalAI@COLIEE2025: Advancing Legal Case Retrieval with Large Language Models and Graph Neural Networks

## Quick Facts
- arXiv ID: 2505.20743
- Source URL: https://arxiv.org/abs/2505.20743
- Reference count: 22
- Primary result: Achieved second place in COLIEE 2025 Task 1 with F1 score of 0.2962

## Executive Summary
This paper presents CaseLink, a legal case retrieval system that combines large language models, graph neural networks, and contrastive learning to identify relevant legal precedents from large case databases. The approach addresses the challenge of retrieving "noticed cases" - precedents referenced by a query - from Federal Court of Canada cases. CaseLink constructs a Global Case Graph (GCG) that captures multiple types of relationships between cases and charges, then uses a GNN to propagate information across this graph structure. The system achieved second place in the COLIEE 2025 competition, demonstrating the effectiveness of graph-based methods for legal case retrieval.

## Method Summary
CaseLink addresses legal case retrieval by constructing a heterogeneous Global Case Graph (GCG) that encodes Case-Case, Case-Charge, and Charge-Charge relationships. The system uses e5-mistral-7b-instruct to generate text embeddings for cases and charges, then applies a Graph Attention Network (GAT) to propagate information across the graph structure. Training employs contrastive learning with InfoNCE loss and degree regularization to prevent representation collapse. The inference pipeline uses two-stage retrieval: first applying BM25 to filter candidates, then CaseLink ranking with year filtering to exclude later-dated cases. Hyperparameters include batch size, GNN layers, dropout, learning rate, regularization coefficient λ, TopK neighbors, and charge similarity threshold δ.

## Key Results
- Achieved F1 score of 0.2962 in COLIEE 2025 Task 1, placing second
- Demonstrated stable performance with minimal variance across submissions
- Lower recall (0.3019) compared to top team (0.3735), attributed to fixed retrieval cutoff of 5 candidates
- Outperformed baseline methods through combination of graph structure and contrastive learning

## Why This Works (Mechanism)

### Mechanism 1: Global Case Graph (GCG) for Structural Relationship Encoding
- Claim: Representing legal cases as a heterogeneous graph with multiple edge types may capture relational patterns that purely text-based similarity metrics miss.
- Mechanism: Three edge types encode distinct legal relationships—Case-Case edges via BM25 top-K similarity, Case-Charge edges when charge text appears in a case, and Charge-Charge edges when embedding similarity exceeds threshold δ. The combined adjacency matrix A ∈ R^(n+m)×(n+m) integrates all relationship signals into a unified structure for GNN propagation.
- Core assumption: Legal relevance is partially determined by citation patterns and shared charge classifications, not just textual similarity.
- Evidence anchors: [abstract] "CaseLink model utilises inductive graph learning and Global Case Graphs to capture the intrinsic case connectivity" [section] Section 4.1.2 defines edge construction with explicit formulas for A_d, A_c, A_b matrices [corpus] "A Reproducibility Study of Graph-Based Legal Case Retrieval" (arXiv:2504.08400) reproduces CaseLink and confirms graph-based methods provide complementary signals to lexical approaches

### Mechanism 2: LLM-Initialized Node Features for Semantic Grounding
- Claim: Initializing graph nodes with high-quality text embeddings from a specialized LLM may provide stronger semantic representations than training embeddings from scratch.
- Mechanism: The e5-mistral-7b-instruct model (chosen for top MTEB legal retrieval performance) encodes case and charge text into d-dimensional embeddings x = LLM(t). These serve as input features X to the GNN, grounding graph structure in semantic content.
- Core assumption: Pre-trained LLM embeddings capture legally relevant semantic properties that transfer to retrieval tasks without domain-specific fine-tuning.
- Evidence anchors: [abstract] "a large language model specialized in text embedding is employed to transform legal texts into embeddings" [section] Section 4.1.3 specifies e5-mistral-7b-instruct selection and 4096-token truncation for long cases [corpus] "ReaKase-8B" (arXiv:2510.26178) demonstrates that LLM-based knowledge representations improve legal retrieval, supporting the semantic grounding hypothesis

### Mechanism 3: Degree Regularization for Citation-Aware Representation Learning
- Claim: Penalizing high cosine similarity between all candidate pairs may prevent representation collapse while encouraging discrimination among non-cited cases.
- Mechanism: After GNN propagation, pseudo-adjacency Â_ij = cos(h_i, h_j) measures pairwise similarity across all candidates. Degree regularization ℓ_DegReg = Σ_i Σ_j (Â_ij) penalizes high total similarity, encouraging representations to spread rather than cluster indiscriminately.
- Core assumption: Legal cases should not be uniformly similar; cited precedents should be distinguishable from the broader candidate pool.
- Evidence anchors: [abstract] "A new contrastive objective, incorporating a regularization on the degree of case nodes, is proposed" [section] Equation 12-13 define degree regularization term with coefficient λ; Section 5.1 notes λ selected from {0, 5e-4, 1e-3, 5e-3} [corpus] Weak direct evidence in corpus for degree regularization specifically; this appears novel to CaseLink

## Foundational Learning

- Concept: Contrastive Learning with InfoNCE Loss
  - Why needed here: Core training objective that learns to pull query-positive pairs together and push query-negative pairs apart in embedding space
  - Quick check question: Can you explain why InfoNCE requires both positive and negative samples, and what happens if negatives are too easy or too hard?

- Concept: Graph Neural Network Message Passing
  - Why needed here: GNN layers propagate information across GCG edges, allowing case representations to incorporate neighbor information (cited cases, shared charges)
  - Quick check question: For a 2-layer GAT on GCG, what is the receptive field of a query node—how many hops of neighbors influence its final representation?

- Concept: Inductive vs. Transductive Graph Learning
  - Why needed here: CaseLink uses inductive learning to handle unseen test cases by constructing separate GCGs for train/test, critical for real-world deployment
  - Quick check question: If test cases were added to the training GCG without retraining, what leakage would occur and how would it invalidate evaluation?

## Architecture Onboarding

- Component map: Raw case text + charge labels → LLM Encoder (e5-mistral-7b-instruct) → Node embeddings X → GCG Construction → GNN (GAT) → Updated representations H → Training Head (InfoNCE + DegReg) → Cosine similarity s(q,d) for ranking → BM25 two-stage filtering + year filtering

- Critical path: LLM embedding quality → GCG edge construction (K, δ hyperparameters) → GNN depth and attention → λ regularization strength. Errors in early stages compound; truncated text or missing edges cannot be recovered downstream.

- Design tradeoffs:
  - Token truncation (4096) vs. full case encoding: Long cases lose information, but full encoding is computationally infeasible
  - Top-K case neighbors: Higher K captures more structure but introduces noise; paper uses K ∈ {3, 5, 10}
  - GNN depth: More layers propagate further but risk over-smoothing; paper tests 1-3 layers
  - Fixed retrieval cutoff (5): Matches average relevant cases (~4) but limits recall vs. top team's higher recall (0.3735 vs. 0.3019)

- Failure signatures:
  - Low recall with high precision: Likely over-conservative cutoff or over-regularization (λ too high)
  - High variance across runs: Hyperparameter sensitivity; check batch size, learning rate, negative sampling
  - Degraded performance on recent cases: Year filtering may be excluding valid candidates if date extraction fails
  - Training instability: Check temperature τ in InfoNCE and negative sampling strategy (easy vs. hard negatives)

- First 3 experiments:
  1. **Ablation on edge types**: Train three variants—GCG with only Case-Case edges, only Case-Charge edges, and full GCG. Measure F1 delta to isolate contribution of each relationship type.
  2. **Token truncation analysis**: Sample 50 long cases (>10K tokens), manually identify key legal content position. Test sliding window vs. head-only truncation to quantify information loss.
  3. **Negative sampling study**: Compare (1 easy, 0 hard) vs. (1 easy, 5 hard) vs. (1 easy, 10 hard) negative configurations. Track training dynamics and final F1 to determine if hard negatives from BM25 top-K provide useful contrastive signal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the truncation of input cases to 4096 tokens significantly degrade retrieval performance for cases where relevant legal arguments are located in the middle or end of the document?
- Basis in paper: [inferred] The paper states that while the average case length is ~30,000 tokens and the maximum is ~680,000, the input is truncated to 4096 tokens due to LLM constraints.
- Why unresolved: The authors do not provide an ablation study on the effect of this information loss on the embedding quality or final retrieval accuracy.
- What evidence would resolve it: A comparative analysis of retrieval performance on full-text cases (using a long-context model) versus the truncated approach.

### Open Question 2
- Question: Would constructing Global Case Graph edges using semantic similarity rather than BM25 improve the retrieval of conceptually similar cases that lack shared terminology?
- Basis in paper: [inferred] The Case-Case edges are constructed strictly using TopK BM25 scores, which is a lexical method, potentially limiting the graph's ability to connect semantically related cases.
- Why unresolved: The paper does not compare the efficacy of lexical-based graph construction against semantic-similarity-based graph construction.
- What evidence would resolve it: An experiment replacing the BM25 edge selection with cosine similarity of the initial LLM embeddings, measuring the change in F1 score.

### Open Question 3
- Question: Can the recall deficit relative to the top-performing team be closed by increasing the number of retrieved candidates beyond the fixed threshold of five?
- Basis in paper: [explicit] The authors note their lower Recall compared to the winner and state, "This can be due to the selection of more candidates in ranking."
- Why unresolved: The paper fixes the final retrieved case count at five based on training set statistics but does not validate if a larger cutoff improves recall.
- What evidence would resolve it: Results from a sweep of different top-$k$ cutoff values (e.g., 10, 20, 50) on the validation set to plot the Precision-Recall trade-off.

## Limitations
- Unknown optimal hyperparameter values selected from search spaces (batch size, learning rate, λ, K, δ)
- Fixed retrieval cutoff of 5 candidates may artificially cap recall compared to top-performing systems
- Token truncation to 4096 tokens for long cases (~30K tokens average) could lose critical legal information
- Degree regularization's effectiveness appears novel but lacks direct supporting evidence in the corpus

## Confidence

- **High Confidence**: Graph-based relational encoding improves over pure text similarity; GNN propagation provides meaningful signal; InfoNCE contrastive learning is well-established; achieved second place with stable F1 performance.
- **Medium Confidence**: LLM-initialized embeddings provide superior semantic grounding; degree regularization prevents representation collapse; two-stage inference with year filtering improves precision; citation networks are sufficiently dense.
- **Low Confidence**: Optimal hyperparameter configuration; exact contribution of each edge type to performance; effectiveness of hard negative sampling strategy; robustness across different legal domains.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically test λ values {0, 5e-4, 1e-3, 5e-3} and GNN layers {1, 2, 3} to identify optimal configurations and measure performance variance.
2. **Edge contribution ablation**: Train three variants—GCG with only Case-Case edges, only Case-Charge edges, and full GCG—to quantify the marginal contribution of each relationship type to F1 score.
3. **Long case information loss quantification**: Sample 50 cases exceeding 4096 tokens, manually identify critical legal content positions, and test sliding window vs. head-only truncation to measure impact on retrieval accuracy.