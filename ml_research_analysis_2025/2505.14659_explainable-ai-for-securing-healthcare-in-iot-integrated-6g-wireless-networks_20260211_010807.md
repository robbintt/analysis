---
ver: rpa2
title: Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks
arxiv_id: '2505.14659'
source_url: https://arxiv.org/abs/2505.14659
tags:
- security
- healthcare
- data
- normal
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper applies explainable AI (XAI) techniques\u2014SHAP,\
  \ LIME, and DiCE\u2014to enhance security in 6G-integrated healthcare IoT systems.\
  \ Using the WUSTL-HDRL-2024 dataset, the authors preprocess and balance the data\
  \ with SMOTE, then train and evaluate machine learning models, finding Random Forest\
  \ achieves 99.85% accuracy."
---

# Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks

## Quick Facts
- arXiv ID: 2505.14659
- Source URL: https://arxiv.org/abs/2505.14659
- Authors: Navneet Kaur; Lav Gupta
- Reference count: 23
- Primary result: XAI techniques (SHAP, LIME, DiCE) enhance security in 6G-integrated healthcare IoT by identifying key attack indicators with 99.85% accuracy using Random Forest.

## Executive Summary
This paper presents a novel approach to securing healthcare IoT systems in 6G networks by applying explainable AI (XAI) techniques. The authors preprocess and balance the WUSTL-HDRL-2024 dataset using SMOTE, then train multiple machine learning models to detect security threats. Random Forest emerges as the top performer with 99.85% accuracy. The study demonstrates how XAI methods like SHAP, LIME, and DiCE can identify critical features driving model predictions, particularly CPU activity and network behavior patterns. This interpretability enables security teams to better understand and respond to threats in complex 6G-enabled medical environments.

## Method Summary
The research employs a systematic methodology beginning with dataset preprocessing and balancing using SMOTE to address class imbalance in the WUSTL-HDRL-2024 dataset. Multiple machine learning models are trained and evaluated, with Random Forest achieving superior performance at 99.85% accuracy. The authors then apply three XAI techniques—SHAP for feature importance, LIME for local interpretability, and DiCE for counterfactual explanations—to validate and cross-check the key features identified across methods. The consistency between these approaches strengthens confidence in the identified security indicators, particularly focusing on CPU activity and network behavior as critical predictors of attacks in healthcare IoT systems.

## Key Results
- Random Forest model achieves 99.85% accuracy in detecting security threats in healthcare IoT
- XAI methods (SHAP, LIME, DiCE) consistently identify CPU activity and network behavior as key attack indicators
- Cross-method consistency validates the reliability of feature importance rankings
- Explainable AI improves interpretability for security teams responding to threats in 6G networks

## Why This Works (Mechanism)
The approach works by combining high-performance machine learning with interpretability techniques that make model decisions transparent. The SMOTE balancing addresses class imbalance, allowing models to learn attack patterns effectively. XAI methods then provide human-understandable explanations of model predictions, identifying which features (like CPU usage spikes or unusual network traffic) indicate potential security breaches. This transparency is crucial in healthcare settings where security teams need to understand why alerts are triggered to make informed response decisions.

## Foundational Learning
- **SMOTE (Synthetic Minority Over-sampling Technique)**: Balances imbalanced datasets by creating synthetic samples of minority classes. Why needed: Attack samples are typically outnumbered by normal traffic, so balancing improves model learning. Quick check: Verify minority class samples increase proportionally after application.

- **SHAP (SHapley Additive exPlanations)**: Provides feature importance scores based on game theory principles. Why needed: Identifies which input features most influence model predictions globally. Quick check: Ensure feature importance scores sum to model prediction for any given instance.

- **LIME (Local Interpretable Model-agnostic Explanations)**: Explains individual predictions by approximating the model locally with an interpretable surrogate. Why needed: Helps understand specific decision boundaries for individual security alerts. Quick check: Confirm local explanations are consistent with global SHAP findings.

- **DiCE (Diverse Counterfactual Explanations)**: Generates diverse counterfactual examples showing what changes would alter model predictions. Why needed: Reveals actionable insights for threat mitigation by showing minimal changes to avoid detection. Quick check: Validate counterfactuals actually change model output when applied.

- **Random Forest**: Ensemble learning method combining multiple decision trees. Why needed: Provides robust classification with inherent feature importance measures and handles complex non-linear relationships. Quick check: Measure out-of-bag error rate to assess model generalization.

## Architecture Onboarding

**Component Map**
WUSTL-HDRL-2024 Dataset -> SMOTE Preprocessing -> ML Model Training -> XAI Analysis (SHAP/LIME/DiCE) -> Security Insights

**Critical Path**
Data preprocessing (SMOTE) -> Model training (Random Forest) -> XAI validation (SHAP/LIME/DiCE consistency) -> Security interpretation

**Design Tradeoffs**
The choice of Random Forest prioritizes accuracy and interpretability over potentially higher-performing but less explainable deep learning models. SMOTE balancing improves attack detection but may introduce synthetic artifacts. Using three XAI methods provides validation but increases computational complexity.

**Failure Signatures**
Overfitting indicated by perfect training accuracy but poor test performance. XAI inconsistency between methods suggesting unreliable feature importance. High false positive rates in clinical deployment indicating poor real-world generalization.

**First 3 Experiments**
1. Test model performance on an independent healthcare IoT security dataset to validate generalization
2. Conduct adversarial attacks targeting the explainable models to assess robustness
3. Deploy in a simulated healthcare environment to measure practical impact on security response times

## Open Questions the Paper Calls Out
None

## Limitations
- Single dataset (WUSTL-HDRL-2024) may not represent diverse real-world 6G healthcare IoT threat landscape
- SMOTE balancing could introduce synthetic data artifacts that don't reflect genuine attack patterns
- Extremely high accuracy (99.85%) raises concerns about potential overfitting to specific dataset characteristics

## Confidence

| Claim | Confidence |
|-------|------------|
| XAI Method Effectiveness | Medium |
| Model Performance Claims | Medium |
| Practical Security Impact | Low |

## Next Checks
1. **External Dataset Validation**: Test the trained models and XAI methods on an independent healthcare IoT security dataset to verify performance and interpretability consistency across different data distributions.

2. **Adversarial Robustness Assessment**: Conduct attacks specifically designed to fool the explainable models (e.g., adversarial examples) to evaluate whether XAI insights remain reliable under attack conditions.

3. **Clinical Integration Study**: Collaborate with healthcare security practitioners to assess whether the XAI-generated explanations actually improve threat detection and response times compared to traditional black-box approaches in realistic scenarios.