---
ver: rpa2
title: 'MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents
  with Tools'
arxiv_id: '2504.20168'
source_url: https://arxiv.org/abs/2504.20168
tags:
- mice
- confidence
- layers
- risk
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MICE improves calibration of tool-calling agents by extracting
  features from intermediate model layers and combining them with raw confidence scores.
  It uses logit lens to decode from each layer and computes BERTScore similarities
  between intermediate and final layer generations.
---

# MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools

## Quick Facts
- **arXiv ID:** 2504.20168
- **Source URL:** https://arxiv.org/abs/2504.20168
- **Reference count:** 21
- **Primary result:** MICE improves tool-calling agent calibration by extracting intermediate layer features and combining them with raw confidence scores, achieving better expected tool-calling utility especially at high risk levels.

## Executive Summary
MICE introduces a method for calibrating tool-calling agents by leveraging model-internal confidence signals. The approach extracts features from intermediate layers using Logit Lens and BERTScore similarity, then combines these with raw confidence scores in a learned classifier. On Llama3 models using the STE dataset, MICE matches or exceeds baseline calibration methods while significantly improving expected tool-calling utility across different risk scenarios. The method is sample-efficient and demonstrates reasonable zero-shot generalization to unseen APIs.

## Method Summary
MICE extracts intermediate layer predictions using Logit Lens (projecting hidden states to vocabulary space), computes BERTScore similarity between these intermediate and final outputs, and combines these features with raw token probability products (excluding formatting tokens). A learned classifier (logistic regression or random forest) then predicts the probability that a tool call is correct. The method requires access to intermediate hidden states during inference and trains on labeled examples of correct/incorrect tool calls.

## Key Results
- MICE matches or beats baseline methods on smooth expected calibration error (smECE) for Llama3 models
- Significantly improves expected tool-calling utility (ETCU) compared to baselines, especially at higher risk levels (fp = -9)
- Demonstrates zero-shot generalization capability, performing comparably to baselines when evaluated on unseen APIs
- Shows sample efficiency, achieving good calibration with limited training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate layer generations evolve toward the final output, and the trajectory of this refinement contains signal about output correctness.
- Mechanism: At each layer *i*, the hidden state *h*^(*i*) is multiplied by the unembedding matrix *W*_{out} to produce logits, then argmax-decoded into a preliminary string *y*^(*i*). BERTScore similarity between *y*^(*i*) and the final output *y* is computed for all layers, producing *ℓ* − 1 features.
- Core assumption: Predictions that "stabilize early" (high BERTScore in later intermediate layers) are more likely correct than those that change abruptly in the final layers.
- Evidence anchors:
  - [abstract] "MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output."
  - [section 2] "We hypothesize that features from intermediate layers' hidden states could provide useful signal for calibration. Drastic changes in the final few layers could indicate the inability for the LLM to pinpoint a tool call."
  - [corpus] Related work on early-exit and intermediate decoding (Schuster et al., 2022; Belrose et al., 2023) supports the premise that layers refine predictions iteratively, though calibration-specific use is novel here.
- Break condition: If BERTScore distributions for correct vs. incorrect outputs overlap heavily at all layers, the feature provides no discriminative signal.

### Mechanism 2
- Claim: Raw token probability alone is poorly calibrated; combining it with internal trajectory features improves calibration, especially in high-risk regimes.
- Mechanism: Raw confidence (product of token probabilities, excluding formatting tokens) is concatenated with BERTScore features and fed to a classifier (logistic regression or random forest). The classifier learns to weight these jointly.
- Core assumption: The classifier can learn a non-trivial mapping from the combined feature vector to correctness probability that neither feature alone captures.
- Evidence anchors:
  - [abstract] "These features are fed into a learned probabilistic classifier to assess confidence in the decoded output."
  - [section 4.4] "Confidence alone performed extremely poorly" for RF; for LR, "confidence accounts for much of the performance," suggesting interaction between model choice and feature utility.
  - [corpus] No direct corpus evidence on this specific combination; mechanism remains hypothesis-driven.
- Break condition: If the classifier overfits to training APIs or fails to generalize, performance degrades to baseline levels.

### Mechanism 3
- Claim: MICE generalizes zero-shot to unseen APIs because the learned calibration signal is feature-based rather than API-specific.
- Mechanism: Training on 49 APIs and evaluating on a held-out 50th API tests generalization. BERTScore features and raw confidence are API-agnostic representations of model behavior.
- Core assumption: The relationship between intermediate layer stability and correctness is consistent across different tool-calling domains.
- Evidence anchors:
  - [section 5] "MICE does worse in this [zero-shot] setting, but only degrades to the level of HRE and NWKR models trained on the full data; they are statistically indistinguishable from them."
  - [corpus] No direct corpus comparison for this claim.
- Break condition: If new APIs have substantially different output distributions or formatting, BERTScore alignment may become noisy.

## Foundational Learning

- **Logit Lens**
  - Why needed here: Core technique for extracting predictions from intermediate layers by projecting hidden states to vocabulary space.
  - Quick check question: Can you explain why multiplying a hidden state by the unembedding matrix yields a valid logit distribution?

- **Expected Calibration Error (ECE) / Smooth ECE**
  - Why needed here: Primary metric for evaluating whether confidence scores match empirical accuracy.
  - Quick check question: Why is a model that always predicts the base rate (e.g., 0.7) considered perfectly calibrated but practically useless?

- **Minimum Bayes Risk (MBR) Decision Threshold**
  - Why needed here: Connects calibrated confidence to action policies (execute vs. abstain) via utility parameters (tp, fp, tn, fn).
  - Quick check question: Given fp = −9 and tp = 1, what confidence threshold should trigger tool execution?

## Architecture Onboarding

- **Component map:**
  1. LLM forward pass → collect hidden states *H*^(*i*) at each layer
  2. Logit lens: *H*^(*i*) *W*_{out} → logits → argmax → *y*^(*i*)
  3. BERTScore(*y*^(*i*), *y*) for each layer → feature vector
  4. Raw confidence: ∏_{t ∈ S} *p*(*w*_{t}|*w*_{<t}) (exclude formatting tokens)
  5. Classifier (LR or RF) → calibrated confidence *p̂*
  6. Decision rule: execute if *p̂* > τ

- **Critical path:** Access to intermediate hidden states is non-negotiable. This rules out closed-source APIs (e.g., OpenAI) unless they expose layer outputs.

- **Design tradeoffs:**
  - LR is simpler and more interpretable; RF captures non-linear feature interactions but risks overfitting with small data.
  - BERTScore requires an external model (DeBERTa-xlarge-mnli), adding compute overhead.
  - Excluding formatting tokens reduces noise but requires token-level labeling logic.

- **Failure signatures:**
  - smECE near zero but ETCU also near zero → classifier is "safe" but uninformative (always abstains).
  - Large gap between train and test smECE → overfitting to seen APIs.
  - High variance across cross-validation folds → insufficient training data.

- **First 3 experiments:**
  1. **Baseline sanity check:** Run Raw Confidence, HRE, and NWKR on your target LLM + dataset. Confirm smECE ordering matches paper (Raw >> HRE ≈ NWKR).
  2. **Feature ablation:** Train MICE RF with (a) confidence only, (b) BERTScores only, (c) both. Verify that both feature types contribute.
  3. **Zero-shot holdout:** Train on N−1 APIs, test on held-out API. Compare degradation to full-training MICE and to baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MICE effectively improve calibration in non-tool-calling domains such as machine translation or question answering?
- Basis: [explicit] Section 9 explicitly states that other settings "have been left to future experiments."
- Why unresolved: The paper restricted evaluation to the tool-calling task on the STE dataset.
- What evidence would resolve it: Application of MICE to standard machine translation or QA benchmarks showing comparable calibration improvements.

### Open Question 2
- Question: Would replacing the logit lens with a learned linear transformation (tuned lens) yield superior confidence estimation features?
- Basis: [explicit] Section 9 mentions "possible encoding tricks" like the tuned lens, noting the authors "do not claim to have found the best variant."
- Why unresolved: The current method relies solely on BERTScore similarities from argmax decoding.
- What evidence would resolve it: Ablation studies comparing the current BERTScore approach against tuned lens embeddings on the Llama3 models.

### Open Question 3
- Question: Can model-internal confidence estimation be adapted for closed-source models that restrict access to internal activations?
- Basis: [explicit] Section 9 highlights that MICE "requires access to model internals, ruling out some of the most capable current LLMs."
- Why unresolved: The method depends entirely on extracting hidden states, which is impossible without model weights.
- What evidence would resolve it: A proxy method or distillation technique that approximates internal confidence signals for closed models without direct access.

## Limitations
- Requires access to intermediate hidden states, incompatible with closed-source APIs without architectural modifications
- BERTScore computation adds significant computational overhead and depends on an external model
- Zero-shot generalization performance is modest, degrading to baseline levels despite claims of generalization
- Requires careful token-level handling to exclude formatting tokens from confidence calculations

## Confidence
- **Mechanism 1**: High confidence - clear empirical evidence and well-grounded in transformer architecture
- **Mechanism 2**: Medium confidence - demonstrated improvements but unclear feature interaction and classifier dependency
- **Mechanism 3**: Medium confidence - generalization claims supported but comparison to baselines is weak

## Next Checks
1. **Feature Ablation Replicability**: Implement and test MICE with (a) raw confidence only, (b) BERTScore features only, and (c) both combined. Verify that both feature types contribute meaningfully to calibration performance, and that the combination outperforms either alone across different risk levels.

2. **Generalization Stress Test**: Conduct a more rigorous zero-shot evaluation by training on subsets of APIs with varying characteristics (e.g., different domains, output formats) and testing on held-out APIs. Compare MICE performance to simple baseline methods like threshold calibration on raw confidence.

3. **Mechanism Validation**: Perform a diagnostic analysis of BERTScore trajectories across correct vs. incorrect predictions. Visualize layer-wise similarity patterns to confirm that stable early predictions correlate with correctness, and test whether this signal persists across different LLM architectures or scales.