---
ver: rpa2
title: 'NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using
  an Unsupervised CriticNL2LTL Framework'
arxiv_id: '2512.18189'
source_url: https://arxiv.org/abs/2512.18189
tags:
- cognitive
- human
- rules
- production
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NL2CA introduces an unsupervised framework that translates natural
  language descriptions of human decision-making into formal Linear Temporal Logic
  (LTL), then into executable production rules for cognitive architectures. The method
  combines a fine-tuned LLM with a heterogeneous Critic Tree for iterative refinement,
  avoiding human-in-the-loop feedback.
---

# NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework

## Quick Facts
- arXiv ID: 2512.18189
- Source URL: https://arxiv.org/abs/2512.18189
- Reference count: 4
- Primary result: Translates natural language driving decisions into formal LTL, achieving 80.6% accuracy and 78.57% success rate in driving simulation

## Executive Summary
NL2CA presents an unsupervised framework that automatically translates natural language descriptions of human decision-making into formal Linear Temporal Logic (LTL), then into executable production rules for cognitive architectures. The method combines a fine-tuned LLM with a heterogeneous Critic Tree for iterative refinement, avoiding human-in-the-loop feedback. This LTL-based formalization supports both accurate translation and interpretable symbolic reasoning. NL2CA is validated in two domains: NL-to-LTL translation (achieving 80.6% accuracy and 0.517 BLEU on expert benchmarks), and cognitive driving simulation (78.57% success rate, 21.54cm average lane deviation).

## Method Summary
The NL2CA framework uses a three-stage pipeline: first, a fine-tuned LLM translates natural language to LTL; second, a heterogeneous Critic Tree refines the LTL using unsupervised feedback from multiple critic models; third, a formalizer converts valid LTL formulas into pyactr production rules. The system filters out complex temporal operators, only accepting G(prop→prop) patterns as executable rules. Cognitive Reinforcement Learning then adjusts rule utilities based on behavioral alignment with human trial data. The framework is validated on two datasets (expert NL-to-LTL benchmark and synthetic LTL generation) and in CARLA driving simulation with 34 human drivers across 3 scenarios.

## Key Results
- NL-to-LTL translation accuracy of 80.6% (BLEU 0.517) on expert benchmarks
- Driving simulation success rate of 78.57% with 21.54cm average lane deviation
- CriticNL2LTL framework outperforms standard self-refine (63.9% accuracy) through heterogeneous critic feedback
- Literal interpretation (M_lit) outperforms supervised generation (M_sup) in human behavior alignment

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Tree-Based Logic Refinement
The framework employs a "Critic Tree" where a fine-tuned LLM generates an initial LTL translation. Unlike standard self-refine loops, this system uses heterogeneous critic LLMs (e.g., different model versions) to evaluate logical correctness. If critics detect a mismatch, a "revisor" LLM spawns a child node with a corrected formula. This tree expansion continues until critics approve or max depth is reached, creating a competitive selection process for the best logical formalization. Core assumption: heterogeneous model feedback provides more robust error signals than homogeneous self-correction. Evidence: Deepseek R1 + Self-Refine achieves 63.9% accuracy, while CriticNL2LTL achieves 80.6% on Dataset 1.

### Mechanism 2: LTL as an Intermediate Constraint Layer
The system translates NL to LTL first, then applies a strict filter: only LTL formulas fitting the pattern `G(<propositions> -> <propositions>)` (Globally/Implication) are transformed into IF-THEN production rules. Complex temporal operators (e.g., "Until", "Finally") are flagged as "Inference Error" and excluded, ensuring the resulting cognitive rules are instantaneous and executable. Core assumption: valid human decision-making rules can be largely expressed as state-based implications rather than complex temporal sequences. Evidence: Figure 3 shows the distribution of "Viable" vs. "Inference Error" results, demonstrating the filter in action.

### Mechanism 3: Utility-Based Cognitive Reinforcement Learning (CRL)
Extracted rules are instantiated with equal utility. A Reinforcement Learning loop runs the agent against human trial data. If the agent matches human behavior, the reasoning chain receives a positive reward, which is discounted backward through the activated production rules based on time delay. This updates the probability of specific rules firing in future ambiguous states. Core assumption: the initial rule set covers the correct action space but differs in priority/selection. Evidence: Figure 4 shows "M_lit" (literal interpretation + CRL) converging with the lowest JS Divergence (highest alignment) to human data.

## Foundational Learning

- **Concept: Linear Temporal Logic (LTL) Syntax**
  - Why needed here: LTL is the "interlingua" of this framework. Understanding operators like `G` (Globally/Always), `F` (Finally/Eventually), and `U` (Until) is required to debug why certain natural language statements are rejected as "Inference Errors" during the logic-to-code transformation.
  - Quick check question: Can you explain why the formula `G(request -> response)` can be a production rule, but `F(response)` cannot?

- **Concept: Production Systems (ACT-R/Soar)**
  - Why needed here: The target architecture is a production system. You must understand "chunks" (declarative memory), "production rules" (procedural memory), and the "perceive-plan-act" cycle to understand what the LLM is trying to generate and how the CRL phase modifies rule utilities.
  - Quick check question: In a production system, what happens when two rules match the current state simultaneously? (Hint: relates to the utility calculation in Mechanism 3).

- **Concept: Prompt Chaining & Tree of Thoughts (ToT)**
  - Why needed here: The "Critic Tree" is a variant of ToT. Understanding how to structure prompts for "Critic" vs. "Revisor" roles and how to manage context window growth across tree depth is critical for implementation.
  - Quick check question: How does the context provided to a child node in the Critic Tree differ from the root node?

## Architecture Onboarding

- **Component map:** Raw NL text → Fine-tuned LLM (LTL translation) → Critic Tree (refinement) → LTL-to-production formalizer → pyactr agent → Cognitive RL trainer

- **Critical path:** The reliability of the system hinges on the CriticNL2LTL module. If the critics approve a logically invalid formula, the downstream code generation will fail or produce hallucinated variables. The paper explicitly notes that standard self-refine failed here, making the architecture of the heterogeneous critic tree the most sensitive component.

- **Design tradeoffs:**
  - Literal vs. Supervised Generation ($M_{lit}$ vs $M_{sup}$): The paper compares strictly following the text ($M_{lit}$) vs. having the LLM infer missing info ($M_{sup}$). Results favor $M_{lit}$. Guidance: Do not try to "fix" incomplete interview transcripts by letting the LLM hallucinate preconditions; strictly constrain generation to grounded text to avoid over-constraining the agent.
  - Tree Depth vs. Latency: The Critic Tree allows configurable depth. Deeper trees refine logic better but increase inference time and cost exponentially.

- **Failure signatures:**
  - Format Mismatch: Generated Python code fails to execute in pyactr. (Solution: Error-guided correction loop)
  - Inference Error: Input text requires complex temporal reasoning ("Wait until X then Y"). (Solution: Currently discarded; may require manual intervention or hybrid architecture)
  - Over-constraining: Agent freezes because LLM-generated rules require too many specific conditions to fire. (Solution: Relaxing rule specificity or using CRL to boost utilities)

- **First 3 experiments:**
  1. Unit Test CriticNL2LTL: Feed the system 10 sentences with known ground-truth LTL. Vary the "number of critics" (δ) and "max depth" (D) to find the inflection point where accuracy plateaus (paper uses δ=2, D=2)
  2. Ablation on Heterogeneity: Run the NL-to-LTL translation using homogeneous critics (e.g., all Deepseek R1) vs. heterogeneous critics. Verify the paper's claim that heterogeneity is required for performance gain over standard self-refine
  3. Static Rule Execution: Generate rules from a driving scenario transcript and run the agent in the CARLA simulator without the CRL phase. Verify that the agent acts "logically" (doesn't crash immediately) even if it doesn't perfectly mimic human timing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework effectively incorporate LLM-inferred prior knowledge to supplement incomplete natural language descriptions without causing the over-constraining issues observed in the $M_{sup}$ method?
- Basis in paper: The authors note that $M_{sup}$, which actively infers missing environmental information, performed worse than $M_{lit}$, likely because extended preconditions led to over-constraining and reduced decision branches.
- Why unresolved: The current implementation treats inferred knowledge as hard constraints, limiting the model's capacity to fit human decision distributions.
- What evidence would resolve it: A modified approach where inferred constraints are treated as "soft" probabilities or are dynamically validated against behavioral data during reinforcement learning.

### Open Question 2
- Question: Can the framework be extended to support the translation of complex temporal operators (specifically "Finally" and "Until") into executable production rules?
- Basis in paper: The paper states that LTL formulas containing operators like "F" (finally) or "U" (until) are currently marked as "Inference Error" because they lack synonyms in the production rule "IF-THEN" format.
- Why unresolved: Production rules typically represent immediate state-action mappings, making the translation of future-oriented or continuous temporal constraints structurally difficult.
- What evidence would resolve it: A successful mapping mechanism or intermediate state representation that allows temporal duration or future-state conditions to be monitored and executed within the cognitive architecture.

### Open Question 3
- Question: To what extent does the framework generalize to other critical interactive systems beyond driving, such as cooperative robotics or complex human-machine interaction tasks?
- Basis in paper: The Discussion section suggests the method could help investigate human performance in "cooperative driving or other critical interactive systems," while the experimental validation is currently limited to driving simulations.
- Why unresolved: The method relies on a specific logic-to-code transformation pipeline tailored for driving scenarios (longitudinal/lateral decisions); it is unclear if this pipeline transfers to domains with different action spaces.
- What evidence would resolve it: Successful application and evaluation of the NL2CA pipeline in a non-driving domain using natural language interviews from that domain.

## Limitations

- **Temporal complexity constraint**: The framework cannot handle "Until" and "Finally" temporal operators, filtering out potentially valid decision rules that require complex temporal reasoning
- **Domain specificity**: Validation is limited to driving scenarios, with unclear generalizability to other cognitive domains like healthcare or finance decision-making
- **Data quality dependency**: Effectiveness depends on complete, unambiguous interview transcripts, with no clear strategy for handling vague or contradictory natural language descriptions

## Confidence

- **High Confidence**: The technical feasibility of using heterogeneous critics for LTL refinement (supported by ablation showing standard self-refine fails). The utility of LTL as a filter for executable rules. The basic CRL mechanism for aligning rule utilities with human behavior.
- **Medium Confidence**: The specific parameter choices (max_depth=2, δ=2 critics, α=2e-4 in CRL). The superiority of literal interpretation (M_lit) over supervised generation (M_sup). The 78.57% success rate in driving simulation.
- **Low Confidence**: The framework's performance on domains outside driving. The robustness of heterogeneous critic approval criteria. The handling of temporal dependencies beyond simple implication patterns.

## Next Checks

1. **Ablation on Heterogeneous Critics**: Implement homogeneous critic versions (all Deepseek R1) and compare performance against the paper's heterogeneous setup. Measure revision depth, approval rates, and accuracy degradation.

2. **Temporal Dependency Testing**: Create a controlled test suite with NL statements containing "Until" and "Finally" operators. Measure how many are incorrectly filtered as "Inference Error" versus manually handling them through hybrid approaches.

3. **Cross-Domain Generalization**: Apply the framework to a non-driving domain (e.g., medical diagnosis decision rules from expert interviews). Compare rule extraction accuracy and agent performance against the driving results.