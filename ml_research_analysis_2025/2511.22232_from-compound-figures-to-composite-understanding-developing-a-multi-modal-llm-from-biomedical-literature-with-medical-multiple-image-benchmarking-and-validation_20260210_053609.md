---
ver: rpa2
title: 'From Compound Figures to Composite Understanding: Developing a Multi-Modal
  LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation'
arxiv_id: '2511.22232'
source_url: https://arxiv.org/abs/2511.22232
tags:
- medical
- compound
- image
- clinical
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of developing multi-modal large
  language models (MLLMs) capable of understanding and reasoning over multiple medical
  images, a critical need in clinical workflows where diagnoses often require synthesizing
  information across different modalities and time points. To overcome the lack of
  large-scale annotated training data for such multi-image scenarios, the authors
  propose a novel five-stage, context-aware instruction generation paradigm that leverages
  compound figures from license-permissive biomedical literature as a rich data source.
---

# From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation

## Quick Facts
- **arXiv ID**: 2511.22232
- **Source URL**: https://arxiv.org/abs/2511.22232
- **Reference count**: 0
- **Primary result**: M3LLM achieves 78.2% semantic textual similarity on multi-image VQA and 90.0% accuracy on multi-choice VQA, significantly outperforming both general and specialized medical MLLMs.

## Executive Summary
This study addresses the critical challenge of developing multi-modal large language models (MLLMs) capable of understanding and reasoning over multiple medical images, a requirement in clinical workflows where diagnoses often require synthesizing information across different modalities and time points. The authors propose a novel five-stage, context-aware instruction generation paradigm that leverages compound figures from license-permissive biomedical literature to overcome the lack of large-scale annotated training data for multi-image scenarios. By systematically decomposing complex multi-image analysis into manageable sub-tasks, the approach enables MLLMs to learn spatial, temporal, and cross-modal relationships. The resulting M3LLM model, trained on over 237,000 compound figures, demonstrates state-of-the-art performance across multi-image, single-image, text-only, and multi-choice tasks, while also showing strong generalization to real-world clinical applications.

## Method Summary
The method involves developing M3LLM through a five-stage instruction generation pipeline applied to compound figures from biomedical literature. The pipeline extracts both visual content and contextual text from compound figures, then systematically transforms them into structured instruction pairs through: inline text condensation, domain knowledge injection, visual perception enhancement via sub-image segmentation, structured context-question-answer generation, and answer-leakage prevention. The resulting dataset (PMC-MI) contains 237,137 compound figures with an average of 4.97 sub-images each. M3LLM architecture combines InternViT vision encoder, a connector module (two fully-connected layers), and QWen2.5-7B language model. Training uses AdamW optimizer with learning rate 5e-5 cosine decay, 3 epochs, and batch size 1, with bf16 precision. The PMC-MI-Bench benchmark provides comprehensive evaluation across multiple task types.

## Key Results
- M3LLM achieves 78.2% semantic textual similarity on multi-image VQA, outperforming both general-purpose and specialized medical MLLMs
- The model demonstrates 90.0% accuracy on multi-choice VQA and strong performance on single-image and text-only tasks
- M3LLM successfully generalizes to real-world clinical applications, achieving 73.9% accuracy on MIMIC longitudinal disease diagnosis
- Ablation studies show multi-image instructions improve single-image task performance through positive transfer

## Why This Works (Mechanism)

### Mechanism 1: Divide-and-Conquer Instruction Generation Decomposes Multi-Image Complexity
- The five-stage paradigm enables MLLMs to learn composite reasoning by systematically transforming complex multi-image scenarios into structured, learnable sub-tasks through inline text condensation, domain knowledge injection, visual perception enhancement, structured generation, and leakage prevention.

### Mechanism 2: Compound Figures as Proxy for Clinical Multi-Image Workflows
- License-permissive biomedical literature compound figures provide scalable training data for spatial, temporal, and cross-modal reasoning that mirrors clinical practice, as they naturally encode multi-panel relationships like CT + histopathology + postoperative imaging.

### Mechanism 3: Leakage-Prevented Context Refinement Forces Genuine Reasoning
- Stage 5 explicitly removes answer-revealing information from context, preventing models from exploiting shortcut cues and ensuring they must reason rather than pattern-match.

## Foundational Learning

- **Concept: Multi-Image Visual Question Answering**
  - Why needed here: The entire framework targets reasoning across multiple images rather than single-image classification.
  - Quick check question: Can you explain the difference between single-image VQA (analyzing one X-ray) vs. multi-image VQA (comparing pre/post treatment scans)?

- **Concept: Instruction Tuning Data Quality**
  - Why needed here: The five-stage paradigm is fundamentally a data curation methodology; understanding why answer leakage, context sufficiency, and knowledge grounding matter is prerequisite.
  - Quick check question: Why would including the correct diagnosis in a question's context harm model learning?

- **Concept: Vision-Language Alignment via Connector Modules**
  - Why needed here: M3LLM uses InternViT + connector + QWen2.5-7B; understanding how visual features map to language token space is essential for debugging multi-image fusion.
  - Quick check question: What role does the connector module play between the vision encoder and LLM?

## Architecture Onboarding

- **Component map**: Compound figures -> Five-stage pipeline -> InternViT (vision encoder) -> Connector (2 FC layers) -> QWen2.5-7B (LLM)
- **Critical path**: Data quality determines performance. Start with understanding the three-step filtering pipeline (license filtering → medical content screening via DenseNet-121 → textual quality control with word thresholds) before touching model code.
- **Design tradeoffs**: Data scale vs. quality (only 237K of 3.1M compound figures pass filters); multi-image vs. single-image focus (multi-image improves single-image via positive transfer); automated vs. manual validation (training data automated, benchmark manual).
- **Failure signatures**: Performance degrades on underrepresented modalities (ultrasound: 78.2% vs. best-in-class modalities >85%); hallucinated anatomical structures when model lacks grounding; progression prediction accuracy (45.1%) significantly lower than diagnosis (73.9%).
- **First 3 experiments**:
  1. Validate data pipeline on small sample: Run five-stage instruction generation on 100 compound figures; manually inspect Stage 5 outputs for residual leakage
  2. Ablate instruction types: Train with only single-image instructions, then add multi-image; measure delta on PMC-MI-Bench (expect ~3-4% STS gain per Table 7)
  3. Modality-specific evaluation: Test on OmniMedVQA splits; confirm performance correlates with training data modality distribution (Fig. 7)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can integrating non-imaging clinical modalities (e.g., laboratory test results, electronic health records) into the M3LLM framework significantly improve diagnostic capabilities compared to the current visual-textual approach?
- **Basis in paper**: Discussion, paragraph 3 ("integrating additional clinical modalities such as laboratory test results, patient histories, and treatment response data could further enhance its diagnostic capabilities...")
- **Why unresolved**: The current study is strictly limited to visual and textual data derived from biomedical literature; it does not process structured clinical data types found in hospital systems.
- **What evidence would resolve it**: A comparative study where M3LLM is fine-tuned or prompted with structured EHR data (labs, vitals) alongside images, showing a statistically significant increase in diagnostic accuracy or prognostic value.

### Open Question 2
- **Question**: What specific domain-specific evaluation benchmarks are required to accurately capture the nuances of clinical reasoning and decision-making that traditional text generation metrics (e.g., BLEU, ROUGE) fail to assess?
- **Basis in paper**: Discussion, paragraph 3 ("traditional metrics like accuracy, BLEU, and ROUGE-L... may not fully capture the nuances of clinical reasoning... Developing domain-specific evaluation benchmarks... will be essential")
- **Why unresolved**: Current evaluation relies on semantic textual similarity or simple accuracy, which may reward verbose or superficially correct answers without validating the underlying clinical logic or safety.
- **What evidence would resolve it**: The development and adoption of a new benchmark validated by medical professionals that specifically scores the logical derivation of a diagnosis (process) rather than just the final text output (outcome).

### Open Question 3
- **Question**: How does the performance of M3LLM degrade on rare diseases or underrepresented imaging modalities (e.g., ultrasound, fundus photography), and what specific strategies can mitigate this data bias?
- **Basis in paper**: Discussion, paragraph 2 ("performance of M3LLM relies on the diversity... of training data... e.g., the underexplored fundus photography and ultrasound imaging... Addressing this limitation will require curating more diverse datasets")
- **Why unresolved**: The paper identifies a correlation between low training volume (e.g., 2.3% Ultrasound) and lower benchmark accuracy, but does not test methods to fix it without massive data collection.
- **What evidence would resolve it**: Ablation studies utilizing few-shot learning, data augmentation, or synthetic data generation for low-resource modalities, demonstrating performance recovery without re-training on massive new datasets.

### Open Question 4
- **Question**: Can the relatively low accuracy (45.1%) in longitudinal progression prediction be improved through specific architectural modifications or training objectives designed for temporal reasoning?
- **Basis in paper**: Results, Figure 6 shows 45.1% accuracy for progression vs ~74-80% for diagnosis, and Discussion notes the "challenges inherent in progression prediction, such as the subtlety... of longitudinal changes."
- **Why unresolved**: While the model outperforms baselines, the absolute score is low, suggesting the current "divide-and-conquer" instruction paradigm may not fully capture subtle temporal changes.
- **What evidence would resolve it**: Experiments incorporating specialized temporal encoders (e.g., video-processing modules adapted for image sequences) or contrastive loss functions specifically for "before-and-after" image pairs.

## Limitations
- Data representation bias: PMC-MI dataset overrepresents CT/MRI and pathology while severely underrepresenting ultrasound and fundus photography, directly impacting model performance on these modalities
- Manual validation bottleneck: Training data relies entirely on automated pipelines without human expert review, introducing potential systematic errors in complex medical reasoning tasks
- Transfer learning assumptions: Literature compound figures may not effectively proxy clinical multi-image workflows, as clinical practice often involves dynamic patient states that differ from static literature representations

## Confidence
- **High confidence** in: technical validity of five-stage instruction generation pipeline, experimental methodology, architectural design of InternViT + connector + QWen2.5-7B
- **Medium confidence** in: claim that compound figures are "rich yet underutilized data source," assertion that divide-and-conquer strategy is superior, interpretation that performance gains reflect genuine reasoning
- **Low confidence** in: generalizability to clinical settings with different modality distributions, effectiveness of automated leakage prevention without human validation, assumption that literature-derived reasoning transfers to clinical practice

## Next Checks
1. **Cross-dataset modality validation**: Evaluate M3LLM on MIMIC-CXR and other clinical datasets stratified by modality to quantify performance degradation for underrepresented image types. Compare against models trained on modality-balanced datasets.
2. **Human expert evaluation of training data quality**: Sample 100 training examples from each stage of the instruction generation pipeline and have domain experts assess medical accuracy, context sufficiency, and reasoning quality. Focus particularly on Stage 5 leakage prevention effectiveness.
3. **Ablation of transfer assumptions**: Train M3LLM on a subset of PMC-MI where compound figure relationships are explicitly labeled as clinical vs. research-oriented. Compare performance on clinical benchmarks to quantify the impact of non-clinical training data on real-world applicability.