---
ver: rpa2
title: Language Models that Think, Chat Better
arxiv_id: '2509.20357'
source_url: https://arxiv.org/abs/2509.20357
tags:
- rlmt
- chat
- language
- grpo
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RLMT (Reinforcement Learning with Model-Rewarded
  Thinking), a method that extends the RLVR paradigm beyond verifiable domains to
  general-purpose chat tasks. RLMT trains language models to generate long chain-of-thought
  reasoning before responses using online RL against preference-based reward models,
  combining benefits of both RLHF and RLVR.
---

# Language Models that Think, Chat Better
## Quick Facts
- arXiv ID: 2509.20357
- Source URL: https://arxiv.org/abs/2509.20357
- Reference count: 37
- Introduces RLMT, extending RLVR to chat tasks, achieving 3-7 point gains on benchmarks and outperforming GPT-4o in chat

## Executive Summary
This paper introduces RLMT (Reinforcement Learning with Model-Rewarded Thinking), a method that extends the RLVR paradigm beyond verifiable domains to general-purpose chat tasks. RLMT trains language models to generate long chain-of-thought reasoning before responses using online RL against preference-based reward models, combining benefits of both RLHF and RLVR. The method demonstrates consistent improvements across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B models, with the best 8B model surpassing GPT-4o in chat and creative writing while rivaling Claude-3.7-Sonnet (Thinking). Notably, RLMT works effectively on base models without SFT warm-start, with a Llama-3.1-8B base trained on just 7K prompts outperforming Llama-3.1-8B-Instruct trained on 25M+ examples.

## Method Summary
RLMT extends RLVR to non-verifiable domains by training language models to generate chain-of-thought reasoning before responses using online reinforcement learning against preference-based reward models. The method combines the verifiable thinking approach of RLVR with the preference modeling of RLHF, allowing models to produce extended reasoning chains that can be evaluated by reward models trained on human preferences. The training process uses DPO, PPO, and GRPO algorithms across multiple model architectures, with evaluation showing consistent performance gains across chat, creative writing, and general knowledge tasks.

## Key Results
- RLMT achieves 3-7 point gains on chat benchmarks (AlpacaEval2, WildBench, ArenaHardV2) compared to standard RLHF pipelines
- The best 8B model surpasses GPT-4o in chat and creative writing while rivaling Claude-3.7-Sonnet (Thinking)
- Llama-3.1-8B base model trained on just 7K prompts outperforms Llama-3.1-8B-Instruct trained on 25M+ examples

## Why This Works (Mechanism)
RLMT works by leveraging chain-of-thought reasoning as an intermediate verifiable step that bridges the gap between non-verifiable chat tasks and preference-based reward models. The extended reasoning process provides more surface area for reward models to evaluate, allowing them to capture nuanced preferences that would be difficult to assess from final responses alone. This approach combines the verifiability benefits of RLVR with the flexibility of RLHF, enabling effective training on complex, open-ended tasks.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Uses verifiable intermediate steps to enable RL training on non-verifiable tasks - needed because traditional RLVR requires ground truth answers
- **Chain-of-Thought Reasoning**: Extended reasoning sequences that can be evaluated for correctness - needed to create verifiable intermediate steps for RLVR extension
- **Preference-based Reward Models**: Models trained to predict human preferences between responses - needed to evaluate non-verifiable outputs
- **Online Reinforcement Learning**: RL algorithms that learn while interacting with environment - needed to train models using reward signals from generated reasoning
- **Base Model Training**: Training directly from base models without supervised fine-tuning - needed to demonstrate effectiveness without warm-start requirements

## Architecture Onboarding
Component map: Reward Model -> Policy Network -> Chain-of-Thought Generator -> Final Response Generator -> Evaluation Pipeline
Critical path: Chain-of-Thought generation -> Reward model evaluation -> Policy update -> Response generation
Design tradeoffs: Longer reasoning chains provide better evaluation signals but increase computational cost and latency
Failure signatures: Over-reliance on reasoning can lead to verbose responses; reward model bias can propagate through training
First experiments:
1. Train on single-task domain with clear reasoning requirements to establish baseline effectiveness
2. Compare chain-of-thought vs direct response training to isolate reasoning contribution
3. Test with different reasoning lengths to optimize the reasoning-to-response ratio

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to Llama-3.1-8B and Qwen-2.5-7B models, limiting generalizability
- Extraordinary claims about base model superiority (7K prompts vs 25M+ examples) require independent validation
- Potential overfitting to specific reward models and evaluation benchmarks not adequately addressed

## Confidence
- **High confidence**: RLMT methodology as extension of RLVR to non-verifiable domains is sound
- **Medium confidence**: Benchmark performance improvements (3-7 points) are likely real but may vary
- **Low confidence**: Base model superiority claims should be treated skeptically until independently verified

## Next Checks
1. Independent replication of base model training results (7K prompts vs 25M+ examples) using different datasets and reward model implementations
2. Ablation studies removing chain-of-thought component to quantify its specific contribution versus other RLMT components
3. Cross-architecture validation testing RLMT on models from different families (Mistral, Gemma) and scales (3B, 70B)