---
ver: rpa2
title: How Good are Foundation Models in Step-by-Step Embodied Reasoning?
arxiv_id: '2509.15293'
source_url: https://arxiv.org/abs/2509.15293
tags:
- reasoning
- physical
- benchmark
- embodied
- bolt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FoMER, a large-scale benchmark for evaluating
  step-by-step embodied reasoning in large multimodal models (LMMs). The benchmark
  includes over 1,100 samples spanning 10 task categories across 8 robot embodiments,
  with detailed reasoning annotations.
---

# How Good are Foundation Models in Step-by-Step Embodied Reasoning?

## Quick Facts
- arXiv ID: 2509.15293
- Source URL: https://arxiv.org/abs/2509.15293
- Reference count: 40
- OpenAI o4-mini achieves highest overall reasoning accuracy (76.34%) on FoMER benchmark

## Executive Summary
This paper introduces FoMER, a large-scale benchmark for evaluating step-by-step embodied reasoning in large multimodal models (LMMs). The benchmark includes over 1,100 samples spanning 10 task categories across 8 robot embodiments, with detailed reasoning annotations. A novel evaluation framework is proposed that measures both action validity and reasoning quality using LLM-as-judge. Testing nine state-of-the-art models, the study finds significant performance gaps in embodied reasoning, especially in tasks requiring social navigation and human-robot interaction. OpenAI o4-mini achieves the highest overall reasoning accuracy (76.34%), while models like Gemini 2.5 Pro and Qwen2.5-VL-32B perform consistently well. The results highlight the need for deeper reasoning capabilities in LMMs before real-world deployment.

## Method Summary
The FoMER benchmark evaluates LMMs on step-by-step embodied reasoning using 1,112 samples from 10 datasets covering 8 robot embodiments. Models must generate chain-of-thought reasoning and final answers for three question types (MCQ, TF, open-ended) based on visual inputs (8 frames for short videos, 32 for long videos). Evaluation uses an LLM-as-judge (GPT-4o) scoring reasoning across 10 criteria including faithfulness, spatial reasoning, physical causality, and safety. Performance is measured through Reasoning Accuracy (RA) and Final Accuracy (FA), with human validation on 50 samples showing 80.93% agreement with automated judging.

## Key Results
- OpenAI o4-mini achieves highest overall reasoning accuracy at 76.34%
- Performance improves significantly with temporal visual context (8 frames vs. middle frame)
- All models struggle with social navigation and human-robot interaction tasks
- Reasoning accuracy consistently lower than final accuracy, indicating many correct answers lack proper reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing embodied decision-making into explicit step-by-step reasoning traces may reveal alignment errors and hallucinations that simple final-answer accuracy metrics hide.
- **Mechanism:** By prompting models to generate Chain-of-Thought (CoT) rationales and evaluating these trails against ground-truth logic using an LLM-as-judge, the framework distinguishes valid causal reasoning from correct-but-lucky guesses.
- **Core assumption:** Verbalized reasoning steps accurately reflect the model's internal decision path rather than post-hoc rationalization.
- **Evidence anchors:**
  - [abstract] The benchmark "disentangles perceptual grounding from action reasoning" via detailed reasoning annotations.
  - [section] Section 5 notes this helps "identify models that understand the problem but fail in execution, versus those that succeed through guessing."
  - [corpus] *DriveLMM-o1* introduces similar step-by-step evaluation for driving scenarios, reinforcing the utility of reasoning trails in physical AI.
- **Break condition:** If models learn to game the evaluator by generating plausible-sounding but logically hollow text, the correlation between reasoning accuracy and real-world competence degrades.

### Mechanism 2
- **Claim:** Performance on embodied reasoning tasks appears contingent on the density and temporal alignment of visual tokens, where access to motion flow significantly improves physical causality prediction.
- **Mechanism:** Providing video context (or uniformly sampled frames) allows the model to observe dynamics (e.g., turning a bolt) rather than just static state, enabling better inference of physical constraints and subsequent actions.
- **Core assumption:** The visual encoder has sufficient temporal resolution and capacity to map frame sequences into coherent state changes without attention dilution.
- **Evidence anchors:**
  - [abstract] The benchmark requires agents to "interpret multimodal observations" and reason about "physical constraints."
  - [section] Table 4 shows performance improves from 51.89% (middle frame) to 62.97% (8 frames) to 69.62% (whole video) for Gemini 2.5 Pro.
  - [corpus] *Nav-R1* emphasizes integrating perception and reasoning in 3D environments, supporting the need for rich visual context.
- **Break condition:** If the visual context exceeds the model's context window or if temporal aliasing occurs (frames miss key physical events), reasoning accuracy likely drops.

### Mechanism 3
- **Claim:** Automated evaluation of physical reasoning quality is feasible using LLM-as-judge architectures grounded in fine-grained, multi-dimensional rubrics (e.g., safety, spatial accuracy).
- **Mechanism:** A strong judge model (e.g., GPT-4o) compares model outputs against human-annotated ground truth across 10 specific criteria (Table 2), outputting a "Reasoning Accuracy" score that proxies for human judgment.
- **Core assumption:** The judge model possesses superior or sufficient domain knowledge to validly critique the physics and safety logic of the test model.
- **Evidence anchors:**
  - [abstract] The framework "measures both action validity and reasoning quality using LLM-as-judge."
  - [section] Table 5 demonstrates high correlation between GPT-4o and human evaluators (80.93% vs 84.47%).
  - [corpus] Corpus evidence for specific *embodied* LLM-judge mechanisms is weak beyond general trends in *DriveLMM-o1*; specific architectures for physical AI judging remain underexplored in neighbors.
- **Break condition:** If the judge model shares the same biases or hallucinations as the test model (collusion), the evaluation scores may be artificially inflated.

## Foundational Learning

### Concept: Embodied Reasoning vs. Visual QA
- **Why needed here:** Standard VQA checks for object recognition; embodied reasoning checks if an action is *physically possible* and *safe* given the visual state.
- **Quick check question:** Can the model distinguish between "the door is closed" (state) and "I can open the door without hitting the obstacle" (embodied constraint)?

### Concept: Hallucination in Physical Logic
- **Why needed here:** Models often infer correct actions using flawed logic (e.g., "withdraw bolt" because "it looks loose" rather than observing the actual unthreading).
- **Quick check question:** Does the reasoning trace cite specific visual evidence (e.g., "the fingers are spinning the bolt") or generic assumptions?

### Concept: LLM-as-Judge Reliability
- **Why needed here:** The entire evaluation relies on an automated judge; understanding its alignment with human logic is critical for interpreting results.
- **Quick check question:** If the judge scores a response 10/10 on safety, does it verify the absence of collisions, or just the presence of safety-related keywords?

## Architecture Onboarding

- **Component map:** Visual Encoder -> Large Multimodal Model (LMM) -> LLM-as-Judge
- **Critical path:** Ensuring the visual context (frames) is sufficient to resolve the physical query (e.g., distinguishing "unscrewing" from "screwing") -> Verifying the Judge prompt accurately enforces the rubric
- **Design tradeoffs:**
  - **Frame Count:** 8 frames are efficient but may miss motion dynamics; full video is accurate but computationally expensive and hits context limits
  - **Judge Model:** GPT-4o is robust but proprietary; Qwen3-32B is open but shows slight variance (Table 6)
- **Failure signatures:**
  - **High Final Accuracy / Low Reasoning Accuracy:** Model is guessing or using superficial correlations
  - **Low scores on "Social Navigation":** Model lacks training on human-robot interaction norms (Figure 7)
- **First 3 experiments:**
  1. **Visual Ablation:** Run a subset of tasks using 1 frame vs. 8 frames vs. full video to quantify the value of temporal context (baseline: Table 4)
  2. **Reasoning vs. Final Score Delta:** Identify tasks where the gap between Reasoning Accuracy and Final Accuracy is highest to locate "lucky guess" behaviors
  3. **Judge Consistency Check:** Swap GPT-4o for Qwen3-32B on a fixed set of model outputs to measure evaluation variance (baseline: Table 6)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can improved performance on social navigation and human-robot object interaction (HROI) tasks be achieved through specialized training data, or do these categories require fundamentally different reasoning architectures compared to manipulation and navigation tasks?
- **Basis in paper:** [explicit] The authors note that "all models perform poorly on humanâ€“robot object interaction (HROI) and social navigation, both of which demand deeper reasoning about social norms and conventions in order to answer precisely" (Section 5, Figure 7).
- **Why unresolved:** The paper identifies these as the hardest categories but does not investigate whether the performance gap stems from lack of training data, model architecture limitations, or the inherent complexity of social reasoning.
- **What evidence would resolve it:** Ablation studies comparing models trained on augmented social-interaction data versus models with modified architectures designed for social norm reasoning.

### Open Question 2
- **Question:** What accounts for the misalignment between reasoning accuracy and final answer accuracy observed in models like Video-R1, which achieve high reasoning scores but low action correctness?
- **Basis in paper:** [inferred] The authors state they measured reasoning accuracy separately from final correctness to "identify models that understand the problem but fail in execution, versus those that succeed through guessing or pattern exploitation" (Section 5), but do not explain the underlying causes of this disconnect.
- **Why unresolved:** The paper quantifies the gap but does not analyze whether it arises from reasoning-trail hallucination, inconsistent logic-to-action translation, or evaluation methodology artifacts.
- **What evidence would resolve it:** Fine-grained error analysis of reasoning trails from high-reasoning/low-accuracy models, paired with interventions that test whether enforcing structured output formats improves alignment.

### Open Question 3
- **Question:** To what extent does the LLM-as-judge evaluation framework introduce systematic bias when assessing reasoning quality for embodied tasks?
- **Basis in paper:** [inferred] While the authors validate their GPT-4o judge against human evaluators on 50 samples (Table 5) and show consistency with Qwen3-32B as an alternate judge (Table 6), the sample size is small and may not capture domain-specific biases in physical reasoning evaluation.
- **Why unresolved:** The paper demonstrates correlational validity but does not examine whether the judge LLM systematically favors certain reasoning styles or fails to detect subtle physical implausibilities that humans would catch.
- **What evidence would resolve it:** A large-scale adversarial evaluation where humans annotate specific reasoning errors (e.g., unsafe suggestions, physics violations) and compare detection rates between LLM judges and human annotators across all task categories.

## Limitations
- **Limited generalization:** The benchmark draws from 10 specific datasets that may not fully represent real-world embodied tasks
- **Automated evaluation reliability:** While the LLM-as-judge shows 80.93% agreement with human evaluators, this still leaves significant room for disagreement
- **Performance measurement gap:** The notable difference between Final Accuracy and Reasoning Accuracy suggests the benchmark may not fully capture models' true embodied reasoning capabilities

## Confidence
- **High confidence:** The finding that step-by-step reasoning evaluation reveals significant performance gaps in current LMMs (Section 5, Table 4) is well-supported by systematic comparisons across 9 models
- **Medium confidence:** The claim that temporal visual context (8+ frames) substantially improves reasoning accuracy is supported by evidence but may be sensitive to specific implementation details
- **Low confidence:** The generalizability of the 80.93% human-judge agreement rate beyond the specific benchmark samples remains uncertain without broader validation

## Next Checks
1. **Temporal Context Validation:** Conduct controlled experiments varying frame count (1, 4, 8, 16) on a representative subset of tasks to quantify the precise value of temporal information beyond the current 8-frame baseline
2. **Judge Consistency Stress Test:** Systematically evaluate model outputs using multiple judge models (GPT-4o, Qwen3-32B, Claude) to measure variance in reasoning scores and identify potential evaluation artifacts
3. **Cross-Dataset Transfer:** Test top-performing models on novel embodied reasoning datasets not included in FoMER to assess whether benchmark performance translates to broader physical AI capabilities