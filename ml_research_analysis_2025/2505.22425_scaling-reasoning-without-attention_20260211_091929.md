---
ver: rpa2
title: Scaling Reasoning without Attention
arxiv_id: '2505.22425'
source_url: https://arxiv.org/abs/2505.22425
tags:
- reasoning
- arxiv
- aime
- prompt
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the architectural inefficiency and lack of\
  \ structured fine-tuning in large language models for complex reasoning tasks. The\
  \ authors propose PromptCoT-Mamba, an attention-free language model built on Mamba-2\u2019\
  s state space dual (SSD) layers, which eliminates self-attention and key-value caching\
  \ for constant-time, fixed-memory inference."
---

# Scaling Reasoning without Attention

## Quick Facts
- arXiv ID: 2505.22425
- Source URL: https://arxiv.org/abs/2505.22425
- Authors: Xueliang Zhao; Wei Wu; Lingpeng Kong
- Reference count: 7
- Outperforms Transformer models on AIME 24/25 and LiveCodeBench with 3.66x higher throughput under memory constraints

## Executive Summary
This paper introduces PromptCoT-Mamba, an attention-free language model designed for complex reasoning tasks. By replacing self-attention with Mamba-2's State Space Dual (SSD) layers, the model achieves constant-time, fixed-memory inference while maintaining strong reasoning performance. The authors employ a two-phase curriculum fine-tuning strategy based on the PromptCoT synthesis paradigm, generating pedagogically structured problems via abstract concept selection and rationale-guided generation. Experiments demonstrate that PromptCoT-Mamba-7B achieves state-of-the-art results on challenging math and code reasoning benchmarks while delivering up to 3.66× higher throughput under memory-constrained settings.

## Method Summary
PromptCoT-Mamba is built on Mamba Codestral and trained through a two-phase curriculum fine-tuning approach. Phase 1 uses OpenCodeReasoning (735k) and OpenThoughts2 (1.14M) with completions from DeepSeek-R1/QwQ. Phase 2 adds PromptCoT-synthesized data (256k), OpenCodeReasoning (246k), and OpenMathReasoning (232k). The model uses SSD layers for constant-memory recurrence, replacing self-attention and key-value caching. Training employs AdamW optimizer with specific hyperparameters, DeepSpeed ZeRO-2 on 8×A100 80GB GPUs, and sequence lengths of 16,384 (Phase 1) and 20,480 (Phase 2).

## Key Results
- Achieves 35.2% on AIME 24 and 24.6% on AIME 25, surpassing Gemma3-27B by 2.6% and 0.6% respectively
- Scores 29.9% on LiveCodeBench, outperforming Gemma3-27B by 3.0%
- Delivers 3.66× higher throughput compared to s1.1-7B under 24GB GPU settings
- AIME performance drops from 35.2% to 11.7% when removing PromptCoT data, highlighting its importance

## Why This Works (Mechanism)

### Mechanism 1: Constant-Memory Recurrence via SSD Layers
The SSD layers use a recurrent update rule (H_t = a_t · H_{t-1} + Z_t) with fixed hidden state dimensions regardless of sequence length, eliminating the O(T) memory overhead of KV caching. This enables fixed-memory, constant-time inference for long reasoning chains.

### Mechanism 2: Pedagogical Scaffolding via PromptCoT Synthesis
The PromptCoT paradigm generates problems by sampling abstract concepts and constructing expert-style rationales, then conditioning problem generation on these rationales. This structured approach teaches the model to mimic expert decomposition and reasoning patterns.

### Mechanism 3: Memory-Constrained Inference Scaling
By removing the KV cache, the model's memory footprint per token doesn't scale with sequence length, allowing for higher effective batch sizes and longer contexts within hardware limits. This enables up to 65k token generation and improved throughput.

## Foundational Learning

- **State Space Models (SSMs) vs. Attention**: Understanding how SSD layers compress history into a fixed-size recurrent state is crucial for grasping the memory efficiency claims. Quick check: Does inference cost increase as sequence length grows from 1k to 50k tokens? (Answer: No)

- **Curriculum Learning**: The two-phase training pipeline (Initialization → Advanced) is essential for understanding the performance gains. Quick check: Why train on "Open Thoughts" before "PromptCoT" data? (Answer: To establish foundational reasoning before tackling hardest problems)

- **Throughput vs. Latency**: The 3.66x speedup refers to throughput (tokens processed per second), not latency (time-to-first-token). Quick check: Does the speedup mean generating answers faster for single users or serving more users simultaneously? (Answer: Throughput typically implies the latter)

## Architecture Onboarding

- **Component map**: Input Embedding (E) → Linear Projection → SSD Core → Vocab Projection
- **Critical path**: The SSD Recurrence Equation (Eq. 1 in Section 2.1) is the critical computational unit determining how history is compressed
- **Design tradeoffs**: Math specialization shows PromptCoT-Mamba-MATH excels at AIME (42.9%) but drops on LiveCodeBench (20.3% vs 29.9%). Fixed hidden state enables long context but may lose precise token-level details
- **Failure signatures**: Catastrophic forgetting if fine-tuning skips initialization phase; efficiency drop on short context with abundant memory
- **First 3 experiments**:
  1. Ablate Data Source: Train using only Phase 1 data to verify PromptCoT's contribution to AIME scores
  2. Throughput Profiling: Benchmark token generation speed against Transformer baseline at sequence lengths > 32k tokens
  3. State Capacity Test: Test model on retrieval-heavy task to verify fixed-memory state compression limits

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on the PromptCoT synthesis pipeline, which is described in a separate paper with unclear implementation details
- SSD recurrence may suffer from numerical instability or information loss over extreme sequence lengths (>50k tokens)
- Claims about generalization to non-synthetic, real-world problems are unsupported by the ablation study

## Confidence

- **High Confidence**: Architectural efficiency claims (3.66× throughput gain, constant-memory inference) are well-supported and directly measurable
- **Medium Confidence**: Reasoning performance improvements are likely real but heavily dependent on unspecified PromptCoT synthesis methodology
- **Low Confidence**: Generalization claims to ambiguously worded real-world problems lack empirical support

## Next Checks

1. **Data Pipeline Replication**: Implement the PromptCoT synthesis pipeline and generate a validation set to verify structural alignment with training data

2. **Sequence Length Stress Test**: Evaluate on retrieval-heavy task (find 5th word in 50k document) to test fixed hidden state's token-level information retention

3. **Efficiency Benchmarking**: Replicate throughput comparison by profiling token generation speed on 24GB GPU against Transformer baseline at sequence lengths > 32k tokens