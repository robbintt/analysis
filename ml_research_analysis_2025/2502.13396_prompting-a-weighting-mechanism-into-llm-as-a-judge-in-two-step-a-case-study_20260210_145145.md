---
ver: rpa2
title: 'Prompting a Weighting Mechanism into LLM-as-a-Judge in Two-Step: A Case Study'
arxiv_id: '2502.13396'
source_url: https://arxiv.org/abs/2502.13396
tags:
- response
- arxiv
- evaluation
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-step prompt engineering approach to
  improve LLM-as-a-judge performance by incorporating explicit error weighting mechanisms.
  The method addresses the problem of LLMs overemphasizing minor details while undervaluing
  critical information in evaluation tasks.
---

# Prompting a Weighting Mechanism into LLM-as-a-Judge in Two-Step: A Case Study

## Quick Facts
- arXiv ID: 2502.13396
- Source URL: https://arxiv.org/abs/2502.13396
- Reference count: 14
- A two-step prompt engineering approach achieves 6.4% average improvement in Human Alignment Rate by incorporating explicit error weighting mechanisms

## Executive Summary
This paper introduces a two-step prompt engineering approach to improve LLM-as-a-judge performance by incorporating explicit error weighting mechanisms. The method addresses the problem of LLMs overemphasizing minor details while undervaluing critical information in evaluation tasks. Through strategic prompt design that categorizes facts into critical, supporting, and trivial types, the approach achieves an average 6.4% improvement in Human Alignment Rate (HAR) compared to baseline models. Among five tested LLMs, Mixtral-8x7B Instruct achieved the highest HAR of 95.8%, while statistical significance testing confirmed consistent performance differences across models.

## Method Summary
The approach uses a two-step evaluation process. First, a baseline LLM-as-a-Judge evaluation is performed using customized GPT-4o without weighting prompts. Second, the same data is re-evaluated using different LLMs with an explicit weighting prompt that categorizes facts as Critical, Supporting, or Trivial. The evaluation uses the Databricks Documentation Evaluation Set (192 samples) containing user queries, expected retrieved context, gold standard responses, and AI-generated responses. The weighting prompt instructs LLMs to tolerate trivial omissions while penalizing missed critical facts, outputting structured JSON with semantic similarity scores, fact match ratios, and final evaluations. Five LLMs are tested: GPT-4o, GPT-4o-mini, Llama 3.1 70B/405B Instruct, and Mixtral-8x7B Instruct.

## Key Results
- Achieved average 6.4% improvement in Human Alignment Rate compared to baseline models
- Mixtral-8x7B Instruct achieved the highest HAR of 95.8%
- Statistical significance testing confirmed consistent performance differences across all five tested LLMs
- The two-step prompt approach effectively prevents LLMs from overemphasizing minor details while undervaluing critical information

## Why This Works (Mechanism)
The approach works by explicitly guiding LLMs to distinguish between critical, supporting, and trivial facts through prompt engineering. By categorizing information importance, the LLM-as-a-judge can appropriately weight evaluation criteria rather than treating all details equally. The two-step process allows for controlled comparison between baseline evaluation and the enhanced weighting mechanism. The JSON output format ensures structured, consistent evaluation results that can be systematically compared against human judgments.

## Foundational Learning
- **Human Alignment Rate (HAR)**: Measures how well AI judgments match human evaluators. Why needed: Provides quantitative metric for evaluating LLM-as-a-judge performance. Quick check: Verify HAR calculation matches paper's reported values.
- **Critical vs Supporting vs Trivial fact categorization**: Explicit prompt instructions for fact importance levels. Why needed: Enables LLMs to properly weight evaluation criteria. Quick check: Ensure fact categorization definitions are consistently applied.
- **Two-step evaluation design**: Baseline evaluation followed by weighted evaluation. Why needed: Controls for model variability while isolating the weighting mechanism effect. Quick check: Confirm both evaluations use identical input data.
- **JSON output formatting**: Structured response containing semantic_similarity, fact_match_ratio, and other metrics. Why needed: Enables systematic comparison and HAR calculation. Quick check: Validate all JSON fields are present and correctly formatted.

## Architecture Onboarding

Component Map:
Databricks Dataset (192 samples) -> Baseline LLM Evaluation -> Weighted LLM Evaluation -> HAR Calculation -> Performance Comparison

Critical Path:
Dataset loading → Prompt execution → JSON parsing → HAR threshold application → Statistical significance testing

Design Tradeoffs:
- Manual fact categorization vs automated classification: Manual ensures consistency but doesn't scale
- Fixed importance categories vs dynamic weighting: Fixed categories simplify prompts but may miss context nuances
- JSON output vs natural language: JSON enables systematic analysis but may constrain LLM's expressive evaluation

Failure Signatures:
- HAR calculation errors due to missing ground truth labels
- Inconsistent fact categorization across LLM runs
- JSON parsing failures from malformed LLM outputs
- Threshold mismatch between paper and reproduction implementation

First Experiments:
1. Load and validate the Databricks dataset structure and content
2. Execute the baseline evaluation prompt and verify JSON output format
3. Compare fact categorization consistency across multiple LLM runs on the same input

## Open Questions the Paper Calls Out
- Does the explicit error weighting mechanism generalize to Natural Language Generation (NLG) tasks outside of technical documentation?
- Can automated prompt optimization techniques replicate or exceed the performance of the manually designed weighting prompts?
- How does the subjective distinction between "Trivial" and "Supporting" facts affect inter-model consistency?

## Limitations
- The approach was tested on a single dataset and may not generalize to diverse NLG tasks
- Manual prompt design is labor-intensive and may not be optimal for all model architectures
- The subjective distinction between "Trivial" and "Supporting" facts may affect evaluation consistency

## Confidence
- HAR improvement claims: Medium
- Statistical significance results: Medium
- Generalization to other domains: Low

## Next Checks
1. Request or reconstruct the human judgment ground truth labels for the 192 samples to enable accurate HAR calculation and verification of the reported improvements.
2. Clarify the threshold/criteria used to convert final_score (0-1) to accept/reject decisions for HAR computation, as this significantly impacts reported performance metrics.
3. Document or reconstruct the "customized GPT-4o" baseline model configuration to enable fair comparison between baseline and weighting-prompt approaches using identical LLM infrastructure.