---
ver: rpa2
title: 'Bayesian Optimization with Expected Improvement: No Regret and the Choice
  of Incumbent'
arxiv_id: '2508.15674'
source_url: https://arxiv.org/abs/2508.15674
tags:
- regret
- lemma
- bound
- cumulative
- bpmi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the cumulative regret upper bound of the classic
  noisy Gaussian process expected improvement (GP-EI) algorithm, which has remained
  an open question despite its widespread use in Bayesian optimization. The study
  focuses on the Bayesian setting where the objective function is assumed to be a
  sample from a Gaussian process, and examines three commonly used incumbents: best
  posterior mean incumbent (BPMI), best sampled posterior mean incumbent (BSPMI),
  and best observation incumbent (BOI).'
---

# Bayesian Optimization with Expected Improvement: No Regret and the Choice of Incumbent

## Quick Facts
- **arXiv ID:** 2508.15674
- **Source URL:** https://arxiv.org/abs/2508.15674
- **Reference count:** 10
- **Primary result:** GP-EI with BPMI and BSPMI are no-regret algorithms with sublinear cumulative regret bounds for both squared exponential and Matérn kernels.

## Executive Summary
This paper resolves a long-standing open question by establishing the first cumulative regret upper bounds for the classic noisy Gaussian process Expected Improvement (GP-EI) algorithm. The study analyzes three incumbent selection strategies—Best Posterior Mean Incumbent (BPMI), Best Sampled Posterior Mean Incumbent (BSPMI), and Best Observation Incumbent (BOI)—and proves that BPMI and BSPMI achieve sublinear regret bounds under standard GP assumptions. The analysis reveals why BOI is considered "brittle" in practice, showing it can fail to achieve no-regret behavior under high noise conditions due to the incumbent dropping below the true optimum.

## Method Summary
The paper analyzes noisy Bayesian optimization using GP-EI with three incumbent selection strategies on standardized benchmark functions. The algorithm uses a zero-mean GP with Matérn kernel (ν=1.5), optimizes hyperparameters via MLE at each iteration, and evaluates cumulative regret over 100 trials. BPMI minimizes the posterior mean over the entire domain, BSPMI minimizes over sampled points for computational efficiency, and BOI uses the minimum observed value. The experimental setup uses functions like Branin, Hartmann 6D, and others with noise levels σ∈{0.001, 0.01, 0.1}, running N=500+10d iterations with n₀=10d initial random samples.

## Key Results
- GP-EI with BPMI achieves O(T^(3/4) log^(d+2/2)(T)) regret for SE kernels and O(T^(3ν+2d)/(4ν+2d) log^(4ν+d)/(4ν+2d)(T)) for Matérn kernels.
- BSPMI provides a computationally efficient approximation to BPMI with only a logarithmic factor degradation in bounds.
- BOI either achieves sublinear cumulative regret or exhibits fast converging noisy simple regret, explaining its practical "brittleness" under high noise.
- BPMI and BSPMI exhibit no-regret behavior in experiments, while BOI performs worse under high noise conditions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GP-EI using BPMI or BSPMI maintains the "no-regret" property in noisy settings, meaning cumulative regret grows sublinearly with iterations.
- **Mechanism:** These incumbents stabilize the "exploitation" term of the Expected Improvement (EI) acquisition function. By deriving the incumbent from the posterior mean (either globally or over sampled points), the algorithm prevents the incumbent from dropping significantly below the true optimum due to observation noise. This stability ensures EI balances exploration and exploitation effectively enough to bound instantaneous regret by the posterior standard deviation.
- **Core assumption:** The objective function is a sample from a Gaussian Process (Bayesian setting) with standard kernel assumptions (SE or Matérn) and Lipschitz continuity.
- **Evidence anchors:**
  - [abstract] "GP-EI with BPMI and BSPMI are no-regret algorithms for both squared exponential and Matérn kernels..."
  - [section 5.1] Theorem 2 establishes $R_T = \mathcal{O}(T^{3/4} \log^{(d+2)/2}(T))$ for BPMI with SE kernels.
  - [corpus] Related work in the corpus ("Improved Regret Bounds for Gaussian Process Upper Confidence Bound...") discusses regret bounds for GP-UCB, providing context for the sublinear rates established here for GP-EI.
- **Break condition:** If observation noise is non-Gaussian or the kernel assumptions are violated (e.g., non-smooth function), the posterior variance bounds used to derive the regret may fail.

### Mechanism 2
- **Claim:** GP-EI using the Best Observation Incumbent (BOI) is "brittle" and does not guarantee no-regret behavior under high noise because the incumbent can drop below the true optimum.
- **Mechanism:** BOI sets the incumbent $\xi^+_t$ to the minimum observed value $y^+_t$. Under high noise, $y^+_t$ can fall below the true minimum $f(x^*)$. When this happens, the exploitation term $\xi^+_t - \mu_{t-1}(x_t)$ becomes negative. This shifts the EI trade-off excessively toward pure exploration too quickly, preventing the convergence of cumulative regret.
- **Core assumption:** Observation noise $\epsilon \sim \mathcal{N}(0, \sigma^2)$ is significant enough that $y^+_t < f(x^*)$ occurs with non-negligible probability.
- **Evidence anchors:**
  - [section 3.3.3] Remark 8 explains that if $y^+_{n-1} - f(x^*)$ becomes small and negative, "BOI's trade-off between exploitation and exploration might be moving towards pure exploration too fast."
  - [section 5.3] Theorem 4 states GP-EI with BOI has a cumulative regret bound dependent on $n_y(T)$, which may not be sublinear.
  - [corpus] Weak direct corpus evidence for the specific "brittleness" mechanism, though general BO literature acknowledges BOI issues.
- **Break condition:** If noise is very low ($\sigma \approx 0$), $y^+_t$ approximates the true function value, and BOI behaves similarly to BPMI/BSPMI.

### Mechanism 3
- **Claim:** BSPMI provides a computationally efficient approximation to BPMI with only a slight degradation in theoretical convergence rates (a logarithmic factor).
- **Mechanism:** BPMI requires solving a global optimization problem to find $\min_{x \in C} \mu_t(x)$ at every step, which is costly. BSPMI restricts this search to the finite set of already sampled points $\{x_1, \dots, x_t\}$. Since these points are already evaluated, finding the minimum posterior mean is $O(t)$. The theoretical cost is a slightly looser regret bound (larger by $\log^{1/2}(T)$).
- **Core assumption:** The set of sampled points effectively covers the input space such that the sampled minimum is close to the global minimum of the posterior mean.
- **Evidence anchors:**
  - [section 5.2] Theorem 3 shows BSPMI bounds are looser by a $\log^{1/2}(T)$ factor compared to BPMI (Theorem 2).
  - [section 5.4] "BSPMI incurs a slightly looser cumulative regret upper bound... but it only involves evaluating the posterior mean at previously sampled points, making it significantly more efficient."
  - [corpus] No specific corpus papers analyzing the BPMI vs. BSPMI trade-off were found.
- **Break condition:** In very high-dimensional spaces where random sampling covers the space poorly, the BSPMI incumbent might significantly lag behind the true BPMI, potentially widening the gap between theoretical and empirical performance.

## Foundational Learning

- **Concept: Gaussian Process (GP) Surrogates**
  - **Why needed here:** The entire theoretical analysis relies on the GP framework to model the objective function and quantify uncertainty (posterior variance $\sigma_t(x)$), which directly appears in the regret bounds.
  - **Quick check question:** How does the posterior variance change as you add more observations near a specific point $x$?

- **Concept: Cumulative vs. Simple Regret**
  - **Why needed here:** The paper distinguishes between algorithms that are good for "online" performance (cumulative regret) vs. finding the final best solution (simple regret). The "no-regret" property specifically refers to cumulative regret $R_T/T \to 0$.
  - **Quick check question:** If an algorithm has sublinear cumulative regret, does it guarantee that the simple regret converges to zero?

- **Concept: Expected Improvement (EI) Acquisition Function**
  - **Why needed here:** The EI function $\mathbb{E}[\max(\xi^+ - f(x), 0)]$ drives the sampling. The paper analyzes how the choice of $\xi^+$ (the incumbent) distorts the exploration/exploitation balance of this function.
  - **Quick check question:** In the standard EI formula, what happens to the acquisition value if the posterior variance $\sigma_t(x)$ goes to zero but the mean $\mu_t(x)$ is worse than the incumbent?

## Architecture Onboarding

- **Component map:**
  Gaussian Process Model -> Incumbent Tracker -> Acquisition Optimizer -> Regret Monitor

- **Critical path:**
  1.  **Initialization:** Sample $T_0$ points to fit initial GP.
  2.  **Incumbent Selection:** Calculate $\xi^+$ using the chosen strategy (BPMI/BSPMI/BOI).
  3.  **Optimization:** Maximize EI to find $x_t$.
  4.  **Update:** Observe $y_t$, update GP and incumbent history.

- **Design tradeoffs:**
  - **BPMI:** Tightest regret bounds vs. High computational overhead (requires solving a global optimization sub-problem for the incumbent at every step).
  - **BSPMI:** Reasonable regret bounds vs. Low overhead (cheap lookup). Recommended default for noisy environments.
  - **BOI:** Simplest implementation vs. Unreliable under noise.

- **Failure signatures:**
  - **Stagnation under noise (BOI):** If $R_t/t$ fails to decrease or begins increasing in later iterations, check if $y^+ < f(x^*)$ (requires oracle).
  - **Computational Bottleneck (BPMI):** If iteration time scales poorly with dimension $d$, the global optimization for the incumbent is failing or too expensive.
  - **Corpus/Code Mismatch:** Implementations claiming "EI" often default to BOI (best observed value); verify the incumbent logic matches the intended theoretical behavior.

- **First 3 experiments:**
  1.  **Baseline Comparison:** Run GP-EI with all three incumbents (BPMI, BSPMI, BOI) on a synthetic function (e.g., Branin) with noise $\sigma=0.1$. Plot $R_t/t$ to verify BPMI/BSPMI convergence vs. BOI divergence.
  2.  **Noise Sensitivity:** Fix the function (e.g., Hartmann 6D) and vary noise $\sigma \in \{0.001, 0.01, 0.1\}$. Identify the noise threshold where BOI performance degrades significantly compared to BSPMI.
  3.  **Timing Analysis:** Measure wall-clock time per iteration for BPMI vs. BSPMI on increasing dimensions (e.g., $d \in \{2, 4, 6, 10\}$) to quantify the computational cost of the BPMI incumbent optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the cumulative regret upper bounds for GP-EI be extended to the frequentist setting?
- Basis in paper: [explicit] Section 5.4 states that extending results to the frequentist setting is "challenging" and notes that standard frequentist confidence intervals would result in "much larger" constants that fail to produce sublinear bounds.
- Why unresolved: The analysis relies on Bayesian confidence intervals valid at specific points $t$, whereas frequentist settings require uniform bounds over $t$ which introduce larger $\beta_t$ terms that break the regret summability.
- What evidence would resolve it: A proof deriving sublinear regret bounds using frequentist assumptions, or a formal demonstration that such bounds are impossible for the classic GP-EI algorithm.

### Open Question 2
- Question: Does the exploration-exploitation trade-off for GP-EI with BOI decay too rapidly to guarantee no-regret behavior?
- Basis in paper: [explicit] Remark 8 offers a conjecture that the derivative ratio of EI with BOI decreases at rate $O(\sigma_{t-1}(x_t))$, potentially moving "towards pure exploration too fast to attain a sublinear cumulative regret."
- Why unresolved: The paper proves BOI is "brittle" by bounding it via a disjunction (either sublinear regret or fast simple regret) rather than proving a unified bound, leaving the specific mechanism of failure as a conjecture.
- What evidence would resolve it: A formal proof establishing a lower bound on the cumulative regret for BOI scenarios where $y^+_{t-1} - f(x^*) < 0$.

### Open Question 3
- Question: Are the derived regret bounds for BPMI and BSPMI tight, or can they be improved to match the rates of GP-UCB?
- Basis in paper: [explicit] Remark 12 notes that the bounds for GP-EI with BPMI are "larger" than those for GP-UCB and GP-TS, and Remark 11 suggests adding parameters to achieve optimal rates.
- Why unresolved: The gap suggests the rates may be a property of the EI function's specific trade-off, but it remains possible that the analytical approach introduced in the paper could be tightened further.
- What evidence would resolve it: Deriving matching lower bounds for GP-EI or finding a proof technique that reduces the upper bound to the order of $O(\sqrt{T\gamma_T})$ without modifying the acquisition function.

## Limitations
- The theoretical analysis assumes a Bayesian setting where the objective function is a sample from a GP, which may not hold in practice.
- The regret bounds depend on specific kernel assumptions (SE or Matérn) and Lipschitz continuity.
- The analysis focuses on cumulative regret, which may not align with all practical optimization goals.

## Confidence
- **High confidence** in the theoretical regret bounds for BPMI and BSPMI under stated GP assumptions (rigorously proven results).
- **Medium confidence** in the explanation of BOI's "brittleness" mechanism (limited direct corpus support, though well-documented in BO literature).
- **High confidence** in the practical guidance favoring BSPMI over BPMI (clear trade-off between theoretical guarantees and computational feasibility).

## Next Checks
1. Implement and run GP-EI with all three incumbents (BPMI, BSPMI, BOI) on a standard benchmark (e.g., Branin) with significant noise (σ=0.1) to verify that BPMI/BSPMI converge to no-regret while BOI fails to converge.

2. Conduct a sensitivity analysis by varying the noise level (σ∈{0.001, 0.01, 0.1}) on a higher-dimensional function (e.g., Hartmann 6D) to identify the noise threshold where BOI performance degrades significantly.

3. Measure the computational cost per iteration for BPMI vs. BSPMI across increasing dimensions (d∈{2, 4, 6, 10}) to quantify the practical impact of the BPMI incumbent optimization.