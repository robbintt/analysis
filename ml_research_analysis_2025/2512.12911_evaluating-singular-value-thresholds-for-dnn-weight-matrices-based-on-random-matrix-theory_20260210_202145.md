---
ver: rpa2
title: Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random
  Matrix Theory
arxiv_id: '2512.12911'
source_url: https://arxiv.org/abs/2512.12911
tags:
- singular
- values
- weight
- distribution
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates thresholds for removing singular values from
  singular value decomposition-based low-rank approximations of deep neural network
  (DNN) weight matrices using random matrix theory (RMT). The authors model each weight
  matrix as the sum of signal and noise matrices, and propose an evaluation metric
  based on the cosine similarity between singular vectors of the signal and original
  weight matrices.
---

# Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory

## Quick Facts
- arXiv ID: 2512.12911
- Source URL: https://arxiv.org/abs/2512.12911
- Reference count: 26
- Primary result: Proposed metric Ave_w(ϕ) strongly correlates with test accuracy (0.852-0.976) for evaluating singular value thresholds in DNN weight matrix compression

## Executive Summary
This paper evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices using random matrix theory. The authors model each weight matrix as the sum of signal and noise matrices, and propose an evaluation metric based on the cosine similarity between singular vectors of the signal and original weight matrices. Numerical experiments compare two threshold estimation methods (BEMA and Gaussian broadening) across three models (3-layer MLP, LeNet, and AlexNet) on MNIST and CIFAR-10 datasets. The proposed metric shows strong correlation with test accuracy, indicating its effectiveness in evaluating singular value thresholds.

## Method Summary
The method involves extracting trained weight matrices from DNN layers, computing their singular value decompositions, and estimating the noise scale parameter σ using either BEMA or Gaussian broadening. A threshold γ₊ is computed using Tracy-Widom correction, and the number of signal components ŝ is determined. The weighted average cosine similarity Ave_w(ϕ) between low-rank and original singular vectors is calculated as a quality metric. The process is repeated across different threshold values to evaluate the optimal rank for low-rank approximation.

## Key Results
- Proposed metric Ave_w(ϕ) correlates strongly with test accuracy (0.852-0.976) across models and datasets
- BEMA and Gaussian broadening yield similar threshold estimates and low-rank approximations
- Both methods perform comparably on MNIST and CIFAR-10 datasets
- The approach effectively distinguishes signal from noise singular values in DNN weight matrices

## Why This Works (Mechanism)

### Mechanism 1
DNN weight matrices can be decomposed into signal and noise components, where noise singular values follow the Marchenko-Pastur distribution. Under RMT assumptions, if W_noise entries are i.i.d. with zero mean and variance σ², the singular values converge to the MP distribution as dimensions grow. Singular values within MP support are treated as noise; outliers are signal. The core assumption is that W_noise entries are i.i.d., zero-mean, finite variance—a simplification that may not hold for all optimizers or architectures.

### Mechanism 2
Weighted average cosine similarity Ave_w(ϕ) between low-rank and original singular vectors correlates with test accuracy and can evaluate threshold quality. From Benaych-Georges and Nadakuditi (2012), for spiked random matrix models, the squared cosine similarity ϕ_i = |⟨ũ_i, u_i⟩|² converges almost surely to a deterministic function of θ_i and σ. Aggregate via Ave_w(ϕ) = Σϕ_i(γ_i - γ₊) / Σ(γ_i - γ₊). Higher Ave_w(ϕ) indicates W_LR better approximates W_signal. The core assumption is that theoretical convergence holds for finite DNN weight matrices.

### Mechanism 3
BEMA and Gaussian broadening yield comparable thresholds and low-rank approximations for DNN weight matrices. Both methods estimate the MP scale parameter σ. BEMA uses bulk eigenvalue matching with Tracy-Widom correction; Gaussian broadening smooths empirical density and fits via least squares. Despite different σ estimates, resulting Ave_w(ϕ) and ŝ are similar. The core assumption is that both estimation methods are unbiased or equally biased for DNN weight spectra.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: The entire method depends on decomposing weight matrices into singular values/vectors. Without SVD literacy, the signal-noise separation and cosine similarity metric are opaque.
  - Quick check question: Given a matrix W ∈ R^(n×m), can you write its SVD and explain what singular values represent geometrically?

- **Concept: Marchenko-Pastur Distribution**
  - Why needed here: This RMT result provides the theoretical baseline for "noise" singular values. Understanding its parameters (σ, q) and support is essential for threshold estimation.
  - Quick check question: If W_noise has i.i.d. entries with variance σ² and aspect ratio q = m/n, what is the theoretical upper bound of its singular value support?

- **Concept: Spiked Random Matrix Model**
  - Why needed here: The signal-plus-noise model is a spiked model; Proposition 3.1 derives from spiked model theory. This explains why only some singular values "pop out" of the MP bulk.
  - Quick check question: In a spiked model, what happens to the singular vectors associated with eigenvalues just above the MP edge as the spike strength decreases toward the edge?

## Architecture Onboarding

- **Component map:** Input -> SVD module -> σ estimator (BEMA/GB) -> Threshold γ₊ -> Rank selector ŝ -> Metric computer -> Output (Ave_w(ϕ), W_LR)
- **Critical path:** 1. Extract weight matrix W from a trained layer. 2. Compute SVD of W. 3. Estimate σ using BEMA or Gaussian broadening. 4. Compute γ₊ via Equation (3). 5. Determine ŝ and construct W_LR. 6. Compute Ave_w(ϕ) to assess threshold quality. 7. Optionally deploy W_LR and measure test accuracy.
- **Design tradeoffs:** BEMA vs. Gaussian broadening: BEMA is algorithmic and faster; Gaussian broadening may be more robust to non-ideal spectra but requires nonlinear least-squares fitting. β (Tracy-Widom percentile): Lower β is more conservative (fewer signal components kept); higher β risks overfitting noise as signal. α in BEMA: Controls bulk fraction used for σ estimation; too small α includes noise/outliers, biasing σ.
- **Failure signatures:** MP fit visually poor (histogram diverges from MP curve): Suggests noise assumption violated; consider alternative models or skip RMT-based compression for that layer. Ave_w(ϕ) near 0.5 or erratic across layers: Threshold may be too aggressive or σ poorly estimated. Test accuracy drops sharply after compression: Likely over-pruning; increase β or re-evaluate layer-wise application.
- **First 3 experiments:** 1. Reproduce Figure 2 for a single layer (e.g., MLP FC2 on MNIST): Compute Ave_w(ϕ) vs. ŝ, compare BEMA and GB thresholds, correlate with test accuracy. 2. Sensitivity to batch size: Train MLP on MNIST with batch sizes 32, 64, 128, 256; compute Ave_w(ϕ) and test accuracy per batch size. 3. Cross-architecture test: Apply the method to a transformer attention weight matrix (not in paper). Assess whether MP fit holds and Ave_w(ϕ) correlates with accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do optimization algorithms other than SGD (e.g., Adam or RMSprop) affect the singular value distribution and the validity of the proposed threshold evaluation metric?
- Basis in paper: The conclusion states, "In future work, weight matrix $W$ optimized by methods other than SGD will be examined."
- Why unresolved: The current study and the underlying theoretical framework (specifically the modeling of $W_{noise}$) are derived exclusively using Stochastic Gradient Descent (SGD).

### Open Question 2
- Question: Does the reduced fit of the singular value distribution to the Marchenko-Pastur (MP) distribution on complex datasets (like CIFAR-10) undermine the reliability of the proposed thresholds?
- Basis in paper: The paper notes that for CIFAR-10, "the singular-value distribution fits the MP distribution less well" and the estimated number of signal components $\hat{s}$ tends to be larger.
- Why unresolved: The proposed metric relies on distinguishing noise via the MP distribution support; a poor fit suggests the noise model may be misspecified for harder tasks.

### Open Question 3
- Question: To what extent does the assumption that noise entries in the weight matrix are independent and identically distributed (i.i.d.) hold given the correlations introduced by SGD?
- Basis in paper: The paper defines $W_{noise}$ with i.i.d. assumptions but cites Staats et al. regarding the behavior of SGD, implying that real-world training dynamics might violate the i.i.d. requirement.

## Limitations
- The signal-plus-noise model with i.i.d. noise entries is not rigorously validated across diverse architectures and optimizers
- Marchenko-Pastur fit deteriorates on CIFAR-10 compared to MNIST, suggesting framework limitations
- Correlation between Ave_w(ϕ) and test accuracy is only validated within this study

## Confidence

- **High Confidence:** The proposed metric Ave_w(φ̂) correlates with test accuracy within the studied models and datasets
- **Medium Confidence:** BEMA and Gaussian broadening yield comparable thresholds and low-rank approximations for the tested DNN weight matrices
- **Low Confidence:** The signal-plus-noise model (W = W_signal + W_noise with i.i.d. noise) accurately describes DNN weight matrices trained with SGD

## Next Checks
1. Apply the RMT-based threshold evaluation to a transformer model (e.g., BERT) attention weight matrices. Assess whether the Marchenko-Pastur distribution fits and whether Ave_w(φ̂) correlates with accuracy, testing generalization beyond CNNs and MLPs.
2. Systematically vary the Tracy-Widom percentile β and Gaussian broadening window width a. Measure their impact on threshold γ₊, rank ŝ, and Ave_w(φ̂) to determine robustness and optimal settings.
3. Train MLPs on MNIST using different optimizers (Adam, SGD with momentum) and batch sizes (32, 64, 128, 256). Analyze the singular value distributions and Ave_w(φ̂) to test the i.i.d. noise assumption and its dependence on training dynamics.