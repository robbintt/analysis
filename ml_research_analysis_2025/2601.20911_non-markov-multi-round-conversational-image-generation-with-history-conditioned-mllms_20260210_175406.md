---
ver: rpa2
title: Non-Markov Multi-Round Conversational Image Generation with History-Conditioned
  MLLMs
arxiv_id: '2601.20911'
source_url: https://arxiv.org/abs/2601.20911
tags:
- image
- editing
- personalization
- generation
- multi-round
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-round conversational
  image generation where models must maintain long-range consistency across multiple
  turns. Existing benchmarks and methods typically rely on Markov assumptions where
  each turn depends only on the most recent image, leading to shortcut solutions that
  ignore long-range context.
---

# Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs

## Quick Facts
- arXiv ID: 2601.20911
- Source URL: https://arxiv.org/abs/2601.20911
- Reference count: 40
- This paper introduces rollback-style editing data construction and name-based personalization to address multi-round consistency in conversational image generation.

## Executive Summary
This work tackles the challenge of maintaining long-range consistency across multiple conversational turns in image generation. The authors identify that existing methods relying on Markov assumptions (where each turn depends only on the most recent image) fail on non-Markov tasks like rollback/undo operations and name-based references to earlier entities. They propose a comprehensive solution including rollback-style data construction, name-based personalization, token-level caching to prevent identity drift, and a multi-stage fine-tuning curriculum. The resulting model demonstrates substantial improvements in multi-round consistency while maintaining strong single-round editing and personalization performance.

## Method Summary
The authors formalize non-Markov multi-round image generation as requiring conditioning on full history rather than just the most recent image. They introduce rollback-style editing data by creating additional edits from earlier states in Markov chains and adding rollback instructions. For personalization, they construct a name-based dataset from video clips with face-centric supervision. A history-conditioned training framework uses token-level caching to prevent reconstruction errors from accumulating across rounds. The model employs a SEED-X architecture with Qwen-VL encoder, LLaMA backbone with LoRA adapters, and a DiT detokenizer. Multi-stage fine-tuning curriculum progresses from identity preservation to editable personalization with paired-identity supervision.

## Key Results
- Achieves substantial improvements in multi-round consistency through rollback-style data construction and token-level caching
- Demonstrates strong single-round editing and personalization performance while maintaining multi-round consistency
- Shows ArcFace improvement from 0.114 (baseline) to 0.597 (Stage 3) with comparable CLIP scores in personalization tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token-level caching prevents compounding reconstruction errors that cause identity drift across multi-round interactions.
- **Mechanism:** Instead of re-encoding generated images back to tokens at each round, the model caches the raw predicted image tokens directly and reuses them as history context for subsequent turns.
- **Core assumption:** The detokenizer's reconstruction errors are non-negligible and compound multiplicatively over turns.
- **Evidence anchors:** [abstract] "history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift"; [Section IV-A3] "small reconstruction errors accumulate"; [corpus] Weak/missing.
- **Break condition:** If the detokenizer achieves near-perfect reconstruction (PSNR >> current ~14.7dB), caching benefits diminish.

### Mechanism 2
- **Claim:** Rollback-style data construction forces models to retrieve earlier visual states, breaking Markov shortcuts.
- **Mechanism:** Starting from Markov editing chains, additional edits are created from earlier states, then rollback instructions require the model to select the correct historical base rather than the most recent image.
- **Core assumption:** Models trained only on Markov chains learn a shortcut of conditioning primarily on the most recent image, which generalizes poorly to non-Markov queries.
- **Evidence anchors:** [abstract] "rollback-style editing data construction that forces retrieval of earlier visual states"; [Section III-B] "correctness requires using the full history"; [corpus] Partial.
- **Break condition:** If the model has sufficient capacity to memorize all visual states in context without explicit retrieval, rollback supervision may be unnecessary.

### Mechanism 3
- **Claim:** Multi-stage fine-tuning curriculum enables identity preservation while maintaining editability.
- **Mechanism:** Three progressive stages: (1) identity-first warm-up prioritizes face preservation over prompt compliance; (2) region-focused conditioning on masked faces teaches selective preservation; (3) paired-identity supervision with same-person variations enables controlled edits while maintaining identity.
- **Core assumption:** Identity preservation and prompt-following are competing objectives that require staged conflict resolution, not joint optimization.
- **Evidence anchors:** [abstract] "multi-stage fine-tuning curriculum for editable personalization"; [Section IV-C] "Stage-1 prevents identity collapse; Stage-2 adds ability of prompt following"; [Fig. 7] Shows ArcFace improvement from 0.114 to 0.597.
- **Break condition:** If a unified loss weighting scheme can balance identity and editability without staging, curriculum complexity may be avoidable.

## Foundational Learning

- **Concept: Markov vs. Non-Markov Generation**
  - **Why needed here:** Understanding that p(It+1|Tt+1, It) (Markov) vs. p(It+1|Tt+1, Ht) (non-Markov) fundamentally changes the model's context requirements and training data design.
  - **Quick check question:** Can you explain why a model that performs well on sequential edits (A1→A2→A3) might fail on "undo the last edit"?

- **Concept: Visual Tokenization in MLLMs**
  - **Why needed here:** The entire framework relies on images being represented as discrete/continuous tokens that the LLM can process autoregressively; understanding the encoder-detokenizer interface is critical for debugging.
  - **Quick check question:** Why does the SEED-X interface use 64 learnable query tokens for image generation, and how does this constraint affect multi-image supervision?

- **Concept: Identity-Editability Trade-off**
  - **Why needed here:** Personalization requires balancing faithful identity preservation against following instructions that may change appearance; the multi-stage curriculum is designed to navigate this tension.
  - **Quick check question:** What happens if you skip Stage 1 and train directly on paired-identity data?

## Architecture Onboarding

- **Component map:** Qwen-VL Encoder -> SEED-X MLLM Backbone -> DiT Detokenizer -> Image Output
- **Critical path:**
  1. Training: Construct non-Markov datasets → Train DiT detokenizer on reconstruction → Multi-stage LLM fine-tuning with history prefixes
  2. Inference: Concatenate full history → Generate image tokens → Cache tokens (not re-encode) → Detokenize to pixels
- **Design tradeoffs:**
  - Final-round supervision only: Prevents multi-image supervision conflicts under SEED-X interface but may underutilize intermediate signal
  - Face-centric vs. full-body personalization: Face supervision is cleaner from videos; full-body requires synthetic diffusion supervision with lower fidelity
  - DiT vs. SDXL detokenizer: DiT improves reconstruction but requires separate fine-tuning on human images for face fidelity
- **Failure signatures:**
  - Markov shortcut: Model ignores rollback instructions, edits from most recent image regardless of prompt
  - Identity drift: Faces become unrecognizable after 3+ rounds without token caching
  - Identity collapse: Model generates generic faces matching prompt semantics instead of conditioned identity (ArcFace < 0.2)
- **First 3 experiments:**
  1. **Token caching ablation:** Run 5-round conversation with and without caching; measure ArcFace degradation curve
  2. **Rollback generalization test:** Train on 2-step rollback data, evaluate on 3-step rollback to verify transfer
  3. **Stage-wise personalization:** Train Stage 1, 1+2, 1+2+3 separately and plot identity (ArcFace) vs. editability (CLIP) frontier

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop standardized benchmarks with quantitative metrics for evaluating non-Markov multi-round image generation?
- **Basis in paper:** [explicit] The authors state "the lack of standardized benchmarks for personalization and non-Markov multi-round generation makes evaluation largely qualitative and human-driven; developing shared benchmarks would greatly benefit the field."
- **Why unresolved:** Current evaluation relies primarily on visual inspection and small-scale human studies, making cross-paper comparison difficult and progress hard to measure objectively.
- **What evidence would resolve it:** Creation of a benchmark dataset with ground-truth annotations for rollback accuracy, identity consistency across rounds, and long-range reference resolution, paired with automated evaluation metrics validated against human judgments.

### Open Question 2
- **Question:** Can the name-based personalization framework be extended effectively to full-body and multi-object scenarios while maintaining identity fidelity?
- **Basis in paper:** [explicit] The conclusion notes "our current name-based dataset primarily uses face-centric supervision for later rounds; extending this to full-body and multi-object personalization remains an open challenge." Additionally, Section V-B3 reports weaker face fidelity in full-body results.
- **Why unresolved:** Full-body supervision is synthesized via diffusion models with imperfect identity preservation, and the full-body dataset is substantially smaller (24K vs 92K face-centric samples), creating data quality and quantity bottlenecks.
- **What evidence would resolve it:** Demonstrating comparable identity preservation metrics (e.g., ArcFace similarity) between face-centric and full-body personalization on a held-out test set, or showing improved fidelity with larger/more diverse full-body training data.

### Open Question 3
- **Question:** How can conversational image generation handle richer long-range dependencies beyond rollback and name-based references, such as relational references and compositional memory?
- **Basis in paper:** [explicit] The authors state "real conversations involve even richer long-range dependencies (e.g., relational references and compositional memory), which we view as promising directions for future conversational image generation research."
- **Why unresolved:** Current non-Markov formulation handles state selection (rollback) and symbolic-to-visual binding (names), but does not address references requiring reasoning about relationships between entities or maintaining compositional scene structure across turns.
- **What evidence would resolve it:** Extending the dataset construction pipeline to include relational prompts (e.g., "move the person standing behind Jasper to the front") and demonstrating successful generation on a benchmark testing compositional consistency across 5+ rounds.

### Open Question 4
- **Question:** Can the SEED-X style generation interface be modified to enable multi-image supervision within a single training sequence, rather than requiring prefix-based splitting?
- **Basis in paper:** [inferred] Section IV-A2 describes a limitation: "if we attempt to predict multiple images sequentially in one forward pass, later image predictions are not naturally conditioned on the earlier predicted image tokens," requiring a workaround that supervises only the final-round image.
- **Why unresolved:** The fixed learnable query tokens decouple later predictions from earlier visual states, potentially weakening the training signal for true multi-round reasoning where intermediate outputs matter.
- **What evidence would resolve it:** Proposing an architecture modification that conditions later image predictions on prior generated tokens within the same sequence, and showing improved multi-round consistency metrics compared to the prefix-splitting approach.

## Limitations

- The effectiveness of token-level caching depends heavily on the quality of the detokenizer's reconstruction, with reported PSNR of only 14.7dB for human images
- The three-stage personalization curriculum is presented as necessary but lacks ablation studies showing what happens if stages are skipped or reordered
- The claim of "substantial improvements" in multi-round consistency relies heavily on qualitative human evaluation without clear baselines for comparison in the same evaluation setup

## Confidence

- **High confidence:** The core problem formulation (non-Markov multi-round generation) is well-defined and the proposed solutions (token caching, rollback data, multi-stage curriculum) are technically sound approaches to the identified challenges.
- **Medium confidence:** The quantitative improvements in ArcFace similarity and CLIP scores are measured, but the human evaluation methodology for multi-round consistency isn't fully detailed, making it difficult to assess the practical significance of the gains.
- **Low confidence:** The claim that their approach "substantially improves" multi-round consistency relies heavily on qualitative human evaluation without clear baselines for comparison in the same evaluation setup.

## Next Checks

1. **Markov shortcut validation:** Train a baseline SEED-X model on only Markov editing chains and evaluate its performance on rollback instructions. Compare success rates with and without rollback supervision to quantify the actual benefit of the non-Markov data construction.

2. **Token caching ablation study:** Run the same multi-round conversation (5+ turns) with identical prompts using the proposed model both with and without token-level caching. Measure ArcFace degradation, PSNR, and identity preservation metrics across rounds to quantify the accumulation of reconstruction errors.

3. **Curriculum stage ablation:** Train personalization models with only Stage 1, Stages 1+2, and all three stages separately. Plot the Pareto frontier of identity preservation (ArcFace) versus prompt compliance (CLIP) to determine if staging is truly necessary or if simpler joint optimization could achieve similar results.