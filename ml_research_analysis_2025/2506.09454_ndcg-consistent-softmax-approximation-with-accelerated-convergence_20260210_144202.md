---
ver: rpa2
title: NDCG-Consistent Softmax Approximation with Accelerated Convergence
arxiv_id: '2506.09454'
source_url: https://arxiv.org/abs/2506.09454
tags:
- loss
- ranking
- chen
- learning
- softmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel softmax approximation framework\
  \ for ranking tasks in extreme similarity learning. The authors propose two ranking-generalizable\
  \ losses (RG2 and RG\xD7) derived from Taylor expansions of the softmax loss, which\
  \ serve as efficient alternatives to the computationally expensive softmax and its\
  \ sampled variant."
---

# NDCG-Consistent Softmax Approximation with Accelerated Convergence

## Quick Facts
- arXiv ID: 2506.09454
- Source URL: https://arxiv.org/abs/2506.09454
- Reference count: 20
- This paper introduces RG² and RGˣ losses that accelerate convergence while maintaining NDCG consistency

## Executive Summary
This paper introduces a novel softmax approximation framework for ranking tasks in extreme similarity learning. The authors propose two ranking-generalizable losses (RG² and RGˣ) derived from Taylor expansions of the softmax loss, which serve as efficient alternatives to the computationally expensive softmax and its sampled variant. These squared-form losses maintain consistency with ranking metrics like NDCG while enabling closed-form solutions through Alternating Least Squares optimization. The theoretical analysis establishes Bayes-consistency with DCG and provides generalization bounds, while empirical evaluations demonstrate comparable or superior ranking performance with significantly accelerated convergence.

## Method Summary
The method introduces RG² and RGˣ losses derived from Taylor expansions of softmax loss at the zero vector. These squared-form losses can be optimized via Alternating Least Squares (ALS), yielding closed-form solutions that enable linear convergence. The losses are shown to be Bayes-consistent with DCG, ensuring asymptotic optimization of the target ranking metric. The framework operates on binary implicit feedback data using a matrix factorization backbone, with weights computed based on positive and negative interaction counts.

## Key Results
- RG² and RGˣ achieve comparable or superior ranking performance to softmax loss
- ALS-based optimization converges 3-5x faster than SGD-based methods
- Theoretical guarantees of Bayes-consistency with DCG are established
- Closed-form solutions enable linear convergence versus SGD's sublinear convergence

## Why This Works (Mechanism)

### Mechanism 1
Taylor expansion of Softmax Loss at the zero vector yields a squared-form approximation that preserves ranking alignment while enabling closed-form optimization. The expansion at o₀ = 0 simplifies gradient information (probabilities become uniform = 1/N), removing nonlinear exponentiation. The second-order term produces a quadratic form with analytically derived weights w_x,y = |I_x| for positives and w_x,y = |I_x|·(n+1)/N for negatives. Core assumptions: optimal solution lies near zero due to regularization; single-click behavior holds for upper bound guarantee. Break condition: logits diverge significantly from zero during training.

### Mechanism 2
RG² and RGˣ are Bayes-consistent with DCG, ensuring minimizing these surrogates asymptotically minimizes the target ranking metric. The losses are reformulated as Bregman divergences with convex generating functions ϕ. Link functions g² and gˣ are inverse order-preserving, preserving relative ordering of scores required for DCG optimization. Core assumptions: uniform weighting across samples; positive scores strictly exceed negative scores at convergence. Break condition: non-uniform weights applied may break inverse order-preservation.

### Mechanism 3
Alternating Least Squares optimization achieves linear convergence (O(ρᵀ), ρ < 1) versus SGD's sublinear convergence (O(1/√T)), accelerating practical training. Squared-form losses yield closed-form solutions for each matrix when the other is fixed. Each ALS iteration solves a linear system exactly, exploiting marginal convexity. Core assumptions: marginal strong convexity and smoothness hold; Hessian systems are well-conditioned. Break condition: embedding dimension K approaches scale of users/items, making matrix inversion prohibitive.

## Foundational Learning

- **Taylor Series Expansion**: Core technique for approximating Softmax—understanding how functions are locally approximated by polynomials is essential to grasp why o₀ = 0 simplifies the problem. Quick check: Why does expanding at o₀ = 0 yield uniform probability 1/N in the Softmax gradient?

- **Bayes-Consistency for Ranking**: Distinguishes RG losses from heuristic losses—the paper proves RG losses asymptotically optimize DCG, not just correlate with it. Quick check: If a surrogate loss is not Bayes-consistent with DCG, what could happen at the optimal solution of the surrogate?

- **Alternating Least Squares (ALS)**: The efficiency gains come entirely from ALS enabling closed-form updates—understanding block coordinate descent is critical. Quick check: Why does ALS require squared-form losses, and why doesn't it work with cross-entropy/Softmax?

## Architecture Onboarding

- Component map: Input: Interaction matrix R ∈ {0,1}^(M×N) → Weight computation (W_x,y, V_x per Eq. 32) → Target matrix S = (1/N)·Diag(|I_x|⁻¹)·R - 1 → ALS Loop: Fix Q, solve for P row-by-row; Fix P, solve for Q row-by-row → Output: P ∈ R^(M×K), Q ∈ R^(N×K) embeddings

- Critical path:
  1. Weight matrix construction (W, V): Incorrect weights break theoretical connection to Softmax approximation
  2. Target matrix S: Must correctly encode rx,y/(N·|I_x|) - 1; off-by-one errors propagate
  3. Regularization λ: Must be tuned to keep embeddings bounded near zero

- Design tradeoffs:
  - RG² vs. RGˣ: RG² is simpler (O(K³) per update) with upper-bound guarantee; RGˣ includes interaction terms (slightly more expensive O(NK²+K³)) but tighter approximation—minimal empirical difference
  - Embedding dimension K: Larger K improves expressiveness but ALS complexity scales as O(K³)—start with K=64
  - Sampling-based negative weight (n in RG̃): Larger n downweights negatives more

- Failure signatures:
  - Exploding logits: Training diverges, logits grow far from zero → Taylor approximation breaks → ranking quality degrades. Fix: increase λ regularization
  - Slow convergence per iteration: Not pre-computing shared matrices (Q⊤Q, S·Q). Fix: cache these before update loop
  - Degraded ranking vs. WRMF: Check weight formula implementation (W_x,y = |I_x|, not heuristic values)

- First 3 experiments:
  1. Sanity check on synthetic data: Generate low-rank interaction matrix with known P*, Q*. Train RG² with ALS, verify ||P - P*||_F decreases linearly per iteration and final ranking recovers ground truth ordering.
  2. Ablation: RG² vs. RGˣ vs. SM on MovieLens: Train all three with matching embedding dimension K=64. Plot NDCG@10 vs. wall-clock time. Expectation: RG methods reach target NDCG ~3-5x faster than SM with SGD.
  3. Regularization sensitivity: Sweep λ ∈ {0.001, 0.01, 0.1, 1.0} on Electronics dataset. Monitor both NDCG@10 and average logit magnitude. Expectation: Very small λ causes logit explosion and ranking degradation.

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical guarantees of RG losses be extended to ensure NDCG-consistency for datasets with multi-level graded relevance scores, rather than being restricted to binary implicit feedback? The derivation of consistency and the single-click assumption in Proposition 7 rely on binary relevance ($r_{x,y} \in \{0, 1\}$), avoiding the complexity of normalization required for true NDCG-consistency in graded relevance scenarios. Evidence needed: theoretical proof or empirical demonstration showing squared-form losses maintain consistency with NDCG when relevance scores are non-binary.

### Open Question 2
Does the theoretical relationship between RG loss coefficients and Weighted Squared Loss (WSL) weights hold under non-uniform or importance sampling distributions? The derivation in Eq. 18 and subsequent discussion of weight coefficients $w_{x,y}$ are explicitly restricted to uniform distribution $D' = Unif[Y]$. Evidence needed: derivation of RG loss coefficients under general distribution $D'$, or empirical study comparing uniform versus importance sampling performance.

### Open Question 3
Do the efficiency advantages of proposed RG losses persist when integrated into deep neural network (DNN) architectures optimized via stochastic gradient descent (SGD) rather than Alternating Least Squares (ALS)? The paper demonstrates superior convergence speed primarily by leveraging ALS's closed-form solutions on a Matrix Factorization backbone, whereas deep similarity learning often requires SGD-based optimization. Evidence needed: empirical benchmarks comparing training time and convergence rate of RG losses versus Softmax Loss on a deep neural network using identical SGD-based optimizers.

## Limitations
- Taylor expansion approximation assumes logits remain near zero throughout training, which may break down with highly imbalanced data
- ALS optimization's O(K³) per-iteration cost becomes prohibitive for very high embedding dimensions
- Framework currently restricted to binary implicit feedback, not multi-level graded relevance

## Confidence
- **High Confidence**: Theoretical consistency proofs (Bayes-consistency with DCG, convergence guarantees for ALS)
- **Medium Confidence**: Empirical efficiency claims—well-supported by convergence plots but lack full ablation studies across datasets
- **Medium Confidence**: Generalization bounds—derived but depend on assumptions about data distribution that aren't fully validated

## Next Checks
1. Regularization sensitivity study: Systematically vary λ across multiple orders of magnitude on all datasets, measuring both ranking performance and average logit magnitude to verify the boundedness assumption
2. Cross-dataset robustness test: Apply RG² to datasets with different interaction sparsity patterns (e.g., social networks vs. e-commerce) to identify regimes where the zero-logit assumption breaks
3. Comparison with learned warm-start: Initialize RG² ALS with embeddings from SM-SGD after a few epochs versus random initialization, measuring convergence speed and final ranking quality to assess practical benefit of the theoretical approach