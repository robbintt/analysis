---
ver: rpa2
title: Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP
  Results
arxiv_id: '2507.21770'
source_url: https://arxiv.org/abs/2507.21770
tags:
- movie
- recommender
- tone
- data
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a novel semantic movie recommendation system
  using ChatGPT to extract tone of voice from movie descriptions, addressing limitations
  of traditional genre-based approaches. The method combines knowledge graph construction
  with collaborative filtering, leveraging semantic information extracted by ChatGPT.
---

# Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results

## Quick Facts
- **arXiv ID:** 2507.21770
- **Source URL:** https://arxiv.org/abs/2507.21770
- **Authors:** Ali Fallahi; Azam Bastanfard; Amineh Amini; Hadi Saboohi
- **Reference count:** 0
- **Primary result:** Tone-of-voice analysis outperforms conventional genre categorization (MAE: 1.55 vs 5.30; RMSE: 1.99 vs 9.12)

## Executive Summary
This study proposes a novel semantic movie recommendation system that leverages ChatGPT to extract tone-of-voice features from movie descriptions, addressing limitations of traditional genre-based approaches. The method combines knowledge graph construction with collaborative filtering, using semantic information extracted by ChatGPT to build more discriminative user preference profiles. Experiments demonstrate that tone-of-voice analysis significantly outperforms conventional genre categorization, achieving much lower error rates when using extracted semantic features compared to IMDb's explicit genres.

## Method Summary
The approach extracts tone-of-voice labels from movie descriptions using ChatGPT, then builds a semantic knowledge graph that replaces traditional genre features. User ratings are filtered to retain only those above 3/5, creating a User-FavoriteItem matrix that is joined with tone features to produce a User-ToneOfVoice matrix. Pearson Correlation Coefficient is computed on this semantic feature space to measure user similarity, and collaborative filtering generates recommendations based on top-N similar users. The system was evaluated on 241 movies from the intersection of IMDb Top 250 and MovieLens datasets, with tone extraction producing 126 unique labels versus only 21 IMDb genres.

## Key Results
- Tone-of-voice analysis achieved significantly better accuracy than genre-based methods (MAE: 1.55 vs 5.30; RMSE: 1.99 vs 9.12)
- The "Suspenseful" tone was identified as the most frequent in the dataset (9.13% frequency)
- 1TOV configuration outperformed 2TOV and combined genre+tone approaches, suggesting simpler feature spaces may reduce noise
- User-ToneOfVoice matrix contained 180,040 active cells across 6,938 users and 127 tone features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extracting implicit semantic features (tone-of-voice) from movie descriptions provides more discriminative item representations than explicit genre labels.
- **Mechanism:** ChatGPT processes free-text movie descriptions and outputs tone labels (e.g., "Suspenseful," "Intense," "Emotional"). These labels form an 80–126 dimensional feature space vs. only 21 IMDb genres, enabling finer-grained item similarity computation.
- **Core assumption:** LLM-extracted semantic tones align with the latent dimensions users actually use to evaluate movies, and these tones are consistent across extractions.
- **Evidence anchors:** Table I shows 126 unique tone-of-voice terms extracted from 241 movies vs. only 21 genre categories; 1TOV achieves MAE 1.55 vs. 5.30 for 3 Genres.
- **Break condition:** If LLM tone extraction is inconsistent (same description yields different tones) or if tones don't correlate with user rating patterns, the feature space becomes noisy and similarity calculations degrade.

### Mechanism 2
- **Claim:** Filtering to high-rated items (>3 of 5) before constructing user-tone preference profiles reduces noise and improves recommendation accuracy.
- **Mechanism:** The User-Item rating matrix is filtered to retain only ratings >3, creating User-FavoriteItem matrix. This is then joined with tone features to produce User-ToneOfVoice matrix (6,938 users × 127 tones, 180,040 active cells).
- **Core assumption:** Ratings ≤3 represent indifference or negative preference and should not contribute to the user's preference profile.
- **Evidence anchors:** Section III: "Values on the User-Item rating matrix were also filtered only to retain the rates greater than three from the maximum rating value, which is 5."
- **Break condition:** If the threshold is too aggressive for sparse datasets, users may have insufficient positive ratings to build meaningful tone profiles, exacerbating cold-start issues.

### Mechanism 3
- **Claim:** Pearson Correlation Coefficient (PCC) computed on semantic tone features yields more accurate user similarity estimates than genre-based features.
- **Mechanism:** PCC measures correlation between users' rating vectors. When vectors are constructed from tone features rather than genres, the finer feature granularity enables more precise similarity matching for collaborative filtering.
- **Core assumption:** Users with correlated tone preferences will also agree on unrated items; tone features preserve this signal better than genre features.
- **Evidence anchors:** Section III presents PCC formula; Section IV shows 1TOV configuration achieves RMSE 1.99 vs. 9.12 for 3 Genres.
- **Break condition:** If tone dimensions are highly correlated (multicollinearity) or sparse, PCC may produce unstable similarity estimates; regularization or dimensionality reduction may be needed.

## Foundational Learning

- **Concept: Collaborative Filtering (CF)**
  - **Why needed here:** CF is the core recommendation algorithm; understanding user-user similarity and top-N neighbor selection is essential to interpret the architecture.
  - **Quick check question:** Given two users with overlapping ratings on 5 movies, how would PCC quantify their similarity?

- **Concept: Feature Sparsity in Recommender Systems**
  - **Why needed here:** The User-ToneOfVoice matrix (6,938 × 127 with 180,040 active cells) is ~20% dense; understanding sparsity impact on similarity calculation is critical.
  - **Quick check question:** What happens to PCC-based similarity when two users share only 1–2 rated items in common?

- **Concept: LLM Feature Extraction Consistency**
  - **Why needed here:** The system depends on ChatGPT producing stable, meaningful tone labels; prompt design and temperature settings affect extraction reliability.
  - **Quick check question:** How would you validate that ChatGPT produces consistent tone labels for the same movie description across multiple calls?

## Architecture Onboarding

- **Component map:** Movie description → ChatGPT prompt → tone extraction → User-Tone matrix → PCC similarity → Top-N neighbors → predicted ratings → recommendations
- **Critical path:** Movie description → ChatGPT prompt → tone extraction → User-Tone matrix → PCC similarity → Top-N neighbors → predicted ratings → recommendations. Any failure in tone extraction or excessive sparsity breaks the pipeline.
- **Design tradeoffs:**
  - 1TOV vs. 2TOV: Paper shows 1TOV (MAE 1.55) outperforms 2TOV (MAE 1.98)—simpler feature space may reduce noise
  - Tone-only vs. Genre+Tone: Pure tone (1TOV) outperforms combined (3G+1TOV: MAE 3.15)—adding genre features may introduce conflicting signals
  - Rating threshold: Filtering at >3 reduces noise but may over-prune for users with few high ratings
- **Failure signatures:**
  - High MAE/RMSE on new users (cold start persists despite semantic features)
  - Dominant tone labels (e.g., "Suspenseful" at 9.13%) may cause popularity bias
  - Inconsistent tone extraction across API calls indicates prompt instability
- **First 3 experiments:**
  1. Reproduce 1TOV vs. 3G baseline: Replicate MAE/RMSE comparison on same 241-movie dataset to validate pipeline correctness
  2. Tone extraction stability test: Run ChatGPT on 20 movie descriptions 5 times each; measure label consistency (Jaccard similarity across runs)
  3. Cold-start analysis: Stratify evaluation by user activity level; compare tone-based vs. genre-based accuracy for users with <10, 10–50, >50 ratings to identify where semantic features help most

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How would incorporating additional semantic features such as movie titles, songs, and lyrics further improve recommendation accuracy?
- **Basis in paper:** The conclusion explicitly states: "For future work, analyzing the titles, songs, and lyrics of the films may be other valuable resources to improve semantic movie recommendation systems."
- **Why unresolved:** The current study only analyzed movie descriptions; no other textual or multimedia content from films was examined.
- **What evidence would resolve it:** Experiments extending the knowledge graph with extracted features from titles, soundtracks, and lyrics, with MAE/RMSE comparisons to the current approach.

### Open Question 2
- **Question:** How does the proposed method scale to larger datasets beyond the 241 movies tested?
- **Basis in paper:** The study used only 241 movies from IMDb Top 250 and MovieLens. No scalability analysis or larger dataset validation was conducted.
- **Why unresolved:** The limited dataset size raises concerns about generalizability to production-scale systems with thousands or millions of items.
- **What evidence would resolve it:** Experiments on larger movie databases (e.g., full MovieLens 20M or Netflix datasets) with performance and computational efficiency metrics.

### Open Question 3
- **Question:** Why does combining explicit genres with extracted tones (3G+1TOV, 3G+2TOV) yield higher MAE/RMSE than using tones alone?
- **Basis in paper:** Table 1 shows counterintuitive results: 3G+1TOV (MAE 3.15) performs worse than 1TOV alone (MAE 1.55), suggesting feature combination degrades performance.
- **Why unresolved:** The paper does not analyze why additional features hurt accuracy, nor explore optimal feature fusion strategies.
- **What evidence would resolve it:** Ablation studies testing alternative fusion methods (weighted combinations, feature selection, dimensionality reduction) with statistical significance testing.

### Open Question 4
- **Question:** How consistent and reliable are ChatGPT's tone-of-voice extractions across different prompts and model versions?
- **Basis in paper:** The paper does not specify which ChatGPT version or prompts were used, nor report extraction consistency metrics.
- **Why unresolved:** LLM outputs can vary based on prompting and version, raising reproducibility and reliability concerns for production deployment.
- **What evidence would resolve it:** Systematic experiments comparing tone extraction consistency across prompts, runs, and GPT versions, with inter-rater agreement metrics against human annotations.

## Limitations
- Limited dataset size (241 movies) raises questions about scalability to production systems
- No specification of ChatGPT version or exact prompts used for tone extraction
- Rating threshold of >3 may not generalize well to sparser datasets with fewer high-rated items

## Confidence
- **High confidence:** The comparative improvement of tone-based features over genre-based features (MAE 1.55 vs 5.30; RMSE 1.99 vs 9.12) is well-supported by experimental results
- **Medium confidence:** The claim that semantic tone extraction provides more discriminative features than genres assumes consistent LLM output and meaningful correlation with user preferences
- **Low confidence:** The optimal rating threshold (>3) and its impact on cold-start users remains inadequately justified, with limited evidence on how this threshold performs across different dataset densities

## Next Checks
1. **Reproduce baseline comparison:** Replicate the MAE/RMSE comparison between 1TOV and 3-genre configurations on the same 241-movie dataset to verify the reported performance gap
2. **Test tone extraction consistency:** Run ChatGPT on 20 movie descriptions 5 times each, measuring Jaccard similarity across extractions to assess label stability and reproducibility
3. **Cold-start analysis:** Stratify evaluation by user activity levels (e.g., <10, 10-50, >50 ratings) to determine where semantic features provide the most benefit versus traditional genre-based approaches