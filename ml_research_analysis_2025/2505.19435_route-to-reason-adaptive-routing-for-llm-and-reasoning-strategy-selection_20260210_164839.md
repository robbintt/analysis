---
ver: rpa2
title: 'Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection'
arxiv_id: '2505.19435'
source_url: https://arxiv.org/abs/2505.19435
tags:
- reasoning
- performance
- routing
- arxiv
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Route-To-Reason (RTR), a framework that jointly
  selects both a language model and a reasoning strategy for each query to balance
  accuracy and computational efficiency. The core idea is to learn compressed embeddings
  for models and strategies, then predict performance and token usage for each pair,
  enabling dynamic routing based on a trade-off parameter.
---

# Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection

## Quick Facts
- arXiv ID: 2505.19435
- Source URL: https://arxiv.org/abs/2505.19435
- Reference count: 40
- Key result: Jointly selecting models and reasoning strategies achieves up to 82.5% accuracy while reducing token usage by over 60% compared to best individual model

## Executive Summary
This paper introduces Route-To-Reason (RTR), a framework that jointly selects both a language model and a reasoning strategy for each query to balance accuracy and computational efficiency. The core innovation is learning compressed embeddings for models and strategies, then predicting performance and token usage for each pair, enabling dynamic routing based on a trade-off parameter. Experiments across seven reasoning tasks show that RTR outperforms single-model baselines and other routing methods, achieving significant accuracy-cost trade-offs while maintaining strong generalization to out-of-distribution datasets.

## Method Summary
RTR encodes queries, models, and strategies using a frozen text encoder (all-mpnet-base-v2) and learnable embeddings, then predicts correctness probability and token count for each model-strategy pair. A scoring function combining these predictions with a trade-off parameter λ selects the optimal pair. The framework trains separate MLPs for performance (BCE loss) and token usage (MSE loss) prediction, enabling flexible adaptation between quality-first, cost-first, and balanced routing policies without retraining predictors.

## Key Results
- RTR achieves up to 82.5% accuracy while reducing tokens by over 60% compared to best individual model
- Maintains 94.2% accuracy on out-of-distribution datasets
- Successfully adapts when toggling between thinking and non-thinking modes in models that support both

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jointly representing models and reasoning strategies as learnable embeddings enables prediction of their combined behavior on specific queries.
- **Mechanism:** Each model and strategy has a dual-component representation: (1) a frozen embedding from encoding textual descriptions via a pretrained encoder, and (2) a randomly initialized learnable embedding updated during training. These are concatenated with the query representation and fed to predictor MLPs.
- **Core assumption:** Models and strategies exhibit consistent behavioral patterns (performance, token usage) that can be captured in low-dimensional embeddings and generalized across queries.
- **Evidence anchors:** [abstract] "RTR learns compressed representations of both expert models and reasoning strategies, enabling their joint and adaptive selection at inference time." [Section 2.3] "These two components combine external knowledge and internal task-specific patterns to form a comprehensive representation."

### Mechanism 2
- **Claim:** Separate prediction heads for performance (binary) and token usage (regression) enable cost-aware routing decisions.
- **Mechanism:** Two independent MLPs are trained—one with BCE loss to predict correctness probability, another with MSE loss to predict output token count. The routing score combines both via: `score = λ · predicted_accuracy − (1 − λ) · predicted_tokens`.
- **Core assumption:** Token count serves as a reliable proxy for computational cost, and both metrics are predictable from query-model-strategy representations.
- **Evidence anchors:** [Section 2.3] Equations (1) and (2) define the separate loss functions; Equation (3) defines the routing score. [Figure 6] Shows prediction accuracy for token usage: ~80% for non-thinking models within 200-token margin, ~60% for thinking models within 600-token margin.

### Mechanism 3
- **Claim:** The λ trade-off parameter allows flexible adaptation between quality-first, cost-first, and balanced routing policies.
- **Mechanism:** λ ∈ [0, 1] controls the scoring function. High λ prioritizes accuracy; low λ prioritizes efficiency. Users can tune λ based on application constraints without retraining predictors.
- **Core assumption:** A single scalar parameter is sufficient to express the accuracy-cost trade-off across diverse query distributions.
- **Evidence anchors:** [Section 3.3, Figure 7] Shows three configurations (RTR-performance, RTR-balanced, RTR-cost) all outperforming vanilla baselines with different token-accuracy operating points.

## Foundational Learning

- **Concept: Embedding fusion for heterogeneous entities**
  - Why needed here: The framework concatenates embeddings from three distinct sources (query, model, strategy) that must share a semantic space for prediction.
  - Quick check question: Can you explain why freezing the textual-description embedding while training the random component might preserve prior knowledge?

- **Concept: Multi-task prediction heads**
  - Why needed here: Performance prediction is classification (BCE); token prediction is regression (MSE). Understanding when to use each loss is essential.
  - Quick check question: Why might token prediction be harder than correctness prediction for thinking models?

- **Concept: Routing as constrained optimization**
  - Why needed here: The routing decision maximizes a score function under implicit budget constraints via λ.
  - Quick check question: If λ = 0.8, does the router favor accuracy or efficiency? What happens as λ → 1?

## Architecture Onboarding

- **Component map:** Query Encoder -> [Query embedding ⊕ Model embedding ⊕ Strategy embedding] -> Predictors -> Routing Table -> argmax selection
- **Critical path:** Query → Encoder → [Query embedding ⊕ Model embedding ⊕ Strategy embedding] → Predictors → Routing Table → argmax selection
- **Design tradeoffs:** Frozen vs. learnable encoder: Paper uses frozen to reduce cost; fine-tuning might improve prediction but increases training complexity. Separate vs. shared predictors: Paper uses separate; shared could reduce parameters but may conflate learning signals.
- **Failure signatures:** Systematic underestimation of token usage for thinking models → budget overruns. Low prediction accuracy on OOD datasets → poor generalization. All queries routed to same model-strategy pair → predictor collapse or embedding degeneracy.
- **First 3 experiments:**
  1. Baseline replication: Implement on 2 models + 2 strategies with 500 training samples; verify prediction accuracy converges per Figure 10.
  2. Ablation on representation: Compare (text-only) vs. (embedding-only) vs. (combined) per Figure 5; expect combined to achieve ~76% prediction accuracy.
  3. λ sensitivity analysis: Sweep λ ∈ {0.2, 0.5, 0.8} and plot accuracy vs. tokens; verify trade-off curve is monotonic and matches Figure 7 shape.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can collaborative multi-model routing improve accuracy-efficiency trade-offs beyond single-model selection?
- **Basis in paper:** [explicit] Authors state "incorporating collaborative decision-making across multiple models represents a promising avenue for future work."
- **Why unresolved:** Current RTR framework only routes to one model-strategy pair per query; combining outputs from multiple models is unexplored.
- **What evidence would resolve it:** Experiments comparing RTR against ensemble methods that aggregate multiple model outputs.

### Open Question 2
- **Question:** How well does RTR generalize to non-reasoning domains such as creative writing or code generation?
- **Basis in paper:** [explicit] Authors note "while our experiments are centered on reasoning tasks, extending evaluation to a broader range of problem domains would further validate the framework's generality."
- **Why unresolved:** Evaluation is limited to mathematical, scientific, and commonsense reasoning benchmarks.
- **What evidence would resolve it:** Experiments on diverse task types (summarization, translation, creative tasks) with appropriate performance metrics.

### Open Question 3
- **Question:** Can RTR efficiently incorporate new models or strategies without requiring full offline evaluation on training data?
- **Basis in paper:** [inferred] Framework requires pre-collected responses from all model-strategy pairs to train predictors; cold-start for new candidates is not addressed.
- **Why unresolved:** Adding a new model or strategy requires generating responses across the training set to learn its embedding.
- **What evidence would resolve it:** Experiments with few-shot or zero-shot adaptation to unseen models/strategies.

### Open Question 4
- **Question:** Would improving token usage prediction accuracy lead to measurably better routing decisions?
- **Basis in paper:** [inferred] Figure 6 shows only ~60% accuracy within 600-token margin for reasoning models; coarse-grained prediction may limit routing quality.
- **Why unresolved:** Relationship between predictor accuracy and downstream routing performance is not quantified.
- **What evidence would resolve it:** Ablation studies correlating prediction error bounds with routing decision quality.

## Limitations

- The 60% token prediction accuracy for thinking models within 600-token margins may be insufficient for reliable routing in high-stakes applications
- Experiments focus on reasoning tasks where all models are similarly capable, leaving unclear whether RTR would maintain advantages when model quality varies significantly
- The fixed λ parameterization may not capture complex, context-dependent trade-offs between accuracy and efficiency

## Confidence

- **High Confidence:** The core prediction framework (dual embeddings + separate heads + λ-weighted scoring) is clearly specified and experimentally validated on multiple datasets
- **Medium Confidence:** The mechanism of joint embedding representation is theoretically sound, but lacks ablation studies isolating the contribution of combined vs. separate embeddings
- **Low Confidence:** The claim about OOD generalization (94.2% accuracy) is based on limited out-of-domain tests

## Next Checks

1. **Ablation on embedding representations:** Replicate Figure 5 by training separate models using only text embeddings, only learnable embeddings, and the combined approach on a subset of the data to verify the claimed ~76% prediction accuracy for the combined method.

2. **Token prediction error analysis:** On thinking models (QwQ-32B, DeepSeek-R1-14B), systematically measure the distribution of prediction errors and compute routing accuracy when using predicted vs. actual token counts to quantify the practical impact of the 60% within-margin accuracy.

3. **Cross-domain λ sensitivity:** Apply the trained RTR model to a non-reasoning task (e.g., summarization or classification) and evaluate whether the same λ values produce reasonable accuracy-cost trade-offs, or if domain-specific tuning is required.