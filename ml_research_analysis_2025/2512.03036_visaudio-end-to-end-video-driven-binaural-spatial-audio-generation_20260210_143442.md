---
ver: rpa2
title: 'ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation'
arxiv_id: '2512.03036'
source_url: https://arxiv.org/abs/2512.03036
tags:
- audio
- spatial
- video
- binaural
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViSAudio, the first end-to-end framework
  for generating binaural spatial audio directly from silent video, addressing the
  limitations of conventional two-stage approaches that accumulate errors and create
  spatio-temporal inconsistencies. ViSAudio leverages a conditional flow matching
  architecture with a dual-branch audio generation design, where two dedicated branches
  independently model the left and right audio latent flows while maintaining channel
  consistency.
---

# ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation

## Quick Facts
- arXiv ID: 2512.03036
- Source URL: https://arxiv.org/abs/2512.03036
- Reference count: 40
- First end-to-end framework for generating binaural spatial audio directly from silent video, avoiding two-stage pipelines that accumulate errors and create spatio-temporal inconsistencies.

## Executive Summary
This paper introduces ViSAudio, the first end-to-end framework for generating binaural spatial audio directly from silent video, addressing the limitations of conventional two-stage approaches that accumulate errors and create spatio-temporal inconsistencies. ViSAudio leverages a conditional flow matching architecture with a dual-branch audio generation design, where two dedicated branches independently model the left and right audio latent flows while maintaining channel consistency. Integrated with a conditional spacetime module, the framework extracts spatio-temporal features from the video and injects them into the generation process, ensuring precise alignment between the generated binaural audio and the input video. To support this task, the authors curate BiAudio, a large-scale dataset of approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Comprehensive experiments demonstrate that ViSAudio outperforms state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments.

## Method Summary
ViSAudio is an end-to-end binaural spatial audio generation framework that directly synthesizes left and right audio channels from silent video using conditional flow matching. The architecture employs a dual-branch design where two independent flow predictors generate left and right audio latents while maintaining synchronization. A conditional spacetime module fuses semantic, temporal, and spatial visual features (via CLIP, Synchformer, and a perception encoder) to guide the generation process. The model is trained on BiAudio, a curated dataset of ~97K video-binaural pairs constructed from 360° videos with synthetic camera rotations to enhance spatial cues. Training uses a conditional flow matching loss with separate single-modal branches for channel-specific refinement and shared multimodal blocks for content consistency.

## Key Results
- Outperforms state-of-the-art methods across objective metrics including FD_VGG, FD_PANN, KL_PANN, DeSync, and IB-Score
- Achieves high subjective quality ratings in MOS evaluations across five dimensions (overall quality, audio-visual sync, spatial impression, speech intelligibility, and audio fidelity)
- Successfully generates binaural audio that adapts to viewpoint changes, sound-source motion, and diverse acoustic environments
- Ablation studies confirm the importance of BiAudio dataset and dual-branch architecture for performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jointly modeling left and right audio channels with dedicated flow-matching branches preserves spatial disparity better than standard stereo VAEs.
- **Mechanism:** The architecture uses a Dual-Branch design where two independent networks predict left and right latent flows ($v^l_\theta, v^r_\theta$). While they share "Multimodal Joint" blocks for semantic synchronization, they utilize separate "Single-Modal Branch" blocks to refine channel-specific spatial characteristics.
- **Core assumption:** Standard stereo audio VAEs tend to "average out" or discard the fine-grained phase/amplitude differences required for spatial perception, a problem solved by treating channels as distinct but correlated generation targets.
- **Evidence anchors:**
  - [section 4.2] States that stereo VAEs discard spatial information, whereas this method learns two correlated latent representations that preserve spatial cues.
  - [section 4.2] Describes the sequential update of latents $x^l_t$ and $x^r_t$ to ensure synchronization while allowing independent refinement.
  - [corpus] Corpus neighbors (e.g., AudioX) focus on diffusion or stereo outputs; direct evidence for "Dual-Branch Flow Matching" specifically is absent in the provided neighbors, indicating a potential architectural novelty.
- **Break condition:** If the "Single-Modal Branch" weights are forced to be shared or identical, the model would likely regress to mono-like behavior (collapsing spatial cues).

### Mechanism 2
- **Claim:** Injecting explicit synchronization and spatial visual features (Conditional Spacetime Module) acts as a control signal to ground generated audio in video timing and spatial layout.
- **Mechanism:** The module extracts "Sync Features" (via Synchformer) and "Spatial Features" (via Perception Encoder with learnable position embeddings $E_l, E_r$). These are fused into "Global Spacetime Features" and injected into the transformer blocks via Adaptive Layer Normalization (adaLN).
- **Core assumption:** General global conditioning (like CLIP text/image features) is insufficient for precise temporal alignment and spatial localization; explicit temporal and positional visual embeddings are required.
- **Evidence anchors:**
  - [section 4.3] Explains that the module balances consistency between channels while preserving distinctive spatial characteristics.
  - [figure 2] Shows the "Conditional Spacetime Module" feeding into the "Single-modal Branch Transformer Blocks" via adaLN.
  - [corpus] Weak evidence; neighbors like *BinauralFlow* use flow matching for rendering but do not explicitly detail a "Conditional Spacetime Module" for video-driven alignment.
- **Break condition:** If the visual spatial features ($F_{pe}$) are ablated, the model should generate semantically correct audio but with random or static spatial positioning (verified in ablation Table 4).

### Mechanism 3
- **Claim:** Training on synthetic dynamic camera rotations (BiAudio dataset) forces the model to learn viewpoint-dependent spatial adaptation rather than memorizing fixed acoustic environments.
- **Mechanism:** The dataset construction takes static 360° scenes and applies random piecewise linear rotation trajectories (yaw, pitch, roll) to render perspective video and corresponding binaural audio. This forces the model to predict audio that changes as the "camera" (listener) turns.
- **Core assumption:** Real-world fixed-view datasets contain insufficient variance in viewpoint-to-audio mappings to train a generalizable binaural generator.
- **Evidence anchors:**
  - [section 3.1] "Spatial Cue Enhancement" describes diversifying camera rotation trajectories to overcome fixed viewpoint constraints.
  - [section 5.4 / table S2] Shows that training without BiAudio significantly degrades performance, effectively breaking the generation quality for open-domain data.
  - [corpus] *The World is Not Mono* emphasizes spatial understanding in LLMs, supporting the need for spatial data, but does not address the specific camera trajectory mechanism used here.
- **Break condition:** If training samples have no camera movement (static viewpoint), the model would likely fail to generalize to user head movements or panning shots in inference.

## Foundational Learning

- **Concept: Conditional Flow Matching (CFM)**
  - **Why needed here:** This is the core generative engine replacing diffusion. You must understand how it defines a probability path from noise ($x_0$) to audio latent ($x_1$) using an Optimal Transport (OT) displacement interpolation.
  - **Quick check question:** How does the vector field $v_\theta$ differ from the score function $\epsilon_\theta$ in diffusion models? (Answer: $v_\theta$ predicts velocity/displacement, not noise).

- **Concept: First-Order Ambisonics (FOA) & HRIR**
  - **Why needed here:** The dataset is constructed by converting FOA audio (4 channels: W, X, Y, Z) to binaural (2 channels: L, R) using Head-Related Impulse Responses (HRIR). Understanding this conversion is vital for understanding the dataset labels.
  - **Quick check question:** In the dataset pipeline, what mathematical operation converts the Ambisonics channels into the final Binaural L/R channels? (Answer: Convolution with HRIR filters).

- **Concept: Spatial Features via PE (Perception Encoder)**
  - **Why needed here:** The paper uses a spatially-tuned Perception Encoder rather than just CLIP. You need to know that this encoder retains spatial layout information (via patches/position embeddings) which is crucial for the "Conditional Spacetime Module."
  - **Quick check question:** Why does the architecture introduce specific learnable spatial embeddings ($E_l, E^r$) for the left and right channels? (Answer: To guide the visual feature extraction to distinct perspectives for the left and right audio streams).

## Architecture Onboarding

- **Component map:** Inputs (Video Frames + Text) -> Encoders (CLIP, Synchformer, Perception Encoder) -> Conditional Spacetime Module (fuses Sync + Spatial features + Global conditioning) -> Dual-Branch Transformer (Joint Blocks + Single-Modal Blocks) -> Decoder (Mono VAE x2) -> Outputs (Left and Right audio waveforms)

- **Critical path:** The "Spatial Feature" extraction ($F_{pe}$) -> "Global Spacetime Feature" fusion -> "Single-Modal Branch" injection. If the spatial features do not reach the single-modal branches correctly, the dual-branch generation collapses to two identical mono streams.

- **Design tradeoffs:**
  - **Mono VAE vs. Stereo VAE:** The paper argues Stereo VAEs discard spatial info. The tradeoff is architectural complexity (dual branches + dual decoding) vs. simpler but spatially-flattened stereo output.
  - **Shared vs. Separate Weights:** Using shared weights in Joint blocks ensures semantic consistency (speech stays the same), while separate weights in Single-modal blocks allow amplitude/phase differences (spatial cues).

- **Failure signatures:**
  - **"Mono Collapse":** Generated Left and Right channels are nearly identical ($L \approx R$). This indicates the "Single-Modal Branch" is not receiving distinct spatial features or the loss isn't penalizing channel similarity effectively.
  - **Temporal Drift:** Audio sync is lost. Check the Synchformer feature injection in the Conditional Spacetime Module.
  - **Static Spatialization:** Sound source doesn't move when camera moves. Indicates the Spatial Cue Enhancement in the dataset or the PE embeddings failed to capture dynamic trajectories.

- **First 3 experiments:**
  1. **Sanity Check (Overfitting):** Train on 1 video-audio pair with camera rotation. Verify the model can reproduce the exact binaural shift as the "camera" moves in that single sample.
  2. **Ablation on Position Embeddings:** Remove the channel-specific learnable embeddings ($E_l, E_r$). Run inference on a "sound on left" vs. "sound on right" sample. If the model outputs center-panned audio for both, the spatial mechanism is broken.
  3. **Dataset Composition Test:** Train a "Dual-Branch" model on fixed-view data (no rotation) vs. BiAudio (dynamic rotation). Evaluate on a panning video shot. The fixed-view model should fail to adapt the stereo field during the pan.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the ViSAudio framework be adapted to handle long-range temporal dependencies in video sequences significantly longer than the current 8-second training clips?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the model "currently handles short video clips, which limits its ability to capture long-range temporal dependencies and complex acoustic events."
- **Why unresolved:** The current architecture and training data are constrained to short durations, making it unclear if the dual-branch flow matching scales efficiently to longer contexts without losing spatio-temporal consistency.
- **What evidence would resolve it:** Successful generation of coherent binaural audio for video clips exceeding 30 seconds that maintains spatial alignment and narrative coherence over time.

### Open Question 2
- **Question:** Can the Dual-Branch Audio Generation architecture be extended to directly synthesize multi-channel spatial audio formats, such as First-Order Ambisonics (FOA)?
- **Basis in paper:** [explicit] The authors explicitly list "extending our framework to support multi-channel audio generation, such as directly producing FOA audio" as a direction for future work.
- **Why unresolved:** The current dual-branch design is specialized for two-channel binaural output; scaling to four or more channels (required for FOA) introduces challenges in maintaining channel consistency and distinctiveness across a more complex spatial layout.
- **What evidence would resolve it:** A modified variant of ViSAudio generating 4-channel FOA that achieves competitive spatial accuracy metrics compared to specialized FOA models like OmniAudio.

### Open Question 3
- **Question:** What mechanisms can effectively mitigate the introduction of artifacts caused by the background noise present in the real-world BiAudio training dataset?
- **Basis in paper:** [inferred] The authors acknowledge that "background noise in the audio may introduce artifacts in the generated outputs" due to the dataset's real-world nature.
- **Why unresolved:** While the dataset includes captions for "invisible sound," the uncontrolled nature of real-world noise (e.g., wind, static) may be learned by the generative model as desirable features rather than noise to be filtered or controlled.
- **What evidence would resolve it:** An ablation study showing that specific architectural changes or data augmentation techniques (beyond the current filtering) significantly reduce unwanted noise artifacts in the generated waveforms.

## Limitations

- The framework relies on a pre-trained mono VAE for decoding, which could be considered a hidden subcomponent rather than true end-to-end generation.
- Dataset construction requires access to Sphere360, which may have licensing restrictions, and the spatial enhancement via synthetic camera trajectories assumes generalization to real head movements.
- Ablation studies on individual conditioning streams (CLIP vs Synchformer vs Perception Encoder) are relatively brief and do not fully isolate their individual contributions to final performance.

## Confidence

- **High**: Performance superiority over baselines on FD_VGG, FD_PANN, KL_PANN, DeSync, and IB-Score (objective metrics).
- **Medium**: Claim of end-to-end generation avoiding two-stage pipelines, given the reliance on pre-trained mono VAE.
- **Medium**: The necessity of the BiAudio dataset for training generalization, based on the ablation result.
- **Low**: The relative importance of each conditioning stream (CLIP, Synchformer, Perception Encoder) in the Conditional Spacetime Module, due to limited ablation depth.

## Next Checks

1. Train and evaluate a variant where the channel-specific spatial embeddings (E^l, E^r) are ablated; confirm if the model produces center-panned (L≈R) audio on left/right source samples.
2. Replace the synthetic camera rotations in BiAudio with real human head movement trajectories; measure degradation in performance to test synthetic generalization.
3. Perform a controlled ablation study isolating CLIP-only, Synchformer-only, and Perception Encoder-only conditioning in the spacetime module to quantify each component's contribution to spatial accuracy.