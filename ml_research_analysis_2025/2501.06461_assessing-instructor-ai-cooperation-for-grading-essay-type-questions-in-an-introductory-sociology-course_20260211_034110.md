---
ver: rpa2
title: Assessing instructor-AI cooperation for grading essay-type questions in an
  introductory sociology course
arxiv_id: '2501.06461'
source_url: https://arxiv.org/abs/2501.06461
tags:
- human
- each
- scoring
- grading
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores using AI as a grading assistant for handwritten\
  \ essay-type questions in a sociology course. Researchers compared human grading\
  \ to GPT models\u2019 transcription and scoring of 70 student exams with six essay\
  \ questions each."
---

# Assessing instructor-AI cooperation for grading essay-type questions in an introductory sociology course

## Quick Facts
- **arXiv ID:** 2501.06461
- **Source URL:** https://arxiv.org/abs/2501.06461
- **Reference count:** 40
- **Key result:** GPT models can serve as a "second grader" to flag inconsistencies and reduce bias in essay grading, but should not fully replace human grading.

## Executive Summary
This study investigates using AI as a grading assistant for handwritten essay-type questions in an introductory sociology course. Researchers compared human grading to GPT models' transcription and scoring of 70 student exams with six essay questions each. GPT-4o-mini slightly outperformed GPT-4o in transcription accuracy. For grading, GPT showed strong correlations with human scores (up to 0.87) when template answers were provided, but discrepancies remained without them. Bland-Altman plots revealed GPT tended to score more generously than humans. The findings suggest GPT can serve as a "second grader" to flag inconsistencies and reduce bias, but should not fully replace human grading. GPT-4o-mini offers a cost-effective alternative for such tasks.

## Method Summary
The study employed GPT-4o and GPT-4o-mini models via OpenAI API to transcribe and grade 420 handwritten essay images from 70 student exams. Images were encoded to Base64 and processed through a transcription pipeline (temperature 0.3) followed by a scoring engine run 100 times per setting and averaged to reduce stochasticity. Template answers were provided to improve grading alignment. Performance was evaluated using cosine similarity for transcription accuracy and Pearson correlation plus Bland-Altman analysis for grading consistency against a human benchmark.

## Key Results
- GPT-4o-mini slightly outperformed GPT-4o in transcription accuracy
- GPT grading showed strong correlations with human scores (up to 0.87) when template answers were provided
- Bland-Altman plots revealed GPT tended to score more generously than humans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing template answers to GPT models substantially improves alignment with human grading by reducing ambiguity in evaluation criteria.
- Mechanism: Template answers shift the model from relying on its general sociological knowledge to performing a direct comparison task. By grounding the evaluation in instructor-provided "ideal" responses, the model assesses student answers against a specific benchmark rather than a diffuse, internally-learned standard.
- Core assumption: The template answer accurately reflects the instructor's grading criteria and course content. A poorly written or misaligned template would likely degrade, not improve, alignment.
- Evidence anchors:
  - [abstract] "For grading, GPT demonstrated strong correlations with the human grader scores, especially when template answers were provided."
  - [section] Comparing scoring prompts, the authors note "prompt 1b [without templates] correlations can be deemed moderate," whereas "All the Pearson's correlations [with templates] on or above .80 indicate a strong consistency."
  - [corpus] "LLM-based Automated Grading with Human-in-the-Loop" discusses the importance of providing context and rubrics, aligning with the finding that template answers improve performance.
- Break condition: The prompt lacks template answers or a clear rubric, forcing the model to guess the grading standard, leading to high variability and only moderate correlation with human scores.

### Mechanism 2
- Claim: Aggregating scores from multiple independent model runs reduces stochastic variability and produces a more stable, reliable grade estimate.
- Mechanism: GPT models are non-deterministic. A single run might produce an outlier score. By running the same grading task 100 times and averaging the outputs, random errors are cancelled out, yielding a score that better represents the model's central tendency for that specific answer.
- Core assumption: The model's score distribution is unimodal. Averaging is effective noise reduction unless the model is fundamentally confused and produces a bimodal distribution (e.g., alternating between a 2 and an 8).
- Evidence anchors:
  - [section] "For each setting, we instructed the models to grade the answers 100 times... Then, the outputs were averaged to utilize one single score for each student using each setting."
  - [section] "Figure 5 displays the standard deviation across 100 runs... GPT-4o-mini provides more consistent results, particularly at a lower temperature setting."
  - [corpus] Weak/missing: The corpus summaries do not explicitly detail the multi-run averaging mechanism for stabilizing scores.
- Break condition: The model's `temperature` parameter is set too high (e.g., 1.0 or above), causing such high variance that the average of 100 runs is still noisy or unrepresentative.

### Mechanism 3
- Claim: The system identifies potentially biased or erroneous grades by flagging large discrepancies between the AI's score and the human benchmark.
- Mechanism: Human grading is established in literature as being subject to fatigue and bias. AI grading is consistent but not infallible. By treating the AI as a "second grader," exams with large score differences (residuals from the correlation line or outliers on a Bland-Altman plot) are identified for re-evaluation, creating a mutual error-checking system.
- Core assumption: A large discrepancy signals a potential problem in *either* the human's or the AI's initial grading, and re-examination will lead to a fairer result.
- Evidence anchors:
  - [abstract] "...highlighting GPT's role as a 'second grader' to flag inconsistencies for assessment reviewing rather than fully replace human evaluation."
  - [section] "The residuals [from the correlation] are the most useful information... For example... an outlier... assessed with a higher score by GPT and a lower score by the instructors... is an indicator that the assessment may need to be revised again."
  - [corpus] Weak/missing: The corpus does not explicitly describe using score residuals for a targeted re-assessment workflow.
- Break condition: The AI has a systematic bias (e.g., consistently grading one type of answer higher) that goes uncorrected. This would cause it to flag legitimate human grades as errors, potentially introducing new unfairness.

## Foundational Learning

- **Concept: Cosine Similarity**
  - Why needed here: It is the quantitative metric used to validate that the AI's image-to-text transcription accurately reflects the student's handwritten response before grading begins.
  - Quick check question: If a student's handwritten answer and the AI transcription are perfectly identical, what would the cosine similarity score be?

- **Concept: Temperature (in LLMs)**
  - Why needed here: This parameter directly controls the randomness of the AI's output. Understanding it is essential for configuring the grading pipeline to be consistent (low temperature) rather than creative (high temperature).
  - Quick check question: To get the most repeatable, deterministic grading results from the model, should you set the temperature closer to 0.2 or 0.7?

- **Concept: Bland-Altman Plot**
  - Why needed here: This visualization is crucial for analyzing the agreement between human and AI scores. It reveals not just correlation, but *systematic bias*â€”for example, if the AI consistently grades higher than the human.
  - Quick check question: On a Bland-Altman plot, if most data points lie above the zero-difference line, what does that tell you about the AI's grading tendency?

## Architecture Onboarding

- **Component map:** Image Input -> Base64 Encoding -> AI Transcription (GPT-4o-mini) -> AI Scoring (N runs) -> Score Averaging -> Discrepancy Flagging
- **Critical path:** Image Input -> Base64 Encoding -> AI Transcription (GPT-4o-mini) -> AI Scoring (N runs) -> Score Averaging -> Discrepancy Flagging
- **Design tradeoffs:**
  - **Cost vs. Performance:** GPT-4o-mini is ~30x cheaper than GPT-4o. It's excellent for transcription, but GPT-4o may offer slightly better scoring alignment. Decision depends on volume and budget.
  - **Latency vs. Stability:** Running 100 scoring iterations per exam increases stability (via averaging) but multiplies the processing time and API costs by 100x.
  - **Flexibility vs. Alignment:** Providing template answers requires more upfront instructor effort but significantly improves grading alignment.
- **Failure signatures:**
  - **Low Cosine Similarity (<0.90):** Suggests poor image quality or illegible handwriting. The subsequent grading will be based on a flawed transcription.
  - **High Standard Deviation Across Runs:** Indicates the model is uncertain. Often caused by a temperature setting that is too high or an ambiguous prompt.
  - **Systematic Score Bias:** The Bland-Altman plot shows a consistent positive or negative mean difference, indicating the AI's grading scale is shifted relative to the human's.
- **First 3 experiments:**
  1. **Transcription Accuracy Check:** Process a small batch of exams (n=10). Calculate cosine similarity between AI and human transcriptions. Aim for >0.95 average. Adjust prompt if below target.
  2. **Scoring Configuration Test:** On the same batch, compare GPT-4o-mini vs. GPT-4o at two temperatures (0.2, 0.7) using template answers. Measure correlation with human scores to select the optimal model and settings.
  3. **Pilot Flagging Workflow:** Run the full pipeline on a complete exam set (n=70). Manually review the top 5-10 most flagged discrepancies. Verify if the flags identify genuine grading inconsistencies worth reviewing.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the instructor-AI cooperation model be effectively extended to longer essay responses or project-based assessments?
  - Basis in paper: [explicit] The authors state that future studies could extend this approach to other forms of assessment, noting that students in the current study were constrained to only half-page answers.
  - Why unresolved: The current methodology was limited to short essay-type questions, and it is unclear if the high transcription similarity and scoring correlations hold for more complex, lengthy, or non-textual submissions.
  - What evidence would resolve it: A replication of the study using longer essays or project documentation, analyzing the consistency of GPT transcription and scoring against human benchmarks.

- **Open Question 2:** Do differences between human and GPT grading correlate with student background characteristics (e.g., ethnicity, gender)?
  - Basis in paper: [explicit] The authors explicitly note they "did not explore variations in consistency based on students' background characteristics" and suggest future research should predict differences based on factors like ethnicity or gender.
  - Why unresolved: The study treated the student cohort as a homogenous group regarding demographic bias analysis.
  - What evidence would resolve it: A regression analysis of the grading residuals (discrepancies between human and AI scores) controlling for student demographic variables to detect systematic bias.

- **Open Question 3:** Does GPT performance in grading essay-type questions vary across academic disciplines with different levels of subjectivity?
  - Basis in paper: [explicit] The conclusion states, "our data came from an introductory sociology course, and it would be valuable to examine GPT performance in other disciplines with varying levels of subjectivity."
  - Why unresolved: Sociology represents a specific epistemological framework; the generalizability of these findings to hard sciences or creative arts remains untested.
  - What evidence would resolve it: Comparative studies applying the same AI-assisted grading protocols across diverse disciplines (e.g., history, engineering, philosophy) to benchmark performance.

## Limitations

- The study relies on a single instructor's grading as the sole human benchmark, raising questions about inter-rater reliability
- The effectiveness of the "second grader" flagging system is conceptually sound but not empirically tested for its impact on final grade accuracy
- The claim that GPT-4o-mini is a "cost-effective alternative" lacks specific cost-benefit analysis relative to the improvement in grading quality or time saved

## Confidence

- **High Confidence:** The transcription accuracy advantage of GPT-4o-mini over GPT-4o is well-supported by the cosine similarity metrics and consistent with the models' known capabilities. The correlation improvements when template answers are provided are clearly demonstrated through comparative analysis.
- **Medium Confidence:** The Bland-Altman analysis revealing GPT's tendency toward more generous scoring is statistically valid but requires external validation to determine if this bias is systematic across different courses or grading styles. The effectiveness of the "second grader" flagging system is conceptually sound but not empirically tested for its impact on final grade accuracy.
- **Low Confidence:** The claim that GPT-4o-mini is a "cost-effective alternative" lacks specific cost-benefit analysis relative to the improvement in grading quality or time saved. The study does not provide quantitative evidence that using AI as a second grader actually reduces instructor workload or improves grading consistency.

## Next Checks

1. Conduct inter-rater reliability testing with multiple human graders to establish baseline grading consistency before comparing to AI performance.
2. Test the grading pipeline across different subjects and question types to assess generalizability of the template answer approach.
3. Implement the flagging system in a live grading workflow and measure its impact on identifying genuine grading errors versus false positives.