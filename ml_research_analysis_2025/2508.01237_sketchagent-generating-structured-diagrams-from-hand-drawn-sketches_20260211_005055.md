---
ver: rpa2
title: 'SketchAgent: Generating Structured Diagrams from Hand-Drawn Sketches'
arxiv_id: '2508.01237'
source_url: https://arxiv.org/abs/2508.01237
tags:
- code
- generation
- diagram
- arxiv
- sketchagent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of converting hand-drawn sketches
  into structured, machine-readable diagrams, a task that remains largely manual despite
  advancements in image generation. The authors introduce SketchAgent, a multi-agent
  system that integrates sketch recognition, symbolic reasoning, and iterative validation
  to transform sketches into semantically coherent and structurally accurate diagrams.
---

# SketchAgent: Generating Structured Diagrams from Hand-Drawn Sketches

## Quick Facts
- arXiv ID: 2508.01237
- Source URL: https://arxiv.org/abs/2508.01237
- Authors: Cheng Tan, Qi Chen, Jingxuan Wei, Gaowei Wu, Zhangyang Gao, Siyuan Li, Bihui Yu, Ruifeng Guo, Stan Z. Li
- Reference count: 11
- Primary result: Achieves 82.34 Pass@1 for sketch generation and 93.12 Pass@1 for sketch editing, outperforming open-source and closed-source models on key metrics

## Executive Summary
This paper addresses the problem of converting hand-drawn sketches into structured, machine-readable diagrams, a task that remains largely manual despite advancements in image generation. The authors introduce SketchAgent, a multi-agent system that integrates sketch recognition, symbolic reasoning, and iterative validation to transform sketches into semantically coherent and structurally accurate diagrams. SketchAgent leverages a compiler module and GPT-4o feedback to ensure code correctness and diagram fidelity. To support this task, the authors also propose the Sketch2Diagram Benchmark, a dataset of over 6,000 high-quality examples across eight diagram categories, including flowcharts, directed graphs, and model architectures. Experiments show that SketchAgent achieves state-of-the-art performance, with a Pass@1 score of 82.34 for sketch generation and 93.12 for sketch editing, outperforming both open-source and closed-source models on key metrics such as CodeBLEU, BLEU, and visual fidelity measures.

## Method Summary
SketchAgent is a multi-agent system that converts hand-drawn sketches into structured diagrams through three specialized agents: Sketch-to-Code Agent (Qwen2-VL-7B fine-tuned), Editing Code Agent (Qwen2.5-Coder-7B fine-tuned), and Check Agent (compiler + GPT-4o feedback). The system maps sketches and instructions to TikZ code representations, validates compilation success, and uses GPT-4o for semantic alignment verification. The method employs iterative feedback loops where failed compilation or semantic misalignment triggers code regeneration. Training uses 4 epochs on 4×80GB A100 GPUs with 4096 token input length, processing the Sketch2Diagram Benchmark dataset of 6,000+ sketch-code pairs.

## Key Results
- Sketch generation: 82.34 Pass@1, 56.78 CodeBLEU, 69.72 BLEU, 83.26 ROUGE-L
- Sketch editing: 93.12 Pass@1, 71.22 CodeBLEU, 81.33 BLEU, 87.45 ROUGE-L
- Ablation shows compiler removal decreases Pass@1 by 1.49 points; GPT-4o feedback removal decreases by 1.16 points
- State-of-the-art performance on both generation and editing tasks across eight diagram categories

## Why This Works (Mechanism)

### Mechanism 1: Symbolic Intermediate Representation via Code Generation
- Claim: Converting ambiguous visual sketches into structured symbolic code (LaTeX/TikZ) before diagram rendering improves semantic fidelity compared to direct image-to-image approaches.
- Mechanism: The Sketch-to-Code Agent maps a hand-drawn sketch S and instruction set Q to an initial code representation Ck, capturing structural semantics as token sequences. This formulation (Equation 1: Ck = Fk(S, Q)) forces explicit structural reasoning rather than implicit pixel-level mappings.
- Core assumption: The symbolic representation language (TikZ) can adequately express all diagram semantics present in freehand sketches.
- Evidence anchors:
  - [abstract] "SketchAgent integrates sketch recognition, symbolic reasoning, and iterative validation to produce semantically coherent and structurally accurate diagrams"
  - [Section 3.1] "The output Ck is modeled as a sequence of tokens, where each token corresponds to a diagram component or an attribute"
  - [corpus] Weak direct evidence; StarFlow uses similar sketch-to-structured-output approach for workflows, but no direct comparison to code intermediates exists.
- Break condition: Sketches containing visual elements that cannot be expressed in the target code schema (e.g., custom icons, non-standard arrows) will fail or produce degraded outputs.

### Mechanism 2: Compiler-Based Deterministic Validation Loop
- Claim: Using a LaTeX compiler as a hard constraint for code validity provides reliable binary feedback that guides regeneration.
- Mechanism: The Check Agent performs syntax validation; if compilation fails, code is returned to the responsible agent for regeneration. This creates a verifiable correctness gate that does not rely on learned approximations.
- Core assumption: Compilation success correlates with diagram correctness; compilable code produces semantically meaningful output.
- Evidence anchors:
  - [Section 3.3] "If Ce fails to compile, it is sent back for regeneration by either the Sketch-to-Code Agent or the Editing Code Agent"
  - [Table 2b] Removing the compiler causes Pass@1 to drop from 82.34 to 80.85 (1.49-point decrease)
  - [corpus] No comparable compiler-based validation found in neighbor papers; this appears novel to SketchAgent.
- Break condition: Code that compiles successfully but produces visually incorrect diagrams (syntactically valid, semantically wrong) will pass this filter incorrectly.

### Mechanism 3: GPT-4o Visual-Semantic Verification as Feedback Signal
- Claim: An external VLM (GPT-4o) provides semantic alignment verification between the rendered diagram and original sketch, enabling iterative correction beyond syntactic validity.
- Mechanism: After successful compilation, GPT-4o compares the compiled diagram D with the original sketch S and user instructions Q. If alignment fails, the Check Agent triggers regeneration.
- Core assumption: GPT-4o's visual comparison reliably detects semantic misalignment that the compiler cannot catch.
- Evidence anchors:
  - [Section 3.3] "Once compilation succeeds, we use GPT-4o to compare the compiled diagram D with the original sketch S and the user instructions Q"
  - [Table 2b] Removing GPT-4o feedback causes Pass@1 to drop from 82.34 to 81.18; removing both compiler and GPT-4o drops it to 78.52 (3.82-point total decrease), showing synergistic effect
  - [Table 3b] Curiously, for editing tasks, removing feedback slightly improves Pass@1 (93.12→94.69), suggesting feedback may introduce noise in refinement scenarios
  - [corpus] SketchJudge evaluates MLLMs on grading hand-drawn diagrams, suggesting VLMs have some capacity for sketch understanding, but no direct evidence for verification-as-feedback loops.
- Break condition: GPT-4o may fail to detect subtle structural errors (wrong arrow direction, missing labels) or introduce false positives that trigger unnecessary regeneration cycles.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) for structured output generation**
  - Why needed here: SketchAgent's Sketch-to-Code Agent requires understanding visual sketch content and producing structured code tokens. Without VLM foundations, the system cannot bridge visual input to symbolic output.
  - Quick check question: Can you explain how Qwen2-VL processes an image and generates token sequences conditioned on visual features?

- **Concept: Multi-agent orchestration with feedback loops**
  - Why needed here: SketchAgent decomposes sketch-to-diagram into specialized agents (recognition, editing, verification) with iterative handoffs. Understanding agent communication patterns and failure handling is essential.
  - Quick check question: How should agent B handle output from agent A when that output fails downstream validation—regenerate, repair, or escalate?

- **Concept: LaTeX/TikZ diagram syntax and compilation**
  - Why needed here: The code representation and compiler validation depend on TikZ syntax. Understanding valid constructs, common failure modes, and debugging strategies is critical for interpreting Check Agent outputs.
  - Quick check question: Given a TikZ compilation error about "undefined control sequence," what are the most likely causes and how would you programmatically categorize them for agent feedback?

## Architecture Onboarding

- **Component map:** User provides sketch S + query Q → Sketch-to-Code Agent generates Ck → Check Agent attempts compilation → [if fail: regenerate] → [if pass: GPT-4o compares D vs S+Q] → [if mismatch: regenerate with feedback] → return final diagram
- **Critical path:** User provides sketch S + query Q → Sketch-to-Code Agent generates Ck → Check Agent attempts compilation → [if fail: regenerate] → [if pass: GPT-4o compares D vs S+Q] → [if mismatch: regenerate with feedback] → return final diagram
- **Design tradeoffs:**
  - Latency vs. accuracy: Each regeneration cycle adds API calls and compilation time; the paper does not report latency metrics
  - Closed-source dependency: GPT-4o verification creates external API dependency and cost
  - Code schema expressiveness: TikZ limits the visual vocabulary available; custom elements require workarounds
- **Failure signatures:**
  - Misaligned structures: Model fails to capture underlying object structure → incomplete or over-simplified outputs (Figure 7a)
  - Misidentified elements: Character distortion, incorrect proportions, wrong font/style (Figure 7b)
  - Misconnected relationships: Arrows connect wrong nodes, skip intermediate elements, or omit connections (Figure 7c)
- **First 3 experiments:**
  1. **Baseline component isolation:** Run Sketch-to-Code Agent alone (no Check Agent, no GPT-4o) on a held-out sketch subset to establish the performance floor and identify most common failure categories.
  2. **Compiler-only vs. compiler+GPT-4o:** Compare Pass@1 and visual fidelity metrics (FID, SSIM) between the two validation configurations to quantify the semantic verification contribution and identify cases where compiler passes but semantics fail.
  3. **Error category stress test:** Curate test cases targeting each failure signature (misaligned structures, misidentified elements, misconnected relationships) to measure per-category Pass@1 and determine which error types are most resistant to iterative refinement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the severe class imbalance in the Sketch2Diagram Benchmark (e.g., 0.03% tables vs. 52.34% model architectures) impact model performance and generalizability on underrepresented diagram types?
- Basis: [inferred] from Section 4.1, which explicitly states that the dataset reveals a significant imbalance, with flowcharts and mind maps being "underrepresented."
- Why unresolved: The paper reports aggregate performance metrics but does not provide a breakdown of results across the eight specific categories, masking potential failures in minority classes.
- What evidence would resolve it: A per-category evaluation of Pass@1 and visual fidelity metrics to compare performance on high-resource versus low-resource diagram types.

### Open Question 2
- Question: Why does the removal of the GPT-4o feedback mechanism result in a higher Pass@1 score (94.69) for the sketch editing task compared to the full model (93.12)?
- Basis: [inferred] from Table 3 and Section 5.2 (Ablation Study), which notes that removing feedback "marginally improves performance" on this specific metric.
- Why unresolved: This counter-intuitive result suggests the feedback loop may introduce noise or unnecessary corrections during code editing, but the paper does not analyze the cause of this regression.
- What evidence would resolve it: An analysis of the "Check Agent's" rejection rates and the quality of the GPT-4o feedback suggestions in the editing loop versus the generation loop.

### Open Question 3
- Question: Can the system effectively handle "misconnected relationships" (e.g., arrows bypassing nodes) through semantic validation rather than just syntactic compilation?
- Basis: [inferred] from Section 5.3 (Error Analysis), which identifies "misconnected relationships" as a distinct error class that undermines semantic integrity.
- Why unresolved: The current "Check Agent" validates code compilability and visual similarity via GPT-4o, but the persistence of these errors implies a lack of explicit structural logic verification.
- What evidence would resolve it: The integration of a graph-based logic validator or a specialized reward model that penalizes disconnected graph components even if the code compiles successfully.

## Limitations

- The evaluation framework's reliance on GPT-4o for semantic verification introduces a critical external dependency that limits reproducibility and creates potential bias in the Pass@1 metric
- The failure mode analysis shows specific error patterns but does not quantify their frequency or determine whether the multi-agent system has systematic weaknesses in certain diagram categories
- The claim of "state-of-the-art" performance lacks context as no comparison to existing sketch-to-diagram systems is provided

## Confidence

- **High confidence**: The compiler-based validation mechanism is well-specified and its contribution is clearly demonstrated through ablation studies. The architectural decomposition into specialized agents follows established multi-agent patterns.
- **Medium confidence**: The sketch recognition and code generation pipeline is well-defined, but the effectiveness of GPT-4o semantic verification is difficult to assess without access to the prompt templates and validation criteria. The 6,000-example dataset claim is supported by HuggingFace repository existence, but dataset quality and annotation consistency are unverified.
- **Low confidence**: The claim that SketchAgent achieves "state-of-the-art" performance lacks context—no comparison to existing sketch-to-diagram systems is provided, and the baseline selection methodology is not explained.

## Next Checks

1. **Compiler-semantics gap analysis**: Generate 100 compilable but visually incorrect TikZ outputs from the Sketch-to-Code Agent, then use the Check Agent pipeline to determine how many GPT-4o catches vs. passes. This quantifies the semantic verification contribution and identifies whether the compiler provides meaningful quality filtering beyond syntactic validation.

2. **Failure type distribution audit**: Categorize 500 random test set outputs into the three failure modes (misaligned structures, misidentified elements, misconnected relationships) and measure Pass@1 per category. This reveals whether the multi-agent system has systematic weaknesses and whether certain diagram categories (e.g., complex flowcharts vs. simple entity relationships) are disproportionately difficult.

3. **Feedback loop sensitivity test**: Run the editing task with GPT-4o feedback disabled, then manually inspect 50 outputs where feedback would have triggered regeneration. Determine whether feedback consistently improves semantic alignment or introduces unnecessary regeneration cycles that degrade output quality.