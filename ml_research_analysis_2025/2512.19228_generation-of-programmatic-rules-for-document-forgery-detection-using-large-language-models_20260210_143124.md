---
ver: rpa2
title: Generation of Programmatic Rules for Document Forgery Detection Using Large
  Language Models
arxiv_id: '2512.19228'
source_url: https://arxiv.org/abs/2512.19228
tags:
- document
- code
- checks
- plausibility
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper tackles the challenge of detecting document forgeries
  by generating programmatic plausibility checks using large language models (LLMs),
  specifically Llama 3.1 8B and OpenCoder 8B, fine-tuned on domain-specific data.
  The approach involves two-phase fine-tuning: first on the document verification
  system''s code base, then on existing plausibility checks and document properties.'
---

# Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models

## Quick Facts
- arXiv ID: 2512.19228
- Source URL: https://arxiv.org/abs/2512.19228
- Reference count: 27
- The paper demonstrates that fine-tuned LLMs can generate programmatic plausibility checks for document forgery detection with high success rates on controlled test queries.

## Executive Summary
This paper addresses the challenge of detecting document forgeries by leveraging large language models to generate programmatic plausibility checks. The approach uses two-phase fine-tuning of Llama 3.1 8B and OpenCoder 8B models on domain-specific code and existing plausibility checks. The resulting models can produce executable Python code that verifies document authenticity through checks like date validation and material consistency. Results show that fine-tuned models, particularly OpenCoder with combined fine-tuning, achieve high success rates in generating correct code, with OpenCoder Instruct reaching 100% success on low- and mid-complexity queries and 70% on high-complexity queries. The study demonstrates the potential of fine-tuned LLMs as scalable tools for supporting document forgery detection in security-sensitive contexts where interpretability is crucial.

## Method Summary
The paper proposes a two-phase fine-tuning approach for generating programmatic plausibility checks for document forgery detection. First, models are fine-tuned on the document verification system's codebase to learn the domain context and coding patterns. Second, they are fine-tuned on existing plausibility checks and document properties to learn specific verification patterns. The fine-tuned models, particularly OpenCoder 8B, are then used to generate executable Python code for verifying document authenticity based on textual queries. The approach is evaluated using zero-shot prompting on a test set of plausibility checks, with success measured by the correctness and executability of the generated code.

## Key Results
- Fine-tuned OpenCoder models achieve 100% success rate on low- and mid-complexity queries and 70% on high-complexity queries
- Llama models excel at integrating document properties, particularly in German language contexts
- OpenCoder with combined fine-tuning shows the highest overall performance across all complexity levels
- Generated code demonstrates high interpretability through explicit Python implementation

## Why This Works (Mechanism)
The approach works by leveraging the pattern recognition capabilities of large language models, which are fine-tuned to understand both the domain-specific code patterns and the logical structure of plausibility checks. The two-phase fine-tuning process first establishes familiarity with the verification system's architecture, then teaches the model to generate specific checks based on document properties and existing verification patterns. This enables the model to translate natural language queries about document authenticity into executable code that can systematically verify multiple aspects of document validity.

## Foundational Learning
- Fine-tuning methodology: Why needed - to adapt general-purpose LLMs to domain-specific tasks; Quick check - compare performance before and after fine-tuning
- Programmatic plausibility checks: Why needed - to create interpretable, executable verification rules; Quick check - verify generated code runs without errors
- Document verification systems: Why needed - to understand the context and constraints of the verification task; Quick check - examine the structure of existing verification code
- Zero-shot prompting: Why needed - to test model generalization without additional training; Quick check - measure success rate on unseen queries
- Multi-language processing: Why needed - to handle documents in different languages; Quick check - compare performance across language variants
- Code generation evaluation: Why needed - to assess the practical utility of generated rules; Quick check - validate correctness of generated code

## Architecture Onboarding

Component map: User query -> LLM fine-tuned on code base -> LLM fine-tuned on plausibility checks -> Executable Python code -> Document verification system

Critical path: The generation of programmatic plausibility checks relies on the successful execution of both fine-tuning phases, followed by the model's ability to translate natural language queries into correct Python code that can be executed by the verification system.

Design tradeoffs: The paper balances between model capability (using larger 8B parameter models) and practical deployment considerations (generating interpretable Python code rather than using model outputs directly). The choice of two-phase fine-tuning allows for both domain adaptation and task-specific learning, though this requires more training data and computational resources than single-phase approaches.

Failure signatures: Model failures manifest as either syntactically incorrect Python code, logically flawed verification checks, or code that cannot be executed by the verification system. Lower success rates on high-complexity queries indicate limitations in handling more intricate verification requirements.

First experiments:
1. Evaluate model performance on a validation set of plausibility checks not seen during training
2. Test the executability of generated code within the document verification system
3. Compare success rates across different query complexities and document types

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions remain about the generalizability of the approach to different document types and verification contexts, the robustness of generated checks against adversarial forgeries, and the scalability of the fine-tuning approach to larger or more diverse document verification systems.

## Limitations
- Limited generalizability beyond specific document types and verification contexts used in training
- Evaluation focused on code generation success rather than actual forgery detection accuracy in real-world scenarios
- Performance differences between Llama and OpenCoder models suggest potential bias toward German document properties
- No assessment of adversarial scenarios where forgers might adapt to circumvent generated checks

## Confidence
- High confidence in the claim that fine-tuned LLMs can effectively generate programmatic plausibility checks for document forgery detection, given the strong performance on controlled test queries
- High confidence in the claim that interpretability is maintained through code generation, as the generated Python code is inherently interpretable
- Medium confidence in the scalability claim, as the study demonstrates technical feasibility but does not provide deployment cost or latency analysis

## Next Checks
1. Evaluate the generated plausibility checks against a diverse, real-world dataset of both genuine and forged documents across multiple languages and document types to assess practical detection accuracy
2. Conduct adversarial testing where simulated forgeries attempt to bypass the generated rules to identify potential vulnerabilities in the approach
3. Perform a cross-lingual transfer study using the fine-tuned models on document properties from languages not represented in the training data to measure generalization capabilities