---
ver: rpa2
title: 'Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM
  Prompt Optimization for NOTAM Semantic Parsing'
arxiv_id: '2511.12630'
source_url: https://arxiv.org/abs/2511.12630
tags:
- notam
- field
- information
- aviation
- fields
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NOTAM semantic parsing, a task extending
  beyond traditional information extraction to achieve deep understanding of aviation
  safety communications. The authors construct Knots, a 12,347-annotation dataset
  covering 194 Flight Information Regions, and propose a multi-agent collaborative
  framework (MDA-HDF) for automated field discovery.
---

# Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing

## Quick Facts
- arXiv ID: 2511.12630
- Source URL: https://arxiv.org/abs/2511.12630
- Reference count: 40
- Introduces NOTAM semantic parsing task and Knots dataset with 12,347 expert annotations across 194 Flight Information Regions

## Executive Summary
This paper addresses NOTAM semantic parsing, extending beyond traditional information extraction to achieve deep understanding of aviation safety communications. The authors construct Knots, a 12,347-annotation dataset covering 194 Flight Information Regions, and propose a multi-agent collaborative framework (MDA-HDF) for automated field discovery. Their systematic evaluation of prompt engineering strategies shows that 5-shot In-Context Learning achieves F1-scores up to 96%, while the MDA-HDF framework achieves 92% F1 in automated field discovery. The work provides concrete guidance for practical implementation, demonstrating that domain-specific optimizations and deterministic temperature settings are essential for achieving safety-critical reliability in aviation applications.

## Method Summary
The method involves two main components: dataset construction and multi-agent framework development. The Knots dataset was created through systematic sampling across 194 Flight Information Regions, with expert annotators providing inferential annotations that capture domain knowledge beyond literal text. The MDA-HDF framework consists of sequential multi-agent discovery (MDA) with Discovery, Analysis, and Validation agents, followed by Hybrid Debate Framework (HDF) refinement with Consolidation/Terminology experts, a Critic Agent, and deterministic FieldManager. The parsing pipeline uses 5-shot In-Context Learning with temperature=0.0 for deterministic generation, evaluated across five aviation domains using strict entity-level F1 requiring exact match on field types and normalized values.

## Key Results
- 5-shot In-Context Learning achieves F1-scores up to 96% (Gemini-2.5-Flash) on NOTAM parsing
- MDA-HDF framework achieves 92% F1 in automated field discovery with 96% recall
- Temperature=0.0 consistently outperforms stochastic sampling across all domains
- HDF improves precision from 84% to 92% by reducing "echo chamber" and "tyranny of majority" failure modes

## Why This Works (Mechanism)

### Mechanism 1: Sequential Multi-Agent Discovery (MDA)
MDA expands conceptual coverage beyond single-LLM approaches through complementary agent roles. Three specialized agents (Discovery, Analysis, Validation) process NOTAMs sequentially, each enriching the previous output. Discovery performs surface scanning, Analysis applies domain knowledge for semantic inference, and Validation filters for consistency and business relevance. A Consensus Aggregator merges outputs using Jaccard similarity (τ=0.7). The core assumption is that agents possess complementary knowledge (ϕ_discover, ϕ_analyst, ϕ_validator) enabling coverage of distinct latent concept spaces. Evidence shows MDA achieves 96% recall; precision improves from 84% to 92% when HDF is added.

### Mechanism 2: Hybrid Debate Framework (HDF)
HDF improves precision by counteracting "echo chamber" and "tyranny of majority" failure modes through adversarial critique and deterministic consolidation. Two expert agents propose improvements (structural consolidation, terminology refinement). An independent Critic Agent challenges flawed proposals. FieldManager applies only unchallenged proposals via deterministic rules—not probabilistic voting—preventing majority override of correct minority views. The core assumption is that adversarial critique reduces posterior probability of erroneous concepts; partitioning into orthogonal subtasks isolates dominance effects. Evidence shows ablation of HDF drops precision from 92% to 84%, and Theorem 1 formally proves HDF reduces convergence to incorrect majority concepts.

### Mechanism 3: 5-shot In-Context Learning
5-shot ICL outperforms zero-shot, CoT, and self-consistency for NOTAM parsing because domain-specific examples encode implicit aviation knowledge and output format constraints. Strategically selected examples demonstrate terminology normalization ("RWY 09L CLSD" → runway closure), JSON structure, and complex inference patterns. This activates pre-trained aviation knowledge while reducing parsing errors from ambiguous instructions. The core assumption is that the model possesses latent aviation knowledge from pre-training; examples primarily guide retrieval and formatting rather than teaching from scratch. Evidence shows 5-shot ICL achieves 96.3% F1 (Gemini-2.5-Flash) vs. 68.5% zero-shot on Airspace Management.

## Foundational Learning

- **Concept:** In-Context Learning (ICL)
  - **Why needed here:** Core mechanism for NOTAM parsing; understanding why 5-shot beats zero-shot is essential for prompt design.
  - **Quick check question:** Can you explain why adding examples might hurt performance if they conflict with model priors?

- **Concept:** Multi-Agent Collaboration Patterns
  - **Why needed here:** MDA-HDF architecture relies on role specialization and adversarial critique; misunderstanding these leads to poor agent design.
  - **Quick check question:** What failure mode occurs when all agents use identical prompts?

- **Concept:** Temperature and Deterministic Generation
  - **Why needed here:** Safety-critical aviation systems require reproducible outputs; paper shows temperature=0.0 consistently outperforms stochastic sampling.
  - **Quick check question:** Why might self-consistency (majority voting across temperatures) fail to improve results in structured extraction tasks?

## Architecture Onboarding

- **Component map:**
  - MDA Stage: DiscoveryAgent → AnalysisAgent → ValidationAgent → ConsensusAggregator (τ=0.7)
  - HDF Stage: ConsolidationExpert + TerminologyExpert (parallel proposals) → CriticAgent (adversarial review) → FieldManager (deterministic application)
  - Parsing Pipeline: NOTAM input → 5-shot ICL prompt (temperature=0.0) → JSON output

- **Critical path:** MDA stage must complete field discovery before HDF refinement begins. For parsing, temperature must be set to 0.0 before any production deployment.

- **Design tradeoffs:**
  - Recall vs. Precision: MDA maximizes recall (96%), HDF trades some recall (93%) for higher precision (92%)
  - Complexity vs. Debuggability: Multi-agent system is complex but modular with JSON-traceable outputs
  - Few-shot vs. Fine-tuning: Paper focuses on prompt engineering; Knots dataset enables future supervised fine-tuning

- **Failure signatures:**
  - High recall, low precision → HDF stage may be disabled or Critic Agent too permissive
  - Inconsistent outputs across runs → Temperature not set to 0.0
  - Poor performance on Airspace/Flight Hazard domains → These require more instructions (7.2 avg) and longer inputs; may need domain-specific prompt tuning

- **First 3 experiments:**
  1. Replicate 5-shot ICL baseline on one domain (e.g., Landing Aid) with temperature=0.0; verify F1 ~95-96%
  2. Ablate HDF: run MDA without CriticAgent on 50 NOTAMs; measure precision drop
  3. Test temperature sensitivity: run same parsing task at 0.0, 0.3, 0.7, 1.0; confirm monotonic F1 degradation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent can supervised fine-tuning on the Knots dataset enhance performance compared to the prompt engineering baselines?
- **Basis in paper:** [explicit] The authors state, "A primary avenue is to leverage the Knots dataset for supervised fine-tuning of Large Language Models (LLMs), which is expected to further enhance model performance and domain adaptation."
- **Why unresolved:** The current study focuses exclusively on prompt engineering (zero-shot, ICL, CoT) to establish "out-of-the-box" baselines, leaving the potential gains from weight updates unexplored.
- **What evidence would resolve it:** Comparative benchmarks showing F1-scores of fine-tuned models (e.g., Llama, Mistral) against the reported 5-shot ICL baselines on the Knots test set.

### Open Question 2
- **Question:** Does the MDA-HDF multi-agent framework generalize effectively to other safety-critical domains without extensive architectural re-engineering?
- **Basis in paper:** [explicit] The authors note, "the generalizability of our multi-agent framework may be constrained, as its architecture and expert-crafted prompts were specifically tailored to the semantic nuances of NOTAMs."
- **Why unresolved:** While the framework is theoretically robust, its reliance on aviation-specific agent roles creates uncertainty regarding its transferability.
- **What evidence would resolve it:** Successful application of the MDA-HDF pipeline to distinct safety-critical datasets (e.g., maritime alerts or medical incident reports) achieving comparable field discovery F1-scores (>90%).

### Open Question 3
- **Question:** Can Selective Refinement via Contrastive Validation (SRCV) be optimized to prevent "collateral" errors in previously accurate fields?
- **Basis in paper:** [explicit] The authors report that SRCV "occasionally introduces unintended side effects in previously accurate extractions" and conclude that "future work should focus on developing more precise targeting criteria."
- **Why unresolved:** The current implementation of SRCV acts as a blunt instrument that may correct one field while degrading another, limiting its reliability in safety-critical operations.
- **What evidence would resolve it:** An ablation study demonstrating a modified SRCV mechanism that achieves a net positive F1-score on complex NOTAMs without any degradation in static field accuracy.

### Open Question 4
- **Question:** What specific mechanisms are required to close the performance gap in NOTAMs requiring deep inferential reasoning?
- **Basis in paper:** [explicit] The conclusion acknowledges, "A performance gap remains in fully resolving the task, particularly for NOTAMs involving highly complex linguistic structures or deep inferential reasoning."
- **Why unresolved:** Advanced strategies like Chain-of-Thought showed inconsistent improvements, suggesting current LLM reasoning capabilities or prompt structures are insufficient for the most complex aviation scenarios.
- **What evidence would resolve it:** Identification of a prompting strategy or model architecture that consistently achieves >90% F1 on the "Airspace Management" and "Flight Hazard" subsets, which currently have the lowest scores.

## Limitations
- Relies heavily on a single proprietary dataset (Knots) specific to aviation NOTAMs, limiting generalizability to other domains
- Ablation studies provide evidence for individual mechanisms but lack comprehensive testing of all component interactions
- Multi-agent collaboration effectiveness depends on agent knowledge diversity, which may not transfer to domains with different knowledge structures

## Confidence

**High Confidence** (supported by strong experimental evidence):
- 5-shot In-Context Learning with temperature=0.0 achieving 96% F1 on NOTAM parsing
- HDF framework improving precision from 84% to 92% when added to MDA
- Temperature sensitivity showing deterministic generation consistently outperforms stochastic approaches

**Medium Confidence** (reasonable but less comprehensive evidence):
- MDA framework achieving 96% recall in field discovery
- Multi-agent collaboration providing complementary coverage beyond single-LLM approaches
- Domain-specific prompt engineering requirements (7.2 avg instructions for Airspace vs 4.3 for Ground Facility)

**Low Confidence** (limited or theoretical evidence):
- Theoretical guarantees about HDF preventing "tyranny of majority" (no empirical validation provided)
- Generalization of 5-shot superiority to other structured extraction domains
- Long-term stability of multi-agent agent performance across different NOTAM distributions

## Next Checks

1. **Cross-domain transferability test**: Apply the 5-shot ICL + temperature=0.0 approach to another structured extraction task (e.g., medical coding or legal document parsing) to verify if the performance gains generalize beyond aviation NOTAMs.

2. **Complete system ablation**: Run a full end-to-end evaluation of MDA + HDF as an integrated system on the 500-sample field discovery test set, measuring both precision and recall, rather than evaluating components separately.

3. **Temperature sensitivity in adversarial setting**: Test whether temperature=0.0 remains superior when the Critic Agent in HDF is replaced with a stochastic reviewer, to validate that deterministic generation is essential for the entire multi-agent pipeline, not just the initial parsing stage.