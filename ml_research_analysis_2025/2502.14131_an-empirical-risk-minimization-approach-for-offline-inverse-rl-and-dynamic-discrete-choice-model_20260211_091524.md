---
ver: rpa2
title: An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic
  Discrete Choice Model
arxiv_id: '2502.14131'
source_url: https://arxiv.org/abs/2502.14131
tags:
- function
- learning
- equation
- reward
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an empirical risk minimization framework\
  \ for offline inverse reinforcement learning and dynamic discrete choice models\
  \ that circumvents the need for explicit transition function estimation. The key\
  \ innovation is jointly minimizing negative log-likelihood and mean squared Bellman\
  \ error terms, which together satisfy the Polyak-\u0141ojasiewicz condition enabling\
  \ global convergence guarantees."
---

# An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model

## Quick Facts
- **arXiv ID:** 2502.14131
- **Source URL:** https://arxiv.org/abs/2502.14131
- **Reference count:** 40
- **Primary result:** GLADIUS achieves 0.12-0.84% MAPE in low-dim Rust bus engine problem and maintains sub-linear error growth in high-dim settings

## Executive Summary
This paper introduces an empirical risk minimization framework for offline inverse reinforcement learning and dynamic discrete choice models that circumvents the need for explicit transition function estimation. The key innovation is jointly minimizing negative log-likelihood and mean squared Bellman error terms, which together satisfy the Polyak-Łojasiewicz condition enabling global convergence guarantees. The proposed GLADIUS algorithm uses alternating gradient ascent-descent to solve this minimax problem and achieves O(1/T) optimization error and O(1/√N) statistical error. In synthetic experiments on the Rust bus engine replacement problem, GLADIUS achieves mean absolute percentage errors of 0.12-0.84% for low-dimensional settings and maintains sub-linear error growth in high-dimensional settings, outperforming or matching benchmark methods including oracle approaches.

## Method Summary
The method reformulates inverse RL/DDC as a joint minimization of negative log-likelihood and mean squared Bellman error through a minimax formulation. The core innovation is introducing a dual variable ζ via the bi-conjugate trick to eliminate the double-sampling bias inherent in TD-based Bellman error estimation. This creates an alternating optimization problem: gradient ascent on ζ to estimate E[V_Q(s')|s,a], followed by gradient descent on Q to minimize the combined loss. The empirical risk minimization framework enables O(1/T) optimization error and O(1/√N) statistical error under the PL condition, which is satisfied when both NLL and Bellman error individually satisfy PL and share the same minimizer.

## Key Results
- GLADIUS achieves 0.12-0.84% MAPE on Rust bus engine problem with 500-1000 trajectories in low-dimensional settings
- Maintains sub-linear error growth (O(1/√N)) as dimensionality increases from 2 to 100+ state variables
- Outperforms or matches oracle methods including SAmQ and IQ-Learn in both low and high-dimensional settings
- Bellman residual and NLL satisfy Polyak-Łojasiewicz condition enabling global convergence at rate O(1/T)

## Why This Works (Mechanism)

### Mechanism 1: Joint Minimization of NLL and Bellman Error Satisfies PL Condition
The expected risk minimization objective (negative log-likelihood + mean squared Bellman error) satisfies the Polyak-Łojasiewicz (PL) condition, enabling global convergence without strong convexity. Individually, both L_NLL and L_BE satisfy PL under Assumption 5 (bounded gradients/Jacobians for function class Q_θ). Critically, Lemma 25 shows their sum remains PL because minimizers intersect at Q*. This permits gradient-based optimization to converge globally at rate O(1/T). Core assumption: Realizability (Q_θ contains Q*) and smooth parametrization with bounded Jacobian singular values ≥ √μ.

### Mechanism 2: Bi-Conjugate Reformulation Eliminates Transition Function Estimation
Introducing dual variable ζ via the bi-conjugate trick removes the double-sampling bias inherent in naive TD-based Bellman error estimation. E[(TQ - Q)²] ≠ E[(ŤTQ - Q)²] due to irreducible variance from stochastic transitions. Reformulating as a minimax problem over ζ yields an unbiased estimator: ζ* converges to E[V_Q(s')|s,a], recovering the transition-dependent expectation implicitly from data without explicit P estimation. Core assumption: Sufficient coverage of state-action pairs in offline data D; ζ class is rich enough to represent conditional expectations.

### Mechanism 3: Anchor Action Identification Ensures Reward Uniqueness
Known rewards for anchor actions a_s per state uniquely identify Q* and r from policy observations. Policy alone only identifies Q up to state-wise constants (softmax is invariant to adding c(s)). Anchor action Bellman equations pin these constants by linking Q(s, a_s) to known r(s, a_s) + β·E[V_Q(s')|s,a_s], yielding unique r recovery via r = Q - β·E[V_Q]. Core assumption: Assumption 3 holds—anchor reward is known for each reachable state.

## Foundational Learning

- **Concept: Bellman Operator and Bellman Residual**
  - Why needed: The entire framework minimizes squared Bellman error; you must understand TQ - Q = 0 as the consistency condition defining optimal Q*.
  - Quick check: Given Q(s,a) and transition kernel P, can you write out TQ(s,a)? Can you explain why Q* is the unique fixed point?

- **Concept: Maximum Entropy IRL ↔ Dynamic Discrete Choice Equivalence**
  - Why needed: The paper unifies these literatures; understanding that entropy-regularized RL with λ=1 is formally equivalent to DDC with T1EV noise (δ=-γ) is essential for interpreting results.
  - Quick check: In both frameworks, what is π*(a|s) in terms of Q*? Why does this matter for the negative log-likelihood term?

- **Concept: Polyak-Łojasiewicz Condition**
  - Why needed: This weaker-than-convexity condition is the theoretical engine enabling global convergence. You must distinguish PL from strong convexity: PL requires ½‖∇f‖² ≥ μ(f - f*), not positive definite Hessians everywhere.
  - Quick check: If a function is PL with constant μ, what convergence rate does gradient descent achieve? Why is this surprising for non-convex objectives?

## Architecture Onboarding

- **Component map:** Q_θ₂ (s,a) → Q-values, ζ_θ₁ (s,a) → E[V_Q(s')|s,a], V_Q(s) = log Σ_a exp(Q(s,a)), Loss = L_NLL + L_TD - β²·L_dual

- **Critical path:**
  1. Ascent step: Update ζ_θ₁ to minimize (V_Q(s') - ζ(s,a))² → ζ approaches E[V_Q|s,a]
  2. Descent step: Update Q_θ₂ to minimize -log p_Q(a|s) + squared TD error (corrected by ζ)
  3. Extract r: Compute r̂(s,a) = Q̂(s,a) - β·ζ̂(s,a)

- **Design tradeoffs:**
  - Neural network Q vs. linear: Non-parametric enables scalability but forfeits extrapolation to unseen state-action pairs
  - Batch size B₁, B₂ vs. full dataset D: Batches reduce memory from O(|D|) to O(|B|) but introduce gradient noise
  - Deterministic transitions special case: If P is deterministic, ζ ascent is unnecessary—Algorithm 2 simplifies to single descent

- **Failure signatures:**
  - MAPE ≈ 100%+ with IQ-Learn/BC: Bellman equation not enforced; Q does not satisfy consistency
  - Divergence on high-dimensional states with SAmQ: Memory overflow from transition probability estimation
  - GLADIUS extrapolation error on rare states: Neural networks overfit to high-frequency (s,a) pairs; oracle linear methods extrapolate better

- **First 3 experiments:**
  1. Reproduce Rust bus engine (low-dim): Train on 500-1000 trajectories, verify MAPE < 1% and compare Q̂ across mileages 1-10 against oracle baselines
  2. High-dimensional stress test: Append K=20-50 dummy state variables; confirm sub-linear MAPE growth and that SAmQ/IQ-Learn fail or degrade
  3. Ablate anchor actions: Remove anchor reward knowledge and observe non-unique reward recovery; verify counterfactual simulation fails

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the GLADIUS algorithm to violations of the expert optimality assumption, specifically regarding suboptimal or noisy demonstrations? The theoretical convergence relies on the data distribution matching π*, yet real-world "expert" data rarely satisfies perfect optimality. It is unclear how deviations (e.g., exploration noise, rational inattention) affect the PL condition or estimation bias.

### Open Question 2
Can the theoretical insights regarding the PL condition of the Bellman residual be formally extended to guarantee global convergence for general gradient-based Offline Reinforcement Learning algorithms? While the property holds for the IRL/DDC inverse problem, the landscape of the forward RL objective under function approximation is notoriously complex and prone to instability.

### Open Question 3
Can the requirement for known "anchor action" rewards be relaxed or replaced with weaker structural assumptions for reward identification? The current identification theorem depends mathematically on these fixed points to resolve the constant offset inherent in the Bellman equation. In high-dimensional or unstructured domains, determining these anchor values a priori may be impractical.

## Limitations
- PL condition relies on strong realizability and smoothness assumptions that may not hold in practice, particularly for complex state spaces or neural network parameterizations with poor initialization
- Bi-conjugate trick's practical performance depends critically on the expressiveness of the ζ network and sufficient state-action coverage in offline data
- Anchor action assumption (known rewards for specific state-action pairs) is restrictive and may be impractical in high-dimensional or unstructured domains

## Confidence

- **High confidence**: Convergence rate guarantees (O(1/T) optimization, O(1/√N) statistical error) under stated assumptions, as these follow directly from established PL theory
- **Medium confidence**: Synthetic experiment results (MAPE values, relative performance against benchmarks), since synthetic settings allow controlled validation but may not capture real-world complexities
- **Low confidence**: Scalability claims to truly high-dimensional problems and the practical necessity of the bi-conjugate reformulation, as neither is thoroughly tested beyond the synthetic Rust bus engine setup

## Next Checks

1. Test GLADIUS on a non-synthetic environment with continuous states (e.g., MuJoCo) to verify the practical value of the bi-conjugate trick beyond synthetic setups
2. Conduct ablation studies removing anchor actions to quantify the impact on reward identification and assess alternative identification strategies
3. Evaluate GLADIUS under data coverage violations (e.g., missing state-action pairs) to understand robustness boundaries when the offline assumption is violated