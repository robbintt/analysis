---
ver: rpa2
title: 'Scaling with Collapse: Efficient and Predictable Training of LLM Families'
arxiv_id: '2509.25087'
source_url: https://arxiv.org/abs/2509.25087
tags:
- training
- loss
- arxiv
- preprint
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that when training large language models under\
  \ maximal update parameterization, the shapes of training loss curves (TLCs) across\
  \ model sizes can collapse onto a universal trajectory\u2014provided the AdamW timescale\
  \ (\u03C4), tokens-per-parameter (TPP), and learning rate schedule are held constant\
  \ or optimally scaled together. This collapse acts as a signature of compute-efficient\
  \ training."
---

# Scaling with Collapse: Efficient and Predictable Training of LLM Families

## Quick Facts
- **arXiv ID**: 2509.25087
- **Source URL**: https://arxiv.org/abs/2509.25087
- **Reference count**: 40
- **Primary result**: Training loss curves collapse across model sizes when AdamW timescale, tokens-per-parameter, and learning rate schedule are held constant, enabling early diagnostics and stopping in hyperparameter tuning.

## Executive Summary
This paper introduces a method for training LLM families where normalized training loss curves collapse onto a universal trajectory across model sizes. This collapse occurs precisely when optimization hyperparameters are set optimally for the given data budget, specifically when AdamW timescale (τ), tokens-per-parameter (TPP), and learning rate schedule are held constant. The authors demonstrate two practical applications: using deviation from collapse as an early diagnostic for training pathologies, and enabling early stopping in hyperparameter tuning by predicting final loss from partial runs. They introduce Celerity, a competitive LLM family trained under collapse conditions, showing improved efficiency and reliability through fixed-TPP bands with optimal τ.

## Method Summary
The method trains LLM families under maximal update parameterization (μP/CompleteP) with fixed tokens-per-parameter (TPP) bands and optimally scaled AdamW timescale (τ). Models are trained with linear decay-to-zero learning rate schedules and normalized training loss curves are monitored for collapse across scales. A parametric model of TLC shape enables early stopping in hyperparameter tuning, while deviation-from-collapse residuals provide early diagnostics of training issues. The Celerity family uses GPT2-style decoder architecture with ALiBi embeddings, SwiGLU FFN, and CompleteP parameterization.

## Key Results
- Normalized training loss curves collapse across model sizes when τ, TPP, and LR schedule are held constant or optimally scaled
- Deviation-from-collapse residuals detect training pathologies earlier than raw loss monitoring (e.g., 60% vs 90% in Celerity 1.8B example)
- Early stopping predicts final loss within 10-30% training, saving significant hyperparameter tuning compute
- Celerity models trained under collapse conditions achieve competitive downstream performance
- Fixed-TPP bands with optimal τ improve parameter efficiency and training reliability

## Why This Works (Mechanism)

### Mechanism 1: Training Loss Curve (TLC) Collapse via Normalized Controls
When AdamW timescale (τ), tokens-per-parameter (TPP), and learning rate schedule are held constant, normalized training loss curves collapse onto a universal trajectory across model sizes. This occurs because τ controls the bias-variance trade-off, TPP sets the power-law decay rate, and the LR schedule phases when bias vs variance reduction occurs. Together, these three normalized controls yield scale-invariant curve shapes.

### Mechanism 2: Deviation-from-Collapse as an Early Diagnostic
Residuals between an in-progress TLC and a reference collapsed curve reveal training pathologies earlier than raw loss trends. When collapse holds, any divergence indicates deviation from expected scale-invariant dynamics—often due to numerical instabilities, data issues, or configuration changes.

### Mechanism 3: Early Stopping via Predicted Final Loss
Fitting a parametric model to normalized TLCs at small scales enables accurate prediction of final loss from 10–30% of training. The fitted normalizer L(T) serves as the estimated final loss, allowing early selection of optimal hyperparameter settings.

## Foundational Learning

- **Maximal update parameterization (μP) / CompleteP**: Enables transfer of base hyperparameters across widths/depths; collapse depends on consistent dynamics under μP-like parameterization. *Quick check*: Can you explain why μP stabilizes feature learning at large widths?

- **AdamW timescale τ**: τ = 1/(ηλT) governs bias-variance trade-off and is a primary control of TLC shape; setting τ optimally for a given TPP yields collapse. *Quick check*: If you double batch size B and keep η, λ, T constant, how does τ change and what happens to the variance floor?

- **Power-law scaling in LLM training (Chinchilla)**: TPP is grounded in compute-optimal scaling (~20 TPP); understanding power-law loss helps interpret how TPP modulates TLC shape. *Quick check*: For a fixed compute budget, if you double model size, approximately how does optimal token count scale under Chinchilla?

## Architecture Onboarding

- **Component map**: Training loss curves (TLCs) -> Normalized controls (τ, TPP, LR schedule) -> Collapse monitor (early-align, residuals) -> Predictive model (Eq. 4) -> Celerity stack (GPT2-style + ALiBi + SwiGLU + CompleteP)

- **Critical path**: 1) Choose TPP band based on compute vs parameter-efficiency trade-off; 2) Tune τ on proxy model at target TPP; 3) Transfer HPs to larger models via CompleteP scaling; 4) Train fixed-TPP band with common optimal τ; 5) Use residuals for monitoring; 6) For HPO, fit predictive model at small scale for early stopping

- **Design tradeoffs**: Fixed TPP vs varying TPP (enables collapse vs optimal compute allocation), D2Z vs constant LR (parameter-efficient vs simpler tuning), CompleteP vs μP (accounts for depth scaling vs simpler width-only scaling)

- **Failure signatures**: Non-collapsing curves (check τ/TPP consistency), late divergence in residuals (numerical kernels, data issues), prediction errors (loss spikes, high noise)

- **First 3 experiments**: 1) Reproduce collapse at 20 TPP across 111M, 266M, 610M models; 2) Early diagnostic simulation with synthetic issue at 50% training; 3) Early stopping validation on 3.3B λ sweep using 111M TLC fits

## Open Questions the Paper Calls Out

- **Can collapse/generalization extend to optimizers without natural EMA form?** Extending timescale analysis to Adagrad, Adafactor, SGD variants is important but unresolved since current derivation relies on AdamW's specific update rule.

- **How do dynamic data regimes affect collapsed trajectory universality?** Shifts in data distribution (curricula, late-stage annealing) may decouple train-loss collapse from downstream behavior and break the universal dynamics.

- **Do predictive model parameters vary across learning rate schedule architectures?** Whether b(τ), q(TPP), and m vary systematically across cosine decay, warmup-stable-decay, or schedule-free schemes remains unknown.

## Limitations

- Collapse mechanism requires holding τ, TPP, and LR schedule constant, limiting flexibility in deployment scenarios and optimal compute allocation
- Predictive model for early stopping requires further validation at larger scales and across different model families
- Early diagnostic capability depends on having a well-specified reference curve; mis-specification could lead to false positives
- Training must be restricted to fixed-TPP bands rather than allowing dynamic adjustment for optimal resource use

## Confidence

**High Confidence**: The theoretical framework linking τ, TPP, and TLC shape is well-supported by the noisy quadratic model and empirical verification across multiple model sizes (111M-3.9B).

**Medium Confidence**: The early diagnostic capability is demonstrated through a single real-world example; broader validation across diverse failure modes would strengthen this claim.

**Medium Confidence**: The early stopping methodology shows promising results on small-scale hyperparameter sweeps but requires scaling validation to larger models and more complex hyperparameter spaces.

## Next Checks

1. **Cross-family collapse verification**: Train a different LLM architecture family under the same collapse conditions to test whether normalized TLC collapse generalizes beyond the Celerity architecture.

2. **Large-scale early stopping validation**: Implement the early stopping methodology on a full-scale hyperparameter sweep (10+ settings on 10B+ parameter models) to verify computational savings and prediction accuracy at production scale.

3. **Failure mode robustness testing**: Systematically inject various training pathologies at different training stages in models of varying sizes, then validate whether collapse residuals consistently detect issues earlier than traditional monitoring methods across all cases.