---
ver: rpa2
title: 'ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models'
arxiv_id: '2510.23558'
source_url: https://arxiv.org/abs/2510.23558
tags:
- instruction
- audio
- lalms
- sensitivity
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ISA-Bench is a benchmark that evaluates instruction sensitivity
  in large audio language models (LALMs) across three dimensions: instruction description,
  output format, and task composition. The benchmark reveals that even state-of-the-art
  LALMs struggle with instruction sensitivity, showing significant performance degradation
  when instructions vary.'
---

# ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models

## Quick Facts
- arXiv ID: 2510.23558
- Source URL: https://arxiv.org/abs/2510.23558
- Reference count: 0
- ISA-Bench reveals LALMs are highly sensitive to instruction variations, with state-of-the-art models showing 50-80% relative performance scores across three instruction dimensions.

## Executive Summary
ISA-Bench introduces a benchmark for evaluating instruction sensitivity in Large Audio Language Models (LALMs) across three dimensions: instruction description (D), output format (F), and task composition (N). The benchmark reveals that even state-of-the-art LALMs struggle with instruction sensitivity, showing significant performance degradation when instructions vary. Experiments with nine LALMs on five tasks show average instruction-following rates of 70-85% and relative performance scores of 50-80%, depending on the dimension and model. While supervised fine-tuning on diverse instruction variants improves compliance by up to 100%, it also causes catastrophic forgetting, where models lose previously mastered task capabilities. This indicates that current LALMs are not robust to instruction variations, and more sophisticated solutions are needed to improve instruction-following robustness in real-world deployment scenarios.

## Method Summary
ISA-Bench evaluates nine LALMs across five public datasets (LibriSpeech, CoV oST2, IEMOCAP, AudioCaps) using three instruction dimensions: description variants (case, semantic complexity, syntax errors), format variants (answer-only, case-sensitive, prefix/suffix, JSON), and task composition (2-task and 3-task combinations). The benchmark uses compliance-aware metrics (MetricIF) that score task performance only when outputs satisfy required formats, along with instruction-following rate (IFR) and relative performance score (RPS). Mitigation experiments involve supervised fine-tuning on diverse instruction variants using GPT-4-generated rewrites, though this approach triggers catastrophic forgetting of previously mastered capabilities.

## Key Results
- LALMs show 50-80% relative performance scores across D, F, and N dimensions, with no single model excelling universally
- Instruction-following rates average 70-85% across models, dropping significantly for complex format constraints
- SFT on diverse instruction variants improves IFR by up to 100% but causes catastrophic forgetting of task capabilities
- Gemini-2.5-Pro handles D and F dimension variations well but fails under JSON-style formatting constraints
- Qwen2-Audio performs best on ASR in D-dimension but worst in F-dimension, highlighting inconsistent model capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Varying instruction descriptions degrades LALM task performance even when task intent remains unchanged.
- **Mechanism:** LALMs trained on specific instruction phrasings during SFT develop overfitting to those surface forms; when deployment instructions diverge (case changes, syntax errors, semantic complexity shifts), the model's learned instruction-to-behavior mapping fails to generalize, producing lower compliance and accuracy.
- **Core assumption:** Instruction-following capability in LALMs is partly memorized rather than abstractly understood.
- **Evidence anchors:** [abstract] "existing LALMs are highly sensitive to how instructions are phrased, affecting both (i) instruction-following rates and (ii) task performance"; [section 1] "published evaluations of LALMs mostly use instruction forms that are seen during supervised fine-tuning (SFT), giving an upper-bound of performance estimate"

### Mechanism 2
- **Claim:** Output format constraints create a dual-objective burden where models must simultaneously satisfy structural compliance and acoustic task accuracy.
- **Mechanism:** LALMs must allocate representational capacity to both (a) parsing and executing format requirements (JSON structure, prefix/suffix tags, case constraints) and (b) performing the core audio understanding task. When format complexity increases, models with limited instruction-following training fail to maintain both objectives, resulting in either format-compliant but inaccurate outputs or accurate but non-compliant responses.
- **Core assumption:** Format compliance and task accuracy compete for model capacity during inference.
- **Evidence anchors:** [section 3.4] "MetricIF credits task performance only when the response satisfies the required format"; [section 4.2] "Gemini-2.5-Pro handles most variations in the D and F dimensions, but fails under JSON-style formatting constraints"

### Mechanism 3
- **Claim:** Supervised fine-tuning on diverse instruction variants improves compliance but triggers catastrophic forgetting of previously mastered task capabilities.
- **Mechanism:** When LALMs are fine-tuned on new instruction style variants, gradient updates modify shared weights that also encode previously learned task competencies. Without explicit retention mechanisms (replay data, regularization, or parameter-isolation strategies), these updates overwrite task-specific knowledge, causing models to either reproduce training-like responses or refuse to answer on out-of-distribution samples.
- **Core assumption:** Task knowledge and instruction-style knowledge share overlapping parameters without isolation.
- **Evidence anchors:** [abstract] "supervised fine-tuning on diverse instruction variants improves compliance by up to 100%, it also causes catastrophic forgetting"; [section 4.3] "models might suffer catastrophic forgetting cases: they lose previously mastered capabilities when fine-tuned on new instruction variants, only reproducing a few responses similar to those seen during training"

## Foundational Learning

- **Concept: LALM Architecture (Audio Encoder → Projector → LLM)**
  - **Why needed here:** ISA-Bench evaluates end-to-end systems; understanding where instruction processing occurs (LLM backbone) versus where acoustic features are extracted (encoder) clarifies why instruction sensitivity manifests despite robust audio perception.
  - **Quick check question:** If you froze the audio encoder and only fine-tuned the LLM backbone on instruction variants, would you expect D-dimension sensitivity to change?

- **Concept: Compliance-Aware Metrics (MetricIF)**
  - **Why needed here:** The benchmark doesn't just measure task accuracy—it penalizes non-compliant outputs by scoring them as empty responses. This dual evaluation reveals where models fail (format vs. content).
  - **Quick check question:** A model achieves 90% WER accuracy but only 60% format compliance—what is its likely MetricIF score relative to a model with 80% accuracy and 95% compliance?

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - **Why needed here:** The paper's mitigation experiments show SFT improves instruction-following but degrades prior capabilities. Recognizing this trade-off is essential before proposing solutions.
  - **Quick check question:** Why might training on diverse instruction variants overwrite task-specific weights rather than augmenting them?

## Architecture Onboarding

- **Component map:** Audio Input → Audio Encoder → Projector Layer → LLM Embedding Space ← Text Instruction → LLM Backbone → Text Output

- **Critical path:**
  1. Instruction tokenization and embedding (LLM backbone handles all text processing)
  2. Audio encoding and projection (parallel pathway)
  3. Cross-modal fusion within LLM attention layers
  4. Output generation with format constraints applied via instruction conditioning

- **Design tradeoffs:**
  - End-to-end fine-tuning vs. frozen encoder: Full fine-tuning improves task performance but increases forgetting risk; frozen encoder preserves acoustic capabilities but limits adaptation.
  - Diverse instruction training vs. stability: More instruction variants improve robustness (D/F-dimensions) but exacerbate catastrophic forgetting without regularization.
  - MetricIF vs. raw task metrics: Compliance-aware scoring better reflects deployment reality but may underrepresent acoustic capability in research comparisons.

- **Failure signatures:**
  - Format non-compliance: Model outputs correct content but ignores JSON structure, case constraints, or prefix requirements → check F-dimension training coverage.
  - Task performance collapse after SFT: Model refuses to answer or produces generic responses → catastrophic forgetting; investigate replay data or parameter isolation.
  - Inconsistent performance across instruction paraphrases: Large variance in accuracy across D-dimension variants → instruction diversity insufficient during training.

- **First 3 experiments:**
  1. Baseline profiling: Run ISA-Bench D/F/N dimensions on your target LALM; identify weakest dimension and compare against Qwen2-Audio and DeSTA2.5-Audio baselines reported in Figure 1.
  2. Instruction diversity ablation: Fine-tune on 3 levels of instruction variant diversity (baseline, 2× variants, 5× variants) and measure both IFR improvement and task metric degradation to quantify the compliance-forgetting trade-off.
  3. Format-constraint isolation: Test your model on F-dimension variants only (holding task and description constant) to determine if format-following is the bottleneck; if JSON failures dominate, consider pre-training format compliance as a separate objective.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What training strategies beyond simple supervised fine-tuning can improve instruction-following robustness in LALMs without inducing catastrophic forgetting of previously mastered task capabilities?
- Basis in paper: [explicit] The authors state that "SFT remains insufficient: fine-tuning with diverse instructions can improve instruction following ability, but it often leads to catastrophic forgetting on mastered tasks. This finding highlights the inherent difficulty of the instruction sensitivity problem and suggests that more sophisticated solutions are required."
- Why unresolved: The paper demonstrates the trade-off empirically—SFT improved instruction-following rates by up to 100% in the N-dimension, but caused models to lose previously mastered capabilities—but offers no solution.
- What evidence would resolve it: A training methodology that achieves high instruction-following rates across D, F, and N dimensions while maintaining or improving baseline task performance metrics (WER, BLEU, ACC, METEOR) on held-out test sets.

### Open Question 2
- Question: Can scaling to significantly larger and more diverse instruction-variant datasets resolve instruction sensitivity, or are architectural modifications to LALMs necessary?
- Basis in paper: [explicit] The authors suggest "Strategies such as those employed in DeSTA2.5-Audio or scaling to much larger, diverse data sets may be considered for instruction sensitivity improvement" but do not test either approach systematically.
- Why unresolved: The paper only experiments with limited SFT data constructed from training subsets of test sets; the relationship between data scale/diversity and instruction robustness remains unexplored.
- What evidence would resolve it: Controlled experiments varying instruction dataset size and diversity, measuring the resulting instruction-following rates and RPS scores across all three dimensions, compared against architectural interventions.

### Open Question 3
- Question: Why do state-of-the-art LALMs exhibit inconsistent performance patterns across different tasks and instruction dimensions, with no single model excelling universally?
- Basis in paper: [inferred] The paper reports that "no model consistently excels across all three tasks and both dimensions" and shows that Qwen2-Audio performs best on ASR in the D-dimension but worst in the F-dimension, yet does not investigate the underlying causes.
- Why unresolved: The benchmark reveals performance inconsistencies but the paper does not analyze whether these stem from training data distribution, model architecture, attention mechanisms, or other factors.
- What evidence would resolve it: Ablation studies correlating specific training data characteristics or architectural components with performance on each dimension, potentially using probing tasks to identify where instruction processing fails.

## Limitations
- Instruction variant generation quality depends on GPT-4 rewriting, which is not fully specified in the paper
- Catastrophic forgetting mitigation demonstrated only on Qwen2-Audio, limiting generalizability across evaluated models
- Format compliance metrics assume strict regex-based parsing, which may not reflect real-world deployment flexibility
- Compliance-aware metrics (MetricIF) may underrepresent true acoustic capabilities when format constraints dominate scoring

## Confidence
- High confidence: LALMs show significant instruction sensitivity across all three dimensions (D, F, N), supported by systematic evaluation of nine models across five tasks
- Medium confidence: Catastrophic forgetting mechanism during SFT mitigation, demonstrated on a single model without ablation studies on fine-tuning duration or instruction variant diversity
- Low confidence: Specific instruction variant generation methodology and compliance regex patterns, which are essential for reproducing exact benchmark results but not fully disclosed

## Next Checks
1. Instruction variant ablation study: Replicate the benchmark with systematically varied instruction diversity levels (baseline, 2×, 5× variants) to quantify the compliance-forgetting trade-off and identify optimal instruction diversity for robust instruction-following without catastrophic forgetting.
2. Format constraint isolation: Evaluate models on F-dimension variants only while holding task and description constant to determine if format-following is the primary bottleneck. Test with both strict JSON parsing and relaxed structural compliance to understand format tolerance thresholds.
3. Cross-model forgetting analysis: Apply the SFT mitigation procedure across multiple LALMs (beyond Qwen2-Audio) with identical hyperparameters and instruction variant sets to assess whether catastrophic forgetting patterns are consistent across different architectures and pre-training approaches.