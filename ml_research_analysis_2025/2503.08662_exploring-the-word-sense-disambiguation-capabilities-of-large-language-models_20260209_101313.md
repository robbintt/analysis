---
ver: rpa2
title: Exploring the Word Sense Disambiguation Capabilities of Large Language Models
arxiv_id: '2503.08662'
source_url: https://arxiv.org/abs/2503.08662
tags:
- word
- sense
- language
- llms
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper explores the ability of Large Language Models (LLMs)
  to perform Word Sense Disambiguation (WSD), a longstanding task in computational
  linguistics. The authors extend a previous benchmark (XL-WSD) to create two subtasks
  suitable for LLMs: 1) generating a definition for a word in a sentence, and 2) selecting
  the correct meaning from a set of predefined options.'
---

# Exploring the Word Sense Disambiguation Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2503.08662
- Source URL: https://arxiv.org/abs/2503.08662
- Reference count: 26
- Primary result: Fine-tuned medium-sized LLMs achieve state-of-the-art performance on XL-WSD benchmark across five languages

## Executive Summary
This paper investigates the ability of Large Language Models to perform Word Sense Disambiguation (WSD) by extending the XL-WSD benchmark with BabelNet glosses and creating two LLM-friendly subtasks: definition generation and multiple-choice sense selection. The authors evaluate both zero-shot learning and fine-tuned approaches across five languages, demonstrating that while zero-shot LLMs perform well, a fine-tuned medium-sized model (Llama3.1-8B) outperforms all other approaches, including larger zero-shot models and previous state-of-the-art systems. The study provides a comprehensive analysis of LLM capabilities for this classical NLP task.

## Method Summary
The researchers created a benchmark using the XL-WSD corpus aligned with BabelNet sense inventory, covering five languages (English, Spanish, French, Italian, and German). They designed two subtasks suitable for LLMs: generating definitions for ambiguous words in context, and selecting correct meanings from predefined options. Zero-shot evaluation was performed on various open LLMs, while a Llama3.1-8B model was fine-tuned using DeepSpeed ZeRO-3 with specific hyperparameters (LR=4e-5, Epochs=1, Batch Size=512). The evaluation used RougeL/BERTScore for generation tasks and accuracy/F1 for multiple-choice tasks.

## Key Results
- Fine-tuned Llama3.1-8B model achieves state-of-the-art performance on XL-WSD benchmark
- Outperforms larger zero-shot models including Llama3.1-70B and 405B variants
- Demonstrates strong multilingual capabilities across all five tested languages
- Shows significant improvement over traditional WSD systems like XLMR-Large

## Why This Works (Mechanism)

### Mechanism 1: Instruction-Following for Sense Mapping
Fine-tuning aligns LLM generation patterns with specific output format requirements and sense inventory organization, enabling better utilization of pre-trained semantic knowledge for WSD. This supervised process adapts existing semantic understanding to the specific vocabulary and structure of the BabelNet sense inventory. Break condition occurs when sense inventory quality is poor or training data coverage is insufficient.

### Mechanism 2: Cross-Lingual Transfer via Multilingual Base Model
Using a natively multilingual base model allows single fine-tuning process to handle multiple languages effectively. The shared multilingual semantic space enables knowledge transfer across languages. Performance degrades for languages with sparse pre-training data or poor quality representations.

### Mechanism 3: Task Re-framing for Generative Evaluation
Recasting WSD as generative tasks (definition generation or text-based multiple-choice) makes it suitable for LLMs and leverages their expressive capabilities. This avoids need for classification heads and uses standard generation metrics. Break condition occurs when mapping generated text back to sense IDs is ambiguous.

## Foundational Learning

### Word Sense Disambiguation (WSD)
Why needed: Core task being evaluated - understanding that it involves identifying which meaning of a word is used in a given context is fundamental to interpreting results.
Quick check: Given "I went to the bank," what are two possible senses of "bank," and how would a WSD system use context to choose?

### Zero-shot vs. Fine-tuning
Why needed: Paper directly compares these paradigms - understanding the difference between using pre-trained model as-is versus training further on task-specific data is crucial.
Quick check: Why might a model fine-tuned on a specific task outperform a much larger model used in zero-shot setting on that same task?

### Sense Inventories (e.g., WordNet, BabelNet)
Why needed: WSD performance is defined relative to specific sense inventory that lists possible meanings. Choice and coverage directly affect evaluation.
Quick check: How does granularity (fine-grained vs. coarse-grained) of sense inventory affect difficulty and reported accuracy of WSD system?

## Architecture Onboarding

### Component map
XL-WSD Corpus -> BabelNet (sense inventory) -> Prompt Templates -> LLMs (Zero-shot and Fine-tuned) -> Evaluation Metrics (RougeL/BERTScore/Accuracy/F1)

### Critical path
1. Data Preparation: Filter XL-WSD for target languages, align with BabelNet for glosses
2. Prompt Generation: Create generation and multiple-choice prompts using templates
3. Zero-shot Run: Feed prompts to pre-trained LLMs, collect outputs
4. Fine-tuning: Train Llama3.1-8B on training prompts
5. Evaluation: Compare outputs against gold answers using specified metrics

### Design tradeoffs
Prompt paradigm adapts task to LLMs but requires non-trivial post-processing for evaluation. Medium-sized model (8B) for fine-tuning balances accessibility and performance. Machine translation used to fill missing glosses increases coverage but may introduce noise.

### Failure signatures
Poor performance on specific languages indicates insufficient pre-training or fine-tuning data. Low RougeL but high BERTScore means semantically correct but lexically different definitions. Zero-shot accuracy below baselines suggests prompt doesn't activate relevant knowledge.

### First 3 experiments
1. Establish Zero-Shot Baseline: Run English multiple-choice prompts through Llama3.1-8B without fine-tuning to confirm capability gap
2. Minimal Fine-Tuning Run: Fine-tune on small English subset (10K data) and measure performance gain on test set
3. Cross-Lingual Probe: Test English-fine-tuned model on Italian prompts to evaluate cross-lingual transfer

## Open Questions the Paper Calls Out

### Open Question 1
Can few-shot prompting strategies outperform zero-shot baseline or approach fine-tuned model accuracy? The paper didn't consider few-shot approaches to reduce complexity, leaving in-context learning capabilities untested.

### Open Question 2
Does fine-tuning larger parameter models (70B or 405B) yield further improvements over fine-tuned 8B model? Hardware constraints prevented testing if larger fine-tuned models would set new state-of-the-art.

### Open Question 3
How does fine-tuning performance generalize to underrepresented or low-resource languages excluded from current five-language subset? The study is restricted to five languages, unknown if results transfer to remaining 13 languages in XL-WSD.

## Limitations
- Reliance on non-trivial post-processing to map generative outputs back to BabelNet sense IDs introduces evaluation noise
- Fine-tuning requires significant computational resources and careful hyperparameter tuning not fully specified
- Performance variability across languages suggests uneven multilingual base model coverage limiting scalability

## Confidence
- **High Confidence**: Core finding that fine-tuned medium-sized LLMs achieve SOTA is well-supported by direct comparisons and thorough evaluation
- **Medium Confidence**: Cross-lingual transfer capability claims supported but uneven performance indicates potential limitations
- **Medium Confidence**: Superiority of fine-tuning demonstrated but task formulation may not generalize to all WSD paradigms

## Next Checks
1. Replicate exact regular expression or parsing logic used to extract multiple-choice answers from 100 test instances to verify accuracy calculation
2. Train Llama3.1-8B on English data only and evaluate on Italian test set to quantify cross-lingual transfer capability
3. Implement alternative method for mapping generated definitions to BabelNet sense IDs and compare performance to validate evaluation robustness