---
ver: rpa2
title: Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable
  AI Models
arxiv_id: '2506.20018'
source_url: https://arxiv.org/abs/2506.20018
tags:
- decision
- systems
- edge
- real-time
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores real-time decision support systems that leverage
  low-latency AI models, integrating advancements in holistic AI-driven decision tools,
  Edge-IoT technologies, and human-AI teamwork. It examines how large language models
  can assist decision-making in resource-limited contexts and investigates technical
  developments such as DeLLMa, model compression, and edge analytics optimization.
---

# Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models

## Quick Facts
- arXiv ID: 2506.20018
- Source URL: https://arxiv.org/abs/2506.20018
- Reference count: 16
- One-line primary result: Survey paper synthesizing approaches for low-latency, interpretable AI models in real-time decision support, integrating edge-IoT, LLMs, and human-AI collaboration.

## Executive Summary
This survey paper explores the design and deployment of real-time decision support systems leveraging low-latency, interpretable AI models. It integrates advances in holistic AI-driven decision tools, Edge-IoT technologies, and human-AI teamwork. The work examines challenges like resource constraints, model interpretability, and the need for adaptable frameworks, offering practical perspectives on development strategies and applications. Key contributions include improved decision-making accuracy through fine-tuned LLMs, efficient deployment techniques for constrained IoT devices, and enhanced human-AI collaboration via behavior descriptions and interpretability methods.

## Method Summary
The paper surveys multiple approaches for building low-latency, interpretable AI systems for real-time decision support. Methods discussed include the DeLLMa framework for LLM decision-making, model compression techniques (quantization, pruning, knowledge distillation), federated and split learning for edge-IoT networks, and LLM fine-tuning methods like LoRA and RLHF. The survey synthesizes these into a unified perspective on achieving trustworthy human-AI collaboration, though specific implementation details, datasets, and hyperparameters are not provided.

## Key Results
- DeLLMa framework reports up to 40% improvement in decision-making accuracy.
- Model compression techniques enable deployment of complex models on resource-constrained IoT hardware.
- Human-AI collaboration accuracy improves with interpretable explanations and behavior descriptions.

## Why This Works (Mechanism)
The system achieves real-time performance by deploying compressed, fine-tuned AI models on edge devices, minimizing latency through local inference rather than cloud reliance. Interpretability is enhanced through explanation generation and behavior descriptions, building trust and enabling effective human-AI collaboration. The architecture balances computational efficiency, accuracy, and user trust through careful design tradeoffs in model selection, compression, and interface design.

## Foundational Learning
- **Concept: Latency vs. Throughput Trade-off**
  - **Why needed here:** Real-time decision support systems prioritize low latency over high throughput. Understanding this distinction is critical when choosing between cloud processing (high throughput, variable latency) and edge processing (lower throughput, consistent low latency).
  - **Quick check question:** If an autonomous vehicle needs to make a braking decision in 50ms, should it prioritize sending batch sensor data to a high-throughput cloud model or running inference on a local, low-latency edge model?

- **Concept: Explainability vs. Interpretability**
  - **Why needed here:** While often used interchangeably, they are distinct. Interpretability implies a human can understand the model's internal mechanics (e.g., a small decision tree). Explainability involves providing post-hoc reasons for a model's output (e.g., a saliency map for an image classifier). The paper argues for both to build trustworthy human-AI teams.
  - **Quick check question:** For a deep neural network used in medical diagnosis, would you say the model is inherently interpretable or does it require an external explainability method?

- **Concept: Model Compression for Edge Deployment**
  - **Why needed here:** Powerful models are often too large and slow for resource-constrained edge devices. Compression techniques like quantization (reducing numerical precision) and pruning (removing unnecessary connections) are essential engineering tools for deploying AI in real-time IoT contexts.
  - **Quick check question:** How does reducing the precision of model weights from 32-bit floating-point numbers to 8-bit integers (quantization) affect model size and inference speed on a typical edge processor? What is the potential trade-off?

## Architecture Onboarding
- **Component map:** Data sources (e.g., IoT sensors, user input) → Edge AI Module (compressed, fine-tuned model) → Human-AI Interface (recommendation + explanation) → Human decision. Optionally, Cloud AI Module handles complex analytics.
- **Critical path:** Data ingestion from sensor → Inference on Edge AI Model → Explanation generation → Presentation on Human-AI Interface → Human decision. Total time must meet real-time deadline (e.g., < 100ms).
- **Design tradeoffs:**
  1. Model Complexity vs. Latency: More complex models yield higher accuracy but increase inference time.
  2. Explanation Granularity vs. Cognitive Load: Detailed explanations improve trust but can overwhelm users under time pressure.
  3. Edge vs. Cloud Processing: Edge processing guarantees low latency and data privacy but limits computational power.
- **Failure signatures:**
  - Latency spikes caused by model inference time exceeding real-time deadline.
  - Human users consistently ignoring AI recommendations ("algorithm aversion") due to poor or absent explanations.
  - Model outputs that are plausible-sounding but incorrect (hallucinations), especially if LLM is not properly fine-tuned or grounded.
- **First 3 experiments:**
  1. **Latency Benchmark:** Deploy selected AI model on target edge hardware and measure end-to-end inference latency under load. Compare against application's requirement.
  2. **Ablation on Explanations:** Run user study comparing decision-making accuracy and self-reported trust for system with (a) no explanation, (b) simple confidence score, and (c) detailed natural language explanation.
  3. **Fine-tuning Validation:** Compare decision accuracy of pre-trained LLM versus fine-tuned version on held-out test dataset from specific domain (e.g., medical or financial data).

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How can human-AI interaction patterns be systematically optimized to balance task-centric efficiency and human-centric objectives like trust and explainability in real-time decision support?
- **Basis in paper:** [explicit] The paper states a need for "more deliberate consideration of interaction designs to achieve clear communication, trustworthiness, and collaboration" and notes debates between "human-centric and task-centric objectives" (Gomez et al., 2023; Buc ¸inca et al., 2024).
- **Why unresolved:** Current paradigms are described as "simplistic," and no unified framework yet exists for designing interaction patterns that dynamically balance these competing priorities in high-stakes, real-time settings.
- **What evidence would resolve it:** Empirical results from user studies comparing different interaction design frameworks in live decision-support deployments, measuring both performance metrics and human factors like trust and cognitive load.

### Open Question 2
- **Question:** What are the optimal trade-offs between LLM model compression techniques (e.g., quantization, pruning) and the preservation of decision-making accuracy and interpretability for edge deployment?
- **Basis in paper:** [explicit] The paper highlights challenges in deploying complex AI models on "resource-constrained IoT hardware" and the use of optimization techniques like model compression, but notes limitations and the need for more efficient methods (Sudharsan et al., 2022; Gholami and Omar, 2023).
- **Why unresolved:** While compression techniques are known, their specific impact on the nuanced reasoning and interpretability required for trustworthy decision support—especially for LLMs fine-tuned on domain-specific data—remains under-characterized.
- **What evidence would resolve it:** Comparative benchmarks evaluating fine-tuned LLMs of varying sizes and compression levels on standard decision-support tasks, reporting inference latency, accuracy, and quality of generated explanations.

### Open Question 3
- **Question:** How can federated and split learning methods be adapted to guarantee real-time performance and synchronization in highly dynamic, distributed edge-IoT networks?
- **Basis in paper:** [inferred] The paper discusses federated learning for AIoT and edge analytics but also flags "scalability and complexity issues," "synchronization and coordination mechanisms," and the need for "effective synchronization" as persistent challenges (Raj et al., 2021; Olaniyan and Maheswaran, 2021; Chen et al., 2024b).
- **Why unresolved:** Current federated and split learning protocols often assume relatively stable participants and communication, which may not hold in real-time systems with fluctuating connectivity, device heterogeneity, and strict latency budgets.
- **What evidence would resolve it:** Proposals and evaluations of adaptive synchronization protocols or consensus mechanisms for federated/split learning that explicitly bound staleness and guarantee inference latency under simulated network dynamics and device churn.

### Open Question 4
- **Question:** Can a unified, predictive model of human reliance behavior in AI-assisted decision-making be developed that integrates trust states, task complexity, and the quality of AI interpretability cues?
- **Basis in paper:** [explicit] The paper mentions modeling human reliance using supervised and reinforcement learning (Li et al., 2023d) and notes that reliance is "influenced not only by model accuracy but also by trust, feedback, and contextual interpretation," but a unified, validated model is not presented.
- **Why unresolved:** Existing models may focus on isolated factors; integrating these into a single predictive framework that can be used to design AI teammates is an open challenge.
- **What evidence would resolve it:** A study presenting a computational model trained and validated on large-scale interaction data from human-AI decision-making tasks, demonstrating its predictive power on held-out scenarios involving varying levels of AI reliability and explanation types.

## Limitations
- No specific datasets, preprocessing steps, or evaluation protocols are defined for any method discussed.
- Critical hyperparameters for model compression, fine-tuning, and federated learning remain unspecified.
- The DeLLMa framework's 40% accuracy improvement is referenced but not detailed, preventing direct replication.

## Confidence
- **High confidence:** The conceptual framework for real-time decision support systems integrating edge computing, interpretable AI, and human-AI collaboration is well-established and technically sound.
- **Medium confidence:** The survey accurately captures current research trends in model compression, federated learning, and LLM fine-tuning for edge deployment, though specific performance claims lack verification data.
- **Low confidence:** The practical effectiveness of combining all surveyed approaches into a unified system remains unproven without concrete experimental validation.

## Next Checks
1. Implement and benchmark DeLLMa or LoRA fine-tuning on a specific decision-making dataset (e.g., medical diagnosis) to verify claimed accuracy improvements and latency requirements.
2. Conduct a controlled user study comparing human-AI team performance with and without interpretability explanations under time pressure conditions.
3. Profile model compression techniques (quantization/pruning) on target edge hardware to establish the accuracy-latency trade-off curve and identify optimal compression ratios.