---
ver: rpa2
title: 'Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based
  TTS'
arxiv_id: '2508.14313'
source_url: https://arxiv.org/abs/2508.14313
tags:
- arxiv
- search
- reasoning
- reward
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AIRL-S, the first framework to unify reinforcement
  learning (RL)-based and search-based test-time scaling (TTS) for large language
  models (LLMs). The key insight is that the reward function learned during RL training
  serves as the ideal process reward model (PRM) for guiding downstream search.
---

# Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS

## Quick Facts
- arXiv ID: 2508.14313
- Source URL: https://arxiv.org/abs/2508.14313
- Reference count: 40
- This paper introduces AIRL-S, the first framework to unify reinforcement learning (RL)-based and search-based test-time scaling (TTS) for large language models (LLMs).

## Executive Summary
This paper introduces AIRL-S, the first framework to unify reinforcement learning (RL)-based and search-based test-time scaling (TTS) for large language models (LLMs). The key insight is that the reward function learned during RL training serves as the ideal process reward model (PRM) for guiding downstream search. AIRL-S leverages adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, eliminating the need for labeled intermediate process data. At inference, this PRM simultaneously acts as a critic for RL rollouts and as a heuristic to guide search procedures, enhancing cross-task generalization and mitigating reward hacking.

## Method Summary
AIRL-S unifies RL and search-based TTS by learning a process reward model (PRM) through adversarial inverse reinforcement learning (AIRL) during RL training. The framework combines AIRL with Group Relative Policy Optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces. During inference, this learned PRM serves dual purposes: acting as a critic for RL rollouts and guiding search algorithms. The approach eliminates the need for labeled intermediate process data by learning rewards directly from reasoning traces, and claims to provide better generalization across tasks while reducing reward hacking.

## Key Results
- AIRL-S improves performance by 9% on average over the base model across eight benchmarks
- The framework matches GPT-4o performance on tested tasks
- The PRM consistently outperforms all baseline PRMs trained with labeled data when integrated into multiple search algorithms
- Demonstrates effectiveness across mathematics, scientific reasoning, and code generation tasks

## Why This Works (Mechanism)
AIRL-S works by leveraging the observation that reward functions learned during RL training inherently capture the reasoning patterns that lead to correct solutions. By using AIRL, the framework can learn a dense reward signal that reflects the value of intermediate reasoning steps, not just final outcomes. This learned reward function serves as an ideal PRM because it was trained on the same type of reasoning traces that occur during search. The dual use of the PRM as both critic and heuristic creates a coherent framework where the same learned knowledge guides both exploration (search) and evaluation (reward assessment).

## Foundational Learning
- **Adversarial Inverse Reinforcement Learning (AIRL)**: Learns reward functions by distinguishing between expert and generated trajectories. Needed because it can learn dense reward signals from demonstration data without requiring explicit reward labels. Quick check: verify the discriminator can distinguish correct from incorrect reasoning traces.
- **Group Relative Policy Optimization (GRPO)**: An RL algorithm that optimizes policies based on group-relative returns. Needed because it provides stable policy updates when learning from dense reward signals. Quick check: ensure policy gradients remain stable during training.
- **Process Reward Modeling**: Evaluating intermediate reasoning steps rather than just final answers. Needed because complex reasoning tasks require guidance throughout the solution process. Quick check: validate that intermediate rewards correlate with final solution quality.
- **Test-Time Scaling (TTS)**: Using additional computation during inference to improve answer quality. Needed because many reasoning tasks benefit from deeper exploration of solution space. Quick check: measure performance improvements against computational budget.

## Architecture Onboarding

**Component Map**: LLM (reasoning traces) -> AIRL (reward learning) -> PRM (dense rewards) -> GRPO (policy optimization) -> Search algorithms (guided exploration) -> Final answers

**Critical Path**: AIRL training -> PRM generation -> Search algorithm integration -> Inference-time reasoning

**Design Tradeoffs**: AIRL-S trades increased training complexity and computational overhead for improved inference-time performance and generalization. The approach requires careful balance between learning a general reward function and overfitting to specific problem types.

**Failure Signatures**: 
- PRM overfitting to training distribution (poor generalization)
- Search algorithms becoming stuck in local optima due to suboptimal heuristics
- Reward signals that are too sparse or noisy to guide effective search
- Computational overhead that outweighs performance gains

**3 First Experiments**:
1. Validate that the learned PRM can distinguish between correct and incorrect reasoning traces on held-out data
2. Test search performance using the PRM against random search and baseline heuristics
3. Evaluate cross-task generalization by testing on reasoning tasks from different domains than training

## Open Questions the Paper Calls Out
None

## Limitations
- Process reward model generalization across out-of-distribution reasoning traces is not thoroughly evaluated
- Computational overhead during inference is not fully analyzed, particularly latency and memory requirements
- Mechanisms for mitigating reward hacking are not fully explained, and potential new vulnerabilities from adversarial training are not addressed

## Confidence
**High Confidence**: The technical implementation using AIRL combined with GRPO is well-documented and reproducible. The experimental methodology and benchmark selection are appropriate.

**Medium Confidence**: Claims about matching GPT-4o performance require qualification due to potential version differences. Cross-task generalization claims are supported but could benefit from additional stress testing.

**Low Confidence**: The "cost-effective solution" claim is not empirically validated with total cost comparisons against alternative approaches.

## Next Checks
1. Conduct systematic evaluation of AIRL-S performance on out-of-distribution reasoning tasks and adversarial examples to quantify generalization limits
2. Measure end-to-end computational overhead including training time, inference latency, and memory usage to assess practical deployment costs
3. Test AIRL-S on extended reasoning chains (>10 steps) and open-ended problem-solving scenarios to evaluate stability over longer time horizons