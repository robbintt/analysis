---
ver: rpa2
title: Practical Multi-Task Learning for Rare Conversions in Ad Tech
arxiv_id: '2507.20161'
source_url: https://arxiv.org/abs/2507.20161
tags:
- conversion
- conversions
- learning
- hard
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a Multi-Task Learning (MTL) approach for improving
  predictions of rare conversion events in online advertising. The key idea is to
  classify conversions into "rare" (hard) and "frequent" (soft) types based on historical
  conversion rates, and use a shared model architecture with separate task-specific
  towers to learn representations for each type.
---

# Practical Multi-Task Learning for Rare Conversions in Ad Tech

## Quick Facts
- arXiv ID: 2507.20161
- Source URL: https://arxiv.org/abs/2507.20161
- Reference count: 19
- Primary result: MTL approach with frequent conversions as auxiliary signal improves rare conversion prediction, achieving 0.69% AUC lift offline and 2% CPA reduction online

## Executive Summary
This paper presents a Multi-Task Learning (MTL) approach for improving predictions of rare conversion events in online advertising. The key innovation is classifying conversions into "rare" (hard) and "frequent" (soft) types based on historical conversion rates, then using a shared model architecture with separate task-specific towers to learn representations for each type. The soft conversions serve as an auxiliary signal to improve hard conversion prediction. The approach was deployed in production and demonstrated consistent improvements: offline AUC lift of 0.69% and online Cost per Action reduction of 2%. The model defines tasks using data-driven historical statistics rather than manual labeling, addressing challenges with data imbalance and noisy metadata.

## Method Summary
The method trains a Multi-Task Learning model with shared embedding and cross layers, splitting into two task-specific towers for "hard" (rare, CVR ≤ α) and "soft" (frequent, CVR > α) conversions. Task assignments are determined by computing time-decayed historical CVR over sliding windows and applying threshold α. The model uses weighted loss L = W_soft · L_soft + W_hard · L_hard, where each example contributes only to its assigned task. The production system uses only the hard-conversion tower for inference to maintain efficiency. Optimal parameters found were W_hard = W_soft = 0.5 and 4 shared layers, using a Deep & Cross Network architecture.

## Key Results
- Offline: 0.69% AUC lift for hard conversions
- Online: 2% reduction in Cost per Action (CPA)
- Balanced loss weights (W_hard = W_soft = 0.5) achieved +1.17% RIG vs. hard-only baseline
- Sharing 4 layers achieved +0.21% RIG over embedding-only sharing

## Why This Works (Mechanism)

### Mechanism 1: Auxiliary Signal Transfer
Frequent conversions improve rare conversion prediction through shared representation learning. Soft conversions (CVR > α) provide abundant gradient signal during training, enabling the shared layers to learn user intent and behavior patterns that transfer to the data-scarce hard conversion task. The auxiliary task acts as implicit regularization. This works because hard and soft conversions share underlying behavioral patterns (user intent, context features) despite different frequency regimes.

### Mechanism 2: Data-Driven Task Segmentation
Statistical thresholding on historical CVR produces more robust task assignments than manual labeling. By computing time-decayed CVR over sliding windows and applying threshold α, task assignment reflects empirical event frequency rather than advertiser-provided metadata, which may be inconsistent or noisy. This approach is more stable and objective than relying on human judgment.

### Mechanism 3: Progressive Representation Sharing
Deeper shared layers improve hard conversion prediction up to a point. Sharing more layers allows the model to learn increasingly abstract common features (user propensity, context similarity) that benefit both tasks, while task-specific towers capture residual specialization. There exists an optimal sharing depth beyond which task-specific patterns dominate.

## Foundational Learning

- **Multi-Task Learning (MTL) with Shared Representations**
  - Why needed: Understanding that auxiliary tasks can improve primary task generalization through shared parameters is central to this approach
  - Quick check: Given two tasks with different data volumes, would you expect equal loss weights to work best? Why or why not?

- **Conversion Rate (CVR) Prediction and Data Sparsity**
  - Why needed: Rare events (<1% CVR) create severe class imbalance, making gradient signal unstable; this motivates the auxiliary signal approach
  - Quick check: Why might a model trained on all conversions together struggle with calibration in the low CVR range?

- **Task Definition via Statistical Thresholds vs. Manual Labels**
  - Why needed: The paper's key innovation is replacing human judgment with empirical CVR statistics; understanding this distinction clarifies robustness claims
  - Quick check: What could go wrong if an advertiser mislabels a frequent conversion as "high-value" in a manual system?

## Architecture Onboarding

- **Component map**: Input Features -> Shared Layers (embedding + cross/feed-forward) -> Task Towers (hard and soft) -> Loss Combiner -> Traffic Router
- **Critical path**: 1) Compute historical CVR for each conversion setup over time-decayed sliding window; 2) Assign examples to hard/soft tasks based on threshold α; 3) Train shared-bottom MTL model with tuned loss weights; 4) Export inference model with only hard-conversion tower; 5) Route traffic using online threshold
- **Design tradeoffs**: Equal loss weights worked best here, but may not generalize; more sharing improved results but computational cost increases; offline α optimized for validation metrics while online threshold lowered for stability
- **Failure signatures**: Task instability (conversions flip between hard/soft), negative transfer (hard task performance degrades), calibration drift (overconfident low CVR predictions), threshold sensitivity (large metric swings)
- **First 3 experiments**: 1) Loss weight sweep with W_hard ∈ {0.2, 0.5, 0.7, 0.9}; 2) Shared layer ablation from 0 to 4 layers; 3) Threshold sensitivity analysis varying α ±0.5% absolute CVR

## Open Questions the Paper Calls Out

### Open Question 1
Would more complex gradient isolation architectures (e.g., MMoE or PLE) yield better performance than the shared-bottom architecture by mitigating negative transfer between hard and soft tasks? The paper demonstrates that sharing layers helps but doesn't verify if the "soft" task conflicts with the "hard" task in ways that advanced gating mechanisms could resolve.

### Open Question 2
Can the binary "hard/soft" classification be refined into a multi-class or continuous spectrum to improve granularity without losing the benefits of Multi-Task Learning? Grouping high-frequency events (e.g., 5% CVR and 50% CVR) into a single task may introduce noise, as the underlying user behaviors for different "soft" conversions may vary significantly.

### Open Question 3
How does the model perform on "cold-start" conversions where historical statistics are insufficient to determine if the task is "hard" or "soft"? The paper doesn't address how the system classifies or routes traffic for new conversion pixels or new advertisers with no historical data.

### Open Question 4
How sensitive is the online performance to the "stability mechanism" described, and what is the quantitative impact of conversions fluctuating between "hard" and "soft" labels? While the mechanism is mentioned, the paper doesn't quantify how often these flips occur or how much performance degrades if the stabilization mechanism is removed.

## Limitations
- Task assignment based on fixed threshold α may not generalize across advertisers with different baseline conversion rates
- Train-serve gap exists because online threshold for task assignment is lower than offline threshold for stability
- Negative transfer risk not fully explored; monotonic improvement with shared layers suggests favorable regime but may not hold for different task pairs
- Limited ablation studies on architecture variations beyond shared-bottom (no comparison to MMoE, PLE, or other MTL variants)

## Confidence

- **High Confidence**: Empirical improvements (0.69% AUC lift, 2% CPA reduction) are well-supported by experimental results
- **Medium Confidence**: Claims about data-driven task definition being superior to manual labeling are supported by internal comparison but lack broader validation
- **Low Confidence**: Optimal architecture parameters (loss weights, shared layer depth) may be overfit to this specific problem instance

## Next Checks

1. **Threshold Robustness Analysis**: Systematically vary α by ±0.5% CVR and measure both task assignment stability and RIG on hard conversions. Plot the tradeoff curve to identify the stability-accuracy frontier.

2. **Cross-Advertiser Transfer Test**: Train the MTL model on a diverse set of advertisers and evaluate hard conversion prediction performance on advertisers excluded from training. This would validate whether the shared representation generalizes.

3. **Negative Transfer Boundary Detection**: Incrementally increase shared layers beyond the tested range (5+ layers) and identify the point where hard conversion RIG begins to degrade. Compare this to the optimal depth to quantify the margin of safety.