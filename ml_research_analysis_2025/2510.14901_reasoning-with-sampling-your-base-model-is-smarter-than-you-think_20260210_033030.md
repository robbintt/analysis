---
ver: rpa2
title: 'Reasoning with Sampling: Your Base Model is Smarter Than You Think'
arxiv_id: '2510.14901'
source_url: https://arxiv.org/abs/2510.14901
tags:
- sampling
- arxiv
- base
- grpo
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a training-free sampling algorithm that significantly\
  \ boosts single-shot reasoning performance of base language models. The core idea\
  \ is to sample from a \"power distribution\" p^\u03B1 (\u03B11), which sharpens\
  \ the base model distribution to favor higher-likelihood reasoning traces."
---

# Reasoning with Sampling: Your Base Model is Smarter Than You Think

## Quick Facts
- arXiv ID: 2510.14901
- Source URL: https://arxiv.org/abs/2510.14901
- Authors: Aayush Karan; Yilun Du
- Reference count: 40
- Key outcome: Training-free sampling algorithm boosts single-shot reasoning performance of base models via power distribution sampling

## Executive Summary
This paper presents a training-free sampling algorithm that significantly boosts single-shot reasoning performance of base language models. The core idea is to sample from a "power distribution" p^α (α>1), which sharpens the base model distribution to favor higher-likelihood reasoning traces. This is achieved using a Markov chain Monte Carlo method that iteratively resamples token subsequences based on base model likelihoods. Across multiple base models and reasoning tasks, the method achieves near-equal or superior performance compared to state-of-the-art RL-posttraining, while maintaining generation diversity. The results demonstrate that base models possess underutilized reasoning capabilities that can be elicited through intelligent inference-time sampling.

## Method Summary
The method uses Metropolis-Hastings MCMC to sample from the power distribution p^α, which sharpens the base model's probability distribution to favor high-likelihood sequences. The algorithm operates autoregressively in blocks, using intermediate distributions to mitigate mixing time issues in high-dimensional spaces. For each block, it runs N_MCMC resampling steps, randomly selecting token positions to resample and accepting/rejecting candidates based on the ratio of power-distribution likelihoods. The approach requires no training, curated datasets, or external verifiers—only the base model's likelihood function.

## Key Results
- Qwen2.5-Math-7B improves from 0.532 to 0.748 single-shot accuracy on MATH500
- Outperforms state-of-the-art GRPO post-training on reasoning benchmarks
- Maintains generation diversity while improving reasoning performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Power distribution sampling upweights tokens leading to high-likelihood future completions, whereas low-temperature sampling greedily averages over all futures.
- **Mechanism:** Sampling from the power distribution p^α(x) computes next-token weights as a sum of exponents over future paths, favoring tokens with fewer but higher-probability completions. Low-temperature sampling uses an exponent of sums, favoring tokens with many low-probability completions.
- **Core assumption:** Higher-likelihood sequences under the base model correlate with correct reasoning outputs.
- **Evidence anchors:**
  - [abstract] "Inspired by MCMC techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods."
  - [section 4.1] Proposition 1 and Observation 1 formally distinguish power distribution from low-temperature sampling.
  - [corpus] Weak/no direct corpus evidence on power vs. low-temperature sampling differences.
- **Break condition:** If base model likelihood does not correlate with task correctness (e.g., adversarial domains), the mechanism may fail.

### Mechanism 2
- **Claim:** Metropolis-Hastings MCMC with random resampling approximates sampling from the intractable power distribution using only base model likelihoods.
- **Mechanism:** The algorithm proposes new token subsequences by resampling from a proposal LLM, accepting/rejecting based on the ratio of power-distribution likelihoods (normalization cancels). Irreducibility and aperiodicity of the proposal ensure convergence.
- **Core assumption:** The random resampling proposal allows sufficient mixing within practical compute budgets.
- **Evidence anchors:**
  - [abstract] "Our method does not require training, curated datasets, or a verifier."
  - [section 4.2] Definition 1 and Equation (9) define MH acceptance criteria.
  - [corpus] No direct external validation of MH for this LLM use case.
- **Break condition:** Exponential mixing time or high-dimensional token spaces may prevent convergence within feasible steps.

### Mechanism 3
- **Claim:** Autoregressive block-wise sampling with intermediate distributions mitigates mixing time issues by leveraging sequential structure.
- **Mechanism:** Instead of sampling full sequences at once, the algorithm samples blocks of size B, using samples from one block as initialization for the next staged MH process. This creates a sequence of intermediate distributions guiding the chain toward the target.
- **Core assumption:** Block size B and MCMC steps N_MCMC are sufficient to transition between intermediate distributions.
- **Evidence anchors:**
  - [section 4.3] Algorithm 1 and Equation (10) define staged sampling.
  - [section 5.3] Figure 6 shows accuracy scaling with N_MCMC and stability with α.
  - [corpus] No corpus papers validate this specific autoregressive MCMC variant.
- **Break condition:** If block size is too large or N_MCMC too small, samples may poorly approximate the power distribution.

## Foundational Learning

- **Concept: Markov Chain Monte Carlo (MCMC)**
  - Why needed here: The paper uses Metropolis-Hastings to sample from an unnormalized power distribution. Understanding proposal distributions, acceptance ratios, and convergence is essential.
  - Quick check question: Why does MH only require unnormalized densities, and what conditions ensure convergence?

- **Concept: Power (Annealed) Distributions**
  - Why needed here: Core idea is sampling from p^α to sharpen the base model distribution. Understanding exponentiation reweighting is key.
  - Quick check question: If p(x)=0.1 and p(y)=0.01, what are the relative weights under p^2?

- **Concept: Autoregressive Language Models**
  - Why needed here: The algorithm operates on token sequences generated autoregressively. Understanding how next-token conditionals compose into joint distributions is necessary.
  - Quick check question: How does the product of conditionals define joint sequence probability?

## Architecture Onboarding

- **Component map:** Base LLM (p) -> Proposal LLM (p_prop) -> Power parameter (α) -> Block size (B) and steps (N_MCMC) -> MH acceptance module -> Autoregressive block sampler

- **Critical path:**
  1. Initialize sequence with proposal LLM.
  2. For each block, run N_MCMC steps of random-resample MH.
  3. Accept/reject candidates based on power-distribution ratio.
  4. Fix prefix after each block and proceed to next.

- **Design tradeoffs:**
  - Larger α: More sharpening, but may over-concentrate; paper finds α=4.0 robust.
  - Larger N_MCMC: Better approximation but more compute; diminishing returns after ~10.
  - Smaller B: Smoother intermediate distributions but more blocks; paper uses B=192.
  - Proposal temperature: Slightly higher temperature (τ=0.5) helps for non-reasoning tasks.

- **Failure signatures:**
  - No improvement over base: Check if N_MCMC≥2, α≥2.0, and proposal differs from target.
  - Excessive compute: Reduce N_MCMC or increase B; monitor token multiplier from Equation (12).
  - Mode collapse: α may be too high; reduce toward 2.0–4.0 range.

- **First 3 experiments:**
  1. Replicate MATH500 results with Qwen2.5-Math-7B, α=4.0, B=192, N_MCMC=10; compare single-shot accuracy to paper's 0.748.
  2. Ablate N_MCMC from 2 to 20 on a MATH500 subset; plot accuracy vs. compute to find sweet spot.
  3. Test on HumanEval with same hyperparameters; verify if out-of-domain gains (e.g., 0.573 for Qwen2.5-Math-7B) replicate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does power sampling effectively generalize to multi-turn agentic interactions or tasks requiring external tool use?
- Basis in paper: [explicit] The conclusion states that employing additional compute at sampling time offers a "promising direction for expanding the scope of reasoning beyond verifiability," noting current evaluation is restricted to single-shot tasks.
- Why unresolved: The paper evaluates static benchmarks (MATH500, HumanEval) but does not test dynamic environments where iterative resampling might conflict with external state changes or tool feedback.
- What evidence would resolve it: Experiments applying the MCMC sampler to agentic frameworks (e.g., web navigation or code execution environments) where the token sequence interacts with an external environment.

### Open Question 2
- Question: Does the block-wise MCMC algorithm theoretically guarantee convergence to the target power distribution in high-dimensional sequence spaces?
- Basis in paper: [inferred] Section 4.3 highlights the risk of "exponential mixing time" in high dimensions and proposes block-wise initialization to "remedy this," but provides no formal convergence proof for the specific autoregressive structure.
- Why unresolved: While standard Metropolis-Hastings guarantees exist, the sequential block dependency introduces initialization biases that are empirically mitigated but theoretically unverified.
- What evidence would resolve it: A theoretical analysis or empirical study mapping the error bound (KL divergence from p^α) as a function of block size B and MCMC steps N_MCMC.

### Open Question 3
- Question: Can the power exponent α be optimized dynamically rather than held static during generation?
- Basis in paper: [inferred] The authors note α=4.0 is chosen empirically and is "relatively robust," but Section 4.1 establishes α as a fundamental trade-off parameter between likelihood and support.
- Why unresolved: A fixed α may be suboptimal across the varying complexity of different reasoning steps within a single sequence, potentially over-sharpening easy steps or under-sharpening difficult ones.
- What evidence would resolve it: A study comparing fixed α performance against an adaptive α schedule that adjusts based on local token entropy or sequence confidence.

### Open Question 4
- Question: How does the inference-time compute scaling of this method compare to simply scaling up the base model parameters?
- Basis in paper: [inferred] The experiments are restricted to 3.5B and 7B models, and the cost analysis (Eq. 12) shows an ~9x increase in token generation relative to standard inference.
- Why unresolved: It is unclear if the reasoning boost achieved by this complex sampling on a 7B model is more FLOP-efficient than performing standard greedy decoding on a significantly larger base model.
- What evidence would resolve it: A Pareto frontier analysis comparing accuracy vs. total FLOPs for power sampling on small models versus standard inference on larger base models.

## Limitations
- Limited ablation on hyperparameters (α, B, N_MCMC) with no systematic exploration of their interactions
- Assumes base model likelihood correlates with correctness without direct validation
- Requires ~9x more token generation than standard inference, raising computational efficiency concerns

## Confidence
- **High confidence**: The empirical results demonstrating accuracy improvements over base models (e.g., Qwen2.5-Math-7B from 0.532 to 0.748 on MATH500) are well-supported by the experimental setup and results presented.
- **Medium confidence**: The theoretical distinction between power distribution and low-temperature sampling (Proposition 1 and Observation 1) is mathematically sound, but the practical implications and whether this distinction fully explains the performance gains require further validation.
- **Medium confidence**: The claim that this method outperforms state-of-the-art RL post-training (e.g., outperforming GRPO on MATH500) is supported by results, but the comparison is limited to specific implementations and may not generalize across all RL approaches.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary α (2.0-6.0), B (64-384), and N_MCMC (2-20) on a subset of tasks to map the performance landscape and identify optimal configurations for different task types.

2. **Base model likelihood correlation study**: For a held-out set of problems where ground truth is known, compute the correlation between base model sequence log-likelihood and correctness. Additionally, test whether the method's gains persist when applied to adversarially constructed examples where high-likelihood sequences are incorrect.

3. **Computational cost-benefit analysis**: Measure the token generation cost per sample across different hyperparameter settings and compare accuracy improvements to the increased computational expense. Identify the point of diminishing returns where additional MCMC steps provide minimal accuracy gains relative to compute cost.