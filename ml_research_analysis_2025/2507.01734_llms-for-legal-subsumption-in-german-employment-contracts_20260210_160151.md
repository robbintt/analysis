---
ver: rpa2
title: LLMs for Legal Subsumption in German Employment Contracts
arxiv_id: '2507.01734'
source_url: https://arxiv.org/abs/2507.01734
tags:
- legal
- void
- sources
- clauses
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates large language models (LLMs) for classifying
  clauses in German employment contracts as valid, unfair, or void using legal subsumption.
  It compares three contexts: no legal knowledge, distilled examination guidelines,
  and full-text laws/court rulings.'
---

# LLMs for Legal Subsumption in German Employment Contracts

## Quick Facts
- arXiv ID: 2507.01734
- Source URL: https://arxiv.org/abs/2507.01734
- Reference count: 40
- Key result: GPT-4o achieves 0.80 weighted F1-score on German employment contract clause classification using distilled examination guidelines

## Executive Summary
This study evaluates large language models for classifying clauses in German employment contracts as valid, unfair, or void using legal subsumption. The research compares three contexts: no legal knowledge, distilled examination guidelines, and full-text laws/court rulings. Results show that while full-text sources moderately improve performance, examination guidelines significantly enhance recall for void clauses (80%+) and overall weighted F1-score. GPT-4o performed best overall. Despite advancements, LLM performance remains below that of human lawyers, especially when using original legal sources. The study contributes an extended dataset with legal sources and demonstrates the potential of LLMs to assist in contract legality review.

## Method Summary
The study employs zero-shot inference via commercial APIs (GPT-4o, DeepSeek-V3/R1, Grok-2, Gemini-1.5-pro) using specific system prompts and JSON-enforced output. Three context variants are tested: No Context (baseline), Examination Guidelines (24 distilled rules), and Full-Text Sources (41 laws/court rulings). The methodology uses the extended "Employment Contract Dataset V2" (891 samples) with precision, recall, and F1-Score metrics calculated per class and averaged. The critical design element is the injection of legal context to force the model to ground classifications in provided text rather than internal parametric knowledge.

## Key Results
- Examination guidelines significantly improve void clause recall (80%+) compared to full-text sources (moderate improvement)
- GPT-4o achieves the best overall performance with 0.80 weighted F1-score using guidelines
- DeepSeek-R1 shows unmatched void clause recall (98%) but lower precision and unreliable JSON formatting
- Models struggle to distinguish "unfair" clauses from valid or void ones, with performance often below random guessing

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation
Human experts convert complex laws and court rulings into concise rules. The LLM then performs pattern matching between the clause and these distilled rules, bypassing the difficulty of interpreting lengthy, ambiguous legal text. The primary bottleneck for LLMs is the interpretation of legal syntax and length, not the logical application of a clearly stated rule.

### Mechanism 2: In-Context Constraint
Providing explicit legal context in the prompt forces the model to ground its classification in the provided text, overriding potentially outdated or incorrect internal parametric knowledge. The system prompt instructs the model to classify a clause as "void" only if it can be subsumed under the provided text, creating a constrained environment where the model acts as a logic processor rather than a knowledge retriever.

### Mechanism 3: Asymmetric Risk Modeling
Certain reasoning models exhibit a bias toward classifying clauses as problematic. In a legal context, flagging a valid clause for review (false positive) is safer than missing a void one (false negative). This creates a workflow where models can be selected or tuned to prioritize high recall for "void" clauses at the expense of precision.

## Foundational Learning

- **Concept: Legal Subsumption**
  - Why needed: This is the core logic unit. You must understand that the system is not just "sentiment analysis" but a formal check of whether specific facts (clause details) satisfy the conditions of a general rule (statute/court ruling).
  - Quick check: Can you explain the difference between a clause being "unfair" (discretionary) vs. "void" (strictly illegal)?

- **Concept: Context Window & Needle-In-A-Haystack (NIAH)**
  - Why needed: The paper relies on stuffing full-text laws into the prompt. You need to understand that LLMs struggle to find relevant details in the middle of long contexts.
  - Quick check: If a court ruling is 8,000 tokens long and placed in the middle of a prompt, does the model attend to it equally? (Implied: No, citing Section 5.4/6 discussion on full-text struggles).

- **Concept: Classification Metrics (Precision vs. Recall)**
  - Why needed: The paper highlights a trade-off between GPT-4o (balanced) and DeepSeek-R1 (high recall). Understanding F1-Score vs. Recall is critical for determining which model fits a specific business risk profile.
  - Quick check: In a legal review, is it worse to miss a void clause (False Negative) or to flag a valid one (False Positive)?

## Architecture Onboarding

- **Component map:** Input (German Clause Text + Section Title) -> Context Injector (fetches guidelines or full-text sources) -> Prompt Constructor (assembles System Instruction + Context + Input) -> LLM Engine (GPT-4o, DeepSeek-R1, or others) -> Output Parser (enforces JSON structure)

- **Critical path:** The creation and maintenance of the Examination Guidelines. The paper shows these are the single biggest driver of performance. Without accurate, distilled guidelines, the system degrades to moderate performance levels.

- **Design tradeoffs:**
  - GPT-4o vs. DeepSeek-R1: GPT-4o offers higher stability and weighted F1 (0.80) suitable for general assistance. DeepSeek-R1 offers near-perfect recall (0.98) for void clauses but is verbose and has lower precision, suitable for "safety-first" screening but requiring more human cleanup.
  - Full-text vs. Guidelines: Guidelines require expensive human legal labor to create and maintain. Full-text is cheaper to source but yields significantly worse LLM performance (F1 0.72 for GPT-4o).

- **Failure signatures:**
  - "Unfair" Class Collapse: The models struggle to distinguish "unfair" from "valid" or "void" (low precision/recall for unfair class across all models).
  - JSON Hallucination: DeepSeek-R1 was excluded from full-text tests due to "unreliably output formatting".
  - Context Overload: Performance drops when relying on full-text sources because the model fails to isolate the relevant legal "needle" from the haystack of text.

- **First 3 experiments:**
  1. **Baseline Test:** Run the dataset with no legal context to establish the model's internal (and likely outdated) legal knowledge.
  2. **Guideline Injection:** Run the dataset with all examination guidelines included (since they are short enough to fit) to measure the upper bound of performance with distilled knowledge.
  3. **Retrieval Simulation (Full-Text):** Simulate a "perfect retrieval" scenario where you provide only the specific court ruling relevant to the clause to isolate the model's ability to subsume under raw text.

## Open Questions the Paper Calls Out

- **Question 1:** Can few-shot learning strategies significantly improve the classification performance for "unfair" clauses?
  - Basis: Current models struggle to distinguish "unfair" clauses from "valid" or "void" ones, likely due to the subtle distinction requiring practical experience rather than just statutory knowledge.
  - Resolution: An evaluation of model performance using prompts containing annotated examples of "unfair" clauses compared to the current zero-shot baselines.

- **Question 2:** Do LLMs generate legally sound argumentation to support their classification predictions?
  - Basis: The study measured classification metrics but did not analyze the explanations generated by the LLMs, leaving the logical validity of the "subsumption" process unverified.
  - Resolution: A qualitative analysis or automated evaluation of the reasoning logs to verify if the models rely on the provided legal sources rather than hallucinations.

- **Question 3:** How does the introduction of imperfect retrieval mechanisms affect LLM performance in legal subsumption tasks?
  - Basis: The methodology explicitly assumed a "perfect retrieval system" which does not reflect real-world applications.
  - Resolution: Experiments combining these LLMs with standard retrieval architectures (RAG) to measure performance degradation when retrieving sources automatically.

## Limitations
- Results are specific to German employment contracts and may not generalize to other legal domains or jurisdictions
- The significant performance advantage of distilled guidelines comes with high human labor costs not quantified in the study
- LLM performance remains below human lawyers, but quantitative human baselines are not provided for direct comparison

## Confidence
- **High Confidence:** The comparative performance between context types and the relative ranking of model performance are well-supported by the experimental design
- **Medium Confidence:** The specific performance metrics are reproducible given the data, but small variations could occur due to unspecified generation parameters
- **Low Confidence:** The generalizability of findings to other legal domains or languages remains speculative without additional validation studies

## Next Checks
1. **Human Benchmark Study:** Conduct a controlled experiment comparing LLM classifications against professional lawyers on the same dataset to establish quantitative performance gaps and identify specific failure patterns.

2. **Cross-Jurisdictional Testing:** Apply the same methodology to employment contracts from other countries (e.g., US, UK, France) to test the portability of the knowledge distillation approach across different legal systems.

3. **Economic Feasibility Analysis:** Calculate the full lifecycle costs of maintaining examination guidelines versus the potential savings from automated contract review to determine breakeven points and ROI.