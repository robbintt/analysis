---
ver: rpa2
title: 'CoP: Agentic Red-teaming for Large Language Models using Composition of Principles'
arxiv_id: '2506.00781'
source_url: https://arxiv.org/abs/2506.00781
tags:
- jailbreak
- red-teaming
- prompt
- agent
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoP, an agentic framework for automating red-teaming
  of large language models through human-provided red-teaming principles. The method
  uses a red-teaming agent to orchestrate and compose jailbreak strategies based on
  these principles, generating effective adversarial prompts while maintaining transparency.
---

# CoP: Agentic Red-teaming for Large Language Models using Composition of Principles

## Quick Facts
- arXiv ID: 2506.00781
- Source URL: https://arxiv.org/abs/2506.00781
- Reference count: 40
- Key outcome: Agentic red-teaming framework achieving up to 19.0x higher attack success rates through principle composition

## Executive Summary
This paper introduces CoP, an agentic framework for automating red-teaming of large language models through human-provided red-teaming principles. The method uses a red-teaming agent to orchestrate and compose jailbreak strategies based on these principles, generating effective adversarial prompts while maintaining transparency. CoP significantly improves single-turn attack success rates against both open-source and proprietary LLMs, achieving up to 19.0 times higher performance than state-of-the-art baselines. The approach reduces computational overhead by up to 17.2 times through efficient principle composition, and identifies effective jailbreak strategies like expansion and multi-principle combinations. CoP successfully circumvents even safety-enhanced models, demonstrating persistent vulnerabilities in current LLM safety mechanisms while providing a modular framework for ongoing safety testing.

## Method Summary
CoP employs an agentic workflow that orchestrates and composes jailbreak strategies based on human-provided red-teaming principles. The framework uses a red-teaming agent (default: Grok-2) to select and combine principles from an inventory of seven options (Generate, Expand, Shorten, Rephrase, Phrase Insertion, Style Change, Replace Words). The agent generates jailbreak prompts through a three-stage process: Initial Seed Prompt Generation transforms harmful queries into research-oriented prompts, Principle Composition creates synergistic jailbreak strategies by combining multiple principles simultaneously, and iterative refinement uses dual-judge evaluation (jailbreak effectiveness + semantic similarity) to optimize prompts. The method achieves significant improvements in attack success rates while reducing computational overhead through efficient principle composition.

## Key Results
- Achieves up to 19.0 times higher attack success rates than state-of-the-art baselines
- Reduces computational overhead by up to 17.2 times through efficient principle composition
- Identifies expansion and multi-principle combinations as most effective jailbreak strategies
- Successfully circumvents safety-enhanced models including O1 and Claude-3.5 Sonnet
- Demonstrates persistent vulnerabilities in current LLM safety mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Multi-Principle Composition in a Single Step
CoP's Red-Teaming Agent selects and combines multiple principles simultaneously, creating synergistic transformations that dilute harmful intent through contextual expansion while embedding trigger phrases in seemingly benign text. This compositional approach is more effective than sequential or individual principle application, as evidenced by a 58% drop in attack success rates when restricted to single principles. The absence of "shorten" from effective strategies suggests safety mechanisms are more susceptible to content dilution than condensation.

### Mechanism 2: Initial Seed Prompt Generation to Bypass Direct Refusal
The framework addresses the challenge that safety-aligned LLMs used as Red-Teaming Agents will refuse to process explicitly harmful queries. A specially crafted prompt (Template 1) transforms harmful queries into initial jailbreak prompts by reframing the task context as "red team security research" and instructing the agent to leverage its understanding of LLM architecture. This intermediate transformation step circumvents the agent's safety alignment without requiring fine-tuning.

### Mechanism 3: Dual-Judge Evaluation (Effectiveness + Similarity)
CoP employs two independent evaluations to prevent "prompt drift" and improve final attack success: jailbreak score assessing Target LLM response harmfulness relative to original query, and similarity score assessing prompt-query semantic alignment. This dual evaluation prevents false positives where attacks succeed by changing the task to something irrelevant. Removing the similarity judge reduces attack success rates by 12% and average similarity scores from 8.9 to 6.36, confirming its importance.

## Foundational Learning

- **LLM Safety Alignment (RLHF/SFT)**: Understanding what safety alignment blocks and how transformations circumvent it is crucial for CoP's effectiveness. Quick check: Can you explain why gradient-based attacks (GCG) differ fundamentally from prompt-based attacks like CoP?

- **Jailbreak Attack Taxonomy**: CoP positions itself against optimization-based (GCG), obfuscation-based (Base64), and LLM-guided methods (PAIR, TAP). Quick check: What is the key limitation of PAIR/TAP that CoP claims to address?

- **Agentic Workflow Design**: CoP relies on an LLM-based agent autonomously orchestrating principles, not just generating text. Quick check: What components must an LLM-based agent have beyond text generation capability?

## Architecture Onboarding

- **Component map**: Harmful query → Initial Seed Generation → P_init → Principle Composition → CoP Strategy → Apply Strategy → P_CoP → Query Target LLM → Judge LLM scores → Update P* or continue → Terminate when s≥η OR max iterations

- **Critical path**: 1) Harmful query → Initial Seed Generation (Template 1) → P_init, 2) P_init → Principle Composition (Template 2) → CoP Strategy, 3) Apply Strategy (Template 3) → P_CoP → Query Target LLM, 4) Judge LLM scores (s, σ) → Update P* or continue, 5) Terminate when s≥η OR max iterations reached

- **Design tradeoffs**: GPT-4o shows better alignment with HarmBench classifier than GPT-4 for frontier models but increases cost; η=10 for strict success; η≥7 for O1/Claude-3.5 Sonnet due to stronger alignment; Grok-2 avoids Direct Refusal while Gemini Pro 1.5 is configurable but 10 pp lower ASR

- **Failure signatures**: Direct Refusal by Red-Teaming Agent (returns safety message instead of JSON), Prompt drift (low similarity scores), JSON parsing failures from Red-Teaming Agent

- **First 3 experiments**: 1) Baseline validation: Run CoP on Llama-2-7B-Chat with 50 HarmBench queries, compare ASR to paper's 77.0% result, 2) Ablation check: Disable multi-principle composition, verify ~58% ASR drop, 3) Judge alignment test: Compare GPT-4 vs GPT-4o as Judge LLM on 20 queries against frontier model, measure agreement with HarmBench classifier

## Open Questions the Paper Calls Out

### Open Question 1
Can the Composition-of-Principles (CoP) framework be effectively extended to multi-turn conversational attacks? The current implementation focuses exclusively on single-turn prompt-response pairs, leaving the complexity of context-dependent, sequential attacks unexplored. Empirical results testing CoP in multi-turn dialogues compared to state-of-the-art multi-turn baselines like X-Teaming would resolve this question.

### Open Question 2
Why does the "expansion" strategy significantly outperform "condensation" strategies like "shorten" in bypassing safety alignment? While the paper identifies the statistical prevalence of expansion, it does not isolate the specific attention or representation mechanisms that cause safety filters to fail on longer, diluted contexts. A mechanistic interpretability study analyzing internal model states during expansion-based attacks versus condensation-based attacks would resolve this question.

### Open Question 3
Can the reliance on human-provided principle inventories be automated or replaced by self-evolving strategies to prevent obsolescence? The paper acknowledges that CoP's performance depends heavily on the initial principle inventory and could diminish against future safety implementations without regular human refinement. It is unclear if the agentic workflow can autonomously generate new principles as effective as human-designed ones when faced with novel or adaptive safety training. An ablation study where the Red-Teaming Agent invents new principles from scratch against a model with dynamic, updated guardrails would resolve this question.

## Limitations
- Heavy reliance on proprietary Judge LLMs (GPT-4, GPT-4o) introduces opacity in evaluation methodology
- Effectiveness appears heavily dependent on specific choice of Grok-2 Red-Teaming Agent
- Primary evaluation through HarmBench classification may not generalize to real-world malicious queries
- Principle inventory may become obsolete without regular human refinement

## Confidence

**High Confidence (Level 4/5)**: Core architectural claim that multi-principle composition achieves significantly higher ASR than single-principle approaches is well-supported by ablation study (58% drop).

**Medium Confidence (Level 3/5)**: Computational efficiency claims are credible given improved success rates, but trade-off between principle complexity and overhead lacks detailed analysis.

**Low Confidence (Level 2/5)**: Generalizability of principle inventory beyond 7 defined principles remains untested; limited evidence of framework's adaptability to emerging attack patterns.

## Next Checks

1. **Judge Prompt Transparency**: Request exact prompt templates used for jailbreak and similarity scoring from authors to enable independent validation of evaluation methodology.

2. **Cross-Dataset Generalization**: Test CoP on distinct malicious query dataset not included in HarmBench to verify 88.0% ASR translates to comparable performance on unseen attack patterns.

3. **Principle Discovery Capacity**: Design experiment where CoP is applied to models with known, undocumented vulnerabilities to track whether framework can discover these through principle composition rather than optimizing known patterns.