---
ver: rpa2
title: 'PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large
  Vision Language Model'
arxiv_id: '2503.18484'
source_url: https://arxiv.org/abs/2503.18484
tags:
- arxiv
- vision
- performance
- languages
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PM4Bench addresses the challenge of fairly evaluating multilingual
  vision-language models by introducing a parallel corpus across 10 languages, eliminating
  content bias, and incorporating a vision setting where text and queries are embedded
  in images. The benchmark covers three tasks: MDUR (multi-discipline understanding
  and reasoning), MIQA (multi-image question answering), and MSOCR (multi-scale OCR
  challenge).'
---

# PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model

## Quick Facts
- arXiv ID: 2503.18484
- Source URL: https://arxiv.org/abs/2503.18484
- Reference count: 22
- 10 LVLMs evaluated showing significant performance drops in vision setting, with OCR capability identified as key bottleneck for multilingual performance

## Executive Summary
PM4Bench introduces a parallel corpus benchmark across 10 languages to fairly evaluate multilingual vision-language models (LVLMs). Unlike existing benchmarks that mix content and capability differences, PM4Bench uses semantically identical content across languages to isolate true capability gaps. The benchmark includes three tasks (MDUR, MIQA, MSOCR) and introduces a vision setting where text is embedded in images rather than provided as separate tokens, revealing OCR as a critical bottleneck for multilingual performance. Evaluation shows that while LVLMs perform well in traditional settings, their vision performance drops significantly, particularly for languages with complex scripts, and that model scaling helps mitigate these cross-lingual disparities.

## Method Summary
PM4Bench evaluates LVLMs across three tasks: MDUR (1730 multi-choice questions from MMMU-pro), MIQA (218 open-ended QA pairs from MMDU), and MSOCR (100 progressive font size images per language). The benchmark covers 10 languages using a parallel corpus design where questions are manually translated and verified for semantic equivalence. Two settings are evaluated: Traditional (separate image and text inputs) and Vision (text rendered onto images via HTML templates). Evaluation uses exact match accuracy for MDUR, LLM-as-judge (DeepSeek-v3.2) for MIQA across 6 dimensions, and font-size-based scoring for MSOCR. OCR capability is specifically tested on MDUR by extracting embedded text while ignoring image content.

## Key Results
- OCR capability is a critical bottleneck, with performance recovery observed when ground-truth text is provided to models
- Vision setting is significantly more challenging than Traditional setting, with large performance drops across all evaluated models
- Model scaling reduces cross-lingual imbalance in vision tasks, with coefficient of variation (S_cv) consistently decreasing as model size increases
- Cross-lingual disparities are pronounced, particularly for languages with complex scripts like Thai and Arabic

## Why This Works (Mechanism)

### Mechanism 1: Parallel Corpus Isolates Cross-Lingual Capability
- **Claim:** Using a strictly parallel corpus across 10 languages allows for the isolation of model capability gaps from dataset artifacts.
- **Mechanism:** By holding semantic content constant (identical questions in different languages) and manually verifying translations, the benchmark ensures that performance variance is attributable to the model's internal language processing rather than differences in question difficulty or cultural context.
- **Core assumption:** Manual translation and LLM-assisted selection effectively eliminate semantic drift and cultural bias across all 10 target languages.
- **Evidence anchors:** [abstract] "PM4Bench features a parallel corpus design across 10 languages, enabling fair and accurate cross-lingual comparisons." [section 3.1] "The content across languages must be semantically identical... enabling accurate evaluation and fair cross-lingual comparison."
- **Break condition:** If translations contain unlocalized idioms or if the visual rendering of text introduces noise specific to a language, the "fairness" of the comparison is compromised.

### Mechanism 2: The Vision Setting Exposes the OCR Bottleneck
- **Claim:** The "Vision Setting," where text is rendered onto the image rather than provided as a separate token stream, reveals that OCR capability is a primary bottleneck for multilingual LVLMs.
- **Mechanism:** This design forces the visual encoder to transcribe text before reasoning. If the OCR fails (particularly for complex scripts), the subsequent reasoning step receives garbage input, causing a cascade of errors not present in "Traditional" settings where text is provided cleanly.
- **Core assumption:** The performance drop in Vision settings is predominantly driven by text recognition failures rather than the loss of textual context length limits or layout understanding.
- **Evidence anchors:** [section 5.4] "OCR capability is a critical bottleneck for vision tasks... disparities among OCR capabilities of languages is a key factor contributing to cross-lingual imbalance." [section 5.2] "Vision setting is more challenging... demonstrates model's limited perception... when processing purely visual inputs."
- **Break condition:** If a model uses a tool-augmented architecture that perfectly transcribes text before passing it to the LLM, this bottleneck mechanism is bypassed.

### Mechanism 3: Scaling Mitigates Visual-Textual Imbalance
- **Claim:** Increasing model scale specifically improves robustness in the Vision setting by enhancing the alignment between visual pixel patterns and textual tokens for diverse scripts.
- **Mechanism:** Larger models possess greater capacity to learn invariant features of scripts across different fonts, scales, and layouts, thereby reducing the coefficient of variation across languages.
- **Core assumption:** The improvement is driven by intrinsic visual-learning capacity rather than simply having more multilingual OCR training data in the pre-training set.
- **Evidence anchors:** [section 5.3] "Scaling helps bridge the gap across languages by enhancing visual text recognition." [section 5.3] Fig. 7 visualization shows S_cv consistently decreasing as model size increases for the Vision setting.
- **Break condition:** If the smaller models were fine-tuned on specific OCR data while larger models were not, the scaling correlation would be confounded.

## Foundational Learning

- **Concept: Parallel Corpora vs. Comparable Corpora**
  - **Why needed here:** To understand why PM4Bench claims "fairness." Unlike comparable corpora (different texts on similar topics), parallel corpora are translations of the exact same content, acting as a control variable in the experiment.
  - **Quick check question:** If a model performs better on English questions about baseball and Japanese questions about sushi, is this a parallel corpus failure or a capability gap? (Answer: It's ambiguous without a parallel corpus; the benchmark eliminates this ambiguity).

- **Concept: OCR (Optical Character Recognition) as a Pre-training Task**
  - **Why needed here:** The paper identifies OCR not just as a text-extraction utility, but as a foundational capability that limits higher-order reasoning.
  - **Quick check question:** Why does the paper argue that improving OCR reduces cross-lingual unfairness? (Answer: Because complex scripts like Thai or Arabic are harder to recognize visually; fixing this levels the playing field).

- **Concept: LVLM Input Modalities (Interleaved vs. Unified)**
  - **Why needed here:** The paper distinguishes between "Traditional" (separate image and text tensors) and "Vision" (text rendered into the image tensor).
  - **Quick check question:** In the "Vision Setting," does the LLM receive the question as token IDs? (Answer: No, it must extract it from the pixel data of the image).

## Architecture Onboarding

- **Component map:** Source Data (MMMU-pro, MMDU, Generated OCR) -> Translation Pipeline (Kimi K2 -> Human -> Claude-4.5) -> Rendering Engine (HTML -> Screenshot) -> Evaluation (DeepSeek-v3.2 / Exact Match)

- **Critical path:** Data Curation (Select English samples) -> Parallel Translation (Ensure semantic equivalence) -> Visual Rendering (Print translated text onto images) -> Evaluation (Run inference, score Traditional vs Vision)

- **Design tradeoffs:** Human vs. LLM Translation uses hybrid approach (Kimi + Human + Claude) to balance cost vs. nuance. Exact Match vs. Semantic Match uses strict Exact Match Accuracy for MSOCR to verify precision for critical data.

- **Failure signatures:** High S_cv indicates model instability across languages (likely failing on non-Latin scripts). Vision drop-off indicates visual-perception/OCR failure rather than reasoning failure.

- **First 3 experiments:**
  1. **Sanity Check (Traditional vs. Vision):** Run baseline model on MDUR in both settings. Massive Vision drop confirms OCR bottleneck hypothesis.
  2. **OCR Ceiling Test:** Run MSOCR task to determine minimum font size model can handle per language. Predicts language-specific failure rates in Vision setting.
  3. **Ablation with Text Supplement:** (As done in paper, Table 4) Run Vision setting with ground-truth text. If performance recovers, error isolated to OCR, not reasoning.

## Open Questions the Paper Calls Out

- **Does training on dedicated multilingual OCR data improve downstream reasoning performance and cross-lingual balance in LVLMs?**
  - **Basis in paper:** [explicit] Section 7 states the lack of a constructed OCR training dataset to verify this transfer is a key limitation and future direction.
  - **Why unresolved:** The paper identifies OCR as a bottleneck via oracle text injection but has not validated if trainable OCR improvements transfer to reasoning tasks.
  - **What evidence would resolve it:** Evaluation of LVLMs fine-tuned on a multilingual OCR corpus on the MDUR and MIQA tasks.

- **Can the MSOCR task serve as a scalable, low-cost proxy for overall multilingual LVLM capabilities?**
  - **Basis in paper:** [explicit] Section 5.5 suggests "future work can readily expand MSOCR's scale" to approximate capabilities due to its high correlation with other tasks.
  - **Why unresolved:** Current scale is small (100 images/language), and it's unproven if this metric generalizes as a standalone evaluator without expensive parallel corpus translation.
  - **What evidence would resolve it:** A large-scale correlation study between expanded MSOCR scores and MDUR/MIQA scores across diverse languages.

- **Is external OCR integration more efficient than model scaling for mitigating cross-lingual performance disparities?**
  - **Basis in paper:** [inferred] Section 5.4 suggests external OCR tools are a "relatively cost-effective optimization strategy," while Section 5.3 shows scaling helps, but the trade-off is undefined.
  - **Why unresolved:** The paper doesn't quantify performance-per-cost ratio of increasing parameters versus augmenting smaller models with OCR tools.
  - **What evidence would resolve it:** Comparative analysis of inference costs and accuracy gains between scaled monolithic models and OCR-augmented lightweight models.

## Limitations

- The LLM-as-judge methodology for MIQA introduces potential bias toward the judge's own capabilities and training data, without inter-judge reliability metrics or human evaluation samples.
- Manual verification process for 10 languages across thousands of samples remains a potential source of undetected semantic drift despite claims of equivalence.
- HTML-based rendering may not fully capture real-world text rendering diversity (screen resolutions, camera distortions, lighting conditions), potentially underestimating OCR difficulty.

## Confidence

- **High Confidence**: OCR capability is a bottleneck for multilingual LVLMs (supported by PCC > 0.5 correlation and ablation study).
- **Medium Confidence**: Scaling reduces cross-lingual imbalance in vision settings (supported by S_cv decreasing with model size, but not controlling for multilingual OCR data exposure).
- **Medium Confidence**: Vision setting is more challenging than traditional inputs (empirically demonstrated but doesn't fully explore all difficulty sources).

## Next Checks

1. **Inter-judge Reliability Test**: Run MIQA samples through multiple LLM judges (DeepSeek-v3.2, GPT-4o, Claude-3.5) to quantify inter-annotator agreement and assess LLM-as-judge methodology stability.

2. **Real-World Rendering Validation**: Generate PM4Bench samples using actual screenshots from mobile devices and desktop browsers under varying conditions to test whether HTML rendering underestimates real-world OCR difficulty.

3. **OCR-Only Capability Mapping**: For each language, create controlled dataset of text-only images varying by font complexity, size, and background noise. Map these to PM4Bench performance to create predictive model of vision setting outcomes based on OCR capability alone.