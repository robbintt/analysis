---
ver: rpa2
title: 'MoE Pathfinder: Trajectory-driven Expert Pruning'
arxiv_id: '2512.18425'
source_url: https://arxiv.org/abs/2512.18425
tags:
- expert
- pruning
- arxiv
- experts
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel expert pruning approach for mixture-of-experts
  (MoE) language models that addresses limitations of existing methods by reformulating
  expert selection as a global optimal path planning problem. The method constructs
  a weighted computation graph where experts are nodes and transitions between adjacent
  layers are edges, with weights computed from transition intensities and expert importance
  scores.
---

# MoE Pathfinder: Trajectory-driven Expert Pruning

## Quick Facts
- arXiv ID: 2512.18425
- Source URL: https://arxiv.org/abs/2512.18425
- Reference count: 18
- Key outcome: Trajectory-driven pruning achieves 53.77% avg accuracy at 50% sparsity vs 28.87% for random baseline on Mixtral-8x7B

## Executive Summary
This paper addresses expert pruning for Mixture-of-Experts language models by reformulating expert selection as a global optimal path planning problem. The method constructs a weighted computation graph where experts are nodes and transitions between adjacent layers are edges, with weights computed from transition intensities and expert importance scores. Dynamic programming identifies top-ranked critical inference trajectories, enabling non-uniform expert retention across layers. Experiments on six benchmark datasets using Mixtral-8x7B models show the approach achieves superior pruning performance compared to baselines, with 50% expert sparsity retaining 53.77% average accuracy versus 28.87% for random pruning.

## Method Summary
The method operates through a calibration phase followed by dynamic programming path finding. First, k-means clustering selects representative calibration samples from each task's training data. For each calibration sample, the model performs a forward pass collecting activation strengths, routing probabilities, and reconstruction losses. Transition intensities are computed as products of upstream activation and downstream routing probability, while expert importance scores are derived from reconstruction fidelity. A weighted DAG is constructed where nodes represent experts and edges represent inter-layer transitions. Dynamic programming with priority queues identifies top-m paths by log-weight sum, and experts appearing on these paths across all calibration samples are retained. The approach naturally yields non-uniform layer-wise retention, preserving critical layers while aggressively pruning redundant ones.

## Key Results
- 50% expert sparsity achieves 53.77% average accuracy vs 28.87% for random pruning baseline on Mixtral-8x7B
- Combined weighting (transition intensity + expert importance) outperforms TI-only (51.98%) and IS-only (43.36%) ablations
- Non-uniform layer-wise retention shows higher expert concentration in deep layers for domain-specific tasks (MedQA, GSM8K) versus distributed importance for general tasks (MMLU)
- Optimal calibration cluster count varies by task (K=12 for WinoGrande, K=10 for ARC, K=5 for MedQA), suggesting task-dependent structural complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global path planning captures cross-layer expert dependencies that local metrics miss.
- Mechanism: The approach constructs a directed acyclic graph where experts are nodes and inter-layer transitions are weighted edges. Dynamic programming identifies top-m highest-weight paths spanning all layers. Experts not appearing on critical paths are pruned.
- Core assumption: Expert importance is better captured through participation in high-value computation trajectories than through isolated per-layer statistics.
- Evidence anchors: [abstract] "casts expert selection as a global optimal path planning problem"; [section 2.3] "we cast expert selection as identifying the top globally significant computation paths through a dynamic-programming-based path exploration scheme"; [corpus] Related work (DiEP, REAP) confirms non-uniform pruning outperforms uniform approaches, supporting the heterogeneity assumption.

### Mechanism 2
- Claim: Combining reconstruction error, routing probability, and activation strength provides complementary signals for expert importance.
- Mechanism: Transition intensity t(l)_{i,j} = a(l)_i × r(l+1)_j factors upstream activation (how much expert i outputs) with downstream routing (how likely expert j is selected). Expert importance e(l)_i = softmax(-L_i) measures reconstruction fidelity. Both are multiplied in the path weight.
- Core assumption: Strong experts both emit meaningful signals and are preferentially routed to; reconstruction loss proxies functional importance.
- Evidence anchors: [section 2.2] Equations 1-10 define the complete weight computation scheme; [table 5] Ablation shows TI-only achieves 51.98% avg, IS-only achieves 43.36% avg (with GSM8K collapse to 0.70%), combined achieves 53.77%; [corpus] Limited direct corpus evidence on multi-signal integration; related papers focus on single metrics.

### Mechanism 3
- Claim: Non-uniform layer-wise retention preserves critical layers while aggressively pruning redundant ones.
- Mechanism: Path aggregation across calibration samples yields E_keep as the union of experts on selected paths. Layers with many critical paths retain more experts; redundant layers naturally retain fewer.
- Core assumption: Expert importance distribution is heterogeneous across layers, with some layers requiring more experts than others.
- Evidence anchors: [abstract] "naturally yields non-uniform expert retention across layers"; [figure 2] Visualization shows concentrated importance in deep layers for domain-specific tasks (MedQA, GSM8K) vs. distributed importance for general tasks (MMLU); [corpus] DiEP and Cluster-Driven Expert Pruning similarly advocate non-uniform sparsity, corroborating the heterogeneity assumption.

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) sparse activation**
  - Why needed here: The entire method assumes understanding that MoE models activate only a subset of experts per token (e.g., 2 of 8 in Mixtral), yet all weights must be loaded during inference.
  - Quick check question: If a model has 8 experts per layer and activates 2 per token, what fraction of expert parameters are used for a single forward pass?

- Concept: **Dynamic programming for k-best paths in DAGs**
  - Why needed here: The core algorithm extends partial paths layer-by-layer, maintaining priority queues at each node to track top-m prefixes without exponential enumeration.
  - Quick check question: Why does processing nodes in topological order guarantee all incoming paths are considered before extending?

- Concept: **Calibration data for importance estimation**
  - Why needed here: Weights are computed from sample statistics; representative calibration data is critical. The paper uses k-means clustering to select diverse samples.
  - Quick check question: What could go wrong if calibration data only covers easy examples from a dataset?

## Architecture Onboarding

- Component map:
Calibration Data (k-means sampled) → Forward Pass → Collect activations, routing probs, outputs → Weight Computation (Transition intensity + Expert importance) → Graph Construction (L layers × N_e experts) → Path Planning (DP: top-m paths) → Expert Union (E_keep) → Pruning (Binary mask M)

- Critical path: The calibration data quality → weight computation → path selection. If calibration samples don't represent task diversity, critical experts may be missed.

- Design tradeoffs:
  - **Cluster count K**: Too few clusters → incomplete coverage; too many → noise/redundancy. Paper finds K=5-20 optimal depending on task (Figure 3).
  - **Path count m**: Controls expert coverage. m=1 for 50% sparsity, m=500 for 25% sparsity on Mixtral.
  - **Sparsity vs. performance**: 50% sparsity retains ~53.77% avg accuracy; 25% sparsity retains ~59.20% (Table 4). Diminishing returns.

- Failure signatures:
  - **GSM8K collapse with IS-only**: 0.70% accuracy (Table 5) indicates mathematical reasoning requires both routing structure and reconstruction fidelity.
  - **Random pruning baseline**: 28.87% avg at 50% sparsity shows MoE is highly sensitive to which experts are removed.
  - **Knowledge-intensive tasks (MMLU, MedQA)**: Higher sensitivity to sparsity than commonsense tasks (WinoGrande), suggesting distributed knowledge across many experts.

- First 3 experiments:
  1. **Sanity check**: Run random pruning at 50% sparsity on Mixtral-8x7B for MMLU. Verify severe degradation (~25% accuracy per Table 2) to confirm the baseline.
  2. **Ablation on weight components**: Implement TI-only, IS-only, and combined weighting on a single dataset (e.g., WinoGrande). Reproduce the ablation pattern from Table 5.
  3. **Cluster sensitivity**: Vary K ∈ {5, 10, 15, 20} for a single task and plot accuracy vs. K. Confirm unimodal pattern from Figure 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the trajectory-driven pruning approach generalize to MoE architectures beyond Mixtral (e.g., Switch Transformers, DeepSeekMoE, GPT-4 MoE) with different expert counts and routing mechanisms?
- Basis in paper: [inferred] The experiments are limited to Mixtral-8x7B and Mixtral-8x7B-Instruct, both with identical architecture (32 layers, 8 experts per layer, top-2 routing).
- Why unresolved: Different MoE architectures may have different routing dynamics, expert specialization patterns, and layer-wise heterogeneity that could affect the validity of the path planning formulation.
- What evidence would resolve it: Experiments applying the method to at least 2-3 other MoE architectures with varying expert counts (e.g., 4, 16, 64 experts) and routing strategies (top-1, expert choice).

### Open Question 2
- Question: How can the optimal number of calibration clusters (K) be determined automatically for a given task without empirical tuning?
- Basis in paper: [explicit] "the optimal number of clusters differs between the two tasks (K=12 for WinoGrande and K=10 for ARC). This discrepancy reflects the inherent structural complexity and feature space heterogeneity of the different datasets, suggesting that building a high-quality calibration set is task-dependent."
- Why unresolved: Currently K is selected via trial-and-error, adding overhead to the pruning pipeline and reducing its practicality for new domains.
- What evidence would resolve it: A systematic study correlating dataset characteristics (e.g., diversity metrics, cluster validity indices) with optimal K, or an adaptive K-selection algorithm that converges without manual intervention.

### Open Question 3
- Question: Does fine-tuning or retraining the router weights after pruning yield additional performance gains compared to simply retaining the original router rows?
- Basis in paper: [inferred] The paper mentions retaining "only the rows in the routing matrix that correspond to surviving experts" but does not explore whether the router could benefit from post-pruning adaptation.
- Why unresolved: The pruning changes the expert distribution non-uniformly across layers, potentially creating routing mismatches where the original router assigns probability mass to pruned experts.
- What evidence would resolve it: Ablation experiments comparing: (1) current approach, (2) router fine-tuning on calibration data post-pruning, (3) joint router-expert fine-tuning, measured across all six benchmarks.

## Limitations

- **Calibration data dependence**: The approach relies heavily on representative calibration samples to compute accurate expert importance weights, with no evaluation of calibration data quality or robustness to distribution shifts.
- **Single-path limitation at high sparsity**: At 50% sparsity, only the top-1 path is retained, creating brittleness where performance depends entirely on a single computation trajectory.
- **GSM8K-specific sensitivity**: The catastrophic failure of IS-only weighting on mathematical reasoning tasks (0.70% accuracy) reveals architecture-specific limitations that may not generalize across task types.

## Confidence

**High confidence** in the core algorithmic framework and implementation. The dynamic programming approach for k-best paths in DAGs is well-established, and the weight computation equations are explicitly specified. The ablation results (TI-only vs IS-only vs combined) show consistent patterns across tasks, supporting the mechanism claims.

**Medium confidence** in cross-layer dependency capture. While the path planning formulation is theoretically sound and outperforms local metrics, the paper provides limited ablation on path vs. layer importance. Figure 2 shows distribution differences across tasks, but doesn't directly test whether global paths outperform simple per-layer top-k retention.

**Low confidence** in calibration methodology robustness. The paper specifies k-means clustering with task-specific K values but doesn't explore calibration data size sensitivity, clustering feature choices, or robustness to calibration distribution shifts. The GSM8K failure with IS-only suggests calibration methodology may be task-sensitive in ways not fully characterized.

## Next Checks

1. **Calibration data quality analysis**: Systematically vary the number of calibration samples (e.g., 10, 50, 100, 500 per cluster) and measure impact on pruning accuracy. Plot accuracy vs. calibration sample count for each task to identify minimum viable calibration size and assess sensitivity.

2. **Multi-path sensitivity at high sparsity**: At 50% sparsity (m=1), measure performance variance across multiple random seeds or calibration sets. Then incrementally increase m (1→5→10→20) and measure accuracy gains to quantify brittleness of single-path selection.

3. **Layer-wise retention analysis**: For each layer, compute the fraction of retained experts and correlate with layer depth and task type. Test whether simple per-layer top-k retention (based on aggregated importance scores) achieves comparable performance to full path planning, to validate whether cross-layer structure is essential.