---
ver: rpa2
title: Automatic Reward Shaping from Multi-Objective Human Heuristics
arxiv_id: '2512.15120'
source_url: https://arxiv.org/abs/2512.15120
tags:
- reward
- task
- performance
- exploration
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MORSE, a framework that automatically learns
  reward weights from human-designed heuristics in multi-objective RL tasks. The method
  formulates reward shaping as a bi-level optimization problem, combining policy training
  in the inner loop with reward weight updates in the outer loop.
---

# Automatic Reward Shaping from Multi-Objective Human Heuristics

## Quick Facts
- arXiv ID: 2512.15120
- Source URL: https://arxiv.org/abs/2512.15120
- Authors: Yuqing Xie; Jiayu Chen; Wenhao Tang; Ya Zhang; Chao Yu; Yu Wang
- Reference count: 40
- Primary result: MORSE learns reward weights from human heuristics, outperforming vanilla bi-level optimization and matching human-tuned oracle rewards across 2-9 objective tasks

## Executive Summary
MORSE addresses the challenge of multi-objective reward shaping by automatically learning reward weights from human-designed heuristics. The framework formulates this as a bi-level optimization problem where the inner loop trains a policy while the outer loop updates reward weights. To overcome local optima in non-convex reward landscapes, MORSE incorporates stochastic exploration guided by task performance and novelty scores computed via Random Network Distillation. Experiments demonstrate MORSE's effectiveness across MuJoCo and Isaac Sim environments, with ablation studies confirming the importance of each component including performance-based exploration and policy resetting.

## Method Summary
MORSE implements a bi-level optimization framework that trains policies using PPO in the inner loop while updating reward weights in the outer loop. The outer loop uses meta-gradients derived from task performance via the Implicit Function Theorem, approximated using Neumann series. To avoid local optima, MORSE incorporates stochastic exploration where weights are sampled based on novelty scores from Random Network Distillation. When new weights are selected, the policy network is reset to maintain high entropy and encourage re-adaptation. The method supports both constrained ([0,1]) and unconstrained ([-1,1]) weight spaces, with exploration triggered based on performance stagnation.

## Key Results
- MORSE consistently outperforms vanilla bi-level optimization across 2-9 objective tasks
- Performance matches human-tuned oracle rewards on all tested environments
- Ablation studies confirm policy resetting is critical for maintaining high entropy during weight updates
- RND-guided exploration enables escape from local optima in non-convex reward landscapes

## Why This Works (Mechanism)

### Mechanism 1: Bi-Level Gradient Alignment
The system computes gradients of task performance with respect to reward weights by treating them as hyper-parameters. Using the Implicit Function Theorem with Neumann series approximation, it avoids expensive Hessian computations while maintaining gradient fidelity, assuming the inner policy has converged locally before outer updates.

### Mechanism 2: Stochastic Exploration via RND in Weight Space
To escape local optima in non-convex reward landscapes, MORSE samples candidate weights based on novelty scores from Random Network Distillation. High prediction error indicates unexplored regions of weight space, with sampling via softmax encouraging efficient exploration beyond what deterministic gradients can achieve.

### Mechanism 3: Policy Resetting for Objective Adaptation
When reward weights change, the policy is reset (actor fully, critic last layer) to restore high entropy and prevent "overfitting" to previous weights. This forces re-adaptation to new objective balances, maintaining exploration capability during weight transitions.

## Foundational Learning

- **Concept: Bi-Level Optimization / Meta-Gradients**
  - Why needed: We're training both the policy and the objective function itself, requiring outer-loop optimization of reward weights
  - Quick check: Can you explain why we need the Implicit Function Theorem to compute dJ/dφ? (Answer: To avoid explicitly computing the expensive inverse Hessian of the inner loop)

- **Concept: Random Network Distillation (RND)**
  - Why needed: RND drives exploration by using prediction error as a proxy for novelty in weight space
  - Quick check: In MORSE, what does the RND predictor try to predict, and what does a high error signify? (Answer: It predicts the output of a fixed network given a weight vector; high error signifies a novel/unvisited region of weight space)

- **Concept: Sparse vs. Dense Rewards**
  - Why needed: MORSE uses dense heuristics for policy training but sparse task criteria for weight updates, preventing reward hacking
  - Quick check: Which reward signal updates the policy parameters (θ), and which signal updates the reward weights (φ)? (Answer: The shaped/dense reward updates θ; the sparse task criteria updates φ)

## Architecture Onboarding

- **Component map:** Inner Loop (PPO trainer) -> Outer Loop (weight optimizer) -> Explorer (RND module) -> Policy Reset
- **Critical path:** 1) Initialize weights φ, 2) Train policy πθ to convergence, 3) Check task performance, 4) If stagnant: train RND → sample new φ via softmax(RND error) → reset policy, 5) Else: update φ via gradient ascent
- **Design tradeoffs:** Reset frequency balances exploration vs. learning stability; RND vs. random exploration trades efficiency for implementation complexity
- **Failure signatures:** Catastrophic forgetting (immediate performance drop after reset), gradient explosion (NaN weights), stagnation (RND overfitting causing uniform novelty scores)
- **First 3 experiments:** 1) Verify gradient flow on synthetic 2D function, 2) Validate RND novelty by visualizing prediction error over weight space, 3) Ablate policy reset on Walker2d-Hard to measure entropy restoration impact

## Open Questions the Paper Calls Out

- **Open Question 1:** Can UCB methods optimize the balance between exploration and exploitation in the outer loop more effectively than the current stochastic approach?
- **Open Question 2:** Can successor features replace the need to reset the policy network during reward weight updates?
- **Open Question 3:** How does MORSE performance scale in extremely high-dimensional objective spaces, and what priors are required to stabilize it?

## Limitations
- Neumann series approximation may inadequately capture true Hessian structure in high-dimensional spaces
- RND exploration effectiveness in reward weight space lacks direct comparative validation against random exploration
- Policy resetting discards learned features, potentially increasing training time despite ablation support

## Confidence

- **Bi-level optimization mechanism**: High confidence - well-established formulation with clear implementation details
- **RND-guided exploration**: Medium confidence - valid technique but limited validation in reward weight space specifically
- **Policy resetting component**: Medium confidence - ablation shows degradation when disabled, but mechanism needs deeper analysis

## Next Checks

1. **Convergence verification**: Test bi-level optimization on synthetic functions with known optima from multiple initializations to validate gradient flow and approximation accuracy
2. **Exploration efficiency test**: Compare RND-guided exploration against pure random weight sampling on benchmark tasks to quantify computational advantage
3. **Hessian approximation sensitivity**: Vary Neumann series truncation depth (K=3, 5, 10) on representative tasks to identify trade-off between cost and gradient fidelity