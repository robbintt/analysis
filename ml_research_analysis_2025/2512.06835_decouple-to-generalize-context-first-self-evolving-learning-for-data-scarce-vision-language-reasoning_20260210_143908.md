---
ver: rpa2
title: 'Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce
  Vision-Language Reasoning'
arxiv_id: '2512.06835'
source_url: https://arxiv.org/abs/2512.06835
tags:
- data
- reasoning
- training
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training vision-language\
  \ models (VLMs) in data-scarce specialized domains like chemistry and earth science,\
  \ where traditional reinforcement learning (RL) methods suffer from reward hacking\
  \ and limited generalization. The proposed DoGe framework decouples the learning\
  \ process into two stages: a Thinker component that explores contextual information\
  \ without direct problem-solving, and a Solver component that solves the original\
  \ task using the Thinker\u2019s analysis."
---

# Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning

## Quick Facts
- arXiv ID: 2512.06835
- Source URL: https://arxiv.org/abs/2512.06835
- Reference count: 40
- Performance gains: 5.7% for 3B models, 2.3% for 7B models over baselines

## Executive Summary
This paper introduces DoGe, a dual-decoupling framework for training vision-language models in specialized domains with limited data. The method separates context exploration from problem-solving into Thinker and Solver components, enabling models to first learn from contextual information before applying knowledge to solve tasks. By employing a two-stage reinforcement learning approach with an evolving curriculum, DoGe prevents reward hacking and improves generalization. Experiments across seven benchmarks show consistent performance improvements over traditional methods, with policy entropy analysis confirming enhanced exploration and stability during training.

## Method Summary
DoGe implements a two-stage reinforcement learning framework for data-scarce vision-language reasoning. Stage 1 trains a Thinker component using question-masked inputs and rewards based on a frozen Solver's pass rate, encouraging contextual analysis without direct problem-solving. Stage 2 applies standard GRPO with annealing from the Stage 1 checkpoint to solve original problems directly. The method includes an iterative self-evolution pipeline that synthesizes training data from an expanded domain knowledge corpus and updates a seed problems pool with occasionally solvable tasks (0.1-0.3 pass rate). The framework uses Qwen2.5VL-3B/7B models, 100-150 training steps per stage, and 8×A100 GPUs for implementation.

## Key Results
- Average performance gains of 5.7% for 3B models and 2.3% for 7B models across seven benchmarks
- Sustained policy entropy throughout training prevents reward hacking compared to naive GRPO baselines
- Three iterations of self-evolution show progressive improvement without performance regression
- Consistent improvements across diverse domains: chemistry, earth science, multimodal math

## Why This Works (Mechanism)

### Mechanism 1: Dual-Decoupling into Thinker-Solver Components
Separating context exploration from problem-solving creates a "learning-application" cycle that reduces reward hacking and improves generalization. The Thinker generates contextual analysis without seeing the specific question, while a frozen Solver attempts to solve problems using this analysis. The Solver's pass rate serves as the reward signal for updating the Thinker.

### Mechanism 2: Two-Stage RL with Sequential Reward Schemes
Sequential training prevents premature convergence to exploitable reward shortcuts. Stage 1 rewards contextual analysis quality using the frozen Solver's success rate, while Stage 2 applies standard GRPO directly on original problems, annealing from the Stage 1 checkpoint.

### Mechanism 3: Policy Entropy Maintenance to Avoid Reward Hacking
Decoupled context-first training elevates and sustains policy entropy, enabling sustained exploration rather than collapse onto high-reward shortcuts. The method reports higher initial entropy and sustained exploration versus naive GRPO baselines.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO replaces the value model in PPO with group-relative advantages, reducing VRAM while enabling RL for VLMs.
  - Quick check question: Can you explain how GRPO computes advantages without a critic network?

- **Reward Hacking in RL**
  - Why needed here: Understanding why models exploit high-reward shortcuts (entropy collapse) is essential to appreciating why DoGe's decoupling matters.
  - Quick check question: What behavioral signature indicates a model is reward hacking rather than learning generalizable reasoning?

- **Curriculum Learning with Seed Problem Pools**
  - Why needed here: DoGe's iterative self-evolution depends on maintaining and updating a pool of "occasionally solvable" problems to drive progressive improvement.
  - Quick check question: Why select problems with pass rates between 0.1–0.3 rather than easy or impossible problems?

## Architecture Onboarding

- **Component map**: Knowledge Pool → Problem Synthesizer → Seed Problem Pool → Thinker (Stage 1) → Solver (Stage 2) → Benchmark Evaluation
- **Critical path**: 1) Collect raw multimodal data → enrich low-information samples → populate Knowledge Pool; 2) Synthesize initial problems → filter by difficulty → populate Seed Pool; 3) Train Thinker with Solver-provided rewards (100–150 steps); 4) Apply GRPO annealing on original problems (150 steps); 5) Evaluate → update Seed Pool with occasionally-solvable problems; 6) Repeat from step 2
- **Design tradeoffs**: Sampling budget (more Solver samples improve reward estimation but increase compute); entropy clip ratios (decoupled ε_l and ε_h stabilize training); temperature settings (T=0.9 in Stage 1, T=1.0 in Stage 2)
- **Failure signatures**: Entropy collapse (policy entropy drops sharply early in training); zero Solver pass rate (Thinker receives no positive signal); performance regression across iterations (Seed Pool contamination); format reward dominance (model outputs correct format but wrong answers)
- **First 3 experiments**: 1) Baseline GRPO vs. DoGe (single iteration) on synthetic dataset comparing policy entropy curves and benchmark performance; 2) Ablate context-masking by running DoGe without masking the question in Stage 1; 3) Vary Solver sample count (n=2, 4, 8) per Thinker output to find compute-optimal settings

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Limited empirical validation for critical failure modes where Thinker analysis consistently fails to improve Solver performance
- Iterative curriculum component depends heavily on seed problem pool curation quality without validation for robustness against contamination
- Three iterations shown, but scaling behavior and long-term self-evolution stability remain unclear

## Confidence
- **High Confidence**: Stage 1 → Stage 2 sequential training improves performance over single-stage GRPO
- **Medium Confidence**: Dual-decoupling specifically prevents reward hacking
- **Low Confidence**: Iterative self-evolution with curriculum updates reliably improves generalization beyond initial data

## Next Checks
1. **Ablate the Thinker-Solver interface**: Run DoGe-1iter without masking the question in Stage 1. If entropy benefits disappear but performance gains remain, decoupling's primary benefit may be temporal sequencing rather than information restriction.

2. **Stress-test the reward signal**: Create synthetic problems where contextual analysis cannot help the Solver. Measure whether DoGe maintains exploration while naive GRPO collapses, confirming the decoupling prevents reward hacking even when context is irrelevant.

3. **Validate curriculum stability**: Run 10+ iterations of self-evolution and track performance on a held-out validation set. Monitor for performance plateaus, regressions, or entropy collapse that might indicate seed pool contamination or overfitting to synthetic data distributions.