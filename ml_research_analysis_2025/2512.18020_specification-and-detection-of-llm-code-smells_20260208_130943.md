---
ver: rpa2
title: Specification and Detection of LLM Code Smells
arxiv_id: '2512.18020'
source_url: https://arxiv.org/abs/2512.18020
tags:
- code
- systems
- smells
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces LLM code smells\u2014recurring poor coding\
  \ practices in LLM integration\u2014and formalizes five such smells identified through\
  \ literature analysis. The authors extend SpecDetect4AI into SpecDetect4LLM, a static\
  \ detection tool, and apply it to a dataset of 200 open-source Python systems."
---

# Specification and Detection of LLM Code Smells

## Quick Facts
- arXiv ID: 2512.18020
- Source URL: https://arxiv.org/abs/2512.18020
- Reference count: 40
- Key outcome: Five LLM code smells identified, affecting 60.50% of analyzed systems with 86.06% precision using SpecDetect4LLM tool

## Executive Summary
This study introduces LLM code smells—recurring poor coding practices in LLM integration—and formalizes five such smells identified through literature analysis. The authors extend SpecDetect4AI into SpecDetect4LLM, a static detection tool, and apply it to a dataset of 200 open-source Python systems. Results show that LLM code smells affect 60.50% of the analyzed systems, with an average detection precision of 86.06%. The catalog and tool provide actionable guidelines for improving code quality, maintainability, and reliability in LLM-integrating software systems.

## Method Summary
The authors analyzed 40 relevant papers to identify five LLM code smells: Unbounded Max Metrics (UMM), No Model Version Pinning (NMVP), No System Message (NSM), No Structured Output (NSO), and Temperature Not Explicitly Set (TNES). They extended SpecDetect4AI with a Domain Specific Language (DSL) to create SpecDetect4LLM, which uses semantic predicates to detect these patterns through static analysis of Python source code. The tool was applied to 200 open-source Python systems containing LLM integrations, with precision validated through stratified manual audits across at least 20 detections and 5 distinct systems per smell.

## Key Results
- 60.50% of analyzed LLM-integrating systems contain at least one code smell
- Detection precision averages 86.06% across all five smells
- NSM (No System Message) is the most prevalent smell at 39.00%
- NSO (No Structured Output) is the least prevalent at 9.00%
- TNES (Temperature Not Explicitly Set) affects 24.50% of systems

## Why This Works (Mechanism)

### Mechanism 1: Static Specification Matching
Static analysis using semantic predicates can identify LLM integration risks with high precision without executing the code. The SpecDetect4LLM tool extends SpecDetect4AI using a DSL that parses code into semantic predicates (e.g., identifying an "LLM call" node) and validates the presence or absence of specific arguments (e.g., temperature, max_tokens) against defined rules. This works because LLM integration patterns are consistent enough across libraries to be captured by generalized static rules, assuming the code doesn't obfuscate these calls dynamically.

### Mechanism 2: Reproducibility via Immutable Versioning
Pinning immutable model versions prevents silent behavioral regressions caused by provider-side updates. Moving aliases (e.g., gpt-4o) act as pointers that providers update, while explicitly pinning a snapshot (e.g., gpt-4o-2024-11-20) decouples system stability from the provider's deployment cycle, ensuring the same weights and safety filters are used. This assumes the provider maintains the availability of older snapshots and doesn't alter the snapshot's behavior post-hoc.

### Mechanism 3: Robustness via Bounded Metrics
Explicitly bounding tokens, timeouts, and retries prevents resource exhaustion and unhandled failures in non-deterministic systems. LLM APIs are stateful and latency-variable, so unbounded requests can hang indefinitely or return oversized payloads. Enforcing limits (e.g., max_retries=3) forces the system into a known failure state rather than an undefined hanging state, assuming downstream logic handles the resulting exceptions or truncation.

## Foundational Learning

- **Code Smells vs. Bugs**: Code smells are recurring poor practices harming maintainability/reliability rather than explicit defects or bugs. This distinction is needed to interpret why a detected smell is a "risk" rather than an "error." Quick check: Does the absence of a system message crash the program, or does it just degrade output consistency?

- **LLM Inference Parameters (Temperature & Tokens)**: Two smells (UMM, TNES) rely on understanding how temperature affects randomness and how max_output_tokens affects cost/capacity. This knowledge is needed to grasp why these parameters matter for system behavior. Quick check: If temperature is not set and the provider changes the default from 0.7 to 1.0, does the system's behavior change?

- **Static Analysis (AST/DSL)**: The detection method uses a DSL on top of code structure, requiring understanding that the tool "reads" the code structure (identifying a function call) rather than "running" it. Quick check: Can a static analyzer detect a smell if the API key is loaded from a file at runtime? (Answer: Yes, if the code structure defining the call is visible).

## Architecture Onboarding

- **Component map**: Catalog (5 smells) -> SpecDetect4LLM (static analyzer) -> Python source files (target)
- **Critical path**: Code Input → Parsing/AST Generation → DSL Rule Evaluation (e.g., "Is model a versioned string?") → Alert Generation
- **Design tradeoffs**: The tool optimizes for precision (86.06%) to avoid noise, potentially missing complex/dynamic instances (lower recall). Static analysis cannot detect runtime configuration smells like dynamic dictionaries passed as **kwargs.
- **Failure signatures**: False positives may occur when flagging calls for "No Temperature" when temperature is set via unrecognized configuration objects. Integration gaps exist as the tool currently focuses on Python, leaving multi-language repos with incomplete coverage.
- **First 3 experiments**:
  1. Run SpecDetect4LLM on a local Python project using OpenAI to identify baseline smell prevalence
  2. Refactor a gpt-4o call to gpt-4o-2024-11-20 and verify the NMVP alert disappears
  3. Implement Structured Output (fixing NSO) on a complex prompt to experience the engineering effort cited in Observation 2

## Open Questions the Paper Calls Out

- Do LLM code smells quantifiably degrade system reliability or performance? The authors plan to measure the impact on performance, robustness, reliability, and maintainability, but the current study focuses on definition and prevalence rather than causal impact.

- Can dynamic analysis effectively capture execution-dependent LLM code smells missed by static detection? The authors propose exploring dynamic detection to capture runtime effects beyond static detection, addressing SpecDetect4LLM's limitations.

- What is the detection recall of SpecDetect4LLM? The authors explicitly note that estimating recall was "out-of-scope" because it would require exhaustive file-level ground truthing, leaving the tool's completeness unknown.

## Limitations
- The detection tool may miss dynamically constructed LLM calls, creating potential blind spots for complex integration patterns
- The 200-system dataset represents a specific snapshot of open-source Python LLM integrations and may not generalize to enterprise codebases or other programming languages
- The five identified smells may not capture the full spectrum of LLM integration risks as the field evolves rapidly

## Confidence

**High Confidence**: Static specification matching approach and model version pinning recommendations are technically sound with empirically measured precision (86.06%) and straightforward reproducibility benefits.

**Medium Confidence**: Bounded metrics recommendation is reasonable but assumes specific failure-handling contexts that vary across applications; empirical prevalence data (60.50% affected) is dataset-specific.

**Low Confidence**: Catalog completeness claim is based on current observations rather than longitudinal studies, as literature-based identification may miss emergent patterns.

## Next Checks

1. **Precision vs. Recall Analysis**: Execute SpecDetect4LLM on a curated test suite containing both obvious and obfuscated LLM calls (e.g., dynamically constructed through **kwargs dictionaries). Measure both precision and recall to understand the false negative rate and identify patterns that evade static detection.

2. **Cross-Language Generalization**: Adapt or extend the DSL rules to detect equivalent smells in at least one other programming language (e.g., JavaScript/TypeScript for Node.js LLM integrations). Compare detection effectiveness and identify language-specific integration patterns that may require different rule specifications.

3. **Longitudinal Stability Study**: Monitor a subset of the 200 systems over 6-12 months to track smell prevalence changes, particularly for NMVP (model version pinning) as providers update their APIs. Measure whether identified smells correlate with reported production incidents or performance degradation over time.