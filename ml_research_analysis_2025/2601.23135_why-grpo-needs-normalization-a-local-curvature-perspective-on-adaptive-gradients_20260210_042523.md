---
ver: rpa2
title: 'Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients'
arxiv_id: '2601.23135'
source_url: https://arxiv.org/abs/2601.23135
tags:
- grpo
- gradient
- arxiv
- normalization
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical explanation for why variance
  normalization in GRPO works by showing it implements an adaptive gradient mechanism.
  The normalization uses reward variance as a proxy for local curvature, scaling updates
  inversely to the smoothness of each prompt's objective function.
---

# Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients

## Quick Facts
- arXiv ID: 2601.23135
- Source URL: https://arxiv.org/abs/2601.23135
- Reference count: 40
- Primary result: GRPO's normalization implements adaptive gradient scaling using reward variance as curvature proxy, achieving faster convergence than unnormalized REINFORCE with benefits characterized by average within-prompt reward standard deviation.

## Executive Summary
This paper provides a theoretical explanation for why variance normalization in GRPO works by showing it implements an adaptive gradient mechanism. The normalization uses reward variance as a proxy for local curvature, scaling updates inversely to the smoothness of each prompt's objective function. Under mild assumptions, the authors prove GRPO achieves faster convergence than unnormalized REINForce, with gains characterized by average within-prompt reward standard deviation. Empirical validation on GSM8K and MATH benchmarks reveals three training phases: early acceleration with high variance and orthogonality, stable transition with moderate variance, and late-stage where reduced orthogonality limits further gains. The results show normalization provides consistent improvements, especially on harder prompts, though benefits plateau as training progresses.

## Method Summary
The study compares GRPO with unnormalized REINFORCE on mathematical reasoning tasks using Qwen2.5-Math-1.5B with LoRA fine-tuning. The key difference is that GRPO normalizes advantages by within-prompt reward standard deviation, while REINFORCE uses only mean-centered advantages. Training proceeds for 500 iterations on GSM8K and MATH benchmarks with binary correctness rewards. The authors validate their theoretical claims through empirical measurements of gradient orthogonality, reward variance, and Fisher-curvature correlation across training phases.

## Key Results
- GRPO's normalization implements adaptive gradient scaling using reward variance as curvature proxy, achieving faster convergence than unnormalized REINForce
- Three training phases observed: early acceleration (high variance/orthogonality), stable transition (moderate variance), late-stage plateau (reduced orthogonality)
- Normalization provides consistent improvements, especially on harder prompts, though benefits plateau as training progresses

## Why This Works (Mechanism)

### Mechanism 1: Variance as Curvature Proxy (Adaptive Gradient)
- Claim: Within-prompt reward standard deviation serves as a local curvature estimate, enabling per-prompt adaptive step sizes that accelerate convergence.
- Mechanism: GRPO's normalization transforms the gradient: ∇J_i^GRPO(θ) = ∇J_i(θ) / √V[π_θ(i)]. Since Lemma 3.1 shows ||∇²J_i(θ)|| ≤ 4X²_max · V[π_θ(i)], dividing by √V effectively scales the step size inversely to local smoothness—larger steps for flatter regions, smaller for sharper ones.
- Core assumption: The local smoothness constant of each per-prompt objective is directly controlled by its reward variance (Lemma 3.1).
- Evidence anchors:
  - [abstract] "standard deviation normalization implements an adaptive gradient"
  - [section 3.1] "∥∇²J_i(θ)∥ ≤ 4X²_max · V[π_θ(i)]" formally links variance to curvature
  - [corpus] "Uncalibrated Reasoning" paper confirms normalization improves optimization speed (though at calibration cost)
- Break condition: High reward variance (accuracy 20–50%) produces noisy gradients that obscure curvature-adaptive benefits—Phase I shows comparable performance despite favorable geometry.

### Mechanism 2: Gradient Orthogonality Enables Prompt Decoupling
- Claim: Near-orthogonal prompt representations allow per-prompt adaptive updates without destructive interference, extending single-prompt analysis to the full objective.
- Mechanism: When feature matrices satisfy X_i X_j^⊤ ≈ 0, gradients for different prompts occupy orthogonal subspaces. Theorem 3.2 exploits this: updates from prompt i leave J_j(θ) unchanged for j ≠ i, enabling independent convergence analysis.
- Core assumption: Assumption 2 (orthogonal representation)—feature matrices for distinct prompts are orthogonal.
- Evidence anchors:
  - [section 4.1] Empirical validation: >90% of prompt pairs have |cos| < 0.15, mean ≈ 0.088
  - [section D.1] Lemma D.1 proves orthogonality implies zero gradient inner products
  - [corpus] Limited direct support; related GRPO papers don't address orthogonality explicitly
- Break condition: Phase III (iterations 300+)—gradient cosine similarity variance increases (std ≈ 0.130), orthogonality degrades, and normalization gains plateau.

### Mechanism 3: Bounded Cross-Prompt Interaction (Relaxed Regime)
- Claim: Exact orthogonality is unnecessary; bounded positive gradient interference preserves GRPO's advantage through controlled cross-prompt coupling.
- Mechanism: Assumption 4 (bounded cross-prompt interaction) ensures M⟨∇J_i, ∇J_j⟩ ≥ ||X_i∇J_j||²/||X_i||², making interference constructive rather than destructive. Theorems 3.3–3.4 show convergence with scaling factors from M, R_1, R_2.
- Core assumption: Assumptions 4–5—cross-prompt gradient inner products are positive and bounded; no single prompt dominates in gradient magnitude or variance.
- Evidence anchors:
  - [section 3.4] Theorems 3.3 and 3.4 formalize convergence under relaxed assumptions
  - [section F.1] M bound remains finite throughout training (M ≈ 418 at Step 500 for n=100)
  - [corpus] No direct corpus evidence for this specific relaxation mechanism
- Break condition: When gradient similarity variance becomes heterogeneous (Phase III), the assumption of uniformly bounded interference weakens, though GRPO maintains (not extends) its advantage.

## Foundational Learning

- **Policy Gradient with Baselines**
  - Why needed here: GRPO builds on REINFORCE's baseline subtraction; understanding why baselines reduce variance without introducing bias is essential.
  - Quick check question: Why does subtracting E[r] from rewards leave E[(r - E[r])∇log π] unbiased?

- **Smoothness and Step Size Selection**
  - Why needed here: The core insight links curvature (L-smoothness) to optimal step size (η ∝ 1/L); this explains why variance normalization helps.
  - Quick check question: For an L-smooth function, why is step size 1/L standard for gradient descent?

- **Feature Orthogonality in High Dimensions**
  - Why needed here: The theoretical analysis assumes orthogonal prompt representations; understanding this geometric property clarifies when decoupling holds.
  - Quick check question: In high-dimensional spaces, why are random vectors approximately orthogonal?

## Architecture Onboarding

- **Component map:**
  - GRPO (Algorithm 2) -> samples G responses per prompt -> computes group mean baseline -> normalizes by group std -> applies clipped PPO-style objective
  - REINFORCE (Algorithm 1) -> single response -> running mean baseline -> no normalization -> vanilla policy gradient
  - PPO -> learned value critic + GAE -> per-token advantages -> clipping on importance ratio

- **Critical path:**
  1. Prompt batch selection (uniform random)
  2. Response sampling (G samples per prompt from π_θ)
  3. Reward computation (verifiable: 0 or 1)
  4. Advantage normalization: Â_i = (r_i - mean(r_group)) / std(r_group)
  5. Gradient update with clipped objective

- **Design tradeoffs:**
  - **Normalization vs calibration**: Corpus paper "Uncalibrated Reasoning" notes normalization improves optimization but hurts probability calibration—consider application context.
  - **Group size G**: Larger G reduces variance in std estimate but increases compute; paper uses group-level statistics.
  - **KL penalty**: Paper focuses on RLVR (unregularized); adding KL term may alter dynamics.

- **Failure signatures:**
  - **Phase I stagnation**: High reward variance (~0.25) produces noisy gradients; normalization shows no clear advantage despite orthogonality.
  - **Phase III plateau**: Gradient orthogonality degrades (std cosine similarity rises to ~0.13), cross-prompt interference increases, normalization gains saturate.
  - **All-zero std**: When all responses in a group receive identical rewards (all correct or all incorrect), std = 0 causes division error—requires epsilon floor.

- **First 3 experiments:**
  1. **Orthogonality validation**: Extract penultimate-layer hidden states for 100+ prompt pairs; compute pairwise cosine similarities; verify >80% satisfy |cos| < 0.15 at initialization.
  2. **Curvature-variance correlation**: At each training step, compute diagonal Fisher information proxy and per-prompt reward variance; verify Pearson correlation >0.3 at same iteration, ~0 across iterations.
  3. **Ablation on normalization timing**: Compare standard GRPO vs. no-std GRPO on stratified easy/hard subsets; expect 3–7% accuracy gap in Phase II (iterations 100–300), convergence in Phase III.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the curvature-adaptive interpretation of GRPO normalization hold for deep neural network policies beyond the log-linear parametrization analyzed theoretically?
- Basis in paper: [explicit] The authors state "We focus on the log-linear policy parametrization, which has been widely studied in the analysis of policy gradient methods" and all convergence proofs (Theorems 3.1-3.4) rely on this assumption.
- Why unresolved: Modern LLMs use highly non-linear architectures where the curvature-variance relationship may not hold as Lemma 3.1 predicts. The theoretical framework doesn't extend to neural networks.
- What evidence would resolve it: Empirical validation that Fisher-curvature correlation remains significant with full neural network policies, or theoretical analysis extending Lemma 3.1 to non-linear function approximators.

### Open Question 2
- Question: How does GRPO's normalization-optimization trade-off interact with probability calibration, and can both be simultaneously improved?
- Basis in paper: [explicit] Related work notes "Bereket & Leskovec (2025) highlights a trade-off between normalization and calibration, showing that removing the std term can improve probability calibration at the cost of optimization speed."
- Why unresolved: The paper addresses optimization benefits but does not analyze calibration effects or propose mechanisms to balance both objectives.
- What evidence would resolve it: Systematic study measuring calibration metrics (e.g., ECE) alongside convergence rates for normalized vs. unnormalized GRPO across training phases.

### Open Question 3
- Question: Can the late-stage plateau in normalization benefits be mitigated through adaptive schemes that account for increasing cross-prompt interference?
- Basis in paper: [explicit] The authors identify Phase III as "a late-stage regime where reduced orthogonality limits further gains" and observe "the marginal benefit plateaus" with gradient similarity variance increasing from 0.066 to 0.130.
- Why unresolved: The paper characterizes the plateau but offers no mechanism to extend benefits into late training when interference grows.
- What evidence would resolve it: Experiments with dynamic normalization schemes that adjust to interference levels, showing sustained improvement through Phase III.

### Open Question 4
- Question: Does the curvature-adaptive mechanism transfer to RLHF settings with learned reward models, or is it specific to deterministic verifiable rewards?
- Basis in paper: [inferred] The paper explicitly states "In this paper, we focus on developing a theoretical understanding of GRPO in the RLVR setting" and validates only on GSM8K/MATH with binary correctness rewards.
- Why unresolved: Learned reward models introduce noise and bias that may corrupt the variance-curvature relationship, but this is unanalyzed.
- What evidence would resolve it: Empirical comparison of normalized GRPO on RLHF benchmarks (e.g., preference optimization tasks) with analysis of variance-curvature correlation under noisy rewards.

## Limitations

- **Phase I mechanism gap**: The theoretical framework assumes variance ≈ curvature, but Phase I shows no normalization advantage despite favorable geometric conditions, suggesting the relationship breaks down under high stochasticity.
- **Orthogonality assumption dependency**: While Theorems 3.2-3.4 provide convergence guarantees under orthogonal conditions, the paper relies heavily on empirical validation rather than proving orthogonality emergence or stability.
- **Empirical generalization limits**: Results demonstrated on GSM8K/MATH with Qwen2.5-Math-1.5B may not generalize to other architectures, reward structures, or task domains where prompt orthogonality doesn't naturally emerge.

## Confidence

**High Confidence**
- GRPO's normalization implements adaptive gradient scaling (Theorem 3.1 and empirical validation)
- Phase structure exists and follows predictable patterns across runs
- Normalization provides consistent gains during Phase II (iterations 100-300)

**Medium Confidence**
- Reward variance serves as a reliable local curvature proxy (Lemma 3.1, but Phase I counterexample)
- Orthogonality assumption holds sufficiently for practical convergence (empirical validation, but theoretical gap)
- Bounded cross-prompt interference preserves advantages (Theorem 3.3-3.4, but Phase III degradation observed)

**Low Confidence**
- Theoretical explanation fully accounts for all observed behaviors (Phase I remains unexplained)
- Relaxation to bounded interference is sufficient for all training regimes (Phase III shows limits)
- Three-phase pattern generalizes beyond current experimental setup

## Next Checks

1. **Phase I Mechanism Investigation**: Conduct controlled experiments varying reward noise levels while holding variance constant. Test whether adding explicit gradient smoothing (e.g., exponential moving averages) to REINFORCE closes the Phase I performance gap, isolating whether the issue is curvature estimation or gradient noise.

2. **Orthogonality Emergence Analysis**: Track gradient orthogonality throughout training for both GRPO and REINFORCE. Compare prompt representation evolution to determine if GRPO actively maintains orthogonality or if it emerges from task structure. Test on non-orthogonal prompt sets to identify when the theoretical framework breaks.

3. **Curvature-Variance Correlation Stability**: Systematically measure the correlation between reward variance and local curvature (via diagonal Fisher approximation) across all three phases. Identify the threshold where this relationship breaks down and test whether alternative normalization schemes (e.g., using running variance estimates) maintain benefits in Phase I.