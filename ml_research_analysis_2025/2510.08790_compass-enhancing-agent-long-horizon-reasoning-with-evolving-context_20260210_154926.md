---
ver: rpa2
title: 'COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context'
arxiv_id: '2510.08790'
source_url: https://arxiv.org/abs/2510.08790
tags:
- context
- tool
- reasoning
- agent
- strategic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'COMPASS addresses long-horizon reasoning challenges in LLM agents
  by separating tactical execution, strategic oversight, and context management into
  three specialized components: a Main Agent for tool use, a Meta-Thinker for anomaly
  detection and strategic interventions, and a Context Manager for maintaining concise,
  relevant progress briefs. This hierarchical architecture improves accuracy by up
  to 20% on GAIA, BrowseComp, and Humanity''s Last Exam benchmarks compared to single-
  and multi-agent baselines.'
---

# COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context

## Quick Facts
- arXiv ID: 2510.08790
- Source URL: https://arxiv.org/abs/2510.08790
- Authors: Guangya Wan, Mingyang Ling, Xiaoqi Ren, Rujun Han, Sheng Li, Zizhao Zhang
- Reference count: 40
- Primary result: Hierarchical agent architecture improves accuracy by up to 20% on GAIA, BrowseComp, and Humanity's Last Exam benchmarks

## Executive Summary
COMPASS addresses long-horizon reasoning challenges in LLM agents by introducing a three-component architecture: a Main Agent for tactical tool execution, a Meta-Thinker for strategic oversight and anomaly detection, and a Context Manager for maintaining concise progress briefs. This hierarchical decoupling improves accuracy by up to 20% compared to single- and multi-agent baselines while reducing token usage by 30% through specialized preference-optimized models. The system demonstrates effectiveness on web-based and general reasoning benchmarks while highlighting limitations in open-ended domains and smaller model backbones.

## Method Summary
COMPASS implements a hierarchical agent architecture where a Main Agent executes ReAct loops for tactical tool use, a Meta-Thinker asynchronously monitors trajectories for strategic failures and issues high-level decisions (Persist, Pivot, Stop), and a Context Manager synthesizes compressed "Notes" briefs to prevent context truncation. The system introduces automated meta-thinking signals and test-time scaling extensions that achieve performance matching established deep-research agents. A specialized 12B Context-12B model trained via SFT and DPO reduces token usage by 30% while maintaining accuracy, with parallel sampling variants further improving reliability under uncertainty.

## Key Results
- 20% accuracy improvement on GAIA, BrowseComp, and Humanity's Last Exam benchmarks versus single- and multi-agent baselines
- 30% token usage reduction achieved through specialized Context-12B model trained via SFT and DPO
- Parallel sampling variants improve reliability under uncertainty while maintaining accuracy
- Test-time scaling extensions achieve performance matching established deep-research agents

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decoupling of Execution and Oversight
Isolating tactical execution from strategic monitoring mitigates error cascades in long-horizon tasks, conditional on the Meta-Thinker's ability to detect anomalies like loops or dead-ends. A Main Agent handles immediate tool use while an asynchronous Meta-Thinker reviews trajectories for strategic failures and issues high-level interventions (Persist, Pivot, or Stop), preventing fixation on failing strategies. Core assumption: anomalies are detectable via textual analysis without full re-execution. Evidence: 20% accuracy improvement claim, specific scenarios where strategic decisions determine success, validation from UltraHorizon paper identifying long-horizon planning limitations.

### Mechanism 2: Structured Context Compression via "Notes"
Maintaining compressed, structured progress briefs prevents "lost in the middle" phenomenon and context truncation, provided extraction logic preserves critical constraints. Instead of appending full logs, a Context Manager synthesizes trajectories into structured briefs (Evidence, Constraints, Open Items) fed back to the Main Agent, keeping input concise while preserving historical dependencies. Core assumption: critical information can be distilled into bullet points without losing nuance for complex tool interactions. Evidence: explicit structured output format definition, support from Context as a Tool and Git Context Controller papers arguing against append-only maintenance.

### Mechanism 3: Specialized Preference-Optimized Summarization
Delegating context management to a smaller 12B model trained with DPO reduces token usage by 30% without accuracy loss, assuming preference dataset captures "strategic utility" accurately. The system distills Context Manager's role into Context-12B using Direct Preference Optimization trained to prefer summaries leading to successful task completion with fewer tokens. Core assumption: the "Context Manager" role is deterministic enough to distill into smaller model without requiring frontier LLM reasoning capacity. Evidence: 30% token reduction claim, DPO objective definition maximizing success and efficiency scores, though corpus evidence on specific DPO mechanism is limited.

## Foundational Learning

**ReAct (Reasoning + Acting) Paradigm**
Why needed: COMPASS's Main Agent operates on ReAct loop. Understanding Thought → Action → Observation cycle is essential to diagnose reasoning vs. execution failures.
Quick check: Can you distinguish between a "Thought" trace and a "Tool Call" in a standard agent log?

**Strategic vs. Tactical Reasoning**
Why needed: Core innovation is separation of these modes. "Tactical" = "How do I call this API?" while "Strategic" = "Should I still be using this API?"
Quick check: Given a failed tool call, is the fix to change API parameters (Tactical) or switch to different data source (Strategic)?

**Context Window Dynamics (Lost in the Middle)**
Why needed: Problem COMPASS solves is degradation of attention over long contexts. Understanding models overlook middle information clarifies why "Context Refresh" is necessary.
Quick check: If constraint is placed at start of 10k-token prompt, how likely is standard LLM to adhere to it vs. if it is in 500-token summary?

## Architecture Onboarding

**Component map:**
Main Agent -> Meta-Thinker -> Context Manager -> Main Agent (loop)

**Critical path:**
1. Query → Meta-Thinker (Initial Plan)
2. Context Manager (Synthesize Context x₀)
3. Main Agent (Execute ReAct Loop)
4. Meta-Thinker (Anomaly Detection) → Decision
5. Context Manager (Update Notes & Refresh Context x₁)
6. Repeat until Decision = STOP

**Design tradeoffs:**
- Latency vs. Robustness: Faster Meta-Thinker checks reduce latency but may miss complex anomalies
- Compression vs. Fidelity: Higher Context Manager compression saves tokens but increases risk of dropping critical details

**Failure signatures:**
- Blind Persistence: High PAR, Low PVR - agent retries same failing tool repeatedly → Fix: Tune Meta-Thinker sensitivity
- Premature Termination: High CA, Low ERC - agent stops with wrong answer → Fix: Strengthen "Verify" signals in Meta-Thinker
- Token Bloat: Total tokens explode → Fix: Increase Context Manager compression or switch to Context-12B model

**First 3 experiments:**
1. Baseline Comparison: Run single-agent ReAct loop on BrowseComp/GAIA tasks to establish "Context Truncation" and "Hallucination" error rates without COMPASS
2. Component Ablation: Run COMPASS with only Meta-Thinker (no Context Manager) and only Context Manager (no Meta-Thinker) to quantify individual contributions to 20% accuracy gain
3. Stress Testing Context-12B: Deploy Context-12B model in loop and verify 30% token reduction claim while monitoring for "Constraint Drift" in final answers

## Open Questions the Paper Calls Out

How does COMPASS performance transfer to open-ended domains utilizing richer interoperability protocols like MCP servers or agent-to-agent (A2A) communication? Current evaluation is restricted to controlled QA-style benchmarks; empirical evaluation on dynamic, real-world tasks using these protocols is needed.

Can the COMPASS architecture and its specialized post-training pipeline salvage long-horizon reasoning capabilities in smaller, open-source models? Study exclusively relies on proprietary frontier models; benchmarks applying Context-12B SFT/DPO pipeline to open-source backbones (Llama, Mistral) would resolve this.

What is the optimal trade-off between context compression and retention of strategic signals required to prevent failure modes like "excessive plan revision"? Paper identifies need to balance compression with signaling but does not define optimal equilibrium for varying model capacities; parameter sweep of compression rates correlated with Strategy Adequacy metrics would help.

## Limitations

Task Transferability: Architecture's effectiveness for code generation, multi-modal reasoning, or domain-specific expert tasks remains untested; assumption that hierarchical oversight generalizes to all long-horizon scenarios may not hold for deep sequential reasoning tasks.

Meta-Thinker Reliability: Asynchronous Meta-Thinker depends on textual anomaly detection without re-execution; ability to detect subtle logical failures not validated, may fail when correct strategic pivot is not obvious from trajectory text alone.

Context-12B Generalization: 30% token reduction claim assumes preference-optimized model generalizes beyond training distribution; may drop critical information when deployed on tasks with novel context structures requiring complex constraint reasoning.

## Confidence

High Confidence: Hierarchical decoupling of tactical execution and strategic oversight is well-supported by error analysis in existing literature and COMPASS error categorization shows clear failure modes where separation would help.

Medium Confidence: Structured context compression via "Notes" is theoretically sound and addresses "lost in the middle" problem, but specific claim that bullet-point extraction preserves all critical constraints needs empirical validation.

Medium Confidence: Context-12B model's efficiency gains are demonstrated, but claim of "no accuracy loss" is qualified by training data distribution and may not generalize to tasks requiring complex constraint reasoning.

## Next Checks

1. **Multi-Agent Coordination Stress Test**: Deploy COMPASS in 3+ agent collaboration scenario with different tool access and measure whether Meta-Thinker detects coordination failures and whether Context Manager maintains coherent cross-agent state without information loss.

2. **Extreme Token Reduction Experiment**: Force Context-12B model to operate at 50% of normal token budget and systematically evaluate where constraint information is lost, comparing against human-written summaries to identify specific failure patterns.

3. **Temporal Reasoning Evaluation**: Test COMPASS on tasks requiring explicit temporal reasoning (e.g., "find events that happened before X but after Y") to determine if structured "Notes" format preserves temporal relationships or if compression loses sequence information critical for such reasoning.