---
ver: rpa2
title: Evaluating the Meta- and Object-Level Reasoning of Large Language Models for
  Question Answering
arxiv_id: '2502.10338'
source_url: https://arxiv.org/abs/2502.10338
tags:
- reasoning
- which
- llms
- object-level
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the meta- and object-level reasoning capabilities
  of large language models (LLMs) in question answering tasks. The authors reframe
  existing reasoning tasks in terms of high-level strategic reasoning (meta-level)
  and lower-level mathematical/natural language reasoning (object-level).
---

# Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering

## Quick Facts
- arXiv ID: 2502.10338
- Source URL: https://arxiv.org/abs/2502.10338
- Reference count: 8
- Primary result: LLMs consistently demonstrate strong meta-level reasoning (plan generation) but struggle with object-level execution (arithmetic, data retrieval), even when prompted to create step-by-step plans.

## Executive Summary
This paper evaluates large language models' capabilities in meta-level strategic reasoning versus object-level mathematical and language reasoning through human-annotated studies. The authors introduce FRANKLIN, a novel dataset requiring both reasoning types, and conduct two annotation studies across four datasets (FRANKLIN, GSM8k, HotpotQA, StrategyQA) using four LLMs. Results show LLMs excel at generating rational approaches and clear step-by-step plans (meta-level) but frequently fail to provide answers for object-level tasks requiring precise arithmetic and data retrieval. Despite successful planning, models exhibit high answer failure rates on FRANKLIN due to errors like data fabrication, inaccurate data, and incorrect arithmetic. The findings suggest LLMs can effectively emulate high-level planning and problem decomposition but struggle with low-level execution of reasoning steps.

## Method Summary
The study employs two human annotation studies using four datasets (FRANKLIN with 400 examples, plus GSM8k, HotpotQA, StrategyQA with 64 samples each) and four LLMs (Llama 3.1 8B, Phi 3.5 Mini, Gemma 2 9B, GPT-4o-mini). Study 1 uses single-prompt answer generation, while Study 2 employs a two-stage approach requiring plan generation before execution. Human annotators rate responses on 5-point Likert scales for presence of answers, plans, rational approaches, and step execution, with ratings mapped to 3-point scales and majority voting across four annotators per example. Metrics include Answer Failure Rate (AFR), Rational Approach Rate (RAR), and Plan Creation Rate (PCR). The study design aims to isolate meta-level reasoning (strategic planning) from object-level reasoning (arithmetic, data retrieval, execution).

## Key Results
- LLMs consistently demonstrate strong meta-level reasoning abilities, with RAR and PCR frequently exceeding 95% across model/dataset combinations
- LLMs frequently fail to provide answers for object-level reasoning tasks, with high AFR even when successfully planning solutions
- FRANKLIN dataset presents particular challenges for object-level reasoning, with models making errors including data fabrication, inaccurate data, and incorrect arithmetic despite successful planning

## Why This Works (Mechanism)

### Mechanism 1: Plan Generation via Language Pattern Matching
- Claim: LLMs generate structured plans through statistical pattern completion rather than formal reasoning
- Core assumption: Planning patterns are sufficiently represented in pre-training data
- Evidence: [abstract] "LLMs consistently demonstrate strong meta-level reasoning abilities"; [section 5.2] RAR/PCR frequently over 95%
- Break condition: Tasks requiring novel planning schemas not in training data

### Mechanism 2: Object-Level Execution Failure via Precision Demands
- Claim: LLMs fail at object-level tasks due to precision and grounding requirements
- Core assumption: Failures stem from inability to reliably ground tokens in external truth
- Evidence: [abstract] "LLMs frequently fail to provide answers (high AFR) for object-level reasoning tasks"; [section 5.3] errors include data fabrication and incorrect arithmetic
- Break condition: Tasks where approximate answers are acceptable or external tools are provided

### Mechanism 3: Plan-First Prompting Reduces Execution Avoidance
- Claim: Instructing models to create plans reduces AFR by making subtasks more salient
- Core assumption: Reduction reflects genuine task decomposition benefits
- Evidence: [section 5.1] "AFR is consistently lower in the setting of study 2"; [section 4.1] Study 2 designed to observe meta-level reasoning and plan influence
- Break condition: Safety-aligned models that refuse to execute generated plans

## Foundational Learning

- **Meta-level vs. Object-level Reasoning Distinction**
  - Why needed: Framework depends on correctly classifying reasoning steps as strategic planning (meta) or execution (object)
  - Quick check: Given "Compare the population values to find the maximum," is this meta-level or object-level? (Answer: Object-level—it performs the comparison operation)

- **Chain-of-Thought Prompting**
  - Why needed: Study builds on CoT techniques, extending from "let's think step-by-step" to explicit plan-then-execute protocols
  - Quick check: What is the core claim of CoT prompting? (Answer: Intermediate reasoning steps in natural language improve performance on multi-step tasks)

- **LLM Hallucination and Precision Limitations**
  - Why needed: Object-level failures are instances of hallucination; understanding precision limitations explains persistent AFR
  - Quick check: Why might an LLM report "The population of Togo in 2020 was 8.43 million" when true value is 8,442,580? (Answer: Model generates plausible approximations rather than retrieving exact values)

## Architecture Onboarding

- **Component map:** Datasets (FRANKLIN, GSM8k, HotpotQA, StrategyQA) -> Model responses (Llama 3.1 8B, Phi 3.5 Mini, Gemma 2 9B, GPT-4o-mini) -> Human annotations (4 per example via Prolific) -> 5-point Likert ratings -> 3-point mapping -> Majority vote -> AFR/RAR/PCR metrics

- **Critical path:** 1. Select question from dataset -> 2. Generate response (with/without plan-first prompt) -> 3. Present to 4 annotators -> 4. Aggregate annotations -> 5. Compute AFR/RAR/PCR -> 6. Compare across models and datasets

- **Design tradeoffs:**
  - Dataset scope focuses on geopolitical indicators; generalization requires validation
  - Only smaller models tested (8-9B parameters, plus GPT-4o-mini); findings may not transfer to frontier models
  - Evaluates presence of reasoning behaviors, not correctness of final answers; low AFR ≠ high accuracy
  - Study 2 uses conversation-based prompting unavailable in Gemma 2, introducing prompt heterogeneity

- **Failure signatures:**
  - High AFR + High RAR/PCR: Model can plan but cannot or will not execute
  - GPT-4o-mini pattern: AFR increases in Study 2 (100% on FRANKLIN)—plan generated but execution declined due to safety guardrails
  - FRANKLIN-specific high AFR: Object-level requirements (precise data retrieval + arithmetic) are the bottleneck

- **First 3 experiments:**
  1. Replicate AFR analysis on larger models (e.g., Llama 70B or GPT-4) to test whether object-level failures persist at scale
  2. Tool-augmented execution: Provide models with calculator and web search tools during object-level execution to measure AFR reduction
  3. Cross-domain FRANKLIN extension: Create analogous templates for financial indicators, scientific metrics, or sports statistics to test domain generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning or increased model scale specifically improve object-level reasoning performance on datasets like FRANKLIN?
- Basis: [explicit] Conclusion states "evaluation of FRANKLIN on LLMs both off-the-shelf and fine-tuned, as well as at larger parameter counts"
- Why unresolved: Study only tested off-the-shelf models at 8B-9B parameters with consistent object-level failures
- What evidence would resolve: Evaluation of fine-tuned variants and larger parameter models (e.g., 70B+) on FRANKLIN measuring AFR reduction

### Open Question 2
- Question: How can systems be designed to bridge the gap between strong meta-level planning and weak object-level execution?
- Basis: [explicit] Central finding is asymmetry between meta-level planning and object-level execution with no proposed solutions
- Why unresolved: Characterizes asymmetry but does not investigate interventions to transfer meta-level quality to successful object-level execution
- What evidence would resolve: Experiments with hybrid architectures showing improved answer success rates while preserving planning quality

### Open Question 3
- Question: How does the distinction between reasoning and imitation manifest in meta- versus object-level tasks?
- Basis: [explicit] "There is debate over whether LLMs are actually reasoning rather than emulating or imitating it"
- Why unresolved: Evaluates surface-level outputs but does not employ methods to probe internal representations or test distributional robustness
- What evidence would resolve: Probing experiments comparing performance stability between meta- and object-level reasoning tasks

## Limitations
- Domain generalization: FRANKLIN focuses on geopolitical indicators; results may not extend to financial, scientific, or medical domains requiring precise numerical reasoning
- Model scale: Only 8-9B parameter models and GPT-4o-mini tested; findings may not transfer to frontier models with enhanced reasoning capabilities
- Annotation subjectivity: Human evaluation relies on Likert scales and majority voting, which may mask nuanced disagreements about reasoning behavior classification

## Confidence
- **High Confidence**: LLMs consistently demonstrate strong meta-level reasoning (plan generation) across datasets and models
- **Medium Confidence**: Object-level reasoning failures are primarily due to precision demands (data fabrication, inaccurate values, arithmetic errors) rather than fundamental inability to execute plans
- **Medium Confidence**: Plan-first prompting reduces AFR by making object-level subtasks more salient

## Next Checks
1. Replicate meta/object reasoning evaluation on larger frontier models (e.g., Llama 70B, GPT-4) to test whether object-level failures persist at scale
2. Implement tool-augmented execution by providing models with external calculator and web search tools during object-level execution to measure AFR reduction and error type shifts
3. Create cross-domain FRANKLIN extension with financial indicators, scientific metrics, or sports statistics to test whether meta/object reasoning gap generalizes beyond geopolitical domains