---
ver: rpa2
title: Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness
  in Multi-Party Social Interactions
arxiv_id: '2510.27195'
source_url: https://arxiv.org/abs/2510.27195
tags:
- social
- game
- multimodal
- dataset
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new multimodal benchmark, MIVA, for detecting
  deception in multi-party social interactions. The authors construct a dataset based
  on the social deduction game Werewolf, providing synchronized video, text, and verifiable
  ground-truth labels for every utterance.
---

# Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions

## Quick Facts
- arXiv ID: 2510.27195
- Source URL: https://arxiv.org/abs/2510.27195
- Reference count: 38
- Key outcome: State-of-the-art Multimodal Large Language Models struggle to reliably detect deception in multi-party social interactions, with even powerful models like GPT-4o achieving only 39.4% Binary Accuracy.

## Executive Summary
This paper introduces MIVA, a multimodal benchmark for detecting deception in multi-party social interactions based on the game Werewolf. The authors construct a dataset with synchronized video, text, and verifiable ground-truth labels for every utterance, then evaluate state-of-the-art MLLMs on veracity assessment. Results reveal significant performance gaps, with models failing due to overly conservative alignment, lack of Theory of Mind, and difficulty grounding language in visual social cues. The work highlights urgent needs for novel approaches to build more perceptive and trustworthy AI systems for social reasoning.

## Method Summary
The authors created MIVA using two corpora: Ego4D-MIVA (819 utterances from 40 game sessions) and YouTube-MIVA (543 utterances from 151 videos). They developed a semi-automated annotation pipeline with LLM assistance to generate veracity labels, validated by human experts. The evaluation tested six MLLM models across multiple conditions: Text-only, Vision (1-frame), Face-CoT, Body-CoT, and 3-frame. Primary metrics included Macro-F1, Binary Accuracy (TRUE/FALSE only), and Joint Accuracy for strategy classification.

## Key Results
- MLLMs achieved only 39.4% Binary Accuracy on truthfulness detection, with even the best model (GPT-4o) struggling significantly.
- Visual inputs improved holistic comprehension (Macro-F1) but degraded critical TRUE/FALSE discrimination compared to text-only approaches.
- Removing conversation history caused catastrophic performance drops (Binary Accuracy from 39.4% to 13.4%), confirming context-dependent reasoning as essential.
- Models exhibited overly conservative alignment, defaulting to NEUTRAL class and achieving low Macro-Recall scores despite high overall accuracy.

## Why This Works (Mechanism)

### Mechanism 1: Visual-Social Grounding Failure
- Claim: Multimodal models can describe visual cues but fail to interpret their social meaning for veracity judgments.
- Mechanism: Chain-of-Thought prompts elicit accurate visual descriptions (eye contact, gestures, posture), but this structured analysis does not propagate into correct social interpretation. Models treat visual analysis and veracity reasoning as disconnected tasks.
- Evidence anchors: [section 4.3] "While the Face-CoT and Body-CoT prompts generated accurate, detailed descriptions of visual cues, this structured analysis failed to correctly interpret their social meaning." [section 4.3] Text-only achieved highest Binary Accuracy (39.4% Ego4D); all visual conditions degraded this metric.

### Mechanism 2: Conservative Alignment Bias
- Claim: Safety-aligned models avoid high-stakes social judgments by defaulting to the NEUTRAL class.
- Mechanism: Models achieve high overall accuracy by correctly classifying the majority NEUTRAL class, but their Macro-Recall remains low because they systematically avoid committing to TRUE/FALSE labels when stakes are high.
- Evidence anchors: [section 4.2] "An overly conservative alignment that causes them to evade high-stakes decisions by defaulting to the 'safe' NEUTRAL class, a behavior confirmed by their low Macro-Recall scores." [section 4.2] GPT-4o achieved best Macro-F1 (51.2) but GPT-4o-mini achieved best Binary Accuracy (39.4%)—smaller model was less conservative.

### Mechanism 3: Context-Dependent Veracity Reasoning
- Claim: Veracity assessment requires global conversation history; strategy classification is more local.
- Mechanism: Removing textual history has minimal impact on strategy classification (+2.4 F1) but catastrophically degrades veracity assessment (Binary Accuracy drops from 39.4% to 13.4%). Truth detection requires tracking speaker knowledge states across time.
- Evidence anchors: [section 4.4] "Removing the history causes a catastrophic drop in veracity assessment performance (Accuracy (Binary) plummets from 39.4% to 13.4%)." [section 4.4] "Assessing truthfulness is a global task that fundamentally relies on contextual reasoning over the entire conversation."

## Foundational Learning

- **Concept: Theory of Mind (ToM) in AI**
  - Why needed here: The paper explicitly identifies ToM deficit as a core failure mode—models cannot infer what speakers know, believe, or intend at each moment.
  - Quick check question: Given a speaker's partial information and a false statement, can you determine whether the speaker is lying (contradicts their knowledge) or guessing (contradicts reality but not their belief)?

- **Concept: Class Imbalanced Evaluation**
  - Why needed here: 90%+ of utterances in some categories are NEUTRAL; accuracy is misleading. The paper uses Macro-F1 and Binary Accuracy specifically.
  - Quick check question: If a model achieves 90% accuracy on a dataset where 85% of samples are class A, is it performing well?

- **Concept: Chain-of-Thought (CoT) for Multimodal Reasoning**
  - Why needed here: The paper introduces Face-CoT and Body-CoT prompts to force explicit visual analysis before judgment.
  - Quick check question: Why might generating intermediate reasoning steps not improve final accuracy if the reasoning steps are correct but not causally connected to the decision?

## Architecture Onboarding

- **Component map:** Video frames + transcript + conversation history + game rules (JSON) -> Visual encoding (frame-level features) -> LLM core (GPT-4o, Gemini-2.5-pro, etc.) -> CoT reasoning module (prompted visual analysis) -> Hierarchical labels (6 strategy categories + 1 veracity label)

- **Critical path:** 1) Night actions manually annotated 2) LLM-assisted pipeline generates veracity labels using game state 3) Human validation on 5% subset (87.8% agreement) 4) MLLM receives: transcript + video + rules + history 5) Model outputs JSON with strategy labels + veracity + reasoning

- **Design tradeoffs:** Text-only vs. multimodal (choose based on task priority); Single-frame vs. multi-frame (3-frame degraded performance); Large vs. small models (smaller models less conservative, better at binary judgments)

- **Failure signatures:** High overall accuracy but low Binary Accuracy (conservative NEUTRAL bias); Accurate visual descriptions but wrong veracity judgments (visual-social grounding failure); Good strategy classification but poor veracity (ToM deficit)

- **First 3 experiments:** 1) Baseline reproduction: Run GPT-4o-mini on MIVA with text-only vs. Face-CoT vs. Body-CoT; verify Binary Accuracy degrades with visual input 2) History ablation: Remove conversation history and measure Binary Accuracy drop (should reproduce 39.4% → 13.4% collapse) 3) Per-category error analysis: Focus on Identity Declaration and Evidence categories; confirm models perform near-chance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can context-adaptive alignment strategies be developed that allow MLLMs to make decisive high-stakes veracity judgments in adversarial settings without defaulting to safe, neutral answers?
- Basis in paper: Authors state models exhibit "overly conservative alignment that causes them to evade high-stakes judgments by defaulting to safe, neutral answers" and call for developing "context-adaptive alignment strategies that are less risk-averse in adversarial settings."

### Open Question 2
- Question: What architectural modifications would enable MLLMs to maintain persistent mental models of multiple speakers' knowledge states, beliefs, and strategic intentions during multi-party interactions?
- Basis in paper: Authors identify "a fundamental lack of a 'Theory of Mind' to infer the strategic intentions behind statements" and call for "new architectures with integrated Theory of Mind reasoning."

### Open Question 3
- Question: Why does incorporating visual information improve holistic comprehension (Macro-F1) while simultaneously degrading performance on critical TRUE/FALSE discrimination?
- Basis in paper: Table 4 shows visual inputs improve Macro-F1 but consistently yield lower Binary Accuracy than text-only. The authors call this visual information a "double-edged sword" but do not resolve the mechanism.

## Limitations
- Limited ecological validity: Werewolf game represents highly structured social scenario that may not generalize to open-ended real-world interactions
- Data scale constraints: Only 1,362 total utterances across two corpora, limiting statistical power for analyzing rare utterance types
- Annotation pipeline opacity: Critical prompt templates are referenced but not provided, creating reproducibility barriers

## Confidence
- **High Confidence:** Visual-Social Grounding Failure (supported by comparative metrics), Conservative Alignment Bias (confirmed through low Macro-Recall scores), Context-Dependent Reasoning (well-supported by performance drop)
- **Medium Confidence:** ToM deficit as primary failure mode (identified but needs controlled experiments), Smaller models being less conservative (observed pattern needs systematic analysis)

## Next Checks
1. Implement a sliding-window approach where models analyze 3-5 consecutive frames to determine if temporal context improves visual-social grounding
2. Evaluate the same MLLMs on deception detection in courtroom testimony or negotiation transcripts to assess cross-domain generalization
3. Design synthetic scenarios where visual cues and textual information are perfectly aligned or deliberately mismatched, measuring whether models can correctly attribute beliefs to speakers based on their private knowledge states