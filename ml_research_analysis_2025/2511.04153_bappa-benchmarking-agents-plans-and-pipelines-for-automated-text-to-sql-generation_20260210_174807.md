---
ver: rpa2
title: 'BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL
  Generation'
arxiv_id: '2511.04153'
source_url: https://arxiv.org/abs/2511.04153
tags:
- instruct
- agent
- coder
- arxiv
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks three multi-agent pipelines for text-to-SQL
  generation using small and mid-sized open-source LLMs. The pipelines include Multi-Agent
  Discussion (agents iteratively critique and refine queries), Planner-Coder (planning
  model generates step-by-step reasoning, coding model generates SQL), and Coder-Aggregator
  (multiple coders generate candidates, reasoning model selects best).
---

# BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation

## Quick Facts
- arXiv ID: 2511.04153
- Source URL: https://arxiv.org/abs/2511.04153
- Reference count: 37
- Primary result: Three multi-agent pipelines (Multi-Agent Discussion, Planner-Coder, Coder-Aggregator) improve text-to-SQL accuracy for small and mid-sized open-source LLMs, with Planner-Coder achieving best results

## Executive Summary
This paper benchmarks three multi-agent pipelines for text-to-SQL generation using small and mid-sized open-source LLMs. The pipelines include Multi-Agent Discussion (agents iteratively critique and refine queries), Planner-Coder (planning model generates step-by-step reasoning, coding model generates SQL), and Coder-Aggregator (multiple coders generate candidates, reasoning model selects best). Experiments on Bird-Bench Mini-Dev and Spider Dev with 24 models (4B–34B) show that reasoning-focused models improve accuracy when used as planners or aggregators. Multi-Agent Discussion improves small model performance, with up to 10.6% increase in execution accuracy for Qwen2.5-7B-Instruct after three rounds. The Planner-Coder pipeline yields the best results, with DeepSeek-R1-32B and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to 56.4%. Gemma 3 27B IT achieves highest zero-shot execution accuracy at 52.4% on Bird-Bench and 78.9% on Spider.

## Method Summary
The study evaluates three multi-agent pipelines for text-to-SQL generation using 24 open-source models ranging from 4B to 34B parameters. Multi-Agent Discussion employs three persona-based agents (Simple, Technical, Thinker) that iteratively critique and refine SQL queries over 1-3 rounds, with a judge synthesizing the final output. Planner-Coder separates reasoning from implementation by having a thinking model generate step-by-step plans that guide a coding model. Coder-Aggregator generates multiple SQL candidates from different coders and uses a reasoning model to select or synthesize the best output. All experiments use zero-shot inference with vllm v0.8.5.post1, evaluating on Bird-Bench Mini-Dev (500 examples) and Spider Dev (1,034 examples) datasets using Execution Accuracy, Soft F1-Score, and R-VES metrics.

## Key Results
- Multi-Agent Discussion improves small model performance, with up to 10.6% increase in Execution Accuracy for Qwen2.5-7B-Instruct after three rounds
- Planner-Coder pipeline achieves best overall results, with DeepSeek-R1-32B and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to 56.4%
- Gemma 3 27B IT achieves highest zero-shot execution accuracy at 52.4% on Bird-Bench and 78.9% on Spider
- CodeLlama and StarCoder models benefit most from Planner-Coder, showing BIRD EX gains of up to +23 points
- Multi-Agent Discussion performance plateaus by round 2, with diminishing returns for high-performing baseline models

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Discussion
- Claim: Iterative critique through multi-agent discussion improves SQL generation for smaller models by exposing reasoning flaws that single-shot generation misses
- Mechanism: Three agents with distinct personas generate initial SQL queries, observe each other's outputs across rounds, and revise based on peer feedback. A judge synthesizes final outputs after each round, with performance typically plateauing by round 2
- Core assumption: Diverse perspectives and cross-critique reduce hallucinations and schema misalignment more effectively than single-agent generation
- Evidence anchors:
  - [abstract] "Multi-Agent discussion can improve small model performance, with up to 10.6% increase in Execution Accuracy for Qwen2.5-7b-Instruct seen after three rounds of discussion"
  - [section 5.2] "Qwen2.5-Coder-14B-Instruct improves substantially from 31.4 → 41.0 EX on BIRD"
  - [corpus] Neighbor papers lack direct evidence on multi-agent discussion; corpus relevance weak for this mechanism
- Break condition: Gains diminish after round 2; high-performing baseline models show minimal improvement, suggesting the mechanism saturates when single-shot reasoning is already strong

### Mechanism 2: Planner-Coder Pipeline
- Claim: Explicit planning by reasoning-focused models amplifies SQL accuracy by decomposing schema understanding from syntax generation
- Mechanism: A planner agent (thinking model) generates step-by-step plans outlining tables, columns, and operations. A coder agent then implements SQL conditioned on both schema and plan, isolating reasoning from execution
- Core assumption: Reasoning models' extended test-time computation captures implicit schema relationships that non-reasoning models miss
- Evidence anchors:
  - [abstract] "DeepSeek-R1-32B and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest score of 56.4%"
  - [section 5.3] "CodeLlama and StarCoder models, which perform poorly in zero-shot settings, benefit the most showing BIRD EX gains of up to +23 points"
  - [corpus] HI-SQL (arXiv:2506.18916) shows dynamic hints improve complex queries, but no direct multi-agent planning evidence
- Break condition: Large coders with strong inherent reasoning (e.g., Gemma 3 27B IT) show diminishing returns, as their baseline already captures schema relationships

### Mechanism 3: Coder-Aggregator Pipeline
- Claim: Aggregating diverse SQL candidates through a reasoning-based selector improves reliability by mitigating individual coder errors
- Mechanism: Multiple coders independently generate SQL candidates with reasoning traces. An aggregator evaluates structural correctness, schema alignment, and execution plausibility to synthesize or select the best query
- Core assumption: Diverse generation paths reduce correlated errors; reasoning models can identify correct SQL even when they cannot generate it from scratch
- Evidence anchors:
  - [abstract] "The Planner-Coder pipeline yields the best results" (strongest pipeline overall)
  - [section 5.4] "QwQ-32B delivers the highest scores using LARGE Coders on BIRD with 54.4 EX / 60.2 Soft F1 / 53.1 R-VES"
  - [corpus] DCMM-SQL (arXiv:2510.23284) explores multi-model collaboration but focuses on training, not inference-time aggregation
- Break condition: Gains are modest for already-strong coders; aggregator quality depends on candidate diversity—homogeneous candidates reduce aggregation value

## Foundational Learning

- **Text-to-SQL task definition**
  - Why needed here: All pipelines aim to convert natural language questions to executable SQL; without understanding the baseline challenge (schema linking, complex joins), pipeline benefits are opaque
  - Quick check question: Can you explain why schema size and complex reasoning cause LLMs to struggle in text-to-SQL tasks?

- **Chain-of-thought (CoT) prompting**
  - Why needed here: Baseline experiments use zero-shot CoT ("Let's think step by step"); understanding CoT distinguishes it from thinking models' implicit reasoning
  - Quick check question: Why would adding "Let's think step by step" harm thinking models like DeepSeek-R1?

- **Execution Accuracy vs. Soft F1 metrics**
  - Why needed here: Results report both exact execution match and partial-credit F1; understanding metric trade-offs is essential for interpreting pipeline improvements
  - Quick check question: When would Soft F1 show improvement while Execution Accuracy remains flat?

## Architecture Onboarding

- **Component map:**
  - Multi-Agent Discussion: Starter Agent → Discussion Agents (3 rounds) → Judge Agent → Final SQL
  - Planner-Coder: Planner Agent (thinking model) → Coder Agent → Final SQL
  - Coder-Aggregator: Multiple Coder Agents → Aggregator Agent → Final SQL
  - All pipelines share: schema + question input → SQL output

- **Critical path:**
  1. Run zero-shot baseline (Figure 2 prompt) to establish model capability
  2. For Planner-Coder: generate plan first, then condition coder on plan
  3. For Multi-Agent Discussion: initialize with Starter Agent, iterate with Discussion Agents, finalize with Judge
  4. For Coder-Aggregator: generate N diverse candidates, aggregate with reasoning model

- **Design tradeoffs:**
  - Multi-Agent Discussion: Higher compute (3+ agents × 3 rounds) vs. modest gains for strong models
  - Planner-Coder: Requires reasoning model as planner; best ROI for weak/mid coders
  - Coder-Aggregator: Benefits from diverse coders; aggregator quality is bottleneck
  - All pipelines: Open-source focus (4B-34B) trades peak performance vs. proprietary models for privacy/cost control

- **Failure signatures:**
  - CodeLlama-13B generates malformed SQL tokens (zero-shot baseline fails)
  - Multi-Agent Discussion regresses for some models (Qwen2.5-32B drops from 43.8 → 23.6 EX)
  - Planner-Coder: If planner hallucinates schema elements, coder propagates errors
  - Coder-Aggregator: Homogeneous candidates (same model family) reduce aggregation value

- **First 3 experiments:**
  1. Replicate zero-shot baseline for Gemma 3 27B IT and Qwen2.5-7B-Instruct on BIRD Mini-Dev subset (100 examples) to validate setup
  2. Run Planner-Coder pipeline pairing QwQ-32B planner with Gemma 3 12B IT coder; compare to zero-shot baseline
  3. Run Multi-Agent Discussion (3 rounds) with Qwen2.5-7B-Instruct; track round-by-round EX to verify plateau by round 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the latency and computational cost trade-off for the proposed multi-agent pipelines compared to single-pass inference?
- Basis in paper: [inferred] The paper highlights accuracy gains but utilizes resource-intensive methods, such as 3-round iterative discussions and 8192-token planning contexts for reasoning models
- Why unresolved: While accuracy improves, the viability of these pipelines for real-time applications depends on whether the performance gains justify the increased inference time and token costs
- What evidence would resolve it: A comparative analysis of wall-clock time and FLOPs/token costs for the Planner-Coder and Multi-Agent Discussion pipelines versus the single-model zero-shot baseline

### Open Question 2
- Question: Do the observed pipeline benefits generalize to proprietary, frontier models (e.g., GPT-4, Claude 3.5)?
- Basis in paper: [inferred] The authors explicitly restrict their benchmark to open-source models (4B–34B) to address privacy and cost, leaving the interaction between these specific agentic frameworks and high-capacity frontier models untested
- Why unresolved: It is unclear if the "Planner-Coder" synergy or "Multi-Agent Discussion" improvements are specific to the reasoning limitations of smaller open models or if they offer additive value to state-of-the-art proprietary models
- What evidence would resolve it: Benchmarking the proposed pipelines using proprietary models as the Planner/Aggregator agents to measure the delta over their standard zero-shot performance

### Open Question 3
- Question: Can adaptive stopping criteria or dynamic persona assignment optimize the Multi-Agent Discussion (MAD) pipeline?
- Basis in paper: [inferred] The results (Section 5.2) indicate that metrics generally plateau after Round 2, while CodeLlama models fail to benefit from discussion at all due to weak grounding
- Why unresolved: A fixed 3-round limit and static personas appear inefficient for high-performing models (where 1 round suffices) and ineffective for weak models (which cannot utilize critique), suggesting a need for dynamic interaction
- What evidence would resolve it: Implementing a consensus-based early-stopping mechanism and ablation studies varying agent personas to determine if discussion depth correlates with specific model capabilities

## Limitations
- Limited evaluation scope: Results may not generalize to real-world enterprise databases with different schema complexities and query patterns
- Model configuration gaps: Exact model identifiers and MAD agent persona texts are not fully specified, creating ambiguity for reproduction
- Pipeline-specific failure modes: Some models show regression with Multi-Agent Discussion, but systematic analysis of failure conditions is lacking

## Confidence
- **High confidence**: The baseline zero-shot results and the general trend that reasoning models perform better as planners/aggregators
- **Medium confidence**: The claim that Multi-Agent Discussion improves small model performance, with mixed results showing both improvements and regressions
- **Medium confidence**: The Coder-Aggregator pipeline results, highly dependent on candidate diversity and aggregator quality

## Next Checks
1. Validate the zero-shot baseline and Planner-Coder pipeline: Replicate the zero-shot baseline for Gemma 3 27B IT and Qwen2.5-7B-Instruct on BIRD Mini-Dev (100 examples) to verify the reported EX scores of 52.4% and 31.4% respectively. Then implement the Planner-Coder pipeline pairing QwQ-32B with Gemma 3 12B IT coder and measure the improvement over baseline.

2. Test Multi-Agent Discussion round-by-round progression: Run the MAD pipeline (3 rounds) with Qwen2.5-7B-Instruct on BIRD Mini-Dev, tracking EX accuracy after each round to verify the plateau effect by round 2. Include a regression test to ensure no models show the Qwen2.5-32B-type degradation.

3. Stress-test the Coder-Aggregator with homogeneous vs. diverse candidates: Generate SQL candidates using multiple instances of the same coder model versus different coder models, then compare the aggregator's performance. This would validate whether the claimed benefit of candidate diversity is real and quantify the impact of homogeneous generation.