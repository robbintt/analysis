---
ver: rpa2
title: 'SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series
  with Missing Data'
arxiv_id: '2511.23238'
source_url: https://arxiv.org/abs/2511.23238
tags:
- attention
- time
- series
- latent
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SDE-Attention, a family of channel-level
  attention mechanisms for SDE-RNNs handling irregularly sampled time series with
  missing data. It addresses the challenge of improving robustness to severe missingness
  in continuous-time latent dynamics models.
---

# SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data

## Quick Facts
- arXiv ID: 2511.23238
- Source URL: https://arxiv.org/abs/2511.23238
- Authors: Yuting Fang; Qouc Le Gia; Flora Salim
- Reference count: 3
- Primary result: Attention mechanisms improve SDE-RNN robustness to severe missingness in irregular time series

## Executive Summary
SDE-Attention introduces channel-level attention mechanisms for SDE-RNNs handling irregularly sampled time series with missing data. The approach addresses robustness to severe missingness in continuous-time latent dynamics models by augmenting the pre-RNN latent state with plug-and-play attention modules. Experiments show consistent improvements over vanilla SDE-RNNs, with LSTM-based time-varying feature attention achieving the highest average accuracy across datasets under varying missingness rates.

## Method Summary
SDE-Attention extends SDE-RNNs by inserting attention modules at the pre-RNN latent state. The framework tests four attention variants: static channel recalibration (SDE-SCHA), time-varying feature attention using LSTM (SDE-TVF-L) or Transformer (SDE-TVF-T), and pyramidal multi-scale self-attention (SDE-PYR). The backbone uses a 50-unit single-layer MLP for drift/diffusion networks, GRU cell updates, and Euler-Maruyama SDE solver. Experiments evaluate performance on synthetic periodic data and 18 UCR/UEA benchmark datasets under missing rates of 0%, 30%, 60%, and 90%.

## Key Results
- LSTM-based time-varying feature attention (SDE-TVF-L) achieved highest average accuracy, raising mean performance by approximately 4-10 percentage points across datasets
- Time-varying feature attention proved most robust on univariate data, while different attention types excelled on different multivariate tasks
- Consistent improvements observed over vanilla SDE-RNNs across all tested missingness rates (30%, 60%, 90%)

## Why This Works (Mechanism)
The approach works by allowing the model to adaptively recalibrate channel importance and temporal features in the latent space before the recurrent update. Attention mechanisms compensate for information loss due to irregular sampling and missing data by dynamically weighting relevant features based on their contribution to downstream prediction tasks.

## Foundational Learning
- **SDE-RNNs**: Stochastic differential equations used to model continuous-time latent dynamics - needed for handling irregular sampling naturally; check by verifying the model uses Euler-Maruyama solver
- **Channel-level attention**: Weighting individual feature channels rather than temporal positions - needed for adapting to missing data patterns; check by confirming recalibration operates per channel
- **Time-varying feature attention**: Attention mechanisms that evolve with time - needed for capturing temporal dependencies in irregular series; check by verifying LSTM/Transformer encoders process time-aware features
- **MCAR masking**: Missing completely at random assumption for synthetic missingness generation - needed for controlled experimental conditions; check by confirming independent missingness probability per time step
- **Euler-Maruyama solver**: Numerical method for approximating SDE solutions - needed for practical implementation of continuous-time models; check by verifying solver step size and stability
- **UCR/UEA benchmarks**: Standard time series classification datasets - needed for comparing against established baselines; check by confirming dataset splits and preprocessing match standard protocols

## Architecture Onboarding

**Component Map**: Input Data -> SDE Integration -> Pre-RNN Latent State -> Attention Module -> GRU Update -> Output

**Critical Path**: Irregular time series → SDE-RNN backbone → Pre-RNN latent state → Attention insertion → Classification/Interpolation

**Design Tradeoffs**: Pre-RNN insertion point allows attention to condition on full latent trajectory before discrete updates, but misses continuous-time attention opportunities; different attention types offer flexibility but require manual selection based on data characteristics

**Failure Signatures**: 
- SDE-PYR underperforms baseline on short series → likely overfitting
- Accuracy collapses at 90% missingness → possible Brownian path initialization issues
- No improvement over baseline → attention module may not be properly integrated

**3 First Experiments**:
1. SDE-TVF-L on Wafer dataset at 60% missingness, comparing accuracy against vanilla SDE-RNN
2. SDE-SCHA on synthetic periodic data, verifying interpolation MSE improvement
3. SDE-PYR on ProximalPhalanxTW, checking for overfitting symptoms on short series

## Open Questions the Paper Calls Out
- Can adaptive mechanisms be designed to automatically select or combine SDE-Attention types for specific datasets?
- How does SDE-Attention compare to strong discrete-time irregular time series baselines?
- Does applying attention directly over the latent SDE trajectory offer benefits over the pre-RNN insertion point?

## Limitations
- No comparison against strong discrete-time irregular time series baselines like GRU-D or masked Transformers
- Attention modules tested only at pre-RNN insertion point, not over continuous latent trajectories
- Optimal attention variant appears task-dependent, requiring manual selection rather than automatic adaptation

## Confidence
- Main claims: Medium
- Reproducibility details: Low (several critical hyperparameters unspecified)
- Generalization beyond UCR/UEA: Medium (limited to standard benchmarks and single synthetic dataset)
- Relative performance to state-of-the-art: Low (no comparison to strong baselines)

## Next Checks
1. Replicate SDE-TVF-L on 3 UCR datasets (Wafer, MoteStrain, ProximalPhalanxTW) at 60% missingness, verifying accuracy within ±3% of reported values and confirming baseline improvement
2. Perform ablation studies on TVF encoder dimensions and SDE solver step sizes to assess sensitivity and SDE-PYR's performance on short series
3. Test SDE-Attention on a held-out dataset (CharacterTrajectories) or synthetic multivariate periodic dataset to evaluate robustness beyond presented benchmarks