---
ver: rpa2
title: Investigating Political and Demographic Associations in Large Language Models
  Through Moral Foundations Theory
arxiv_id: '2510.13902'
source_url: https://arxiv.org/abs/2510.13902
tags:
- responses
- your
- moral
- llms
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (LLMs) are increasingly used for advice-giving
  roles in sensitive domains, raising concerns about potential ideological biases
  in their outputs. To quantify these biases, this paper applies Moral Foundations
  Theory (MFT), a framework categorizing moral reasoning into five dimensions: Harm,
  Fairness, Ingroup Loyalty, Authority, and Purity.'
---

# Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory

## Quick Facts
- arXiv ID: 2510.13902
- Source URL: https://arxiv.org/abs/2510.13902
- Reference count: 17
- Key outcome: LLMs show asymmetric ideological alignment—better approximating liberal than conservative human responses, with persona-based role-play amplifying stereotyped associations beyond human distributions

## Executive Summary
This study investigates ideological biases in large language models by applying Moral Foundations Theory (MFT) to quantify moral reasoning patterns across five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity. The research examines whether LLMs inherently align with human liberal or conservative moral preferences, how accurately they represent ideological perspectives when explicitly prompted, and whether demographic-based role-play affects their responses. Across multiple experiments using GPT-4o-mini, Claude-3-Haiku, DeepSeek, and Vicuna, the results reveal that LLMs do not consistently align with either liberal or conservative human responses in baseline outputs. When explicitly prompted to adopt ideological perspectives, models partially mimic liberal responses but diverge significantly from conservative patterns, especially on Ingroup and Harm foundations. Persona-based role-play further amplifies these differences, with conservative personas eliciting extreme responses on Purity and Authority that exceed documented human data.

## Method Summary
The study administered Moral Foundations Theory questionnaires to four LLMs across three conditions: inherent/baseline, explicit ideological role-play, and persona-based role-play. The MFT instrument measured five moral foundations using 6-point Likert scales, with responses compared against human survey data from Graham et al. (2009) using independent samples t-tests. The researchers tested 28 constructed personas (14 liberal-aligned, 14 conservative-aligned) based on Pew Research demographics, with models required to provide numeric-only responses. Statistical analysis focused on mean differences (M) and standardized effect sizes (d), with d≥0.8 considered a notable effect.

## Key Results
- LLMs do not inherently align with either liberal or conservative human responses in baseline outputs
- When explicitly prompted, models partially mimic liberal responses but diverge significantly from conservative patterns, especially on Ingroup and Harm foundations
- Persona-based role-play amplifies stereotyped associations, with conservative personas eliciting extreme responses on Purity and Authority (d=2.300 and d=2.644) that exceed human data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moral Foundations Theory provides a structured, quantifiable framework for detecting ideological biases in LLMs that would otherwise remain opaque
- Mechanism: The five foundations capture morally-loaded intuitions that differentiate human political ideologies. By administering identical questionnaires to LLMs and comparing against human baselines, divergences reveal where model "moral reasoning" departs from human patterns
- Core assumption: LLM responses to MFT items reflect comparable cognitive processes to human moral judgment, rather than surface-level text patterns
- Evidence anchors:
  - [abstract]: "To quantify these biases, this paper applies Moral Foundations Theory (MFT), a framework categorizing moral reasoning into five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity"
  - [section]: "Moral Foundations Theory (Haidt and Graham 2007) asserts that human morality can be broken down into five main values, or foundations, that shape ethical judgments and behaviors"
- Break condition: If LLMs interpret Likert-scale questions fundamentally differently than humans (e.g., acquiescence bias producing uniformly high scores), MFT comparisons become artifact-driven rather than ideologically meaningful

### Mechanism 2
- Claim: Explicit ideological prompting produces asymmetric alignment—models better approximate liberal than conservative human responses
- Mechanism: Prompts activate learned associations between ideological labels and response patterns in training data. However, these associations appear stronger/better-calibrated for liberal patterns, potentially due to training data composition or alignment procedures
- Core assumption: Ideological labels in prompts reliably activate separable ideological representations within the model
- Evidence anchors:
  - [abstract]: "When explicitly prompted to adopt ideological perspectives, models partially mimic liberal responses but diverge significantly from conservative patterns, especially on Ingroup and Harm foundations"
  - [section]: "The largest deviations were in the conservative groups on the Ingroup and Harm foundations (d=.835 and .824 respectively), suggesting that model's current role-playing of conservative ideology are most out of line with MFT statements"
- Break condition: The 2009 human reference data had liberals 3x more represented than conservatives—if this biases the baseline, model "divergence" from conservative patterns may partially reflect reference data limitations

### Mechanism 3
- Claim: Demographic persona prompts amplify stereotyped associations beyond human population distributions, particularly for conservative-aligned traits
- Mechanism: Personas activate bundled correlations between demographic features and moral positions learned from training data. These correlations appear caricatured—models overshoot on culturally-associated foundations (Purity, Authority for conservatives) beyond any documented human patterns
- Core assumption: Models learn robust, accessible associations between demographic profiles and ideological positions from training corpora
- Evidence anchors:
  - [abstract]: "Persona-based role-play further amplifies these differences, with conservative personas eliciting extreme responses on Purity and Authority that exceed human data"
  - [section]: "The most extreme differences in conservative persona responses can be seen on the Purity (d=2.300) and Authority (d=2.644) foundations...no human findings we had access to support the level of support for either foundation expressed in the responses"
- Break condition: If persona effects stem from prompt complexity/length rather than demographic content per se, the stereotype amplification interpretation fails

## Foundational Learning

- Concept: **Likert Response Artifacts in LLMs**
  - Why needed here: The paper notes LLMs "broadly agreed more strongly (respond higher) than humans with statements across all foundation categories"—distinguishing acquiescence bias from ideological alignment is critical
  - Quick check question: On neutral statements with 1-6 scales, does your model systematically prefer higher values? Test before interpreting ideological differences

- Concept: **Effect Size Interpretation (Cohen's d)**
  - Why needed here: The paper emphasizes d≥0.8 as "notable"—understanding that d=2.300 means ~2.3 standard deviations separation helps assess practical significance beyond p-values
  - Quick check question: A conservative persona scores d=2.644 higher than humans on Authority—is this a small, medium, or large effect in practical terms?

- Concept: **Reference Data Temporal Drift**
  - Why needed here: Human baseline data from 2009 may not reflect 2025 ideological distributions; political landscapes have shifted
  - Quick check question: If your model's conservative persona doesn't match 2009 conservatives, what proportion might reflect reference data obsolescence versus model misalignment?

## Architecture Onboarding

- Component map: MFT Instrument -> Prompt Template System -> Multi-Model Harness -> Comparison Pipeline
- Critical path:
  1. Validate prompt compliance across all models (numeric responses only, no refusals)
  2. Collect inherent (neutral), explicit ideological, and persona-conditioned responses
  3. Compute foundation-level comparisons against human baselines
  4. Identify systematic divergence patterns (especially Purity/Authority amplification)
- Design tradeoffs:
  - Aggregating across models obscures model-specific patterns but increases generalizability
  - Rich personas (7-9 traits) increase realism but introduce confound complexity
  - 2009 human data enables direct comparison but may be temporally stale
- Failure signatures:
  - Non-compliant outputs (refusals, justifications) → prompt refinement needed
  - Ceiling effects (uniformly high scores) → acquiescence bias suspected
  - Inherent ≈ explicit-conservative responses → model may default conservative without prompting
- First 3 experiments:
  1. Compliance validation: Run neutral MFT prompt across all target models; verify clean 1-6 numeric outputs without refusals or explanations
  2. Ideological sensitivity: Compare inherent vs. explicit-liberal vs. explicit-conservative; quantify whether explicit-conservative differs from inherent
  3. Single-trait isolation: Test simplified personas varying one demographic dimension at a time (age only, religion only) to identify which features drive amplification

## Open Questions the Paper Calls Out

- Question: Do reasoning models using chain-of-thought produce different Moral Foundations Theory (MFT) scores or provide better explanations for moral decisions compared to non-reasoning models?
  - Basis in paper: [explicit] The authors note in the Limitations section that they "focused on non-reasoning chat models," and suggest that reasoning models "may provide an interesting context for further experimentation" by accessing model "reasoning"
  - Why unresolved: The current study only evaluated GPT-4o-mini, Claude Haiku, DeepSeek, and Vicuna, none of which utilize explicit chain-of-thought reasoning in the way models like DeepSeek-R1 or o1 do
  - What evidence would resolve it: Repeating the MFT prompt experiments on reasoning models and analyzing the generated thought traces to see if scores align better with human data or reveal different bias mechanisms

- Question: Which specific demographic traits within a persona most strongly drive the observed ideological shifts in LLM responses?
  - Basis in paper: [explicit] The authors state that their complex personas (with 4-9 traits) prevent isolating variables, and propose that "a more detailed analysis of simplified or modular personas... could reveal which characteristics models tend to use"
  - Why unresolved: The study designed personas with multiple overlapping attributes (age, geography, religion, etc.), making it impossible to determine if, for example, "Protestant" or "Rural" drove the extreme "Purity" scores in conservative personas
  - What evidence would resolve it: An ablation study using single-attribute personas to measure the causal impact of individual traits (e.g., "Urban" vs. "Rural") on moral foundation scores

- Question: Is the tendency for LLMs to provide higher agreement scores across all foundations an inherent "acquiescence bias" or an artifact of the specific prompt engineering used?
  - Basis in paper: [explicit] In the RQ1 results, the authors ask: "Whether this reflects an inherent tendency of LLMs to respond affirmatively in response to Likert scale questions or if it is an artifact of our prompt engineering would require additional experiments"
  - Why unresolved: The analysis showed models consistently agreed more strongly than humans, but the study design could not distinguish between a model's intrinsic bias toward "agreeing" and a reaction to the specific phrasing of the prompt constraints
  - What evidence would resolve it: Administering the survey with reversed polarity questions (e.g., negatively worded items) or varying the prompt strictness to see if the high-agreement trend persists or inverts

## Limitations

- Reliance on decade-old human reference data (2009) that may not reflect current ideological distributions
- Overrepresentation of liberals (3:1 ratio) in the baseline human dataset, potentially biasing divergence measurements
- Multi-model aggregation approach that obscures model-specific patterns critical for understanding architectural differences

## Confidence

- **High**: Models do not inherently align with liberal or conservative human responses in baseline outputs; explicit ideological prompting produces asymmetric alignment favoring liberal patterns
- **Medium**: Demographic persona prompts amplify stereotyped associations beyond human distributions, particularly for conservative-aligned traits; effect sizes (d≥0.8) indicate practically meaningful differences
- **Low**: The mechanism by which LLMs learn and reproduce demographic-moral associations remains incompletely understood, limiting causal interpretation of amplification effects

## Next Checks

1. Replicate the analysis using contemporary human MFT data to separate reference data drift from actual model misalignment
2. Conduct ablation studies testing individual persona traits to isolate which demographic features drive the most extreme amplification effects
3. Test cross-cultural generalization by administering MFT items to LLMs trained on non-Western corpora and comparing against corresponding human baselines