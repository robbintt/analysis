---
ver: rpa2
title: Learning-Based TSP-Solvers Tend to Be Overly Greedy
arxiv_id: '2502.00767'
source_url: https://arxiv.org/abs/2502.00767
tags:
- instances
- nearest-neighbor
- distribution
- density
- solvers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the performance of deep learning-based TSP
  solvers when trained on uniform distributions. The authors introduce a new statistical
  measure called "nearest-neighbor density" to quantify how often the nearest neighbor
  is chosen in the optimal solution.
---

# Learning-Based TSP-Solvers Tend to Be Overly Greedy

## Quick Facts
- arXiv ID: 2502.00767
- Source URL: https://arxiv.org/abs/2502.00767
- Authors: Xiayang Li; Shihua Zhang
- Reference count: 40
- Primary result: Uniform TSP datasets lead to greedy solver behavior; data augmentation with scale-free and drilling patterns improves generalization

## Executive Summary
This paper investigates why deep learning-based TSP solvers trained on uniform distributions exhibit overly greedy behavior. The authors introduce a novel statistical measure called "nearest-neighbor density" to quantify the frequency of nearest neighbor selection in optimal solutions. They demonstrate that uniform datasets have high nearest-neighbor density, which causes solvers to develop greedy strategies. To address this limitation, they propose a data augmentation method that generates TSP instances with lower nearest-neighbor density using scale-free networks and drilling problem patterns. Fine-tuning solvers on these augmented instances significantly improves their generalization performance across various TSP distributions.

## Method Summary
The authors develop a new metric called "nearest-neighbor density" to quantify how often the nearest neighbor is chosen in optimal TSP solutions. They analyze three deep learning-based TSP solvers and find that uniform training data leads to high nearest-neighbor density, causing greedy behavior. To address this, they create a data augmentation pipeline that generates TSP instances with lower nearest-neighbor density by sampling from scale-free networks and drilling problem patterns. The augmented dataset is then used to fine-tune existing TSP solvers, resulting in improved performance on benchmark datasets.

## Key Results
- Uniform TSP datasets lead to high nearest-neighbor density in optimal solutions
- Three deep learning-based TSP solvers exhibit greedy behavior when trained on uniform distributions
- Data augmentation using scale-free networks and drilling problem patterns reduces nearest-neighbor density
- Fine-tuning on augmented instances significantly improves solver generalization performance
- Claims that truly universal neural TSP solvers are infeasible based on current approaches

## Why This Works (Mechanism)
The paper's mechanism centers on the relationship between training data distribution and solver behavior. When TSP solvers are trained on uniform distributions, the optimal solutions tend to select nearest neighbors frequently due to the spatial characteristics of uniform point distributions. This creates a feedback loop where solvers learn to prioritize local optimization over global exploration. By augmenting the training data with instances that have lower nearest-neighbor density (from scale-free networks and drilling problems), the solvers are exposed to more diverse solution patterns that require global reasoning. This forces the models to develop more balanced strategies that consider both local and global optimization.

## Foundational Learning
- Nearest-neighbor density metric: A statistical measure quantifying the frequency of nearest neighbor selection in optimal TSP solutions. Needed to identify and quantify greedy behavior patterns in trained solvers. Quick check: Verify metric correlates with greedy behavior across different solver architectures.
- Scale-free network generation: A method for creating TSP instances with power-law degree distributions. Needed to generate diverse problem instances that require global optimization strategies. Quick check: Confirm generated instances have significantly different structural properties from uniform distributions.
- Data augmentation principles: Techniques for expanding training datasets to improve model generalization. Needed to address distributional bias in original training data. Quick check: Validate that augmented data improves performance across multiple TSP distributions.

## Architecture Onboarding
**Component map:** Data Generator -> Nearest-Neighbor Density Calculator -> TSP Solver Trainer -> Performance Evaluator
**Critical path:** Training data generation → NN density analysis → Solver fine-tuning → Benchmark evaluation
**Design tradeoffs:** The approach trades computational overhead of data augmentation for improved generalization performance. The choice of scale-free networks and drilling patterns represents a balance between generating diverse instances and maintaining computational feasibility.
**Failure signatures:** Poor performance on non-uniform distributions, over-reliance on local optimization, failure to capture global solution structures
**First experiments:**
1. Baseline evaluation of three TSP solvers on uniform vs non-uniform distributions
2. NN density analysis of optimal solutions across different TSP distributions
3. Fine-tuning experiments with varying proportions of augmented data

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is constrained to uniform TSP instances and three specific TSP solvers, limiting generalizability
- The "nearest-neighbor density" metric, while useful, is a novel construct requiring independent validation
- Data augmentation approach relies on two specific generative mechanisms whose representativeness for real-world applications remains unclear
- Performance improvements demonstrated only on benchmark datasets, not practical deployment scenarios
- Claim of universal solver infeasibility based on limited experimental evidence

## Confidence
- NN density metric validity: Medium
- Data augmentation effectiveness: High
- Universal solver infeasibility: Low
- Greedy behavior generalization: Medium

## Next Checks
1. Test the NN density metric and data augmentation approach across additional TSP distributions (e.g., clustered, Gaussian) and solver architectures beyond the three studied models
2. Conduct ablation studies to isolate the contribution of each data augmentation component (scale-free vs drilling patterns) to performance gains
3. Evaluate the computational overhead and scalability of the augmentation pipeline for larger TSP instances (n>100) in practical deployment scenarios