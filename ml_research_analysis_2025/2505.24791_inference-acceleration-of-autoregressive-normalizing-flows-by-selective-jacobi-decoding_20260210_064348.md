---
ver: rpa2
title: Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi
  Decoding
arxiv_id: '2505.24791'
source_url: https://arxiv.org/abs/2505.24791
tags:
- inference
- sequential
- jacobi
- generation
- autoregressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the inference inefficiency of autoregressive
  normalizing flows due to their inherent sequential generation process. The authors
  observe that strict sequential dependency contains redundancy, particularly varying
  across network layers, and propose a selective Jacobi decoding (SeJD) strategy to
  accelerate inference via parallel iterative optimization.
---

# Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding

## Quick Facts
- arXiv ID: 2505.24791
- Source URL: https://arxiv.org/abs/2505.24791
- Reference count: 37
- Achieves up to 4.7× speedup over standard sequential inference while maintaining generation quality

## Executive Summary
This work addresses the inference inefficiency of autoregressive normalizing flows due to their inherent sequential generation process. The authors observe that strict sequential dependency contains redundancy, particularly varying across network layers, and propose a selective Jacobi decoding (SeJD) strategy to accelerate inference via parallel iterative optimization. The method achieves up to 4.7× speedup over standard sequential inference while maintaining generation quality. Theoretical analysis demonstrates superlinear convergence and finite convergence guarantees. Extensive experiments on CIFAR-10, CIFAR-100, and AFHQ datasets validate the effectiveness and generalizability of SeJD, showing consistent acceleration without significant degradation in Fréchet Inception Distance (FID) or other quality metrics.

## Method Summary
The method reformulates autoregressive inference as solving a system of non-linear equations via Jacobi iteration, enabling parallel updates with guaranteed finite convergence. SeJD selectively applies Jacobi decoding to non-initial layers, using standard sequential decoding for the first layer where dependencies are strong. This optimizes the speed-quality trade-off by matching decoding strategy to layer-specific redundancy characteristics. The approach achieves superlinear convergence near the fixed point and finite convergence in at most L iterations, where L is the sequence length.

## Key Results
- Achieves up to 4.7× speedup over standard sequential inference on CIFAR-10
- Maintains generation quality with no significant degradation in FID scores
- Theoretical analysis demonstrates superlinear convergence and finite convergence guarantees
- Extensive experiments validate effectiveness across CIFAR-10, CIFAR-100, and AFHQ datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strict sequential dependencies in autoregressive normalizing flows contain exploitable redundancy that varies by layer depth.
- Mechanism: The paper demonstrates that masking the o-nearest preceding patches during inference still produces meaningful images, with later layers showing higher tolerance (lower cosine similarity deviation) than the first layer. This suggests the first layer performs structure initiation from noise (high dependency) while subsequent layers refine existing structure (lower dependency).
- Core assumption: The observed redundancy in trained models generalizes across datasets and model configurations.
- Evidence anchors:
  - [Section 3.2]: "The deviation is significantly larger for the first layer compared to subsequent ones. This result empirically confirms the low redundancy in the first layer."
  - [Section 3.2, Figure 2]: Images generated with o=5 masked dependencies remain coherent on CIFAR-10 and AFHQ.
  - [corpus]: Related work on diagonal decoding for autoregressive video generation (arXiv:2503.14070) similarly exploits parallelism in sequential generation, suggesting broader applicability of redundancy exploitation.
- Break condition: If convergence requires >L iterations consistently, or if FID degrades >5% from baseline, redundancy assumption may not hold for that model/dataset.

### Mechanism 2
- Claim: Reformulating autoregressive inference as solving a system of non-linear equations via Jacobi iteration enables parallel updates with guaranteed finite convergence.
- Mechanism: Instead of computing z_k,l sequentially (depending on all z_k,<l), Jacobi iteration computes all L elements in parallel using values from the previous iteration z^(t-1). The triangular dependency structure guarantees convergence in at most L iterations, with superlinear convergence near the fixed point due to the spectral radius ρ(J_F) = 0.
- Core assumption: The Jacobi iteration map F is C¹-diffeomorphic near the fixed point; initialization is sufficiently close to the true solution for superlinear convergence.
- Evidence anchors:
  - [Section 3.4, Proposition 3.1]: "∃δ>0, s.t. ∀z_k,0 satisfies ||z_k,0 - z_k|| < δ, the iterative sequence converges to z_k with superlinear convergence rate."
  - [Section 3.4, Proposition 3.2]: "Denoting the sequence length of z_k as L, we have z_k^L = z_K."
  - [corpus]: Jacobi decoding has been applied to language models (Santilli et al., ACL 2023) and image generation (Teng et al., ICLR 2025), though often with quality degradation—this paper's continuous-space setting may mitigate that issue.
- Break condition: If iteration count approaches L without convergence, or if ∥z_k^t - z_k^(t-1)∥ oscillates rather than decreases, the fixed-point assumption may be violated.

### Mechanism 3
- Claim: Selective application of Jacobi decoding to non-initial layers optimizes the speed-quality trade-off by matching decoding strategy to layer-specific redundancy characteristics.
- Mechanism: SeJD uses standard sequential decoding for the first layer (where dependencies are strong and Jacobi would incur high per-iteration cost without parallel benefit) and switches to Jacobi iteration for remaining layers (where redundancy is high and convergence is fast). This avoids the memory overhead and KV-cache incompatibility of uniform Jacobi decoding on low-redundancy layers.
- Core assumption: The depthwise heterogeneity pattern (low redundancy in layer 1, high in subsequent layers) holds across model scales and datasets.
- Evidence anchors:
  - [Section 3.5]: "It utilizes standard sequential decoding for the first layer, which is a dependency-heavy layer. Subsequently, it switches to the parallel Jacobi iteration only for the remaining layers."
  - [Table 1]: Uniform Jacobi Decoding (UJD) fails on AFHQ (0.8× speedup, slower than baseline), while SeJD achieves 4.5× speedup with identical FID.
  - [corpus]: No direct corpus comparison for selective layer strategies in normalizing flows; related work on cache-aware attention (MARché, arXiv:2506.12035) addresses similar recomputation overhead but through different mechanisms.
- Break condition: If layer-wise convergence analysis shows similar convergence rates across all layers, selective strategy provides no benefit; if FID degradation concentrates in later layers, the redundancy assumption for those layers may be wrong.

## Foundational Learning

- **Normalizing Flows and Change of Variables**
  - Why needed here: The paper builds on invertible transformations f_k where log-likelihood requires computing log|det(∂f_k/∂z_k)|; understanding Eq. (1-4) is essential for grasping why inverse inference is sequential.
  - Quick check question: Given z_{k-1} = f_k(z_k), can you derive why computing z_k from z_{k-1} requires sequential dependency in the autoregressive formulation?

- **Jacobi vs. Gauss-Seidel Iteration**
  - Why needed here: The paper frames sequential inference as implicit Gauss-Seidel and SeJD as Jacobi iteration; understanding the difference explains why parallelization is possible.
  - Quick check question: In Jacobi iteration for solving F(z) = 0, why can all components be updated simultaneously, and what is the trade-off compared to Gauss-Seidel?

- **Convergence Rates (Linear, Superlinear, Finite)**
  - Why needed here: The paper claims superlinear convergence (Prop. 3.1) and finite convergence (Prop. 3.2); distinguishing these is critical for interpreting theoretical guarantees.
  - Quick check question: If ∥e_{t+1}∥ = o(∥e_t∥), what does this imply about error reduction as t increases, and how does it differ from linear convergence ∥e_{t+1}∥ ≤ c∥e_t∥ for c < 1?

## Architecture Onboarding

- **Component map:**
  - Input: Latent variable z_K ~ p_K (Gaussian)
  - K flow layers: Each f_k implements autoregressive transformation with s_k(·) and g_k(·) networks (typically causal ViT blocks in TarFlow)
  - SeJD controller: Routes layer 1 to sequential decoder, layers 2-K to Jacobi decoder with threshold τ
  - Jacobi decoder: Iteratively solves F(z_k,l, z_k,<l, z_{k+1,l}) = 0 in parallel across all L patches
  - Output: Generated sample x = F(z_K)

- **Critical path:**
  1. Sample z_K from Gaussian prior
  2. Apply sequential decoding for layer 1 (z_K → z_{K-1})
  3. For each subsequent layer k: initialize z_k^0, iterate Jacobi updates until ∥z_k^t - z_k^{t-1}∥_∞ < τ
  4. Return final x

- **Design tradeoffs:**
  - τ (stopping threshold): Lower τ → higher quality, more iterations; τ=0.5 is default (Figure 5 shows FID-time trade-off)
  - Memory vs. speed: Jacobi requires storing all L patches per iteration vs. 1 patch sequentially
  - Layer selection: Paper uses first-layer-sequential heuristic; assumption-based and may not generalize

- **Failure signatures:**
  - Divergent iterations: ∥z_k^t - z_k^{t-1}∥ increases or oscillates → check initialization, reduce τ
  - FID spike on new dataset: May indicate different depthwise redundancy pattern → run masking experiment (Sec 3.2) to verify
  - Slower than sequential: Likely high per-iteration cost on low-redundancy layers → verify selective strategy is applied

- **First 3 experiments:**
  1. **Convergence validation**: Run Jacobi decoding on a single layer with τ=0.5, plot ∥z_k^t - z_k^{t-1}∥ vs. t; confirm convergence in <L iterations and compare layer 1 vs. layer K convergence rates.
  2. **Redundancy probing**: Replicate the o-nearest masking experiment (Eq. 5) on your target dataset; verify cosine similarity deviation is larger for layer 1 than subsequent layers.
  3. **End-to-end speedup baseline**: Compare sequential inference, UJD, and SeJD on a small batch; measure inference time, FID, and iteration counts to confirm Table 1 patterns hold for your setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed sequential redundancy and depthwise heterogeneity persist in partially-trained or under-trained models?
- Basis in paper: [explicit] Appendix C states: "it remains unknown whether this redundancy persists to the same extent in partially-trained or under-trained models. This uncertainty might affect SeJD's effectiveness when models are under-fitting or in earlier stages of training."
- Why unresolved: All experiments used fully-trained models; no analysis was conducted on checkpoints during training or on models with different convergence states.
- What evidence would resolve it: Evaluate SeJD on TarFlow checkpoints saved at various training epochs, measuring both redundancy patterns (via cosine similarity analysis) and acceleration performance.

### Open Question 2
- Question: Can SeJD principles be leveraged to optimize the training process or guide neural architecture design for inherently more efficient models?
- Basis in paper: [explicit] Appendix C states the potential for leveraging SeJD's principles to optimize training or guide architecture design "has not been explored" and suggests future work could investigate this.
- Why unresolved: This work focuses exclusively on inference-time acceleration without modifying training or architecture.
- What evidence would resolve it: Design experiments that incorporate redundancy-aware loss terms during training, or modify architectural components (e.g., attention patterns) to increase redundancy in layers where parallel decoding is most beneficial.

### Open Question 3
- Question: Why does uniform Jacobi decoding (UJD) slow down inference on AFHQ (0.8×) while accelerating on CIFAR datasets (2.4–2.9×)?
- Basis in paper: [inferred] Table 1 shows UJD achieves speedup on CIFAR but performs worse than sequential baseline on AFHQ. Authors suggest "higher per-iteration computational costs combined with potentially stronger dependencies" but do not isolate or verify these factors.
- Why unresolved: The exact interaction between dataset scale, dependency strength, and Jacobi iteration overhead remains uncharacterized.
- What evidence would resolve it: Conduct controlled experiments varying image resolution and sequence length while measuring per-iteration costs and convergence rates; analyze dependency strength via masking experiments across dataset scales.

### Open Question 4
- Question: Can SeJD generalize to other autoregressive architectures beyond normalizing flows, such as autoregressive transformers for language or discrete token generation?
- Basis in paper: [inferred] The paper notes vanilla Jacobi decoding has shown limited success in discrete token spaces (Section 3.3) and SeJD's effectiveness in continuous-space normalizing flows may rely on "smoother iterative convergence." Whether the selective strategy transfers remains untested.
- Why unresolved: No experiments were conducted on non-flow autoregressive models.
- What evidence would resolve it: Apply SeJD to autoregressive language models or image tokenizers, comparing against prior Jacobi decoding methods and analyzing whether depthwise heterogeneity analogues exist.

## Limitations
- The depthwise heterogeneity assumption may not generalize to all autoregressive normalizing flow architectures or data modalities.
- Superlinear convergence depends on the diffeomorphism assumption near the fixed point, which is theoretically plausible but not exhaustively validated.
- Memory overhead for parallel iteration remains a concern for high-resolution generation, though not explicitly quantified.

## Confidence
- **High confidence**: The existence of layer-wise redundancy variation and finite convergence of Jacobi iteration are directly supported by mathematical proofs and empirical measurements.
- **Medium confidence**: The selective layer strategy's effectiveness relies on assumptions about depthwise patterns that may vary with architecture.
- **Medium confidence**: The superlinear convergence claim depends on the diffeomorphism assumption near the fixed point.

## Next Checks
1. **Layer-wise convergence validation**: Measure Jacobi iteration counts and convergence rates for each layer on CIFAR-10 and AFHQ; verify that layer 1 consistently requires more iterations or shows different convergence behavior than subsequent layers.
2. **Threshold sensitivity analysis**: Systematically vary τ ∈ {0.1, 0.3, 0.5, 1.0, 2.0} on CIFAR-10; quantify FID degradation and iteration count changes to confirm the claimed stability for τ ≤ 1.0.
3. **Memory overhead quantification**: Profile GPU memory usage for sequential vs. Jacobi decoding across different batch sizes and sequence lengths; confirm that the 4.7× speedup doesn't come with prohibitive memory scaling.