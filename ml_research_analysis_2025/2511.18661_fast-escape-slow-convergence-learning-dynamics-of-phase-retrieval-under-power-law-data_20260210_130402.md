---
ver: rpa2
title: 'Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under
  Power-Law Data'
arxiv_id: '2511.18661'
source_url: https://arxiv.org/abs/2511.18661
tags:
- phase
- learning
- dynamics
- scaling
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides the first rigorous analysis of scaling laws
  in nonlinear regression with anisotropic power-law data. It studies phase retrieval
  with anisotropic Gaussian inputs, where the input covariance spectrum follows a
  power law, leading to an infinite hierarchy of coupled equations for summary statistics.
---

# Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data

## Quick Facts
- arXiv ID: 2511.18661
- Source URL: https://arxiv.org/abs/2511.18661
- Authors: Guillaume Braun; Bruno Loureiro; Ha Quang Minh; Masaaki Imaizumi
- Reference count: 40
- Primary result: First rigorous analysis of scaling laws in nonlinear regression with anisotropic power-law data, revealing three-phase learning dynamics and explicit spectral control of convergence.

## Executive Summary
This work provides the first rigorous analysis of scaling laws in nonlinear regression with anisotropic power-law data. The authors study phase retrieval with anisotropic Gaussian inputs where the input covariance spectrum follows a power law, leading to an infinite hierarchy of coupled equations for summary statistics. Through a tractable reduction via Duhamel's formula, they reveal a three-phase trajectory: fast escape from low alignment, slow convergence of summary statistics, and spectral-tail learning of small-eigenvalue directions. The results show how spectral decay governs convergence times and error curves, validated through experiments.

## Method Summary
The method analyzes phase retrieval with anisotropic Gaussian inputs where the input covariance Q has power-law eigenvalues λᵢ ∝ i⁻ᵃ (a > 1). The population gradient flow dynamics ẇ = -∇L(w) are studied from random initialization on the unit sphere. The key innovation is using Duhamel's formula to reduce the infinite hierarchy of coupled ODEs for summary statistics to a tractable scalar Volterra integral equation. This enables explicit characterization of three distinct phases: fast escape from near-zero correlation, slow convergence of energy and alignment, and spectral-tail learning where MSE decays according to the power-law spectrum.

## Key Results
- Three-phase learning trajectory: fast escape from low alignment, slow convergence of summary statistics, and spectral-tail learning
- Explicit scaling laws for MSE showing how spectral exponent a dictates convergence time and error decay
- Duhamel reduction successfully handles infinite hierarchy of coupled equations
- Phase II requires crossing critical threshold s = 1/3 to avoid saddle regions
- Spectral-tail learning produces power-law decay curves for MSE

## Why This Works (Mechanism)

### Mechanism 1: Infinite Hierarchy Reduction via Duhamel's Formula
In isotropic cases, dynamics collapse to a 2D ODE system. Under anisotropy, the time derivative of order-k summary statistics depends on order-(k+1) statistics, propagating dependencies infinitely. The paper shows that lifting the system to a Banach space and applying Duhamel's formula allows projection back to a scalar Volterra integral equation for the alignment u(t). Core assumption: Q is diagonalizable with power-law spectrum λᵢ ∝ i⁻ᵃ where a > 1.

### Mechanism 2: Three-Phase Learning Trajectory
Learning begins near initialization with low correlation (u ≈ 0). Phase I sees exponential escape driven by top eigenvalues. Phase II involves alignment u(t) and energy s(t) approaching limits, crucially crossing threshold s=1/3. Phase III occurs after summary statistics stabilize, where MSE decays as model learns directions associated with small eigenvalues. Core assumption: Gradient flow dynamics with initial correlation u(0) ≈ d⁻¹/².

### Mechanism 3: Spectral Control of Scaling Laws
In Phase III, coordinate error eᵢ(t) = wᵢ(t) - w*ᵢ decays exponentially at rate λᵢ. Because λᵢ follows power law, MSE is weighted spectral average where tail (small eigenvalues) decays slowest, creating power-law decay curve for error. Core assumption: Spectrum λᵢ ∝ i⁻ᵃ has heavy tail (a > 1).

## Foundational Learning

**Concept: Phase Retrieval & Loss Geometry**
Why needed here: Analyzes specific non-convex loss landscape (y = ⟨x, w*⟩²) with unique critical points. Understanding geometry essential to grasp why "escaping mediocrity" is distinct phase.
Quick check question: Can you identify why origin is local maximum and not saddle point in this specific loss landscape?

**Concept: Anisotropic Power-Law Data**
Why needed here: Core "slow convergence" phenomenon arises because eigenvalues λᵢ decay as i⁻ᵃ, causing unbalanced learning speeds across directions. This is the "Anisotropy" variable in analysis.
Quick check question: How does larger exponent a (steeper spectral decay) qualitatively change difficulty of learning tail directions?

**Concept: Duhamel's Formula / Volterra Equations**
Why needed here: Primary analytical tool used to solve infinite hierarchy of ODEs. Allows authors to write solution for u(t) as integral equation involving kernel K(t).
Quick check question: How does solving Volterra integral equation differ from solving standard linear ODE?

## Architecture Onboarding

**Component map:**
Data Generator -> Phase Retrieval Model -> Gradient Flow Dynamics

**Critical path:**
1. Initialization: w(0) random on unit sphere (u(0) ≈ 0)
2. Phase I (Escape): u(t) grows exponentially until u, u(2) ≥ δ
3. Phase II (Transition): Energy s(t) must cross critical threshold s = 1/3
4. Phase II (Convergence): u(t), s(t) → 1
5. Phase III (Tail Learning): MSE decays according to spectral tail scaling law

**Design tradeoffs:**
- Spectral Exponent a: Larger a implies few dominant directions (faster initial escape) but heavier tail of difficult directions (potentially slower final MSE decay rate relative to dimension)
- Dimension d: Affects time to reach accuracy ε (requires ε ≳ d⁻⁽ᵃ⁻¹⁾/² for tractability)

**Failure signatures:**
- Stuck in Mediocrity: Gradient small near initialization; if spectral gap insufficient or step size wrong, u(t) may not escape
- Plateauing MSE: If Phase II ends but tail not learned, MSE will plateau at level determined by σ*²
- u(t) oscillates or goes negative: Initialization sign random; ensure positive growth by checking w*₁ has constant order

**First 3 experiments:**
1. Replicate Figure 1: Plot MSE vs time (log-log) for different spectral exponents a (e.g., 1.5, 2, 3) to observe change in decay slope
2. Verify Phase II Threshold: Track s(t) to confirm it crosses 1/3 and stabilizes above it, preventing return to saddle region
3. Test Volterra Approximation: Compare empirical trajectory of u(t) against theoretical prediction from Volterra equation (Eq. 5.3) during Phase I

## Open Questions the Paper Calls Out

### Open Question 1
Can the explicit scaling laws and phase decomposition derived for gradient flow be rigorously extended to discrete-time stochastic gradient descent (SGD)?
Basis: Authors state in Section 6 that "extending them to discrete-time SGD remains an open problem," despite providing empirical validation for online SGD. Why unresolved: Theoretical framework relies on continuous-time ODEs and infinite-data population limit. A rigorous proof connecting these dynamics to finite step-size discrete updates is missing.

### Open Question 2
How do finite-sample effects influence the spectral-tail learning phase and resulting compute-error scaling laws?
Basis: Section 6 lists "analyzing finite-sample effects" as necessary extension to strengthen link to practical deep learning. Why unresolved: Analysis focuses on population dynamics (infinite data), which ignores variance and estimation error inherent in finite datasets. A generalization error bound decomposing error into spectral-tail term and finite-sample variance term would show at what sample size n the spectral effect dominates.

### Open Question 3
Do analogous phase decompositions and scaling laws govern training dynamics of wider neural networks or general multi-index models?
Basis: Section 6 raises question of whether "analogous phase decompositions and scaling laws govern training dynamics of wider neural networks." Why unresolved: Paper analyzes specific single-index model (phase retrieval) with quadratic activations. Unknown if "infinite hierarchy of coupled equations" mechanism transfers to deeper architectures.

## Limitations
- Analytical tractability relies on projecting infinite-dimensional system to scalar equation; assumption that higher-order statistics become negligible after Phase II lacks complete rigorous justification
- Analysis operates in population gradient flow regime, assuming exact gradients without finite-sample error bounds
- Initial condition sensitivity: Phase I depends critically on initialization having near-zero correlation (u(0) ≈ 0)

## Confidence

**High Confidence:**
- Infinite hierarchy reduction via Duhamel's formula is mathematically sound and Volterra equation correctly captures Phase I dynamics
- Three-phase decomposition (fast escape → slow convergence → spectral-tail learning) is empirically validated and theoretically grounded
- Spectral exponent a directly controls MSE decay rate in Phase III through power-law tail

**Medium Confidence:**
- Critical threshold s = 1/3 for Phase II completion is derived from loss landscape analysis and appears consistent across experiments
- Approximation that higher-order statistics become negligible after Phase II enabling scalar reduction
- Condition ε ≳ d⁻⁽ᵃ⁻¹⁾/² for tractable tail learning

**Low Confidence:**
- Exact scaling of convergence times with respect to dimension d for general a
- Robustness of three-phase structure under non-Gaussian perturbations to input distribution
- Precise relationship between finite-sample noise and phase transitions

## Next Checks
1. Implement stochastic gradient descent with varying batch sizes and step sizes. Measure how three-phase structure persists or breaks down as noise increases, and quantify additional time required compared to population gradients.

2. Replace Gaussian input distribution with sub-Gaussian or heavy-tailed distributions while maintaining power-law spectrum. Verify whether three-phase structure and scaling laws remain intact or require modification.

3. Systematically vary initialization scheme (different sphere radii, structured initialization, warm-starts) and measure impact on Phase I escape time and overall convergence. Determine whether near-zero initialization assumption is essential or merely sufficient.