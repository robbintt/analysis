---
ver: rpa2
title: Uncovering the Vulnerability of Large Language Models in the Financial Domain
  via Risk Concealment
arxiv_id: '2509.10546'
source_url: https://arxiv.org/abs/2509.10546
tags:
- financial
- risk
- llms
- arxiv
- regulatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Risk-Concealment Attacks (RCA), a red-teaming
  framework that systematically exploits compliance blind spots in large language
  models deployed in financial contexts. RCA iteratively refines prompts to embed
  high-risk financial intent within seemingly compliant queries, bypassing safety
  filters through multi-turn deception.
---

# Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment

## Quick Facts
- arXiv ID: 2509.10546
- Source URL: https://arxiv.org/abs/2509.10546
- Reference count: 11
- Average attack success rate of 93.18% across nine leading models

## Executive Summary
This paper introduces Risk-Concealment Attacks (RCA), a red-teaming framework that systematically exploits compliance blind spots in large language models deployed in financial contexts. RCA iteratively refines prompts to embed high-risk financial intent within seemingly compliant queries, bypassing safety filters through multi-turn deception. Experiments on FIN-Bench, a domain-specific benchmark, show RCA achieves an average attack success rate of 93.18% across nine leading models, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1. RCA consistently outperforms single- and multi-turn baselines in both effectiveness and efficiency, requiring fewer tokens and lower latency. These results reveal a critical gap in current alignment techniques and highlight the urgent need for stronger moderation mechanisms in financial applications.

## Method Summary
RCA operates as a black-box, multi-turn framework that iteratively conceals regulatory risks through professional language framing. The framework uses three agents: an Auxiliary Agent that generates and refines prompts, a Target Model being tested, and a Judge Agent that evaluates compliance. The attack begins with a harmful query that is re-written using a structured template (Role, Scenario, Goal) to mimic legitimate financial workflows. Through iterative refinement based on feedback from the Judge Agent, RCA dynamically adapts to the target model's refusals, progressively escalating toward tactical details while maintaining a facade of compliance. The process continues for up to five rounds or until success is achieved.

## Key Results
- RCA achieves an average attack success rate of 93.18% across nine leading models
- RCA outperforms single-turn baselines with 98.28% ASR on GPT-4.1 and 97.56% on OpenAI o1
- RCA requires fewer tokens and lower latency compared to baseline approaches like Crescendo and FITD

## Why This Works (Mechanism)

### Mechanism 1: Intent Embedding via Professional Framing
Embedding malicious financial intent within professionally structured, compliant-language queries allows adversaries to bypass safety filters trained primarily on overt toxicity. The framework utilizes a "Deceptive Context Generation" phase where a harmful query is re-written using a structured template (Role, Scenario, Goal). This mimics legitimate financial workflows, causing the model to classify the interaction as safe professional assistance rather than regulatory violation. This mechanism likely fails if the target LLM employs a domain-specific "system prompt" explicitly restricting unlicensed financial advice or requiring verified disclaimers for specific regulatory topics.

### Mechanism 2: Feedback-Driven Intent Refinement
An iterative, multi-turn feedback loop allows the attack to dynamically adapt to the target model's refusals, significantly increasing success rates over static prompts. An auxiliary agent generates follow-up questions based on the "dialogue history" and a binary "jailbreak indicator" from a Judge agent. If the target refuses, the agent reframes the query using abstract language; if it complies, the agent escalates toward tactical details. This exploits the model's context window to normalize the risky topic gradually. Defense mechanisms that analyze cumulative intent across the entire conversation history can detect the gradual semantic drift toward non-compliance.

### Mechanism 3: Semantic Risk Dilution
Iteratively paraphrasing prompts to lower their "perceived risk level" while maintaining executable harmful instructions enables the attack to evade intent-classifiers. The "Iterative Deception Refinement" phase rewrites queries to remove explicit risk signals and replaces them with compliant phrasing, effectively laundering the semantic risk. This mechanism relies on safety classifiers that use surface-level semantic correlation with risk keywords rather than deep logical inference of outcomes. Robust "Chain-of-Thought" reasoning defenses that simulate the consequence of the advice before generating output can potentially counter this approach.

## Foundational Learning

- **Interpersonal Deception Theory (IDT)**: The paper explicitly cites IDT as the intuition for the multi-turn design. IDT posits that deception is a dynamic, feedback-driven process. Understanding this helps explain why static defenses fail against adaptive dialogue. Quick check: How does the "feedback loop" in RCA mirror the psychological concept of "building trust" or "testing boundaries" in social engineering?

- **Black-Box Threat Model**: The framework operates without access to model weights or gradients (API-only). This defines the constraints for the "Auxiliary Agent" (attacker) and explains why the method relies on prompt engineering rather than gradient-based adversarial attacks. Quick check: Since the attacker cannot see the model's internal probabilities, what signal does the "Judge Agent" use to guide the next attack iteration?

- **Regulatory vs. Toxic Harm**: This distinction is the paper's core motivation. Standard alignment (RLHF) focuses on toxicity (harm to individuals), while financial safety requires compliance (harm to market integrity/legal standing). Quick check: Why would a model trained to be "helpful" be more vulnerable to "regulatory circumvention" than "hate speech"?

## Architecture Onboarding

- **Component map**: Auxiliary Agent ($f_a$) -> Target Model ($f_t$) -> Judge Agent ($f_j$) -> Auxiliary Agent (feedback loop)
- **Critical path**: 1. Generate Phase 1 prompt from harmful query using Deceptive Context template. 2. Send prompt to Target Model. 3. Judge evaluates response for jailbreak. 4. If failed, Auxiliary Agent reads history + Judge feedback to generate Phase 2 follow-up. 5. Terminate if Judge returns success or max iterations (T=5) reached.
- **Design tradeoffs**: Efficiency vs. Stealth - RCA is more token-efficient than Crescendo or FITD but requires a sophisticated auxiliary model. Judge Accuracy - Using an LLM as a judge is scalable but may hallucinate violations or misses; high correlation with human eval is reported but remains an approximation.
- **Failure signatures**: Explicit Refusal - Target model responds with "I cannot assist with..."; Empty Advice - Target model responds but with vague, generic statements that fail the "Real-world Executability" criteria.
- **First 3 experiments**: 1. Baseline Comparison - Run RCA vs. Single-Turn attacks (e.g., DeepInception) on GPT-4o to replicate the ASR gap. 2. Ablation on Feedback - Disable the "jailbreak indicator" feedback loop to measure the drop in ASR, verifying the importance of the iterative mechanism. 3. Category Stress Test - Run RCA specifically on "Insider Trading" vs. "Money Laundering" prompts to identify which regulatory categories are more robust.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited scope of threat model - The attack assumes API-based black-box access but does not evaluate scenarios with strict system-level constraints or authenticated financial advisory licenses.
- Judge agent reliability - The binary judgment mechanism relies on an LLM rather than human evaluation for all cases, potentially misclassifying subtle compliance violations.
- Transferability gaps - Success against commercial APIs may not generalize to models with fundamentally different alignment strategies or domain-specific financial compliance fine-tuning.

## Confidence
- **High confidence**: The overall attack effectiveness (93.18% average ASR) and comparative advantage over baselines are well-supported by experimental data.
- **Medium confidence**: The mechanism explanations are plausible and consistent with results but rely on indirect evidence or assumptions about classifier internals.
- **Low confidence**: The generalizability of RCA to all financial risk categories and its performance against models with different alignment regimes remains uncertain.

## Next Checks
1. **Judge accuracy validation**: Conduct a human evaluation study on a random sample of RCA-generated responses to quantify the true positive and false negative rates of the binary judge mechanism.

2. **System prompt constraint test**: Evaluate RCA's effectiveness against models known to have strict system-level financial advisory restrictions or disclaimers, to test the "break condition" hypothesis.

3. **Category-specific robustness analysis**: Perform a deeper failure analysis on the lower-performing categories (e.g., insider trading) to identify if specific prompt structures or regulatory contexts are inherently more defensible.