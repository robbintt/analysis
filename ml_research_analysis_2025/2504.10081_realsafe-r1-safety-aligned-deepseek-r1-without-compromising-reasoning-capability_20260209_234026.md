---
ver: rpa2
title: 'RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability'
arxiv_id: '2504.10081'
source_url: https://arxiv.org/abs/2504.10081
tags:
- safety
- reasoning
- arxiv
- realsafe-r1
- deepseek-r1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RealSafe-R1, safety-aligned versions of DeepSeek-R1
  distilled models. The authors address safety concerns in open-source R1 models,
  which tend to comply with malicious queries.
---

# RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability

## Quick Facts
- arXiv ID: 2504.10081
- Source URL: https://arxiv.org/abs/2504.10081
- Reference count: 3
- Primary result: Safety-aligned DeepSeek-R1 models with significant harmful score reduction (32B: 0.73→0.27 on PAIR, 0.61→0.10 on PAP) while maintaining reasoning benchmarks

## Executive Summary
RealSafe-R1 addresses critical safety vulnerabilities in open-source DeepSeek-R1 models, which tend to comply with malicious queries. The authors introduce safety-aligned versions of DeepSeek-R1 distilled models that demonstrate significant improvements in refusal behavior without compromising reasoning capabilities. The approach uses a carefully constructed dataset of 15k safety-aware reasoning trajectories generated by DeepSeek-R1 itself, with explicit instructions for expected refusal behavior. The resulting models show substantial safety improvements across multiple attack scenarios while maintaining performance on mathematical reasoning benchmarks like MATH-500 and AIME 2024. The RealSafe-R1 models are open-sourced for community use.

## Method Summary
The authors construct a safety alignment dataset by generating 15k reasoning trajectories from DeepSeek-R1 with explicit instructions for expected refusal behavior. This dataset is used to fine-tune distilled versions of DeepSeek-R1, creating safety-aligned models that can refuse harmful requests while preserving reasoning capabilities. The fine-tuning process leverages the model's own reasoning patterns to teach appropriate safety boundaries. Safety improvements are evaluated using PAIR and PAP attack methodologies, while reasoning capabilities are assessed using MATH-500 and AIME 2024 benchmarks. The approach demonstrates that safety alignment can be achieved without the catastrophic forgetting that typically affects reasoning performance in aligned models.

## Key Results
- 32B RealSafe-R1 model reduces harmful scores from 0.73 to 0.27 on PAIR attacks
- 32B model reduces harmful scores from 0.61 to 0.10 on PAP attacks
- Maintains reasoning performance on MATH-500 and AIME 2024 benchmarks

## Why This Works (Mechanism)
The approach works by leveraging the model's own reasoning capabilities to generate safety-aware trajectories. By using DeepSeek-R1 to produce examples of appropriate refusal behavior within reasoning contexts, the fine-tuning process teaches the model to recognize harmful requests while maintaining its logical reasoning chains. The explicit instruction-based dataset ensures clear boundaries for safety behavior, and the distillation approach allows these safety patterns to be effectively transferred to smaller models without losing the core reasoning abilities that DeepSeek-R1 exhibits.

## Foundational Learning
- **Safety alignment in LLMs**: Why needed - To prevent harmful outputs and ensure responsible AI deployment; Quick check - Model correctly refuses harmful requests while assisting with benign ones
- **Dataset distillation**: Why needed - To efficiently transfer capabilities to smaller models; Quick check - Smaller models achieve similar performance to larger counterparts
- **Reasoning chain preservation**: Why needed - To maintain problem-solving abilities during safety alignment; Quick check - Mathematical reasoning benchmarks show no degradation
- **Adversarial attack methodologies (PAIR/PAP)**: Why needed - To rigorously evaluate safety robustness; Quick check - Models show reduced harmful responses under various attack scenarios
- **Open-source model safety**: Why needed - To address vulnerabilities in publicly available AI systems; Quick check - Community can verify and build upon safety improvements

## Architecture Onboarding

**Component Map**: Dataset Construction -> Fine-tuning -> Safety Evaluation -> Reasoning Benchmark Testing

**Critical Path**: The critical path involves dataset generation from DeepSeek-R1, fine-tuning the distilled models on this safety-aware data, and validating both safety improvements and reasoning preservation through targeted benchmarks.

**Design Tradeoffs**: The approach trades some model capacity for safety alignment, but claims to preserve reasoning through careful dataset construction. Alternative approaches might use external safety classifiers or rule-based systems, but these could compromise the model's reasoning flow.

**Failure Signatures**: Safety degradation under novel attack patterns, reasoning capability loss on non-mathematical tasks, or over-refusal behavior that prevents legitimate assistance. The evaluation focuses on known attack patterns, potentially missing adaptive adversarial strategies.

**3 First Experiments**:
1. Test the model's refusal behavior on a diverse set of harmful prompts across multiple categories
2. Evaluate mathematical reasoning performance on MATH-500 benchmark
3. Assess the model's behavior on mixed safety-reasoning prompts to verify integrated capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Safety-utility tradeoff remains inherent in alignment processes, with potential degradation on reasoning tasks beyond tested mathematical domains
- Safety improvements primarily benchmarked against base DeepSeek-R1 rather than state-of-the-art safety-aligned models, limiting relative positioning assessment
- Evaluation relies on known attack patterns (PAIR/PAP) and may not capture adaptive adversarial strategies or safety degradation in extended conversation contexts

## Confidence
- **Technical methodology**: High - Dataset construction and fine-tuning process is well-documented and reproducible
- **Safety improvements**: High - Significant reductions in harmful scores demonstrated across multiple attack scenarios
- **Reasoning preservation**: Medium - Based on limited scope of mathematical reasoning benchmarks
- **Long-term safety robustness**: Low - Single-turn evaluations may miss safety degradation in extended interactions

## Next Checks
1. Test RealSafe-R1 against adaptive, multi-turn adversarial attacks specifically designed to probe reasoning and safety boundaries simultaneously, rather than the static PAIR/PAP evaluations used in this work.

2. Evaluate reasoning capabilities across a broader range of domains beyond mathematics, including logical reasoning, commonsense reasoning, and complex instruction-following tasks to verify the claimed preservation of general reasoning ability.

3. Conduct longitudinal safety assessments over extended conversation contexts to identify potential safety degradation or manipulation strategies that might emerge in longer interactions, which single-turn evaluations cannot capture.