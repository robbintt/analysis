---
ver: rpa2
title: Benchmarking Machine Learning Models for Fault Classification and Localization
  in Power System Protection
arxiv_id: '2510.00831'
source_url: https://arxiv.org/abs/2510.00831
tags:
- fault
- protection
- power
- system
- windows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks classical machine learning models for fault
  classification and localization in power system protection. Using voltage and current
  waveforms from electromagnetic transient simulations, models were evaluated across
  sliding window lengths (10-50 ms) under realistic real-time constraints.
---

# Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection

## Quick Facts
- arXiv ID: 2510.00831
- Source URL: https://arxiv.org/abs/2510.00831
- Reference count: 0
- Primary result: Best FC model achieved F1 = 0.992±0.001; best FL model achieved R² = 0.806±0.008

## Executive Summary
This paper benchmarks classical machine learning models for fault classification (FC) and localization (FL) in power systems using voltage and current waveforms from EMT simulations. The study evaluates models across sliding window lengths (10-50 ms) under realistic real-time constraints. Results show FC is substantially easier than FL, with the best FC model (MLP) achieving near-perfect accuracy while FL plateaus at lower performance. The work establishes a baseline for ML-based power system protection and identifies the need for physics-informed approaches for localization tasks.

## Method Summary
The study uses EMT simulations in DIgSILENT PowerFactory to generate 9023 episodes of 1-second three-phase voltage and current waveforms at 6400 Hz sampling rate. Data is cropped to ±80ms around fault onset and segmented using sliding windows (10-50ms lengths, 5ms step). The dataset includes a Double Line topology with domain randomization for line lengths, load conditions, and fault parameters. Classical ML models from scikit-learn are evaluated using 5-fold cross-validation with standardized features. FC is treated as multi-class classification (11 classes) and FL as regression predicting fault location percentage. Models are assessed on macro-F1 (FC) and R²/MAE/RMSE (FL) metrics, with runtime measured in milliseconds.

## Key Results
- FC achieves near-perfect accuracy with F1 = 0.992±0.001 using MLP, demonstrating the task's relative simplicity
- FL plateaus at R² = 0.806±0.008 with MLP, significantly lower than FC performance despite longer window lengths
- Simpler models (linear regression, KNN) completely fail on FL while achieving reasonable FC performance
- FC performance improves with longer windows (10→50ms), while FL accuracy is largely insensitive to window length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Longer temporal windows improve Fault Classification (FC) performance.
- Mechanism: Longer windows (e.g., 50 ms vs. 10 ms) capture more of the pre-fault and post-fault transient dynamics, providing richer context to distinguish between fault types. This increases the share of fault-containing segments (3.2% to 39.1%), making the decision boundary clearer for the classifier.
- Core assumption: Discriminative features for fault types manifest over longer durations and are not instantaneous.
- Evidence anchors: [abstract] "Results show fault classification is substantially easier than localization... While FC benefits from longer windows..." [section] "Increasing the window length reduces the overall number of generated windows by about 25.8% while raising the share of fault-containing segments from 3.2% to 39.1%..."
- Break condition: Would break if discriminative features were purely instantaneous or very short-lived.

### Mechanism 2
- Claim: Fault Localization (FL) requires higher model capacity than Fault Classification (FC).
- Mechanism: FC is discrete multi-class classification, while FL is continuous regression requiring complex, likely non-linear mapping from raw waveforms to distance. High-capacity models like MLP can learn this complex relationship while simpler models cannot.
- Core assumption: The relationship between raw V/I signals and fault distance is highly non-linear and complex.
- Evidence anchors: [abstract] "...with simpler models failing for the latter task." [section] "Overall, these findings demonstrate that FL is substantially more complex than FC, requiring models with higher capacity..."
- Break condition: Would break if the V/I-to-distance relationship was simple and linearly separable.

### Mechanism 3
- Claim: Raw V/I signals are sufficient for high-performance FC but not for FL without physics-informed data.
- Mechanism: FC achieves near-perfect accuracy (F1 = 0.992) using only raw V/I waveforms, but FL plateaus at R² = 0.806. The paper concludes that raw signals alone don't contain enough information to precisely determine location without grid parameters like impedance or topology.
- Core assumption: Raw waveforms contain enough information to identify fault types but not precise locations without physical topology knowledge.
- Evidence anchors: [abstract] "...highlighting the need for physics-informed approaches beyond raw signal processing." [section] "...exposing the physical limits of relying solely on raw V/I signals without incorporating grid parameters..."
- Break condition: Would break if advanced models could implicitly learn the distance relationship from raw signals alone.

## Foundational Learning

- **Concept: Electromagnetic Transient (EMT) Simulation**
  - Why needed: Source of dataset; captures high-frequency time-domain dynamics of faults
  - Quick check: What is the key advantage of EMT simulation over steady-state analysis for studying power system faults?

- **Concept: Sliding Window Segmentation**
  - Why needed: Core preprocessing technique; results are framed around window length tradeoffs
  - Quick check: How does increasing window length from 10 ms to 50 ms affect the number of samples and proportion containing faults?

- **Concept: F1 Score (Macro-Averaged) vs. R² (Coefficient of Determination)**
  - Why needed: Primary evaluation metrics; FC is classification (F1), FL is regression (R²)
  - Quick check: If a FL model has R² of 0.806, what does that tell you compared to R² of 1.0?

## Architecture Onboarding

- **Component map:** DIgSILENT PowerFactory -> Preprocessing Pipeline (Crop → Sliding Window → Labeling → Standardization) -> Model Zoo (FC & FL models) -> Training & Evaluation (5-fold CV, Metrics & Runtime)

- **Critical path:** Data Generation (EMT Simulation) -> Preprocessing (Windowing, Labeling) -> Model Training (FC & FL) -> Evaluation (Performance Metrics & Runtime Analysis) -> Conclusion (Task Difficulty & Physics Limits)

- **Design tradeoffs:**
  1. Window Length: Shorter windows provide more samples but lower accuracy; longer windows provide fewer samples but higher fault-density and better FC accuracy, while FL plateaus
  2. Model Complexity vs. Runtime: High-capacity models provide best accuracy but are significantly slower (2+ orders of magnitude); simple models are fast but fail on FL
  3. Task Formulation: Treating FC as classification and FL as regression works well, but regression for FL is fundamentally harder due to lack of explicit physics in raw data

- **Failure signatures:**
  - FC Failure: Low F1 score, especially for shorter window lengths; simpler models failing to generalize
  - FL Failure: Low or negative R² score; tree ensembles plateauing; simpler models completely failing (near-zero or negative R²)
  - General Failure: Model runtime exceeding practical limits for real-time protection

- **First 3 experiments:**
  1. Reproduce the FC Window Sensitivity: Train MLP on FC task; evaluate F1 for each window length (10, 20, 30, 40, 50 ms) to confirm performance increases with window length
  2. Reproduce the FL Plateau: Train MLP on FL task; evaluate R² for each window length to confirm performance is largely insensitive to window length
  3. Investigate a Simpler Model on FL: Train Linear Regression or KNN on FL task with 50ms window; confirm failure to generalize (near-zero or negative R²)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can physics-informed approaches incorporating grid parameters (e.g., impedance) overcome the accuracy plateau observed in fault localization using raw signal processing?
- Basis: [explicit] The authors conclude that the localization plateau highlights "the need for physics-informed approaches beyond raw signal processing" and propose integrating impedance or line length.
- Why unresolved: The current study evaluated models solely on raw voltage and current waveforms without explicit physical constraints or parameters.
- What evidence would resolve it: A benchmark showing fault localization R² scores significantly exceeding 0.806 on the same dataset using physics-guided features or loss functions.

### Open Question 2
- Question: Do deep learning architectures offer superior accuracy or efficiency compared to the top-performing classical MLP models for these tasks?
- Basis: [explicit] The conclusion states, "Future work will investigate deep learning architectures" to build upon the classical ML baseline established here.
- Why unresolved: This work restricted its scope to classical machine learning models, excluding modern deep learning techniques.
- What evidence would resolve it: A comparative analysis of deep learning models (e.g., CNNs, Transformers) against the MLP baseline using the established EMT dataset.

### Open Question 3
- Question: To what extent do models trained on the "Double Line" topology transfer to diverse grid structures or unseen operating conditions?
- Basis: [explicit] The paper identifies "assessing transferability across topologies, operating conditions, and fault scenarios" as a crucial step for demonstrating robustness.
- Why unresolved: The evaluation relied on a specific benchmark topology and domain randomization, leaving cross-topology generalization untested.
- What evidence would resolve it: Performance metrics (F1, R²) of trained models applied to significantly different grid topologies without retraining.

## Limitations
- Unknown MLP architecture specifications and domain randomization ranges affect reproducibility
- Simpler models' failures on FL warrant investigation into whether model capacity or feature representation is the limiting factor
- Physics-informed approaches were logically concluded but not empirically tested with hybrid models

## Confidence

**High:** FC performance claims - Well-validated with multiple models and clear trends across window lengths

**Medium:** FL performance claims - Strong evidence but dependent on unexplained MLP architecture choices

**Medium:** Physics-informed approach necessity - Logical conclusion from results but not empirically tested with hybrid models

## Next Checks

1. Test whether adding physics-based features (line impedance, topology) to MLP improves FL beyond 0.806 R²

2. Compare MLP performance against deep learning models (CNN, Transformer) on the same dataset to assess if classical ML has reached performance limits

3. Evaluate model robustness to noise and measurement errors that would occur in real-world deployment