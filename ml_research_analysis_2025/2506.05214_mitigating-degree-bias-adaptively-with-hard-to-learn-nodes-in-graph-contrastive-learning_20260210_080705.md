---
ver: rpa2
title: Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive
  Learning
arxiv_id: '2506.05214'
source_url: https://arxiv.org/abs/2506.05214
tags:
- nodes
- degree
- graph
- learning
- sharp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses degree bias in Graph Neural Networks (GNNs),
  where nodes with different degrees exhibit varying prediction performance, with
  low-degree nodes often performing worse due to insufficient and noisy information
  from their neighbors. To mitigate this bias, the authors propose the Hardness Adaptive
  Reweighted (HAR) contrastive loss, which introduces more positive pairs by leveraging
  node labels and assigns adaptive weights to positive and negative pairs based on
  their learning hardness.
---

# Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning

## Quick Facts
- arXiv ID: 2506.05214
- Source URL: https://arxiv.org/abs/2506.05214
- Authors: Jingyu Hu; Hongbo Bo; Jun Hong; Xiaowei Liu; Weiru Liu
- Reference count: 40
- Primary result: SHARP framework reduces degree bias in GNNs by focusing on hard-to-learn nodes, outperforming baselines on 4 datasets

## Executive Summary
This paper addresses degree bias in Graph Neural Networks (GNNs), where low-degree nodes suffer from insufficient and noisy neighborhood information, leading to worse prediction performance compared to high-degree nodes. The authors propose the Hardness Adaptive Reweighted (HAR) contrastive loss, which leverages node labels to add more positive pairs and assigns adaptive weights to positive and negative pairs based on their learning hardness. This ensures the model focuses on hard-to-learn nodes, providing more accurate information to low-degree nodes. The method is extended to semi-supervised settings through the SHARP framework, which incorporates pseudo-labels for unlabeled data.

## Method Summary
The paper introduces HAR, a contrastive loss that mitigates degree bias by adding more positive pairs using node labels and adaptively weighting positive and negative pairs based on learning hardness. For positive pairs, it encourages the model to focus on hard-to-learn samples while considering information from easy ones. For negative pairs, it assigns higher weights to hard negatives (nodes from different classes with high feature similarity). The SHARP framework extends HAR to semi-supervised learning by first pre-training on labeled data, then generating pseudo-labels for unlabeled data and fine-tuning the model using these pseudo-labels.

## Key Results
- SHARP outperforms baseline methods (GCN, GRACE, SCL, Debias) on 4 datasets (Cora, CiteSeer, PubMed, Wiki-CS) in both fully supervised and semi-supervised settings
- SHARP significantly improves performance for low-degree nodes while maintaining competitive overall results, effectively reducing degree bias
- Theoretical analysis confirms that SHARP's misclassification risk remains bounded even under worst-case hyperparameter settings
- SHARP achieves better performance at both global and degree levels in 14 out of 16 tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted positive pairs increase information flow to low-degree nodes, improving their representation quality under limited neighborhood access.
- Mechanism: Standard GCL uses a single positive pair per node, restricting information gain for nodes with sparse neighborhoods. HAR leverages node labels to include all nodes of the same class as positive pairs, effectively multiplying the positive signal and providing low-degree nodes with more reference points to learn from.
- Core assumption: Labels provide a reliable signal for identifying semantically similar nodes that can serve as high-quality positive examples.
- Evidence anchors: [abstract] "It adds more positive pairs by leveraging node labels and adaptively weights positive and negative pairs based on their learning hardness." [Section 1, Page 2] "SCL treats all nodes of the same class as positives... However, different nodes have different levels of learning hardness... Assigning equal weights to noisy nodes may prevent the model from capturing more important information."

### Mechanism 2
- Claim: Adaptive weighting of negative pairs focuses model capacity on hard-to-discriminate examples, leading to more robust decision boundaries.
- Mechanism: Standard GCL treats all negative pairs equally, but many are trivially easy to distinguish. HAR assigns higher weights to "hard negatives" – nodes from different classes that have high feature similarity, forcing the model to learn more nuanced and precise decision boundaries.
- Core assumption: The "hard-to-learn" definition correctly identifies the most informative negative examples for refining the model's representations.
- Evidence anchors: [Section 3.2, Page 5] "For negative samples, we refer to CL in debiasing (Chuang et al., 2020) and reweighting (Kalantidis et al., 2020) strategies which assigns higher weights to v−hard and lower weights to v−easy." [Section 4.2.2, Page 8] "The proportion of low-degree nodes in the hard-to-learn nodes distribution is higher than the base distribution."

### Mechanism 3
- Claim: The two-step SHARP framework with pseudo-labeling enables the HAR loss to be applied in semi-supervised settings, extending its bias mitigation capabilities.
- Mechanism: HAR requires labels to define positive/negative pairs. SHARP first pre-trains a model on available labeled data using HAR, then uses this model to predict pseudo-labels for unlabeled nodes, allowing the HAR loss to be applied to the entire dataset.
- Core assumption: The pre-trained model generates pseudo-labels with sufficient accuracy so that using them in the HAR loss adds more signal than noise.
- Evidence anchors: [Section 3.3, Page 6] "Our proposed Semi-supervised Hardness Adaptive Reweighted with Pseudo-labels (SHARP) consists of two steps. First, the model is pre-trained using the labelled training data. Second, the pre-trained model generates pseudo-labels for the unlabelled training data." [Section 4.2.1, Page 7] "SHARP outperforms the baseline in 14 out of 16 tests... [and] achieves better performance at both global and degree levels."

## Foundational Learning

- **Graph Contrastive Learning (GCL)**: Understanding the standard GCL pipeline (data augmentation, encoder, projection head, contrastive loss) is essential since SHARP is a modification of this pipeline.
  - Quick check: Can you describe the role of the projection head and the standard InfoNCE loss used in a typical GCL setup?

- **Message Passing in GNNs and Degree Bias**: The problem SHARP aims to solve (degree bias) is a direct consequence of how GNNs aggregate information from neighbors.
  - Quick check: Why does a GCN node with only one neighbor potentially learn a worse representation than a node with 100 neighbors?

- **Pseudo-labeling for Semi-Supervised Learning**: The SHARP framework extends HAR to semi-supervised learning using pseudo-labels.
  - Quick check: What is a key risk when using pseudo-labels generated by a model to fine-tune that same model?

## Architecture Onboarding

- **Component map**:
  - Graph Data (G = {V, E, X, Y}) -> GCL Encoder (H(·)) -> Projection Head (Z(·)) -> HAR Loss -> Two-Step Trainer (SHARP)

- **Critical path**:
  1. Split G into GL (labeled) and GU (unlabeled)
  2. Pre-train on GL using HAR loss with true labels
  3. Generate pseudo-labels for GU using pre-trained model
  4. Fine-tune on combined GL and pseudo-labeled GU using HAR loss
  5. Evaluate final embeddings on G_test for downstream tasks

- **Design tradeoffs**:
  - Label-based Positives vs. Augmentation-only: Using labels increases positive pairs but removes purely self-supervised nature and requires labeled data
  - Pseudo-labeling vs. Purely Supervised: Pseudo-labeling allows scaling to larger datasets but introduces noise from incorrect labels
  - Hard Negative Weighting (β): Focusing on hard negatives improves boundaries but could make the model too focused on difficult distinctions if over-emphasized

- **Failure signatures**:
  - No convergence or exploding loss: Check temperature τ and scaling factors α and β
  - Worse performance on low-degree nodes: Check if hard negative identification is working and verify pseudo-label accuracy
  - Performance collapse with high r (fewer labels): Expected limitation due to weak pre-trained model

- **First 3 experiments**:
  1. Baseline Comparison (Global & Degree-Level): Run SHARP vs. baselines on all four datasets with fixed r=0 and r=0.3, reporting macro F1-score and F1-score stratified by node degree
  2. Ablation on HAR Components: Compare HAR (full), HAR with equal weighting (α=1, β=1), and HAR without label-based positives to validate adaptive weighting and label-based positives
  3. Hyperparameter Sensitivity: Systematically vary unlabeled data ratio r and reweighting parameters α and β on Cora to understand stability and failure points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of SHARP be improved when the proportion of labelled data is particularly low (e.g., less than 30%)?
- Basis in paper: [explicit] Conclusion section states: "One limitation of the current work is that when the proportion of labelled data is particularly low (e.g., less than 30%), it can restrict the improvement of SHARP."
- Why unresolved: SHARP relies on sufficient labelled data to generate meaningful pseudo-labels; with very limited labels, pseudo-label quality degrades, limiting overall effectiveness.
- What evidence would resolve it: Experiments showing consistent improvement with very low label proportions (10-30%), or a modified approach incorporating external knowledge or pre-trained models.

### Open Question 2
- Question: Can pre-trained models be leveraged to add richer semantic information to low-degree nodes in the HAR/SHARP framework?
- Basis in paper: [explicit] Conclusion section explicitly states as future work: "Future work could further explore using pre-trained models to add richer semantic information to low-degree nodes (Zhao et al., 2022; Yan et al., 2023)..."
- Why unresolved: Low-degree nodes suffer from insufficient neighborhood information; pre-trained models could provide complementary semantic features, but integration with adaptive reweighting in HAR loss remains unexplored.
- What evidence would resolve it: An augmented SHARP framework incorporating text-attributed or feature-rich pre-trained embeddings, with experiments demonstrating improved low-degree node performance.

### Open Question 3
- Question: How does the HAR loss extend to heterogeneous graphs with multiple node and edge types?
- Basis in paper: [explicit] Conclusion section mentions: "...as well as extend this approach to more diverse scenarios such as heterogeneous graphs." Additionally, Related Work references degree bias mitigation on heterogeneous graphs.
- Why unresolved: HAR assumes homogeneous graphs with a single node type; heterogeneous graphs introduce varied degree distributions and label availability across node types, complicating positive/negative pair definition.
- What evidence would resolve it: A modified HAR formulation accounting for node/edge type heterogeneity, evaluated on heterogeneous graph benchmarks showing degree bias reduction across diverse node categories.

### Open Question 4
- Question: What is the impact of different fine-tuning strategies on SHARP's stability and performance in semi-supervised learning?
- Basis in paper: [explicit] Section 4.2.1 states: "We focus on how GCL settings benefits graph structural fairness in this paper, and leave the discussion of different fine-tuning strategies in semi-supervised learning for future work."
- Why unresolved: The simple two-step SHARP framework uses a fixed early stopping criterion, which may not be optimal; performance fluctuations suggest sensitivity to fine-tuning dynamics.
- What evidence would resolve it: A comparative study of fine-tuning strategies with SHARP, showing reduced variance and consistent gains across datasets, particularly for low-degree nodes.

## Limitations
- Performance heavily depends on quality and quantity of node labels for defining positive pairs
- Degraded performance when labeled data is very limited (<30%), indicating reliance on sufficient initial supervision
- Specific definition of "hard-to-learn" nodes may not always capture the most beneficial learning signals in practice

## Confidence

- **High**: The core premise that degree bias exists in GNNs and that low-degree nodes suffer from insufficient and noisy information is well-established. The theoretical analysis confirming SHARP's misclassification risk remains bounded under worst-case hyperparameter settings is also highly reliable.
- **Medium**: The empirical results demonstrating SHARP's effectiveness in reducing degree bias and outperforming baselines on four datasets are strong, but the method's dependence on label quality and sufficient labeled data introduces uncertainty in its robustness across diverse real-world scenarios.
- **Low**: The specific definition and identification of "hard-to-learn" nodes as the most informative examples is theoretically sound but may not always capture the most beneficial learning signals in practice, depending on the feature space and initial encoder quality.

## Next Checks

1. **Label Quality Sensitivity**: Conduct experiments with varying levels of label noise (e.g., 0%, 10%, 20%, 30%) on a single dataset to quantify how label corruption impacts SHARP's ability to mitigate degree bias and its overall performance.

2. **Scalability to Label Scarcity**: Systematically evaluate SHARP with very low labeled data ratios (e.g., r = 0.1, 0.05, 0.01) across multiple datasets to identify the absolute minimum label requirement for the method to provide a benefit over standard supervised baselines.

3. **Hard Negative Identification Robustness**: Design a controlled experiment where the initial encoder is deliberately weakened (e.g., by reducing layers or using random features) to test if SHARP's hard negative weighting mechanism still identifies meaningful examples or if it fails when the base similarity metric is unreliable.