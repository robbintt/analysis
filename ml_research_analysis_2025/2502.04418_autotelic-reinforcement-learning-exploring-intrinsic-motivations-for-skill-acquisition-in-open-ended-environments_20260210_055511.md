---
ver: rpa2
title: 'Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill
  Acquisition in Open-Ended Environments'
arxiv_id: '2502.04418'
source_url: https://arxiv.org/abs/2502.04418
tags:
- agents
- learning
- where
- builder
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Autotelic Reinforcement Learning (ARL), a
  framework for autonomous skill acquisition through intrinsic motivation in open-ended
  environments. The core method involves agents autonomously generating and pursuing
  self-defined goals using a reward-free Markov Decision Process (MDP), guided by
  intrinsic motivations.
---

# Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments

## Quick Facts
- arXiv ID: 2502.04418
- Source URL: https://arxiv.org/abs/2502.04418
- Reference count: 23
- Primary result: Agents achieve up to 99% success in goal-oriented tasks using self-defined goals and intrinsic motivation

## Executive Summary
This paper introduces Autotelic Reinforcement Learning (ARL), a framework for autonomous skill acquisition through intrinsic motivation in open-ended environments. The core method involves agents autonomously generating and pursuing self-defined goals using a reward-free Markov Decision Process (MDP), guided by intrinsic motivations. The approach emphasizes the role of knowledge-based and competence-based motivations in skill development. Experimental results show that agents can effectively explore, generalize, and adapt to dynamic environments, achieving success rates of up to 99% in goal-oriented tasks. The framework also demonstrates robustness through perturbation tests, with agents maintaining performance under varying conditions. ARL advances autonomous learning by enabling agents to self-organize goals and skills without external rewards, mimicking human-like adaptability and creativity.

## Method Summary
The Autotelic Reinforcement Learning framework consists of two main components: the CURVES contrastive representation learning algorithm for communication and the ABIG (Architect-Builder Iterated Guiding) system for goal-directed interaction. CURVES uses two CNN encoders to map referents and expressions to a shared latent space, where agents learn to communicate through contrastive learning with cosine similarity energy functions. The ABIG system organizes interactions between an Architect (who knows goals but cannot act) and a Builder (who can act but lacks goal knowledge) into structured frames. The Architect learns a model of the Builder's policy, then uses this model with MCTS to generate messages that guide the Builder toward goals. The Builder updates its policy through self-imitation learning on successful guided trajectories.

## Key Results
- Agents achieve up to 99% success rates in goal-oriented tasks using self-defined goals
- Framework demonstrates robust performance under perturbation tests with maintained accuracy
- Agents successfully generalize and adapt to dynamic environments without external rewards
- CURVES contrastive learning enables interpretable and coherent communication protocols

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Representation Learning for Referential Grounding
The contrastive learning framework enables agents to develop a shared latent space where referents and expressions are aligned, facilitating communication without pre-defined semantics. Two agents use distinct CNN encoders to map referents and expressions to a shared d-dimensional latent space, learning an energy landscape through maximizing similarity for positive pairs and minimizing it for negative pairs. The Speaker generates expressions by optimizing for high energy with the target referent, creating a learned semantic alignment through iterative interaction.

### Mechanism 2: Goal Self-Organization via Intrinsic Motivation
Agents autonomously acquire skills and explore their environment in the absence of external rewards by generating and pursuing self-defined goals, driven by intrinsic motivations like curiosity or competence. The learning problem is framed as a reward-free MDP where agents generate goals from a goal space, attempt to achieve them using goal-conditioned policies, and update based on internal reward functions. This Intrinsically Motivated Goal Exploration Process allows agents to build a repertoire of skills through self-directed learning.

### Mechanism 3: Emergent Communication via Iterated Guiding and Shared Intentionality
A communication protocol emerges between two agents with asymmetric information and capabilities by organizing interactions into structured frames based on shared intentionality. In alternating modeling and guiding frames, the Architect learns a model of the Builder's policy, then uses this model with MCTS to generate steering messages. The Builder learns through self-imitation on successful trajectories, developing a reusable communication protocol that generalizes to unseen tasks.

## Foundational Learning

Concept: Contrastive Learning (e.g., SimCLR, CLIP)
Why needed here: This is the core of the CURVES algorithm for mapping referents and expressions to a shared embedding space
Quick check question: Can you explain how a contrastive loss function (like InfoNCE) maximizes agreement between a positive pair (referent, expression) while minimizing it for all negative pairs in a batch?

Concept: Goal-Conditioned Reinforcement Learning (GCRL)
Why needed here: Autotelic RL is fundamentally about agents learning to achieve self-generated goals
Quick check question: How does a hindsight experience replay (HER) mechanism allow an agent to learn from failed trajectories?

Concept: Imitation Learning / Behavioral Cloning
Why needed here: The ABIG algorithm uses Behavioral Cloning for both the Architect (to model the Builder) and the Builder (to learn from guided trajectories)
Quick check question: What is the primary cause of distributional shift in Behavioral Cloning, and how does iterative expert query (like DAGGER) attempt to mitigate it?

## Architecture Onboarding

Component map: CNN encoders (referents) -> Shared latent space -> CNN encoders (expressions) -> Contrastive learning -> Expression generation; Architect modeling -> MCTS -> Message generation -> Builder self-imitation -> Communication protocol

Critical path: For the GREG game, the critical path is training the CURVES contrastive encoders. For the ABP, the critical path is the iterative loop of the Architect modeling the Builder and then guiding it, which allows the communication protocol to emerge.

Design tradeoffs: The paper notes a tradeoff in CURVES expression generation between "distinct" (maximizing similarity) and "discriminative" (minimizing uncertainty) approaches. Results show similar performance, suggesting the extra computation for discriminative expressions may not be justified. For ABIG, the tradeoff is between the stability of fixed interaction frames and their data inefficiency.

Failure signatures: 1) In CURVES, failure is indicated by high training success rates but near-random performance on compositional generalization tests, suggesting overfitting to training referents. 2) In ABIG, failure is signaled if the Builder's performance does not improve over the "ABIG-no-intent" baseline, indicating the guidance signal is not being learned or the communication protocol is not forming.

First 3 experiments:
1. GREG Baseline Reproduction: Replicate the one-hot referent experiment. Train the CURVES encoders and measure the communicative success rate. Verify it reaches ~99% as stated in Table 1.
2. Compositional Generalization Test: Take the trained model from experiment 1 and evaluate it on the compositional referent sets (Table 1). Confirm the performance drop from one-hot to visual-unshared settings.
3. ABIG Ablation: Implement a simplified ABIG in the BuildWorld environment. Compare the full ABIG algorithm against the "ABIG-no-intent" control to quantify the contribution of the guiding frame.

## Open Questions the Paper Calls Out

Open Question 1: How can builder agents internalize guidance to achieve autonomy without continuous architect intervention?
Basis in paper: The Conclusion states the builder "remains dependent on the architect's guidance" and suggests a "Vygotskian approach" for internalization.
Why unresolved: Current ABIG methods require active messages at every step; the mechanism for a builder to act independently after learning is undefined.
Evidence to resolve: Successful task completion by a builder agent in evaluation phases where the communication channel is disabled or the architect is absent.

Open Question 2: Can the stationarity assumption be relaxed to improve data efficiency in emergent communication frameworks?
Basis in paper: The Conclusion notes that training in a "fixed interaction framework" is "data-inefficient" and suggests future work should allow learning from "dynamic, non-stationary data buffers."
Why unresolved: The current ABIG algorithm relies on structured interaction frames to stabilize learning, which consumes excessive data.
Evidence to resolve: An updated algorithm demonstrating stable convergence and high success rates while learning from a continuous, unstructured stream of past behaviors.

Open Question 3: To what extent do specific sensorimotor constraints facilitate the emergence of compositional graphical languages?
Basis in paper: The Discussion highlights the need for an "examination of the impact of the sensory-motor constraints on the topology of graphical signs."
Why unresolved: While agents achieve high communicative success, the generated articulations lack clear compositional structures; the link between physical constraints and language structure remains unexplored.
Evidence to resolve: Empirical analysis showing a correlation between specific motor primitive constraints and higher topographic similarity scores in the generated signals.

## Limitations

- Empirical validation is primarily internal with limited external corpus support for core mechanisms
- Insufficient detail on CNN encoder architectures and training hyperparameters for faithful reproduction
- Reliance on synthetic datasets (MNIST-derived referents) that may not capture real-world communication complexity
- Focus on small-scale environments without validation in larger, more complex domains

## Confidence

High Confidence: The fundamental framework of Autotelic RL for autonomous skill acquisition via intrinsic motivation is well-grounded in existing literature. The ABIG algorithm's iterative structure is logically sound.

Medium Confidence: The CURVES contrastive learning mechanism for referential grounding is internally coherent but lacks external validation. Specific performance metrics are stated but not fully contextualized against baselines.

Low Confidence: Claims about emergent communication protocols being "interpretable and coherent" are supported by Hausdorff distance metrics but require more rigorous qualitative analysis of actual learned expressions/referents.

## Next Checks

1. Compositional Generalization Stress Test: Extend the compositional evaluation to include more complex referent compositions (e.g., |S|=4) and measure performance degradation. Track whether the model's failure is due to limited encoder capacity or insufficient training data.

2. Architecture Ablation for CURVES: Implement and compare alternative encoder architectures (e.g., ResNet, Vision Transformer) and latent space dimensions (d) to identify the minimum viable configuration for successful communication.

3. Large-Scale Environment Validation: Port the Autotelic RL framework to a more complex environment (e.g., MiniGrid or DeepMind Lab) to assess scalability and robustness of goal self-organization in richer state spaces.