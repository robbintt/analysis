---
ver: rpa2
title: 'VLA-0: Building State-of-the-Art VLAs with Zero Modification'
arxiv_id: '2510.13054'
source_url: https://arxiv.org/abs/2510.13054
tags:
- action
- vla-0
- arxiv
- vlas
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing trend in Vision-Language-Action
  (VLA) model design by proposing VLA-0, which uses a base Vision-Language Model (VLM)
  to predict actions directly as text without any architectural modifications. The
  authors find that this simple approach outperforms more complex alternatives, including
  models that modify the VLM vocabulary or add specialized action heads.
---

# VLA-0: Building State-of-the-Art VLAs with Zero Modification

## Quick Facts
- arXiv ID: 2510.13054
- Source URL: https://arxiv.org/abs/2510.13054
- Reference count: 25
- VLA-0 achieves 94.7% average success rate on LIBERO, outperforming π0.5-KI, OpenVLA-OFT, and SmolVLA

## Executive Summary
This paper challenges the prevailing trend in Vision-Language-Action (VLA) model design by proposing VLA-0, which uses a base Vision-Language Model (VLM) to predict actions directly as text without any architectural modifications. The authors find that this simple approach outperforms more complex alternatives, including models that modify the VLM vocabulary or add specialized action heads. On the LIBERO benchmark, VLA-0 achieves state-of-the-art performance among models trained on the same robotic data, surpassing methods like π0.5-KI, OpenVLA-OFT, and SmolVLA, with an average success rate of 94.7%. Notably, without large-scale robotics-specific pretraining, it also outperforms models trained on extensive robotic data, including π0.5-KI, π0, GR00T-N1, and MolmoAct.

## Method Summary
VLA-0 is built on top of existing VLMs without any architectural modifications. Instead of modifying the VLM's vocabulary or adding specialized action heads, VLA-0 predicts actions directly as text tokens. The approach relies on careful training techniques, including masked action augmentation and prediction ensembling, to achieve high performance. The model was evaluated on the LIBERO benchmark and real-world manipulation tasks using the SO-100 robot, demonstrating superior performance compared to existing VLA methods.

## Key Results
- Achieves 94.7% average success rate on LIBERO benchmark
- Outperforms π0.5-KI, OpenVLA-OFT, and SmolVLA among models trained on same robotic data
- Shows 12.5-point improvement over SmolVLA on real-world SO-100 robot tasks despite lacking pretraining on large-scale SO-100 data

## Why This Works (Mechanism)
The paper demonstrates that zero-modification VLMs can directly predict text-based actions with high accuracy. The key to VLA-0's success lies in careful training techniques, including masked action augmentation and prediction ensembling, which are essential for unlocking its high performance. By avoiding architectural modifications, VLA-0 leverages the strong generalization capabilities of pre-trained VLMs while maintaining simplicity and avoiding the complexities of specialized action heads or modified vocabularies.

## Foundational Learning

1. **Vision-Language-Action (VLA) Models**
   - Why needed: VLA models bridge visual perception, language understanding, and robotic control
   - Quick check: Can the model process both image inputs and text commands to generate appropriate actions?

2. **Masked Action Augmentation**
   - Why needed: This training technique improves robustness by randomly masking action tokens during training
   - Quick check: Does the model maintain performance when portions of action sequences are missing or corrupted?

3. **Prediction Ensembling**
   - Why needed: Combining multiple model predictions improves accuracy and reduces uncertainty
   - Quick check: Does averaging predictions across multiple forward passes yield better action sequences?

4. **Zero-Modification Architecture**
   - Why needed: Avoiding architectural changes maintains VLM generalization while simplifying implementation
   - Quick check: Can the base VLM generate coherent action sequences without specialized action heads?

## Architecture Onboarding

**Component Map:** Image/Vision Input -> VLM Backbone -> Text Action Output -> Robot Controller

**Critical Path:** Visual input is encoded by the VLM, which generates text-based action sequences that are then executed by the robot controller

**Design Tradeoffs:** Zero-modification simplicity vs. potential performance gains from specialized architectures; maintains VLM generalization but may miss domain-specific optimizations

**Failure Signatures:** Poor action predictions when visual scenes are complex or when language commands are ambiguous; performance degradation on tasks requiring fine-grained control

**3 First Experiments:**
1. Test zero-modification approach on simple pick-and-place tasks with clear visual cues
2. Evaluate performance degradation when masking different percentages of action tokens
3. Compare single vs. ensemble predictions on tasks with moderate complexity

## Open Questions the Paper Calls Out
None

## Limitations

- Evaluation scope is narrow, primarily limited to LIBERO benchmark and SO-100 robot tasks
- Training methodology lacks critical implementation details for masked action augmentation and prediction ensembling
- Generalizability to diverse robotic domains, different robot morphologies, and varied environmental conditions remains untested

## Confidence

- High confidence: The core finding that zero-modification VLMs can directly predict text-based actions is technically sound and the empirical results on LIBERO and SO-100 are clearly demonstrated
- Medium confidence: The assertion that zero-modification is "superior to all existing VLA methods" extends beyond the empirical evidence and requires broader validation
- Medium confidence: The claim that VLA-0 can "outperform models trained on extensive robotic data" without large-scale pretraining needs context about specific benchmarks and tasks

## Next Checks

1. Evaluate VLA-0 on diverse robotic platforms beyond SO-100, including different robot morphologies and sensor configurations to test cross-platform generalizability

2. Conduct ablation studies isolating the impact of masked action augmentation and prediction ensembling from the zero-modification architecture to determine which components drive performance gains

3. Test VLA-0 on long-horizon, multi-step manipulation tasks requiring complex reasoning and planning to assess scalability beyond the relatively simple tasks demonstrated in current evaluations