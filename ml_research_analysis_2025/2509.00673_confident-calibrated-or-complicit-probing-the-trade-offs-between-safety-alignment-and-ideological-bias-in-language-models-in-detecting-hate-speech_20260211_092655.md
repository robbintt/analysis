---
ver: rpa2
title: 'Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety
  Alignment and Ideological Bias in Language Models in Detecting Hate Speech'
arxiv_id: '2509.00673'
source_url: https://arxiv.org/abs/2509.00673
tags:
- hate
- accuracy
- implicit
- performance
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compares how censored and uncensored Large Language Models
  detect hate speech, using a balanced dataset of explicit, implicit, and non-hateful
  social media posts. Models were tested under four political persona prompts to assess
  ideological framing effects.
---

# Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech

## Quick Facts
- arXiv ID: 2509.00673
- Source URL: https://arxiv.org/abs/2509.00673
- Reference count: 4
- Censored models significantly outperform uncensored models in hate speech classification (78.7% vs 64.1% strict accuracy)

## Executive Summary
This study compares how censored and uncensored Large Language Models detect hate speech, using a balanced dataset of explicit, implicit, and non-hateful social media posts. Models were tested under four political persona prompts to assess ideological framing effects. Censored models significantly outperformed uncensored models in strict classification accuracy (78.7% vs 64.1%), mainly due to lower misclassification rates. Uncensored models were more sensitive to persona-induced bias, while censored models were more stable but still prone to systemic overconfidence and unequal fairness across target groups. Implicit hate involving irony was especially challenging, with high misclassification.

## Method Summary
The study used the Latent Hatred dataset (3,267 samples, balanced across explicit_hate, implicit_hate, and not_hate classes) to evaluate five LLMs with varying censorship levels. Zero-shot prompting with JSON-structured output was used, testing four political persona framings (Progressive, Conservative, Libertarian, Centrist). Models were run at temperature 0.7 with single inference per sample. Evaluation included strict accuracy (refusals counted as errors), Expected Calibration Error (ECE), false positive/negative rates, and two-way ANOVA for interaction effects between censorship level and persona.

## Key Results
- Censored models achieved 78.7% strict accuracy vs 64.1% for uncensored models
- Censored models showed greater stability against persona-induced bias (4-7% variance vs 10-15% for uncensored)
- Systemic overconfidence observed with ECE of 0.094, confidence >70% on incorrect predictions
- Significant fairness disparities: 94.0% accuracy for Jewish people vs 39.9% for unspecified targets
- Irony detection accuracy only 69%, highlighting challenges with implicit hate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety alignment improves hate speech classification accuracy while simultaneously creating resistance to ideological manipulation.
- Mechanism: RLHF-style alignment appears to improve instruction-following for complex classification tasks beyond simply adding behavioral guardrails, while "locking in" an ideological baseline that resists persona-based perturbation.
- Core assumption: The observed stability reflects the alignment process itself rather than architectural or training data differences between model families.
- Evidence anchors:
  - [abstract] "censored models significantly outperform their uncensored counterparts in both accuracy and robustness, achieving 78.7% versus 64.1% strict accuracy"
  - [section 3.1] "censored models had a total error rate of only 21.4%, driven almost entirely by misclassifications (21.3%) with a negligible refusal rate (0.1%)"
  - [corpus] Weak direct evidence on RLHF mechanisms; neighboring papers focus on alignment breaking rather than alignment benefits.
- Break condition: If uncensored models from the same family with controlled alignment levels show similar accuracy improvements, the mechanism may reflect capability differences rather than alignment per se.

### Mechanism 2
- Claim: Political persona induction creates predictable directional bias in classification, but the magnitude depends on model alignment level.
- Mechanism: Prompted personas activate latent ideological representations in the model, but safety-aligned models have these representations constrained, reducing malleability.
- Core assumption: The persona prompts activate pre-existing representations rather than creating novel ideological stances.
- Evidence anchors:
  - [abstract] "safety alignment acts as a strong ideological anchor, making censored models resistant to persona-based influence, while uncensored models prove highly malleable"
  - [section 3.2, Figure 4] "The progressive persona exhibited a 'liberal bias' (a high false positive rate), while the libertarian persona showed a 'conservative bias' (a high false negative rate)"
  - [corpus] "Probing the Subtle Ideological Manipulation of Large Language Models" confirms LLM susceptibility to ideological manipulation in politically sensitive areas.
- Break condition: If non-political personas produce similar variance patterns, the mechanism may reflect general prompt sensitivity rather than ideological activation.

### Mechanism 3
- Claim: Model confidence is systematically miscalibrated, with high confidence on incorrect predictions making self-reported certainty unreliable for human-in-the-loop systems.
- Mechanism: Models lack sufficient uncertainty quantification; they produce high confidence scores for plausible-but-incorrect classifications on ambiguous content.
- Core assumption: The observed overconfidence reflects fundamental calibration limitations rather than prompt design artifacts.
- Evidence anchors:
  - [abstract] "systemic overconfidence that renders self-reported certainty unreliable"
  - [section 3.6] "mean confidence for incorrect predictions was consistently high across all classes: 71.7% for explicit_hate, 72.8% for implicit_hate, and 74.0% for not_hate"; ECE of 0.094
  - [corpus] No direct corpus support for calibration mechanisms in this context.
- Break condition: If calibration improves with different prompting strategies (e.g., chain-of-thought), the mechanism may be prompt-dependent rather than fundamental.

## Foundational Learning

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: The paper uses ECE to quantify the gap between model confidence and actual accuracy; understanding this metric is essential for interpreting reliability claims.
  - Quick check question: If a model has ECE=0.094, does this indicate better or worse calibration than a model with ECE=0.02?

- Concept: **Safety Alignment via RLHF**
  - Why needed here: The study compares censored (aligned) vs uncensored models; understanding what alignment does beyond "guardrails" explains the performance differences.
  - Quick check question: What are the two competing hypotheses for why censored models outperform uncensored ones in this study?

- Concept: **Implicit vs Explicit Hate Speech**
  - Why needed here: The paper disaggregates performance by hate type; irony (69% accuracy) vs stereotypical (83.6%) reveals different failure modes.
  - Quick check question: Which implicit hate category showed the highest misclassification rate, and what does this suggest about model limitations?

## Architecture Onboarding

- Component map:
  - Dataset loading -> Persona prompt construction -> Model inference (T=0.7) -> JSON parsing -> Classification evaluation -> Statistical analysis

- Critical path:
  1. Dataset balancing (3,267 samples across 3 classes)
  2. Persona prompt construction
  3. Model inference at T=0.7
  4. JSON parsing with refusal handling
  5. Strict accuracy computation
  6. Two-way ANOVA for interaction effects

- Design tradeoffs:
  - Single-run inference (T=0.7) prioritizes ecological validity over reproducibility
  - Binary classification task (merging explicit/implicit hate) simplifies evaluation but loses granularity
  - UGI score as censorship proxy may conflate willingness-to-answer with actual alignment differences

- Failure signatures:
  - High refusal rate (>10%) on specific target groups indicates "model avoidance bias"
  - Confidence >0.8 on misclassifications signals unreliable certainty
  - >5% accuracy variance across personas indicates malleability vulnerability

- First 3 experiments:
  1. **Baseline comparison**: Run all 5 models with neutral persona on balanced dataset; compute strict accuracy and refusal rates per model.
  2. **Persona sensitivity test**: Repeat inference with all 4 political personas; compute per-persona accuracy variance to identify high-malleability models.
  3. **Calibration audit**: Extract confidence scores from all successful classifications; compute ECE and plot confidence distributions for correct vs incorrect predictions to identify overconfidence patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does classification stability vary across multiple inference runs when using non-zero temperatures, and does this stochasticity affect the reliability of the ideological anchoring observed in censored models?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that "Future work could explore the variance in model responses across multiple runs," as the study relied on single runs at T=0.7.
- Why unresolved: The current results reflect a single stochastic pass per sample; it remains unknown if the "strict accuracy" and persona malleability are consistent distributions or outliers of the model's probabilistic generation.
- What evidence would resolve it: A repeated measures study (e.g., n=5 runs per sample) analyzing the standard deviation of classification outputs for the same persona-prompt pairs.

### Open Question 2
- Question: Do the performance trade-offs and ideological biases identified in this study persist across non-English languages or datasets with different cultural annotation standards?
- Basis in paper: [explicit] The paper lists a limitation where "observed biases and performance gaps may differ across datasets with different content distributions and annotation standards," noting the exclusive use of an English dataset.
- Why unresolved: Safety alignment is often culturally specific (Western/Protestant bias cited in the paper), and it is unclear if censored models provide the same "ideological anchor" or fairness disparities in languages other than English.
- What evidence would resolve it: Replicating the experimental protocol using a multilingual hate speech dataset (e.g., MLSegal) to compare the interaction effects between censorship and persona across languages.

### Open Question 3
- Question: Can technical interventions (e.g., fairness-aware fine-tuning or counterfactual data augmentation) mitigate the 54.1 percentage point performance disparity in detecting hate speech against different target groups?
- Basis in paper: [inferred] The authors highlight "alarming fairness disparities" and a massive accuracy gap between target groups (e.g., 94.0% for Jewish people vs. 39.9 for unspecified targets), but they do not propose a method to resolve this "unequal protection."
- Why unresolved: The paper establishes the existence of the bias but does not test whether the safety alignment process itself can be modified to close the performance gap for under-protected groups.
- What evidence would resolve it: An experiment comparing the target-group recall of baseline models against models specifically trained with fairness constraints or balanced target-group data.

## Limitations
- UGI score conflates multiple dimensions of censorship without clear separation of alignment effects
- Single-run inference design lacks reproducibility checks for temperature-dependent variance
- Binary classification task obscures important failure mode differences between explicit and implicit hate

## Confidence

**High confidence**: The core finding that censored models outperform uncensored models (78.7% vs 64.1% strict accuracy) and demonstrate greater resistance to persona-induced bias is robust and well-supported by the data. The calibration analysis showing systemic overconfidence (ECE=0.094) is methodologically sound.

**Medium confidence**: The claim that safety alignment creates ideological anchoring while improving accuracy requires stronger evidence. The UGI-based stratification may not cleanly isolate alignment effects, and the interaction effects show statistical significance but limited practical magnitude.

**Low confidence**: The specific mechanisms by which RLHF improves classification accuracy beyond basic capability differences remain speculative. The paper does not establish whether alignment changes the underlying reasoning process or merely constrains output behavior.

## Next Checks

1. **Controlled alignment comparison**: Repeat the experiment using multiple model variants from the same family with different alignment levels (e.g., GPT-4o with/without safety filters, Llama 405b variants) to isolate alignment effects from architectural differences.

2. **Confidence calibration under prompting variations**: Test whether chain-of-thought prompting or multiple sampling passes improves calibration, addressing whether overconfidence is fundamental or prompt-dependent.

3. **Fine-grained classification validation**: Re-run the analysis maintaining the three-class distinction (explicit_hate, implicit_hate, not_hate) to better understand failure modes, particularly for implicit hate categories like irony that showed 69% accuracy.