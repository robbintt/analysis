---
ver: rpa2
title: Exploring Training Data Attribution under Limited Access Constraints
arxiv_id: '2509.12581'
source_url: https://arxiv.org/abs/2509.12581
tags:
- training
- data
- attribution
- target
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates training data attribution (TDA) under real-world
  constraints where model access and computational resources are limited. The authors
  systematically formalize five restricted-access scenarios and propose proxy-based
  strategies to perform TDA when the target model is inaccessible.
---

# Exploring Training Data Attribution under Limited Access Constraints

## Quick Facts
- arXiv ID: 2509.12581
- Source URL: https://arxiv.org/abs/2509.12581
- Reference count: 19
- This work investigates training data attribution (TDA) under real-world constraints where model access and computational resources are limited.

## Executive Summary
This work investigates training data attribution (TDA) under real-world constraints where model access and computational resources are limited. The authors systematically formalize five restricted-access scenarios and propose proxy-based strategies to perform TDA when the target model is inaccessible. Experiments across image, music, and text modalities show that proxy models with similar architectures yield attribution results comparable to full-access settings, while mismatched architectures degrade performance. Surprisingly, even randomly initialized or pretrained models without task-specific fine-tuning produce meaningful attribution scores. The findings demonstrate that effective TDA is feasible under limited information, particularly when architectural similarity is preserved, and that low-cost alternatives can provide valuable insights for data selection and model analysis in resource-constrained environments.

## Method Summary
The paper formalizes five access constraint scenarios for training data attribution, ranging from full model access (S0) to fully black-box models (S4). For limited-access settings, the authors propose using proxy models trained on the same dataset as the target. Attribution scores are computed using the TRAK method on the proxy, then evaluated against ground truth obtained by retraining the target on random data subsets. The evaluation metric LDS measures Spearman correlation between proxy-attributed influence and actual target model behavior. Knowledge distillation is tested as a proxy alignment strategy, but results show it does not meaningfully improve attribution quality.

## Key Results
- Proxy models with similar architectures yield attribution results comparable to full-access settings
- Randomly initialized or pretrained models without task-specific fine-tuning produce meaningful attribution scores
- Knowledge distillation improves output alignment but does not meaningfully improve gradient-based attribution alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proxy models sharing the same architecture family as the target model can approximate TDA scores effectively, even when parameter counts differ substantially.
- Mechanism: When architecture γ is known, external attributors construct a proxy configuration ϕ′ = (γ, η′) by combining the known architecture with guessed hyperparameters, then train on the same dataset S. Architectural alignment preserves gradient structure similarity, enabling TRAK and other gradient-based methods to produce correlated attribution scores.
- Core assumption: The gradient-space geometry of architecturally similar models remains sufficiently aligned for attribution ranking correlation, even when trained separately.
- Evidence anchors:
  - [abstract]: "proxy models with similar architectures yield attribution results comparable to full-access settings, while mismatched architectures degrade performance"
  - [section 4.2, Table 3]: TinyResNet (0.015× parameters) achieved LDS=0.2118 vs ResNet-9 baseline LDS=0.3235; ResNet-18 (similar params, different architecture) dropped to LDS=0.0994
  - [corpus]: LoRIF paper confirms gradient-based methods exploit gradient structure for influence functions, supporting architectural dependency
- Break condition: Attribution quality degrades sharply when proxy architecture γ′ diverges from target γ (e.g., using CNN/MLP proxies for ResNet targets yields LDS≈0.05-0.14).

### Mechanism 2
- Claim: Attribution scores computed on randomly initialized or pre-trained (unfine-tuned) models remain informative for data selection and influence ranking.
- Mechanism: Gradients from untrained models still reflect loss-reduction directions in parameter space. TRAK projects these gradients to identify training-test similarity; random initialization provides structured representations through architecture-induced inductive bias rather than learned features.
- Core assumption: The projection of random gradients captures meaningful data relationships that persist through training, not just task-specific learned patterns.
- Evidence anchors:
  - [abstract]: "even randomly initialized or pretrained models without task-specific fine-tuning produce meaningful attribution scores"
  - [section 4.3.1, Figure 3]: TRAK on random-initialized ResNet-9 achieved LDS>0.25 and AUC>0.6 (above 0.5 baseline)
  - [section 4.3.2, Table 4]: Pre-trained GPT-2 (unfine-tuned) achieved LDS=0.1821, slightly exceeding fine-tuned GPT-2's LDS=0.1613
  - [corpus]: Limited corpus evidence on untrained-model attribution; primarily this paper's contribution
- Break condition: RPS (Representer Point Selection) performs near-random on untrained models (relies on learned feature representations).

### Mechanism 3
- Claim: Knowledge distillation improves output alignment but does not meaningfully improve gradient-based attribution alignment.
- Mechanism: Distillation minimizes KL divergence between proxy (student) and target (teacher) output distributions. However, models with similar decision boundaries can have substantially different gradient vectors on the same inputs—output similarity ≠ gradient similarity.
- Core assumption: Attribution methods depend on gradient-space geometry rather than output-space behavior.
- Evidence anchors:
  - [section 3.1]: "even if two models share very similar decision boundaries, their gradient vectors on the same data points may still differ significantly"
  - [section 4.2]: Distilled proxies showed lower KL divergence but "attribution performance remained largely unchanged" across ResNet-9, MusicTransformer, and GPT-2 settings
  - [corpus]: No direct corpus evidence on distillation-TDA interaction
- Break condition: Structural architectural mismatch (e.g., CNN proxy for Transformer target) cannot be rescued by distillation.

## Foundational Learning

- **Concept: Gradient-based TDA (Influence Functions, TRAK, TracIn)**
  - Why needed here: The entire proxy approach depends on understanding how gradient-based methods compute influence scores via training-test gradient similarity and (for IF) Hessian inverse products.
  - Quick check question: Can you explain why TRAK uses random projection on gradients rather than computing full influence functions directly?

- **Concept: Linear Datamodeling Score (LDS)**
  - Why needed here: The paper's primary evaluation metric; measures Spearman correlation between model predictions on subsets and attribution-predicted influence scores.
  - Quick check question: Why does LDS evaluate attribution across a family of models rather than a single trained checkpoint?

- **Concept: Knowledge Distillation (KL-based)**
  - Why needed here: The paper tests distillation as a proxy-alignment strategy; understanding the loss formulation (temperature-scaled softmax KL + supervised loss) is necessary to interpret the negative result.
  - Quick check question: Why might minimizing output KL divergence fail to align gradient-space representations?

## Architecture Onboarding

- **Component map:** Target model f = A(S, ϕ) where ϕ = (γ: architecture family, η: hyperparameters) -> Proxy model f′ = A′(S, ϕ′) trained on same dataset S with estimated configuration -> TDA method τ (e.g., TRAK): computes attribution scores τ(z, S; f) for test point z -> Query interface Q(·): black-box access returning f(x) outputs (Scenario 1, 3 only) -> LDS evaluation: requires retraining target on 50 random 50% subsets for ground truth

- **Critical path:**
  1. Identify which scenario (S0-S5) applies based on available information
  2. If architecture γ known: select proxy from same family (e.g., ResNet variants for ResNet target)
  3. If only query access: attempt distillation but expect limited attribution improvement
  4. If no architecture info: attribution likely infeasible (Scenario 4 limitation)
  5. If compute-limited: try TRAK on pre-trained backbone without fine-tuning

- **Design tradeoffs:**
  - Proxy capacity: Smaller proxies (TinyResNet at 0.015× parameters) work surprisingly well if architecture matches
  - Distillation cost: KD adds training overhead without proportional attribution gains
  - Untrained model speed: Eliminates training entirely but works better for TRAK than RPS

- **Failure signatures:**
  - LDS near 0 with mismatched architecture (e.g., MLP proxy for ResNet target)
  - AUC near 0.5 on noisy label detection indicates random attribution
  - OOM with larger proxy ensembles (GPT-2 Medium proxy in Table 3)

- **First 3 experiments:**
  1. Replicate ResNet-9/CIFAR-2 proxy experiment: train ResNet-9-Small on same data, compute TRAK scores, compare LDS against full-access baseline using 50 subset retrainings
  2. Test random-initialization attribution: initialize ResNet-9 randomly (no training), run TRAK with 10-100 ensembles on MNIST-10, measure LDS and AUC for noisy label detection (10% flipped labels)
  3. Validate distillation futility: distill a CNN proxy from ResNet-9 target using query access, compare attribution LDS against undistilled CNN proxy—expect no significant improvement despite lower output KL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop application-agnostic evaluation metrics that assess attribution quality for a specific trained model checkpoint rather than aggregating performance across a model family?
- Basis in paper: [explicit] The authors identify a "lack of effective evaluation metrics for assessing attribution quality on specific trained models" and note that existing metrics like LDS focus on model families rather than fixed model instances.
- Why unresolved: Metrics like LDS evaluate generalization across configurations but fail to capture whether attribution scores correctly explain the behavior of a single specific model checkpoint, limiting the evaluation of instance-specific methods.
- What evidence would resolve it: A new metric that quantifies the alignment between proxy-attributed influence and the actual behavioral change of a single target model instance without relying on retraining ensembles.

### Open Question 2
- Question: Can effective training data attribution be achieved under strict Scenario 4 constraints (fully black-box) where no architectural information or internal parameters are available?
- Basis in paper: [explicit] The paper states that "current methods still fail to support data attribution for fully black-box models" and explicitly calls for the exploration of techniques that operate under such strict access constraints.
- Why unresolved: The experiments demonstrate that attribution performance degrades significantly when the proxy model's architecture differs from the target, suggesting architectural priors are currently essential.
- What evidence would resolve it: A method that maintains non-trivial attribution accuracy (e.g., high LDS or AUC) in Scenario 4 without relying on specific architectural priors or query access.

### Open Question 3
- Question: How can TDA methods be adapted for real-world scenarios where the external attributor has limited or no access to the candidate training dataset?
- Basis in paper: [explicit] The authors list "limited data access" as a limitation, noting, "we assume that the training data can be fully accessed by the external attributors, which is still not the case for many generative AI models."
- Why unresolved: The standard definition of TDA requires quantifying the contribution of specific training examples, a task that is ill-defined or impossible if the attributor cannot inspect the dataset.
- What evidence would resolve it: A framework that integrates membership inference or zero-shot learning techniques to perform attribution without full visibility of the training corpus.

## Limitations
- Attribution quality drops sharply when proxy architecture differs from target
- LDS metric requires retraining target models on 50 subsets, creating substantial computational overhead
- Study focuses primarily on TRAK and RPS methods, leaving other attribution approaches relatively underexplored

## Confidence
- **High Confidence:** Proxy models with similar architectures produce comparable attribution results (supported by systematic ablation across ResNet, MusicTransformer, and GPT-2 architectures)
- **Medium Confidence:** Untrained models yield meaningful attribution scores (based on single-method evidence, particularly TRAK performance)
- **Medium Confidence:** Distillation does not improve attribution alignment (consistent across architectures but limited methodological exploration)

## Next Checks
1. Test untrained model attribution across multiple methods (RPS, TracIn, IF) beyond TRAK to verify architecture-induced bias generality
2. Investigate attribution performance when proxy architecture is known but capacity differs substantially (>10× parameter variation) from target
3. Validate noisy label detection robustness across diverse data distributions and noise rates beyond the 10% case studied