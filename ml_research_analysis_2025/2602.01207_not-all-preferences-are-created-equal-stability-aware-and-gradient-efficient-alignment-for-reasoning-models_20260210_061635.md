---
ver: rpa2
title: 'Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient
  Alignment for Reasoning Models'
arxiv_id: '2602.01207'
source_url: https://arxiv.org/abs/2602.01207
tags:
- training
- sage
- preference
- reasoning
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAGE, a dynamic preference selection framework
  for improving the stability and efficiency of alignment in reasoning models. SAGE
  addresses the inefficiency of standard methods like DPO, which treat all preference
  pairs equally and suffer from noise in high-curvature regions.
---

# Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models

## Quick Facts
- arXiv ID: 2602.01207
- Source URL: https://arxiv.org/abs/2602.01207
- Reference count: 22
- Key outcome: SAGE accelerates alignment convergence and outperforms static baselines on reasoning tasks by dynamically selecting high-stability preference pairs.

## Executive Summary
This paper introduces SAGE (Stability-Aware and Gradient-Efficient alignment), a dynamic preference selection framework for improving the stability and efficiency of alignment in reasoning models. SAGE addresses the inefficiency of standard methods like DPO, which treat all preference pairs equally and suffer from noise in high-curvature regions. The method uses a coarse-grained difficulty-based curriculum to refresh candidate pools and a fine-grained stability-aware scoring function to prioritize high-signal, confident errors while filtering out unstable samples. Experiments on mathematical reasoning benchmarks show that SAGE accelerates convergence and outperforms static baselines, with gains particularly pronounced on harder tasks and larger models.

## Method Summary
SAGE combines a dynamic curriculum for selecting preference pairs and a stability-aware scoring mechanism to improve gradient efficiency during alignment. The framework maintains a candidate pool refreshed via a coarse-grained difficulty-based curriculum, ensuring diversity in training data. A fine-grained scoring function prioritizes preference pairs with high-confidence errors and stability, reducing noise and avoiding catastrophic forgetting. The approach is tested on mathematical reasoning tasks using both smaller and larger models, demonstrating improved accuracy and training efficiency over static preference sampling methods.

## Key Results
- SAGE achieves higher accuracy on mathematical reasoning benchmarks compared to static preference sampling baselines.
- The method shows pronounced performance gains on harder tasks and larger models.
- Dynamic preference selection accelerates convergence and improves training efficiency.

## Why This Works (Mechanism)
SAGE improves alignment by dynamically selecting preference pairs that are both stable and high-signal, avoiding the pitfalls of treating all pairs equally. By focusing on confident errors and filtering out unstable samples, the framework reduces noise in the gradient updates, leading to more efficient learning. The combination of a coarse-grained curriculum and fine-grained scoring ensures that the model is exposed to a diverse yet high-quality set of training examples, which is particularly beneficial for reasoning tasks that require complex decision-making.

## Foundational Learning
- **Preference-based alignment**: Why needed - Aligns models to human preferences for better task performance. Quick check - Verify alignment improves accuracy on target tasks.
- **Gradient efficiency**: Why needed - Reduces training time and computational cost. Quick check - Measure convergence speed and resource usage.
- **Stability-aware scoring**: Why needed - Filters out noisy samples that hinder learning. Quick check - Evaluate stability metrics on synthetic and real data.

## Architecture Onboarding

**Component Map**: Candidate Pool -> Coarse-grained Curriculum -> Fine-grained Scoring -> Preference Selection

**Critical Path**: The critical path involves refreshing the candidate pool via the coarse-grained curriculum, applying the fine-grained scoring function to select stable and high-signal pairs, and using these pairs for preference-based alignment.

**Design Tradeoffs**: Balancing diversity in the candidate pool with the quality of selected pairs; computational overhead of stability estimation versus training efficiency gains.

**Failure Signatures**: Poor performance on harder tasks may indicate insufficient diversity in the candidate pool; instability in training could signal ineffective filtering of noisy samples.

**First Experiments**:
1. Evaluate SAGE on human-annotated preference datasets to verify stability metrics generalize beyond synthetic data.
2. Conduct ablation studies isolating the contribution of coarse-grained vs. fine-grained components.
3. Measure computational overhead and wall-clock training time with full-scale implementation.

## Open Questions the Paper Calls Out
None

## Limitations
- The primary claims rest on synthetic preference pair generation and controlled synthetic reasoning datasets, which may not fully capture the noise and distribution shift present in real human preference data.
- The assumption that stability metrics derived from small candidate pools generalize to broader preference distributions requires further empirical validation.
- The computational overhead of repeated fine-tuning for stability estimation, though mitigated in the final framework, represents a practical concern for large-scale deployment.

## Confidence
- **SAGE's superiority over static preference sampling methods**: High confidence - well-supported by controlled experiments across multiple reasoning tasks and model scales, with statistically significant improvements in accuracy metrics.
- **Stability-aware scoring effectively filters noisy samples**: Medium confidence - demonstrated through ablation studies and qualitative analysis, but validation on real human preference data remains untested.
- **Dynamic curriculum improves convergence efficiency**: High confidence - evidenced by training curves and comparison with fixed baselines, though absolute wall-clock time savings depend on implementation specifics.

## Next Checks
1. Evaluate SAGE on human-annotated preference datasets (e.g., Arcee's RL preference pairs) to verify stability metrics generalize beyond synthetic data.
2. Conduct ablation studies isolating the contribution of coarse-grained vs. fine-grained components to quantify their relative impact on performance.
3. Measure computational overhead and wall-clock training time with full-scale implementation to assess practical deployment viability.