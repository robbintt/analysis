---
ver: rpa2
title: Learning a Continue-Thinking Token for Enhanced Test-Time Scaling
arxiv_id: '2506.11274'
source_url: https://arxiv.org/abs/2506.11274
tags:
- token
- learned
- baseline
- eggs
- shells
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning a specialized continue-thinking
  token to improve test-time scaling in language models. The approach augments a distilled
  reasoning model with a learned token embedding, optimized via reinforcement learning
  while keeping the model weights frozen.
---

# Learning a Continue-Thinking Token for Enhanced Test-Time Scaling

## Quick Facts
- arXiv ID: 2506.11274
- Source URL: https://arxiv.org/abs/2506.11274
- Reference count: 40
- This paper introduces a method for learning a specialized continue-thinking token to improve test-time scaling in language models

## Executive Summary
This paper presents a method for learning a specialized `<|continue-thinking|>` token embedding to improve test-time scaling in language models. The approach augments a distilled reasoning model (DeepSeek-R1-Distill-Qwen-1.5B) with a single learned token embedding, optimized via reinforcement learning while keeping all model weights frozen. During inference, the learned token replaces end-of-thinking tokens to encourage extended reasoning within a compute budget. Experiments show that the learned token yields greater accuracy improvements than fixed tokens like "Wait," achieving up to a 4% absolute gain on math benchmarks. Performance gains are attributed to longer, more reflective reasoning rather than formatting improvements, with the token demonstrating generalization across inference settings.

## Method Summary
The method adds a new `<|continue-thinking|>` token to the vocabulary and initializes its embedding from the "Wait" token. All model parameters are frozen except for this single token embedding. Training uses GRPO with a reward function combining format adherence (boxed answer) and correctness (ground truth match). During inference, when the model generates an end-of-thinking token (```) and budget constraints allow, it's replaced with the learned continue-thinking token to prompt extended reasoning. The approach is trained on DeepScaleR-Preview-Dataset (40K math questions) and evaluated on GSM8K-Platinum, MATH500, AIME24, and AIME25.

## Key Results
- Learned token improves GSM8K accuracy from 78.41% to 81.39% (3.0% absolute gain) with Cmax=1
- Outperforms fixed "Wait" token baseline (1.3% gain vs 4.2% gain on GSM8K)
- Generalizes to inference settings with Cmax=2,3 despite training only with Cmax=1
- Shows diminishing returns on harder AIME datasets where budget forcing provides no baseline benefit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A learned token embedding triggers more productive reasoning continuations than fixed textual tokens
- Mechanism: The `<|continue-thinking|>` token's embedding vector is optimized via reinforcement learning (GRPO) while all model weights remain frozen. During generation, whenever the model outputs an end-of-thinking marker (```), it is replaced with this learned token, prompting extended deliberation. Unlike static tokens like "Wait," the learned embedding is shaped by reward signals (correctness + format) to steer the model toward continuations that actually improve accuracy.
- Core assumption: The frozen model already possesses sufficient reasoning capacity; performance is limited by premature termination rather than lack of capability.
- Evidence anchors: [abstract] "We augment a distilled version of DeepSeek-R1-Distill-Qwen-1.5B with a single learned `<|continue-thinking|>` token, training only its embedding via reinforcement learning while keeping the model weights frozen."

### Mechanism 2
- Claim: Extended reasoning traces directly cause accuracy improvements through error correction and self-verification
- Mechanism: The forced continuation allows the model to revisit, backtrack, and correct earlier mistakes. Qualitative analysis shows the token frequently triggers phrases like "no, let me try that again" or "Wait, did I just repeat the same steps?", indicating reflective reasoning rather than mere lengthening.
- Core assumption: Longer reasoning is beneficial only when it includes genuine re-evaluation, not verbose restatement.
- Evidence anchors: [section 4.3] "The most common continuations often prompt the model to self-verify or reconsider its previous steps, indicating that the token effectively encourages reflective reasoning and backtracking."

### Mechanism 3
- Claim: The learned token generalizes to inference settings not seen during training (multiple forced continuations, different budget limits)
- Mechanism: Training uses Cmax=1 (single forced continuation) and Bmax=8192, but at test time the token remains effective with Cmax=2,3 and Bmax=9216. This suggests the embedding encodes a general "continue deliberating" signal rather than overfitting to specific token counts.
- Core assumption: The embedding space captures abstract reasoning prolongation rather than position-specific patterns.
- Evidence anchors: [section 4.3] "Notably, although our model was trained exclusively with Cmax=1, we observe that increasing Cmax to 2 and 3 during inference often leads to further improvements in accuracy."

## Foundational Learning

- Concept: **Budget Forcing**
  - Why needed here: This is the base technique the paper builds upon. Budget forcing controls test-time compute by overriding termination tokens and enforcing maximum token counts. Without understanding this, the contribution (improving upon fixed tokens) is unclear.
  - Quick check question: Can you explain what happens when a model generates ``` but budget forcing replaces it with "Wait"?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The training method for the token embedding. GRPO samples multiple completions per prompt, computes rewards, and updates policy relative to group performance. This enables exploration of productive continuations without supervised labels.
  - Quick check question: How does GRPO differ from standard PPO in terms of advantage estimation?

- Concept: **Frozen Backbone Training**
  - Why needed here: The entire method hinges on training only a single embedding vector while keeping all other parameters fixed. This constrains the optimization problem and enables efficient training, but limits what can be learned.
  - Quick check question: What are the memory and compute implications of freezing the backbone vs. full fine-tuning?

## Architecture Onboarding

- Component map:
  Input Prompt → Frozen LLM (DeepSeek-R1-Distill-Qwen-1.5B) → Generate tokens autoregressively → [If ``` generated AND Cmax not reached AND Bmax not exceeded] → Replace with <|continue-thinking|> token → Continue generation → [If Bmax exceeded] Force-append ``` → Final answer
  
  Trainable parameters: Single embedding vector θT ∈ ℝ^d (where d = hidden dimension)

- Critical path:
  1. Token initialization: Initialize `<|continue-thinking|>` embedding from "Wait" token embedding
  2. Generation with injection: Monitor for end-of-thinking token; replace and continue
  3. Reward computation: Binary format reward (boxed answer) + binary correctness reward
  4. GRPO update: Compute advantages across 16 generations per example; update only θT

- Design tradeoffs:
  - **Fixed vs. Learned Token**: Fixed tokens require no training but yield lower gains (1.3% vs 4.2% on GSM8K). Learned tokens require training data and compute but maximize improvement where budget forcing helps.
  - **Training Cmax vs. Inference Cmax**: Training with Cmax=1 is cheaper but relies on generalization. Training with higher Cmax might improve multi-continuation performance but increases training cost.
  - **Evaluation Method**: Regex-only evaluation overestimates gains by conflating format adherence with reasoning. LLM-based evaluation is more trustworthy but adds complexity.

- Failure signatures:
  1. **No baseline improvement → No learned improvement**: On AIME datasets where fixed-token budget forcing provides no gain over baseline, learned token also shows no statistically significant benefit.
  2. **Regex vs. LLM evaluation mismatch**: If regex-only evaluation shows large gains but LLM evaluation shows minimal gains, the improvement is formatting-driven, not reasoning-driven.
  3. **Excessive token generation without correction**: If continuations repeat the same reasoning without self-correction, the token isn't triggering reflective behavior.

- First 3 experiments:
  1. **Validate baseline budget forcing first**: Before training a learned token, test whether fixed-token budget forcing ("Wait", "Hmm", "Alternatively") improves accuracy on your target dataset. If no improvement, the learned token approach won't help either.
  2. **Train with LLM-based evaluation**: Implement both regex and LLM-based evaluation during development. Monitor the gap between them to ensure gains are reasoning-driven, not formatting artifacts.
  3. **Ablate initialization strategy**: Compare initializing the learned token from "Wait" embedding vs. random initialization vs. other token embeddings to verify the paper's claim that "Wait" initialization is effective.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would learning multiple distinct continue-thinking tokens (one for each forced continuation) outperform a single learned token?
- Basis in paper: [explicit] Future Directions proposes: "one can add a special token that will be used for the first forced continuation and a second new token that will be used for the second forced continuation."
- Why unresolved: The current work trains only a single token with Cmax=1, then tests generalization to Cmax>1 with that same token.
- What evidence would resolve it: Train separate token embeddings for each continuation position and compare accuracy against the single-token approach on the same benchmarks.

### Open Question 2
- Question: Does the learned token approach transfer effectively to larger language models beyond the 1.5B distilled model tested?
- Basis in paper: [explicit] Future Directions states: "examining its impact on larger-scale language models warrants investigation."
- Why unresolved: Computational constraints limited experiments to DeepSeek-R1-Distill-Qwen-1.5B; scaling behavior remains unknown.
- What evidence would resolve it: Apply the same training procedure to larger models (e.g., 7B, 14B, 70B) and report accuracy gains relative to baselines.

### Open Question 3
- Question: Can the learned continue-thinking token improve performance in domains without verifiable rewards, such as LLM alignment?
- Basis in paper: [explicit] Future Directions proposes: "extending the scope of this research to diverse domains, potentially even those lacking explicit verifiable rewards, such as in the context of LLM alignment."
- Why unresolved: Current training relies on verifiable correctness rewards from math problems; reward design for open-ended domains is unspecified.
- What evidence would resolve it: Adapt the reward function for alignment tasks (e.g., helpfulness, harmlessness) and evaluate on alignment benchmarks.

### Open Question 4
- Question: What underlying factors determine whether budget forcing provides any benefit at all, and why does the learned token's effectiveness depend on this precondition?
- Basis in paper: [inferred] Results show learned token offers gains only when fixed-token budget forcing already helps (e.g., GSM8K, MATH500) but not on AIME where budget forcing fails. The paper notes this contingency but does not explain it.
- Why unresolved: The correlation is empirical; no analysis explains why some problem types respond to extended reasoning while others do not.
- What evidence would resolve it: Analyze problem features (difficulty, reasoning steps required, solution space structure) that predict budget-forcing effectiveness, then test whether learned tokens help specifically on those problem types.

## Limitations

- Evaluation methodology concerns: The paper uses regex-only evaluation for main results, which conflates formatting improvements with actual reasoning gains. The claimed 4% absolute improvement on GSM8K may be substantially inflated by better adherence to the `\boxed{}` format rather than improved problem-solving.
- Generalization across domains: While the learned token shows improvements on GSM8K and MATH500, it fails to provide statistically significant benefits on AIME datasets, raising questions about when the approach is actually useful.
- Training cost and efficiency: The paper reports substantial compute requirements (8×A100 GPUs, 936 training steps with batch=16 and 64 gradient accumulation steps), which may not be "cheap" for all research contexts.

## Confidence

- **High Confidence**: The learned token improves GSM8K accuracy from 78.41% to 81.39% (3.0% absolute gain) with Cmax=1. The ablation showing learned token outperforms fixed tokens like "Wait" (1.3% gain vs 4.2% gain on GSM8K) is well-supported by the data.
- **Medium Confidence**: The claim that learned token generalizes to Cmax=2,3 despite training only on Cmax=1 is supported by GSM8K results but shows diminishing returns on AIME datasets.
- **Low Confidence**: The 4.2% absolute improvement on GSM8K is likely overestimated due to regex evaluation artifacts. The claim that the approach is broadly applicable across domains is weakened by the failure to improve AIME performance.

## Next Checks

1. **Implement dual evaluation pipeline**: Run experiments using both regex extraction and LLM-based evaluation (Qwen2.5-7B-Instruct or similar) in parallel from the start. Compute the gap between these evaluations for both baseline and learned token conditions. If the gap is substantial (e.g., >2% absolute), the claimed improvements are likely dominated by formatting artifacts rather than reasoning quality.

2. **Validate baseline budget forcing first**: Before implementing the learned token approach, thoroughly test fixed-token budget forcing ("Wait", "Hmm", "Alternatively") on your target dataset. Only proceed to learned token training if fixed-token budget forcing shows statistically significant improvement over the no-forcing baseline.

3. **Ablate initialization strategy systematically**: Train learned tokens with three different initializations: (a) from "Wait" token embedding (paper's approach), (b) from random initialization, and (c) from other fixed tokens ("Hmm", "Alternatively"). Compare final performance across these conditions to verify the importance of initialization strategy.