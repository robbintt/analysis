---
ver: rpa2
title: 'Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries'
arxiv_id: '2601.18899'
source_url: https://arxiv.org/abs/2601.18899
tags:
- language
- family
- speech
- languages
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates connector strategies for multilingual ASR using
  LLM-based systems. It compares language-specific connectors (LANGCONN) versus family-level
  connectors (FAMCONN) across ten language families and nearly forty languages.
---

# Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries

## Quick Facts
- **arXiv ID:** 2601.18899
- **Source URL:** https://arxiv.org/abs/2601.18899
- **Reference count:** 8
- **Primary result:** Family-level connectors in multilingual ASR consistently outperform language-specific connectors across 10 language families and nearly 40 languages, with WER improvements ranging from 2% to 71%.

## Executive Summary
This paper evaluates connector strategies for multilingual ASR using LLM-based systems, comparing language-specific connectors against family-level connectors across ten language families and nearly forty languages. Experiments demonstrate that family-level connectors consistently achieve lower word error rates than language-specific connectors, with improvements ranging from 2% to 71% across different families. Cross-domain evaluation shows family connectors provide better generalization when trained on one corpus and tested on another. The study concludes that family-level grouping offers the most effective and reliable solution for multilingual ASR, balancing transferability and specialization while avoiding the instability of language-specific approaches.

## Method Summary
The study uses an Encoder–Connector–Decoder architecture with Whisper-large-v3 as the frozen encoder and Gemma-2-2b or Salamandra-2b as the frozen decoder. The connector is implemented as a downsampling layer (concatenating K frames) followed by two linear layers with GELU activation. Training uses 10 epochs with early stopping, AdamW optimizer (lr=1e-4, weight decay=1e-6), and batch size 10. Evaluation measures Word Error Rate across FLEURS and CommonVoice_22 datasets, with languages grouped by family for FAMCONN and kept separate for LANGCONN. Cross-domain generalization is tested by training on one corpus and evaluating on another.

## Key Results
- Family-level connectors achieve consistently lower WER than language-specific connectors across all evaluated language families
- Cross-domain evaluation demonstrates superior generalization of family connectors when trained on one corpus and tested on another
- Language-specific connectors show instability and high failure rates, particularly with Salamandra decoder producing repetitive outputs
- Performance improvements range from 2% to 71% WER reduction depending on the language family

## Why This Works (Mechanism)
Family-level connectors leverage shared acoustic and linguistic patterns across related languages, enabling more robust parameter sharing while maintaining sufficient specificity. The downsampling approach reduces temporal resolution while preserving cross-lingual acoustic features that transfer well within families. This architecture balances the trade-off between specialization (language-specific) and generalization (family-level), avoiding the instability that occurs when connectors must learn highly specific mappings for individual languages.

## Foundational Learning
- **Multilingual ASR architecture**: Understanding how encoder-connector-decoder frameworks enable language transfer; needed for grasping the system design; quick check: identify each component's role in speech recognition pipeline.
- **Word Error Rate (WER)**: Standard metric for speech recognition accuracy; needed for evaluating connector performance; quick check: calculate WER from hypothesis and reference transcripts.
- **Cross-domain generalization**: Ability of models trained on one dataset to perform well on different datasets; needed for assessing real-world applicability; quick check: compare in-domain vs cross-domain WER results.
- **Language family classification**: Grouping languages by genealogical relationships; needed for understanding FAMCONN grouping strategy; quick check: map languages to their correct families in Table 1.
- **LLM-based speech processing**: Using large language models for speech recognition tasks; needed for understanding the decoder integration; quick check: identify how LLM decoder processes connector outputs.
- **Early stopping in training**: Technique to prevent overfitting by halting training when validation performance plateaus; needed for understanding training methodology; quick check: determine appropriate patience threshold for stopping criterion.

## Architecture Onboarding

**Component Map:** Whisper-large-v3 (encoder) → Connector (downsampling + L1 → GELU → L2) → LLM decoder (Gemma-2-2b/Salamandra-2b)

**Critical Path:** Speech frames → Encoder → Connector downsampling → Projector layers → LLM decoder → Text output

**Design Tradeoffs:** Family-level connectors balance data efficiency and specialization better than language-specific approaches, but may underperform on highly diverse families where acoustic similarity doesn't match genealogical relationships.

**Failure Signatures:** Extremely high WER (often >80%) with LANGCONN + Salamandra, characterized by repetitive outputs and excessive length (e.g., 94% repetition rate on Polish).

**First 3 Experiments:**
1. Compare WER of FAMCONN vs LANGCONN on a single diverse family (e.g., Afro-Asiatic) to observe performance differences
2. Test cross-domain generalization by training on FLEURS and evaluating on CommonVoice for both connector types
3. Measure repetition and overlong output rates for high-WER cases to confirm failure mode diagnosis

## Open Questions the Paper Calls Out
- **Typology-based groupings**: How would connector performance change when grouping languages by typological similarity or shared script rather than genealogical family membership? The paper explicitly calls for future work on "typology-based or script-based groupings" to better understand cross-lingual influence.
- **Branch-level subdivisions**: Would branch-level subdivisions within large language families provide a better trade-off between data pooling and specificity than family-level connectors? The authors state future work could consider "branch-level subdivisions within large families."
- **Decoding strategy mitigation**: Can specific decoding strategies or constraints effectively mitigate the repetition and overlong output errors observed in high-WER cases? The paper identifies these as primary failure modes but offers no solution.

## Limitations
- Connector architecture specifics (downsampling factor K and exact projector dimensions) are unspecified, requiring assumptions that could affect performance
- Data preprocessing pipeline details (audio normalization, frame-text alignment) are not provided, creating reproducibility gaps
- The study only evaluates genealogical families, not exploring typology-based or script-based groupings that might better align with acoustic similarity

## Confidence
- **Family-level connector superiority**: High confidence - directly supported by comparative WER results across multiple families
- **Cross-domain generalization advantage**: High confidence - demonstrated through systematic corpus-swapping experiments
- **Instability of language-specific connectors**: Medium confidence - supported by high WER examples but exact failure mechanisms need validation

## Next Checks
1. **Architecture ablation**: Test connector variants with different downsampling factors (K=2, 3, 4) and hidden dimensions to identify optimal configuration and validate assumptions
2. **Family diversity analysis**: Evaluate connector performance on sub-family groupings within high-variance families (Afro-Asiatic, Dravidian) to quantify trade-off between family-level simplicity and linguistic specificity
3. **Robustness stress test**: Systematically induce and measure repetition/overlong output failure modes across connector types to quantify stability differences beyond aggregate WER