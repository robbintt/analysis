---
ver: rpa2
title: Research Community Perspectives on "Intelligence" and Large Language Models
arxiv_id: '2505.20959'
source_url: https://arxiv.org/abs/2505.20959
tags:
- intelligence
- systems
- respondents
- criteria
- survey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper surveyed 303 researchers across fields including NLP,\
  \ ML, Cognitive Science, and Neuroscience to explore their definitions of \u201C\
  intelligence\u201D and perceptions of LLM-based systems. The survey identified three\
  \ widely agreed-upon criteria: generalization (86%), adaptability (83%), and reasoning\
  \ (83%)."
---

# Research Community Perspectives on "Intelligence" and Large Language Models

## Quick Facts
- arXiv ID: 2505.20959
- Source URL: https://arxiv.org/abs/2505.20959
- Reference count: 19
- Primary result: Majority of researchers (71%) do not consider current LLM-based systems intelligent

## Executive Summary
This survey of 303 researchers across NLP, ML, Cognitive Science, and Neuroscience reveals a complex landscape of perspectives on artificial intelligence and LLMs. While researchers broadly agree on three core criteria for intelligence—generalization (86%), adaptability (83%), and reasoning (83%)—a significant majority (71%) do not consider current LLM-based systems intelligent. The study identifies that researchers' personal goals strongly influence their perceptions, with only 16.2% viewing creating intelligent systems as their research objective. Notably, these "builder" researchers are more likely to attribute intelligence to current systems, suggesting a confirmation bias effect.

## Method Summary
The study employed an online survey distributed through academic mailing lists (ACL, corpora-list, ML News, Connectionist) and social media platforms (Bluesky, LinkedIn) over three months (Nov 2024-Jan 2025). The survey instrument included 14 questions with binary and Likert-scale responses, collecting demographic information and research goals. Statistical analysis utilized Fisher's exact test for group comparisons, phi (ϕ) coefficients for binary variable associations, and Cramer's V for effect sizes. The dataset was later coarsened for public release, removing gender information and merging geographic regions.

## Key Results
- 71% of respondents do not consider current LLM-based systems intelligent
- Only 16.2% view developing intelligent systems as their research goal
- Generalization (86%), adaptability (83%), and reasoning (83%) are the top three agreed-upon intelligence criteria
- Researchers focused on creating intelligent systems are more likely to attribute intelligence to current LLMs (ϕ = .208)

## Why This Works (Mechanism)

### Mechanism 1: Criteria Coherence via Abstraction
Researchers across diverse fields converge on generalization, adaptability, and reasoning as core intelligence criteria because these traits represent the ability to handle novelty and logic rather than mere knowledge storage. This shared conceptual framework suggests a fundamental understanding of intelligence that transcends disciplinary boundaries. The assumption that these criteria accurately represent intelligence for the surveyed population is supported by the strong correlation between generalization and adaptability (ϕ = .451), though direct comparative evidence is limited.

### Mechanism 2: Goal-Dependent Perception Bias
A researcher's stance on LLM intelligence is causally linked to their personal research agenda through confirmation bias or lowered success thresholds. Those motivated by creating intelligent technology are more prone to attribute intelligence to current systems, potentially viewing partial successes as validation of their goals. This correlation (ϕ = .208) implies that research objectives shape perception rather than the reverse, though the directional influence requires further validation.

### Mechanism 3: Skepticism Gradient via Career Stage
Senior researchers demonstrate greater skepticism toward LLM intelligence, likely due to experience with historical hype cycles and awareness of "wishful mnemonics." This pattern shows ≈77% of senior researchers disagreeing with current LLM intelligence compared to ≈48-56% of early-career researchers. The mechanism assumes this difference stems from accumulated wisdom rather than generational understanding gaps, though the current LLM paradigm may limit the generalizability of this historical skepticism.

## Foundational Learning

- **Concept: Construct Validity**
  - Why needed here: The paper critiques current benchmarks (like MMLU) for lacking construct validity—measuring what they claim to measure (intelligence) vs. memorization.
  - Quick check question: Does a high score on a benchmark imply the system possesses the underlying capability, or just surface-level statistical patterns?

- **Concept: The Eliza Effect**
  - Why needed here: This cognitive bias explains "seeming intelligence," where users attribute humanity to simple pattern matching.
  - Quick check question: When you read a coherent LLM output, are you inferring reasoning capability that isn't demonstrably there?

- **Concept: Phi Coefficient (ϕ)**
  - Why needed here: The paper uses ϕ to measure association between binary criteria (e.g., selecting "generalization" vs. "adaptability").
  - Quick check question: If two criteria have a ϕ of 0.45, does this mean they are synonymous or just frequently co-selected? (Answer: The latter; they are correlated but distinct).

## Architecture Onboarding

- **Component map:**
  - Core Criteria (The "Intel" Layer): Generalization, Adaptability, Reasoning (Top 3 required components)
  - Supporting Criteria: Planning, Knowledge Acquisition, Perception (Secondary components)
  - Exclusion Zone: Embodiment, Consciousness (Explicitly low consensus or rejected for current LLMs)
  - Evaluation Layer: Generalization Tests (e.g., ARC) vs. Static Benchmarks (e.g., MMLU)

- **Critical path:**
  1. Define Scope: Select the triad (Generalization, Adaptability, Reasoning)
  2. Assess System: Test system against "novelty" handling (not static training data)
  3. Filter: Reject "seeming intelligence" (Eliza effect) by requiring mechanism validation (interpretability)

- **Design tradeoffs:**
  - Binary vs. Spectrum: The paper notes the flaw in binary "Intelligent/Not" labels, yet uses them for survey clarity. In architecture, prefer a "spectrum" score (0-100) on specific capabilities rather than a boolean flag.
  - Utility vs. Intelligence: 58% aim for "useful tech" vs. 16% for "intelligent tech." Design systems for utility; treat intelligence as a potential emergent property, not the requirement.

- **Failure signatures:**
  - Anthropomorphization: Attributing "understanding" to statistical likelihoods
  - Benchmark Gaming: High scores on MMLU/SAT without robust reasoning
  - Static Knowledge reliance: Relying on "familiar domain problem solving" rather than "novel task handling"

- **First 3 experiments:**
  1. OOD Generalization Test: Evaluate the system on data explicitly outside the training distribution (e.g., ARC challenge) to satisfy the "Generalization" criteria
  2. Counterfactual Reasoning: Test logic on synthetic/counterfactual scenarios to verify "Reasoning" independent of "Knowledge Acquisition"
  3. Dynamic Adaptability Probe: Measure performance degradation when task rules change mid-session, targeting the "Adaptability" criteria

## Open Questions the Paper Calls Out

### Open Question 1
How does framing intelligence as a continuous spectrum rather than a binary attribute influence researchers' willingness to attribute intelligence to LLMs? The survey's binary choices conflicted with free-text comments arguing intelligence is a "spectrum" or "matter of degree," requiring follow-up studies with continuous slider scales.

### Open Question 2
Do definitions of intelligence and perceptions of LLMs differ significantly in non-Western research communities? The current sample is heavily biased toward North America and Europe, leaving perspectives from Asia, Africa, and other regions underrepresented.

### Open Question 3
Do researchers apply a coherent, consistent definition of "intelligence" across different entity types (e.g., biological vs. artificial)? It's unclear if researchers who require "reasoning" for AI also apply that same requirement to animals or humans.

## Limitations
- Survey sampling was predominantly academic (80%) and Western (65%), limiting generalizability
- Only 303 responses from an initial sample pool of ~12,000, raising selection bias concerns
- Binary criteria selection may oversimplify complex concepts of intelligence
- Conducted during rapid LLM advancement, potentially influencing responses

## Confidence

- **High confidence:** Identification of generalization, adaptability, and reasoning as top criteria (86%, 83%, 83% agreement)
- **Medium confidence:** Correlation between research goals and attribution of intelligence to current LLMs
- **Low confidence:** Claims about causal relationship between career stage and skepticism levels

## Next Checks

1. Replicate with industry practitioners: Conduct the same survey with balanced sample (50% industry, 50% academia) to test whether identified criteria and skepticism patterns hold across professional contexts.

2. Longitudinal tracking: Administer survey at 6-month intervals to track how researcher perspectives evolve as LLM capabilities advance, particularly focusing on the 60% who remain skeptical of future systems.

3. Cross-cultural validation: Translate and administer survey in non-Western contexts (East Asia, Latin America, Africa) to assess whether identified criteria cluster and skepticism patterns are universal or culturally specific.