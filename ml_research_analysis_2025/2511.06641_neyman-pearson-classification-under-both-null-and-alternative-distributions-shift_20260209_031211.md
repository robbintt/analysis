---
ver: rpa2
title: Neyman-Pearson Classification under Both Null and Alternative Distributions
  Shift
arxiv_id: '2511.06641'
source_url: https://arxiv.org/abs/2511.06641
tags:
- error
- target
- source
- learning
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies Neyman-Pearson classification in a transfer
  learning setting where both class-0 and class-1 distributions may shift between
  source and target domains. The authors propose an adaptive two-stage procedure that
  leverages source data to control the target Type-I error while improving Type-II
  error.
---

# Neyman-Pearson Classification under Both Null and Alternative Distributions Shift

## Quick Facts
- **arXiv ID:** 2511.06641
- **Source URL:** https://arxiv.org/abs/2511.06641
- **Reference count:** 40
- **Key outcome:** This paper studies Neyman-Pearson classification in a transfer learning setting where both class-0 and class-1 distributions may shift between source and target domains. The authors propose an adaptive two-stage procedure that leverages source data to control the target Type-I error while improving Type-II error.

## Executive Summary
This work addresses Neyman-Pearson classification in a transfer learning setting where both class-0 (μ₀) and class-1 (μ₁) distributions shift between source and target domains. The authors propose an adaptive two-stage procedure that first determines an effective threshold on source Type-I error to align source and target constraints, then uses source class-1 samples to further reduce target Type-II error. The method guarantees avoidance of negative transfer when the source is uninformative and provides computational guarantees by reformulating the learning procedure as a sequence of convex programs solved via stochastic optimization. Experiments on climate datasets demonstrate the method's effectiveness in maintaining Type-I error control while improving Type-II error compared to using only target data.

## Method Summary
The proposed method consists of a two-stage adaptive procedure. First, it computes an effective threshold $\hat{\alpha}_S$ on the source domain that aligns source and target constraints, ensuring the target Type-I error constraint is satisfied. This involves solving a minimax optimization problem to find the intersection of source and target feasible hypothesis sets. Second, it uses this aligned threshold to solve a constrained optimization problem that minimizes target Type-II error while leveraging source class-1 samples. The method is reformulated as a sequence of convex programs solved via Stochastic Gradient Descent-Ascent (SGDA) with projection (CP-Solver), achieving polynomial-time convergence with statistical guarantees. The algorithm automatically falls back to target-only learning when the source is uninformative, thereby avoiding negative transfer.

## Key Results
- The adaptive procedure maintains target Type-I error control (≤ α) while improving Type-II error compared to target-only methods.
- The method successfully avoids negative transfer by conditionally selecting source samples only when they provably reduce target Type-II error.
- Computational guarantees are achieved through convex program reformulation, allowing polynomial-time convergence to the statistical rate.
- Experiments on climate datasets demonstrate superior performance over baseline methods in maintaining Type-I error constraints while reducing Type-II error.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** An adaptive threshold $\hat{\alpha}_S$ on the source domain corrects for the distribution shift in the null class ($\mu_0$), ensuring the target Type-I error constraint is satisfied.
- **Mechanism:** The procedure does not naively apply the target constraint $\alpha$ to the source data. Instead, it solves for an inflated or deflated source threshold $\hat{\alpha}_S$ (Eq. 5) such that the set of classifiers satisfying this source threshold intersects with the set of classifiers satisfying the target constraint. This alignment step effectively "remaps" the source feasibility region to match the target's requirements before transfer occurs.
- **Core assumption:** There exists a non-empty intersection between the source and target feasible hypothesis sets (implied by the definition of $\alpha_S$ in Eq. 3).
- **Evidence anchors:**
  - [abstract] "first determines an effective threshold on source Type-I error to align source and target constraints"
  - [section 4] Defines $\hat{\alpha}_S$ as the infimum where the empirical source feasible set intersects the target set.
  - [corpus] Related work (Kalan et al., 2025) addressed shift only in $\mu_1$; this mechanism is the distinct extension to $\mu_0$ shift.
- **Break condition:** If the distribution shift in $\mu_0$ is extreme such that the source feasible set has zero overlap with the target feasible set, the adaptive threshold cannot bridge the gap, potentially leading to conservative performance or fallback.

### Mechanism 2
- **Claim:** The procedure guarantees the avoidance of negative transfer by conditionally selecting source samples only when they provably reduce target Type-II error.
- **Mechanism:** The algorithm defines restricted sets $\hat{H}'_{1,S}$ and $\hat{H}'_{1,T}$ (Eq. 6) based on empirical risk bounds. It explicitly checks for an intersection between these sets (Eq. 7). If the intersection is empty—meaning the source hypothesis that minimizes source risk cannot also bound target risk—the algorithm discards the source data and falls back to the target-only solution, thereby preventing negative transfer.
- **Core assumption:** Uniform convergence of empirical risks to population risks (bounded by Rademacher complexity in Assumption 1).
- **Evidence anchors:**
  - [abstract] "automatically adapt to situations where the source is uninformative, thereby avoiding negative transfer"
  - [section 4] Describes the conditional logic in Eq. 7 for selecting $\hat{h}$.
  - [corpus] Similar robustness is sought in "Efficient and Provable Algorithms for Covariate Shift," but this method specifically targets the asymmetry of NP classification.
- **Break condition:** If sample sizes are too small to reliably estimate the risk bounds ($\epsilon_{1,D}$), the sets $\hat{H}'_{1,D}$ may be misspecified, leading to either false positives (negative transfer occurring) or false negatives (failing to use helpful source data).

### Mechanism 3
- **Claim:** Reformulating the constrained learning problem as a sequence of convex programs allows for polynomial-time convergence with statistical guarantees.
- **Mechanism:** The authors utilize a Stochastic Gradient Descent-Ascent (SGDA) method (CP-Solver) to solve the resulting minimax optimization problems (e.g., Eq. 10). By enforcing convexity (Assumption 2) and Lipschitz continuity (Assumption 3), the algorithm navigates the trade-off between constraint satisfaction and loss minimization efficiently, avoiding the combinatorial hardness often associated with constrained 0-1 loss minimization.
- **Core assumption:** The hypothesis class $\mathcal{H}$ is convex and the loss function is convex and Lipschitz.
- **Evidence anchors:**
  - [abstract] "reformulating the learning procedure as a sequence of convex programs solved via stochastic optimization"
  - [section 5.2] Theorem 2 provides the computational guarantee (gradient complexity).
  - [corpus] "Learning Neural Networks with Distribution Shift" also relies on convex relaxations or specific structures for efficiency; here, convexity is a strict requirement for the guarantees.
- **Break condition:** If non-convex hypothesis classes (like standard deep neural networks without convex hull relaxations) are used, the convergence guarantees of Theorem 2 do not hold, and the algorithm may converge to local minima that violate constraints.

## Foundational Learning

- **Concept: Neyman-Pearson (NP) Classification**
  - **Why needed here:** This is the problem setting. Unlike standard classification which minimizes total error, NP classification minimizes Type-II error (missed detections) subject to a strict upper bound $\alpha$ on Type-I error (false alarms). Understanding this asymmetry is crucial to seeing why standard transfer learning fails here.
  - **Quick check question:** Can you explain why minimizing total accuracy might result in a classifier that violates the user's requirement of "no more than 5% false positives"?

- **Concept: Uniform Convergence & Rademacher Complexity**
  - **Why needed here:** The algorithm relies on empirical samples to estimate the "feasible sets" of classifiers. To guarantee these empirical sets reflect reality, we need bounds on how much empirical risk deviates from population risk. Rademacher complexity provides the theoretical glue for Theorem 1.
  - **Quick check question:** Why is a single validation set insufficient for selecting the threshold $\hat{\alpha}_S$, necessitating complexity-based bounds?

- **Concept: Transfer Modulus / Exponent**
  - **Why needed here:** This quantifies "relatedness." It measures how much a change in source error implies a change in target error. The method's adaptivity relies on these terms being small (for positive transfer) or the algorithm correctly detecting they are large (to avoid negative transfer).
  - **Quick check question:** If the Transfer Modulus $\phi^{S \to T}_1(\epsilon)$ is large, does this imply the source is "informative" or "uninformative"?

## Architecture Onboarding

- **Component map:**
  - **Input:** Labeled Source (Class 0 & 1), Labeled Target (Class 0 & 1)
  - **Module 1 (Threshold Estimator):** Implements Eq. 5. Uses CP-Solver (Alg 1) to find the alignment threshold $\hat{\alpha}_S$
  - **Module 2 (Set Restrictor):** Computes the intersection of feasible hypothesis sets (Eq. 6)
  - **Module 3 (Adaptive Selector):** Implements the logic in Eq. 7/Alg 2. Decides whether to use the joint Source-Target solution or fall back to Target-only

- **Critical path:**
  1. **Initialization:** Estimate reference errors using target data (Warm-start in Alg 2)
  2. **Alignment:** Compute $\hat{\alpha}_S$ via CP-Solver. *This is the most critical step for handling $\mu_0$ shift*
  3. **Minimization:** Run CP-Solver again on the restricted set to minimize Type-II error
  4. **Validation:** Check the adaptive condition (if intersection exists) to output the final model

- **Design tradeoffs:**
  - **Constraint Slackness vs. Feasibility:** The algorithm uses a slack parameter $\xi$ in the projection step of CP-Solver. Larger slack makes the optimization easier but risks violating the Type-I constraint $\alpha$. Tuning $\xi$ relative to the error tolerance $\epsilon$ is vital (see Alg 2 setup)
  - **Surrogate Loss:** The theory relies on convex surrogate losses (e.g., logistic). If the deployment requires strict 0-1 loss interpretation, the "proxy" nature of the surrogate must be accepted as an approximation

- **Failure signatures:**
  - **Infeasibility Error:** If CP-Solver fails to find $\hat{\alpha}_S$, it implies the source and target distributions are too disparate (no overlap in feasible sets)
  - **Constraint Leakage:** If Type-I error consistently exceeds $\alpha + \epsilon$ on test sets, the slackness parameters in Algorithm 2 are likely under-specified for the data complexity (violating the bounds in Theorem 1)

- **First 3 experiments:**
  1.  **Sanity Check (Synthetic):** Generate Gaussian data with fixed $\mu_1$ but shifting $\mu_0$ means. Vary the distance between $\mu_{0,S}$ and $\mu_{0,T}$ to verify that $\hat{\alpha}_S$ adapts correctly and Type-I error remains bounded
  2.  **Negative Transfer Test:** Use a source dataset that is intentionally "uninformative" (e.g., randomized labels). Verify that the algorithm successfully falls back to the "Only Target" baseline performance (Fig 3 behavior)
  3.  **Ablation on Sample Size:** Run the pipeline while decreasing target sample size $n_{0,T}$. Compare the degradation curve against a "Target-Only" baseline to visualize the benefit of the transfer mechanism in low-data regimes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the computational guarantees be extended to non-convex hypothesis classes, such as deep neural networks, without relying on convex hull relaxations?
- **Basis in paper:** [inferred] Assumptions 2 and 3 require the hypothesis class and loss to be convex to establish the computational guarantee via convex programming. The authors note that neural networks generally do not satisfy these closure properties directly.
- **Why unresolved:** The proposed Algorithm 2 relies on stochastic convex optimization (SGDA) solvers which require convexity to guarantee convergence to the statistical rate in polynomial time.
- **What evidence would resolve it:** A modification of the optimization procedure (e.g., using differentiable optimization or surrogate management) that provably converges for non-convex models while maintaining Type-I error constraints.

### Open Question 2
- **Question:** How does the adaptive procedure extend to the multi-source transfer learning setting where multiple source domains with varying degrees of shift are available?
- **Basis in paper:** [inferred] The problem setup explicitly defines a single source domain $S$ and a single target domain $T$. The alignment of thresholds ($\hat{\alpha}_S$) is calculated based on a specific source-target pair.
- **Why unresolved:** It is unclear how to aggregate information or select among multiple sources to determine the effective threshold $\alpha_S$ when sources differ significantly in their relatedness to the target.
- **What evidence would resolve it:** An extension of Theorem 1 providing generalization bounds for a multi-source algorithm that adaptively weights sources based on their transfer modulus.

### Open Question 3
- **Question:** Are the derived generalization bounds expressed via the transfer modulus $\phi(\epsilon)$ minimax optimal?
- **Basis in paper:** [inferred] The paper establishes upper bounds (Theorem 1) and mentions recovering rates from prior works, but does not provide information-theoretic lower bounds for the general double-shift setting.
- **Why unresolved:** Without lower bounds, it is undetermined if the reliance on the transfer modulus is a fundamental limitation of the problem class or a limitation of the proposed two-stage procedure.
- **What evidence would resolve it:** Deriving a matching minimax lower bound that depends on the same transfer modulus parameters defined in Definition 4.

## Limitations
- **Convexity Requirement:** The computational guarantees rely on convexity assumptions (Assumption 2), limiting applicability to non-convex models like deep neural networks without additional relaxations or approximations.
- **Unbounded Shift:** While the method handles arbitrary shifts in both μ₀ and μ₁, extreme shifts may result in empty feasible sets, causing the algorithm to fall back to target-only performance.
- **Sample Complexity:** The error bounds scale with 1/√n, requiring substantial target samples for tight constraint satisfaction in high-dimensional settings.

## Confidence
- **High Confidence:** The theoretical framework (Theorem 1) for avoiding negative transfer and the computational guarantees (Theorem 2) are rigorously derived and mathematically sound.
- **Medium Confidence:** The empirical validation on climate datasets demonstrates effectiveness, but the results are limited to specific domains and moderate sample sizes.
- **Low Confidence:** The exact implementation details for non-convex hypothesis classes and the practical performance under extreme distribution shifts remain unexplored.

## Next Checks
1. **Extreme Shift Test:** Evaluate the algorithm on synthetic data with increasing distances between μ₀,S and μ₀,T to quantify the threshold at which performance degrades to target-only baseline.
2. **Non-Convex Extension:** Implement the method with neural networks using convex relaxations (e.g., convex hulls of weights) and compare computational efficiency against the theoretical guarantees.
3. **Sample Efficiency Analysis:** Systematically vary target sample sizes (n₀,T, n₁,T) and measure the trade-off between constraint satisfaction (Type-I error) and Type-II error reduction compared to target-only baselines.