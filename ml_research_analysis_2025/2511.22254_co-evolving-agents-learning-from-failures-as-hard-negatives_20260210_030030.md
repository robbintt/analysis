---
ver: rpa2
title: 'Co-Evolving Agents: Learning from Failures as Hard Negatives'
arxiv_id: '2511.22254'
source_url: https://arxiv.org/abs/2511.22254
tags:
- agent
- failure
- hard
- agents
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes a framework where two agents\u2014a target\
  \ and a failure agent\u2014co-evolve to improve learning from failures. The failure\
  \ agent is trained solely on failure trajectories to generate high-quality \"hard\
  \ negatives,\" which are near-success failures that provide stronger supervision\
  \ than typical failure samples."
---

# Co-Evolving Agents: Learning from Failures as Hard Negatives

## Quick Facts
- **arXiv ID:** 2511.22254
- **Source URL:** https://arxiv.org/abs/2511.22254
- **Reference count:** 20
- **Primary result:** Co-evolving agents with a failure agent generating hard negatives improves average reward by 4.8-6.8% over baselines

## Executive Summary
This paper introduces a framework where two agents co-evolve: a target agent being optimized for task success and a failure agent dedicated to generating high-quality "hard negatives" from failure trajectories. The failure agent learns to distinguish between different levels of failure quality, producing near-success failures that provide stronger contrastive signals than typical failures. These hard negatives are then incorporated into the target agent's preference optimization, leading to improved decision boundaries and generalization. Experiments on WebShop, ScienceWorld, and InterCodeSQL benchmarks show consistent performance gains across different model architectures, demonstrating that systematically leveraging failures can be more effective than treating them as mere byproducts.

## Method Summary
The framework trains two agents in an alternating co-evolutionary loop. Both start from the same SFT-initialized base policy. The target agent is trained with preference optimization using a dataset combining expert trajectories with both regular failures and hard negatives generated by the failure agent. The failure agent is trained exclusively on pairs of failure trajectories, learning to prefer higher-reward failures. As training progresses, the failure agent produces increasingly sophisticated hard negatives from the target agent's failures, creating an arms race that sharpens decision boundaries. The method uses DPO with a weighted loss combining preference optimization and auxiliary SFT loss for the target agent.

## Key Results
- Average reward improvements of 4.8-6.8% over competitive baselines across WebShop, ScienceWorld, and InterCodeSQL benchmarks
- Failure agents generate more diverse and structured failures compared to standard expert-to-failure comparisons
- Consistent performance gains observed across different model architectures (Llama-2-7B, Llama-2-13B, Qwen3-4B)
- Largest improvements on unseen ScienceWorld tasks suggest enhanced generalization

## Why This Works (Mechanism)

### Mechanism 1: Hard Negatives via Failure-Failure Contrastive Learning
- **Claim:** Training a dedicated failure agent to distinguish between high-reward and low-reward failure trajectories generates "hard negatives" that provide stronger learning signals than raw failures.
- **Core assumption:** Failure trajectories contain latent structure and reward signals that can be learned and exploited to identify "better" failures.
- **Evidence:** Performance improvements and qualitative analysis showing failure agents produce near-success failures with structured behavior.

### Mechanism 2: Co-evolutionary Training for a Sharpened Preference Landscape
- **Claim:** Alternating training of target and failure agents creates a co-evolutionary dynamic that sharpens the target agent's decision boundaries and improves generalization.
- **Core assumption:** Better agents produce more informative failures, and learning from more informative failures leads to better agents.
- **Evidence:** The framework's ability to consistently improve performance through alternating training iterations.

### Mechanism 3: Improved Generalization via Diverse and Structured Contrastive Signals
- **Claim:** Using structured, near-success hard negatives in preference optimization provides more diverse and informative learning signals than simple expert-vs-failure pairs.
- **Core assumption:** Features learned to distinguish hard negatives from successes are transferable and essential for task completion.
- **Evidence:** Largest improvements on unseen ScienceWorld tasks suggest enhanced generalization.

## Foundational Learning

- **Direct Preference Optimization (DPO):** The core learning algorithm for both agents, optimizing policy based on pairwise preferences.
  - *Why needed:* Enables optimization without explicit reward models during policy updates.
  - *Quick check:* Can you explain how DPO bypasses the need for an explicit reward model during policy optimization?

- **Hard Negatives in Contrastive Learning:** Near-positive samples that are difficult to distinguish from positive examples.
  - *Why needed:* The central innovation of generating informative failure samples that improve learning.
  - *Quick check:* Why are "hard negatives" considered more valuable for training a classifier than "easy negatives"?

- **Partially Observable Markov Decision Process (POMDP):** Formalizes the LLM agent's interaction with its environment.
  - *Why needed:* Provides the framework for defining trajectories, states, actions, and rewards.
  - *Quick check:* In the context of an LLM agent, what constitutes the observation and the state within the POMDP formulation?

## Architecture Onboarding

- **Component map:**
  - **Target Agent (πθt)** -> Trained with SFT + DPO on expert trajectories + hard negatives
  - **Failure Agent (πθf)** -> Trained with DPO on failure-failure pairs only
  - **Shared Base Policy (πbase)** -> Initial SFT model for both agents
  - **Preference Datasets** -> `Dfail` (failure pairs) and `Dtgt` (mixed pairs)
  - **Trajectory Buffer** -> Stores generated trajectories and rewards

- **Critical path:** Alternating training loop: (1) both agents generate trajectories, (2) trajectories evaluated and sorted into preference datasets, (3) failure agent trained on `Dfail`, (4) target agent trained on `Dtgt`, (5) repeat.

- **Design tradeoffs:**
  - Co-evolution vs. single agent: More complex but produces higher-quality learning signals
  - Failure agent specialization: Focused on failure landscape but may miss success patterns
  - Auxiliary SFT loss: Balances pure preference optimization with stability

- **Failure signatures:**
  - Failure agent collapses to trivial failures
  - Reward hacking exploits loopholes rather than generating genuine near-successes
  - Target agent overfits to expert trajectories
  - Training instability in co-evolutionary loop

- **First 3 experiments:**
  1. Baseline comparison: Standard ETO vs. co-evolutionary framework on all three benchmarks
  2. Failure agent ablation: Remove co-evolving failure agent or replace with frozen model
  3. Hard negative quality analysis: Qualitative inspection of failure agent-generated failures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the failure agent be optimized to consistently generate "super-hard" negatives with rewards above 0.7?
- **Basis:** Current threshold is 0.6 due to rarity of higher-reward failures (<1% occurrence)
- **What would resolve:** Modified training objective pushing failure agent to 0.7-0.8 reward range

### Open Question 2
- **Question:** What are the trade-offs between computational cost and performance when increasing exploration rollouts?
- **Basis:** Currently limited to single rollout due to computational costs of long trajectories
- **What would resolve:** Ablation study comparing performance with k=1 vs. k>1 rollouts per instruction

### Open Question 3
- **Question:** Can a failure agent trained on one domain transfer its hard negative generation to a different domain?
- **Basis:** Framework trains domain-specific target-failure pairs; cross-domain transfer untested
- **What would resolve:** Experiments with failure agent trained on WebShop generating negatives for InterCodeSQL

### Open Question 4
- **Question:** How does the method's efficacy scale with base model reasoning capabilities?
- **Basis:** Benefits of hard negative mining on 7B/13B models unclear for more capable models
- **What would resolve:** Scaling laws analysis across increasing model sizes

## Limitations

- Dependence on high-quality reward functions makes method vulnerable to reward model inaccuracies
- Theoretical foundation connecting failure landscape geometry to learning signal quality remains underdeveloped
- Current evaluation limited to relatively constrained decision-making tasks with short trajectories

## Confidence

- **High Confidence:** Experimental results showing 4.8-6.8% average reward gains across three benchmarks with different model architectures
- **Medium Confidence:** Claim that co-evolutionary training creates sharper decision boundaries versus other preference optimization methods
- **Low Confidence:** Scalability and stability of the co-evolutionary loop in more complex, real-world environments

## Next Checks

1. **Reward Sensitivity Analysis:** Systematically vary hard negative thresholds (0.6, 0.7, 0.8) to test robustness to different reward granularities

2. **Single-Agent Ablation with Expert-Designed Hard Negatives:** Replace co-evolving failure agent with static model generating expert-designed hard negatives

3. **Cross-Domain Failure Transfer:** Train failure agent on one benchmark and test its hard negative generation quality on target agents from different domains