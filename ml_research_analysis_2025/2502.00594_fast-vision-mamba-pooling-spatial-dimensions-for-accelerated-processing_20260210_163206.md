---
ver: rpa2
title: 'Fast Vision Mamba: Pooling Spatial Dimensions for Accelerated Processing'
arxiv_id: '2502.00594'
source_url: https://arxiv.org/abs/2502.00594
tags:
- pooling
- fastvim
- mamba
- tokens
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Fast Vision Mamba (FastVim), which accelerates
  Vision Mamba (Vim) by applying spatial pooling to reduce the number of recurrent
  steps in the SSM block. By alternating pooling along image dimensions, FastVim maintains
  model performance while achieving up to 72.5% speedup in inference speed for high-resolution
  images.
---

# Fast Vision Mamba: Pooling Spatial Dimensions for Accelerated Processing

## Quick Facts
- **arXiv ID**: 2502.00594
- **Source URL**: https://arxiv.org/abs/2502.00594
- **Reference count**: 40
- **Primary result**: Up to 72.5% speedup in inference speed for high-resolution images while maintaining model performance

## Executive Summary
Fast Vision Mamba (FastVim) accelerates Vision Mamba models by applying spatial pooling to reduce the number of recurrent steps in the selective state space model (SSM) block. By alternately pooling tokens along image dimensions, FastVim achieves a 2× reduction in parallel scan steps while maintaining performance comparable to baseline Vision Mamba. The method demonstrates state-of-the-art performance across multiple vision tasks including image classification, cell perturbation prediction, segmentation, and object detection, with particular effectiveness at high resolutions.

## Method Summary
FastVim introduces spatial pooling before the SSM scan operation in Vision Mamba blocks. The method alternates pooling across rows and columns across successive blocks via transposition operations. After pooling tokens along one spatial dimension (reducing sequence length from h×w to h), the SSM scan processes the compressed representation, which is then repeated back to original dimensions before the skip connection. This approach maintains information flow while reducing computational complexity from quadratic to linear with resolution. The architecture requires post-SSM LayerNorm for stability in base-sized and larger models, and employs RMSNorm at the input of each block.

## Key Results
- Achieves up to 72.5% speedup in inference speed for high-resolution images (2048×2048)
- FastMaskVim achieves new state-of-the-art performance for Mamba-based encoders on ImageNet-1k (86.7% top-1 accuracy) when pretrained with MAE
- Maintains competitive performance across image classification, cell perturbation prediction, semantic segmentation, and object detection tasks
- Reduces FLOPs by approximately 35% compared to baseline Vision Mamba while preserving accuracy

## Why This Works (Mechanism)

### Mechanism 1: Spatial Pooling Reduces Parallel Scan Steps from Quadratic to Linear with Resolution
Pooling tokens along one spatial dimension before the SSM scan reduces parallel scan steps from log(h²) to log(h), where h is the token dimension along height/width. Mean pooling compresses the 2D token grid from h×w tokens to h tokens before the selective scan operation. Since the parallel scan algorithm requires log(L) steps for L tokens, reducing L from h² to h cuts scan steps by half in logarithmic terms.

### Mechanism 2: Alternating Pooling Dimensions Enables Implicit Global Token Interaction
Alternating pooling across rows and columns across successive blocks enables all tokens to interact implicitly across the network depth without full quadratic attention. Pooling tokens across columns prevents tokens in the same row from interacting during that block's SSM scan, while pooling across rows similarly isolates column neighbors. This alternation allows row-wise interactions in one block and column-wise in the next.

### Mechanism 3: Post-SSM LayerNorm Stabilizes Training in Large Vim Models
Adding LayerNorm after the SSM scan in each block prevents loss spikes and stabilizes convergence for base-sized and larger Vim models. The authors observed loss spikes during Vim-B training with default settings. Adding LayerNorm post-SSM eliminated instability and improved accuracy from 81.2% to 82.6%.

## Foundational Learning

- **Concept: State Space Models (SSMs) and Discretization**
  - **Why needed here**: FastVim builds on Mamba, which uses selective SSMs with input-dependent parameters. Understanding how continuous SSMs discretize is essential for grasping why parallel scan is required and how pooling reduces scan steps.
  - **Quick check question**: Given a sequence of L tokens, how many parallel scan steps are needed vs. sequential steps? (Answer: log(L) parallel vs. L sequential.)

- **Concept: Selective Scan in Mamba**
  - **Why needed here**: Mamba makes B, C, Δ input-dependent, enabling content-based reasoning but eliminating convolution-based parallelism. This is why pooling before scan directly impacts computational cost.
  - **Quick check question**: Why can't Mamba use convolution like S4 for efficient computation? (Answer: Input-dependent parameters break LTI assumption required for convolution equivalence.)

- **Concept: Vision Mamba (Vim) Tokenization**
  - **Why needed here**: Vim converts images to patch tokens (L = H×W/P²) with bidirectional forward/backward SSM scans. FastVim modifies this by pooling tokens before each scan and repeating afterward.
  - **Quick check question**: For a 224×224 image with patch size 16, how many tokens does Vim process per SSM scan? (Answer: (224/16)² = 196 tokens.)

## Architecture Onboarding

- **Component map**: Image → Patch embedding → Token sequence → Norm → Linear expansion → Transpose → Pool (mean across columns) → Conv1d → SSM (forward + backward) → Repeat (restore grid) → Skip connection → Norm

- **Critical path**: 1. Token grid reshape (L → h×w), 2. Pool across one dimension (h×w → h), 3. SSM scan with compressed tokens, 4. Repeat output (h → h×w), 5. Add skip connection, 6. Transpose for next block

- **Design tradeoffs**: Mean pooling is default and fastest; max/attention pooling may help for longer sequences but add overhead. Post-SSM norm improves stability but reduces throughput ~5-10%; essential for base+ models. Skip connection must occur after repeat operation.

- **Failure signatures**: Training loss spikes in base/large models → Missing post-SSM LayerNorm. Accuracy drop of 1-2% vs baseline → Pooling dimension not alternating. MAE linear probing fails → Missing scaling factor (0.25) during transfer. Throughput not improving at high resolution → Check if pooling/repeat operations are fused in kernel.

- **First 3 experiments**:
  1. **Ablate pooling alternation**: Train FastVim-T on ImageNet-1k with Pool_col only vs. alternating pooling. Expect ~1% gap.
  2. **Profile SSM scan time vs. resolution**: Measure forward+backward SSM time for Vim-T vs. FastVim-T at resolutions 224, 512, 1024, 2048. Expect FastVim SSM time to remain nearly constant while Vim increases ~74×.
  3. **Test post-SSM norm impact on stability**: Train Vim-B with and without LayerNorm post-SSM, monitoring for loss spikes. Expect stable training with norm, spikes without.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a refined pretraining recipe enable Vision Mamba models to match or surpass Vision Transformers in Masked Autoencoder (MAE) frameworks? The authors acknowledge that "ViT excels in pretraining with MAE... highlighting the need for more exploration of MAE pretraining/fine-tuning recipe for the Vision Mamba."

- **Open Question 2**: Does advanced pooling (max or attention-based) provide significant accuracy gains over mean pooling for FastVim on natural image datasets? The supplement notes that while max/attention pooling showed promise on microscopy data, "Exploring effect of these pooling operation in natural imaging is left for future studies."

- **Open Question 3**: How does the alternating pooling mechanism scale to extreme sequence lengths in gigapixel histopathology imaging or the temporal dimension of video? The conclusion lists applying FastVim to "gigapixel imaging, such as histopathology... as well as in video domain" as future work.

## Limitations

- The method's performance on tasks with weaker spatial priors (e.g., tabular data, point clouds) remains untested
- Without kernel fusion, repeat overhead can reach 25% of block time at extreme resolutions (2048×2048), potentially reducing practical gains
- The stability findings for base/large models are based on empirical observation rather than theoretical analysis

## Confidence

**High Confidence**:
- Spatial pooling reduces parallel scan steps from log(h²) to log(h), enabling 2× speedup in SSM computation
- Alternating pooling dimensions is necessary for achieving baseline-level performance
- Post-SSM LayerNorm stabilizes training for base/large models
- FastVim achieves 72.5% speedup at 2048×2048 resolution with minimal accuracy loss

**Medium Confidence**:
- MAE pretraining scales effectively to FastVim (86.7% top-1 with FastMaskVim)
- Pooling/repeat operations maintain sufficient information for downstream tasks
- Throughput gains are maintained at extreme resolutions with optimized implementations

**Low Confidence**:
- Method generalizes to non-spatial or weakly-spatial domains
- Alternating pooling is optimal versus other sparse interaction strategies
- Performance scaling holds beyond tested model sizes and resolutions

## Next Checks

1. **Resolution scaling validation**: Train FastVim-B at resolutions 224, 512, 1024, and 2048 to verify the claimed 72.5% speedup at extreme resolution. Measure both FLOPs reduction and actual wall-clock throughput, ensuring pooling/repeat operations are kernel-fused to eliminate the 25% repeat overhead observed in naive implementations.

2. **Pooling strategy ablation**: Systematically compare mean pooling versus max pooling and attention pooling for FastVim-T across ImageNet-1k, COCO, and ADE20K. Validate the trade-off between accuracy and computational overhead at different resolutions.

3. **Stability threshold identification**: Train a series of models from Vim-S to Vim-B with and without post-SSM LayerNorm, monitoring loss stability and accuracy. Identify the minimal model size at which instability manifests and determine whether this threshold depends on model width, depth, or both.