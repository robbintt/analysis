---
ver: rpa2
title: 'Feature-Guided Analysis of Neural Networks: A Replication Study'
arxiv_id: '2511.00052'
source_url: https://arxiv.org/abs/2511.00052
tags:
- recall
- rules
- test
- neural
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Feature-Guided Analysis (FGA) is a technique for interpreting neural
  network decisions by extracting rules based on neuron activations. This replication
  study evaluates FGA on MNIST and LSC datasets, as the original benchmarks were unavailable.
---

# Feature-Guided Analysis of Neural Networks: A Replication Study

## Quick Facts
- arXiv ID: 2511.00052
- Source URL: https://arxiv.org/abs/2511.00052
- Reference count: 40
- One-line primary result: FGA achieves higher test precision on new benchmarks than originally reported, with stable precision and variable recall depending on architecture and training

## Executive Summary
Feature-Guided Analysis (FGA) is a technique for interpreting neural network decisions by extracting rules based on neuron activations. This replication study evaluates FGA on MNIST and LSC datasets, as the original benchmarks were unavailable. The authors reimplemented FGA and compared its effectiveness to the original results, measuring precision and recall of extracted rules. Results show that FGA achieves higher test precision on the new benchmark than originally reported, with comparable or better recall. The study also found that the choice of neural network architecture and training significantly impacts recall, while precision remains stable. Feature selection affects recall more than precision. Overall, FGA is validated as a reliable interpretability method, especially when high precision is required, though recall can be improved by careful neural network and training selection. The work includes a complete replication package to support reproducibility.

## Method Summary
The authors reimplemented Feature-Guided Analysis (FGA) to evaluate its effectiveness in interpreting neural network decisions through neuron activation-based rules. Since the original benchmarks were unavailable, the study used MNIST and LSC datasets for testing. The replication compared the reimplemented FGA's precision and recall against the original reported results. The method involved extracting rules from neuron activations and measuring their accuracy in capturing the neural network's decision-making process. The study also investigated how different neural network architectures and training regimes affect the recall of extracted rules, while maintaining stable precision. Feature selection's impact on recall versus precision was also analyzed. A complete replication package was provided to ensure reproducibility of the results.

## Key Results
- FGA achieves higher test precision on the new benchmark than originally reported
- Recall is significantly impacted by neural network architecture and training choices
- Precision remains stable across different configurations, while feature selection affects recall more than precision

## Why This Works (Mechanism)
FGA works by extracting interpretable rules from neuron activations in neural networks. The mechanism relies on identifying which neurons contribute most to classification decisions and formalizing these contributions as human-readable rules. When precision is prioritized, FGA successfully captures the most salient features that drive network decisions, leading to higher test precision on benchmarks. The stability of precision across different architectures suggests that the rule extraction process effectively identifies core decision-making patterns regardless of model complexity. However, recall varies with architecture and training because different neural network configurations encode information differently in their activation patterns, affecting how comprehensively FGA can capture all relevant decision factors.

## Foundational Learning
- Neural network interpretability methods: Why needed - to understand and trust AI decisions; Quick check - can you explain how LIME or SHAP differs from rule-based approaches?
- Neuron activation analysis: Why needed - forms the basis of FGA's rule extraction; Quick check - can you describe what neuron activations represent in classification tasks?
- Precision and recall metrics in interpretability: Why needed - to quantify the effectiveness of extracted rules; Quick check - can you calculate precision and recall from a confusion matrix of rule coverage?
- Feature selection impact on model behavior: Why needed - understanding how input representation affects interpretability; Quick check - can you explain how different feature subsets might change rule extraction outcomes?

## Architecture Onboarding
Component map: Input data -> Neural network training -> Neuron activation extraction -> Rule generation -> Precision/recall evaluation
Critical path: Data preparation → Model training → Activation analysis → Rule extraction → Performance measurement
Design tradeoffs: The method balances between comprehensive rule coverage (high recall) and rule accuracy (high precision), with architecture choice affecting this balance
Failure signatures: Low recall indicates incomplete capture of decision factors; stable precision suggests robust core pattern identification
First experiments: 1) Test FGA on simple linear models to establish baseline precision/recall; 2) Compare FGA across different MNIST architectures (CNN vs fully connected); 3) Evaluate impact of feature normalization on rule extraction quality

## Open Questions the Paper Calls Out
None

## Limitations
- Absence of access to the original authors' implementation may have missed optimizations or nuances
- Reliance on reimplementation rather than direct comparison introduces potential methodological discrepancies
- Results may not generalize beyond MNIST and LSC datasets or across all model configurations

## Confidence
- High confidence: FGA achieves higher test precision on the new benchmark than originally reported
- Medium confidence: FGA is validated as a reliable interpretability method when high precision is required
- Low confidence: Feature selection affects recall more than precision, as this conclusion is based on a limited set of architectures and datasets

## Next Checks
1. Validate findings on additional datasets beyond MNIST and LSC to assess generalizability
2. Test FGA across a broader range of neural network architectures and training regimes to confirm the impact on recall
3. Compare results with the original authors' implementation (if available) to identify potential discrepancies in methodology or optimization