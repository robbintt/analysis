---
ver: rpa2
title: When to Say "Hi" -- Learn to Open a Conversation with an in-the-wild Dataset
arxiv_id: '2512.03991'
source_url: https://arxiv.org/abs/2512.03991
tags:
- robot
- interaction
- data
- user
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the Interaction Initiation System (IIS), which
  aims to determine the optimal timing and manner for a socially interactive agent
  to initiate conversations with users. The system uses machine learning models trained
  on in-the-wild data collected from 201 interactions at a museum, where a Furhat
  robot served as an information point.
---

# When to Say "Hi" -- Learn to Open a Conversation with an in-the-wild Dataset

## Quick Facts
- arXiv ID: 2512.03991
- Source URL: https://arxiv.org/abs/2512.03991
- Reference count: 18
- Primary result: Interaction Initiation System achieves 69% macro F1-score for predicting conversation initiation timing from body language

## Executive Summary
This paper presents the Interaction Initiation System (IIS), a machine learning approach for determining when and how a socially interactive robot should initiate conversations with users. The system processes video data using pose estimation and forecasting to predict whether the robot should greet first, wait, or listen to the user. Trained on 201 spontaneous museum interactions, the IIS demonstrates that machine learning can predict conversation initiation timing from body language, though performance on the "listen" class remains challenging due to subtle mouth movement detection.

## Method Summary
The IIS uses a two-stage pipeline: a BlockRNN time series model forecasts future human poses from the last 10 frames (1 second at 10 fps) to predict the next 5 frames (0.5 seconds ahead), giving the robot reaction time. An SVM classifier with RBF kernel then processes either direct pose features or predicted poses to classify user intent into three actions: "wait" (user too far/uncertain), "speak" (robot should greet first), or "listen" (user will speak first). The system extracts 1,682 features per frame from MediaPipe, including 543 body, face, and hand landmarks plus 52 facial expression shapes, with visibility thresholds of 0.5 for landmark validity.

## Key Results
- Timing classifier achieves macro F1-score of 69% and weighted F1-score of 74% on test data
- Action classifier alone achieves 75.3% accuracy when classifying "wait," "speak," and "listen" actions
- Human pose forecasting model achieves RMSE of 0.0426 on pose predictions
- "Listen" class performance is notably lower due to subtle mouth movement detection challenges at 10 fps

## Why This Works (Mechanism)

### Mechanism 1: Pose Forecasting for Anticipatory Response
Predicting future body positions enables the system to classify greeting intent before the user acts. A BlockRNN time series model consumes the last 10 frames of pose data (1 second at 10 fps) and forecasts the next 5 frames (0.5 seconds ahead). These predicted poses are then classified, giving the robot reaction time to enter "listen" mode before the user speaks. Core assumption: Body pose trajectories follow predictable patterns that correlate with conversational intent.

### Mechanism 2: Multi-Signal Feature Fusion for Action Classification
Combining body landmarks, facial expression shapes, and hand positions provides discriminative signals for three-way action classification. MediaPipe extracts 543 landmarks per frame (33 body, 468 face, 42 hand points), plus 52 facial expression shape coefficients. An SVM with RBF kernel maps this 1,682-dimensional feature vector to "wait," "speak," or "listen" classes. Core assumption: The three intent classes have separable representations in high-dimensional pose space.

### Mechanism 3: In-the-Wild Data for Natural Behavior Generalization
Training on spontaneous, uninstructed museum visitor interactions produces models that generalize to real deployment conditions. Data was collected during regular museum hours without task instructions. Users approached the robot naturally, and the system randomly alternated between greeting first or waiting, capturing diverse opening behaviors. Core assumption: Spontaneous user behavior in semi-public spaces represents the target deployment distribution better than laboratory-collected data.

## Foundational Learning

- **Time Series Forecasting with BlockRNN**
  - Why needed here: The system must predict future poses to give the robot time to switch modes before a user speaks.
  - Quick check question: Given 10 fps video, why does forecasting 5 frames ahead provide only 0.5 seconds of lead time?

- **Multi-Class Classification with Class Imbalance**
  - Why needed here: The dataset is skewed ("wait": 12,898, "listen": 6,542, "speak": 7,235), requiring careful metric selection.
  - Quick check question: Why does the macro F1-score (69%) differ from the weighted F1-score (74%), and which better reflects minority class performance?

- **Pose Estimation Robustness**
  - Why needed here: MediaPipe landmark visibility varies with camera angle, user height, and occlusion, directly affecting feature quality.
  - Quick check question: What happens to landmark coordinates when visibility falls below the 0.5 threshold?

## Architecture Onboarding

- **Component map:**
  Video Input (3 cameras, 10 fps) -> YOLO Person Detection -> Consent Verification -> MediaPipe Feature Extraction (1,682 features/frame) -> [Frames 1-10] Direct -> Action Classifier (SVC) OR [Frame 11+] -> Human Pose Forecasting (BlockRNN) -> Action Classifier -> Decision Output: "wait" | "speak" | "listen"

- **Critical path:**
  1. User enters interaction area → YOLO detects presence
  2. User presses consent buzzer → system exits idle state
  3. MediaPipe begins extracting pose features from front camera
  4. Frames 1-10: Action Classifier directly processes features
  5. Frame 11+: BlockRNN forecasts 5 frames ahead, Action Classifier processes predicted poses
  6. If "speak" → robot initiates greeting; if "listen" → robot enters listening mode; if "wait" → continue monitoring

- **Design tradeoffs:**
  - Frame rate (10 fps): Lower computational cost but may miss rapid mouth movements; paper notes higher fps would improve speech movement detection
  - Feature set (all 1,682 vs facial-only 1,404): All features yield 75.3% accuracy; facial-only drops to 69.5%. Body landmarks contribute to "wait" vs "speak" discrimination
  - Train/test split (89%/11%): Test set contains only 22 interactions (11 female, 11 male), limiting statistical robustness of performance estimates

- **Failure signatures:**
  - "Listen" class underperformance: Confusion matrix shows 318 correct vs 614 incorrect (380 as "speak", 234 as "wait"). Caused by subtle mouth movements that are difficult to detect at 10 fps
  - Pose noise: MediaPipe accuracy affected by recording angle and movement type (cited: Dill et al., 2023), propagating errors through forecasting
  - Tall/short user detection failures: YOLO may miss users outside typical height range, causing early termination

- **First 3 experiments:**
  1. Reproduce baseline: Train SVC with reported hyperparameters (C=1, gamma="scale", RBF kernel, balanced class weights) on the 201-interaction dataset; verify ~75% accuracy and identify per-class failure modes
  2. "Listen" class augmentation: Apply SMOTE or increase class weight for "listen" class; measure macro F1-score improvement on minority class specifically
  3. Higher frame rate test: Re-record subset of interactions at 30 fps with higher-resolution camera; evaluate whether mouth movement detection improves "listen" classification

## Open Questions the Paper Calls Out

### Open Question 1
Does the Type Classifier component appropriately select greeting formulas that users perceive as natural and suitable for the context? The authors state they did not evaluate the "Type Classifier" component for selecting the greeting type, noting future studies should investigate whether the greeting behavior is perceived as natural and whether the selected greeting formula is appropriate.

### Open Question 2
How significantly would a larger, more balanced dataset improve Timing Classifier performance, particularly for the "listen" class? The authors note the current approach would likely yield better performance with a larger dataset, as there would be sufficient training examples for all classes, and that the "listen" class is distinguished only by minimal mouth movements which are difficult to recognize and predict.

### Open Question 3
Can synthetic data generated from user simulation improve IIS performance compared to real training data alone? The authors suggest it might be beneficial to use an approach that uses user simulation with synthetic data based on real training data to achieve better results.

### Open Question 4
How transferable is the IIS approach to mobile robot scenarios with changing camera perspectives? The authors state that in scenarios with a mobile robot, the approach is transferable to a limited extent, as a changing camera perspective influences the recording of the user and therefore larger amounts of data are required.

## Limitations
- Small test set size (22 interactions) limits statistical significance of performance metrics
- "Listen" class performance remains problematic due to subtle mouth movement detection challenges at 10 fps
- BlockRNN architecture details are underspecified, making exact reproduction difficult

## Confidence
- **High Confidence**: Overall system architecture and data collection methodology are well-documented
- **Medium Confidence**: Reported performance metrics are based on test data but small test set size limits statistical significance
- **Low Confidence**: BlockRNN model architecture specifics are not fully specified; exact integration of predicted poses with SVM classifier is unclear

## Next Checks
1. **Class-wise Performance Validation**: Re-run evaluation focusing exclusively on the "listen" class, applying SMOTE or weighted loss to address class imbalance and measure macro F1-score improvement for minority classes
2. **Higher Frame Rate Validation**: Record a subset of interactions at 30 fps with higher-resolution cameras, comparing "listen" class detection rates to determine if frame rate limitations are the primary bottleneck
3. **Cross-Context Deployment Test**: Deploy the trained model in a different context (e.g., office reception, conference booth) and measure performance degradation to validate generalization beyond the museum setting