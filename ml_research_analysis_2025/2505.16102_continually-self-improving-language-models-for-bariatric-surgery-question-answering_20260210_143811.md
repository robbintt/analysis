---
ver: rpa2
title: Continually Self-Improving Language Models for Bariatric Surgery Question--Answering
arxiv_id: '2505.16102'
source_url: https://arxiv.org/abs/2505.16102
tags:
- surgery
- bariatric
- language
- clinical
- braggen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces bRAGgen, an adaptive retrieval-augmented generation
  system for bariatric surgery patient education. It autonomously retrieves and integrates
  real-time medical evidence when model confidence falls below dynamic thresholds,
  ensuring responses stay current and clinically accurate.
---

# Continually Self-Improving Language Models for Bariatric Surgery Question--Answering

## Quick Facts
- arXiv ID: 2505.16102
- Source URL: https://arxiv.org/abs/2505.16102
- Reference count: 40
- bRAGgen with Llama3-8B achieved 4.51/5 (expert) and 4.44/5 (LLM-as-Judge) on bariatric surgery QA

## Executive Summary
bRAGgen is an adaptive retrieval-augmented generation system for bariatric surgery patient education that autonomously retrieves and integrates real-time medical evidence when model confidence falls below dynamic thresholds. The system uses perplexity-based monitoring to trigger web retrieval from authoritative .gov/.edu sources, applies low-rank adaptation for efficient domain updates, and maintains a semantic cache for retrieval efficiency. Evaluated against baselines using expert surgeon review and LLM-as-Judge, bRAGgen with Llama3-8B achieved the highest scores across factuality, clinical relevance, and comprehensiveness metrics. The work also introduces bRAGq, a 1,302-question benchmark validated by bariatric surgeons covering the full spectrum of patient concerns.

## Method Summary
The system S = (C, R, G, L) integrates semantic caching (BioClinicalBERT+Faiss with τc=0.7, cache size 500), multi-source web retrieval via DuckDuckGo API with BM25 ranking prioritizing .gov/.edu domains, Llama3-8B with LoRA (rank r=32), and perplexity-triggered retrieval (τp=4.5). Online learning uses an experience buffer with diversity sampling and regularized cross-entropy loss. The bRAGq dataset contains 1,302 bariatric surgery questions validated by expert surgeons. Evaluation used three metrics (Factuality, Clinical Relevance, Comprehensiveness) on 5-point Likert scales, assessed by both expert surgeons and LLM-as-Judge (ChatGPT-4o).

## Key Results
- bRAGgen with Llama3-8B achieved 4.51/5 average expert rating and 4.44/5 LLM-as-Judge score across three metrics
- Outperformed offline RAG, zero-shot, and context-prompted baselines on factuality, clinical relevance, and comprehensiveness
- Demonstrated consistent improvement over time through autonomous knowledge integration
- Introduced bRAGq benchmark with 1,302 validated bariatric surgery questions across seven clinical categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system autonomously retrieves external evidence when internal confidence falls below dynamic thresholds.
- Mechanism: Perplexity-based confidence monitoring triggers web retrieval; if perplexity P(r|q) exceeds threshold τp = 4.5, the system retrieves from authoritative biomedical sources and re-generates the response.
- Core assumption: Perplexity correlates with factual uncertainty and response quality in medical QA contexts.
- Evidence anchors:
  - [abstract]: "autonomously integrates real-time medical evidence when response confidence dips below dynamic thresholds"
  - [section 4.3]: "If the perplexity of the response exceeds a threshold τp = 4.5, it indicates that the model's output is not sufficiently confident or relevant"
  - [corpus]: Related work on RAG evaluation (ASTRID) notes perplexity as a quality signal, but corpus lacks direct validation of perplexity-triggered retrieval in clinical settings
- Break condition: If perplexity does not correlate with factual accuracy in your specific domain (e.g., highly technical sub-specialties), the triggering mechanism may yield unnecessary retrieval or miss low-confidence cases.

### Mechanism 2
- Claim: Restricting retrieval to authoritative top-level domains improves clinical reliability.
- Mechanism: Retrieval is framed as a Markov Decision Process where the reward function R(s,a) = I{TLD∈.gov,.edu} × BM25(s,q) filters for government and educational domains while scoring relevance.
- Core assumption: Domain suffix correlates with medical trustworthiness and currency.
- Evidence anchors:
  - [abstract]: "reducing the risk of misinformation"
  - [section 4.2]: "ensures the retrieved documents are from authoritative sources (i.e., websites with '.gov' or '.edu' top-level domains)"
  - [corpus]: Weak direct evidence; corpus papers do not specifically validate domain filtering for clinical RAG
- Break condition: If authoritative domains contain outdated guidelines or if critical evidence exists only on .org or clinical society sites, retrieval coverage will be incomplete.

### Mechanism 3
- Claim: Low-rank adaptation enables efficient domain specialization without full model retraining.
- Mechanism: LoRA applies low-rank updates to transformer layers via hadapt_l = hbase_l + ΔW_l × hbase_l, where ΔW_l = B_l × A_l with rank r = 32, allowing online parameter updates from retrieved evidence.
- Core assumption: Low-rank updates capture sufficient domain knowledge without catastrophic forgetting or interference across edits.
- Evidence anchors:
  - [section 4.3]: "enables the model to adjust quickly to specific medical domains without the need for full retraining"
  - [section 7.1]: bRAGgen improved smaller models (Phi-3, Mistral) by +0.6–0.7 points compared to context-prompted baselines
  - [corpus]: No direct corpus validation of LoRA effectiveness specifically for clinical RAG
- Break condition: If rank-32 is insufficient for complex medical knowledge representation, or if sequential edits cause interference in dense regions of representation space (noted in Appendix A).

## Foundational Learning

- Concept: Perplexity as Uncertainty Proxy
  - Why needed here: The adaptive retrieval trigger depends entirely on interpreting model perplexity as a signal of confidence.
  - Quick check question: Can you explain why high perplexity might indicate low confidence in a medical QA context, and what domain factors might weaken this correlation?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Understanding how bRAGgen updates model parameters online without full fine-tuning.
  - Quick check question: What tradeoffs emerge if the LoRA rank is set too low versus too high for a specialized clinical domain?

- Concept: Semantic Cache Eviction Policies
  - Why needed here: The cache must balance recency, frequency, and relevance to maintain high-quality retrieval with bounded size.
  - Quick check question: How would you modify the eviction score ψ(dk) = α×fu(dk) + (1−α)×e^(−t/β) if your use case prioritizes recent guidelines over frequently accessed legacy documents?

## Architecture Onboarding

- Component map:
  Semantic Cache (C) -> Retrieval Engine (R) -> Generator (G) -> Learner (L)

- Critical path:
  Query → Cache lookup (cosine similarity ≥ 0.7) → (if miss) Web retrieval with domain/BM25 scoring → LoRA generation → Perplexity check → (if > 4.5) Additional retrieval + regeneration → Constrained decoding for safety → Response

- Design tradeoffs:
  - Cache size (500 docs) vs. retrieval latency: larger cache improves hit rate but increases lookup cost
  - Perplexity threshold (4.5): lower values increase retrieval frequency (higher latency, more cost), higher values risk confident but incorrect outputs
  - LoRA rank (32): higher rank captures more knowledge but risks overfitting; lower rank is faster but may underfit complex medical reasoning

- Failure signatures:
  - Infinite retrieval loops where perplexity never drops below threshold
  - High-value documents evicted due to low access frequency despite clinical importance
  - Retrieval of outdated content from authoritative domains
  - LoRA update interference from conflicting evidence across sequential queries

- First 3 experiments:
  1. Threshold ablation: Test τp ∈ {3.0, 4.5, 6.0} on a held-out subset of bRAGq; measure retrieval frequency, latency, and expert-rated factuality to identify optimal operating point.
  2. Domain filter validation: Compare .gov/.edu-only retrieval against open-web retrieval (with source credibility scoring) on factuality and clinical relevance metrics.
  3. Cache policy comparison: Benchmark the current α = 0.6 frequency-weighted eviction against recency-only (α = 0) and frequency-only (α = 1) policies using hit rate and expert-rated response quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum number of sequential knowledge edits bRAGgen can accumulate before experiencing interference or capacity saturation in dense representation regions?
- Basis in paper: [explicit] Appendix A states: "the cumulative impact of many such edits—particularly in dense regions of the representation space—may lead to interference or capacity saturation. Future work could explore dynamic pruning or hierarchical edit graphs to manage long-term scalability."
- Why unresolved: The paper evaluates bRAGgen on 1,302 questions but does not systematically test long-term edit accumulation or measure when performance degrades due to parameter interference.
- What evidence would resolve it: A longitudinal study measuring response accuracy and factuality after thousands of sequential edits, with analysis of representation space density and interference patterns.

### Open Question 2
- Question: How does bRAGgen's performance generalize when evaluated by multiple independent clinical experts across different institutions?
- Basis in paper: [inferred] The evaluation relies on a single board-certified bariatric surgeon for expert review (105 instances). While LLM-as-Judge showed high correlation (ρ = 0.94), inter-rater reliability among multiple human experts is not reported.
- Why unresolved: Single-expert evaluation may not capture the full spectrum of clinical judgment variability or potential blind spots in the benchmark.
- What evidence would resolve it: A multi-center evaluation with 3+ independent bariatric surgeons, reporting inter-rater agreement (e.g., Fleiss' κ) on factuality, clinical relevance, and comprehensiveness scores.

### Open Question 3
- Question: How sensitive is bRAGgen's performance to the choice of confidence threshold (τp = 4.5) and cache similarity threshold (τc = 0.7)?
- Basis in paper: [inferred] The perplexity threshold τp = 4.5 and cosine similarity threshold τc = 0.7 are fixed hyperparameters without ablation studies or theoretical justification for these specific values.
- Why unresolved: Suboptimal thresholds could either trigger unnecessary retrievals (increasing latency) or miss critical knowledge updates (reducing accuracy).
- What evidence would resolve it: A systematic ablation study varying τp ∈ [2.0, 6.0] and τc ∈ [0.5, 0.9], reporting precision-recall tradeoffs for retrieval triggering and downstream response quality.

## Limitations
- The correlation between perplexity and factual accuracy in medical domains remains weakly validated, with no direct evidence that τp = 4.5 optimally balances retrieval frequency and quality.
- Domain filtering to .gov/.edu sources may exclude high-quality clinical guidelines from professional societies (.org) or recent research from .com medical journals.
- LoRA rank selection (r = 32) lacks empirical justification specific to clinical knowledge representation, and sequential online updates may cause interference in dense parameter regions.

## Confidence
- **High**: Performance improvements over baselines on bRAGq (4.51/5 expert, 4.44/5 LLM-as-Judge), multi-source retrieval architecture, semantic cache implementation
- **Medium**: Effectiveness of perplexity-triggered retrieval (no direct validation of τp = 4.5), LoRA adaptation for clinical knowledge (no ablation studies), domain filtering impact (no comparison with broader retrieval)
- **Low**: Long-term adaptation stability, safety constraint efficacy, real-world patient engagement outcomes

## Next Checks
1. **Threshold ablation study**: Systematically vary τp ∈ {3.0, 4.5, 6.0} on held-out bRAGq samples; measure retrieval frequency, response latency, and expert-rated factuality to identify optimal operating point
2. **Domain filter comparison**: Evaluate .gov/.edu-only retrieval against open-web retrieval (with source credibility scoring) on factuality and clinical relevance metrics
3. **Cache policy benchmarking**: Compare current α = 0.6 frequency-weighted eviction against α = 0 (recency-only) and α = 1 (frequency-only) using hit rate and expert-rated response quality