---
ver: rpa2
title: Text to Speech System for Meitei Mayek Script
arxiv_id: '2508.06870'
source_url: https://arxiv.org/abs/2508.06870
tags:
- speech
- meitei
- audio
- system
- manipuri
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first Text-to-Speech (TTS) system for
  the Manipuri language using the Meitei Mayek script. By adapting Tacotron 2 and
  HiFi-GAN architectures, the authors developed a phoneme mapping from Meitei Mayek
  to ARPAbet and curated a single-speaker dataset.
---

# Text to Speech System for Meitei Mayek Script
## Quick Facts
- arXiv ID: 2508.06870
- Source URL: https://arxiv.org/abs/2508.06870
- Reference count: 10
- First TTS system for Manipuri language using Meitei Mayek script with MOS 3.43

## Executive Summary
This work introduces the first Text-to-Speech (TTS) system for the Manipuri language using the Meitei Mayek script. By adapting Tacotron 2 and HiFi-GAN architectures, the authors developed a phoneme mapping from Meitei Mayek to ARPAbet and curated a single-speaker dataset. The resulting system generates intelligible and natural-sounding speech, validated through subjective and objective metrics including a Mean Opinion Score of 3.43. The study addresses the technological gap for under-resourced tonal languages and provides foundational tools for linguistic preservation and digital inclusion of Manipuri.

## Method Summary
The system uses Tacotron 2 architecture fine-tuned on a single-speaker dataset of 818 speech samples (40 minutes, 22.05 kHz) with custom phoneme mapping from Meitei Mayek characters to adapted ARPAbet. Text preprocessing includes normalization and phoneme conversion, followed by training with Adam optimizer (LR 0.001, exponential decay) for 310 epochs. HiFi-GAN vocoder converts predicted mel-spectrograms to waveforms. The model was evaluated using Mean Opinion Score metrics for naturalness, pronunciation, and overall quality.

## Key Results
- Achieved Mean Opinion Score of 3.43 (Naturalness: 3.34±0.70, Pronunciation: 3.51±0.31, Overall: 3.43±0.51)
- Successfully mapped 55 unique Meitei Mayek characters to ARPAbet phonemes for Tacotron 2 processing
- Demonstrated functional synthesis of Manipuri speech with intelligible pronunciation

## Why This Works (Mechanism)
### Mechanism 1: Phoneme Mapping Bridge
Mapping Meitei Mayek characters to ARPAbet phonemes enables Tacotron 2 to learn Manipuri pronunciation without architectural modifications. A curated lookup table converts each of the 55 unique Meitei Mayek characters to approximate ARPAbet representations, bridging orthography to pronunciation through a familiar phonetic space.

### Mechanism 2: Transfer Learning from Pretraining
Fine-tuning Tacotron 2 on a small single-speaker corpus produces intelligible mel-spectrograms despite limited training data. The encoder extracts temporal features from phoneme embeddings while location-sensitive attention aligns encoder outputs with decoder timesteps. Transfer learning from English pretraining provides initialization, reducing data requirements.

### Mechanism 3: GAN-based Vocoder Efficiency
HiFi-GAN vocoder reconstructs natural waveforms from predicted mel-spectrograms with lower latency than WaveGlow. Using adversarial training with multi-scale discriminators, HiFi-GAN learns the mapping from mel-spectrograms to waveforms through upsampling layers with residual blocks, avoiding autoregressive bottlenecks.

## Foundational Learning
- **Mel-spectrograms and the Mel Scale**: Why needed - Tacotron 2 predicts mel-spectrograms, not waveforms directly. Quick check - Can you explain why the mel scale compresses high frequencies relative to low frequencies?
- **Location-Sensitive Attention**: Why needed - The attention mechanism aligns phoneme sequences with spectrogram frames. Quick check - How does adding convolutional features from previous attention weights improve monotonic alignment?
- **GAN Training Dynamics (Generator-Discriminator Equilibrium)**: Why needed - HiFi-GAN's audio quality depends on stable adversarial training. Quick check - What symptoms indicate discriminator overpowering during vocoder training?

## Architecture Onboarding
- Component map: Text Input → Text Normalization → Phoneme Conversion (ARPAbet mapping) → Character Embedding → Encoder (3 CNN + BiLSTM) → Attention → Decoder (2 LSTM + Linear Projection) → Mel-Spectrogram → HiFi-GAN Vocoder → Audio Output
- Critical path: Phoneme mapping accuracy → Encoder representations → Attention alignment quality → Spectrogram prediction accuracy → Vocoder reconstruction fidelity
- Design tradeoffs: Single-speaker dataset limits voice diversity but ensures acoustic consistency; ARPAbet mapping simplifies integration but may underspecify tonal contrasts; HiFi-GAN trades training stability for faster inference
- Failure signatures: Skipping/repeating syllables (attention failure), muffled output (insufficient high-frequency detail), wrong pronunciation for rare words (underexposed phoneme sequences), flat intonation (lack of prosody modeling)
- First 3 experiments: 1) Baseline reproduction with 818-sample dataset and 80:10:10 split, 2) Phoneme coverage audit mapping all 55 Meitei Mayek characters, 3) Attention alignment visualization extracting attention weight matrices

## Open Questions the Paper Calls Out
- Can explicit prosody modeling mechanisms better preserve the tonal contrasts of Manipuri compared to the standard phoneme mapping approach?
- To what extent does transfer learning from resource-rich languages improve pronunciation accuracy and rhythm in low-resource Meitei Mayek synthesis?
- Does increasing dataset duration and speaker variety mitigate the issues of flat intonation and unnatural rhythm in long, syntactically complex sentences?

## Limitations
- Single-speaker, low-resource dataset (40 minutes, 818 samples) constrains voice diversity and tonal range
- ARPAbet mapping likely fails to fully represent Manipuri's two-tone system, particularly falling tone distinctions
- Manual text-audio alignment introduces potential transcription errors affecting model learning

## Confidence
- High confidence: Core mechanism of phoneme mapping enabling Tacotron 2 processing is technically sound
- Medium confidence: MOS scores indicate functional synthesis but lack comparative baselines with alternative approaches
- Low confidence: Claims about linguistic preservation and digital inclusion are aspirational rather than empirically validated

## Next Checks
1. **Tonal accuracy audit**: Test model's ability to distinguish and produce minimal pairs differing only in tone (level vs. falling), measuring MOS drop for tonal words
2. **Generalization stress test**: Evaluate synthesis quality on held-out text containing rare phonemes, compound words, and out-of-vocabulary terms, tracking pronunciation accuracy degradation
3. **Speaker variability impact**: Record multi-speaker dataset (2-3 additional speakers) and measure performance degradation when fine-tuning or cross-adapting the single-speaker model