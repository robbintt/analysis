---
ver: rpa2
title: 'ACT as Human: Multimodal Large Language Model Data Annotation with Critical
  Thinking'
arxiv_id: '2511.09833'
source_url: https://arxiv.org/abs/2511.09833
tags:
- data
- annotation
- human
- budget
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ACT, a data annotation pipeline that leverages
  multimodal large language models (MLLMs) for annotation while using another MLLM
  as a criticizer to identify potentially erroneous labels. Human reviewers focus
  only on the most suspicious cases flagged by the criticizer, significantly improving
  annotation efficiency.
---

# ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking

## Quick Facts
- arXiv ID: 2511.09833
- Source URL: https://arxiv.org/abs/2511.09833
- Authors: Lequan Lin; Dai Shi; Andi Han; Feng Chen; Qiuzheng Chen; Jiawen Li; Zhaoyang Li; Jiyuan Li; Zhenbang Sun; Junbin Gao
- Reference count: 40
- Primary result: MLLM-based annotation pipeline with criticizer reduces human annotation costs by up to 90% while maintaining <2% performance gap to fully human-annotated data

## Executive Summary
ACT introduces a data annotation pipeline that uses multimodal large language models (MLLMs) for initial annotation while employing another MLLM as a criticizer to identify potentially erroneous labels. Human reviewers focus only on the most suspicious cases flagged by the criticizer, significantly improving annotation efficiency. The method works across NLP, computer vision, and multimodal domains without requiring training or model access. Theoretical analysis supports using modified loss functions (e.g., active M-estimation) to ensure models trained on ACT data perform comparably to those trained on fully human-annotated data.

## Method Summary
ACT uses an MLLM (e.g., GPT-4o with Chain-of-Thought) to annotate all unlabeled data, then employs a criticizer MLLM to estimate error probability for each annotation. Human reviewers examine only samples where the estimated error probability exceeds a threshold or ranks highest, based on available budget B. The corrected labels are combined with machine annotations and used to train downstream models using ACT loss (thresholding or exponential weighting variants). The pipeline requires no training or model access, making it broadly applicable.

## Key Results
- Reduces human annotation costs by up to 90% compared to full human review
- Achieves <2% performance gap to fully human-annotated training on most benchmarks
- Cross-criticism often outperforms self-criticism for error detection
- Normalization sampling fails under small budgets (64.70% vs 88.66% on CIFAR10), while thresholding and exponential weighting remain stable

## Why This Works (Mechanism)

### Mechanism 1: Selective Human Review via MLLM Criticism
Directing human effort to samples flagged as potentially erroneous by a critic MLLM improves annotation efficiency compared to random sampling or full human review. A criticizer MLLM estimates error probability for each machine-annotated sample. Humans review only samples where the estimated probability exceeds a threshold or ranks highest, concentrating limited human budget on likely errors. Core assumption: criticizer error estimates correlate with actual errors; MLLMs can meaningfully evaluate their own or others' annotations.

### Mechanism 2: ACT Loss Enables Training on Mixed-Quality Data
A modified loss function allows downstream models to achieve performance within ~2% of fully human-annotated training while using ~10-30% human labels. The ACT loss weights human-corrected samples inversely to their transformed sampling probability, providing an unbiased estimate of ground-truth loss. Thresholding or exponential weighting keeps the transformed probability bounded away from zero, controlling variance. Core assumption: the loss is strongly convex; the gradient gap between machine-annotated and ground-truth loss is bounded.

### Mechanism 3: Cross-Criticism Often Outperforms Self-Criticism
Using a different MLLM as criticizer than as annotator generally yields better error detection than using the same model for both. Different models may have complementary strengths/weaknesses; a second model may catch errors the first misses. However, self-criticism remains competitive in many cases. Core assumption: the annotator and criticizer have sufficiently different failure modes to provide non-redundant signal.

## Foundational Learning

- **Concept: Label Noise and Its Impact on Training**
  - Why needed here: ACT explicitly handles mixed-quality labels (machine + human). Understanding how noisy labels degrade training helps motivate the modified loss.
  - Quick check question: Can you explain why training on labels with ~10-20% error rate might still yield reasonable downstream performance, and when it would catastrophically fail?

- **Concept: Active Learning / Selective Sampling**
  - Why needed here: ACT is fundamentally an active annotation strategy—selecting which samples warrant human review based on estimated uncertainty/error.
  - Quick check question: How does thresholding-based sampling differ from uncertainty-based sampling in traditional active learning, and what are the tradeoffs?

- **Concept: MLLM Capabilities and Failure Modes**
  - Why needed here: The pipeline's success depends on MLLMs' annotation quality and self/cross-criticism reliability. Knowing where MLLMs typically fail (fine-grained visual distinctions, irony, domain-specific tasks) informs annotator/criticizer selection.
  - Quick check question: Why might an MLLM perform well on VQA but poorly on fine-grained image classification (e.g., Stanford Cars)?

## Architecture Onboarding

- **Component map:**
  1. Annotator MLLM (e.g., GPT-4o with CoT) → generates labels for all unlabeled data
  2. Criticizer MLLM (same or different model) → estimates error probability for each annotation
  3. Sampling module → selects top-k or threshold-exceeding samples for human review based on estimated errors and budget B
  4. Human review interface → corrects labels for selected samples
  5. Training module → trains downstream model using ACT loss (exponential weighting or thresholding recommended)

- **Critical path:**
  1. Choose annotator MLLM (start with GPT-4o + CoT as default)
  2. Choose criticizer MLLM (self-criticism is simplest; cross-criticism may improve results)
  3. Set human budget B (paper suggests starting with ~[1 - annotator accuracy] as "ideal" budget, plus 10% buffer)
  4. Run annotation → criticism → sampling → human review
  5. Train downstream model with ACT thresholding loss

- **Design tradeoffs:**
  - Self vs. cross-criticism: Self is simpler and cheaper; cross may catch more errors but requires additional API/model costs
  - Black-box vs. white-box criticism: Black-box (prompt-based error probability) works with any MLLM; white-box (logit/perplexity) requires model access but can provide finer-grained signal
  - Sampling rule: Normalization fails under small budgets; thresholding is simplest and robust; exponential weighting may slightly outperform but requires hyperparameter tuning
  - Human budget: Lower budget saves cost but risks missing errors; buffer budget (ideal + 10%) recommended

- **Failure signatures:**
  - Downstream performance far below human-only baseline → likely criticizer is miscalibrated or budget too low
  - Normalization loss collapses training (near-zero accuracy) → switch to thresholding or exponential weighting
  - High false positive rate in human review → criticizer overestimates errors; consider adjusting threshold or using cross-criticism
  - Consistent performance gap even with high budget → some error types systematically missed by criticizer

- **First 3 experiments:**
  1. Baseline calibration: On a small held-out set with human labels, compare GPT-4o annotation accuracy with and without CoT. Measure criticizer calibration (do estimated error probabilities correlate with actual errors?).
  2. Budget sweep: Fix annotator/criticizer (GPT-4o self-criticism). Vary human budget from 5% to 50% and plot downstream accuracy. Identify the knee point where returns diminish.
  3. Loss comparison: At fixed budget (e.g., 15%), compare normalization vs. exponential weighting vs. thresholding ACT losses on downstream performance. Confirm normalization underperforms under limited budget.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can ACT be effectively extended to open-ended generation tasks such as text summarization, where binary correctness judgments are inapplicable?
  - Basis: The conclusion states the main limitation is focusing primarily on classification tasks without involving complex tasks like text summarization or open-ended question answering.
  - Why unresolved: Current pipeline relies on error probability estimation for discrete labels; extending to free-form outputs requires defining new notions of "correctness" and error detection mechanisms.
  - Evidence needed: Demonstrating ACT variants on summarization or QA benchmarks with appropriate quality scoring and comparable human cost savings.

- **Open Question 2:** Would combining annotator confidence scores with criticizer error estimations improve error detection over either method alone?
  - Basis: Appendix G.2 states it may be beneficial to combine the annotator's confidence scores with the criticizer's error estimations to better capture insights from both perspectives.
  - Why unresolved: Current pipeline treats annotator and criticizer outputs independently; their complementary information has not been jointly leveraged.
  - Evidence needed: Ablation experiments comparing current ACT against variants that fuse confidence scores and error probabilities, measuring ABS and downstream performance.

- **Open Question 3:** As open-source MLLMs approach black-box model capabilities, will white-box strategies consistently outperform black-box criticism strategies?
  - Basis: Section 4.2.2 conjectures that as MLLMs advance, a future white-box model with capabilities comparable to black-box models could make white-box strategies consistently achieve superior performance.
  - Why unresolved: Current white-box models lag in capability; the theoretical advantage of logit access remains unrealized in practice.
  - Evidence needed: Systematic comparison of white-box and black-box strategies using future model generations as capabilities converge, tracking ABS and error detection precision.

## Limitations

- Pipeline effectiveness depends critically on criticizer MLLM's calibration; poor correlation between estimated and actual errors misallocates human effort
- Cross-criticism introduces API cost and complexity with variable empirical advantage across datasets
- ACT loss theory assumes strong convexity and bounded gradient gaps that may not hold for all architectures or tasks
- Budget allocation is sensitive—too low risks missing errors, too high eliminates cost savings
- No public benchmark dataset exists for direct comparison, potentially overstating gains vs. human-only baselines

## Confidence

- **High confidence**: MLLM-based annotation + selective human review improves cost efficiency vs. full human annotation (Section 4.2 empirical results, ABS/AQG metrics). ACT loss is unbiased and converges under bounded sampling probabilities (Theorem 5.2). GPT-4o + CoT is a viable default annotator (CIFAR10 88.66% accuracy).
- **Medium confidence**: Cross-criticism consistently outperforms self-criticism across all datasets (mixed evidence; Insight 3 notes self-criticism remains "highly competitive"). ACT achieves <2% performance gap vs. human-only baseline (result depends on dataset, budget, and criticizer quality). Theoretical bounds predict performance but assume ideal conditions.
- **Low confidence**: Normalization sampling is universally inferior to thresholding under limited budgets (Table 3 shows 64.70% vs 88.66% on CIFAR10; normalization requires careful hyperparameter tuning). MLLM criticizers can reliably estimate error probabilities across diverse tasks (calibration quality varies; no ablation on criticizer model choice).

## Next Checks

1. **Criticizer calibration audit**: On a small annotated test set, plot criticizer-estimated error probabilities vs. actual errors for both self- and cross-criticism. Measure Pearson correlation and calibration curves to quantify reliability.

2. **Budget sensitivity sweep**: Vary human budget B from 5% to 50% and plot downstream accuracy. Identify the knee point where returns diminish and test whether the paper's recommended "[1 - annotator accuracy] + 10%" heuristic holds.

3. **Loss variant comparison**: At fixed budget (e.g., 15%), compare normalization vs. exponential weighting vs. thresholding ACT losses. Confirm normalization's instability under limited budgets and measure variance across runs.