---
ver: rpa2
title: Few-shot multi-token DreamBooth with LoRa for style-consistent character generation
arxiv_id: '2510.09475'
source_url: https://arxiv.org/abs/2510.09475
tags:
- images
- training
- generation
- fidelity
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating unlimited, style-consistent
  characters from a limited set of reference images, extending DreamBooth with LoRA-based
  fine-tuning and a multi-token strategy. The approach assigns separate tokens to
  individual characters and their collective style, removes class-specific regularization,
  and introduces random tokens or embeddings during generation to ensure diversity
  while preserving style.
---

# Few-shot multi-token DreamBooth with LoRa for style-consistent character generation

## Quick Facts
- arXiv ID: 2510.09475
- Source URL: https://arxiv.org/abs/2510.09475
- Reference count: 14
- Authors: Ruben Pascual; Mikel Sesma-Sara; Aranzazu Jurio; Daniel Paternain; Mikel Galar
- Primary result: Achieves 1-5% absolute Fidelity improvement over baselines across five specialized datasets using multi-token strategy with clustering-based token selection

## Executive Summary
This paper addresses the challenge of generating unlimited, style-consistent characters from a limited set of reference images, extending DreamBooth with LoRA-based fine-tuning and a multi-token strategy. The approach assigns separate tokens to individual characters and their collective style, removes class-specific regularization, and introduces random tokens or embeddings during generation to ensure diversity while preserving style. Experiments on five small, specialized datasets (Virus, Scary, Daily, Hipster, Trans) demonstrate improved stylistic fidelity and generation validity compared to baselines, with the clustering-based token selection achieving the highest Fidelity scores. Human evaluation by both general participants and professional artists further validates the method's effectiveness in producing high-quality, diverse characters that align with the original artistic style.

## Method Summary
The method combines DreamBooth fine-tuning with LoRA adapters, using a multi-token prompting strategy "[specific_id] [shared_id] style" where specific tokens represent individual characters and a shared token captures the collective style. Token selection uses k-means clustering on CLIP text embeddings to ensure semantic distinctness, with the rarest token reserved for the style. Regularization is removed to prevent invalid generations, and random embeddings (univariate or multivariate) are injected during generation for unlimited character diversity. The approach is trained on 10-30 images per dataset for 20,000 steps with batch size 4 and learning rate 1e-4.

## Key Results
- DBL_MT-C (clustered token selection) consistently achieves highest Fidelity scores across all five datasets, improving by 1-5% absolute over single-token baseline
- Regularization removal reduces invalid image rates by up to 99% (e.g., Hipster from 55.75% to 0.70%)
- Clustering-based token selection outperforms rare-token selection in Fidelity by 1-3% absolute
- Human evaluation confirms preference for multi-token approach on style and diversity metrics
- Multivariate random embedding generation provides best balance of Fidelity and Diversity

## Why This Works (Mechanism)

### Mechanism 1: Multi-Token Disentanglement of Identity and Style
- Claim: Assigning separate tokens to individual characters and a shared style token improves stylistic fidelity while reducing overfitting to training images.
- Mechanism: The prompt structure "[specific_id] [shared_id] [class]" explicitly partitions learning: character-specific tokens capture unique traits (body shapes, features) while the shared token learns global style (color palette, textures, outlines). This prevents the model from conflating any single character's identity with the broader artistic style.
- Core assumption: The diffusion model's cross-attention layers can independently attend to multiple token embeddings without catastrophic interference during few-shot learning.
- Evidence anchors:
  - [abstract]: "clustering to assign separate tokens to individual characters and their collective style"
  - [Section 3.2]: "This aims to explicitly disentangle individual character features from the broader artistic style, allowing the model to learn them separately."
  - [Table 2]: DBL_MT-C achieves highest Fidelity across datasets (e.g., Scary 0.7617, Trans 0.7645) vs. single-token DBL baseline.
  - [corpus]: Weak direct evidence; neighbor papers focus on style consistency but not multi-token strategies specifically.
- Break condition: If training set has fewer characters than ~3, disentanglement may fail due to insufficient examples for the shared style token to generalize.

### Mechanism 2: Clustering-Based Token Selection in Embedding Space
- Claim: Selecting tokens that are maximally distant in CLIP embedding space improves character distinction compared to frequency-based selection.
- Mechanism: K-means clustering (k-means++ init, cosine distance, 10 restarts) partitions the embedding space into N clusters. Selecting the token nearest each centroid ensures semantic distinctness, preventing unintended overlaps where similar tokens would cause character features to bleed together.
- Core assumption: Tokens with distant CLIP embeddings will activate sufficiently disjoint regions of the diffusion model's cross-attention, enabling cleaner separation during generation.
- Evidence anchors:
  - [Section 3.2]: "we prioritize selecting tokens that are as distinct as possible from each other within the model's learned embedding space"
  - [Table 2]: DBL_MT-C (clustered) consistently outperforms DBL_MT-R (rarest) on Fidelity across all 5 datasets.
  - [Section 5.2]: "the clustering-based variant (DBL_MT-C) consistently achieves higher Fidelity scores than the rare-token variant (DBL_MT-R)"
  - [corpus]: No direct corpus evidence for clustering-based token selection in this context.
- Break condition: If the rare token list contains insufficient semantic diversity (e.g., all tokens cluster near common concepts), clustering provides no benefit over random selection.

### Mechanism 3: Regularization Removal with Random Embedding Injection
- Claim: Removing class-specific regularization and injecting random embeddings during generation enables unlimited diverse characters while maintaining style coherence.
- Mechanism: Standard DreamBooth regularization preserves prior knowledge of a class (e.g., "dog"), but this is counterproductive when the goal is style adaptation. Removing it eliminates a source of invalid generations (defective images resembling regularization set). Random multivariate embedding sampling—using covariance structure from rare token embeddings—creates novel character identities that remain compatible with learned style representations.
- Core assumption: The shared style token has learned a sufficiently robust style representation that it can condition novel embeddings without requiring further optimization.
- Evidence anchors:
  - [Section 3.2]: "removing the regularization dataset to encourage style adaptation"
  - [Table A.6]: Removing regularization reduces invalid images (e.g., Hipster 55.75% → 0.70% for Token generation) and improves Fidelity.
  - [Section 5.1]: "regularization process may interfere with the model's ability to generalize, particularly in low-data scenarios"
  - [corpus]: Weak indirect evidence; StyleGuard paper mentions DreamBooth/Textual Inversion for style customization but not regularization effects.
- Break condition: If training data is too diverse (no coherent style), the shared token fails to learn meaningful style, and random embeddings produce incoherent outputs.

## Foundational Learning

- Concept: **Diffusion Model Text Conditioning (Cross-Attention)**
  - Why needed here: Multi-token strategy relies on understanding how text tokens map to spatial regions in generated images through cross-attention layers.
  - Quick check question: Can you explain how modifying the text encoder embeddings affects which regions of the output image change most?

- Concept: **LoRA Low-Rank Adaptation**
  - Why needed here: The method uses LoRA for parameter-efficient fine-tuning; understanding rank decomposition is essential for debugging convergence issues.
  - Quick check question: If LoRA rank is too low, what symptom would you expect in generated characters—lack of detail, style drift, or training instability?

- Concept: **CLIP Text and Image Embeddings**
  - Why needed here: Token selection uses CLIP text embeddings; Fidelity metric uses CLIP image embeddings with a style-specialized encoder.
  - Quick check question: Why would a standard CLIP image encoder be suboptimal for measuring stylistic similarity versus identity similarity?

## Architecture Onboarding

- Component map:
  - Training pipeline: Stable Diffusion v1-5 + LoRA adapters → multi-token prompts ([specific_id] [shared_id] style) → no regularization set
  - Token selection module: Rare token list → CLIP text encoder → k-means clustering → centroid-based selection
  - Generation module: Random token OR random embedding (univariate/multivariate) + [shared_id] + "style"
  - Evaluation pipeline: Validity filters (copies, defective, multiple subjects, duplicates) → Fidelity (style-CLIP-I) + Diversity (embedding std)

- Critical path:
  1. Prepare dataset (10-30 images, consistent style)
  2. Run clustering on candidate tokens to assign [specific_id] tokens
  3. Fine-tune with LoRA (20k steps, lr=1e-4, batch=4)
  4. Generate with multivariate random embeddings for best fidelity-diversity balance
  5. Filter invalid outputs before metric computation

- Design tradeoffs:
  - Token vs. Embedding generation: Token method uses existing vocabulary (more stable), embedding method enables unlimited variety but may produce incoherent outputs
  - Rarest vs. Clustered token selection: Rarest is simpler; Clustered improves fidelity by ~1-3% but adds clustering overhead
  - With vs. without regularization: Regularization increases diversity but dramatically increases invalid image rates (up to 64% vs. <3%)

- Failure signatures:
  - **Training copies in output**: Overfitting to reference images → increase learning rate or reduce training steps
  - **Defective images (wrong style)**: Style token failed to generalize → increase dataset size or check style consistency
  - **Multiple subjects in single image**: Cross-attention bleeding → reduce LoRA rank or check token distinctness
  - **Mode collapse (duplicates)**: Random embedding variance too low → switch to multivariate sampling or increase covariance scaling

- First 3 experiments:
  1. **Baseline validation**: Train DBL (single-token + LoRA + regularization) on one dataset; measure invalid rate and Fidelity. Expected: 10-60% invalid rate depending on dataset.
  2. **Token selection ablation**: Compare DBL_MT-R vs. DBL_MT-C on 2 datasets. Expected: Clustered selection improves Fidelity by 1-5% absolute.
  3. **Generation strategy comparison**: For a fixed DBL_MT-C model, generate 100 images each using Token, Univar, Multivar methods. Report Fidelity/Diversity tradeoffs. Expected: Token highest diversity, Multivar highest fidelity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed multi-token DreamBooth strategy be effectively generalized to newer diffusion architectures, such as Flux or Stable Diffusion 3.5?
- Basis in paper: [explicit] The authors state in the conclusion, "Future research will explore the application of our proposed techniques with newer models such as Flux or Stable Diffusion 3.5."
- Why unresolved: The current study relies exclusively on Stable Diffusion v1-5, and the authors acknowledge that the method's effectiveness may be influenced by this specific model's biases and constraints.
- What evidence would resolve it: Quantitative results (Fidelity and Diversity) and human evaluation scores demonstrating that the method functions correctly when implemented on these newer model architectures.

### Open Question 2
- Question: Can refined quantitative metrics be developed to capture subtle stylistic differences in scenarios where the quality of generated images is already high?
- Basis in paper: [explicit] The authors state, "We aim to develop refined metrics capable of capturing subtle stylistic differences, particularly when image quality is already high."
- Why unresolved: The current quantitative metrics (Fidelity and Diversity) were found to be informative but imperfect, often failing to distinguish meaningful stylistic nuances from simple deviations in high-quality outputs.
- What evidence would resolve it: The proposal of a new metric that correlates more strongly with human expert ratings than the current Fidelity metric when assessing high-quality image sets.

### Open Question 3
- Question: Do the favorable results regarding stylistic fidelity hold true under broader, more diverse human evaluation conditions?
- Basis in paper: [explicit] The authors note, "Broader human evaluations will further complement these efforts, enhancing the reliability and practical relevance of character generation methods."
- Why unresolved: The current study was limited to specific participant groups (135 university students and 6 professional artists), which may not fully represent the broader population's perception of style and character consistency.
- What evidence would resolve it: Results from a large-scale study involving a more diverse demographic of participants that replicate the preference for the multi-token approach (DBL_MT-C) over baselines.

## Limitations
- **Token diversity representativeness**: Clustering assumes CLIP text embeddings meaningfully capture semantic distinctness, but the rare token list may lack sufficient diversity for specialized domains
- **Style generalization boundaries**: Method claims unlimited generation but doesn't characterize limits when style is highly specific or training data is insufficient
- **Evaluation metric completeness**: Fidelity measures style but not character identity preservation; diversity metric doesn't capture perceptual diversity

## Confidence

**High confidence**: Multi-token strategy improves stylistic fidelity over single-token baselines. The quantitative results across five datasets consistently show 1-5% absolute improvements in Fidelity for clustered selection. The mechanism (explicit disentanglement via separate tokens) is well-grounded in cross-attention theory.

**Medium confidence**: Regularization removal significantly reduces invalid image rates without sacrificing quality. The results show dramatic reductions (up to 99% improvement in validity) with modest Fidelity changes, though the exact mechanism for why regularization causes so many defects in few-shot scenarios isn't fully explained.

**Low confidence**: Clustering-based token selection is superior to rare-token selection. While the paper shows consistent Fidelity improvements, the magnitude (~1-3%) may not justify the added complexity of k-means clustering. The superiority claim assumes the rare token list has sufficient semantic diversity, which isn't validated.

## Next Checks

1. **Token diversity validation**: Before training, compute pairwise CLIP text embedding distances for the selected tokens. If the minimum distance falls below a threshold (e.g., 0.3 cosine distance), regenerate the token selection using alternative rare token lists or increase the number of clusters to ensure semantic distinctness.

2. **Style generalization stress test**: Generate 1000 characters from a trained model and cluster them by CLIP image embeddings. Compute the silhouette score of this clustering. If the score is below 0.3, the style token hasn't learned a coherent style representation, and the model should be retrained with more diverse training images or different hyperparameters.

3. **Character identity preservation check**: Select 10 training characters and generate 5 variations each. Use a pre-trained face recognition model (if applicable) or CLIP similarity to measure identity preservation. If the average similarity between generated variations and their source character drops below 0.7, the multi-token strategy isn't properly preserving individual character traits despite maintaining style.