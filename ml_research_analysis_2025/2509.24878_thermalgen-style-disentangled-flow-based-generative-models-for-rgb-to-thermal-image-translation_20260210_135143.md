---
ver: rpa2
title: 'ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal
  Image Translation'
arxiv_id: '2509.24878'
source_url: https://arxiv.org/abs/2509.24878
tags:
- thermal
- image
- datasets
- rgb-t
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating thermal images
  from RGB inputs to enable cross-modality training in the absence of paired RGB-thermal
  data. The authors propose ThermalGen, a flow-based generative model with an RGB
  conditioning architecture and style-disentangled mechanism.
---

# ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation

## Quick Facts
- arXiv ID: 2509.24878
- Source URL: https://arxiv.org/abs/2509.24878
- Authors: Jiuhong Xiao; Roshan Nayak; Ning Zhang; Daniel Tortei; Giuseppe Loianno
- Reference count: 40
- Key outcome: Proposes ThermalGen, a flow-based generative model with style-disentangled embeddings and RGB conditioning, achieving 70.09 FID on FLIR dataset for RGB-to-thermal image translation

## Executive Summary
This paper addresses the challenge of generating thermal images from RGB inputs to enable cross-modality training in the absence of paired RGB-thermal data. The authors propose ThermalGen, a flow-based generative model with an RGB conditioning architecture and style-disentangled mechanism. They train on a large-scale collection of 11 RGB-thermal datasets and introduce three new satellite-aerial datasets. ThermalGen demonstrates superior performance in FID and LPIPS metrics compared to GAN- and diffusion-based baselines, achieving 70.09 FID on the FLIR dataset. The method is shown to effectively handle variations in sensor, viewpoint, and environment, enabling high-fidelity thermal image synthesis across diverse domains.

## Method Summary
ThermalGen is a flow-based generative model that translates RGB images to thermal images using a style-disentangled mechanism with learnable dataset-specific embeddings. The model employs a SiT (Scalable Interpolate Transformer) backbone with RGB conditioning via latent concatenation and adaLN-Zero for style modulation. Training involves two stages: first, a KL-VAE thermal encoder/decoder is trained separately (200k steps), then the flow model is trained with concatenated RGB latents and style embeddings (200k steps). The approach handles variations in sensor, viewpoint, and environment through discrete style embeddings, enabling straightforward extension to new datasets. The model is evaluated on 11 RGB-thermal datasets with three new satellite-aerial datasets introduced, achieving superior FID and LPIPS metrics compared to GAN- and diffusion-based baselines.

## Key Results
- Achieves 70.09 FID on FLIR dataset, outperforming GAN- and diffusion-based baselines
- Introduces three new satellite-aerial datasets: DJI-day, Bosonplus-day, and Bosonplus-night
- Demonstrates superior cross-dataset generalization through style-disentangled embeddings
- CFG scale tuning significantly improves performance: FLIR FID improves from 70.09 to 63.43 with scale 4.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Style-disentangled embeddings enable cross-domain generalization without retraining
- Mechanism: Learnable dataset-specific embeddings y∈R^(1×D) are injected via adaLN-Zero conditioning, modulating scale/shift parameters in transformer blocks. During training, random selection between dataset-specific (y_i) and unconditional (y_un) embeddings enables classifier-free guidance at inference.
- Core assumption: RGB-T mapping variations (sensor, viewpoint, environment) can be captured as discrete, transferable style vectors rather than being entangled in model weights.
- Evidence anchors:
  - [abstract]: "incorporating an RGB image conditioning architecture and a style-disentangled mechanism"
  - [Section 3.2]: "This design allows for straightforward extension to additional datasets by appending new style embeddings"
  - [corpus]: No direct corpus evidence for this specific mechanism; related RGB-T papers focus on fusion/segmentation, not style-disentangled translation
- Break condition: Fails when style embeddings cannot capture continuous variations (e.g., gradual sensor calibration drift) or when target domain is fundamentally outside training distribution (see LLVIP t-SNE analysis showing distribution gap).

### Mechanism 2
- Claim: Concatenation-based RGB conditioning outperforms cross-attention for RGB-T translation
- Mechanism: RGB latent z_RGB is concatenated with noised thermal latent ẑ_t,T along channel dimension before SiT blocks, rather than using z_RGB as query in cross-attention (ẑ_t,T as key/value). This preserves spatial correspondence between RGB semantics and thermal predictions.
- Core assumption: RGB spatial structure directly informs thermal spatial structure (pixels with similar RGB features should have correlated thermal responses).
- Evidence anchors:
  - [Section 5.2, Figure 4b]: "concatenating RGB latents yields overall better FID performance than incorporating them as query inputs"
  - [Section 3.2]: "This method enables convenient fine-tuning from pretrained SiT weights"
  - [corpus]: No corpus comparison; cross-attention is common in general image translation (Palette), but RGB-T specific evaluation is novel here
- Break condition: May fail when RGB-thermal spatial alignment is imperfect (misaligned training pairs) or when thermal features depend on non-spatial factors (e.g., subsurface heat not visible in RGB).

### Mechanism 3
- Claim: Flow-based velocity prediction in latent space enables stable multi-domain training
- Mechanism: Uses SiT (Scalable Interpolate Transformer) to predict velocity v(ẑ_t,T, t) = α̇_t·E[z_0|z_t=z] + σ̇_t·E[ϵ|z_t=z] in KL-VAE latent space (8× compression). ODE sampler integrates reverse flow from z_1=ϵ to z_0 over T=50 steps.
- Core assumption: Thermal image distribution can be approximated by continuous flow in compressed latent space; KL-regularized latent is sufficiently expressive.
- Evidence anchors:
  - [Section 3.1]: velocity function defined via conditional expectations over diffusion process
  - [Table 7]: Thermal autoencoder achieves FID 2.0-18.5, PSNR 28-45dB across datasets, confirming latent expressiveness
  - [corpus]: Flow-based approaches gaining traction in general image synthesis; limited direct RGB-T comparison
- Break condition: High-frequency thermal details (sharp temperature gradients) may be lost in 8× latent compression; extreme domain shifts require more denoising steps or different noise schedules.

## Foundational Learning

- Concept: Flow matching / Continuous normalizing flows
  - Why needed here: ThermalGen uses ODE-based velocity prediction rather than discrete diffusion steps. Must understand z_t = α_t·z_0 + σ_t·ϵ as interpolation between data and noise.
  - Quick check question: Given α_t=1-t and σ_t=t, what is z_0.5 when z_0 is a data sample and ϵ is Gaussian noise?

- Concept: Classifier-Free Guidance (CFG)
  - Why needed here: Paper shows CFG scale 4.0-8.0 significantly improves FID on challenging datasets (FLIR: 70.09→63.43, Boson-night: 161.22→116.46).
  - Quick check question: How does increasing CFG scale from 2.0 to 8.0 affect the balance between conditional and unconditional predictions?

- Concept: adaLN-Zero conditioning
  - Why needed here: Style embeddings modulate transformer via adaptive layer normalization (scale γ, shift β). Must understand how y_i,t → c_{yi,t} affects attention/FFN outputs.
  - Quick check question: Why does modifying normalization parameters achieve style transfer more effectively than adding style vectors directly?

## Architecture Onboarding

- Component map:
Input: RGB image (H×W×3)
├── RGB Encoder (KL-VAE, frozen) → z_RGB (H/8 × W/8 × 4)
├── Style Embedding Selection → y (1×1024)
├── Time Embedding → t_emb
└── Flow Model (SiT-XL/2)
    ├── AdaLN-Zero blocks (modulated by y, t)
    ├── Self-Attention + RGB Concatenation
    └── Pointwise FFN
→ Velocity prediction v_θ(ẑ_t,T, t, z_RGB, y)
ODE Sampler (50 steps) → Denoised latent ẑ_0,T
Thermal Decoder (KL-VAE) → Thermal image (H×W×1)

- Critical path:
  1. Ensure KL-VAE encoder/decoder trained first (200k steps, separate from flow model)
  2. Verify style embedding dimension (1024) matches adaLN projection
  3. Check RGB latent concatenation axis (channel dim, not spatial)
  4. Validate ODE solver returns z_0∈data range, not noise

- Design tradeoffs:
  - **SiT-L/2 vs SiT-XL/2**: XL/2 gives best FID (Figure 4a) but higher compute. L/2 is practical compromise.
  - **Concatenation vs Cross-Attn**: Concatenation wins on FID but requires modifying input channels (harder to use pretrained weights directly).
  - **Patch size 2 vs 4 vs 8**: Smaller patches capture finer thermal gradients but increase sequence length (4× compute).
  - **CFG scale**: Higher values (>8) can over-emphasize style, degrading output (Table 5 shows 16.0 fails on FLIR).

- Failure signatures:
  - **Dark/blurry outputs** (Boson-night pattern): CFG scale too low; increase to 8.0
  - **Random samples ignoring RGB** (DDIM pattern, Figure 7): RGB conditioning not propagating; check concatenation/attention implementation
  - **Grid artifacts** (VQGAN pattern in Figure 6): Codebook issues not relevant here, but similar artifacts suggest latent space discretization problems
  - **Sharp boundaries** (DiffV2IR pattern): Model overfitting to edge features; may need more diverse training data or regularization

- First 3 experiments:
  1. **Baseline sanity check**: Train on single dataset (e.g., M3FD) without style embeddings. Verify RGB→thermal mapping learns before adding complexity.
  2. **Style embedding ablation**: Compare FID with (a) unconditional only, (b) dataset-specific only, (c) CFG=2.0, on 3 datasets with distinct styles (Bosonplus-day, NII-CU, FLIR). Reproduce Figure 4c pattern.
  3. **CFG sweep for failure modes**: On Boson-night (low-contrast) and FLIR (blurred distant objects), sweep CFG ∈ {2, 4, 8, 16}. Identify optimal scale per dataset type; verify paper's claim that CFG mitigates specific challenges.

## Open Questions the Paper Calls Out

- Question: To what extent does training on ThermalGen's synthesized thermal data improve performance on downstream cross-modal tasks compared to training solely on real paired data?
- Basis in paper: [explicit] The Conclusion states, "Future work will focus on systematically incorporating synthesized thermal data from multiple sources to further advance cross-modal model training."
- Why unresolved: The current study evaluates the model primarily on image fidelity metrics (FID, LPIPS) and qualitative visual quality, but it does not quantify the "sim-to-real" gap or the efficacy of the generated images for training downstream networks like detectors or matchers.
- What evidence would resolve it: Benchmarks comparing the accuracy of downstream tasks (e.g., object detection mAP, homography estimation error) when models are trained on ThermalGen data versus ground-truth thermal data.

## Limitations
- Style embeddings may fail to capture continuous variations like gradual sensor calibration drift
- 8× latent compression may limit ability to capture fine thermal details with sharp temperature gradients
- Requires paired RGB-thermal training data for each target domain, limiting true zero-shot adaptation

## Confidence
- **High Confidence**: Flow-based architecture with SiT backbone and superior performance metrics (FID, LPIPS) compared to baselines
- **Medium Confidence**: Style-disentangled mechanism's effectiveness across truly novel domains, partially supported by t-SNE analysis showing distribution gaps
- **Medium Confidence**: Claim that RGB concatenation outperforms cross-attention, supported by ablation studies but lacking broader comparison

## Next Checks
1. Test ThermalGen on a held-out RGB-thermal dataset not included in training to verify true cross-dataset generalization beyond the LLVIP-style evaluation
2. Conduct ablation studies varying the latent compression factor (4×, 8×, 16×) to quantify the trade-off between computational efficiency and thermal detail preservation
3. Evaluate the model's sensitivity to misalignment in RGB-T pairs by synthetically shifting training pairs and measuring degradation in FID and visual quality