---
ver: rpa2
title: Evaluating SAP Joule for Code Generation
arxiv_id: '2509.24828'
source_url: https://arxiv.org/abs/2509.24828
tags:
- code
- available
- joule
- generation
- accessed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates SAP Joule's JavaScript code generation capabilities\
  \ using the HumanEval-X benchmark, comparing it against 29 other models via strict\
  \ accuracy. Despite being a general-purpose LLM primarily designed for business\
  \ process support, SAP Joule achieves a strict accuracy of 80.49%, ranking fifth\
  \ overall\u2014just 1.5 percentage points behind the top-performing models (GPT-4o\
  \ and Claude 3.5 Sonnet at 85.98%)."
---

# Evaluating SAP Joule for Code Generation

## Quick Facts
- arXiv ID: 2509.24828
- Source URL: https://arxiv.org/abs/2509.24828
- Authors: Joshua Heisler; Johannes Reisinger; Andreas Fischer
- Reference count: 40
- Primary result: SAP Joule achieves 80.49% strict accuracy on JavaScript code generation, ranking 5th among 30 models evaluated on HumanEval-X.

## Executive Summary
This paper evaluates SAP Joule's JavaScript code generation capabilities using the HumanEval-X benchmark, comparing it against 29 other models via strict accuracy. Despite being a general-purpose LLM primarily designed for business process support, SAP Joule achieves a strict accuracy of 80.49%, ranking fifth overallâ€”just 1.5 percentage points behind the top-performing models (GPT-4o and Claude 3.5 Sonnet at 85.98%). Among open-source models, Qwen2.5 leads with 81.71% accuracy. The evaluation reveals that code generation performance depends more on model architecture and training data than parameter count. This is the first quantitative evaluation of SAP Joule's code generation abilities, demonstrating strong performance despite its non-specialized design. Future work includes assessing ABAP code generation capabilities.

## Method Summary
The evaluation uses the HumanEval-X benchmark consisting of 164 JavaScript programming problems. Models are prompted with each problem, appending "Use JavaScript." to prevent language drift. Responses are manually copied from SAP Joule's UI (due to lack of API access) or collected via APIs for other models. A Python script sanitizes outputs by removing markdown, natural language explanations, and example calls, keeping only the first complete function. The cleaned code is concatenated with hidden test cases and executed in Node.js v18+. Success is determined by exit code 0, with strict accuracy calculated as the percentage of problems solved correctly on the first attempt.

## Key Results
- SAP Joule achieves 80.49% strict accuracy, ranking 5th among 30 evaluated models.
- Code generation performance correlates weakly (r=0.23) with parameter count, indicating architecture and training data matter more than size.
- Qwen2.5 achieves the highest accuracy among open-source models at 81.71%.
- The top models (GPT-4o, Claude 3.5 Sonnet) reach 85.98% accuracy, only 1.5 percentage points above SAP Joule.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Single-sample "Strict Accuracy" serves as a viable proxy for code generation capability when automated sampling (e.g., pass@k) is infeasible due to interface constraints.
- **Mechanism:** Because modern LLMs have significantly reduced variance in code generation compared to earlier iterations, a binary pass/fail result on a single attempt across a large problem set (164 tasks) provides a statistically indicative signal of functional correctness without requiring multiple generations per prompt.
- **Core assumption:** The variance of the evaluated models is low enough that a single sample does not suffer from excessive noise or luck (bias), as cautioned by Hendrycks et al.
- **Evidence anchors:**
  - [abstract] Mentions SAP Joule achieves 80.49% strict accuracy, ranking 5th, implying the metric successfully stratified model performance.
  - [section III-B-3] Argues that while strict accuracy has historical bias issues, "utilization of this approach is supported by the substantial progress in the generative capabilities of modern LLMs."
  - [corpus] Weak direct support; "Evaluating LLM Metrics Through Real-World Capabilities" suggests benchmarks must reflect real-world usage, but does not validate strict accuracy specifically.
- **Break condition:** If a model has high temperature settings or inherent stochasticity causing significant output drift between runs, single-sample accuracy becomes an unreliable indicator of true capability.

### Mechanism 2
- **Claim:** Code generation performance is driven primarily by training data quality and architecture specialization rather than raw parameter count.
- **Mechanism:** Specialized models (e.g., code-tuned instruction models) learn syntax and logic patterns more efficiently, allowing smaller parameter counts to outperform larger, generic models. This decouples the expected positive correlation between size and accuracy.
- **Core assumption:** The evaluated open-source models accurately represent the relationship between architecture, data, and size for the broader model ecosystem.
- **Evidence anchors:**
  - [abstract] States "code generation performance depends more on model architecture and training data than parameter count."
  - [section IV-C] Notes that plotting strict accuracy against parameters reveals "only a weak correlation (r= 0.23)" and highlights that "specialized models can effectively compete with larger, generic models."
  - [corpus] "CodeVisionary" discusses evaluating LLMs in code generation but focuses on evaluation frameworks rather than the causal drivers of performance.
- **Break condition:** If the evaluation benchmark (HumanEval-X) contains data leaked into the training sets of specific "specialized" models, the performance would be due to memorization rather than architectural efficiency.

### Mechanism 3
- **Claim:** Explicit language constraints in prompts are necessary to prevent "language drift" in multilingual code generation.
- **Mechanism:** Multilingual models operate on a probabilistic selection of the "most likely" solution path. Without explicit instruction, ambiguous task definitions may cause the model to default to its dominant training language (often Python) rather than the target language (JavaScript).
- **Core assumption:** The model possesses sufficient capability in the target language to comply if prompted correctly.
- **Evidence anchors:**
  - [section III-D-1] States: "Tests indicate that models often do not respond in JavaScript but use other well-known programming languages... As a countermeasure, the prefix 'Use JavaScript.' is appended."
  - [abstract] Confirms the evaluation is strictly for JavaScript capabilities.
  - [corpus] No specific corpus papers address the mechanism of prompt prefixing for language enforcement in code generation.
- **Break condition:** If a model lacks sufficient latent capacity for the target language, adding the prefix may result in syntactically valid but functionally incorrect code (hallucinated syntax).

## Foundational Learning

- **Concept: Execution-Based vs. Reference-Based Metrics**
  - **Why needed here:** The paper explicitly rejects reference-based metrics (like BLEU) because there are multiple ways to write correct code; comparing generated code to a single "gold standard" often yields false negatives.
  - **Quick check question:** Does the metric measure if the code *looks* like the reference solution, or if the code *runs* and passes tests?

- **Concept: Pass@k vs. Strict Accuracy**
  - **Why needed here:** The paper relies on "Strict Accuracy" (pass@1 effectively) due to manual constraints, whereas the industry standard is often pass@k (probability of success in k tries). Understanding this distinction is critical for contextualizing the 80.49% score.
  - **Quick check question:** If a model fails a test on the first try but succeeds on the second, does Strict Accuracy capture this capability?

- **Concept: The HumanEval-X Benchmark**
  - **Why needed here:** This is the standardized exam used. It consists of 164 hand-written problems translated from Python to JavaScript. Validating the results requires knowing that the evaluation script concatenates the model's solution with hidden unit tests.
  - **Quick check question:** Is the model evaluated on its ability to write text explanations, or on its ability to output executable function bodies that satisfy assertions?

## Architecture Onboarding

- **Component map:** HumanEval-X JSON (Prompts + Tests) -> Prompting Interface (Manual/UI vs. API/Batch) -> Sanitization Layer (Python script) -> Execution Environment (Node.js v18+)

- **Critical path:** The **Response Processing** step (Section III-D-3) is the most fragile component. Because LLMs naturally output conversational text ("Here is the solution..."), a deterministic cleaning script is required to extract *only* the executable code. If this fails, syntax errors occur that do not reflect the model's coding ability.

- **Design tradeoffs:**
  - **Metric Choice:** Chose Strict Accuracy (low cost, manual-friendly) over Pass@k (high cost, API-friendly).
  - **Assumption:** Accepted potential single-run variance in exchange for the feasibility of evaluating a UI-only model.
  - **Language Scope:** Focused on JavaScript only (via prefixing) to align with SAP Joule's current support, ignoring ABAP future capabilities.

- **Failure signatures:**
  - **Language Drift:** Model outputs Python code (e.g., `def` instead of `function`).
  - **Timeouts:** Infinite loops in generated code trigger the Node.js timeout mechanism.
  - **Partial Generation:** Model cuts off mid-function; the cleaner must attempt to close brackets or discard the sample.

- **First 3 experiments:**
  1. **Reproduce Baseline:** Run the provided evaluation script against a low-cost model (e.g., GPT-3.5 Turbo or a local Llama instance) to verify the Node.js execution environment and cleaning script function correctly.
  2. **Ablation on Prefixing:** Execute the benchmark with and without the "Use JavaScript" prefix to quantify the rate of language drift and validate the mechanism described in Section III-D-1.
  3. **Manual vs. API Delta:** For a model that has both API and UI access (e.g., GPT-4), compare Strict Accuracy results from the API (automated) vs. Manual (copy-paste) to estimate the "human factor" error introduced by the manual evaluation process used for SAP Joule.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the capability of SAP Joule in generating ABAP code compared to its JavaScript performance?
- Basis in paper: [explicit] The conclusion states that "a future evaluation of its code generation capabilities with the ABAP programming language is warranted."
- Why unresolved: At the time of the study, SAP Joule did not officially support ABAP generation, so the evaluation was restricted to JavaScript.
- Evidence: A future study evaluating SAP Joule on a standardized ABAP benchmark or a translated HumanEval-ABAP dataset.

### Open Question 2
- Question: How does SAP Joule perform on SAP-specific frameworks (e.g., CAP, SAPUI5) compared to general JavaScript?
- Basis in paper: [inferred] The paper notes Joule is designed for proprietary environments like the SAP Cloud Application Programming (CAP) model, but the evaluation relied on the general HumanEval-X benchmark.
- Why unresolved: The study utilized a general benchmark to allow comparison with 29 other models, leaving the model's performance on domain-specific SAP syntax and logic untested.
- Evidence: An evaluation using a custom benchmark specifically designed for SAP CAP or SAPUI5 development tasks.

### Open Question 3
- Question: Does SAP Joule's performance improve significantly when evaluated with pass@k metrics rather than strict accuracy?
- Basis in paper: [inferred] The authors identify the "lack of API access" as a constraint that "precludes the use of... pass@k," forcing a reliance on strict accuracy which risks single-run variance.
- Why unresolved: Without the ability to automate multiple generations per prompt (n > 1), the model's consistency and potential ceiling performance remain obscured by the manual evaluation method.
- Evidence: A re-evaluation of the model using a released API to calculate pass@k scores based on multiple samples per problem.

## Limitations

- **Manual evaluation constraint:** SAP Joule's lack of API access forces manual copy-paste evaluation, introducing potential human error and preventing pass@k metrics that could better capture model capability.
- **Single-sample risk:** Reliance on strict accuracy (pass@1) assumes low variance in modern LLMs but doesn't empirically validate this assumption against pass@k results.
- **Language drift mitigation:** The "Use JavaScript." prefix is assumed effective for preventing language drift but not experimentally validated within the paper.

## Confidence

- **High confidence:** SAP Joule's 80.49% strict accuracy ranking (5th among 30 models) and the fundamental observation that JavaScript code generation performance depends more on architecture/training than parameter count are well-supported by the evaluation methodology.
- **Medium confidence:** The assertion that single-sample strict accuracy is a viable proxy for code generation capability is reasonable given modern LLM capabilities but not empirically validated against pass@k results.
- **Low confidence:** The effectiveness of the "Use JavaScript." prefix in preventing language drift is assumed rather than measured, and the paper does not rule out data leakage as an explanation for specialized model performance.

## Next Checks

1. **Ablation study on prompt prefixing:** Run the HumanEval-X benchmark with identical models both with and without the "Use JavaScript." prefix to measure the actual reduction in language drift and validate this mitigation mechanism.
2. **Variance analysis across multiple runs:** For a subset of models, execute the benchmark multiple times with different random seeds to quantify the variance in strict accuracy and assess whether single-sample results reliably reflect true capability.
3. **Cross-lingual capability assessment:** Evaluate SAP Joule's ability to generate code in ABAP (its primary business language) using an equivalent benchmark to determine if the JavaScript performance pattern generalizes to SAP's domain-specific language.