---
ver: rpa2
title: Dynamic Compressing Prompts for Efficient Inference of Large Language Models
arxiv_id: '2504.11004'
source_url: https://arxiv.org/abs/2504.11004
tags:
- compression
- prompt
- llm-dcp
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-DCP, a task-agnostic prompt compression
  method that reduces token usage in large language models (LLMs) while maintaining
  output quality. The approach models prompt compression as a Markov Decision Process
  (MDP), enabling a DCP-Agent to iteratively remove redundant tokens while preserving
  essential content through context-aware decision making.
---

# Dynamic Compressing Prompts for Efficient Inference of Large Language Models

## Quick Facts
- **arXiv ID:** 2504.11004
- **Source URL:** https://arxiv.org/abs/2504.11004
- **Reference count:** 40
- **Primary result:** LLM-DCP achieves 3.04% ROUGE-2 improvement and 12.9x compression ratio on Arxiv-March23 dataset versus state-of-the-art methods.

## Executive Summary
This paper introduces LLM-DCP, a task-agnostic prompt compression method that reduces token usage in large language models (LLMs) while maintaining output quality. The approach models prompt compression as a Markov Decision Process (MDP), enabling a DCP-Agent to iteratively remove redundant tokens while preserving essential content through context-aware decision making. A reward function balances compression rate, output distribution, and key information retention, and the method does not require direct access to target LLMs during training. A Hierarchical Prompt Compression (HPC) strategy is also introduced to progressively increase compression difficulty. Experiments demonstrate that LLM-DCP achieves approximately 3.04% improvement in Rouge-2 score and a 12.9x compression ratio on the Arxiv-March23 dataset compared to state-of-the-art methods.

## Method Summary
LLM-DCP formulates prompt compression as a sequential decision-making process using reinforcement learning. A DCP-Agent, built on xlm-roberta-large, processes tokens and outputs binary retain/remove decisions in an iterative manner. The method employs a reward function balancing compression rate, output quality (via BertScore), and distribution alignment (via KL divergence from a small aligned model). A Hierarchical Prompt Compression strategy gradually increases compression difficulty during training to improve learning stability and final performance.

## Key Results
- Achieves 12.9x compression ratio on Arxiv-March23 dataset while maintaining output quality
- Improves ROUGE-2 score by approximately 3.04% compared to state-of-the-art methods
- 1-shot GSM8K results: 77.03 EM with 6.9x compression versus 76.57 EM with 5.5x without HPC
- Maintains task-agnostic performance across multiple domains without requiring black-box LLM access during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling prompt compression as a sequential decision-making process (MDP) enables context-aware token removal that adapts to evolving prompt states, potentially outperforming static entropy-based methods.
- **Mechanism:** The DCP-Agent treats each prompt as an initial state, iteratively applying binary retain/remove actions per token. Each compression step produces a new state (compressed prompt), which serves as input for the next decision. This allows the agent to learn which tokens become redundant only after earlier removals have changed the context.
- **Core assumption:** Token importance is not static; it depends on what has already been removed. Sequential decisions can capture dependencies that single-pass methods miss.
- **Evidence anchors:**
  - [abstract] "We model prompt compression as a Markov Decision Process (MDP), enabling the DCP-Agent to sequentially remove redundant tokens by adapting to dynamic contexts and retaining crucial content."
  - [Section IV-A] Defines MDP components: states (compressed prompts), actions (binary token decisions), transitions (token removal operations), and policies.
  - [corpus] Related work "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression" similarly argues that entropy-based methods overlook sequential dependencies, supporting this hypothesis.
- **Break condition:** If token importance were independent of prior removals (e.g., purely additive information), sequential modeling would offer no advantage over single-pass ranking.

### Mechanism 2
- **Claim:** A multi-objective reward function balancing compression ratio, output distribution alignment, and key information retention allows training without querying the target black-box LLM, reducing costs while preserving output quality.
- **Mechanism:** The reward R(sₜ, aₜ) = α/ρ + βD(s₀, sₜ) - γKL(P(sₜᴳ|sₜ), P(s₀ᴳ|s₀)) - penalties uses: (1) inverse compression rate ρ, (2) BertScore between original and compressed prompts, and (3) KL divergence between output distributions from a small aligned model (Llama3-8B instruction-tuned on black-box outputs). Penalties discourage extreme over/under-compression.
- **Core assumption:** A small instruction-tuned model can approximate the target black-box LLM's output distribution sufficiently for reward signal generation.
- **Evidence anchors:**
  - [abstract] "We develop a reward function for training the DCP-Agent that balances the compression rate, the quality of the LLM output, and the retention of key information. This allows for prompt token reduction without needing an external black-box LLM."
  - [Section IV-B, Eq. 4] Full reward function specification with compression rate, BertScore for information retention, and KL divergence terms.
  - [Section IV-D] Describes distribution alignment using Llama3-8B fine-tuned on alpaca-gpt4-data to approximate target LLM behavior.
  - [corpus] No direct corpus papers validate the distribution alignment assumption; this remains untested beyond the paper's experiments.
- **Break condition:** If the small model poorly approximates the target LLM's distribution (e.g., different reasoning patterns), the KL term may provide misleading gradients.

### Mechanism 3
- **Claim:** Curriculum-style progressive difficulty training (HPC) enables the agent to first learn conservative compression before attempting aggressive removal, improving final compression-quality trade-offs.
- **Mechanism:** Training proceeds through P stages where compression rate bounds [cₛ, cₗ] tighten: cₛ = 0.6 - (Pᵢ + t/Tₘₐₓ)ψ, cₗ = 1.0 - (Pᵢ + t/Tₘₐₓ)ψ. Early stages allow high retention (easier), later stages force lower retention (harder). This prevents early catastrophic information loss that could derail learning.
- **Core assumption:** Compression difficulty is monotonic in retention rate; starting with easier tasks transfers to harder ones.
- **Evidence anchors:**
  - [abstract] "We introduce a Hierarchical Prompt Compression (HPC) training strategy that gradually increases the compression difficulty, enabling the DCP-Agent to learn an effective compression method that maintains information integrity."
  - [Section IV-C, Algorithm 1] Formal HPC procedure with P stages, dynamic bound adjustment, and PPO updates.
  - [Table III] Ablation shows LLM-DCP without HPC achieves 76.57 EM vs. 77.03 with HPC, and 5.5x vs. 6.9x compression ratio—a 25.5% relative compression improvement.
  - [corpus] No corpus papers explicitly test curriculum learning for prompt compression; this is a novel application.
- **Break condition:** If compression strategies don't transfer across difficulty levels (e.g., aggressive compression requires qualitatively different decisions), curriculum progression would fail.

## Foundational Learning

- **Markov Decision Processes (MDPs):**
  - **Why needed here:** The entire LLM-DCP framework formulates compression as an MDP. Understanding states, actions, transitions, rewards, and policies is essential to follow Section IV.
  - **Quick check question:** Given a 10-token prompt, if the current state is tokens [1,3,5,7,9] and the agent outputs action [1,0,1,0,1], what is the next state?

- **Proximal Policy Optimization (PPO):**
  - **Why needed here:** The paper uses PPO to train the DCP-Agent. Understanding clipped objective functions and value networks helps parse Algorithm 1 and Eq. 7.
  - **Quick check question:** Why does PPO clip the policy ratio δ = πθ/πθ_old rather than allowing unconstrained updates?

- **KL Divergence for Distribution Matching:**
  - **Why needed here:** The reward function penalizes KL divergence between output distributions. Grasping what KL measures (and its limitations) clarifies Section IV-B.
  - **Quick check question:** If P(original prompt) = [0.5, 0.5] and P(compressed prompt) = [0.9, 0.1], is the KL divergence zero? Why or why not?

## Architecture Onboarding

- **Component map:** Input Prompt (tokens) → [DCP-Agent: xlm-roberta-large encoder + linear classifier] ← Actor (πθ) → Binary Actions (retain/remove per token) → Compressed Prompt → [Aligned Small Model (Llama3-8B)] → Output Distribution → [Critic: xlm-roberta-large + 2 linear layers] ← Value Network (Vφ) → Reward Calculation (Eq. 4) → PPO Update (Eq. 7)

- **Critical path:** The reward function's KL term depends on the aligned small model accurately approximating target LLM outputs. If alignment fails, gradients become unreliable.

- **Design tradeoffs:**
  - **Actor/Critic encoder choice:** xlm-roberta-large provides bidirectional context but adds computational overhead vs. smaller models.
  - **Reward weights (α, β, γ):** Paper sets values implicitly; tuning required per task domain. Over-weighting compression (α) risks information loss; over-weighting retention (β) limits compression gains.
  - **HPC stage count (P=3):** More stages may improve learning but increase training time; fewer stages may cause catastrophic forgetting.

- **Failure signatures:**
  - **Over-compression:** Output EM/ROUGE drops sharply; reward penalty term I(ρ < cₛ)Pₛ should activate. Check if cₛ bound is too low.
  - **Under-compression:** Token count barely reduces; penalty I(ρ > cₗ)Pₗ activates. Check if cₗ bound is too high or agent is reward-hacking.
  - **Unstable training:** PPO clip range ε=0.15 may be too large for this sparse-reward problem. Reduce to 0.1 or increase batch size.
  - **Cross-task failure:** If training on QA data but applying to summarization, compression may remove domain-critical tokens. Verify task diversity in training data.

- **First 3 experiments:**
  1. **Reproduce GSM8K 1-shot results:** Train DCP-Agent on 2048 alpaca-gpt4-data samples, test on GSM8K with target compression 6x. Verify EM ≈ 77 and token count ≈ 343. If EM < 75, check reward weight balance.
  2. **Ablate distribution alignment:** Replace aligned Llama3-8B with random/frozen model. If performance degrades significantly (>2 EM), alignment is critical; if not, the KL term may be redundant.
  3. **Stress-test at extreme compression:** Force 20x compression on Arxiv-March23. Compare output coherence vs. LLMLingua-2. If outputs are garbled, HPC may not generalize to out-of-distribution compression rates.

## Open Questions the Paper Calls Out
None

## Limitations
- **Black-box Distribution Alignment Risk:** The approach relies on a small aligned model (Llama3-8B) to approximate target LLM output distributions. If the black-box LLM has reasoning patterns, knowledge, or generation behaviors significantly different from the aligned model, the KL divergence reward signal may misguide compression decisions, potentially removing tokens critical for the black-box's reasoning process.
- **Task Transferability Concerns:** While presented as task-agnostic, the DCP-Agent is trained on specific datasets (GSM8K, Arxiv-March23). The paper does not demonstrate cross-task robustness—whether a model trained on QA data transfers effectively to summarization, creative writing, or code generation without task-specific fine-tuning.
- **Computational Overhead Trade-off:** The method adds significant complexity: sequential MDP processing, PPO training, and distribution alignment infrastructure. While it reduces inference token count, the upfront computational cost and memory requirements may offset savings, particularly for low-latency applications.

## Confidence
- **High Confidence (8/10):** The sequential MDP formulation improves upon static methods by capturing context-dependent token importance. The ablation showing LLM-DCP outperforming LLMLingua-2 on ROUGE-2 and EM metrics is robust and directly supports this claim.
- **Medium Confidence (6/10):** Distribution alignment through instruction-tuned small models provides sufficient signal for training. While the methodology is sound, the paper lacks external validation of alignment quality beyond in-distribution performance metrics.
- **Low Confidence (4/10):** HPC curriculum training significantly improves final compression-quality trade-offs. The ablation shows modest gains (1.4% EM improvement, 25.5% relative compression increase), but without testing alternative curriculum designs or stage counts, the claimed superiority remains partially supported.

## Next Checks
1. **Cross-Model Distribution Alignment Test:** Train DCP-Agent with three different aligned models (varying sizes, architectures, or instruction-tuning data) and test on the same black-box LLM. Measure correlation between alignment quality (KL divergence on held-out data) and downstream compression performance. If alignment quality doesn't predict performance, the KL reward term may be unreliable.

2. **Out-of-Distribution Compression Stress Test:** Apply models trained on Arxiv-March23 to domains with different compression needs (e.g., medical literature, legal documents, conversational dialogue). Measure performance degradation and analyze which token types the agent systematically removes incorrectly in new domains.

3. **Real-Time Inference Overhead Benchmark:** Measure end-to-end inference time and memory usage for DCP-Agent preprocessing vs. baseline approaches across varying prompt lengths (50, 500, 5000 tokens). Calculate break-even points where compression savings offset preprocessing overhead, and identify scenarios where the approach is counterproductive.