---
ver: rpa2
title: Modeling Image-Caption Rating from Comparative Judgments
arxiv_id: '2602.00381'
source_url: https://arxiv.org/abs/2602.00381
tags:
- comparative
- task
- learning
- caption
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a comparative learning framework to model image-caption
  rating, addressing the challenges of subjective and noisy direct human ratings.
  Instead of predicting absolute scores, the model learns from pairwise preferences,
  determining which image-caption pair better matches each other.
---

# Modeling Image-Caption Rating from Comparative Judgments

## Quick Facts
- arXiv ID: 2602.00381
- Source URL: https://arxiv.org/abs/2602.00381
- Reference count: 3
- Key outcome: Comparative learning achieves correlation levels close to direct regression while offering better scalability and data efficiency for subjective image-caption evaluation.

## Executive Summary
This paper addresses the challenge of subjective and noisy direct human ratings in image-caption quality assessment by proposing a comparative learning framework. Instead of predicting absolute scores, the model learns from pairwise preferences to determine which image-caption pair better matches each other. Using pre-trained ResNet-50 and MiniLM encoders with a shared-weight siamese architecture, the approach achieves comparable performance to direct regression (Pearson's ρ: 0.7609) while being more scalable. A small human evaluation confirms comparative judgments are faster and have higher inter-rater agreement than direct ratings, demonstrating the effectiveness of this alternative approach.

## Method Summary
The framework uses pre-trained ResNet-50 (2048-dim) for visual features and MiniLM-L6-v2 (384-dim) for text features, concatenated into a 2432-dim joint vector. A 4-layer feedforward network with ReLU, batch normalization, and dropout produces scalar scores. For regression, a ranking-penalized MAE loss is used with Adam optimizer and cosine decay. For comparative learning, a siamese architecture with shared weights processes two image-caption pairs, optimizing hinge loss to rank one pair above another. The VICR dataset (15,646 pairs, 68,217 ratings) is split 80/20 for training and testing, with ratings normalized to [0,1] and captions lowercased.

## Key Results
- Regression model achieves Pearson's ρ: 0.7609 and Spearman's rs: 0.7089 on VICR dataset
- Comparative model reaches correlation levels of ~0.67/0.69 with N=20 comparisons per caption
- Comparative model achieves ~0.85 accuracy in identifying the better caption for the same image
- Human evaluation shows comparative judgments are faster and have higher inter-rater agreement than direct ratings

## Why This Works (Mechanism)
The framework leverages comparative learning to sidestep the subjectivity inherent in absolute rating scales. By training on pairwise preferences rather than direct scores, the model learns a relative utility function that is more robust to individual rater biases. The siamese architecture with shared weights ensures both image-caption pairs are evaluated using the same scoring function, making their comparison meaningful. Hinge loss specifically optimizes for correct ranking rather than precise score prediction, which aligns better with the ordinal nature of quality judgments. This approach is particularly effective for subjective tasks where absolute scales vary between annotators.

## Foundational Learning
- **Concept: Siamese Networks & Shared Weights**
  - Why needed here: The comparative model uses two identical networks to process two different image-caption pairs. Understanding why the weights must be shared is crucial—it ensures both inputs are transformed by the *exact same function*, making their output scores directly comparable.
  - Quick check question: If you processed one image-caption pair with a randomly initialized network and the second pair with a trained network, would the score comparison be meaningful?

- **Concept: Ranking Loss (Hinge Loss)**
  - Why needed here: Unlike Mean Squared Error (MSE) which penalizes based on the *difference* between a prediction and a truth value, hinge loss penalizes based on the *margin* between two predictions. This is the mathematical engine that forces the model to rank one item higher than another.
  - Quick check question: If the model correctly ranks Pair A above Pair B with scores of 0.9 and 0.1, vs. scores of 0.51 and 0.50, which scenario incurs *zero* loss under a margin of 0.5?

- **Concept: Multimodal Fusion (Early Fusion)**
  - Why needed here: The model combines image and text features by concatenating them before the final dense layers. This "early fusion" approach allows the network to learn joint features that capture the interaction *between* visual and textual elements, rather than treating them independently.
  - Quick check question: What is the dimensionality of the input vector to the first dense layer if the image encoder outputs 2048 features and the text encoder outputs 384?

## Architecture Onboarding

- **Component map:**
  - Inputs: 1) Image, 2) Caption.
  - Encoders: 1) ResNet-50 (image -> 2048-dim vector), 2) MiniLM (text -> 384-dim vector).
  - Fusion Layer: Concatenation (2048 + 384 = 2432-dim joint vector).
  - Scoring Head: 4 Dense Layers (ReLU, Batch Norm, Dropout) -> Single Scalar Score.
  - Loss Function: Hinge Loss (`max(0, margin - (score_preferred - score_less_preferred))`).

- **Critical path:**
  1. Pre-process images (resize to 224x224) and text (lowercase).
  2. Generate embeddings using frozen pre-trained encoders.
  3. For each training step, sample an anchor pair and a comparison pair.
  4. Pass both pairs through the *same* scoring head.
  5. Calculate the score difference and apply hinge loss.
  6. Backpropagate to update only the scoring head's weights.

- **Design tradeoffs:**
  - **Fixed vs. Fine-tuned Encoders:** The paper uses fixed ResNet and MiniLM. This is faster and requires less GPU memory but may limit performance if the pre-trained features are not perfectly suited to the specific nuance of image-caption alignment. Fine-tuning would likely improve performance but at a significant computational cost.
  - **Random vs. Hard Negative Sampling:** The paper samples comparison pairs randomly. Active or "hard negative" mining (choosing pairs that are difficult to distinguish) could accelerate convergence but was not the chosen approach.

- **Failure signatures:**
  - **Stagnant Loss:** The model fails to distinguish between pairs. This could indicate that the learning rate is too low, or that the `margin` in the hinge loss is too large for the subtle quality differences in the data.
  - **Overfitting:** High training accuracy but poor test correlation. This suggests the model is memorizing specific pairs rather than learning a generalizable utility function.
  - **Intransitivity:** The model creates circular rankings (A>B, B>C, but C>A). This can happen if the training data is noisy or contradictory.

- **First 3 experiments:**
  1. **Baseline Ablation:** Train the regression model on the direct ratings to establish the upper-bound performance benchmark (Pearson ~0.76).
  2. **Data Efficiency Test:** Train the comparative model with an increasing number of random comparisons per image-caption pair (N=1, 2, 5, 10, 20). Plot correlation vs. N to verify the "steady improvement" claim.
  3. **Same-Image Preference Task:** Isolate the subset of data where two captions describe the same image. Train and test the comparative model on only this data to measure accuracy on fine-grained distinctions, as per RQ2.

## Open Questions the Paper Calls Out
- Can this comparative learning framework be successfully extended to complex multimodal matching tasks involving video-text and audio-text data? The conclusion states plans to extend this framework to multimodal matching tasks such as video-text and audio-text, but the current study only validates the framework on static image-caption pairs.
- Can active learning strategies effectively reduce the computational cost of generating pairwise comparisons for large-scale datasets? The discussion identifies the computational burden of creating paired comparisons and suggests "smarter ways like focused selection could help," while the conclusion explicitly mentions plans to explore active learning strategies.
- How can the ordinal utility scores produced by the comparative model be accurately calibrated to predict absolute numerical ratings? The discussion notes a limitation where "turning these into accurate number-based forecasts could need extra stages when exact ratings matter."

## Limitations
- The main claim about comparative learning's superiority is supported primarily by correlation metrics and a small-scale human evaluation (N=8 participants), with the comparison between direct ratings and comparative judgments relying on indirect inference.
- The fixed pre-trained encoders may limit the model's ability to capture domain-specific image-caption relationships, though this trade-off is acknowledged.
- The human evaluation conclusions (comparative judgments are "easier" and have "higher agreement") are based on very small sample sizes and indirect measurements rather than head-to-head performance.

## Confidence
- **High Confidence:** The methodology for comparative learning (siamese architecture, hinge loss) is technically sound and well-established in the literature.
- **Medium Confidence:** The reported correlation improvements and pairwise accuracy gains are plausible given the framework, but depend heavily on implementation details not fully specified.
- **Low Confidence:** The human evaluation conclusions (comparative judgments are "easier" and have "higher agreement") are based on very small sample sizes and indirect measurements.

## Next Checks
1. **Implementation Verification:** Reproduce the regression baseline to confirm the reported Pearson ρ=0.7609 before testing the comparative model.
2. **Data Efficiency Validation:** Systematically vary the number of comparisons per caption (N=1, 2, 5, 10, 20) and plot correlation metrics to verify the claimed steady improvement.
3. **Direct Comparison Test:** Conduct a controlled human evaluation with sufficient participants (N≥30) comparing direct rating vs. comparative judgment on identical image-caption pairs, measuring both time and agreement directly.