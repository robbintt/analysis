---
ver: rpa2
title: Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning
  in Dynamic Environments
arxiv_id: '2507.00030'
source_url: https://arxiv.org/abs/2507.00030
tags:
- action
- learning
- duration
- contextual
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes integrating contextual bandits with Deep Q-Networks
  (DQN) to adaptively select action durations in reinforcement learning, addressing
  the limitation of static action repetition rates in existing DRL methods. The approach
  augments DQN with a contextual bandit module that learns to choose optimal action
  repetition rates based on state contexts, enabling finer-grained temporal control.
---

# Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments

## Quick Facts
- **arXiv ID**: 2507.00030
- **Source URL**: https://arxiv.org/abs/2507.00030
- **Reference count**: 12
- **Primary result**: Bandit-DQN achieves 15% higher scores than DFDQN in Seaquest and 10% higher in Enduro, with overall improvements across five Atari games

## Executive Summary
This paper addresses the limitation of static action repetition rates in deep reinforcement learning by integrating contextual bandits with Deep Q-Networks (DQN). The proposed Bandit-DQN architecture augments standard DQN with a bandit module that learns to select optimal action repetition durations based on state contexts. The bandit module is trained using rewards derived from Q-value changes, enabling adaptive temporal control without manual tuning. Experiments on five Atari 2600 games demonstrate significant performance improvements over static duration baselines and the Dynamic Frameskip DQN method, with the bandit module learning environment-specific duration distributions (60% short durations in Space Invaders vs 45% long durations in Enduro).

## Method Summary
The Bandit-DQN method integrates a contextual bandit module with standard DQN architecture to adaptively select action repetition durations. The system shares convolutional feature extraction (3 conv layers processing 4×84×84 stacked frames) between DQN and bandit components. The bandit module outputs a probability distribution over discrete durations d ∈ {1, 2, ..., 20} and is trained via policy gradient using rewards computed as the difference in Q-values before and after action execution: rb = Q(st+dt, a'; θ) - Q(st, at; θ). The complete architecture includes shared conv layers, DQN head (2 FC layers producing |A| Q-values), bandit head (separate FC layer producing |D| outputs), replay buffer storing (st, at, dt, rt, st+dt) transitions, and target network for stable TD learning.

## Key Results
- Bandit-DQN achieves 15% higher scores than DFDQN in Seaquest and 10% higher in Enduro
- Overall improvements across all five tested games (Seaquest, Space Invaders, Alien, Enduro, Q*Bert)
- Duration selection analysis shows 60% short durations (1-5 frames) for Space Invaders vs 45% long durations (8-11 frames) for Enduro
- The bandit module demonstrates environment-specific learning of optimal duration distributions

## Why This Works (Mechanism)

### Mechanism 1: Contextual Bandit for Duration Selection
A contextual bandit module learns to select appropriate action durations by conditioning on state features, enabling environment-specific temporal adaptation. The bandit receives the same state representation as the DQN and outputs a probability distribution over durations, decoupling duration selection from action selection while sharing perceptual features.

### Mechanism 2: Q-Value Difference as Intrinsic Bandit Reward
The difference between Q-values before and after action execution provides a learning signal for duration selection without requiring separate reward shaping. This captures whether holding the action for a given duration improved the value estimate.

### Mechanism 3: State-Conditioned Duration Distribution Adaptation
Different environments require fundamentally different duration profiles, and the bandit learns these distributions from experience rather than manual tuning. The policy gradient objective shapes the duration distribution toward high-reward selections, developing game-specific priors over time.

## Foundational Learning

- **Contextual Bandits vs. Multi-Armed Bandits**: Why needed here - The paper explicitly contrasts contextual bandits (state-conditioned) with multi-armed bandits (state-independent). Quick check: Given a state s and two durations d1, d2, would a standard multi-armed bandit select between them differently than a contextual bandit?

- **Policy Gradient for Bandit Parameters**: Why needed here - The bandit is updated via REINFORCE-style gradient, not Q-learning. Quick check: Why is policy gradient preferred over value-based methods for the bandit module when both receive scalar rewards?

- **Temporal Abstraction in RL (Options/Semi-MDPs)**: Why needed here - The paper positions adaptive duration as a form of temporal abstraction, complementing options and macro-actions. Quick check: How does learning a duration policy differ from learning option termination conditions?

## Architecture Onboarding

- **Component map**: 4×84×84 stacked frames → Shared conv layers (3 conv) → DQN head (2 FC layers → |A| Q-values) and Bandit head (FC layer → |D| duration probabilities) → Duration sample → Action execution → Reward and next state

- **Critical path**: Observe state st → pass through conv backbone → DQN head outputs Q(st, a; θ) → select at via ε-greedy → Bandit head outputs πb(d | st; θb) → sample dt → Execute at for dt frames → observe rt, st+dt → Compute rb = Q(st+dt, a'; θ) - Q(st, at; θ) → Store transition; sample minibatch; update θ (DQN loss), θb (policy gradient)

- **Design tradeoffs**: Discrete vs continuous durations (paper uses discrete {1,...,dmax}); shared vs separate features (sharing reduces parameters but may create gradient interference); Q-difference reward vs environment reward (intrinsic vs aligned with task)

- **Failure signatures**: Bandit collapses to single duration (insufficient exploration); unstable training with exploding bandit gradients (poor reward scaling); performance no better than ARR=4 (learning rate issues or insufficient training)

- **First 3 experiments**: 1) Baseline comparison on Seaquest: Train Bandit-DQN vs DQN (ARR=4, ARR=20) for 200 epochs, verify 15% improvement; 2) Ablation on bandit reward: Replace Q-difference with environment reward rt, compare convergence and performance; 3) Cross-game duration profile: Train on Space Invaders, evaluate frozen bandit on Enduro to test environment-specific learning

## Open Questions the Paper Calls Out
None

## Limitations
- Bandit module architecture details (layer sizes, learning rate) remain underspecified, creating significant implementation variance risk
- No statistical significance testing reported for performance claims, making it unclear whether observed differences exceed random variation
- Q-value difference reward signal lacks validation against raw environment rewards, raising questions about whether improvements stem from adaptive mechanism or reward shaping effects

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Contextual bandits provide effective temporal adaptation | High |
| 15-25% improvements over static baselines | Medium |
| Q-value differences provide optimal bandit rewards | Low |

## Next Checks

1. Replicate the core experiment (Seaquest) with statistical significance testing (n=10 seeds, t-tests) to verify 15% improvement over ARR=4/20 baselines

2. Implement ablation comparing Q-difference reward vs raw environment reward for bandit updates to isolate the value of the proposed reward formulation

3. Conduct cross-game evaluation: freeze bandit policy from one game (e.g., Space Invaders) and test on another (e.g., Enduro) to validate environment-specific learning claims