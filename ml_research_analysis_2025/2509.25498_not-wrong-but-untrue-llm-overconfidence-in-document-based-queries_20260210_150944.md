---
ver: rpa2
title: 'Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries'
arxiv_id: '2509.25498'
source_url: https://arxiv.org/abs/2509.25498
tags:
- hallucination
- documents
- hallucinations
- tools
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how three widely-used LLMs (ChatGPT, Gemini,
  and NotebookLM) perform on document-based journalism tasks using a 300-document
  corpus on U.S. TikTok policy.
---

# Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries

## Quick Facts
- **arXiv ID:** 2509.25498
- **Source URL:** https://arxiv.org/abs/2509.25498
- **Reference count:** 40
- **Primary result:** NotebookLM (RAG with citations) hallucinated 13% of the time, while ChatGPT and Gemini hallucinated 40% on document-based journalism tasks

## Executive Summary
This study evaluates how three widely-used LLMs perform on document-based journalism tasks using a 300-document corpus on U.S. TikTok policy. The research finds that 30% of outputs contained hallucinations, with Gemini and ChatGPT showing substantially higher rates (40%) compared to NotebookLM (13%). Most errors stemmed from interpretive overconfidence—unsupported characterizations and attribution drift rather than factual fabrication. The results reveal an epistemological mismatch between journalism's source-based standards and LLMs' tendency to generate authoritative-sounding but unverified claims.

## Method Summary
The study used a 300-document corpus (150 news articles, 145 legal documents, 5 academic papers) on U.S. TikTok policy and evaluated three LLM systems: ChatGPT (GPT-5 via Projects), Gemini 2.5 Pro, and NotebookLM. Researchers ran five queries of varying specificity on random document samples of 10, 100, and 300 documents. Outputs were annotated at the sentence level using the Rawte et al. (2023) hallucination taxonomy, which classifies errors by orientation (distortion vs. elaboration), category, and degree (mild/moderate/alarming).

## Key Results
- 30% of outputs contained hallucinations across all three models tested
- NotebookLM (RAG with mandatory citations) hallucinated 13% of the time, versus 40% for ChatGPT and Gemini
- Most errors were interpretive overconfidence—unsupported characterizations of sources and attribution drift—rather than factual fabrication
- Even with specific queries naming documents, models still generated unsupported interpretations of those documents' contents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented architectures with mandatory citation requirements reduce interpretive hallucinations compared to in-context processing
- Mechanism: NotebookLM's RAG system forces explicit source attribution for each claim, creating structural constraints against unsupported interpretation
- Core assumption: The citation infrastructure—not model capability—primarily drives the 3x performance gap between NotebookLM (13%) and ChatGPT/Gemini (40%)
- Evidence anchors: Abstract shows 13% vs 40% hallucination rates; section 5.1 links lower rate to RAG implementation with explicit citations

### Mechanism 2
- Claim: LLMs exhibit systematic "interpretive overconfidence"—adding analytical claims that sound authoritative but lack textual support—rather than fabricating facts
- Mechanism: Models generate fluent text optimized for plausibility, not evidentiary grounding, smoothing over attribution chains and inferring unsupported metadata
- Core assumption: This behavior stems from training objectives that reward fluency over provenance tracking
- Evidence anchors: Abstract identifies most errors as unsupported characterizations; section 5.2 documents claims about document metadata lacking textual basis

### Mechanism 3
- Claim: Prompt specificity reduces but does not eliminate hallucinations; document sample size has limited effect
- Mechanism: More specific prompts constrain the output space but do not solve the fundamental attribution problem
- Core assumption: The epistemological mismatch between journalism's requirements and LLM behavior persists across prompt variations
- Evidence anchors: Section 5.1 shows hallucinations regardless of specificity level; section 4.1 demonstrates hallucinations across all query types

## Foundational Learning

- **Hallucination taxonomy (orientation, category, degree)**
  - Why needed here: The paper uses Rawte et al.'s taxonomy to classify errors, distinguishing distortion from elaboration and severity levels
  - Quick check question: If a model transforms a senator's attributed opinion into a universal fact, what hallucination orientation and category does this represent?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The performance gap between NotebookLM (RAG with citations) and ChatGPT/Gemini (in-context or retrieval without mandatory citations) is the central architectural finding
  - Quick check question: Does RAG inherently prevent hallucinations, or does it require additional constraints (like mandatory citations) to be effective?

- **Epistemological mismatch**
  - Why needed here: The paper's core theoretical contribution is that journalism and LLMs have incompatible evidence-handling paradigms
  - Quick check question: Why might a statement be "not wrong, but untrue" from a journalism perspective even if factually accurate?

## Architecture Onboarding

- **Component map:** Document ingestion (300 documents, chunking) → Retrieval system (NotebookLM RAG, Gemini full-context, ChatGPT Projects) → Generation model (GPT-5, Gemini 2.5 Pro, NotebookLM) → Citation infrastructure (NotebookLM mandatory, others optional) → Human verification

- **Critical path:** 1. Document upload → 2. Retrieval/context assembly → 3. Query processing → 4. Generation with (or without) citation → 5. Human verification of attribution accuracy

- **Design tradeoffs:**
  - Fluency vs. attribution enforcement: Systems optimizing for smooth output produce more interpretive errors
  - Context window vs. retrieval: Full-context doesn't solve attribution; structured retrieval with citations shows better results
  - Generality vs. domain specificity: General-purpose tools lack journalism-specific constraints; custom tools could enforce sourcing

- **Failure signatures:**
  - Attribution drift: Attributed opinions become universal statements (e.g., "Senator X said Y" → "Y is true")
  - Editorializing: Unsupported claims about document purpose, audience, or speaker intent
  - Fabricated quotations: Direct quotes invented but not present in source documents
  - Clustering: When hallucinations occur, they typically appear multiple times per response (1.5–4 per erroneous output)

- **First 3 experiments:**
  1. **Citation enforcement test:** Run identical queries through RAG systems with mandatory vs. optional vs. no citations to measure attribution accuracy
  2. **Prompt specificity ladder:** Test same corpus with 5 specificity levels to identify whether specific queries reduce interpretive overconfidence
  3. **Attribution drift detection:** Design queries with clear attributed opinions; evaluate whether models preserve attribution chains or convert to universal claims

## Open Questions the Paper Calls Out

- **Cross-domain generalizability:** Do journalism-specific hallucination subtypes (e.g., "unsupported source characterization," "attribution drift") generalize across beats, document types, and languages beyond U.S. policy/legal reporting? The study focused on one topic area; no cross-domain validation was conducted.

- **Causal mechanism of RAG:** To what extent does RAG with mandatory citation infrastructure causally reduce interpretive overconfidence versus simply changing output format? The three tools differ in both underlying models and document-handling mechanisms; no controlled comparison isolates the citation requirement.

- **Automated detection:** Can fine-grained taxonomy extensions capturing interpretive overconfidence be reliably operationalized for automated or semi-automated detection in newsroom workflows? The proposed subtypes were identified through manual inductive analysis; no validated annotation scheme exists for these error categories.

## Limitations
- Reliance on human annotation to distinguish "interpretive overconfidence" from standard summarization errors, with detailed guidelines not fully specified
- Performance gap between RAG systems and in-context processing cannot be definitively attributed to citation enforcement alone
- Evaluation limited to a single domain (TikTok policy) and three specific model versions, limiting generalizability

## Confidence
- **High Confidence:** 30% hallucination rate across all models is well-supported by sentence-level annotation methodology
- **Medium Confidence:** NotebookLM's RAG implementation reduces hallucinations, but differences could be influenced by other architectural factors
- **Medium Confidence:** Characterization of most errors as "interpretive overconfidence" is supported by qualitative analysis but relies on subjective annotation decisions

## Next Checks
1. Conduct inter-annotator reliability testing on "interpretive overconfidence" classification to establish consistent boundaries
2. Test same document corpus across multiple RAG implementations with varying citation enforcement strengths
3. Evaluate model performance on additional domains (e.g., medical, legal) to determine if interpretive overconfidence patterns persist