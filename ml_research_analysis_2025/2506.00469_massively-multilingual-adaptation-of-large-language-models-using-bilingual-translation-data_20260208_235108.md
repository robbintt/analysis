---
ver: rpa2
title: Massively Multilingual Adaptation of Large Language Models Using Bilingual
  Translation Data
arxiv_id: '2506.00469'
source_url: https://arxiv.org/abs/2506.00469
tags:
- latn
- llama
- data
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of bilingual translation data
  for massively multilingual language adaptation of the Llama3 family of models to
  500 languages. The authors construct the MaLA bilingual translation corpus containing
  data from more than 2,500 language pairs and develop the EMMA-500 Llama 3 suite
  of four models continually pre-trained from the Llama 3 family of base models on
  diverse data mixes up to 671B tokens.
---

# Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data

## Quick Facts
- arXiv ID: 2506.00469
- Source URL: https://arxiv.org/abs/2506.00469
- Reference count: 40
- Authors demonstrate state-of-the-art machine translation and competitive classification/reasoning performance for 500 languages using bilingual translation data

## Executive Summary
This paper presents EMMA-500, a suite of massively multilingual language models adapted from Llama 3 and Llama 3.1 through continual pre-training on a novel bilingual translation corpus spanning 500 languages. The authors construct the MaLA bilingual translation corpus containing data from over 2,500 language pairs and develop four models trained on diverse data mixes up to 671B tokens. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data significantly enhances language transfer and performance, particularly for low-resource languages, though achieving consistently high performance across diverse benchmarks remains challenging due to inherent trade-offs in multilingual generalization.

## Method Summary
The authors adapt Llama 3/3.1 8B parameter models through full-parameter continual pre-training using a novel MaLA bilingual translation corpus containing over 2,500 language pairs. Training employs GPT-NeoX on 512 AMD MI250x GPUs with 40,000 steps (671B tokens) for bilingual mix and 25,000 steps (419B tokens) for monolingual mix. The methodology includes manual data mixing with aggressive oversampling of low-resource languages, concatenation of translation pairs into pseudo-documents with explicit language tags, and use of a reduced learning rate (1e-4) to prevent training instability observed with the standard 3e-4 rate.

## Key Results
- Bilingual data consistently improves machine translation performance by 9% to 140% compared to monolingual-only training
- EMMA-500 models achieve state-of-the-art results on machine translation benchmarks (Flores200)
- Low-resource language performance shows significant gains through bilingual translation data incorporation
- Trade-offs persist between multilingual generalization and task specialization, with some high-resource language performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inclusion of bilingual translation data (bitext) during continual pre-training improves multilingual performance, particularly for low-resource languages and machine translation tasks.
- **Mechanism:** Parallel texts provide explicit alignment signals between languages. By concatenating source and target sentences into "pseudo-documents," the model learns to associate semantic representations across languages, facilitating cross-lingual transfer.
- **Core assumption:** The model has sufficient residual capacity to learn cross-lingual mappings without catastrophically degrading its pre-trained English capabilities.
- **Evidence anchors:**
  - [abstract] "Bilingual data tends to enhance language transfer and performance, particularly for low-resource languages."
  - [section 3.1] "CPT... with bilingual translation data consistently improves... machine translation... from 9% to 140%."
  - [corpus] Related work on cross-lingual knowledge transfer (e.g., MultiSlav) supports the utility of leveraging shared linguistic structures to combat low-resource data scarcity.

### Mechanism 2
- **Claim:** Heavily pre-trained models (e.g., Llama 3, trained on 15T tokens) exhibit higher resistance to adaptation than earlier versions (e.g., Llama 2, trained on 2T tokens).
- **Mechanism:** As models consume more tokens during initial pre-training, their weights become more rigidly optimized for the pre-training distribution. This reduces the plasticity required to incorporate new linguistic knowledge (500+ new languages) via CPT without destabilizing the model.
- **Core assumption:** The "rigidity" is a function of the pre-training token count (saturation) rather than architectural differences between Llama 2 and 3.
- **Evidence anchors:**
  - [section 3.3] "Heavily pre-trained models (e.g., Llama 3 and 3.1)... are more resistant to further adaptation than English-centric models (e.g., Llama 2)."
  - [section 2.3] Mentions that using the original Llama 3 learning rate (0.0003) caused instability, forcing a reduction to 0.0001.
  - [corpus] Related research on domain adaptation suggests mechanisms for knowledge acquisition are complex, but this paper specifically links resistance to the volume of pre-training data.

### Mechanism 3
- **Claim:** Deliberate oversampling of low and medium-resource languages in the training mix prevents the model from collapsing into a high-resource (English-dominant) solution.
- **Mechanism:** In a massively multilingual setup, high-resource languages dominate the gradient updates. By manually curating the mix to overrepresent low-resource data (and oversampling specific pairs), the model is forced to maintain representational fidelity for rare tokens.
- **Core assumption:** The quality of the low-resource data is sufficient to provide a learning signal, rather than just noise.
- **Evidence anchors:**
  - [section 2.2] "We manually mix up different types of data... ensuring that low- and medium-resource languages remain overrepresented."
  - [table 10] Shows aggressive oversampling rates (e.g., 20x to 50x) for low-resource categories compared to high-resource.

## Foundational Learning

- **Concept: Continual Pre-Training (CPT)**
  - **Why needed here:** The paper adapts a frozen model (Llama 3) to new domains (languages) without training from scratch. Understanding CPT vs. fine-tuning is critical, as CPT uses a self-supervised objective (causal language modeling) rather than supervised instruction tuning.
  - **Quick check question:** How does the learning rate schedule in CPT differ from instruction fine-tuning?

- **Concept: Tokenization & Vocabulary Expansion**
  - **Why needed here:** The paper notes they did *not* modify the tokenizer. This implies the model must map new languages into existing sub-word units (likely BPE/Byte-level). Success depends on the existing vocabulary's coverage of Unicode scripts.
  - **Quick check question:** Why might retaining the original tokenizer harm performance for low-resource languages not represented in the original pre-training set?

- **Concept: Cross-Lingual Transfer**
  - **Why needed here:** The core value proposition of bilingual data is enabling transferâ€”learning a task in one language to improve performance in another.
  - **Quick check question:** Does the paper suggest that improving English performance helps low-resource languages, or vice versa?

## Architecture Onboarding

- **Component map:** Base Models (Llama 3/3.1 8B) -> Data Pipeline (MaLA Corpus -> Normalization -> Script Recognition -> Pseudo-document formatting) -> Training Stack (GPT-NeoX on AMD MI250x GPUs) -> Evaluation (lm-evaluation-harness + custom scripts)

- **Critical path:**
  1. **Data Formatting:** Group sentence pairs into chunks of 10 (pseudo-documents) with explicit language tags (`[src_lang]`, `[tgt_lang]`). This format is crucial for teaching the model alignment.
  2. **Stability Management:** Use a lower learning rate (0.0001) than the base model default to prevent spikes.
  3. **Ablation:** Run Mono vs. Bilingual mixes to isolate the effect of parallel data.

- **Design tradeoffs:**
  - **No Tokenizer Expansion:** Simplifies deployment and maintains compatibility with the base ecosystem, but may limit the "fluency" of languages poorly covered by the original vocabulary.
  - **Full-Parameter CPT:** Maximizes adaptation potential but requires significant compute (800k GPU hours) compared to parameter-efficient methods (like LoRA).
  - **Manual Data Mixing:** Relies on intuition/experience rather than algorithmic search due to cost.

- **Failure signatures:**
  - **Training Instability:** "Fast spikes" in loss, corrected by lowering the learning rate from 0.0003 to 0.0001.
  - **Performance Degradation:** High-resource languages (e.g., English, French) or reasoning tasks (Math) may degrade (Section 3.5) due to the "trade-off in multilingual generalization and task specialization."

- **First 3 experiments:**
  1. **Sanity Check (Tokenizer):** Evaluate the base Llama 3 model on low-resource languages in the MaLA corpus to establish a baseline and verify if the tokenizer produces excessive "unknown" or byte-level tokens.
  2. **Learning Rate Sweep:** Run small-scale CPT (5k steps) with the "Bilingual Mix" using Llama 3's original LR (3e-4) vs. the paper's recommended LR (1e-4) to replicate the instability finding.
  3. **Zero-Shot Translation:** After CPT, test the model on translation directions strictly *not* seen in the parallel training data to measure true cross-lingual transfer capabilities.

## Open Questions the Paper Calls Out
- **Data Mixing Optimization:** Searching for optimal data composition ratios remains an interesting direction because systematic grid search is computationally prohibitive.
- **Catastrophic Forgetting:** How to mitigate performance degradation in high-resource languages while extending to hundreds of low-resource languages remains unresolved.
- **Evaluation Validity:** The extent to which translated evaluation benchmarks obscure true linguistic capabilities of multilingual models is unclear due to lack of native-language test sets.

## Limitations
- Performance trade-offs persist between multilingual generalization and task specialization
- Data quality and representation gaps exist for low-resource language pairs
- Tokenizer limitations may systematically disadvantage certain scripts or language families

## Confidence
**High Confidence:** Bilingual translation data improves machine translation performance and benefits low-resource languages; continual pre-training with bilingual data is more effective than monolingual-only approaches; heavily pre-trained models are more resistant to adaptation.

**Medium Confidence:** Oversampling strategies effectively prevent high-resource language dominance; the 8B parameter scale is sufficient for 500-language adaptation; performance improvements translate to practical utility.

**Low Confidence:** The model achieves "state-of-the-art" performance across all evaluated tasks; the adaptation approach scales efficiently to even larger model sizes; the learned cross-lingual representations generalize to truly unseen language pairs.

## Next Checks
1. **Zero-shot Cross-Lingual Transfer Evaluation:** Test EMMA-500 models on translation pairs never seen during training to quantify genuine cross-lingual transfer capabilities beyond memorization of training pairs.

2. **High-Resource Language Degradation Analysis:** Conduct detailed ablation study measuring performance degradation on high-resource languages after adaptation to quantify the actual cost of multilingual expansion.

3. **Tokenizer Coverage Impact Assessment:** Compare adapted model's performance with a tokenizer specifically expanded for the 500 target languages to isolate whether tokenizer limitations are constraining adaptation effectiveness.