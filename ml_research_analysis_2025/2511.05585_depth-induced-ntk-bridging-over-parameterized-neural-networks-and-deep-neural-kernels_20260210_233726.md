---
ver: rpa2
title: 'Depth-induced NTK: Bridging Over-parameterized Neural Networks and Deep Neural
  Kernels'
arxiv_id: '2511.05585'
source_url: https://arxiv.org/abs/2511.05585
tags:
- kernel
- neural
- networks
- deep
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a depth-induced neural tangent kernel (NTK)
  to bridge over-parameterized neural networks and deep neural kernels. The key innovation
  is defining an NTK based on shortcut-related network architectures that converges
  to a Gaussian process as network depth approaches infinity, addressing the limitation
  of existing NTK theories that focus on infinite width rather than depth.
---

# Depth-induced NTK: Bridging Over-parameterized Neural Networks and Deep Neural Kernels

## Quick Facts
- arXiv ID: 2511.05585
- Source URL: https://arxiv.org/abs/2511.05585
- Reference count: 40
- Primary result: Proposes depth-induced NTK based on shortcut architectures that converges to Gaussian process as depth approaches infinity

## Executive Summary
This paper introduces depth-induced neural tangent kernel (NTK) that addresses the gap between existing NTK theories focused on infinite width and practical deep neural networks. The key innovation is defining an NTK based on shortcut-related network architectures where layers separated by a constant ℏ become weakly dependent, enabling convergence to a Gaussian process as network depth approaches infinity. The authors theoretically analyze three main properties: training invariance, spectrum properties (bounds on eigenvalues and singular values), and existence of the Gaussian process limit.

## Method Summary
The method defines depth-induced NTK(d) using shortcut-connected networks where output is computed as a mean aggregation of activations at shortcut layers. The NTK is computed as a sum over these shortcut layers of scaled inner products of parameter gradients. The theoretical analysis relies on establishing weak dependence between layers separated by ℏ using the "stable-pertinent" weight constraint, enabling application of the Generalized Central Limit Theorem to prove convergence to a Gaussian process. Experimental validation uses kernel regression on synthetic and standard datasets (MNIST, Fashion-MNIST, CIFAR-10) with networks trained using vanilla gradient descent.

## Key Results
- NTK(d) converges to Gaussian process as depth approaches infinity, with weak dependence emerging through ℏ-separated shortcut architecture
- Smallest eigenvalue lower bound grows at least linearly with input dimension (λ_min ≥ Ω(d))
- Largest singular value upper bound grows quadratically with shortcut count (σ_max ≤ K(2K+1)/6n²_max)
- Experimental results show NTK(d) achieves comparable performance to traditional width-induced NTK across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1: Weak Dependence via Shortcut-Related Architecture
The shortcut-related network architecture creates weakly dependent sequences across layers, enabling convergence to Gaussian process as depth approaches infinity. Layers separated by ℏ become weakly dependent through the "stable-pertinent" weight constraint (C_φ||W||_s ≤ ε < 1), breaking inter-layer correlations that prevent applying the classical Central Limit Theorem.

### Mechanism 2: Block Diagonal Scaling of Gradient Inner Products
The depth-induced NTK employs block diagonal matrices to scale parameter gradients, creating a kernel representation that depends only on shortcut-connected layers. This structure ensures computational tractability and enables the weak dependence property.

### Mechanism 3: Spectral Bounds and Training Invariance
The smallest eigenvalue lower bound (Ω(d)) prevents kernel degeneration while the largest singular value upper bound (O(K²)) constrains condition number for stable training. Training invariance emerges when width n scales as Ω(K²/λ), maintaining kernel stability during optimization.

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK)**
  - **Why needed here:** NTK is the core mathematical object bridging neural network gradient descent and kernel methods
  - **Quick check question:** Can you explain why NTK(w) requires infinite width to remain invariant during training?

- **Concept: Weak Dependence and β-Mixing Sequences**
  - **Why needed here:** The theoretical contribution relies on proving weak dependence to apply the Generalized Central Limit Theorem
  - **Quick check question:** What condition on layer separation ℏ ensures that {z_κℏ} becomes a weakly dependent subsequence?

- **Concept: Spectral Analysis of Kernel Matrices**
  - **Why needed here:** The paper's theoretical contribution includes bounds on eigenvalues and singular values of the NTK matrix
  - **Quick check question:** Why does a lower bound on λ_min relate to generalization capacity in kernel regression?

## Architecture Onboarding

- **Component map:** Input layer (d dimensions) → L hidden layers with ReLU/Leaky ReLU activation → K shortcut connections at positions {ℏ, 2ℏ, ..., Kℏ} → Mean aggregation output
- **Critical path:**
  1. Forward propagation through z^l = ϕ(W^l z^{l-1}/√n_l)
  2. Output computation via f(x) = (1/√M_z) Σ_{κ=0}^K J_{κℏ} z_{κℏ}(x)
  3. NTK(d) computation via sum over shortcut layers of scaled gradient inner products
  4. Kernel regression: μ* = K(X*,X)K(X,X)^{-1}Y^T
- **Design tradeoffs:**
  - Larger ℏ → stronger weak dependence but requires deeper networks
  - Larger K → better approximation to infinite-depth limit but increases σ_max bound quadratically
  - Width n must scale as Ω(K²/λ) for training invariance → computational cost increases with depth
  - Narrow hidden width (n=20 in experiments) prioritizes depth contribution over width
- **Failure signatures:**
  - Gradient vanishing/explosion if weights violate stable-pertinent constraint
  - NTK degeneration to zero if data distribution is not well-scaled
  - Training invariance loss if width-depth ratio n/K² is too small
  - Performance degradation if ℏ is too small (insufficient separation for weak dependence)
- **First 3 experiments:**
  1. Sine function kernel regression: Train wide and deep architectures for 5 epochs, compute NTK(w) and NTK(d) kernels, fit sin(x) on [0,π] using kernel regression
  2. Spectral bound validation: Vary input dimension d from 0 to 500, compute λ_min(NTK(d)) for different sample sizes N; vary K from 10 to 100 and compute σ_max
  3. Training invariance test: Train shortcut-related networks with varying K (5, 10, 15) and matched widths n=K²; compute NTK(d) at initialization and after 20 epochs

## Open Questions the Paper Calls Out

### Open Question 1
Can the depth-induced NTK framework be generalized to other neural network architectures beyond the specific shortcut-related architecture, such as standard residual networks or transformer architectures? The authors state "it is interesting to extend this investigation to a broader class of architectures in future work."

### Open Question 2
Does the smallest eigenvalue of NTK(d) admit an upper bound that scales linearly with input dimension, i.e., λ_min(NTK(d)) ≤ O(d)? Section 4.4 notes "This suggests that the upper bound might also scale linearly with d... which we leave as future work."

### Open Question 3
What is the theoretical mechanism by which NTK(d) overcomes the severe performance degradation that NTK(w) suffers as networks become deeper? The conclusion states "Future research could further investigate the underlying mystery of why our depth-induced NTK(d) kernel overcomes this degradation."

## Limitations
- The stable-pertinent weight constraint may be difficult to maintain in practical initialization schemes
- Training invariance requires quadratic scaling of width with depth (n = Ω(K²/λ)), which may be computationally prohibitive
- Limited validation on real-world problems with complex data distributions where well-scaled assumption might fail

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Theoretical framework and existence proof for depth-induced NTK convergence | High |
| Practical utility of depth-induced NTK with comparable performance to width-induced NTK | Medium |
| Robustness of training invariance property across different architectures and data distributions | Low |

## Next Checks
1. Systematically vary initialization variance and weight scaling to test how deviations from the stable-pertinent constraint affect NTK convergence and training invariance
2. Test depth-induced NTK on architectures with different shortcut patterns (non-uniform spacing, skip connections) to verify whether the weak dependence property generalizes beyond the regular ℏ-separated pattern
3. Apply depth-induced NTK to regression tasks on high-dimensional tabular data or time series where the well-scaled assumption may not hold, and measure performance degradation compared to width-induced NTK