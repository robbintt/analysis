---
ver: rpa2
title: 'EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time
  Evolutionary Learning'
arxiv_id: '2601.22964'
source_url: https://arxiv.org/abs/2601.22964
tags:
- agent
- action
- cost
- diagnosis
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EvoClinician is a self-evolving agent for multi-turn medical diagnosis
  that learns at test time via a Diagnose-Grade-Evolve loop. It uses action-level
  feedback from a Process Grader to update both its prompt rules and external memory
  between cases, balancing diagnostic accuracy against resource efficiency.
---

# EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning

## Quick Facts
- **arXiv ID:** 2601.22964
- **Source URL:** https://arxiv.org/abs/2601.22964
- **Reference count:** 34
- **Key outcome:** EvoClinician improved mean diagnostic grade while reducing turns and normalized costs compared to static and memory-only baselines, with action-level grading and cost-aware feedback identified as key drivers.

## Executive Summary
EvoClinician introduces a self-evolving agent for multi-turn medical diagnosis that learns at test time through a Diagnose-Grade-Evolve loop. The agent interacts with patients by asking questions and ordering tests, balancing diagnostic accuracy against resource efficiency. After each case, a Process Grader provides action-level feedback, and an Evolver updates the agent's strategic rules and experiential memory. Experiments across four model backbones show improvements in diagnostic grade while reducing both turns and costs compared to static baselines.

## Method Summary
EvoClinician uses a three-agent loop: an Actor (LLM with prompt rules and memory) interacts with the environment, a Process Grader evaluates each action's clinical yield and efficiency, and an Evolver updates the Actor's prompt rules and memory entries between cases. The system operates on the DiagnosisArena dataset of 915 real-world clinical cases. The Actor chooses between asking questions, ordering tests, or submitting diagnoses, with the Process Grader assigning labels like HIGH_YIELD, LOW_YIELD, INEFFICIENT, or CRITICAL_ERROR to each action. The Evolver then proposes prompt rule edits and memory additions/deletions within budget constraints.

## Key Results
- EvoClinician improved mean diagnostic grade compared to static and memory-only baselines across four model backbones
- The method reduced both the number of turns and normalized costs while maintaining diagnostic accuracy
- Action-level grading and cost-aware feedback were identified as key drivers of performance gains
- Ablation studies showed significant performance drops when removing action-level grading or cost awareness

## Why This Works (Mechanism)

### Mechanism 1: Action-level grading for effective credit assignment
The Process Grader assigns discrete labels to each turn based on clinical yield and resource efficiency, creating dense signals that link final outcomes to specific earlier actions. This enables the Evolver to identify which action patterns help or harm diagnostic performance.

### Mechanism 2: Complementary roles of prompt evolution and memory evolution
Prompt rules encode generalizable strategic guidance (e.g., "ask about X before ordering Y"), while memory stores specific context-action-outcome triples for retrieval. Rules transfer across broad contexts; memory handles rare cases rules miss.

### Mechanism 3: Cost-aware feedback prevents over-testing
The Process Grader evaluates both clinical yield and resource efficiency, labeling actions that provide useful information but could have been obtained more cheaply as INEFFICIENT. This signals the Evolver to prefer lower-cost alternatives.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: Med-Inquire explicitly models diagnosis as POMDP where the hidden state is the complete case file; the agent only observes responses to its actions
  - Quick check question: Can you explain why a medical diagnosis agent cannot observe the full state, and what action types it can use to reduce uncertainty?

- **Concept: Credit Assignment in Long-Horizon Tasks**
  - Why needed here: Multi-turn diagnosis has delayed rewards; identifying which early actions contributed to success/failure is the core learning challenge
  - Quick check question: Why is a single final score insufficient for learning which questions or tests were good decisions?

- **Concept: Dense vs Sparse Reward Signals**
  - Why needed here: The Process Grader transforms sparse final judgment into dense per-action labels, enabling gradient-free updates after each episode
  - Quick check question: How does per-action feedback change what the Evolver can learn compared to end-of-episode feedback only?

## Architecture Onboarding

- **Component map:** Actor (LLM + rules + memory) -> Process Grader (labels actions) -> Evolver (updates rules/memory) -> Environment (Patient/Examination/Judge/Cost agents)
- **Critical path:** Diagnose (Actor interacts until SubmitDiagnosis) → Grade (Process Grader labels each turn) → Evolve (Evolver updates rules and memory) → repeat for next case
- **Design tradeoffs:**
  - Rule budget B_P: Larger allows more specific rules but increases prompt length and potential conflicts
  - Memory budget B_M: Larger retains more rare-case experiences but slows retrieval
  - Retrieval count k: Higher k provides more context but may include noisy suggestions
- **Failure signatures:**
  - Policy drift: Rules become contradictory after noisy grading; detect via rule redundancy or conflicting recommendations
  - Over-testing without cost awareness: Grade improves but cost remains high; check for missing INEFFICIENT labels
  - Memory pollution: Retrieval surfaces low-quality entries; check retrieval frequency and grade distribution in memory
- **First 3 experiments:**
  1. Reproduce the Diagnose–Grade–Evolve loop on 50 cases with a single backbone; log per-episode grade, turns, cost, and rule/memory changes to verify update logic
  2. Ablate action-level grading (use transcript-level feedback only) on same 50 cases; expect grade drop and cost increase as per Table 3
  3. Run yield-only grading (no cost criterion) on 50 cases; expect grade similar to full method but higher cost, confirming cost-awareness role

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims rely on a single ablation study across four model backbones without external validation sets
- Key hyperparameters (B_P, B_M, k, α/β) and the complete cost table are not specified
- Method relies on a proprietary dataset (DiagnosisArena) limiting independent verification
- The study doesn't explore different parameter settings for architectural design choices

## Confidence
- Performance improvement over baselines: **Medium**
- Action-level grading as key mechanism: **High**
- Cost-awareness preventing over-testing: **Medium**
- Complementary roles of prompt rules and memory: **Medium**

## Next Checks
1. Replicate the Diagnose-Grade-Evolve loop on a small held-out validation set to confirm generalization beyond training order
2. Systematically vary the cost-awareness parameter (e.g., remove INEFFICIENT labels) to quantify its marginal contribution to cost reduction
3. Test different memory retrieval counts (k=1,3,5) to determine optimal context trade-off for clinical reasoning