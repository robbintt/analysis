---
ver: rpa2
title: 'Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small Models
  for Enhanced Reasoning Distillation'
arxiv_id: '2502.12744'
source_url: https://arxiv.org/abs/2502.12744
tags:
- reasoning
- paths
- small
- sert
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing reasoning capabilities
  in small language models, which typically struggle to match the performance of larger
  models. The authors propose Self-Enhanced Reasoning Training (SERT), a method that
  leverages the latent reasoning capabilities within small models by first generating
  and filtering high-quality reasoning paths through enhanced sampling techniques,
  then fine-tuning the model on these filtered paths to increase the frequency and
  coherence of reasoning path generation.
---

# Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small Models for Enhanced Reasoning Distillation

## Quick Facts
- arXiv ID: 2502.12744
- Source URL: https://arxiv.org/abs/2502.12744
- Reference count: 27
- Primary result: SERT improves small model reasoning by generating and filtering high-quality reasoning paths, achieving 57.21% accuracy on StrategyQA and 26.03% on CommonsenseQA

## Executive Summary
This paper introduces Self-Enhanced Reasoning Training (SERT), a method to improve reasoning capabilities in small language models by activating their latent reasoning abilities. The approach addresses the challenge that small models typically underperform larger models in reasoning tasks by leveraging enhanced sampling techniques to generate reasoning paths, filtering them for quality, and fine-tuning on the filtered paths to increase reasoning frequency and coherence. Experiments demonstrate that SERT significantly improves both the quality of generated reasoning paths and task accuracy compared to direct reasoning distillation approaches.

## Method Summary
SERT operates through a three-stage pipeline: enhanced sampling to generate diverse reasoning paths, quality filtering to select high-quality paths based on teacher model evaluation and coherence metrics, and fine-tuning on the filtered paths to reinforce reasoning capabilities. The method specifically addresses the issue that small models often generate repetitive or low-quality reasoning paths during direct distillation by first expanding the reasoning path space through sampling, then selectively reinforcing the best paths. This creates a self-enhancement cycle where the model learns to generate more frequent and coherent reasoning through exposure to its own high-quality outputs.

## Key Results
- SERT+RD models achieved 57.21% accuracy on StrategyQA compared to baseline reasoning distillation
- Repetition rates decreased significantly in SERT-generated reasoning paths
- Coherence scores improved across both StrategyQA and CommonsenseQA datasets
- The method demonstrates particular effectiveness in generating high-quality reasoning paths even when base model performance is limited

## Why This Works (Mechanism)
SERT works by addressing the fundamental limitation of direct reasoning distillation, where small models struggle to generate diverse and high-quality reasoning paths. By first expanding the reasoning path space through enhanced sampling and then filtering for quality, the method ensures that fine-tuning focuses on reinforcing the model's best reasoning capabilities rather than its average or poor outputs. This selective reinforcement increases both the frequency and coherence of reasoning path generation, effectively activating latent capabilities that exist but are not reliably expressed in standard fine-tuning scenarios.

## Foundational Learning
- **Reasoning path generation**: The process by which language models produce step-by-step logical explanations for answering questions. Why needed: Understanding this is crucial for grasping how SERT improves reasoning quality through sampling and filtering.
- **Knowledge distillation**: A training technique where a smaller "student" model learns from a larger "teacher" model. Why needed: SERT builds upon and enhances traditional distillation methods specifically for reasoning tasks.
- **Perplexity filtering**: Using language model uncertainty scores to assess reasoning path quality. Why needed: This metric is central to SERT's quality filtering mechanism.
- **Coherence scoring**: Evaluating the logical consistency and flow of reasoning paths. Why needed: Coherence is a key metric for assessing the quality of generated reasoning in SERT.
- **Enhanced sampling techniques**: Methods to generate diverse outputs beyond standard sampling approaches. Why needed: These techniques enable SERT to discover latent reasoning capabilities in small models.

## Architecture Onboarding

**Component Map**: Enhanced Sampling -> Quality Filtering -> Fine-tuning -> Reasoning Path Generation

**Critical Path**: The sequence from generating reasoning paths through enhanced sampling, filtering them based on quality metrics, to fine-tuning on the selected paths represents the core workflow that drives SERT's effectiveness.

**Design Tradeoffs**: SERT trades computational efficiency for quality by requiring multiple teacher model interactions during sampling and filtering phases, but this investment yields significantly better reasoning capabilities in the student model.

**Failure Signatures**: The method may fail if the teacher model cannot reliably distinguish high-quality from low-quality reasoning paths, or if the filtering thresholds are too strict and eliminate potentially useful reasoning diversity.

**First Experiments**:
1. Compare reasoning path quality metrics (repetition rate, coherence) between SERT and direct distillation approaches
2. Measure the impact of different perplexity thresholds on downstream task accuracy
3. Evaluate the robustness of SERT across different small model sizes (GPT-2 small, medium, large)

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does SERT generalize to modern, instruction-tuned architectures (e.g., LLaMA-3, Mistral), or is the presence of "latent reasoning" a specific artifact of the GPT-2 architecture?
- Basis in paper: [inferred] The "Implementation Details" explicitly restrict the student models to GPT-2 variants (small, medium, large) and the teacher to GPT-3.5.
- Why unresolved: The paper does not demonstrate if the observation that "small models can generate high-quality reasoning paths during sampling" holds true for newer architectures with different pre-training objectives.
- What evidence would resolve it: Applying the SERT methodology to recent small open-source models (e.g., LLaMA-3-8B or Gemma-2B) and measuring the yield of latent reasoning paths.

### Open Question 2
- Question: Can SERT effectively activate reasoning capabilities for tasks requiring strict logical or symbolic manipulation, such as mathematical reasoning?
- Basis in paper: [inferred] The experiments are limited to CommonsenseQA and StrategyQA, which rely heavily on world knowledge and semantic reasoning rather than formal logic.
- Why unresolved: GPT-2 is notoriously weak at arithmetic; it is unclear if self-training on filtered latent paths is sufficient to induce capabilities that are nearly absent in the base model's distribution.
- What evidence would resolve it: Evaluation of SERT-trained models on mathematical benchmarks like GSM8K to assess if logical coherence improves alongside semantic coherence.

### Open Question 3
- Question: How sensitive is the SERT pipeline to the specific heuristic thresholds used in the filtering stage (e.g., perplexity > 5, repetition < 20%)?
- Basis in paper: [inferred] The "Implementation Details" describe specific cutoff values for filtering, but the ablation study focuses on the *presence* of filtering steps rather than the *robustness* of the hyperparameters.
- Why unresolved: Optimal perplexity thresholds likely vary significantly between datasets and model sizes; fixed thresholds might filter out valid reasoning in some contexts or retain noise in others.
- What evidence would resolve it: A sensitivity analysis plotting downstream reasoning performance against varying perplexity and repetition filter thresholds across different datasets.

## Limitations
- Experimental scope limited to GPT-2 architectures with GPT-3.5 as teacher, raising generalizability concerns
- Absolute performance levels remain modest despite improvements (StrategyQA: 57.21% vs. >80% for frontier models)
- Computationally expensive teacher model interactions required for enhanced sampling and filtering

## Confidence
- Reasoning Path Quality Improvements (High): Quantitative metrics and controlled comparisons provide robust evidence
- Task Accuracy Improvements (Medium): Documented improvements but limited to two datasets with modest absolute performance
- Latent Reasoning Activation Theory (Medium): Conceptual framework well-articulated but empirical evidence could be strengthened

## Next Checks
1. Test SERT across diverse model architectures (LLaMA, Mistral, or open-source alternatives) to assess generalizability beyond GPT variants
2. Conduct ablation studies isolating the contributions of each SERT component to understand which mechanisms drive performance gains
3. Evaluate long-term retention and generalization by testing models on out-of-distribution reasoning tasks not seen during training or fine-tuning