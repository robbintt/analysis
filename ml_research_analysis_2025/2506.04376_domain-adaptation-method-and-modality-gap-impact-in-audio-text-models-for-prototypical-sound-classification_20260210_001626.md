---
ver: rpa2
title: Domain Adaptation Method and Modality Gap Impact in Audio-Text Models for Prototypical
  Sound Classification
arxiv_id: '2506.04376'
source_url: https://arxiv.org/abs/2506.04376
tags:
- background
- audio
- sound
- classification
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how background sound sources degrade zero-shot
  environmental sound classification performance in audio-text models. The core problem
  is that audio-text models struggle to coherently represent background as a high-level
  concept, leading to classification errors when sounds are embedded within complex
  soundscapes.
---

# Domain Adaptation Method and Modality Gap Impact in Audio-Text Models for Prototypical Sound Classification

## Quick Facts
- arXiv ID: 2506.04376
- Source URL: https://arxiv.org/abs/2506.04376
- Authors: Emiliano Acevedo; Martín Rocamora; Magdalena Fuentes
- Reference count: 0
- One-line primary result: Background subtraction method improves zero-shot environmental sound classification in polyphonic environments by quantifying and removing background sound contributions.

## Executive Summary
This paper addresses the challenge of zero-shot environmental sound classification when sounds are embedded within complex soundscapes containing background noise. The core insight is that audio-text models struggle to coherently represent background as a high-level concept, leading to classification errors. The authors propose a domain adaptation method that quantifies and subtracts the contribution of background sound sources from the classification process using profile subtraction. This approach significantly improves accuracy across various background types and SNR conditions without requiring model retraining, with audio-based adaptation outperforming text-based methods. The study also demonstrates that narrowing the modality gap between audio and text embeddings through Text-Guided Audio Prototypes (TGAP) further enhances classification performance.

## Method Summary
The method operates in a shared audio-text embedding space where class prototypes are compared to query audio embeddings via cosine similarity. For each test audio mixture, a "profile" is computed as a vector of cosine similarities between the audio embedding and all class prototypes. Background profiles are derived either from text prompts describing the background or from actual background audio recordings. The final classification profile is obtained by subtracting a scaled background profile (Pf = Ps − Pb × τ) from the test profile, with τ optimized via grid search. The approach uses LION-CLAP as the audio-text model and can incorporate TGAP to reduce the modality gap by computing class prototypes from nearest unlabeled audio embeddings rather than text anchors alone.

## Key Results
- Background subtraction method significantly improves zero-shot classification accuracy across various background types (Park, Airport, Shopping Mall, Street Traffic, Public Square, Metro Station) and SNR conditions (6-10 dB)
- Audio-based background profiles yield larger accuracy gains than text-based profiles, with optimal τ values of 0.7 (audio) vs 0.2 (text)
- TGAP (Text-Guided Audio Prototypes) further improves performance by narrowing the modality gap between audio and text embeddings
- Method generalizes effectively across state-of-the-art prototypical approaches without requiring model retraining
- Performance degradation correlates with lower SNR, with values below 6 dB making foreground indistinguishable

## Why This Works (Mechanism)

### Mechanism 1: Profile Subtraction for Background Removal
The method computes profiles (cosine similarity vectors between audio embeddings and class prototypes) and subtracts background contributions to reduce spurious class confusions. The background contribution is approximately additive in profile space, so subtraction attenuates background-driven confusions without destroying foreground signal. The degree of subtraction is controlled by τ, with optimal values found via grid search (τ≈0.2 for text, τ≈0.7 for audio).

### Mechanism 2: Audio-Based Background Profiles Outperform Text
Audio-based background profiles yield larger gains because text prompts poorly capture the background concept in ATMs. Background-only scenes achieve only 33.0% classification accuracy, with errors systematically correlating with constituent background sounds. Actual background recordings provide more precise embeddings, enabling better-aligned background profiles and more effective subtraction.

### Mechanism 3: Modality Gap Reduction via TGAP
TGAP improves classification by better aligning class prototypes to audio embeddings. It retrieves N nearest unlabeled audio embeddings to a text prompt and computes their centroid as the class prototype, pulling prototypes closer to actual audio embeddings than text anchors alone. This reduces cosine distances and improves alignment, though not identical to real audio centroids.

## Foundational Learning

- Concept: Zero-shot prototypical classification
  - Why needed here: The method operates in a shared audio-text embedding space where class prototypes are compared to query audio embeddings via cosine similarity
  - Quick check question: Given an audio embedding and three class prototype vectors, can you compute their cosine similarities and identify the top-1 predicted class?

- Concept: Modality gap in contrastive models
  - Why needed here: ATMs trained with contrastive loss often exhibit a modality gap—audio and text embeddings cluster in separate regions, explaining why text anchors are suboptimal
  - Quick check question: If audio and text embeddings are linearly separable in the latent space, what does that imply for nearest-neighbor classification across modalities?

- Concept: Signal-to-noise ratio (SNR) in soundscapes
  - Why needed here: Performance degradation is primarily SNR-driven; the method's effectiveness varies with SNR and τ must be tuned accordingly
  - Quick check question: At lower SNR (e.g., 6 dB vs. 10 dB), do you expect background subtraction to require a larger or smaller τ, and why?

## Architecture Onboarding

- Component map: Audio Encoder -> Audio Embeddings -> Profile Computation -> Background Profile Subtraction -> Classification
- Critical path: 1) Encode test audio mixture -> 2) Compute test profile (cosine sims vs. class prototypes) -> 3) Obtain background profile (text prompts or background audio) -> 4) Subtract scaled background profile (τ) -> 5) Classify via argmax on refined profile
- Design tradeoffs:
  - Text vs. audio background profiles: Text requires no recordings but yields smaller gains; audio yields larger gains but needs representative background data
  - Zero-Shot vs. TGAP prototypes: TGAP reduces modality gap and improves accuracy but requires unlabeled in-domain audio and adds retrieval/centroid computation cost
  - τ selection: Fixed grid-searched values work across conditions but are not adaptive; context-specific tuning can marginally improve results
- Failure signatures:
  - Over-subtraction (τ too high): True foreground class scores drop; accuracy declines or falls below baseline
  - Unrepresentative background audio: Subtraction introduces noise; minimal or negative gains
  - No unlabeled audio for TGAP: Must fall back to text anchors; reduced but still usable performance
  - Extreme SNR (≤6 dB): Foreground may be indistinguishable; both baseline and adaptation struggle
- First 3 experiments:
  1. Reproduce SNR sensitivity: Synthesize soundscapes at 6, 8, 10 dB SNR; compare baseline Zero-Shot vs. text adaptation vs. audio adaptation
  2. Ablate background profile source: For fixed background type, compute profiles using text prompts, averaged multi-prompt text profile, single background recording, and averaged multi-recording audio profile
  3. Integrate TGAP and test modality gap reduction: Build TGAP prototypes; measure cosine distances; compare full pipeline (TGAP + audio adaptation) to Zero-Shot + audio adaptation

## Open Questions the Paper Calls Out
- How can the parameter τ (controlling the degree of background subtraction) be automatically selected without relying on grid search?
- Is the proposed domain adaptation method effective when background sound sources are dominant (SNR < 6 dB)?
- Can the performance of text-based background adaptation be improved to match audio-based adaptation without requiring background recordings?

## Limitations
- Prompt engineering dependency: Effectiveness relies heavily on prompt selection, with sensitivity to alternative phrasings not explored
- Representative background audio requirement: Assumes access to representative background recordings, limiting practical applicability in unknown or variable conditions
- SNR threshold effects: Does not establish clear performance boundaries or identify SNR thresholds below which the method fails

## Confidence
- High confidence: Background subtraction improves classification accuracy across different background types and SNR conditions
- Medium confidence: Audio-based adaptation outperforms text-based adaptation due to better background representation
- Medium confidence: Narrowing the modality gap via TGAP improves classification performance

## Next Checks
1. Systematically evaluate the method's performance across a wider SNR range (4-20 dB) with finer granularity to identify precise performance boundaries and optimal τ values
2. Conduct experiments varying N (number of nearest neighbors) in TGAP from 1 to 100 to determine optimal values and assess sensitivity to this parameter
3. Test the method on additional datasets with different acoustic characteristics and with real-world polyphonic recordings to evaluate generalization beyond the experimental setup