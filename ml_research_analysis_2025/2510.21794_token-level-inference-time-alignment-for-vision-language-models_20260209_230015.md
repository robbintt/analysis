---
ver: rpa2
title: Token-Level Inference-Time Alignment for Vision-Language Models
arxiv_id: '2510.21794'
source_url: https://arxiv.org/abs/2510.21794
tags:
- reward
- arxiv
- alignment
- visual
- tita
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TITA (Token-level Inference-Time Alignment) addresses hallucinations
  in Vision-Language Models (VLMs) by introducing a lightweight framework that aligns
  token-level generation without retraining the base model. The method trains a small
  reward model to approximate the base VLM's distribution and uses log-probability
  ratios during inference to provide fine-grained, autoregressive corrective signals.
---

# Token-Level Inference-Time Alignment for Vision-Language Models

## Quick Facts
- arXiv ID: 2510.21794
- Source URL: https://arxiv.org/abs/2510.21794
- Authors: Kejia Chen; Jiawen Zhang; Jiacong Hu; Kewei Gao; Jian Lou; Zunlei Feng; Mingli Song
- Reference count: 12
- Primary result: TITA improves VLMs with negligible overhead via token-level inference-time alignment

## Executive Summary
TITA addresses hallucinations in Vision-Language Models (VLMs) through a lightweight inference-time alignment framework that works without retraining the base model. The method trains a small reward model to approximate the base VLM's distribution and uses log-probability ratios during inference to provide fine-grained corrective signals. Evaluations on LLaVA-1.5-7B and 13B demonstrate consistent improvements across 12 benchmarks, including significant gains on MMVet and POPE, while maintaining negligible computational overhead.

## Method Summary
The TITA framework introduces token-level inference-time alignment for VLMs by training a reward model to approximate the base model's distribution. During inference, it computes log-probability ratios between the base model and reward model to generate corrective signals for each token. This approach enables fine-grained alignment without requiring any retraining of the original VLM. The method has been tested on LLaVA-1.5-7B and 13B models, showing consistent performance improvements across multiple benchmarks while maintaining minimal computational overhead.

## Key Results
- 8.6% improvement on MMVet benchmark for LLaVA-1.5-7B
- 6.7% improvement on POPE benchmark for LLaVA-1.5-7B
- Generalizes to other LVLMs including Qwen2.5-VL-7B and DeepSeek-VL2-27.5B
- Achieves better performance than sequence-level inference-time methods

## Why This Works (Mechanism)
TITA leverages token-level log-probability ratios between the base VLM and a trained reward model to provide fine-grained corrective signals during generation. This approach addresses hallucinations by aligning token-level predictions without requiring model retraining, enabling real-time correction while maintaining the base model's capabilities.

## Foundational Learning

**Vision-Language Models (VLMs)**: Multimodal models that process both visual and textual inputs, typically combining image encoders with language models. Understanding VLMs is crucial as TITA operates on their outputs.

**Inference-time Alignment**: Techniques that modify model behavior during inference without retraining. This is essential for TITA's approach of correcting hallucinations without modifying the base model.

**Reward Modeling**: Training models to approximate the distribution of another model. TITA uses this to create a reference distribution for comparison with the base VLM.

**Log-Probability Ratios**: Mathematical measures comparing the likelihood of different predictions. TITA uses these ratios to quantify alignment at the token level.

**Autoregressive Generation**: Sequential token generation where each step depends on previous outputs. TITA's token-level corrections work within this generation framework.

## Architecture Onboarding

**Component Map**: Base VLM -> Reward Model -> Log-Probability Ratio Calculator -> Token-level Correction Module -> Final Output

**Critical Path**: Image input → Base VLM encoding → Text generation → Reward model approximation → Log-probability ratio computation → Token correction → Final output

**Design Tradeoffs**: 
- Lightweight correction vs. comprehensive retraining
- Token-level granularity vs. sequence-level simplicity
- Inference-time computation vs. pre-training overhead

**Failure Signatures**: 
- Inconsistent corrections across similar contexts
- Computational overhead exceeding negligible threshold
- Degradation in base model's native capabilities

**First Experiments**:
1. Validate log-probability ratio computation accuracy on simple text generation tasks
2. Test token-level correction effectiveness on synthetic hallucination cases
3. Measure computational overhead impact on various hardware configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused primarily on VLMs and specific vision-language tasks
- Generalization to broader multimodal applications untested
- Computational overhead characterization incomplete for production systems

## Confidence

**High Confidence**: Empirical improvements on tested benchmarks and technical feasibility of token-level alignment approach are well-supported.

**Medium Confidence**: Performance claims compared to sequence-level methods are supported but limited to specific models and tasks tested.

**Medium Confidence**: Generalization to other LVLMs demonstrated but extent across broader VLM landscape needs further validation.

## Next Checks

1. Test TITA's effectiveness on a wider range of VLM architectures and multimodal tasks beyond current scope.

2. Conduct thorough analysis of computational overhead in production-like environments with varying resource constraints.

3. Evaluate robustness against adversarial inputs and out-of-distribution data to assess generalization capabilities.