---
ver: rpa2
title: 'ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving'
arxiv_id: '2505.12717'
source_url: https://arxiv.org/abs/2505.12717
tags:
- reasoning
- puzzle
- tasks
- totqwen3-8b
- totrl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToTRL, a novel on-policy reinforcement learning
  framework designed to transition large language models from sequential chain-of-thought
  (CoT) reasoning to the more efficient tree-of-thoughts (ToT) reasoning strategy.
  ToTRL employs a two-stage training approach and uses puzzle games that require tree-based
  reasoning to cultivate ToT capabilities in LLMs.
---

# ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving

## Quick Facts
- arXiv ID: 2505.12717
- Source URL: https://arxiv.org/abs/2505.12717
- Reference count: 33
- Primary result: ToTRL trains LLMs to perform tree-of-thoughts reasoning using puzzle games, achieving 0.973 accuracy on in-distribution tasks and 0.750 on out-of-distribution math tasks

## Executive Summary
This paper introduces ToTRL, a novel on-policy reinforcement learning framework designed to transition large language models from sequential chain-of-thought reasoning to the more efficient tree-of-thoughts reasoning strategy. ToTRL employs a two-stage training approach and uses puzzle games that require tree-based reasoning to cultivate ToT capabilities in LLMs. The method is evaluated on both in-distribution and out-of-distribution logic reasoning tasks, as well as mathematical benchmarks, showing significant performance improvements over existing models.

## Method Summary
ToTRL uses on-policy reinforcement learning (PPO-style) to train LLMs on puzzle games (Sudoku, Alphametics) with a two-stage approach: first in "no-thinking" mode (blank tags) to break sequential habits, then in "thinking" mode to enable full tree reasoning. The training employs strict binary rewards based on format validity and complete solution accuracy. The method is evaluated using Qwen3-8B, with test-time efficiency measured by accuracy versus thinking budget constraints.

## Key Results
- ToTQwen3-8B achieves 0.973 average accuracy on in-distribution puzzle tasks
- Outperforms existing models on OOD mathematical tasks (0.750 average accuracy)
- Demonstrates improved reasoning efficiency with smaller thinking budgets compared to CoT-based counterparts
- Ablation studies confirm the necessity of both training stages and strict binary rewards

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraint-satisfaction puzzles (e.g., Sudoku, Alphametics) may function as "curriculum" for inducing search policies.
- **Mechanism:** The paper posits that these puzzles require managing interdependent choices and backtracking. By using them as training data within an RL framework, the model is pressured to learn a non-linear "trial-and-error" management strategy—specifically, branching and pruning—that generalizes to other reasoning tasks.
- **Core assumption:** The reasoning patterns learned from logical constraints in games transfer to mathematical or structural reasoning in other domains.
- **Evidence anchors:** [abstract] "Solving puzzle games inherently necessitates exploring interdependent choices... requiring the construction and exploration of a thought tree." [section 2.4] "Sudoku puzzle is an ideal game... placing a single digit significantly impacts other cells, necessitating a forward-looking evaluation."

### Mechanism 2
- **Claim:** A two-stage training strategy (No-thinking → Thinking) is likely necessary to overcome "sequential habituation."
- **Mechanism:** Pre-trained LLMs default to linear CoT. The paper suggests that forcing the model to learn ToT structures in a "no-thinking mode" (empty thought tags) first acts as a "habit breaker," allowing the model to mold a new ToT policy before integrating it with its powerful existing CoT capabilities in the second stage.
- **Core assumption:** The "thinking mode" is too rigidly defined by pre-training to allow for the initial acquisition of a drastically different tree-based policy.
- **Evidence anchors:** [section 2.3] "Directly building ToT reasoning based on CoT... is challenging due to the established habituation... ToTRL employs a multi-stage... strategy." [table 6] Ablation study shows significant performance drop when Stage 1 is skipped.

### Mechanism 3
- **Claim:** Strict rule-based rewards likely enforce global consistency and prevent "local" reasoning collapse.
- **Mechanism:** Unlike partial rewards, the paper uses a strict binary reward (+1/-1) based on format validity and complete solution sets. This mechanism forces the model to maintain a global view of the problem state to ensure all constraints are met simultaneously, rather than just satisfying the immediate next step.
- **Core assumption:** The gradient signal from a sparse, binary reward is sufficient to guide the model toward the complex behavior of generating all valid solutions without intermediate supervision.
- **Evidence anchors:** [section 2.2] "This function employs a strict hierarchical evaluation protocol... prioritizing format validity before evaluating correctness." [table 10] Ablation shows that "Partial-Credit" rewards cause the model to output subsets of answers.

## Foundational Learning

- **Concept: On-Policy Reinforcement Learning (PPO/REINFORCE)**
  - **Why needed here:** ToTRL uses a modified on-policy algorithm (Eq. 1) to update the model. You must understand how $A_i$ (Advantage) is calculated relative to the current policy $\pi_{\theta_{old}}$.
  - **Quick check question:** Why does the paper set the KL penalty coefficient $\beta = 0$ during training? (Answer: To allow the model distribution to diverge significantly from the initial frozen reference model for exploration).

- **Concept: Constraint Satisfaction Problems (CSPs)**
  - **Why needed here:** The training tasks (Sudoku, Alphametics) are classic CSPs. Understanding "forward checking" and "backtracking" is required to analyze why the model learns to prune branches.
  - **Quick check question:** In the context of the paper, what feature of Alphametic puzzles necessitates a "tree-based" approach? (Answer: The interdependency of carry-overs and the dual-constraint of math correctness + digit uniqueness).

- **Concept: Test-Time Scaling (Thinking Budget)**
  - **Why needed here:** The paper evaluates efficiency by varying the "thinking budget" (token limit for intermediate thought).
  - **Quick check question:** How does ToTRL claim to improve efficiency regarding the thinking budget? (Answer: It reaches higher accuracy with smaller budgets because the tree structure allows for pruning unproductive paths, unlike linear CoT).

## Architecture Onboarding

- **Component map:** User Prompt + ToT Guidance Prompt -> Base LLM (Qwen3-8B) wrapped in ToTRL Policy -> On-Policy RL Engine -> Rule-Based Reward -> ToT Tags with tree search + <answer>

- **Critical path:**
  1. **Data Curation:** Generation of Puzzle Tasks (Sudoku/Alphametic) with ground truth
  2. **Stage 1 Training:** Initialize policy; Set thinking tags to blank (no-thinking mode); Train on 1080 puzzles
  3. **Stage 2 Training:** Enable thinking mode; Train on 360 puzzles
  4. **Evaluation:** Run on OOD tasks (Math/Crosswords) with specific Thinking Budgets

- **Design tradeoffs:**
  - **Implicit vs. Explicit Search:** ToTRL trains the model to simulate a tree search (implicit) rather than wrapping the LLM in an external BFS/MCTS algorithm (explicit). This is faster at inference but harder to debug.
  - **Reward Sparsity:** Using strict binary rewards (+1/-1) avoids "reward hacking" where the model outputs partial solutions, but may make training less stable initially.

- **Failure signatures:**
  - **"CoT Collapse":** The model ignores `<tot>` tags and generates a linear stream of consciousness
  - **Format Hallucination:** The model generates valid-looking JSON but fails the "Format Validity" checks (e.g., missing tags)
  - **Incomplete Search:** The model finds one solution and stops, failing to explore other branches (triggering the -1 penalty for multi-solution puzzles)

- **First 3 experiments:**
  1. **Ablation on Stage 1:** Train a model without the "no-thinking" stage to verify if "habituation" truly prevents ToT acquisition (Table 6)
  2. **Reward Sensitivity:** Test "Partial-Credit" rewards vs. "Full-Credit" to see if the model collapses into outputting only single solutions (Table 10)
  3. **OOD Generalization:** Train on puzzles, test on AIME/Math. Verify if the "tree-exploration" skill transfers to algebraic reasoning (Table 4)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How effectively does ToT reasoning trained on puzzles transfer to broader real-world reasoning tasks beyond logic and mathematics?
- **Basis in paper:** [explicit] The Limitations section states: "The transferability of ToTQwen3-8B to different reasoning tasks requires a thorough investigation."
- **Why unresolved:** The paper shows promising but modest improvements on tool use and writing benchmarks (Table 13), with no significant gains on coding tasks, leaving the generalization question open.
- **What evidence would resolve it:** Systematic evaluation across diverse domains (scientific reasoning, legal analysis, medical diagnosis) showing whether ToT capabilities learned from puzzles meaningfully transfer.

### Open Question 2
- **Question:** What are the trade-offs between implicit ToT induction through ToTRL versus explicit tree search algorithms in terms of optimality and computational efficiency?
- **Basis in paper:** [explicit] The Limitations section notes: "This implicit induction... may exhibit different exploratory dynamics and optimality compared to explicit tree search algorithms."
- **Why unresolved:** ToTRL achieves comparable performance to TS-LLM and ToT (Table 12) without external search control, but a rigorous comparison of search quality, redundancy reduction, and computational trade-offs remains unexplored.
- **What evidence would resolve it:** Head-to-head comparison measuring solution completeness, token efficiency, and wall-clock time on identical tasks between ToTRL and explicit BFS/MCTS methods.

### Open Question 3
- **Question:** What is the minimal training data diversity required to activate robust ToT reasoning that generalizes across puzzle types?
- **Basis in paper:** [inferred] Training used only 1,440 puzzles across two types (Sudoku and Alphametic), yet achieved strong OOD performance. The relationship between training task diversity and generalization capability is not analyzed.
- **Why unresolved:** Without ablations on training task variety, it remains unclear whether ToT capabilities require diverse puzzle types or can emerge from a single well-designed task class.
- **What evidence would resolve it:** Ablation studies varying the number and type of training puzzles to identify the minimal curriculum that enables comparable OOD transfer.

### Open Question 4
- **Question:** Can ToTRL scale effectively to larger model architectures (e.g., 70B+ parameters) while maintaining or improving reasoning efficiency gains?
- **Basis in paper:** [inferred] The paper only evaluates an 8B parameter model. Larger models may have different emergent reasoning patterns or may not benefit as substantially from explicit ToT training if they already possess stronger implicit search capabilities.
- **Why unresolved:** No experiments were conducted with larger model sizes, leaving scalability as an open question.
- **What evidence would resolve it:** Training and evaluating ToTRL on models spanning 8B to 70B+ parameters, comparing efficiency gains and performance improvements across scales.

## Limitations
- Limited generalizability of puzzle-based training to real-world reasoning tasks without clear constraints
- Reliance on self-generated datasets without access to generation code or specifications
- Focus on accuracy metrics without analyzing the quality or efficiency of generated tree structures

## Confidence
- **High confidence:** Ablation studies (Tables 6 and 10) demonstrate that both two-stage training and strict binary rewards are necessary components
- **Medium confidence:** Test-time scaling efficiency claims are supported, but comparison to CoT models may not be entirely fair
- **Low confidence:** The fundamental claim that ToTRL teaches models to "think with trees" rather than simulate tree search remains somewhat speculative

## Next Checks
1. **OOD Generalization Test:** Train ToTRL on Sudoku/Alphametics, then evaluate on completely different reasoning tasks like commonsense reasoning benchmarks (e.g., StrategyQA) or multi-hop inference tasks to verify if tree-exploration skills transfer beyond constraint satisfaction problems.

2. **Interpretability Analysis:** Use mechanistic interpretability tools to analyze whether ToTRL models generate actual tree structures in their activations during reasoning, rather than just linear sequences with formatting artifacts.

3. **Scaling Law Validation:** Test ToTRL across different model sizes (e.g., Qwen3-14B, Qwen3-32B) to determine if the two-stage training advantage holds as model capacity increases, or if larger models can acquire ToT reasoning more directly without the "habit breaker" stage.