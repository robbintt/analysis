---
ver: rpa2
title: 'CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits
  of Large Language Models'
arxiv_id: '2505.13559'
source_url: https://arxiv.org/abs/2505.13559
tags:
- llms
- en-zh
- en-ta
- en-ms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CS-Sum, the first benchmark for code-switching
  dialogue summarization across Mandarin-English, Tamil-English, and Malay-English,
  featuring 900-1300 human-annotated dialogues per language pair. Evaluating ten large
  language models, including open-source and closed-source variants, across few-shot,
  translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) settings, the
  authors find that while automated metrics are high, LLMs frequently make subtle
  errors that distort meaning.
---

# CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models

## Quick Facts
- **arXiv ID**: 2505.13559
- **Source URL**: https://arxiv.org/abs/2505.13559
- **Reference count**: 15
- **Primary result**: LLMs achieve high automated metrics on CS dialogue summarization but make systematic comprehension errors, with error rates varying by language pair and model

## Executive Summary
This paper introduces CS-Sum, the first benchmark for code-switching dialogue summarization across Mandarin-English, Tamil-English, and Malay-English, featuring 900-1300 human-annotated dialogues per language pair. Evaluating ten large language models, including open-source and closed-source variants, across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) settings, the authors find that while automated metrics are high, LLMs frequently make subtle errors that distort meaning. Three error types are identified: Code-Switching Loss (ignoring non-English content), Meaning Shift from Poor Translation (misinterpreting CS segments), and Speaker Misattribution (incorrectly assigning dialogue). Error rates vary by language pair and model, with fine-tuning on synthetic data not reliably improving comprehension, highlighting the need for specialized training on real code-switched data.

## Method Summary
The CS-Sum benchmark is constructed by translating English DialogSum/SAMSum dialogues to code-switched variants via native speakers, creating 900-1300 human-annotated dialogues per language pair (EN-ZH, EN-TA, EN-MS). The evaluation pipeline employs five automated metrics (ROUGE-L, BERTScore, SBERT-Cosine, Jaccard, METEOR) and three error categories (CSL, MST, SMA) detected via GPT-4o. Ten LLMs are evaluated across four settings: Few-Shot (3 examples), Translate-Summarize (internal translation), LoRA fine-tuning, and QLoRA fine-tuning on a synthetic CS-Sum-Syn corpus (19,014 examples generated by Gemini-2-flash). LoRA/QLoRA training uses specific hyperparameters (rank r, scaling α, learning rate) with 4 epochs, batch size 8, gradient accumulation 8, bf16 precision, and cosine LR schedule with 3% warmup.

## Key Results
- LLMs achieve high automated metrics (BERTScore >0.88) but exhibit significant comprehension errors, with CSL exceeding 50% in few-shot settings
- Fine-tuning on synthetic CS data often degrades performance due to distributional mismatch, particularly for EN-ZH (KL divergence 2.48)
- Error rates vary substantially by language pair, with EN-TA showing the highest error rates (>90% CSL in several cases)
- Larger general models (Gemma-2-9B) outperform regionally-specialized models (SEA-LION-Gemma-9B) on metrics, but specialized models show lower CSL/SMA rates

## Why This Works (Mechanism)

### Mechanism 1: Native Speaker Translation for Benchmark Validity
- Claim: Translating established monolingual dialogue benchmarks to code-switched variants via native speakers preserves task validity while enabling CS comprehension evaluation
- Mechanism: Native speakers convert English DialogSum/SAMSum dialogues to CS in their respective language pairs, maintaining dialogue structure and semantic content while introducing authentic switching patterns
- Core assumption: Translation from native speakers preserves semantic equivalence and reflects natural CS behavior in multilingual communities
- Evidence anchors:
  - [abstract]: "900-1300 human-annotated dialogues per language pair"
  - [section 3.1]: "We recruited 7 native speakers for each language to translate the English dialogues to CS dialogues in their respective languages. The translators were native speakers, all university students."
  - [corpus]: CS-Dialogue dataset similarly constructs natural CS corpora through spontaneous conversation collection
- Break condition: Translation introduces systematic biases (formality shifts, code-switching density changes) that don't represent natural CS production

### Mechanism 2: Error Taxonomy Captures Systematic CS Comprehension Failures
- Claim: Three error types—Code-Switching Loss (CSL), Meaning Shift from poor Translation (MST), and Speaker Misattribution (SMA)—systematically capture how LLMs fail on CS discourse-level comprehension
- Mechanism: Models trained predominantly on monolingual English data either ignore non-English spans (CSL), misinterpret CS segments through implicit faulty translation (MST), or lose speaker-tracking when processing mixed-language dialogue (SMA)
- Core assumption: These three categories capture the majority of semantically significant failures; finer-grained errors are subsumable under these
- Evidence anchors:
  - [abstract]: "Three error types are identified: Code-Switching Loss (ignoring non-English content), Meaning Shift from Poor Translation (misinterpreting CS segments), and Speaker Misattribution."
  - [section 5]: "Across all 9 models and the three language pairs, CSL exceeds 50% in the Few-Shot setting."
  - [table 6]: EN-TA shows CSL rates up to 95.03% (Ministral-8B), demonstrating systematic rather than random failure
  - [corpus]: PingPong benchmark identifies related multi-turn CS comprehension challenges, supporting error taxonomy validity
- Break condition: Error categories fail to capture language-pair-specific morphological or syntactic failure modes (e.g., agglutinative vs. isolating language issues)

### Mechanism 3: Distribution Divergence Between Synthetic and Natural CS
- Claim: Synthetic CS data generated by LLMs (e.g., Gemini) exhibits measurable distributional mismatch from natural CS, causing fine-tuning to degrade rather than improve comprehension
- Mechanism: Synthetic data approximates surface-level CS patterns (switching frequency, span length distributions) but diverges in structural properties (burstiness, memory, span entropy), leading models to learn shallow pattern-matching rather than true CS understanding
- Core assumption: KL/JS divergence on CS metrics meaningfully captures the gap between synthetic and natural CS quality
- Evidence anchors:
  - [section 5]: "Fine-tuning on the Gemini-generated CS-Sum-Syn corpus degrades performance when the training distribution diverges from CS-Sum, most notably for EN-ZH, whose KL divergence is 2.48 versus ≤0.55 for the other pairs."
  - [table 3]: EN-ZH shows KL divergence of 2.4786 vs. 0.5089 (EN-TA) and 0.5481 (EN-MS), correlating with worse fine-tuning outcomes
  - [figure 2a]: EN-ZH synthetic data shows flatter memory and burstiness curves with long-tailed span entropy
  - [corpus]: Limited direct corpus evidence on synthetic CS data quality; related work on LLM-generated code-mixed text (Yong et al. 2023) notes language collapse issues
- Break condition: Alternative filtering strategies (e.g., Mahalanobis distance filtering described in Appendix D) consistently improve synthetic data utility across architectures

## Foundational Learning

- Concept: **Code-Switching Structural Metrics (M-Index, I-Index, Burstiness, Span Entropy, Memory)**
  - Why needed here: These metrics quantify CS behavior differences between natural and synthetic data; understanding them is essential for diagnosing why synthetic fine-tuning fails
  - Quick check question: If a dataset has M-Index = 0.40, Burstiness = -0.79, and Memory = -0.08, what does this tell you about its switching behavior?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA/QLoRA)**
  - Why needed here: All fine-tuning experiments use these methods; hyperparameters (rank r, scaling α, learning rate) directly affect adaptation quality
  - Quick check question: Given Table 7, why might SEALLM-7B use rank=64 while Gemma-2-9B uses rank=32? What tradeoff does this represent?

- Concept: **Semantic Similarity Metrics vs. Factual Accuracy (ROUGE, BERTScore, SBERT-Cosine)**
  - Why needed here: The paper demonstrates that high semantic similarity scores (BERTScore > 0.88) can coexist with severe factual errors—critical for interpreting evaluation results
  - Quick check question: Figure 3 shows a summary with BERTScore 0.903 that fabricates speaker opinions. Why does SBERT-Cosine fail to penalize this?

## Architecture Onboarding

- Component map: CS-Sum Benchmark -> CS-Sum-Syn Synthetic Data -> Evaluation Pipeline (5 metrics + 3 error categories) -> Model Zoo (10 LLMs) -> Training Configurations (Few-shot, Translate-Summarize, LoRA/QLoRA)

- Critical path:
  1. Data Quality Assessment: Before any fine-tuning, compute CS metrics (M-Index, I-Index, Burstiness, Span Entropy, Memory) on training vs. evaluation data; high KL/JS divergence signals risk
  2. Baseline Establishment: Run few-shot evaluation on all models to establish CSL/MST/SMA error baselines; expect >50% CSL across models
  3. Fine-tuning with Guardrails: If using synthetic data, implement error rate monitoring (not just metric improvements); track CSL/MST/SMA on validation set during training
  4. Error-Aware Evaluation: Augment automated metrics with LLM-based error detection (CSL, MST, SMA prompts provided in Appendix C)

- Design tradeoffs:
  - Few-shot vs. Fine-tuning: Few-shot has high error rates but predictable behavior; fine-tuning on synthetic data shows metric improvements but error rate increases under distribution shift
  - Model Scale vs. CS Specialization: Larger general models (Gemma-2-9B) outperform regionally-specialized models (SEA-LION-Gemma-9B) on metrics, but SEA models show lower CSL/SMA rates—choose based on deployment priority (fluency vs. accuracy)
  - Translate-Summarize Pipeline: Expected to help but shows inconsistent results; translation introduces additional errors for low-resource pairs (EN-TA, EN-MS)
  - Filtering Synthetic Data: Mahalanobis distance filtering (Appendix D) helps some architectures (Mistral-7B, SEA-LION-Gemma-9B) but harms others (Qwen2.5-7B)—not a universal solution

- Failure signatures:
  - High metric scores + high CSL: Model generates fluent English summaries that ignore non-English dialogue content; detectable via CSL error analysis
  - Fine-tuning amplifies errors: LoRA/QLoRA on synthetic data improves ROUGE/METEOR but increases CSL from ~60% to >80% (Table 6, EN-ZH SEA-LION-Gemma-9B)
  - Language-pair-specific collapse: EN-TA consistently shows highest error rates across all models (>90% CSL in several cases); morphological complexity hypothesis in Section 5
  - Speaker tracking degradation in CS: SMA rates spike when processing CS dialogue vs. monolingual; likely due to attention dispersion across languages

- First 3 experiments:
  1. Reproduce CSL baseline: Take Gemma-2-9B or LLaMA-3-8B, run few-shot evaluation on EN-TA subset (highest error rates), compute CSL/MST/SMA using Appendix C prompts—expect 70-95% CSL depending on model
  2. Synthetic data quality audit: Compute KL/JS divergence between CS-Sum-Syn and CS-Sum for your target language pair; if KL > 1.0, expect fine-tuning degradation per Section 5 findings
  3. Error-aware validation during fine-tuning: Fine-tune LLaMA-3-8B on CS-Sum-Syn subset with LoRA (Table 7 config), but track CSL rate on held-out CS-Sum samples every 500 steps—plot metric improvement vs. error rate to detect divergence

## Open Questions the Paper Calls Out

- **Question**: Can alternative data filtering strategies or thresholds beyond Mahalanobis distance yield consistent generalization improvements across diverse LLM architectures?
- **Basis in paper**: [explicit] Appendix D states, "Further investigation is required to determine if alternative filtering thresholds or strategies could yield better generalization across LLMs" after finding that Mahalanobis filtering benefited Mistral but harmed Qwen
- **Why unresolved**: The authors' experiment with distribution-based filtering showed inconsistent results (improving some models while degrading others), leaving the optimal method for closing the distribution gap between synthetic and real CS data undefined
- **What evidence would resolve it**: Systematic ablation studies of filtering thresholds across a wider range of model architectures, or the application of alternative domain adaptation techniques that show uniform improvements in error rates (CSL, MST, SMA)

- **Question**: Does fine-tuning on human-annotated, naturally occurring code-switched data resolve the comprehension failures and error amplification observed with synthetic data?
- **Basis in paper**: [explicit] Section 7 (Limitations) notes that fine-tuning was limited to synthetic data generated by Gemini-2, which "does not capture the complexities of real-world CS data"
- **Why unresolved**: The study found that fine-tuning on synthetic data often increased specific error rates (e.g., Code-Switching Loss), but it did not test if actual human-annotated data would mitigate these "hallucinated" comprehension errors
- **What evidence would resolve it**: A comparison of models fine-tuned on the human-authored CS-Sum validation set versus the synthetic CS-Sum-Syn set, specifically measuring the reduction in CSL and MST error rates

- **Question**: Do the identified failure modes (CSL, MST, SMA) persist across other long-context generative tasks like machine translation or dialogue generation?
- **Basis in paper**: [explicit] Section 7 (Limitations) states that extending the benchmark to tasks such as machine translation and dialogue generation would provide a "broader understanding," as the current study focused solely on summarization
- **Why unresolved**: It is currently unclear if the models' tendency to ignore non-English segments (CSL) or misinterpret CS phrases (MST) is specific to the compression requirements of summarization or a fundamental limitation in CS comprehension
- **What evidence would resolve it**: Applying the LLM-driven error analysis framework (developed in Section 5) to model outputs for CS machine translation and dialogue generation tasks

## Limitations
- Error taxonomy validation relies on GPT-4o analysis without human expert annotation verification, raising potential bias concerns
- Synthetic data quality assessment lacks direct comparison to human-generated CS dialogues in the same domain
- Benchmark construction through translation rather than natural CS dialogue collection may miss authentic domain-specific patterns

## Confidence
- **High Confidence**: LLMs frequently ignore non-English content (CSL > 50% in few-shot settings) and high automated metrics coexist with semantic errors
- **Medium Confidence**: Synthetic data distributional mismatch causing fine-tuning degradation is supported by KL/JS divergence correlations but lacks direct evidence of improved results with better synthetic data
- **Low Confidence**: Fine-tuning on synthetic CS data is generally unhelpful requires qualification—architecture-dependent effects show some models benefit from filtered synthetic data

## Next Checks
1. **Human Validation of Error Taxonomy**: Have human annotators (bilingual speakers) classify 100 randomly sampled summaries from the CS-Sum test set using the CSL/MST/SMA rubric. Compare inter-annotator agreement and GPT-4o vs. human classification rates to validate automated error detection reliability.

2. **Synthetic vs. Natural CS Data Quality**: Collect 50-100 natural CS dialogues in each language pair (e.g., from CS-Dialogue or social media) and compute the same CS structural metrics (M-Index, I-Index, Burstiness, Span Entropy, Memory). Compare these distributions to CS-Sum-Syn to determine whether synthetic data quality or domain mismatch drives fine-tuning failures.

3. **Architecture-Specific Synthetic Data Filtering**: For each model architecture (Gemma-2, Qwen2.5, Mistral, etc.), implement Mahalanobis distance filtering on CS-Sum-Syn and evaluate whether filtered subsets improve CSL/MST/SMA rates during fine-tuning. This tests whether the "synthetic data unhelpful" conclusion should be qualified by architecture-specific filtering strategies.