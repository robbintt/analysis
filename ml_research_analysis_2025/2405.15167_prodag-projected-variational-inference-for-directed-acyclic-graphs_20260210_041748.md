---
ver: rpa2
title: 'ProDAG: Projected Variational Inference for Directed Acyclic Graphs'
arxiv_id: '2405.15167'
source_url: https://arxiv.org/abs/2405.15167
tags:
- sample
- size
- prodag
- dags
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProDAG addresses the challenge of quantifying uncertainty in DAG
  structure learning by developing a Bayesian variational inference framework using
  novel distributions with exact support on sparse DAGs. The core idea involves projecting
  arbitrary continuous distributions onto the space of sparse weighted acyclic adjacency
  matrices using recent continuous reformulations of acyclicity constraints.
---

# ProDAG: Projected Variational Inference for Directed Acyclic Graphs

## Quick Facts
- **arXiv ID:** 2405.15167
- **Source URL:** https://arxiv.org/abs/2405.15167
- **Reference count:** 40
- **Primary result:** ProDAG achieves superior accuracy and uncertainty quantification in DAG structure learning through novel variational inference using projected distributions.

## Executive Summary
ProDAG introduces a Bayesian variational inference framework for learning directed acyclic graphs (DAGs) with quantified uncertainty. The method projects continuous distributions onto the space of sparse weighted acyclic adjacency matrices using recent continuous reformulations of acyclicity constraints. This approach ensures exact mass on zeros for sparsity and valid probability distributions over DAGs without discrete approximations. Experiments demonstrate ProDAG outperforms state-of-the-art methods in accuracy and uncertainty quantification across both linear and nonlinear synthetic datasets, with strong results on real biological data.

## Method Summary
ProDAG uses variational inference to approximate the intractable posterior over DAG structures. The core innovation involves projecting continuous samples from a Gaussian variational distribution onto the space of sparse weighted DAGs through a two-step process: first enforcing acyclicity via a path-following algorithm on the DAGMA function $h(W) = -\log \det(I - W \circ W)$, then projecting onto an $\ell_1$-ball for sparsity using soft-thresholding. Gradients are computed analytically via the implicit function theorem applied to the Karush-Kuhn-Tucker conditions of the projection problem, enabling scalable optimization with Adam. The Evidence Lower Bound (ELBO) is maximized to learn variational parameters while providing uncertainty quantification.

## Key Results
- ProDAG achieves superior Brier scores and expected structural Hamming distances compared to state-of-the-art methods on synthetic linear and nonlinear SEMs
- The method provides well-calibrated uncertainty quantification, with Brier scores improving as sample size increases
- Experiments demonstrate scalability to graphs with 100 nodes while maintaining computational feasibility

## Why This Works (Mechanism)

### Mechanism 1: Continuous Projection Enforces DAG Constraints
Projecting continuous distributions onto DAG space via a measurable mapping induces a valid probability distribution over DAGs without discrete approximations. The projection operation $\text{pro}_\lambda(\tilde{W})$ maps unconstrained matrices $\tilde{W} \sim P$ to the nearest acyclic, $\ell_1$-constrained adjacency matrix $W$. Acyclicity is enforced using the DAGMA function $h(W) = -\log \det(I - W \circ W)$, where $h(W)=0 \iff W \text{ is a DAG}$. Theorem 1 proves this mapping is almost-surely unique and measurable.

### Mechanism 2: Scalable Variational Inference via Implicit Gradients
Variational inference scales to high-dimensional DAGs by using analytical gradients of the projection, avoiding differentiation through iterative optimization. The ELBO is maximized with respect to variational parameters $\theta$. Gradients $\nabla_\theta \text{ELBO}$ require gradients of the projected samples $W$ w.r.t. the base samples $\tilde{W}$, computed analytically via the implicit function theorem applied to the KKT conditions of the projection problem, yielding sparse Jacobians.

### Mechanism 3: Sparsity via $\ell_1$-Ball Constraint
Enforcing an $\ell_1$ constraint during projection produces sparse DAGs with exact zeros. The projection is onto the intersection of the DAG set and an $\ell_1$-ball of radius $\lambda$. This is solved by first projecting for acyclicity, then applying non-iterative soft-thresholding to project onto the $\ell_1$-ball, providing probability mass on exact zeros unlike Laplace priors.

## Foundational Learning

- **Concept: Variational Inference (VI) and the Evidence Lower Bound (ELBO)**
  - **Why needed here:** ProDAG relies on VI to approximate the intractable posterior $p(W|X)$. The ELBO is the objective function maximized to learn the variational parameters.
  - **Quick check question:** Can you explain why maximizing the ELBO is equivalent to minimizing the KL divergence between the approximate posterior $q(W)$ and the true posterior $p(W|X)$?

- **Concept: Reparameterisation Trick**
  - **Why needed here:** This trick allows gradients to be backpropagated through stochastic nodes. In ProDAG, it enables gradient flow from the ELBO through the samples $\tilde{W}$ and into the variational parameters.
  - **Quick check question:** For a Gaussian random variable $\tilde{w} \sim \mathcal{N}(\mu, \sigma^2)$, can you rewrite the sampling process to express $\tilde{w}$ as a deterministic function of $\mu, \sigma$ and a noise variable $\epsilon$?

- **Concept: Continuous Characterisation of DAGs (e.g., NOTEARS, DAGMA)**
  - **Why needed here:** This is the key innovation enabling gradient-based optimization. It replaces the combinatorial DAG constraint with a differentiable function $h(W)$.
  - **Quick check question:** For the DAGMA function $h(W) = -\log \det(I - W \circ W)$, what is the value of $h(W)$ if $W$ contains a directed cycle?

## Architecture Onboarding

- **Component map:** Base Distribution Sampler -> DAG Projector (Acyclicity Projector -> Sparsity Projector) -> Implicit Gradient Calculator -> ELBO Optimiser
- **Critical path:** The performance hinges on the **DAG Projector**. If this fails to produce valid DAGs or converges too slowly, the entire pipeline stalls. The **Implicit Gradient Calculator** is crucial for training efficiency; an error here leads to non-convergence.
- **Design tradeoffs:**
  - **Gaussian vs. Other Base Distributions:** The paper suggests a multivariate Gaussian for simplicity. More complex distributions might capture multimodality better but complicate the KL term.
  - **Projection Complexity vs. Sample Size:** Projection is $O(p^3)$ due to matrix inversions. Increasing the number of samples $L$ per ELBO estimate linearly increases cost but reduces variance.
  - **Nonlinear SEM Architecture:** The number of hidden units $d$ in the neural network increases parameter count but projection complexity depends only on $p$ (via $W(\omega)$).
- **Failure signatures:**
  - **High SHD, Low AUROC:** Projection may be failing to enforce acyclicity properly. Check values of $h(W)$ post-projection.
  - **Gradient Explosion/NaNs:** Issue in the implicit gradient calculation or path-following algorithm. Check for singularity in $(I - W \circ W)$.
  - **No Convergence:** Learning rate might be too high or the projection might be unstable for the current $\lambda$.
- **First 3 experiments:**
  1. **Linear Synthetic Sanity Check (p=5, n=100):** Implement the linear SEM setup. Verify that the projection produces valid DAGs ($h(W) \approx 0$) and that the ELBO increases during training. Compare the learned graph to the ground truth.
  2. **Gradient Check (Finite Difference vs. Analytical):** For a small graph, numerically approximate the gradient of a projected edge weight $\partial w_{jk} / \partial \tilde{w}_{qr}$ using finite differences and compare it to the value from the implicit gradient formula in Proposition 2.
  3. **Sparsity Tuning (λ Sensitivity):** Run the algorithm on a graph with a known number of edges $s$. Vary $\lambda$ and observe its effect on the sparsity of the learned graph and the Brier score. Find a suitable range for the validation set procedure.

## Open Questions the Paper Calls Out

- **Can ProDAG be effectively extended to handle interventional data, and how would its uncertainty quantification compare to existing interventional DAG learning methods?**
  - Basis in paper: Though our focus is observational data, ProDAG could be adapted for interventional settings by replacing the observational likelihood with one appropriate for interventions while leaving the projected distributions unchanged. Developing this extension and assessing ProDAG in interventional contexts offers a natural and important next step for research.

- **Does the learned variational posterior adequately approximate the true posterior, and can improved approximation guarantees be developed despite the nonconvexity of the acyclicity constraint?**
  - Basis in paper: As with most Bayesian approaches that employ variational inference, ours does not guarantee that the learned variational posterior approximates the true posterior well. The possibility of a poor variational approximation is further complicated by the nonconvexity of the acyclicity constraint.

- **Can ProDAG scale to graphs with thousands of nodes while maintaining computational feasibility and inference quality?**
  - Basis in paper: The method has cubic complexity O(p³) and experiments only extend to p = 100 nodes. Training time for p = 100 exceeds one hour, suggesting scalability limitations for larger real-world networks.

- **How robust is ProDAG's posterior uncertainty quantification to various forms of likelihood misspecification, including incorrect noise distributions or nonlinear SEM assumptions?**
  - Basis in paper: if the likelihood is misspecified, different graphs may induce indistinguishable likelihoods, leading to a loss of identifiability. In such cases, ProDAG represents the remaining structural ambiguity through the spread of its posterior distribution.

## Limitations

- The projection algorithm's stability and convergence guarantees for large graphs (p=100) are not fully verified, particularly regarding numerical issues with the log-determinant computation.
- The validation procedure for selecting the sparsity parameter λ relies on a single validation set split, which may not generalize well across different data regimes.
- The method's performance on datasets with significant hidden confounding or measurement error is untested.

## Confidence

- **High Confidence:** The theoretical foundations of the projection operation (Theorem 1) and the implicit gradient derivation (Proposition 2) are mathematically rigorous.
- **Medium Confidence:** The experimental results show strong performance on synthetic benchmarks, but the sample sizes and number of random seeds are limited (n=5 for linear, n=10 for nonlinear).
- **Low Confidence:** The scalability claims for p=100 are based on limited testing, and the computational cost analysis is incomplete.

## Next Checks

1. **Convergence Analysis:** Monitor the path-following algorithm's convergence rate and success rate across different graph densities and sizes.
2. **Sensitivity Analysis:** Systematically vary the number of validation set splits and the λ grid resolution to assess robustness of the hyperparameter selection.
3. **Real-World Stress Test:** Apply ProDAG to a larger, more complex biological dataset (e.g., DREAM challenges) with ground truth or consensus networks for validation.