---
ver: rpa2
title: Statistical-Neural Interaction Networks for Interpretable Mixed-Type Data Imputation
arxiv_id: '2601.12380'
source_url: https://arxiv.org/abs/2601.12380
tags:
- mean
- data
- imputation
- attention
- missforest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Statistical-Neural Interaction (SNI) addresses mixed-type tabular\
  \ data imputation by combining correlation-derived statistical priors with neural\
  \ attention. Its Controllable-Prior Feature Attention (CPFA) module learns head-wise\
  \ coefficients \u03BBh that regulate attention toward the prior, enabling a balance\
  \ between linear statistical patterns and nonlinear interactions."
---

# Statistical-Neural Interaction Networks for Interpretable Mixed-Type Data Imputation

## Quick Facts
- **arXiv ID:** 2601.12380
- **Source URL:** https://arxiv.org/abs/2601.12380
- **Reference count:** 40
- **Primary result:** SNI achieves competitive NRMSE/R² on continuous imputation and provides interpretable dependency networks, but underperforms on severely imbalanced categorical variables.

## Executive Summary
Statistical-Neural Interaction (SNI) is a mixed-type tabular imputation method that blends statistical correlation-derived priors with neural attention to balance linear and nonlinear feature dependencies. Its Controllable-Prior Feature Attention (CPFA) module learns per-head coefficients λ_h that modulate the influence of the prior, while an EM-inspired loop co-evolves statistical and neural parameters. SNI also outputs a directed feature-dependency matrix D for intrinsic interpretability diagnostics. Evaluated on six real-world datasets under MCAR/MAR at 30% missingness, SNI is generally competitive on continuous metrics (average rank 3.33 for NRMSE, 3.50 for R²) and outperforms MissForest on some datasets, though it is often outperformed by accuracy-first baselines on categorical variables, especially under severe imbalance. Computational cost is higher than classical methods, making SNI more suitable for offline analysis where interpretability and continuous-variable fidelity are priorities.

## Method Summary
SNI is a mixed-type tabular imputation method that combines statistical correlation-derived priors with neural attention. It computes a correlation matrix from standardized/one-hot encoded data, derives per-feature priors P_f, and uses a CPFA module with multi-head attention to predict missing values. The method co-evolves priors and neural parameters via an EM-inspired loop and outputs a directed feature-dependency matrix D for interpretability. It is trained with a reconstruction loss plus a prior regularization term weighted by learnable λ_h coefficients.

## Key Results
- On average, SNI ranks 3rd for NRMSE and 3.5th for R² across six datasets at 30% missingness under MCAR/MAR.
- SNI's dependency network and λ_h coefficients provide model-reliance insights without post-hoc explainers.
- SNI is computationally heavier than classical methods, making it more suitable for offline analysis where interpretability and continuous-variable fidelity are priorities.
- MNAR stress tests suggest SNI is sensitive to non-ignorable missingness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correlation-derived priors regularize attention toward linear statistical patterns while allowing data-driven deviation for nonlinear interactions.
- Mechanism: The CPFA module computes a prior Pf from Pearson correlations on a standardized/one-hot "correlation-design matrix," then penalizes deviations via L_prior = αΣ_h λ_h ||A^(h) - P_f||². The softplus-transformed λ_h = softplus(θ_λ,h) controls per-head regularization strength.
- Core assumption: Pearson correlation on the correlation-design matrix provides a meaningful proxy for pairwise feature dependencies under MAR.
- Evidence anchors:
  - [abstract] "CPFA learns head-wise prior-strength coefficients {λ_h} that softly regularize attention toward the prior while allowing data-driven deviations when nonlinear patterns appear to be present."
  - [section 3.2] "Large λ_h indicates that A^(h) ≈ P_f, whereas a small λ_h allows the head to deviate and capture complex interactions."
  - [corpus] Weak direct support; related work (CDTD, MissHDD) focuses on diffusion for mixed-type data rather than prior-regularized attention.
- Break condition: If the correlation matrix is unreliable (small samples, heavy tails, extreme nonlinearity), P_f may misguide attention rather than stabilize it.

### Mechanism 2
- Claim: EM-inspired alternating optimization co-evolves statistical priors and neural parameters, improving imputation iteratively.
- Mechanism: Each iteration (g): (1) compute Σ^(g-1) from current completed data X^(g-1); (2) derive feature-specific P_f; (3) train CPFA to minimize reconstruction loss plus prior penalty; (4) update X^(g) with predictions; (5) refine via StatRefine. Convergence when ||X^(g) - X^(g-1)||_F / ||X^(g-1)||_F < ε.
- Core assumption: The variational lower bound derivation (Supplementary S1) holds under MAR; neural M-step is approximately optimizing the expected complete-data log-likelihood.
- Evidence anchors:
  - [section 3.1] "Through EM-inspired iterations recomputing P_f, statistical priors and neural attention co-evolve."
  - [section 3.4] "Because the neural step is optimized approximately by stochastic gradient descent, we do not claim monotonic improvement at every iteration."
  - [corpus] No direct corpus comparison to EM-based imputation hybrids.
- Break condition: If SGD optimization in the M-step diverges or gets stuck in poor local minima, the EM loop may oscillate rather than converge.

### Mechanism 3
- Claim: Aggregating multi-head attention into a directed dependency matrix D provides intrinsic, post-hoc-free interpretability of imputer reliance patterns.
- Mechanism: For each head h, CPFA produces attention matrix A^(h) ∈ R^(d×d); aggregate as D = (1/H) Σ_h A^(h) with D_ii = 0. Entry D_ij quantifies how much the imputer relied on source j for target i.
- Core assumption: Attention weights reflect meaningful predictive reliance, not just optimization artifacts (cf. "attention is not explanation" debate).
- Evidence anchors:
  - [abstract] "The method also produces a directed feature-dependency matrix as intrinsic interpretability diagnostics."
  - [section 4.5, Table S21] Synthetic sanity check shows SNI improves edge recovery over NoPrior baseline (AUROC 0.807 vs 0.695 on nonlinear_mixed; 0.848 vs 0.757 on interaction_xor), suggesting D is not merely restating marginal correlations.
  - [corpus] Weak; related interpretable-imputation work is sparse.
- Break condition: When features are highly redundant, D may distribute attention across correlated sources without reflecting true data-generating structure.

## Foundational Learning

- Concept: **Multi-head feature attention**
  - Why needed here: CPFA uses H parallel attention heads, each learning potentially distinct dependency patterns between features.
  - Quick check question: Can you explain why multi-head attention might capture different types of feature relationships compared to single-head?

- Concept: **EM algorithm for missing data**
  - Why needed here: SNI's outer loop is EM-inspired; understanding E-step (imputation) and M-step (parameter update) is essential for debugging convergence.
  - Quick check question: Why does EM guarantee monotonic likelihood improvement in theory, and why might SNI violate this in practice?

- Concept: **Missingness mechanisms (MCAR/MAR/MNAR)**
  - Why needed here: SNI is theoretically grounded under MAR; MNAR results are stress tests, not guaranteed solutions.
  - Quick check question: If missingness depends on unobserved values themselves, why does standard imputation fail to recover the true distribution?

## Architecture Onboarding

- Component map:
  1. **Statistical Step**: Compute Σ from correlation-design matrix → derive P_f per feature via Eq. (3).
  2. **Pseudo-Masking**: Sample M_f ~ Bernoulli(ρ) on training indices for self-supervision.
  3. **CPFA Module**: Multi-head attention with learnable λ_h; feature bypass for linear signal; predictor head (regression/classification).
  4. **StatRefine**: Optional post-processing (e.g., MICE polish) on completed matrix.
  5. **EM Loop**: Iterate until Δ < ε or max iterations G reached.

- Critical path: Initialization (MeanModeImpute) → loop { Σ computation → P_f extraction → CPFA training → imputation → StatRefine → convergence check } → output X, D, {λ_h}.

- Design tradeoffs:
  - Higher H (heads) increases expressiveness but computational cost is O(d × H × h²).
  - Larger λ initialization strengthens prior adherence (more stable but may miss nonlinear patterns).
  - Focal loss for categorical targets mitigates imbalance but does not fully close gap to MissForest.

- Failure signatures:
  - **Non-convergence**: Δ oscillates across iterations—check learning rate, prior strength α, or data scaling.
  - **Categorical collapse**: Macro-F1 near 0 on imbalanced targets—severe class imbalance overwhelms focal loss.
  - **Degenerate D**: Near-uniform attention across sources—prior too weak or attention learning failed.

- First 3 experiments:
  1. **Sanity check on synthetic**: Generate data from known DAG (linear_gaussian, nonlinear_mixed); verify D recovers edges above chance (compare to Table S21 baseline).
  2. **Ablation on prior strength**: Compare SNI vs NoPrior vs HardPrior on one dataset (e.g., NHANES); quantify impact on NRMSE and Macro-F1.
  3. **Imbalance stress test**: Take eICU categorical subset; vary class imbalance artificially; plot Macro-F1 vs imbalance ratio for SNI vs MissForest to characterize break point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can imbalance-aware objectives or calibrated probability outputs substantially close the categorical performance gap between SNI and tree-based baselines on severely skewed discrete targets?
- Basis in paper: [explicit] Authors state: "SNI is not consistently the strongest method for severely imbalanced categorical variables... This suggests that stronger discrete inductive biases (e.g., imbalance-aware objectives, calibrated probability outputs, or targeted post-processing) may be needed."
- Why unresolved: The focal-loss variant used in experiments only partially mitigates imbalance; substantial Macro-F1 gaps remain on datasets like eICU and AutoMPG.
- What evidence would resolve it: Ablation experiments comparing focal loss against alternative strategies (class-balanced sampling, calibrated Platt scaling, cost-sensitive learning) on synthetic datasets with controlled imbalance ratios.

### Open Question 2
- Question: Can low-rank or sparse attention approximations reduce SNI's O(d²) cost while preserving both imputation accuracy and dependency-matrix fidelity?
- Basis in paper: [explicit] Authors note: "Potential directions for future work include: (i)... (ii) low-rank/sparse attention approximations to reduce O(d²) cost" and that higher computational cost "limits applicability to settings where offline processing is acceptable."
- What evidence would resolve it: Benchmark comparing full attention against efficient variants (e.g., Performer, Linformer, or block-sparse attention) on datasets with 100+ features, measuring both NRMSE degradation and correlation between full and approximated dependency matrices.

### Open Question 3
- Question: Does extending SNI's controllable-prior mechanism to temporal data (e.g., via time-aware attention or recurrent priors) retain the interpretability benefits observed in static tables?
- Basis in paper: [explicit] "Extending SNI to longitudinal settings—while retaining explicit feature-dependency summaries—is a potential direction for future work."
- Why unresolved: Current evaluation is limited to static tabular benchmarks, yet high-impact domains (ICU monitoring, industrial logs) have temporal structure and time-dependent missingness.
- What evidence would resolve it: A temporal SNI variant evaluated on time-series clinical data (e.g., MIMIC-IV waveforms), reporting both imputation accuracy and whether time-resolved dependency matrices provide clinically meaningful insights.

### Open Question 4
- Question: How stable are the learned dependency matrix D and prior coefficients {λ_h} across random seeds, missingness realizations, and preprocessing choices?
- Basis in paper: [explicit] "Their stability across seeds, missingness realizations, and preprocessing choices should be examined when these outputs are used for domain-facing conclusions."
- Why unresolved: The paper reports mean ± SD for accuracy metrics but does not quantify variability of the interpretability artifacts themselves.
- What evidence would resolve it: Systematic study measuring pairwise similarity (e.g., Spearman correlation of edge weights, coefficient variance) of D and {λ_h} across multiple runs with different seeds, mask draws, and alternative correlation estimators (Pearson vs. robust vs. copula).

## Limitations
- SNI is sensitive to non-ignorable missingness (MNAR), as indicated by stress tests, though it is theoretically grounded under MAR.
- SNI often underperforms on severely imbalanced categorical variables, even with focal loss, due to limited discrete inductive biases.
- The method has higher computational cost than classical approaches, limiting its applicability to offline processing.

## Confidence
- **Theoretical grounding**: Medium (approximate EM, not monotonic improvement)
- **Interpretability claims**: Low-Medium (attention weights may not reflect true reliance)
- **Imputation accuracy**: Medium (strong on continuous, weaker on imbalanced categorical)
- **Scalability**: Low (O(d²) attention, offline-only recommendation)

## Next Checks
1. **Prior strength ablation**: Systematically vary α across datasets to map the tradeoff between statistical stability and nonlinear capture.
2. **Synthetic DAG recovery**: On controlled linear_gaussian and nonlinear_mixed graphs, verify D’s edge recovery beats baselines as in Table S21.
3. **Class imbalance scaling**: Gradually increase imbalance in categorical features; quantify SNI vs MissForest break points in Macro-F1.