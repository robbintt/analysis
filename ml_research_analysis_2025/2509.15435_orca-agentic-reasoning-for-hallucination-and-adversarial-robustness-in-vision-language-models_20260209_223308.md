---
ver: rpa2
title: 'ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language
  Models'
arxiv_id: '2509.15435'
source_url: https://arxiv.org/abs/2509.15435
tags:
- orca
- adversarial
- reasoning
- robustness
- standalone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ORCA, an agentic reasoning framework designed\
  \ to enhance the robustness of pretrained large vision-language models (LVLMs) against\
  \ hallucinations and adversarial attacks. ORCA operates via an Observe\u2013Reason\u2013\
  Critique\u2013Act loop, using an LLM-based agent to query a suite of small vision\
  \ models (\u22643B parameters) for evidential cross-validation and iterative refinement."
---

# ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models

## Quick Facts
- arXiv ID: 2509.15435
- Source URL: https://arxiv.org/abs/2509.15435
- Authors: Chung-En Johnny Yu; Hsuan-Chih; Chen; Brian Jalaian; Nathaniel D. Bastian
- Reference count: 39
- Primary result: ORCA improves LVLM robustness to hallucinations and adversarial attacks via agentic reasoning without retraining

## Executive Summary
This paper introduces ORCA, an agentic reasoning framework designed to enhance the robustness of pretrained large vision-language models (LVLMs) against hallucinations and adversarial attacks. ORCA operates via an Observe–Reason–Critique–Act loop, using an LLM-based agent to query a suite of small vision models (≤3B parameters) for evidential cross-validation and iterative refinement. Unlike prior methods, ORCA requires no model access, fine-tuning, or additional training data. Experiments show that ORCA consistently improves accuracy on hallucination benchmarks (e.g., +3.64% to +40.67% on POPE) and exhibits strong adversarial robustness without requiring adversarial training or defense mechanisms.

## Method Summary
ORCA implements a test-time reasoning framework that leverages architectural diversity among small vision models to detect and correct hallucinations in LVLMs. The method uses an LLM agent (LLaMA-3.2-Vision-Instruct) to orchestrate a reasoning loop that queries multiple specialized vision tools (object detector, VLM, VQA model) for evidential cross-validation. When inconsistencies are detected between the LVLM's output and auxiliary model responses, the agent generates attribute-guided follow-up questions to probe specific details. A symbolic rule-based aggregator then combines responses to produce a final decision. The entire process requires no training data or model modifications, making it applicable to any pretrained LVLM.

## Key Results
- ORCA achieves +3.64% to +40.67% accuracy improvements on POPE hallucination benchmarks across different LVLM variants
- Under adversarial attacks, ORCA demonstrates average accuracy gains of +20.11% across LVLMs without requiring adversarial training
- The framework maintains strong performance on AMBER benchmarks with reduced CHAIR, Hal, and Cog scores indicating fewer hallucinations
- ORCA shows effectiveness across multiple LVLM backbones (mPLUG-Owl, MiniGPT-4, Qwen-VL-Chat) without model-specific tuning

## Why This Works (Mechanism)

### Mechanism 1: Diversity-Based Error Triangulation
If a base LVLM hallucinates due to intrinsic bias or an adversarial perturbation, querying a suite of architecturally diverse, smaller vision models can isolate the error through disagreement. The system dispatches queries to multiple vision models (e.g., object detector vs. VLM) with different backbones and inductive biases. Because these models have different vulnerabilities, an adversarial perturbation successful on the primary LVLM is less likely to fool all auxiliary models simultaneously.

### Mechanism 2: Attribute-Guided Evidential Querying
Binary "yes/no" questions about object existence are prone to bias; decomposing them into specific attribute queries forces the model to ground its reasoning in visual evidence. When an inconsistency is detected, the agent generates follow-up questions based on object attributes (e.g., asking "What color is the shirt?" rather than just "Is there a shirt?"). This tests the internal logical consistency of the scene description, as hallucinations often fail to maintain logical consistency across granular attributes.

### Mechanism 3: Iterative Refinement via Symbolic Consistency Rules
The "Collective Decision-Making" module applies hard-coded rules (e.g., prioritizing Object Detector "Yes" over VLM "Uncertain") rather than a weighted average. This exploits the higher precision of specialized models for specific tasks. The symbolic mechanism applies rule-based templates that encode human knowledge about which model outputs to trust in different scenarios.

## Foundational Learning

**Concept: Adversarial Transferability**
Why needed here: The paper defends against attacks where a perturbation optimized on one model is applied to the target ORCA system. Understanding that attacks often transfer across similar architectures explains why ORCA uses diverse architectures to break this transfer chain.
Quick check question: Why does ORCA use models with different backbones (e.g., CLIP, EVA, OpenCLIP) rather than three versions of the same model?

**Concept: Object Hallucination vs. Distribution Shift**
Why needed here: The paper distinguishes between intrinsic errors (hallucinations) and external attacks. Both result in the same symptom (asserting non-existent objects), but the mitigation relies on cross-validation.
Quick check question: Does ORCA differentiate between a "natural" hallucination and an "adversarially induced" error during inference?

**Concept: ReAct (Reasoning + Acting) Pattern**
Why needed here: ORCA implements a ReAct-style loop. You must understand that the LLM acts as a "controller" that generates thoughts (reasoning traces) and actions (tool calls) iteratively.
Quick check question: In the ORCA loop, what triggers the transition from "Observe" to "Act"?

## Architecture Onboarding

**Component map:**
Controller (LLaMA-3.2-Vision-Instruct 11B) -> Base LVLM (target model) -> Tool Suite (DETR, PaliGemma, BLIP) -> Collective Decision-Making -> Final Output

**Critical path:**
1. Initial Query: User Question → Base LVLM + Tools
2. Consistency Check: Compare responses using symbolic rules
3. Branching: If Inconsistent → Generate Attribute Queries → Re-query Tools
4. Final Aggregation: Return result based on majority/rule consensus

**Design tradeoffs:**
- Latency vs. Robustness: ~33× increase in inference time compared to standalone LVLMs due to iterative looping and multiple model queries
- Training-Free vs. Performance: No fine-tuning required, but relies on quality of "off-the-shelf" lightweight tools
- Simplicity vs. Flexibility: Symbolic rules are interpretable but may not capture all edge cases

**Failure signatures:**
- Confident Consensus on Error: If all tools (LVLM and detectors) hallucinate the same object or are all fooled by the same perturbation, ORCA will output the wrong answer with high confidence
- Distribution Mismatch: If the image is out-of-distribution for the small tools but the LVLM generalizes, the tools may wrongly veto the correct LVLM answer

**First 3 experiments:**
1. Baseline Stress Test: Run the Base LVLM on the POPE dataset (Random subset) to establish the hallucination rate without ORCA
2. Component Ablation: Disable the "Evidential Query Generation" step and rely only on initial queries. Measure the drop in accuracy on the "Concurrence" subset to quantify the value of the iterative loop
3. Robustness Check: Apply "Adversarial Illusions" perturbations to a set of images and run ORCA. Verify that the "Cross-Model Querying" mechanism detects the inconsistency between the perturbed LVLM output and the detector output

## Open Questions the Paper Calls Out

**Open Question 1: Confidently Incorrect Cross-Model Agreement**
How can the framework detect or mitigate "confidently incorrect cross-model agreement"? ORCA currently relies on consistency across models; if models share similar biases or are fooled by the same perturbation, the reasoning loop may reinforce an incorrect answer. Evidence would require a module capable of flagging high-consensus predictions that contradict ground truth.

**Open Question 2: Attribute and Relationship Hallucinations**
Can the evidential querying paradigm effectively mitigate non-object hallucinations regarding attributes and relationships? While the work focuses on object-level hallucinations, the paradigm "naturally extends" to these more complex forms, but this capability is currently unverified. Validation would require evaluation results on benchmarks specifically designed for attribute and relational hallucinations.

**Open Question 3: Text-Based Adversarial Attacks**
Is the central LLM-based agent susceptible to text-based adversarial attacks or prompt injections? The paper evaluates robustness against image perturbations but relies on a single LLM agent to reason over text. Text-based attacks manipulating the agent's logic could bypass the visual cross-validation, though this remains unexplored.

## Limitations
- Performance depends heavily on the quality and diversity of the auxiliary vision tools; shared blind spots could create single points of failure
- Computational overhead is substantial (~33× more inference time), making it impractical for latency-sensitive applications
- Limited to object-level hallucinations; does not address attribute errors, relationship confusions, or other non-object hallucination types

## Confidence

**High Confidence** - The core mechanism of using diverse vision models for cross-validation is technically sound and well-supported by empirical results showing 3.64% to 40.67% accuracy improvements on POPE and +20.11% gains under adversarial attacks.

**Medium Confidence** - Claims about "defense without retraining" are accurate for tested threat models, but effectiveness against adaptive adversaries targeting the cross-validation mechanism remains unproven. The symbolic rule-based aggregation may be exploitable.

**Low Confidence** - Limited evidence about ORCA's performance on out-of-distribution data or when vision tools themselves hallucinate. The framework's reliance on specialized tools with lower recall rates could lead to false negatives in detecting genuine objects.

## Next Checks

1. **Adversarial Transferability Test**: Design and apply adversarial perturbations specifically crafted to fool the auxiliary vision models while leaving the base LVLM's output unchanged. Measure whether ORCA still detects the inconsistency or if the attack succeeds through tool compromise.

2. **Distribution Shift Evaluation**: Test ORCA on image domains significantly different from COCO (e.g., medical imaging, satellite imagery, or artistic renderings) where the small vision tools were not trained. Quantify performance degradation and false negative rates when tools incorrectly veto correct LVLM answers.

3. **Component Ablation Under Stress**: Systematically disable each ORCA component (initial querying, evidential query generation, symbolic aggregation) and measure performance under increasing adversarial perturbation strength. Identify which mechanism provides the most robustness per computational cost.