---
ver: rpa2
title: Culture Matters in Toxic Language Detection in Persian
arxiv_id: '2506.03458'
source_url: https://arxiv.org/abs/2506.03458
tags:
- hate
- language
- detection
- persian
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses toxic language detection in Persian, a low-resource
  language with limited annotated datasets. The authors compare various methods including
  fine-tuning, data enrichment, zero/few-shot learning, and cross-lingual transfer
  learning using multiple large language models.
---

# Culture Matters in Toxic Language Detection in Persian

## Quick Facts
- arXiv ID: 2506.03458
- Source URL: https://arxiv.org/abs/2506.03458
- Authors: Zahra Bokaei; Walid Magdy; Bonnie Webber
- Reference count: 40
- Primary result: Transfer learning effectiveness for Persian toxic language detection depends significantly on cultural context, with models trained on culturally similar languages (Arabic, Indonesian) outperforming those trained on English

## Executive Summary
This study investigates toxic language detection in Persian, a low-resource language with limited annotated datasets. The authors compare multiple approaches including fine-tuning, data enrichment via distant supervision, zero/few-shot learning, and cross-lingual transfer learning using various large language models. A novel distant supervision method enriches the PHATE dataset with 3,291 additional toxic tweets and 3,200 neutral tweets. The key finding demonstrates that cultural context significantly impacts transfer learning effectiveness - models trained on languages from culturally similar countries (Arabic and Indonesian) achieve better performance than those trained on English, highlighting the importance of cultural alignment in toxic language detection across languages.

## Method Summary
The study employs multiple approaches to toxic language detection in Persian. The baseline PHATE dataset (7,056 tweets) is enriched through distant supervision using a 604-keyword lexicon expanded via FastText embeddings. Multiple models are fine-tuned including ParsBERT, XLM-R, mT5, Llama 3 variants, Dorna2-Llama3, GEMMA 2, and GPT 3.5. Transfer learning experiments use balanced 8,050-sample subsets from Arabic, Indonesian, and English hate speech datasets. Models are evaluated using F1-macro across hate/vulgarity/violence classes with best epochs selected by validation F1. The study also tests zero/few-shot learning with GPT 3.5 using English prompts.

## Key Results
- Transfer learning effectiveness depends on cultural similarity - Arabic and Indonesian models outperform English for hate/vulgarity detection in Persian
- Dorna2-Llama3 Instruct (8B, Persian-specific) outperforms multilingual models including Llama 3 Base and XLM-R
- ParsBERT with distant supervision shows substantial improvements (F1=75 for Hate, F1=72 for Vulgar) but introduces noise, particularly in violence classification
- Distant supervision improves BERT-scale models but degrades larger LLMs due to differential noise tolerance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning for toxic language detection is more effective when source and target languages share cultural context rather than merely linguistic features or dataset scale.
- Mechanism: Culturally similar regions (Arabic-speaking countries, Indonesia) share hate speech targets—religion, politics, sociopolitical events—that align with Persian discourse. Models trained on these datasets learn relevant conceptual patterns, not just surface linguistic features, which transfer more effectively to Persian toxicity detection.
- Core assumption: Toxic language is partially defined by shared cultural knowledge (what topics are sensitive, what rhetorical patterns signal harm) rather than purely lexical markers.
- Evidence anchors:
  - [abstract] "We show that the language of a country with cultural similarities to Persian yields better results in transfer learning. Conversely, the improvement is lower when the language comes from a culturally distinct country."
  - [section 6.4] "The general culture of hate in Persian, Arabic, and Indonesian appears to be more similar, particularly in targets related to religion, politics, and common controversial events... In contrast, the English hate dataset predominantly focuses on contexts diverging significantly from the Persian hate dataset (e.g. sexual orientation and ethnic groups)."
  - [corpus] Related work on cross-cultural transfer (e.g., "Cross-Cultural Transfer of Commonsense Reasoning in LLMs") supports that cultural alignment improves transfer, though specific evidence for toxicity detection beyond this paper is limited.
- Break condition: If the source culture's hate targets diverge substantially from the target culture's (e.g., English focusing on sexual orientation, Persian focusing on political/religious targets), transfer gains diminish or reverse.

### Mechanism 2
- Claim: Distant supervision via lexicon-based labeling improves BERT-scale models but degrades larger LLMs due to differential noise tolerance.
- Mechanism: A manually curated toxic lexicon (604 keywords across hate/vulgarity/violence) is expanded via FastText embeddings, then used to label historical tweets. BERT-based models (ParsBERT, XLM-R) benefit from the additional training signal despite noise. Larger models (Llama 3, GEMMA 2) appear to overfit to noisy labels or struggle with the inconsistent supervision signal.
- Core assumption: Keyword presence correlates sufficiently with toxicity labels to provide useful training signal, and model capacity determines whether noise hurts or helps.
- Evidence anchors:
  - [section 5.3] "Distant supervision improves mT5 and significantly enhances BERT base models. However, it performs poorly on Llama 3 and GEMMA 2. The metrics reveal that the results on Llama 3 are 50% worse than those on GEMMA 2, suggesting that Llama 3 is less tolerant to noise when trained on Persian."
  - [section 5.2] ParsBERT with distant supervision achieved F1=75 for Hate and F1=72 for Vulgar, vs. baseline F1=60 and F1=60 respectively.
  - [corpus] Weak evidence in corpus for distant supervision specifically in Persian; this appears to be a novel contribution of the paper.
- Break condition: When lexicon keywords appear in neutral contexts (e.g., "kill" used humorously), precision degrades—68% of neutral tweets were misclassified as "violence" in the enriched dataset.

### Mechanism 3
- Claim: Native Persian-trained instruction-tuned models outperform multilingual models even when the latter have higher parameter counts, suggesting language-specific pretraining provides task-agnostic advantages.
- Mechanism: Dorna2-Llama3 Instruct (8B, Persian-specific training) outperforms Llama 3 Base (8B, multilingual), XLM-R (125M, multilingual), and mT5 (120M, multilingual) on Persian toxicity detection. Pre-training on Persian text likely improves morphological understanding and implicit meaning detection.
- Core assumption: Language-specific pretraining captures morphological patterns, script processing, and cultural priors that transfer across tasks.
- Evidence anchors:
  - [section 5.2] "Dorna2-Llama3.1-Instruct... outperformed both Llama 3 – Instruct and Llama 3 – Base, achieving higher F1 scores for the 'Violence' and 'Hate' classes. Notably, among all fine-tuned models in our experiments, this model achieved the highest results for detecting 'Violence'."
  - [section 6.2] "Regarding models specifically trained on Persian, in comparison to others, ParsBERT still lagged in detecting toxic language. In contrast, the recent Dorna2-Llama3.1-Instruct achieved better overall results."
  - [corpus] PerCul and MELAC benchmarks similarly show that Persian-specific evaluation reveals gaps in multilingual models, supporting the language-specific pretraining advantage.
- Break condition: If the task requires cross-lingual generalization (detecting loanwords, code-switching), language-specific models may underperform multilingual alternatives.

## Foundational Learning

- Concept: **Transfer Learning in NLP**
  - Why needed here: The paper's core finding—that cultural similarity determines transfer success—requires understanding how pre-trained models adapt to new tasks and what features transfer across languages.
  - Quick check question: Can you explain why a model trained on English hate speech might fail to detect Persian hate speech even if both use similar lexical patterns?

- Concept: **Distant Supervision**
  - Why needed here: The paper introduces a novel distant supervision method for Persian; understanding the trade-off between data quantity and label noise is essential.
  - Quick check question: What types of labeling errors would you expect when using keyword matching to detect toxicity, and how might different model architectures respond?

- Concept: **Toxic Language Taxonomy**
  - Why needed here: The paper distinguishes hate, vulgarity, and violence—these categories have different cultural expressions and detection challenges.
  - Quick check question: Why might "wishing harm" be classified differently than "threatening harm," and how does cultural context affect this distinction?

## Architecture Onboarding

- Component map: Input → Tokenization (Persian-aware) → Model Backbone → Classification Head → [Option A: BERT-base (ParsBERT/XLM-R)] or [Option B: Llama-family (Dorna2)] → Training Data: Baseline PHATE | + Distant Supervision | + Transfer Source

- Critical path:
  1. **Data preparation**: Start with PHATE dataset (7,056 tweets). If enriching, build lexicon via native speaker annotation → FastText expansion → label historical tweets.
  2. **Model selection**: For BERT-scale, use ParsBERT with distant supervision. For LLM-scale, use Dorna2-Llama3 Instruct with fine-tuning only (avoid distant supervision for large models).
  3. **Transfer source selection**: If using transfer learning, prioritize Arabic/Indonesian over English for hate/vulgarity; English may help for violence.
  4. **Evaluation**: Use F1-macro across hate/vulgarity/violence classes on held-out test set.

- Design tradeoffs:
  - ParsBERT + distant supervision: Best precision/recall balance, lower compute, but requires lexicon curation.
  - Dorna2-Llama3 Instruct: Best overall performance without distant supervision noise, but requires more compute.
  - Transfer from Arabic/Indonesian: Improves hate detection (cultural alignment), but adds data preparation complexity.
  - Transfer from English: Marginal gains, simpler to implement (abundant datasets), but cultural mismatch limits effectiveness.

- Failure signatures:
  - **Violence precision drop with distant supervision**: Keywords like "kill," "explode" used in neutral contexts cause false positives.
  - **Vulgarity recall stagnation**: Implicit profanity (sarcasm, euphemisms) not captured by lexicon.
  - **English transfer underperformance**: Model learns irrelevant hate targets (sexual orientation) missing from Persian discourse.
  - **GPT few-shot plateau**: Performance doesn't improve beyond 2-shot; context-dependent hate remains undetected.

- First 3 experiments:
  1. **Baseline**: Fine-tune ParsBERT on PHATE training split. Evaluate on test set. Expected: F1 ~57-60 per class.
  2. **Distant supervision enrichment**: Build 604-keyword lexicon, label 6,491 additional tweets, fine-tune ParsBERT on combined data. Expected: Hate F1 ~75, Vulgar F1 ~72, Violence precision may drop.
  3. **Cross-lingual transfer**: Fine-tune Llama 3-Base on Arabic hate/vulgarity data (8,050 samples), evaluate on Persian test set. Expected: Hate F1 ~81-87 with Arabic+Persian combined.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating Persian data alongside Arabic and Indonesian datasets improve performance for the hate speech detection task?
- Basis in paper: [explicit] The authors state, "Further investigation is needed to determine why incorporating Persian did not lead to additional improvements" when combining Indonesian, Arabic, and Persian datasets.
- Why unresolved: The authors observed a slight performance drop when Persian was added to the Arabic+Indonesian combination, but error analysis failed to reveal a clear pattern explaining this decline.
- What evidence would resolve it: A detailed analysis of how Persian data interacts with Arabic and Indonesian data during training, potentially identifying conflicts or redundancies in the combined dataset.

### Open Question 2
- Question: How can toxic language detection systems be adapted to handle evolving, event-specific toxicity that may emerge in the future?
- Basis in paper: [explicit] The authors acknowledge that "some keywords in the lexicon are heavily event-specific and may lose relevance over time as those events fade from public memory."
- Why unresolved: The distant supervision approach relies on a fixed lexicon that cannot anticipate new forms of toxicity associated with future events.
- What evidence would resolve it: A longitudinal study evaluating lexicon effectiveness over time or development of a dynamic lexicon update mechanism.

### Open Question 3
- Question: How can cross-lingual transfer learning be optimized to better handle idiomatic expressions and culturally-dependent references?
- Basis in paper: [inferred] The transfer learning approaches revealed challenges in detecting instances containing "idiomatic expressions and culturally dependent references that require specific background knowledge."
- Why unresolved: The paper doesn't propose specific methods to address this challenge beyond adding Persian data, which still leaves some culturally-specific content undetected.
- What evidence would resolve it: Comparative experiments with culturally-aware transfer learning approaches or inclusion of cultural context features in model training.

## Limitations
- Dataset scale constraints: The PHATE dataset contains only 7,056 tweets, which is relatively small for training large language models.
- Lexicon-based noise: The distant supervision approach introduces significant labeling noise, with 68% of neutral tweets being misclassified as "violence."
- Cultural generalization: The study focuses on comparing Arabic, Indonesian, and English as transfer sources, but doesn't explore other culturally relevant languages.
- Zero-shot limitations: The zero/few-shot learning results show poor performance (F1 scores around 47-58), but the paper doesn't explore whether prompt engineering might improve these results.

## Confidence
- **High confidence**: The core finding that culturally similar languages (Arabic, Indonesian) outperform linguistically or geographically distant languages (English) in transfer learning is well-supported by the experimental results and consistent across multiple evaluation metrics.
- **Medium confidence**: The distant supervision mechanism's differential impact on BERT vs. LLM models is supported by the data, but the underlying reasons (noise tolerance, overfitting) are inferred rather than directly tested.
- **Medium confidence**: The claim that Persian-specific pretraining (Dorna2) provides advantages over multilingual models is supported by the results, but could benefit from ablation studies comparing different pre-training approaches.

## Next Checks
1. **Noise characterization study**: Sample 200 false positive predictions from the distant supervision-enriched dataset to quantify the types and frequencies of labeling errors.
2. **Cultural transfer expansion**: Replicate the transfer learning experiments using additional culturally proximate languages (e.g., Urdu, Turkish, or other regional languages).
3. **Prompt optimization for few-shot learning**: Systematically test different prompt templates, shot counts, and chain-of-thought approaches for GPT 3.5 to determine if the poor few-shot performance is due to fundamental limitations or suboptimal prompting strategies.