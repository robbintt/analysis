---
ver: rpa2
title: 'FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data'
arxiv_id: '2501.17144'
source_url: https://arxiv.org/abs/2501.17144
tags:
- data
- delimiter
- tuple
- reasoning
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FactCG, a factuality classifier for detecting
  hallucinations in LLM-generated text. The authors identify that existing training
  data lacks multi-hop reasoning complexity, leading to disconnected reasoning.
---

# FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data

## Quick Facts
- arXiv ID: 2501.17144
- Source URL: https://arxiv.org/abs/2501.17144
- Reference count: 16
- Primary result: Achieves SOTA on LLM-Aggrefact benchmark, outperforming similar-sized models and GPT-4-o

## Executive Summary
FactCG introduces a novel approach to detecting hallucinations in LLM-generated text by addressing the critical gap in multi-hop reasoning capabilities found in existing training datasets. The system combines a graph-based context generation method (CG2C) with a specialized factuality classifier, achieving state-of-the-art performance on the LLM-Aggrefact benchmark. By generating synthetic training data through context graphs rather than relying on LLM-based labeling, FactCG demonstrates more connected reasoning while reducing dependency on potentially hallucinated labels.

## Method Summary
The FactCG system employs a two-stage approach: first, it uses CG2C to generate multi-hop claims from context graphs without LLM-based labeling; second, it trains a factuality classifier on this synthetic data. The CG2C method extracts relational triples from source documents, constructs context graphs, and generates claims that require multi-hop reasoning. This approach addresses the fundamental limitation that existing datasets lack the complexity needed for realistic hallucination detection, where models must trace reasoning across multiple connected facts rather than isolated statements.

## Key Results
- FactCG achieves state-of-the-art performance on LLM-Aggrefact benchmark
- Outperforms models of similar size and even GPT-4-o on hallucination detection
- Demonstrates more connected reasoning compared to baselines trained on existing datasets

## Why This Works (Mechanism)
The mechanism works by addressing the core limitation in current hallucination detection: the lack of multi-hop reasoning in training data. By constructing context graphs that capture relationships between facts, FactCG forces the model to learn connected reasoning patterns rather than isolated fact verification. The synthetic data generation approach eliminates the circularity problem where LLMs generate both the claims and the labels, ensuring more reliable training signals.

## Foundational Learning

**Graph-based reasoning** - Understanding how nodes and edges represent factual relationships
*Why needed:* Enables construction of multi-hop reasoning paths
*Quick check:* Can identify shortest path between two facts in a graph

**Multi-hop inference** - Ability to combine multiple facts to reach conclusions
*Why needed:* Real hallucinations often require tracing through several connected facts
*Quick check:* Can correctly answer questions requiring 2+ reasoning steps

**Synthetic data generation** - Creating training examples programmatically
*Why needed:* Avoids dependence on potentially unreliable LLM-generated labels
*Quick check:* Generated examples maintain factual consistency and diversity

## Architecture Onboarding

**Component map:** Context Graph Builder -> Claim Generator -> Factuality Classifier

**Critical path:** Document extraction → Triple extraction → Graph construction → Multi-hop claim generation → Classifier training → Inference

**Design tradeoffs:** 
- Synthetic vs. human-labeled data (quality vs. scalability)
- Graph complexity vs. computational efficiency
- Model size vs. inference speed

**Failure signatures:** 
- Over-reliance on local context rather than global graph structure
- Inability to handle claims requiring reasoning beyond graph depth
- Sensitivity to graph construction errors

**First experiments:** 
1. Test classifier on claims requiring exactly 2-hop reasoning
2. Evaluate performance degradation with increasing graph depth
3. Compare synthetic vs. human-labeled data performance

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Synthetic training data may not capture all real-world hallucination scenarios
- Evaluation limited to single benchmark (LLM-Aggrefact)
- Computational overhead of maintaining context graphs for real-time applications not addressed

## Confidence

**High Confidence:** FactCG achieves SOTA on LLM-Aggrefact benchmark with clear empirical support

**Medium Confidence:** Existing training data lacks multi-hop reasoning complexity - reasonable but not definitively proven

**Low Confidence:** FactCG demonstrates "more connected reasoning" - qualitative observation lacking quantitative validation

## Next Checks

1. **Cross-domain evaluation:** Test FactCG on factuality benchmarks from different domains (healthcare, legal, scientific) to assess generalization

2. **Human evaluation study:** Conduct human judgment study comparing FactCG's reasoning paths with baselines to quantitatively measure "connected reasoning" improvement

3. **Real-world deployment test:** Deploy FactCG in controlled environment with actual LLM-generated content to measure precision, recall, and computational efficiency in practical scenarios