---
ver: rpa2
title: 'Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models'
arxiv_id: '2512.13703'
source_url: https://arxiv.org/abs/2512.13703
tags:
- harmful
- content
- methods
- safe2harm
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Safe2Harm, a semantic isomorphism attack method
  that exploits the underlying principle similarity between harmful and legitimate
  scenarios to jailbreak large language models. The method rewrites harmful questions
  into semantically safe ones, extracts thematic mappings, generates safe responses,
  then inverts them to produce harmful content.
---

# Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models

## Quick Facts
- arXiv ID: 2512.13703
- Source URL: https://arxiv.org/abs/2512.13703
- Authors: Fan Yang
- Reference count: 40
- Primary result: Safe2Harm achieves highest attack success rates compared to existing methods, with effectiveness increasing on larger models

## Executive Summary
Safe2Harm introduces a novel jailbreaking technique that exploits semantic isomorphism between harmful and legitimate scenarios to bypass LLM safety mechanisms. The method transforms harmful queries into semantically safe equivalents that share underlying principles, generates detailed responses, then inverts them back to harmful content. Experiments across seven mainstream LLMs and three benchmark datasets demonstrate that Safe2Harm outperforms existing attack methods, particularly on larger models where traditional safety scaling breaks down.

## Method Summary
Safe2Harm operates through a four-stage pipeline: (1) rewriting harmful queries into semantically safe equivalents with shared underlying principles, (2) extracting thematic mappings between harmful and safe topic pairs, (3) generating detailed responses to the safe queries using the target LLM, and (4) inverting these responses back to harmful content using the extracted mappings. The approach uses an auxiliary LLM (Qwen-Plus) for stages 1-3 and the target LLM itself for stage 4, with automated quality verification via a judge LLM (Doubao) providing up to three retries per stage.

## Key Results
- Safe2Harm achieves the highest attack success rates compared to existing methods across seven mainstream LLMs
- Attack effectiveness increases with model size, inverting the typical trend where larger models are safer
- Qwen3Guard demonstrates over 77% accuracy in detecting harmful content generated by Safe2Harm
- The constructed 358-sample harmful content evaluation dataset reveals detection gaps in current safety systems

## Why This Works (Mechanism)

### Mechanism 1: Principle-Level Semantic Gap
LLM safety mechanisms prioritize surface-level semantic categorization over deeper principle-level reasoning. Safe2Harm exploits the gap between how models classify scenarios ("harmful" vs. "safe" labels) and the technical principles those scenarios share. By rewriting a harmful query into a semantically safe but principle-equivalent form, the input bypasses semantic filters, allowing the model to generate detailed responses that are later inverted back to harmful content.

### Mechanism 2: Scaling Paradox
Larger, more capable models are paradoxically more vulnerable to Safe2Harm due to improved task execution. As model size increases, language understanding and instruction-following capabilities improve, enabling more accurate completion of the four-stage attack pipeline. This leads to higher attack success rates, inverting the typical trend where larger models are safer.

### Mechanism 3: Automated Quality Verification
Automated quality verification using LLM-as-judge can stabilize the multi-stage attack pipeline with minimal human intervention. Each stage is validated by a separate judge LLM, with outputs failing quality checks triggering up to 3 retries per stage. This automation ensures reliability without requiring manual review.

## Foundational Learning

- **Concept: Jailbreak Attacks** - Understanding the broader attack taxonomy (white-box gradient-based vs. black-box optimization) contextualizes Safe2Harm's novelty as a principle-level semantic attack. Quick check: Can you explain why Safe2Harm is considered a black-box attack despite using an auxiliary LLM?

- **Concept: Safety Alignment** - The attack targets alignment mechanisms (SFT, RLHF, DPO); knowing how these work reveals why semantic surface-matching filters are vulnerable. Quick check: How might DPO-trained models differ in their vulnerability to principle-level attacks compared to RLHF?

- **Concept: Semantic Isomorphism** - This is the paper's core theoretical contribution, meaning two inputs with different surface semantics map to near-identical internal principle representations. Quick check: Give an example of a harmful/safe scenario pair that is not semantically isomorphic but shares some principles.

## Architecture Onboarding

- **Component map**: Harmful input → [Rewrite via Qwen-Plus] → Safe query → [Map Extraction via Qwen-Plus] → Topic pairs → [Safe Generation via Target LLM] → Safe response → [Inversion via Target LLM] → Harmful output → [Judge via Doubao]

- **Critical path**: The attack pipeline consists of four sequential stages, each with quality verification and up to 3 retries. Stages 1-3 use Qwen-Plus, while stages 4 and inversion use the target LLM. The judge model validates each output before proceeding.

- **Design tradeoffs**: Automation vs. reliability (fully automated pipeline risks silent failures if judge is flawed), auxiliary model choice (single LLM simplifies deployment but may propagate biases), and retry limit (3 retries balances success rate vs. API costs).

- **Failure signatures**: Stage 1 refusals if rewritten query still triggers safety filters, Stage 2 mapping errors leading to incoherent inversion, Stage 3 vague responses producing insufficient detail, Stage 4 inversion refusals at final step.

- **First 3 experiments**:
  1. Baseline comparison on 3 models (Qwen3-8B, Llama-3-8B, GPT-5) against GCG and PAIR on 100 HarmBench samples, measuring ASR and iteration counts
  2. Ablation study removing quality verification step on Qwen3-4B across JailbreakBench to measure ASR drop
  3. Defense evaluation deploying Qwen3Guard 4B as output filter on Safe2Harm-attacked GPT-5 pipeline using the 358-sample evaluation dataset

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the semantic isomorphism vulnerability transfer effectively to Vision-Language Models (VLMs) in cross-modal scenarios? The current study restricts evaluation to text-based LLMs, leaving multimodal interaction unexplored.

- **Open Question 2**: Can fine-tuning thematic mapping models overcome the failure of current attacks on high-sensitivity tasks like generating explicit content? The current method struggles with highly sensitive topics without triggering rejection.

- **Open Question 3**: Does the internal "Thinking Mode" (Chain-of-Thought) inherent to advanced reasoning models systematically dilute safety alignment capabilities? The paper observes correlation between thinking length and successful attacks but doesn't determine if this is fundamental trade-off or training limitation.

## Limitations

- The core mechanism relies on the existence of consistent principle-level mappings between harmful and safe scenarios, which may not be exhaustive across all domains
- The quality verification system depends entirely on the judge model's accuracy, with no fallback if the judge has systematic blind spots
- The labor costs of manually curating evaluation datasets and whether principle mappings can be automated at scale are not addressed

## Confidence

- **High Confidence**: Safe2Harm achieves superior ASR compared to baselines; larger models show increased vulnerability; Qwen3Guard demonstrates strong detection performance
- **Medium Confidence**: Semantic isomorphism is "previously overlooked"; four-stage pipeline design is optimal; quality verification with 3 retries achieves optimal success rate
- **Low Confidence**: Method generalizes to all harmful content types; attack will maintain effectiveness as models evolve

## Next Checks

1. **Principle Mapping Validation**: Systematically evaluate principle mapping coverage and reliability by testing Safe2Harm on diverse harmful scenarios not in original benchmarks, measuring success rates and identifying failure categories

2. **Judge Model Robustness**: Conduct adversarial testing of the judge model by introducing subtle principle distortions and measuring false negative rates, comparing against human evaluation to establish baseline accuracy

3. **Cross-Domain Transferability**: Apply Safe2Harm to specialized domains (biomedical, legal, cybersecurity) where principle-level reasoning differs from general knowledge, measuring whether attack maintains effectiveness or requires domain adaptation