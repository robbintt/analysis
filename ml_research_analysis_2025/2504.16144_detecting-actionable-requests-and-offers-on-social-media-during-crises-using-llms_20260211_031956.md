---
ver: rpa2
title: Detecting Actionable Requests and Offers on Social Media During Crises Using
  LLMs
arxiv_id: '2504.16144'
source_url: https://arxiv.org/abs/2504.16144
tags:
- data
- social
- taxonomy
- media
- requests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of identifying and classifying
  actionable requests and offers on social media during crises. The authors propose
  a fine-grained hierarchical taxonomy with three dimensions: supplies, emergency
  personnel, and actions.'
---

# Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs

## Quick Facts
- arXiv ID: 2504.16144
- Source URL: https://arxiv.org/abs/2504.16144
- Authors: Ahmed El Fekih Zguir; Ferda Ofli; Muhammad Imran
- Reference count: 40
- Primary result: Proposes QSF Learning with RAG for multi-label crisis tweet classification; achieves up to 15% F1-score gains over baselines.

## Executive Summary
This paper introduces a fine-grained hierarchical taxonomy for classifying actionable requests and offers in crisis-related social media posts, spanning three dimensions: supplies, emergency personnel, and actions. To address the challenge of scarce labeled data, the authors propose Query-Specific Few-shot Learning (QSF Learning), which leverages retrieval-augmented generation to dynamically retrieve relevant training examples for each input message. Experiments across multiple LLM models show that QSF Learning significantly outperforms traditional prompting strategies, with consistent gains in F1-score for multi-label classification tasks and robust performance on both synthetic and real-world crisis datasets.

## Method Summary
The method employs a hierarchical taxonomy with 1,093 elements across six depth levels, organized under three root branches: supplies, actions, and emergency personnel. For classification, the authors introduce QSF Learning, which uses retrieval-augmented generation to retrieve k/2 nearest neighbor examples (by cosine similarity) and k/2 random examples from the training set for each input message. These examples are appended to the prompt alongside the taxonomy and output format instructions, and inference is run using instruction-tuned LLMs. The approach is evaluated on both synthetic tweets (1,346, 50/50 train-test split) and a real-world Hurricane Sandy dataset (300 tweets, 107/200 train-test split), with performance measured using micro and macro F1-scores.

## Key Results
- QSF Learning improves F1-scores by up to 15% over baseline prompting strategies on multi-label classification tasks.
- The approach consistently outperforms baselines across multiple LLM models, including Llama 3, Llama 3.1, Gemma 2, Mistral 7B, and GPT-4o mini.
- Performance gains are observed on both synthetic and real-world crisis datasets, demonstrating the method's robustness and effectiveness.

## Why This Works (Mechanism)
The QSF Learning approach improves classification by dynamically retrieving contextually relevant examples for each input message, ensuring that the few-shot examples provided to the LLM are tailored to the specific request or offer being classified. This dynamic retrieval, combined with random sampling for diversity, mitigates the limitations of static few-shot examples and enhances the model's ability to generalize across diverse crisis scenarios. The hierarchical taxonomy further supports fine-grained classification, enabling more precise identification of actionable content.

## Foundational Learning
- **Hierarchical taxonomy**: Organizes labels into a structured, multi-level framework to support fine-grained classification. *Why needed*: Enables precise identification of nuanced requests and offers. *Quick check*: Verify taxonomy depth and branching logic in the dataset documentation.
- **Multi-label classification**: Allows each message to be assigned multiple labels across different dimensions (e.g., supplies, actions, personnel). *Why needed*: Crisis posts often contain multiple types of actionable content. *Quick check*: Confirm output format supports multiple labels per category.
- **Retrieval-augmented generation (RAG)**: Dynamically retrieves relevant training examples for each input to improve few-shot learning. *Why needed*: Tailors examples to the specific context of each message, improving classification accuracy. *Quick check*: Inspect retrieved examples for relevance to input messages.
- **Cosine similarity embedding**: Measures similarity between input messages and training examples for retrieval. *Why needed*: Ensures retrieved examples are contextually relevant. *Quick check*: Validate embedding quality and retrieval relevance.
- **Instruction-tuned LLMs**: Models fine-tuned for following task-specific instructions, enabling better prompt compliance. *Why needed*: Improves consistency and accuracy of generated labels. *Quick check*: Test model outputs with and without instruction tuning.

## Architecture Onboarding

**Component map:**
Taxonomy -> Embedding Database -> Retrieval Engine -> Prompt Generator -> LLM -> Output Parser

**Critical path:**
Input Message -> Embedding & Retrieval -> Prompt Assembly -> LLM Inference -> Label Extraction

**Design tradeoffs:**
- Static vs. dynamic few-shot examples: QSF uses dynamic retrieval for contextual relevance, but introduces retrieval latency and dependency on embedding quality.
- Taxonomy granularity: Fine-grained labels improve precision but increase sparsity and classification difficulty.
- Retrieval balance: Combining nearest neighbors and random examples aims to maximize relevance and diversity, but optimal k is dataset-dependent.

**Failure signatures:**
- LLM outputs labels outside the taxonomy or malformed JSON: Check embedding quality and prompt structure.
- QSF underperforms baselines on some models (e.g., Gemma 2): Verify retrieval relevance and inspect few-shot example selection.
- Inconsistent performance across models: Investigate model-specific prompt sensitivity and retrieval quality.

**First experiments:**
1. Test baseline prompting (BL1-BL5) on a small subset of synthetic data to establish performance floor.
2. Implement QSF Learning with k=8 and compare against best baseline on the same subset.
3. Run ablation: QSF with only nearest neighbors vs. only random examples to assess retrieval impact.

## Open Questions the Paper Calls Out
- **Multilingual applicability**: The authors identify a key limitation as the focus solely on English-language posts, noting that expanding to multilingual datasets is necessary for future work due to the common multilingual nature of crisis communication.
- **Taxonomy evaluation**: While the taxonomy introduces valuable structure, its evaluation has primarily been qualitative. The authors suggest developing quantitative metrics to measure trade-offs between granularity and label sparsity.
- **Model optimization**: The current methodology relied exclusively on prompting strategies. The authors suggest that incorporating optimization techniques, such as fine-tuning or multi-agent frameworks, may offer additional performance gains.

## Limitations
- The primary dataset consists of 1,346 synthetic tweets generated via GPT-4o, which may not capture the full diversity and noise of real crisis communications.
- The real-world Hurricane Sandy dataset contains only 300 messages with highly imbalanced label distributions, limiting the reliability of performance estimates for rare classes.
- The taxonomy is limited to three root branches (supplies, actions, emergency personnel), potentially missing other relevant request/offer categories.

## Confidence
- **High confidence**: The proposed QSF Learning framework is technically coherent, the taxonomy is clearly defined, and the retrieval-augmented few-shot approach is valid and implementable.
- **Medium confidence**: The quantitative improvements over baselines (up to 15% F1-score gains) are plausible given the methodology, but are based on a synthetic dataset and a small, imbalanced real-world test set, limiting generalizability.
- **Medium confidence**: The observation that retrieval quality and model choice affect performance is well-supported, but the inconsistent results (e.g., Gemma 2) indicate sensitivity to implementation details and dataset characteristics.

## Next Checks
1. Test QSF Learning on a larger, more diverse real-world crisis dataset to assess robustness and generalizability beyond Hurricane Sandy.
2. Experiment with varying the number of retrieved and random examples (k) to determine optimal few-shot set size and its effect on performance stability.
3. Compare outputs against a held-out validation set with expert annotations to verify that retrieved examples are truly relevant and that model predictions align with human judgment.