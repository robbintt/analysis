---
ver: rpa2
title: 'SITA: A Framework for Structure-to-Instance Theorem Autoformalization'
arxiv_id: '2511.10356'
source_url: https://arxiv.org/abs/2511.10356
tags:
- problem
- lean
- proof
- euclideanspacer
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SITA, a framework for structure-to-instance
  theorem autoformalization that automates the generation of Lean formalizations for
  concrete optimization problems from abstract mathematical structures. Given a formalized
  abstract structure and a natural language problem description, SITA generates instance-specific
  definitions, verifies structural assumptions, and instantiates theorems through
  a pipeline integrating LLM-based generation with feedback-guided refinement.
---

# SITA: A Framework for Structure-to-Instance Theorem Autoformalization

## Quick Facts
- **arXiv ID**: 2511.10356
- **Source URL**: https://arxiv.org/abs/2511.10356
- **Reference count**: 40
- **Primary result**: 57.14% file-level success rate generating Lean formalizations for 42 optimization problems, compared to 0% for direct generation baselines

## Executive Summary
SITA introduces a novel framework for automatically generating formal mathematical proofs in Lean from natural language problem descriptions. The key innovation is structure-to-instance theorem autoformalization, where abstract mathematical structures serve as reusable templates that constrain and guide the generation of concrete problem instances. The system integrates LLM-based generation with iterative error correction and proof refinement, achieving 57.14% file-level success on optimization problems versus 0% for direct generation baselines. SITA demonstrates that decomposing formalization into template matching, instance declaration, and proof verification enables scalable automation of research-level mathematical formalization.

## Method Summary
SITA implements a three-stage pipeline: skeleton construction, error correction, and proof refinement. Given a formalized abstract structure template and natural language problem description, the system first generates a skeleton with `sorry` placeholders using one-shot LLM prompting. It then applies iterative error correction combining rule-based fixes with retrieval-augmented LLM corrections using an evolving error knowledge base. Finally, it refines proofs through whole-proof generation for each `sorry` placeholder, extracting local context and using the knowledge base for guidance. The framework uses DeepSeek-R1/V3 models and runs on Lean 4.13.0 with Mathlib4 and Optlib, supporting 42 optimization problems across five algorithm families.

## Key Results
- 57.14% file-level success rate (fully compiling Lean files) on 42 optimization problems
- 90.7% syntactic correctness rate for generated code
- 51.23% proof success rate (no remaining `sorry` placeholders)
- Ablation studies confirm iterative correction, retrieval-augmented error fixing, and proof refinement all significantly improve performance
- Outperforms direct generation baselines by 57.14 percentage points on file-level success

## Why This Works (Mechanism)

### Mechanism 1
Structure templates reduce the search space for instance formalization by constraining generation to reusable patterns. Abstract structures (D, O, C, T tuples) pre-define definitions, operations, conditions, and theorems. LLMs generate instance-specific code by aligning natural language descriptions to these modular templates rather than synthesizing from scratch. Instance declarations then certify compatibility between concrete and abstract components. The core assumption is that the target problem admits representation as an instantiation of a known mathematical structure.

### Mechanism 2
Retrieval-augmented error correction with a self-updating knowledge base improves syntactic correctness through accumulated pattern matching. A hybrid error fix combines rule-based static corrections for syntactic issues and LLM-driven corrections guided by querying an error knowledge base K with similar past errors. Successful fixes update K, creating compound learning across iterations. The core assumption is that Lean error messages map to reusable correction patterns; errors are not fundamentally novel.

### Mechanism 3
Whole-proof generation with context extraction outperforms tactic-by-tactic synthesis for verifying structural assumptions. For each `sorry` placeholder, the system extracts local hypotheses, definitions, and context, then prompts the LLM for a complete proof term. The error knowledge base guides retries. This targets the simpler subtask of verifying instance satisfies abstract conditions rather than proving general theorems. The core assumption is that structural assumption verification is intrinsically easier than theorem proving from first principles.

## Foundational Learning

- **Lean type classes and instance declarations**: Understanding Lean's typeclass mechanism is essential for linking concrete instance definitions to abstract structures. Without this, you cannot debug linking errors or extend the framework. *Quick check*: Given a Lean class `composite_pro (f h : E → R)`, how would you declare an instance certifying that `Lasso_pro` satisfies this interface?

- **Mathematical structure formalism (D, O, C, T)**: The framework operationalizes this decomposition; understanding what goes into Definitions, Operations, Conditions, and Theorems is prerequisite to designing new abstract structures or diagnosing where instantiation fails. *Quick check*: For gradient descent on ridge regression, what would be the C (conditions) component that must be verified before applying convergence theorems?

- **Retrieval-augmented generation for code correction**: The error knowledge base is central to iterative refinement. Understanding how to construct retrieval queries from compiler errors and how to structure K entries determines whether the system scales. *Quick check*: If Lean reports "failed to synthesize HSub," what information should be stored in K to help future corrections?

## Architecture Onboarding

- **Component map**: Input layer (formalized template + problem description) → Skeleton construction (LLM one-shot prompting) → Error fix layer (rule-based → Lean type-check → K retrieval → LLM fixer) → Proof refinement (context extraction → whole-proof generation) → Output layer (compile clean file or fallback)

- **Critical path**: Skeleton construction → First error fix pass (dominates 56.5% of runtime) → Proof refinement (22.5% runtime). If skeleton generation fails structurally, downstream correction cannot recover.

- **Design tradeoffs**: Whole-proof vs stepwise (SITA chooses whole-proof for simplicity, trading fine-grained control); Template-based vs free generation (templates constrain search but require upfront formalization effort; free generation fails at 0% file-level success); Retry limits (3 correction attempts balance quality vs cost).

- **Failure signatures**: Type mismatch in matrix-vector operations; Unsynthesized implicit arguments (Huber loss δ parameter); Complex type transformations (matrix to continuous linear map); Long-chain reasoning in proofs (KL property, uniqueness arguments).

- **First 3 experiments**: 1) Run SITA on a single optimization problem (e.g., Lasso) with verbose logging to trace template matching, generated skeleton, error sequence, K retrieval hits. 2) Ablate the error knowledge base: Run with empty K and compare syntactic correctness rates against full K on a 5-problem subset. 3) Extend to a new algorithm class not in Optlib: Formalize a simple momentum variant's abstract structure, then apply SITA to 3 instances.

## Open Questions the Paper Calls Out

- **Can SITA's paradigm generalize beyond optimization?**: While current experiments focus on optimization, the framework is not domain-specific and can extend to other mathematical areas where formal abstraction and reusable operational structure play central roles. All experiments use only optimization problems, so generalization remains untested.

- **What improvements could raise proof completion rates?**: Proof success rates vary from 20.00% (ADMM) to 63.28% (Nesterov), with overall 51.23%, indicating proof synthesis remains the primary bottleneck. The paper attributes failures to "intricate reasoning steps" but doesn't propose targeted solutions for complex lemmas like KL property verification.

- **How to handle Lean's strict type coercion requirements?**: Common human workarounds involve explicit type conversions using let bindings. The error knowledge base still fails on type mismatches like BinaryExpClass where vector addition types don't unify, requiring not just Mathlib familiarity but thorough understanding of possible error messages.

## Limitations

- Template coverage poses the primary scalability bottleneck—the framework requires pre-formalized abstract structures matching the target problem's mathematical framework.
- The error knowledge base K may accumulate superficial patterns rather than deep mathematical understanding, potentially failing on genuinely novel error types.
- Evaluation focuses on optimization problems with well-established formal structures, limiting generalizability to domains with less standardized mathematical frameworks.

## Confidence

- **High confidence**: Core mechanism of using structured templates to constrain generation and the iterative error correction pipeline with retrieval augmentation. Ablation studies directly demonstrate these components' effectiveness.
- **Medium confidence**: Claim that SITA can scale to research-level formalization across diverse mathematical domains. While successful on 42 optimization problems, this represents a narrow mathematical subdomain.
- **Low confidence**: System's ability to handle genuinely novel mathematical structures not present in the template library. The paper doesn't evaluate performance when templates must be created from scratch.

## Next Checks

1. **Template Generalization Test**: Apply SITA to 10 optimization problems from a different mathematical domain (e.g., optimal control or stochastic optimization) where template coverage is partial. Measure performance degradation and identify specific failure modes when templates don't fully match problem structure.

2. **Knowledge Base Evolution Study**: Track K's growth over multiple correction iterations across all 42 problems. Analyze whether new error patterns continue emerging or if K reaches saturation. Measure correction success rates for errors seen vs unseen during training to quantify K's learning capacity.

3. **Stepwise vs Whole-Proof Comparison**: Implement a stepwise proof generation variant alongside SITA's whole-proof approach. Run both on a subset of problems requiring long-chain proofs (e.g., KL property verification). Compare proof success rates and generation efficiency to validate the claimed tradeoff.