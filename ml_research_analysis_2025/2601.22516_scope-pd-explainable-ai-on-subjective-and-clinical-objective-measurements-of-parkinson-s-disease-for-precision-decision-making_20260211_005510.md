---
ver: rpa2
title: 'SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements
  of Parkinson''s Disease for Precision Decision-Making'
arxiv_id: '2601.22516'
source_url: https://arxiv.org/abs/2601.22516
tags:
- features
- subjective
- objective
- data
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study develops SCOPE-PD, an explainable AI framework integrating\
  \ subjective and objective clinical data to improve Parkinson\u2019s disease (PD)\
  \ diagnosis. Machine learning models were trained and compared using data from the\
  \ Parkinson\u2019s Progression Markers Initiative (PPMI), combining patient-reported\
  \ outcomes and expert-assessed motor/cognitive measures."
---

# SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making

## Quick Facts
- arXiv ID: 2601.22516
- Source URL: https://arxiv.org/abs/2601.22516
- Reference count: 30
- Primary result: Multimodal AI model (RF) achieves 98.66% accuracy for PD diagnosis, with tremor, bradykinesia, and facial expression as top features

## Executive Summary
This study develops SCOPE-PD, an explainable AI framework integrating subjective and objective clinical data to improve Parkinson's disease (PD) diagnosis. Machine learning models were trained and compared using data from the Parkinson's Progression Markers Initiative (PPMI), combining patient-reported outcomes and expert-assessed motor/cognitive measures. Random Forest achieved the highest accuracy of 98.66% when both data types were combined. SHAP-based analysis revealed that tremor, bradykinesia, and facial expression were the top three most influential features in PD prediction, providing interpretable, patient-specific risk assessments. The results demonstrate that multimodal integration enhances diagnostic accuracy while model interpretability fosters clinical trust and supports personalized decision-making in neurodegenerative disease care.

## Method Summary
The SCOPE-PD framework trains binary classifiers (LR, SVM, KNN, RF, XGBoost) on baseline PPMI data to distinguish PD from healthy controls. Data from 15 clinical assessments were preprocessed, normalized to [0,1], and combined into 148 features. Models were tuned via 5-fold GridSearchCV with F1 optimization, using class weighting to handle imbalance. The best-performing Random Forest model (98.66% accuracy) was analyzed with SHAP TreeExplainer to identify top predictive features and generate patient-specific explanations.

## Key Results
- Random Forest achieved 98.66% accuracy (F1=0.9917±0.0055) on combined subjective and objective features
- SHAP analysis identified tremor, bradykinesia, and facial expression as the three most important features
- Multimodal integration outperformed single-modality models, demonstrating the value of combining patient-reported and clinical assessment data
- Model interpretability through SHAP enables transparent, clinically actionable explanations for individual patient predictions

## Why This Works (Mechanism)
The framework succeeds by leveraging complementary information from multiple data sources: subjective patient-reported outcomes capture lived experience while objective clinical assessments provide standardized measurements of motor and cognitive function. Random Forest effectively handles the high-dimensional feature space and non-linear relationships between symptoms and diagnosis. SHAP interpretability bridges the gap between complex ensemble models and clinical understanding, revealing which symptoms drive predictions and building trust in the AI system.

## Foundational Learning
- Concept: Cross-Validation and Data Leakage
  - Why needed here: The paper uses 5-fold stratified cross-validation to report model performance. Understanding this is critical to assess whether the high accuracy (98.66%) is a robust estimate or a result of overfitting.
  - Quick check question: How does the paper prevent data from the same patient from appearing in both the training and validation sets of a single fold?

- Concept: Imbalanced Data Handling
  - Why needed here: The PD and Healthy Control cohorts are not equally represented (e.g., 1,050 PD vs 253 HC in subjective tests). The paper addresses this via class weighting.
  - Quick check question: Why did the authors choose native class weighting (`class_weight='balanced'`, `scale_pos_weight`) over techniques like SMOTE?

- Concept: SHAP (SHapley Additive exPlanations) Values
  - Why needed here: This is the core interpretability tool used to explain the model's predictions both globally (top features) and locally (for an individual patient).
  - Quick check question: In a local SHAP force plot for a PD patient, what does a positive attribution value for the "tremor" feature indicate?

## Architecture Onboarding
- Component map: Data Ingestion & Preprocessing -> Normalization -> Model Training Pipeline -> Imbalance Handler -> Explainability Engine -> Output Module
- Critical path: The pipeline must run in sequence: **Preprocessing -> Model Training (RF) -> Explainability (SHAP)**. Skipping preprocessing will break the model due to inconsistent data scales and missing values. Applying SHAP to a model other than the final tuned Random Forest will not yield the reported feature importances.
- Design tradeoffs:
  - Baseline-only data vs. Longitudinal data: The authors chose to use only baseline data to ensure data consistency across modalities, sacrificing the ability to model disease progression.
  - Class weighting vs. SMOTE: The authors chose class weighting to avoid introducing potential biases from synthetic samples, trading off the possibility of better capturing the minority class structure.
  - Model Complexity vs. Interpretability: While ensemble models like RF and XGBoost are complex, the authors chose to apply SHAP *after* model selection, prioritizing high performance first and then using a post-hoc method for interpretability.
- Failure signatures:
  - Data Mismatch Error: Features from subjective and objective tests not aligning for a given participant ID during the "combined" dataset creation.
  - All-Zero SHAP Values: Indicates the `TreeExplainer` was likely applied to a model other than the trained Random Forest or that the input data was not properly scaled.
  - Perfect 1.0 Accuracy: Would indicate a critical data leakage error, such as a feature directly encoding the diagnosis label being inadvertently included.
- First 3 experiments:
  1. Reproduce Baseline Performance: Load the PPMI data, preprocess according to section 2.2, and train a Random Forest model on the combined features. Verify that the accuracy is in the range of 98.66% using the specified 5-fold stratified cross-validation.
  2. Ablation on Feature Modality: Train and evaluate the Random Forest model separately on (a) only subjective features and (b) only objective features. Compare the results to the combined model to quantify the performance gain from multimodal integration.
  3. Generate SHAP Explanations: Apply `shap.TreeExplainer` to the trained combined Random Forest model. Generate both a global summary plot to confirm tremor, bradykinesia, and facial expression are top features, and a local force plot for a specific test patient to verify the patient-specific explanations.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the SCOPE-PD framework maintain its diagnostic accuracy and feature importance rankings when validated on independent, multi-site cohorts such as PDBP or BioFIND?
- Basis in paper: [explicit] The authors state, "The absence of external validation datasets limits the ability to fully assess robustness and real-world applicability. Future efforts will incorporate multi-site data from PDBP and BioFIND for external validation."
- Why unresolved: The current 98.66% accuracy is derived solely from the PPMI dataset; without external validation, the model's generalizability to diverse clinical settings remains unproven.
- What evidence would resolve it: Performance metrics (e.g., AUC, F1-score) and SHAP feature rankings generated by applying the trained model to the PDBP and BioFIND datasets without retraining.

### Open Question 2
- Question: Can the integration of genetic markers (e.g., GBA, SNCA) and MRI/fMRI imaging data improve the predictive performance of the SCOPE-PD framework beyond the current 98.66% accuracy achieved with clinical assessments?
- Basis in paper: [explicit] The paper notes, "An expanded version of SCOPE-PD will integrate genetic (e.g., GBA, SNCA) and MRI/fMRI imaging modalities... to enable a full multimodal diagnostic framework."
- Why unresolved: It is unclear if biological and neuroimaging markers provide significant incremental value over the highly accurate subjective and objective clinical features currently utilized.
- What evidence would resolve it: A comparative study showing a statistically significant increase in classification metrics or enhanced discriminative power for difficult cases when genetic and imaging features are added to the model.

### Open Question 3
- Question: Can the SCOPE-PD framework be successfully adapted for longitudinal analysis to predict individual disease progression trajectories using follow-up visit data?
- Basis in paper: [explicit] The authors acknowledge, "Only baseline data (first visit) were used... and progression modeling was beyond the current scope. Incorporating longitudinal trajectories from follow-up visits is a next step..."
- Why unresolved: The current model is cross-sectional (baseline only) and cannot capture temporal changes in symptoms or predict the rate of neurodegeneration over time.
- What evidence would resolve it: Development of a time-aware model variant (e.g., using recurrent neural networks or survival analysis) that accurately forecasts changes in MDS-UPDRS scores or disease stages over multiple years.

## Limitations
- The study uses only baseline data, preventing analysis of disease progression over time
- Results are derived from a single dataset (PPMI) without external validation
- Missing hyperparameter grids and random seeds prevent exact numerical replication
- The framework focuses on PD vs HC classification, not disease staging or progression prediction

## Confidence
- High Confidence: The core methodology (multimodal data integration, Random Forest with SHAP interpretability) is clearly specified and the reported feature importances (tremor, bradykinesia, facial expression) align with clinical expectations for PD.
- Medium Confidence: The reported performance (98.66% accuracy) is highly specific but may be an optimistic estimate due to potential unreported hyperparameters or data leakage. The decision to use class weighting over SMOTE is explicitly stated but its impact on minority class performance is not fully characterized.
- Low Confidence: Without the exact hyperparameter grids and random seeds, the precise numerical results (e.g., the F1-score of 0.9917±0.0055) cannot be guaranteed to be reproducible.

## Next Checks
1. External Validation: Apply the trained model to an independent PD dataset (e.g., from a different clinical trial or hospital) to test the robustness of the 98.66% accuracy claim and the stability of the top SHAP features.
2. Sensitivity Analysis: Perform an ablation study on the class imbalance handling method. Compare the current results using class weighting against a model trained with SMOTE to quantify any performance differences and assess the impact of synthetic data on interpretability.
3. Longitudinal Extension: Re-run the analysis using multiple timepoints of data to evaluate if the model can learn and predict PD progression, addressing the current limitation of using only baseline measurements.