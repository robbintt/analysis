---
ver: rpa2
title: Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction
arxiv_id: '2507.23021'
source_url: https://arxiv.org/abs/2507.23021
tags:
- human
- visual
- scanpath
- scandiff
- scanpaths
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ScanDiff, a novel diffusion-based architecture
  for scanpath prediction that explicitly models the stochastic nature of human visual
  attention. By combining diffusion models with Vision Transformers and incorporating
  textual conditioning, ScanDiff generates diverse and realistic gaze trajectories
  that better capture the variability inherent in human visual exploration.
---

# Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction

## Quick Facts
- arXiv ID: 2507.23021
- Source URL: https://arxiv.org/abs/2507.23021
- Reference count: 40
- Introduces ScanDiff, a diffusion-based architecture achieving state-of-the-art scanpath prediction across free-viewing and visual search tasks

## Executive Summary
This paper presents ScanDiff, a novel diffusion-based architecture for predicting human gaze scanpaths that explicitly models the stochastic nature of visual attention. By combining diffusion models with Vision Transformers and incorporating textual conditioning, ScanDiff generates diverse and realistic gaze trajectories that better capture the variability inherent in human visual exploration. The model achieves state-of-the-art performance on benchmark datasets including COCO-FreeView, MIT1003, and COCO-Search18, outperforming existing methods in both traditional similarity metrics and novel diversity-aware measures.

## Method Summary
ScanDiff predicts human gaze scanpaths using a diffusion model conditioned on both image and task information. The architecture employs DINOv2 to extract visual features and CLIP for textual task encoding, which are fused through cross-attention. A 6-layer Transformer encoder with cross-attention learns to denoise scanpath trajectories through iterative steps, while a length prediction module enables variable-length outputs. The model is trained on combined COCO-FreeView and MIT1003 datasets for free-viewing tasks and COCO-Search18 for visual search, using a four-component loss function that balances reconstruction accuracy with alignment to human consistency patterns.

## Key Results
- Achieves state-of-the-art KL divergence scores across COCO-FreeView, MIT1003, and COCO-Search18 datasets
- Generates more diverse scanpaths (highest RSS scores) while maintaining high accuracy in spatial and temporal prediction
- CLIP text encoder with cross-attention fusion outperforms alternatives including RoBERTa and direct concatenation
- Length prediction module successfully captures variable fixation counts matching human behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models enable diverse scanpath generation by learning to reverse a gradual noising process, capturing the inherent stochasticity of human gaze.
- Mechanism: Starting from Gaussian noise z_T ~ N(0, I), the model iteratively denoises through T timesteps, conditioned on image and task. The stochastic sampling process naturally produces varied outputs from different noise seeds, modeling the observation that "the decision of where to look next at any given moment is neither entirely deterministic nor completely random."
- Core assumption: Human scanpath variability follows a learnable distribution that can be approximated through progressive denoising.
- Evidence anchors: [abstract]: "explicitly models scanpath variability by leveraging the stochastic nature of diffusion models"; [Section 3.1]: "we propose a non-autoregressive approach to generate scanpath trajectories by learning a diffusion model ϕ_θ"; [corpus]: Weak direct validation; corpus papers focus on scanpath applications rather than diffusion architectures.
- Break condition: If T is too small (<200), noise patterns dominate; if too large (>1500), overfitting to noise degrades performance (see Table 4 ablation).

### Mechanism 2
- Claim: Cross-attention fusion of visual and textual features enables unified task adaptation without architectural changes.
- Mechanism: Visual features from DINOv2 (v(I) ∈ R^(h×w×768)) and textual features from CLIP (ψ(c) ∈ R^512) are projected to a joint d-dimensional space and combined via multi-head cross-attention rather than direct concatenation. This allows "dynamic modulation of the interaction between gaze dynamics and visual-semantic information."
- Core assumption: Vision-language pre-training (CLIP) provides semantic alignment that pure language models (RoBERTa) cannot match.
- Evidence anchors: [Section 3.2.2]: "combining z_t and the visual-semantic features V_joint only in the cross-attention layer"; [Table 3]: CLIP text encoder outperforms RoBERTa across all metrics; [corpus]: SemanticScanpath paper similarly combines gaze with language for HRI, suggesting multimodal grounding is broadly useful.
- Break condition: Using concatenation instead of cross-attention degrades performance (Table 8: MM rises from 0.078 to 0.108 on COCO-FreeView).

### Mechanism 3
- Claim: Length prediction module enables variable-length scanpaths that better match human fixation count variability.
- Mechanism: A linear function l_θ predicts validity probability û_i for each token; valid consecutive tokens determine final scanpath length. This addresses the limitation that "existing works... typically produce fixed-length scanpaths."
- Core assumption: Scanpath length is predictable from multimodal context and correlates with viewing task difficulty.
- Evidence anchors: [Section 3.2.2]: "we introduce a length prediction module... allowing the model to flexibly adapt to diverse visual search objectives"; [Table 5]: ScanDiff achieves highest RSS (coverage of human variability) across all datasets; [corpus]: No direct validation; variable-length generation remains underexplored in related work.
- Break condition: Maximum length hyperparameter (set to 16) bounds predictions; may truncate longer human scanpaths.

## Foundational Learning

- Concept: **Diffusion probabilistic models (forward/reverse processes)**
  - Why needed here: Understanding how gradual noise addition (Eq. 1) and learned denoising (Eq. 2) enable generative sampling is essential for debugging convergence issues.
  - Quick check question: Can you explain why the reverse process requires learning μ_θ and Σ_θ while the forward process uses fixed β_t?

- Concept: **Vision Transformer patch embeddings and positional encoding**
  - Why needed here: DINOv2 outputs 37×37 patches at 518×518 resolution; understanding spatial token structure is critical for cross-attention debugging.
  - Quick check question: How does patch size (14×14 for ViT-B/14) affect the spatial resolution of the conditioning signal?

- Concept: **Cross-attention for multimodal fusion**
  - Why needed here: Unlike concatenation, cross-attention allows query-key matching between scanpath tokens and image patches, enabling selective grounding.
  - Quick check question: What happens to attention patterns if visual and textual features occupy different magnitude ranges?

## Architecture Onboarding

- Component map: Image I → DINOv2 ViT-B/14 → v(I) ∈ R^(37×37×768) → Linear projections → V_joint ∈ R^(1369×512); Task c → CLIP ViT-B/32 → ψ(c) ∈ R^512 → Linear projections → V_joint; V_joint → 6-layer Transformer with cross-attention → γ_θ, l_θ; z_T ~ N(0,I) → Iterative denoising → ŝ, û

- Critical path: Ground-truth scanpath s → g_θ embedding → z_0 → Forward diffusion corrupts z_0 → z_t at timestep t → Cross-attention fuses z_t with V_joint → Transformer outputs z̃_0 → decode to ŝ and validity scores → Losses guide parameter updates

- Design tradeoffs: DINOv2 vs. CLIP visual encoder: DINOv2 better overall; CLIP stronger on SemSS (semantic alignment); T=1000 balances quality vs. inference speed; T=200 faster but degrades SS metrics; Cross-attention vs. concatenation: Cross-attention improves semantic alignment at cost of complexity

- Failure signatures: Deterministic outputs (identical scanpaths across samples): Check that z_T sampling uses different random seeds; Mode collapse (low diversity): L_T loss may be disabled; verify inclusion; Poor task adaptation: Verify CLIP text encoder, not RoBERTa; check task string formatting (empty string for free-viewing); Length mismatch: L_val loss drives validity prediction; check BCE computation

- First 3 experiments: 1) Sanity check: Generate multiple scanpaths for same image with different z_T samples; verify visual diversity; 2) Ablation: Disable L_T loss on COCO-Search18; expect MM degradation (0.048→0.058 per Table 4); 3) Cross-attention vs. concatenation: Run both conditioning modes on COCO-FreeView; expect MM gap (0.078 vs. 0.108)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can diffusion-based scanpath models capture individual-specific gaze patterns and traits, rather than just population-level variability?
- Basis in paper: [explicit] The paper notes that variability "reveals individual idiosyncrasies, which are significant in clinical and psychological studies" and that randomness reflects factors "at both the individual and group levels," but the model generates population-level diverse outputs without individual conditioning.
- Why unresolved: The current model samples from a learned distribution without mechanisms to bind generated scanpaths to specific individuals or trait profiles.
- What evidence would resolve it: Experiments showing that conditioning the model on user identifiers or learned trait embeddings yields scanpaths that match specific individuals' gaze patterns more closely than unconditioned sampling.

### Open Question 2
- Question: How can the computational cost of iterative diffusion denoising be reduced for real-time or interactive scanpath prediction applications?
- Basis in paper: [inferred] The model uses T=1000 diffusion steps, and while the ablation study examines varying timesteps (200-1500), it does not address inference speed or feasibility for latency-sensitive applications like autonomous systems or human-computer interaction.
- Why unresolved: No analysis of inference time or acceleration techniques is provided, despite targeting applications requiring responsive gaze prediction.
- What evidence would resolve it: Benchmarks of inference latency across different timestep counts and comparisons with accelerated sampling methods (e.g., DDIM, consistency models).

### Open Question 3
- Question: What is the optimal balance between scanpath diversity and accuracy when diffusion models generate variable-length sequences?
- Basis in paper: [explicit] The paper introduces a novel diversity-aware metric (DSS) and observes that "commonly used metrics such as MM, SM, and SS tend to reward predictions that closely match an aggregated ground truth, thus favoring models that generate a single representative scanpath," yet does not systematically analyze the trade-off between diversity and accuracy.
- Why unresolved: While the results show ScanDiff performs well on both traditional and diversity metrics, the relationship between these objectives and whether they can be jointly optimized remains unclear.
- What evidence would resolve it: A controlled study varying diffusion guidance or temperature parameters to generate scanpaths at different diversity levels, then measuring how accuracy metrics change as diversity increases.

## Limitations
- Limited gaze data scale with only ~7K images and 20 scanpaths per image, constraining generalization
- Fixed maximum length of 16 fixations may truncate longer human scanpaths
- Diffusion sampling requires T=1000 steps, creating computational cost for real-time applications

## Confidence
- High confidence (90-95%): Diffusion mechanism enables stochastic scanpath generation; Cross-attention fusion superiority; Length prediction utility
- Medium confidence (70-85%): State-of-the-art performance claims; Diversity-accuracy tradeoff balance
- Low confidence (30-60%): Generalization to new tasks beyond tested scenarios

## Next Checks
1. **Diversity verification**: Generate 100 scanpaths for a single image and compute pair-wise SS/SM scores. If SS(sg, sg) approaches SS(sh, sh), the model is collapsing to deterministic outputs despite claims of stochasticity.

2. **Ablation replication**: Disable L_T loss on COCO-Search18 and measure MM degradation. Table 4 reports increase from 0.048 to 0.058; replication confirms alignment preservation is critical for task adaptation.

3. **Cross-attention necessity**: Run identical conditioning using concatenation instead of cross-attention on COCO-FreeView. Expect MM increase from 0.078 to 0.108 (Table 8), validating the architectural choice.