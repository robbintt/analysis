---
ver: rpa2
title: 'Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation'
arxiv_id: '2510.05532'
source_url: https://arxiv.org/abs/2510.05532
tags:
- diffusion
- teamwork
- image
- intrinsic
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Teamwork addresses the problem of expanding the number of input
  and output channels in pretrained diffusion models for graphics applications like
  SVBRDF estimation and intrinsic image decomposition, where current solutions are
  often application-specific and difficult to adapt. The core method introduces a
  flexible and efficient unified framework that achieves channel expansion without
  altering the pretrained diffusion model architecture by coordinating and adapting
  multiple instances of the base diffusion model (teammates).
---

# Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation

## Quick Facts
- arXiv ID: 2510.05532
- Source URL: https://arxiv.org/abs/2510.05532
- Authors: Sam Sartor; Pieter Peers
- Reference count: 7
- Primary result: Achieves competitive or superior performance across multiple graphics tasks (inpainting, SVBRDF estimation, intrinsic decomposition) by expanding diffusion model channels via coordinated multiple model instances

## Executive Summary
Teamwork addresses the challenge of expanding input/output channels in pretrained diffusion models for graphics applications like SVBRDF estimation and intrinsic image decomposition. Current solutions are often application-specific and difficult to adapt, requiring bespoke architectures for each task. The core method introduces a flexible and efficient unified framework that achieves channel expansion without altering the pretrained diffusion model architecture by coordinating and adapting multiple instances of the base diffusion model (teammates). Teamwork employs a novel variation of Low Rank-Adaptation (LoRA) to jointly address both adaptation and coordination between different teammates, supporting dynamic (de)activation of teammates.

The primary results demonstrate that Teamwork performs similarly or better than prior work that relies on bespoke solutions across various graphics tasks. For inpainting, Teamwork achieves a SI-FID of 13.236 and CLIP IQA of 0.490 on the PixelProse dataset. For SVBRDF estimation under colocated lighting, Teamwork achieves a render RMSE of 0.204 with 0.057 diffuse albedo error. For intrinsic image decomposition, Teamwork achieves RMSE values of 0.116 for diffuse albedo and 0.121 for normals on the InteriorVerse test set. Teamwork also shows better generalization capabilities on HyperSim and Infinigen datasets compared to competing methods, with LPIPS scores of 0.356 for diffuse albedo on HyperSim. The method is computationally efficient, with linear complexity in the number of teammates versus quadratic for competing approaches like Joint Attention.

## Method Summary
Teamwork modifies LoRA for T teammates by constructing a dense coordination matrix ΔW = [A₁…A_T]ᵀ[B₁…B_T] where A_i∈ℝ^{m×r} and B_i∈ℝ^{r×n} (rank r, default 16). Unlike standard LoRA's block-diagonal structure, this creates dense off-diagonal blocks allowing cross-channel coordination. Input-teammates receive noise-free conditioning images at every diffusion step and function as feature extractors rather than diffusion processes. The method maintains computational efficiency by keeping coordination in linear layers rather than attention layers, achieving O(T) complexity versus O(T²) for competing approaches. Training uses Prodigy optimizer with 16× gradient accumulation, cosine learning rate schedule, and unmaterialized weights to preserve efficiency.

## Key Results
- Achieves SI-FID of 13.236 and CLIP IQA of 0.490 on PixelProse inpainting dataset
- For SVBRDF estimation, achieves render RMSE of 0.204 with 0.057 diffuse albedo error under colocated lighting
- For intrinsic image decomposition, achieves RMSE of 0.116 (diffuse albedo) and 0.121 (normals) on InteriorVerse test set
- Shows better generalization on HyperSim and Infinigen datasets with LPIPS of 0.356 for diffuse albedo on HyperSim

## Why This Works (Mechanism)

### Mechanism 1
Standard LoRA uses block-diagonal weight offsets, preventing coordination between different output channels. Teamwork constructs a dense offset matrix ΔW using concatenated low-rank factors [A₁; ...; A_T] and [B₁, ..., B_T]. This structure creates non-zero off-diagonal blocks, mathematically allowing features from one teammate (e.g., surface normals) to influence the linear computation of another (e.g., albedo). The core assumption is that cross-channel dependencies can be captured in a low-rank subspace.

### Mechanism 2
Teamwork maintains computational efficiency by keeping coordination logic within linear layers rather than attention layers. Unlike Joint Attention which concatenates sequences across teammates (incurring quadratic self-attention cost O(T²)), Teamwork performs coordination via matrix multiplication in linear layers. The paper argues this is the "dual" of Joint Attention: sharing features via linear weights allows coordination without modifying the attention mechanism, leveraging the fact that linear layers are 4× more prevalent than attention layers in diffusion models.

### Mechanism 3
Input channel expansion functions by treating input-teammates as feature extractors (vision models) rather than diffusion processes. Input-teammates receive noise-free conditioning images at every diffusion step and do not output denoised images but instead propagate features forward. The shared ΔW allows gradients from the diffusion loss on output-teammates to adapt input-teammates to extract relevant conditioning features. The core assumption is that noise-free inputs provide stable, useful features at every step regardless of output noise level.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** Teamwork is an architectural extension of LoRA; understanding W + ΔW = W + AB is required to understand how teammates coordinate
  - **Quick check question:** Can you explain why standard LoRA applied independently to multiple models would fail to coordinate them (hint: block diagonal matrix)?

- **Concept: Diffusion Model Inversion/Expansion**
  - **Why needed here:** The paper frames graphics tasks as channel expansion problems (RGB → multi-channel SVBRDF/intrinsics)
  - **Quick check question:** How does a standard pretrained diffusion model (3 channels) conceptually generate a 12-channel SVBRDF map?

- **Concept: Batch Processing vs. Instance Coordination**
  - **Why needed here:** The method relies on a "Batch Trick" to run multiple teammates simultaneously while forcing interaction
  - **Quick check question:** What is the difference between processing a batch of independent images and processing a "team" of correlated channels in a single forward pass?

## Architecture Onboarding

- **Component map:** Base Model (frozen pretrained diffusion) -> Teammates (T instances running in parallel) -> Coordination Adapter (shared Low-Rank matrices ΔW) -> Controller (logic to mask/drop teammates)

- **Critical path:**
  1. Concatenate inputs (Image, Mask, etc.) into a batch of size T
  2. Pass through frozen Linear Layers W (block-diagonal interaction)
  3. Pass through Teamwork LoRA (ΔW) which mixes features across batch dimension (dense interaction)
  4. Compute loss only on output-teammates (input-teammates have no loss)

- **Design tradeoffs:**
  - Do not materialize W + ΔW into a single weight matrix (degrades complexity from O(T) to O(T²) and increases memory)
  - Higher rank r allows better coordination but increases parameters; larger teams T increase VRAM linearly

- **Failure signatures:**
  - Ghosting/Incoherence: Outputs look correct individually but don't align spatially (indicates coordination failed or rank is too low)
  - Mode Collapse: All teammates output the same channel (possible if A matrices are not distinct enough)

- **First 3 experiments:**
  1. **Sanity Check (Inpainting):** Implement 2 teammates (Image, Mask) → 1 output. Verify the model learns to copy unmasked regions and denoise masked regions
  2. **Ablation on Coordination:** Compare "No Coordination" (block-diagonal LoRA) vs. Teamwork on a multi-channel task (e.g., SVBRDF) to measure the quantitative drop in coherence
  3. **Dynamic Activation:** Train with dropout on teammates and verify the model can still perform inference when specific input channels (e.g., depth) are missing

## Open Questions the Paper Calls Out
None

## Limitations
- The low-rank coordination assumption may not hold for tasks requiring complex cross-channel dependencies, potentially limiting performance on highly intricate graphics problems
- The paper does not thoroughly investigate the sensitivity to rank hyperparameter r, which could significantly affect coordination quality versus parameter efficiency
- The dynamic activation feature, while promising, is only briefly explored and may introduce training instability if not carefully implemented

## Confidence

- **High confidence:** The core mechanism of using shared low-rank matrices for cross-channel coordination is mathematically sound and well-explained. The quantitative results showing competitive or superior performance across multiple graphics tasks are well-supported by the reported metrics.
- **Medium confidence:** The claim about computational efficiency being linear in teammates is supported by the architecture description, but practical implications depend heavily on implementation details (e.g., memory management for unmaterialized weights). The generalization results on HyperSim and Infinigen are promising but based on limited comparisons.
- **Low confidence:** The dynamic activation feature's effectiveness and practical utility are not thoroughly validated beyond a brief mention. The paper does not provide systematic ablation studies on rank sensitivity or the trade-offs between coordination fidelity and parameter efficiency.

## Next Checks

1. **Coordination ablation study:** Implement both Teamwork and a block-diagonal LoRA variant for a multi-channel task (e.g., SVBRDF) and measure the quantitative difference in cross-channel consistency metrics (e.g., LPIPS between albedo and normal maps).

2. **Rank sensitivity analysis:** Systematically vary the rank parameter r (e.g., 8, 16, 32) and measure the trade-off between coordination quality (cross-channel coherence metrics) and parameter count/memory usage.

3. **Dynamic activation robustness:** Train with different dropout rates on teammates and evaluate performance degradation when specific input channels are missing during inference, measuring both quantitative metrics and qualitative coherence.