---
ver: rpa2
title: 'Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency
  Graph'
arxiv_id: '2510.04520'
source_url: https://arxiv.org/abs/2510.04520
tags:
- lean
- aria
- mathlib
- formal
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Aria introduces a novel agentic pipeline for conjecture-level formalization
  in Lean that addresses core limitations of LLMs through a Graph-of-Thought (GoT)
  process and retrieval-augmented generation. The system recursively decomposes statements
  into a dependency graph, synthesizes definitions from grounded concepts in a bottom-up
  manner, and employs a compiler-guided reflection loop.
---

# Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph

## Quick Facts
- arXiv ID: 2510.04520
- Source URL: https://arxiv.org/abs/2510.04520
- Reference count: 40
- Aria achieves 91.6% compilation success rate and 68.5% final accuracy on ProofNet, 44.0% on FATE-X, and 42.9% on homological conjectures—surpassing all baselines.

## Executive Summary
Aria introduces a novel agentic pipeline for conjecture-level formalization in Lean that addresses core limitations of LLMs through a Graph-of-Thought (GoT) process and retrieval-augmented generation. The system recursively decomposes statements into a dependency graph, synthesizes definitions from grounded concepts in a bottom-up manner, and employs a compiler-guided reflection loop. AriaScorer, a semantic correctness checker, retrieves definitions from Mathlib for term-level grounding, enabling rigorous verification. Evaluations show Aria achieving 91.6% compilation success rate and 68.5% final accuracy on ProofNet, 44.0% on FATE-X, and 42.9% on homological conjectures—surpassing all baselines. The ablation studies confirm that the Reflection, GoT, and RAG modules are essential for success, especially on complex, research-level problems.

## Method Summary
Aria is an agentic pipeline that formalizes informal mathematical statements into Lean 4 code. It uses a Graph-of-Thought (GoT) planner to recursively decompose statements into a dependency graph, then retrieves existing Mathlib definitions for grounded concepts while synthesizing new definitions for ungrounded ones. The system employs a compiler-in-the-loop reflection process (up to 16 iterations) to ensure syntactic validity, and uses AriaScorer—a semantic correctness checker that performs term-level grounding via retrieval—to verify the final formalization. The method leverages LeanSearch for retrieval and uses the Herald dataset for term definitions, with the Sugeno fuzzy integral for scoring.

## Key Results
- Aria achieves 91.6% compilation success rate and 68.5% final accuracy on ProofNet.
- On FATE-X, Aria reaches 44.0% accuracy, significantly outperforming baselines (e.g., 3.8% for FIM).
- For 14 homological conjectures, Aria achieves 42.9% accuracy, demonstrating capability on research-level problems.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Recursive dependency decomposition appears to mitigate the failure mode where LLMs hallucinate interfaces for complex or novel concepts.
- **Mechanism:** The Graph-of-Thought (GoT) planner decomposes an informal statement into a dependency graph. It performs a top-down expansion: if a concept (node) exists in the library (Mathlib), it is grounded; if not, the planner recursively expands it into sub-concepts until they are groundable, then synthesizes them bottom-up.
- **Core assumption:** Mathematical statements possess an acyclic dependency structure that can be explicitly mapped and resolved incrementally.
- **Evidence anchors:**
  - [Section 3.1.1] describes the "conceptual dependency graph" where nodes represent concepts and edges represent dependencies.
  - [Section C.2] ablation study shows that removing GoT reduces success on conjectures from 6 to 1, causing "synthesis failure" and "interface hallucination."
  - [Corpus] Neighbors like *Graph Counselor* and *Trification* support the general efficacy of graph-based planning and tree-based strategies for complex reasoning, though they do not validate the specific Lean synthesis logic.
- **Break condition:** The mechanism likely fails if the informal statement describes a concept with circular dependencies or if the retrieval tool fails to find existing definitions for mid-level nodes, forcing an impossible synthesis.

### Mechanism 2
- **Claim:** A compiler-in-the-loop reflection loop likely enables syntactic convergence for novel definitions that the base LLM cannot generate correctly in one pass.
- **Mechanism:** The system generates a candidate definition and immediately compiles it. If compilation fails, the error message is fed back to the LLM as context for a correction attempt (up to 16 iterations). This transforms the LLM from a generator to a debugger.
- **Core assumption:** Lean compiler error messages provide sufficient signal for the LLM to identify and fix the specific syntactic or type error.
- **Evidence anchors:**
  - [Section 3.1.2] details the "compiler-in-the-loop reflection process" ensuring syntactic validity.
  - [Section C.1] shows that removing reflection drops FATE-X accuracy from 44% to 14% and Conjecture accuracy to 0%.
  - [Corpus] While not explicitly discussed in neighbors, the *SLICEMATE* paper implies the value of static analysis feedback in agentic loops.
- **Break condition:** Effectiveness degrades if error messages are cryptic or if the required fix involves a paradigm shift the LLM cannot infer from the error text alone (e.g., needing a completely different type class hierarchy).

### Mechanism 3
- **Claim:** Term-level semantic grounding (AriaScorer) reduces false positives in verification compared to surface-level text matching.
- **Mechanism:** During verification, AriaScorer does not just compare the informal text to the formal code. It retrieves the authoritative Mathlib definitions for every term used in the formal code and injects this context into the LLM prompt. This forces the evaluation to reason over the *actual* semantics of the Lean terms rather than their string literals.
- **Core assumption:** The LLM can reliably detect contradictions between the informal problem statement and the retrieved formal definitions of the terms used.
- **Evidence anchors:**
  - [Section 3.2.2] introduces "term-level retrieval and interpretation" to detect "subtle definitive discrepancies."
  - [Section 4.3.2] shows AriaScorer achieves 89.9% accuracy vs 71.0% for the baseline (LeanScorer) which lacks grounding.
  - [Corpus] The *ProofBridge* paper mentions joint embeddings for auto-formalization, but specific evidence for "definition injection during scoring" is weak or missing in the provided corpus.
- **Break condition:** The mechanism may fail if the Mathlib definition is excessively complex or relies on meta-programming that the evaluating LLM cannot parse or understand.

## Foundational Learning

- **Concept: Lean 4 Type Classes and Definitions**
  - **Why needed here:** Aria works by synthesizing definitions and theorems in Lean 4. Understanding the strict typing, type class inference, and definition syntax is required to interpret the agent's outputs and failure modes.
  - **Quick check question:** Can you distinguish between a `def` (definition) and an `axiom` in Lean, and explain why the former is preferred for auto-formalization?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The system relies on "LeanSearch" to ground concepts in Mathlib. You must understand how vector search or keyword search maps a natural language concept (e.g., "Noetherian ring") to a formal identifier.
  - **Quick check question:** How does the system handle a concept that exists in the mathematical literature but has no equivalent in the target library (Mathlib)?

- **Concept: Agentic Planning (Graph-of-Thought)**
  - **Why needed here:** Aria moves beyond single-prompt generation to a stateful graph construction. Understanding how nodes represent states and edges represent dependencies is crucial.
  - **Quick check question:** In a dependency graph for "A is a Noetherian Ring," what are the likely child nodes required before "Noetherian Ring" can be defined?

## Architecture Onboarding

- **Component map:** Input (Informal Math Statement) -> GoT Planner (Decompose to dependency graph) -> Retriever (LeanSearch for Mathlib grounding) -> Synthesizer (Generate code for ungrounded nodes) -> Reflector (Compiler-in-the-loop validation, up to 16 iterations) -> AriaScorer (Semantic verification with term-level grounding)

- **Critical path:** The **Synthesis Loop** (Synthesizer -> Compiler -> Reflector). This is the rate-limiting step because it involves iterative LLM calls and compilation checks for every new definition required by the conjecture.

- **Design tradeoffs:**
  - **Depth vs. Computation:** The GoT planner recursively expands dependencies. This ensures correctness but increases latency and API costs (average 17.7 calls per problem noted in [Section 4.2]).
  - **Strictness vs. Recall:** AriaScorer uses a threshold $\alpha$. High $\alpha$ (0.9) maximizes precision (95.5%) but lowers recall; low $\alpha$ (0) maximizes recall (96.2%) but allows more "minor inconsistencies" [Section 4.3.2].

- **Failure signatures:**
  - **Infinite Expansion:** The planner keeps decomposing a concept infinitely if it cannot find a ground truth or a stopping condition.
  - **Hallucinated Grounding:** The Retriever matches a natural language term to an incorrect Mathlib definition (e.g., matching "Field" to a specific instance rather than the general structure), leading to a type mismatch later in the compiler.

- **First 3 experiments:**
  1. **Grounding Validation:** Run the Retrieval module on a set of 10 standard mathematical concepts (e.g., "Group", "Ring", "Prime Ideal"). Verify manually if the top result matches the canonical Mathlib definition.
  2. **Reflection Loop Stress Test:** Provide the Synthesizer with an intentionally malformed definition (e.g., wrong type class parameter) and observe if the Reflector/Compiler loop successfully corrects it within 3 iterations.
  3. **Scorer Threshold Calibration:** Run AriaScorer on a batch of formalized statements with known semantic errors. Sweep the threshold $\alpha$ to find the optimal balance between Precision and Recall for your specific error profile.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Aria's pipeline be effectively integrated with automated theorem provers to achieve end-to-end resolution of research-level conjectures?
  - **Basis:** [explicit] The conclusion states that successful formalization establishes a "solid foundation for future work on automated mathematical proof at this frontier of research."
  - **Why unresolved:** The current study rigorously evaluates the auto-formalization of statements but does not implement or evaluate the subsequent step of generating proofs for these formalized conjectures.
  - **What evidence would resolve it:** A system combining Aria with a prover that successfully solves a subset of the formalized homological conjectures.

- **Open Question 2:** How can the semantic correctness of *synthesized* definitions (not present in Mathlib) be rigorously verified?
  - **Basis:** [inferred] Section 3.2.2 details AriaScorer's reliance on retrieving "authoritative definitions" from Mathlib for term-level grounding, while Section 3.1.2 describes the synthesis of novel definitions for concepts absent from the library.
  - **Why unresolved:** If a definition is synthesized (e.g., `IsNil` in Appendix A), there is no ground truth in Mathlib for AriaScorer to retrieve, potentially creating a blind spot in semantic verification.
  - **What evidence would resolve it:** An ablation study or method demonstrating how semantic fidelity is ensured for user-defined structures without existing library entries.

- **Open Question 3:** Does the Graph-of-Thought (GoT) architecture introduce unnecessary complexity for simpler, undergraduate-level problems?
  - **Basis:** [inferred] Table 5 in Appendix C shows that on the simpler FATE-H benchmark, the GoT-ablated version achieved a higher compilation success rate (95%) than the full Aria system (89%), despite lower final accuracy.
  - **Why unresolved:** While GoT is proven essential for complex conjectures, the data suggests the overhead might negatively impact syntactic success rates on standard problems where dependency decomposition is unnecessary.
  - **What evidence would resolve it:** A dynamic agent architecture that bypasses the GoT decomposition phase for low-complexity inputs to optimize efficiency and compilation rates.

## Limitations
- The core mechanism assumes mathematical concepts have an acyclic dependency structure, which is not proven for all domains.
- The effectiveness of compiler error messages as sufficient signal for the reflection loop is asserted but not empirically tested with problematic error types.
- The semantic grounding through term-level retrieval assumes the Herald dataset comprehensively covers Mathlib's informal descriptions, which may not hold for advanced or recently added definitions.

## Confidence
- **Mechanism 1 (GoT Decomposition):** Medium confidence. The ablation study provides strong evidence for its importance, but the general efficacy of graph-based planning in the corpus doesn't validate the specific Lean synthesis logic.
- **Mechanism 2 (Reflection Loop):** Medium confidence. The dramatic performance drop without reflection is compelling, but the effectiveness on various error types is not explored.
- **Mechanism 3 (AriaScorer Grounding):** Medium confidence. The accuracy improvement over LeanScorer is clear, but the "definition injection" mechanism is not explicitly discussed in the corpus, and its robustness for complex definitions is untested.
- **Overall Performance Claims:** Low confidence in absolute numbers. Without access to the exact datasets, prompts, and model versions, the 91.6%, 68.5%, 44.0%, and 42.9% figures cannot be independently verified.

## Next Checks
1. **Grounding Validation:** Run the Retrieval module on a set of 10 standard mathematical concepts (e.g., "Group", "Ring", "Prime Ideal"). Verify manually if the top result matches the canonical Mathlib definition.
2. **Reflection Loop Stress Test:** Provide the Synthesizer with an intentionally malformed definition (e.g., wrong type class parameter) and observe if the Reflector/Compiler loop successfully corrects it within 3 iterations.
3. **Scorer Threshold Calibration:** Run AriaScorer on a batch of formalized statements with known semantic errors. Sweep the threshold $\alpha$ to find the optimal balance between Precision and Recall for your specific error profile.