---
ver: rpa2
title: Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource
  Flemish?
arxiv_id: '2506.04139'
source_url: https://arxiv.org/abs/2506.04139
tags:
- valence
- pattern
- liwc
- dutch
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated lexicon-based tools (LIWC, Pattern) and Dutch-tuned
  LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, GEITje-7B-ultra) for valence analysis
  in Flemish narratives. Using 24,854 self-reported Flemish texts, lexicon methods
  achieved near-complete coverage (99.9%) with moderate correlation to human ratings
  (Pattern: Pearson r = 0.31).'
---

# Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?

## Quick Facts
- **arXiv ID**: 2506.04139
- **Source URL**: https://arxiv.org/abs/2506.04139
- **Reference count**: 12
- **Primary result**: Lexicon tools achieve 99.9% coverage with r=0.31 correlation; Dutch-tuned LLMs show higher r=0.35 on subsets but fail at 69.9-1.8% coverage

## Executive Summary
This study evaluates lexicon-based tools (LIWC, Pattern) against Dutch-tuned LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, GEITje-7B-ultra) for valence analysis in Flemish narratives. Using 24,854 self-reported Flemish texts, lexicon methods achieved near-complete coverage (99.9%) with moderate correlation to human ratings (Pattern: Pearson r = 0.31). LLMs showed higher correlations on subsets (ChocoLlama: r = 0.35) but had incomplete coverage (69.9-1.8%). The findings indicate that lexicon tools remain the gold standard for low-resource Flemish due to their reliability and coverage, despite LLMs' contextual advantages. This underscores the need for culturally tailored NLP solutions in under-resourced languages.

## Method Summary
The study compared lexicon-based sentiment analysis tools (LIWC 2015 Dutch PosEmo/NegEmo categories and Pattern.nl) against three Dutch-tuned LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, GEITje-7B-ultra) on 24,854 Flemish narratives collected via EMA app. Participants provided open-ended text responses rated on continuous scale (-50 to +50). Models were evaluated using Pearson and polyserial correlations with human ratings, plus coverage rates. Zero-shot LLM inference used standardized English prompts requesting [1-7] scale outputs. Analysis employed GPU-based inference with HuggingFace APIs.

## Key Results
- Lexicon methods achieved 99.9% coverage, Pattern showed Pearson r = 0.31 correlation with human valence ratings
- LLMs achieved 69.9-1.8% coverage with higher correlation on subsets (ChocoLlama: r = 0.35)
- Reynaerde-7B coverage dropped to 1.8% due to instruction-following failures
- Few-shot prompting degraded LLM performance (ChocoLlama r dropped from 0.35 to -0.03)

## Why This Works (Mechanism)

### Mechanism 1: Lexicon Coverage Stability
- Claim: Lexicon-based tools achieve near-universal coverage because word-matching is deterministic and format-agnostic.
- Mechanism: Dictionary lookup matches any recognized word in text → produces score regardless of syntactic coherence, text length, or semantic structure.
- Core assumption: Input texts contain at least one dictionary-matched token (held for 99.9% of corpus).
- Evidence anchors:
  - [abstract] "lexicon methods achieved near-complete coverage (99.9%)"
  - [section 6.1] "LIWC and Pattern produced scores for 24,848 texts (99.9%)"
  - [corpus] Neighbor paper on lexicon-based sentiment analysis confirms consistent coverage reliability
- Break condition: Texts with entirely novel vocabulary, heavy code-switching, or non-standard orthography.

### Mechanism 2: LLM Coverage Failure via Output Format Non-Compliance
- Claim: Dutch-tuned LLMs produce incomplete coverage because instruction-following degrades on domain-mismatched or unexpected inputs.
- Mechanism: Prompt requests `[number]` → LLM generates explanatory text or malformed output → parser rejects → counted as missing. Coverage varies: ChocoLlama 69.9%, GEITje 38%, Reynaerde 1.8%.
- Core assumption: Coverage gaps stem from output format non-compliance, not tokenization or inference failures.
- Evidence anchors:
  - [abstract] "LLMs showed higher correlations on subsets (ChocoLlama: r = 0.35) but had incomplete coverage (69.9-1.8%)"
  - [section 6.1] "LLMs' coverage gaps risk introducing selection bias"
  - [corpus] Weak direct corpus support; low-resource LLM behavior under-documented
- Break condition: Constrained decoding, grammar-based sampling, or robust output parsing.

### Mechanism 3: Domain Mismatch Reduces Valence Alignment
- Claim: Dutch-tuned LLMs underperform on informal narratives because pretraining corpora emphasize formal registers over colloquial emotional expression.
- Mechanism: Training on legal documents, web text, social media → models learn formal sentiment patterns → fail to capture implicit affective cues in spontaneous first-person narratives.
- Core assumption: Narrative-style emotional language differs systematically from available Dutch training corpora.
- Evidence anchors:
  - [section 7] "overreliance on publicly available corpora imbues LLMs with a formal, technical register that rarely features colloquialism"
  - [section 7] "colloquial and pragmatic features of Flemish underrepresented"
  - [corpus] Neighbor "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages" supports training data quality importance
- Break condition: Fine-tuning on annotated narrative corpora; data augmentation with informal Flemish texts.

## Foundational Learning

- **Concept: Polyserial Correlation**
  - Why needed here: Measures alignment between ordinal LLM outputs (1-7 scale) and continuous self-reported valence (-50 to +50).
  - Quick check question: Why is polyserial more appropriate than Pearson for comparing LLM ratings to human scores?

- **Concept: Low-Resource Language NLP**
  - Why needed here: Flemish (6.5M speakers) lacks large annotated datasets; models trained on standard Dutch or English transfer poorly.
  - Quick check question: What defines a "low-resource" language in NLP, beyond speaker population?

- **Concept: Ecological Validity in Emotion Research**
  - Why needed here: Ambulatory assessment (real-time, in-context reporting) reduces retrospective bias vs. lab surveys or social media data.
  - Quick check question: Why might self-reported valence be more reliable than third-party annotator labels?

## Architecture Onboarding

- **Component map:** Data layer (24,854 Flemish narratives with valence ratings) → Analysis layer (LIWC word counts, Pattern.nl sentiment, Dutch LLMs) → Evaluation layer (Pearson r, polyserial correlation, coverage rates)

- **Critical path:**
  1. Load text → run LIWC/Pattern → always returns score
  2. Load text + prompt → run LLM → parse for `[number]` → score or failure
  3. Correlate model scores with user valence → report r and coverage

- **Design tradeoffs:**
  - Coverage vs. accuracy: Lexicons give universal coverage (r≈0.31); LLMs give higher r (0.35) on smaller, potentially biased subsets
  - Zero-shot vs. few-shot: Paper tested both; few-shot degraded LLM performance (ChocoLlama r dropped from 0.35 to -0.03)
  - Proprietary vs. open: LIWC is closed-source; Pattern is open; LLMs vary by license

- **Failure signatures:**
  - Reynaerde: 1.8% coverage → severe instruction-following failure
  - GEITje: Rating distribution skewed toward 5 → mid-scale bias
  - LIWC/Pattern: Miss sarcasm, irony, context-dependent valence

- **First 3 experiments:**
  1. Add constrained decoding or regex-based output extraction to force valid `[X]` format from LLMs.
  2. Build hybrid: use Pattern.nl as baseline, apply LLM refinement only where confidence is high.
  3. Fine-tune smallest Dutch LLM (e.g., Reynaerde) on 500-1000 manually annotated Flemish narratives to test domain adaptation impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would hybrid approaches combining LLM outputs with lexicon-driven scores achieve better coverage and accuracy than either method alone for Flemish valence analysis?
- Basis in paper: [explicit] Authors state: "Addressing this gap requires augmenting training data with narrative-style corpora or combining LLM outputs with lexicon-driven scores to create hybrid models better suited for daily Flemish narratives."
- Why unresolved: This study only evaluated lexicon tools and LLMs separately; no hybrid approach was tested.
- What evidence would resolve it: Experiments comparing pure lexicon, pure LLM, and combined ensemble methods on the same Flemish narrative corpus with self-reported valence ground truth.

### Open Question 2
- Question: Would larger Dutch-capable models (e.g., LLaMA-2 70B) or newer architectures achieve higher coverage and correlation than the 7-8B parameter models tested?
- Basis in paper: [explicit] In Limitations: "Our evaluation of three Dutch-tuned LLMs... does not account for larger models (e.g., LLaMA-2 70B), newer architectures or hybrid approaches that may outperform current benchmarks."
- Why unresolved: Only ChocoLlama-8B, GEITje-7B, and Reynaerde-7B were evaluated; larger models were not tested due to computational constraints.
- What evidence would resolve it: Benchmarking larger Dutch-tuned or multilingual models on the same 24,854-text dataset to measure coverage rates and correlation with self-reported valence.

### Open Question 3
- Question: To what extent does the low coverage of Dutch-tuned LLMs reflect training data domain mismatch versus architectural limitations?
- Basis in paper: [inferred] The authors attribute LLM underperformance to "domain mismatch" from training on "formal, technical register" rather than everyday narratives, but do not empirically disentangle this from other factors like model size or tokenization choices.
- Why unresolved: Coverage varied dramatically (69.9% to 1.8%) across models with different architectures and training strategies; causal factors were not isolated.
- What evidence would resolve it: Ablation studies testing models fine-tuned specifically on narrative-style Flemish corpora versus formal text, controlling for architecture and size.

## Limitations
- Coverage bias from LLM failures (69.9-1.8% coverage means results based on non-representative subsets)
- Single-language focus limits generalizability to other low-resource languages
- Proprietary LIWC tool constraints prevent full methodological transparency

## Confidence
- **High confidence**: Lexicon coverage reliability (99.9%) and fundamental observation that lexicon tools remain viable for low-resource languages
- **Medium confidence**: Superiority of lexicon tools over LLMs for this specific task, though complex tradeoffs exist
- **Low confidence**: Specific reasons for LLM coverage failures and their generalizability across different contexts

## Next Checks
1. Perform systematic error analysis of LLM outputs to categorize failure types (format violations, tokenization issues, inference failures)
2. Replicate methodology on another low-resource language with available Dutch-tuned LLMs to test generalizability
3. Implement and evaluate hybrid system combining lexicon baseline with LLM refinement based on confidence metrics