---
ver: rpa2
title: Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling
  Abstract Formulation From Arithmetic Computation
arxiv_id: '2505.23701'
source_url: https://arxiv.org/abs/2505.23701
tags:
- abstraction
- computation
- symbolic
- figure
- patching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (LLMs) are commonly evaluated on math word
  problems using final-answer accuracy, which conflates two distinct sub-skills: abstract
  formulation (translating natural language into mathematical expressions) and arithmetic
  computation (executing calculations). This study disentangles these skills through
  targeted evaluation on GSM8K and SVAMP datasets.'
---

# Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation

## Quick Facts
- **arXiv ID:** 2505.23701
- **Source URL:** https://arxiv.org/abs/2505.23701
- **Reference count:** 32
- **Primary result:** Large language models show higher accuracy on abstract formulation than arithmetic computation in math word problems, suggesting poor final-answer performance often stems from computational errors rather than reasoning deficits.

## Executive Summary
This study disentangles two distinct skills in LLM math reasoning: abstract formulation (translating natural language into mathematical expressions) and arithmetic computation (executing calculations). Through targeted evaluation on GSM8K and SVAMP datasets, the authors find that without chain-of-thought prompting, models consistently perform better on abstraction than computation. Chain-of-thought primarily improves computation with limited gains in abstraction. Mechanistic interpretability reveals an "abstract-then-compute" mechanism where models first capture problem abstractions (around layers 13-14), then execute computations (around layers 15-18). Causal patching confirms these abstractions are present, transferable across symbolic/concrete forms, and composable with subsequent computation stages.

## Method Summary
The authors implement a disentangled evaluation framework for math word problems, creating symbolic variants of GSM8K and SVAMP datasets using GPT-4o-mini with generate-then-validate pipeline. They evaluate zero-shot performance with/without chain-of-thought prompting across multiple Llama-3 and Qwen2.5 models (1B-32B) using greedy decoding. For mechanistic analysis, they generate 3,600 custom 1-2 step problems from 1,200 templates and apply logit attribution and activation patching (Algorithm 1) to identify critical layers for abstraction and computation. Exact match accuracy is computed for numeric answers, with symbolic expressions evaluated by GPT-4o-mini and sympy for numeric expressions.

## Key Results
- Models consistently achieve higher accuracy on abstract formulation (60.3-90.2%) than arithmetic computation (26.3-42.6%) without chain-of-thought
- Chain-of-thought improves computation by +58.7% on average but only improves abstraction by +6.7-17.6%
- Logit attribution reveals operator tokens concentrate in later attention and MLP layers (abstraction), while operand/answer tokens appear in later MLP layers (computation)
- Activation patching shows abstraction representations are transferable and composable with computation stages

## Why This Works (Mechanism)
The paper demonstrates that LLMs employ a two-stage reasoning process for math word problems: first forming an abstract mathematical representation, then executing arithmetic computations. This sequential mechanism explains why models often fail at final answers despite correctly formulating the problem - computational errors dominate final-answer accuracy. Chain-of-thought prompting primarily aids the computation stage rather than the abstraction stage, suggesting these are distinct cognitive processes that can be targeted independently for improvement.

## Foundational Learning
- **Abstract formulation vs arithmetic computation:** Critical distinction showing models can understand problems without being able to solve them numerically. Understanding this separation helps diagnose whether reasoning or calculation errors dominate performance.
- **Chain-of-thought prompting effects:** Reveals CoT primarily helps with computation, not abstraction. This suggests CoT may not be the best intervention for improving reasoning capabilities.
- **Mechanistic interpretability methods:** Logit attribution and activation patching provide causal evidence for internal reasoning processes. These methods can validate whether models truly understand problems versus memorizing patterns.

## Architecture Onboarding
- **Component map:** Problem text -> Tokenizer -> Embedding layers -> Attention/MLP layers (L13-14: abstraction, L15-18: computation) -> Output layer -> Symbolic/Numeric evaluation
- **Critical path:** The abstract-then-compute mechanism follows a sequential path where abstraction must occur before computation, with critical transitions at layers 13-14 to 15-18
- **Design tradeoffs:** Zero-shot evaluation without CoT isolates pure reasoning ability but may underestimate model potential; mechanistic analysis uses simplified problems that may not generalize to complex reasoning
- **Failure signatures:** High abstraction accuracy but low computation accuracy indicates reasoning ability without calculation proficiency; CoT improving computation but not abstraction suggests separate mechanisms
- **First experiments:** 1) Replicate disentangled evaluation on GSM8K with symbolic variants; 2) Apply logit attribution to identify abstraction/computation layers; 3) Validate activation patching recovery scores on clean/corrupted pairs

## Open Questions the Paper Calls Out
- **Cross-linguistic generalization:** Whether the abstract-then-compute mechanism generalizes to non-English math word problems and languages with different numeral systems or syntactic structures, as the study only evaluated English datasets.
- **Scaling to larger models:** How the abstract-then-compute mechanism scales to models beyond 12B parameters and whether layer-wise separation persists or shifts proportionally.
- **Mechanistic explanation for CoT asymmetry:** Why chain-of-thought disproportionately improves arithmetic computation (+58.7% avg) while yielding limited gains in abstract formulation (+6.7-17.6%), as the interpretability analysis focuses on single-pass generation.
- **Exploiting abstraction representations:** Whether the identified abstraction representations can be leveraged to improve arithmetic computation through targeted interventions or architectural modifications, as the causal patching experiments stop at validation.

## Limitations
- The mechanistic interpretability results are based on simplified 1-2 step problems that may not generalize to complex multi-step reasoning
- Symbolic abstraction evaluation relies on GPT-4o-mini as an automatic grader with 94% human agreement, introducing potential systematic biases
- Activation patching analysis is conducted on a constrained problem set and may not capture full complexity of naturalistic reasoning

## Confidence
- **High confidence:** Disentanglement of abstract formulation from arithmetic computation as distinct skills; CoT primarily improves computation rather than abstraction
- **Medium confidence:** Mechanistic interpretation showing abstract-then-compute mechanism with specific layer boundaries (abstractions at layers 13-14, computation at 15-18)
- **Low confidence:** Generalizability of findings to complex, real-world mathematical reasoning tasks

## Next Checks
1. **Generalization to complex problems:** Validate whether the abstract-then-compute mechanism holds for multi-step problems (5+ steps) by applying the same logit attribution and activation patching methodology to GSM8K's more complex examples.
2. **Cross-model consistency:** Test whether the identified mechanism (abstractions at layers 13-14, computation at 15-18) is consistent across different model families and architectures using the same interpretability pipeline.
3. **Human evaluation validation:** Conduct human evaluation on a random sample of symbolic abstraction outputs to independently verify the 94% agreement claim of GPT-4o-mini, particularly focusing on edge cases where expression format or operator precedence might cause systematic evaluation errors.