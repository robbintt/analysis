---
ver: rpa2
title: 'InnerThoughts: Disentangling Representations and Predictions in Large Language
  Models'
arxiv_id: '2501.17994'
source_url: https://arxiv.org/abs/2501.17994
tags:
- layer
- language
- which
- question
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes InnerThoughts, a method to improve large language
  model (LLM) accuracy on multiple-choice question answering tasks by learning a small
  predictor network on hidden states from all transformer layers at the final token
  position, rather than just the last layer. The predictor module is trained while
  keeping the LLM parameters frozen, requiring only a single forward pass through
  the model.
---

# InnerThoughts: Disentangling Representations and Predictions in Large Language Models

## Quick Facts
- arXiv ID: 2501.17994
- Source URL: https://arxiv.org/abs/2501.17994
- Reference count: 13
- InnerThoughts improves LLM MCQ accuracy by training a predictor on all hidden states at the final token position, achieving near-QLoRA performance at 16% of the computational cost.

## Executive Summary
This paper introduces InnerThoughts, a method that improves LLM accuracy on multiple-choice question answering by learning a small predictor network on hidden states from all transformer layers at the final token position, rather than just the last layer. The approach keeps LLM parameters frozen and requires only a single forward pass through the model. Experiments on six benchmarks show InnerThoughts significantly outperforms previous calibration methods and achieves performance approaching parameter-efficient fine-tuning (QLoRA) at a fraction of the computational cost. Analysis reveals the method is most effective when LLMs show low confidence in their answers and benefits from using hidden states from all layers, particularly after layer 35.

## Method Summary
InnerThoughts extracts hidden states from all 80 transformer layers at the final token position for each MCQ example, then trains a small MLP-Mixer style predictor module while keeping the LLM frozen. The predictor uses LayerNorm and Linear layers to transform the (80, 8192) hidden state tensor into predictions over the answer choices. Training uses cross-entropy loss for up to 50 epochs with early stopping. The method disentangles representation learning (handled by the frozen LLM) from task-specific prediction (handled by the trainable predictor), requiring only a single forward pass through the LLM for training.

## Key Results
- InnerThoughts achieves 35% of QLoRA's accuracy gains at 16% of the computational cost
- Method shows highest improvements on datasets where LLM exhibits lowest confidence margins (AQuA, HellaSwag)
- Using hidden states from all layers, particularly after layer 35, provides significant performance benefits over using only final layer representations

## Why This Works (Mechanism)

### Mechanism 1: Distributed Task-Relevant Information
Hidden states from intermediate transformer layers contain task-relevant information that gets compressed or overwritten in the final hidden state. The cumulative dimensionality of all attention contributions (655,360 dimensions) vastly exceeds the final hidden space (8192 dimensions), causing information loss. Logit Lens analysis shows predictive accuracy jumps at layer 35 across all benchmarks, indicating this as a critical transition point.

### Mechanism 2: Disentangled Representation and Prediction
By keeping the LLM frozen and only training a lightweight predictor, the framework separates general representational abilities from task-specific prediction. This enables efficient training requiring only a single forward pass through the LLM, as the predictor learns to extract signal from the fixed representations rather than modifying them.

### Mechanism 3: Confidence-Aware Correction
The predictor module is most effective when the base LLM exhibits low confidence in its answers. When the confidence margin (top probability minus second probability) is small, the final layer's representation contains ambiguous signals that the predictor can resolve by leveraging intermediate-layer patterns.

## Foundational Learning

- **Hidden states and residual connections in transformers**: Understanding that each layer's output is the cumulative result of residual additions explains why intermediate states might contain "purer" representations before overwriting occurs. *Quick check*: Can you explain why h_l = h_{l-1} + Attention(h_{l-1}) means earlier-layer information is technically preserved but practically compressed?

- **Logit Lens / early exit strategies**: The paper uses Logit Lens analysis to show layer 35 as a critical transition point; understanding this technique clarifies why intermediate layers can be used for prediction independently. *Quick check*: If you apply the output projection matrix to layer 35's hidden state instead of layer 80, what are you assuming about that layer's representation?

- **Calibration vs. fine-tuning tradeoffs**: InnerThoughts is positioned between lightweight calibration (logistic regression on logits) and full fine-tuning (QLoRA); understanding this spectrum motivates the design. *Quick check*: Why does training a predictor on hidden states offer more capacity than logistic regression on logits, but less flexibility than LoRA adapters within transformer blocks?

## Architecture Onboarding

- **Component map**: Input extraction → Hidden state caching (L, d) → MLP-Mixer predictor (Norm→Linear→ReLU→Norm→Linear→ReLU→Flatten→Norm→Linear→Softmax) → Output predictions

- **Critical path**: Hidden state extraction is the computational bottleneck (~98% of training time); hyperparameter choices (n1=32, n2=8) control capacity; LayerNorm placement affects gradient flow.

- **Design tradeoffs**: MLP-Mixer vs. self-attention predictor (MLP-Mixer is simpler and parameter-efficient); all layers vs. subset (all layers provide maximum information but increase input dimension); frozen vs. fine-tuned backbone (freezing drastically reduces cost but limits ceiling performance).

- **Failure signatures**: No improvement over baseline (check if LLM already has high confidence margins); predictor overfits (reduce n1/n2 or add dropout); layer ablation shows no benefit (verify hidden state extraction correctly captures all layers).

- **First 3 experiments**: 1) Reproduce single-dataset result on HellaSwag with n1=32, n2=8; target ~5% accuracy gain over Direct. 2) Layer ablation: train predictors using only layers 1-35, 36-60, 61-80, and all layers; verify layers after 35 contribute most. 3) Confidence-margin stratification: bin test examples by LLM confidence margin; plot InnerThoughts gain per bin to confirm low-confidence examples benefit most.

## Open Questions the Paper Calls Out

- **Chain-of-Thought integration**: How to integrate with CoT frameworks when it's unclear if CoT introduces the same biases InnerThoughts corrects.

- **Fine-tuned model application**: Whether applying InnerThoughts on top of fine-tuned models or training simultaneously in an end-to-end manner provides additional gains.

- **Numerical regression adaptation**: Changing the predictor to a regression model for tasks like arithmetic (GSM8k) and extending to few-shot settings.

- **Skill learning vs. calibration**: Whether the predictor learns missing non-linguistic skills (e.g., algebraic reasoning) or merely calibrates existing linguistic representations.

## Limitations

- The Logit Lens analysis showing layer 35 as critical may be specific to Llama3's architecture and not generalize to other model families.

- The method's effectiveness depends on the assumption that intermediate hidden states contain complementary information, but this is only supported by theoretical dimensionality arguments rather than empirical feature importance analysis.

- It's unclear whether the method recovers genuine knowledge corrupted in final layers or learns task-specific heuristics that may not generalize to distribution shifts.

## Confidence

**High Confidence**: InnerThoughts significantly outperforms previous calibration methods; the method requires only a single forward pass through the LLM; datasets with lowest LLM confidence margins show highest gains.

**Medium Confidence**: Hidden states from all layers provide complementary information; layer 35 represents a critical transition point; disentanglement between representation and prediction enables efficiency.

**Low Confidence**: The MLP-Mixer architecture is optimal for this task; information compression in residual connections is the primary driver.

## Next Checks

1. **Layer Importance Ablation with Permutation Testing**: Systematically remove layers in different positions and measure predictor performance degradation, comparing to randomly selected layer subsets to test whether the 35+ layers are genuinely more important.

2. **Cross-Model Generalization Test**: Apply InnerThoughts trained on Llama3 70B to a different LLM architecture (e.g., Mistral) without retraining the predictor to assess architecture-specificity of learned corrections.

3. **Uncertainty-Aware Error Analysis**: For predictions where InnerThoughts corrects the LLM, analyze whether corrections are based on genuine confidence signals or pattern matching using uncertainty quantification or adversarial perturbations.