---
ver: rpa2
title: Efficient Prediction of Pass@k Scaling in Large Language Models
arxiv_id: '2510.05197'
source_url: https://arxiv.org/abs/2510.05197
tags:
- sampling
- pass
- arxiv
- scaling
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting the pass@k metric
  at scale when only a limited sampling budget is available. The authors critique
  prior methods, such as log-log regression and discretized beta fitting, for their
  statistical shortcomings in sample-limited regimes.
---

# Efficient Prediction of Pass@k Scaling in Large Language Models

## Quick Facts
- **arXiv ID**: 2510.05197
- **Source URL**: https://arxiv.org/abs/2510.05197
- **Reference count**: 40
- **Primary result**: A robust estimation framework using beta-binomial distribution and dynamic sampling strategy achieves lower mean squared error in pass@k predictions than existing methods, especially for large k values.

## Executive Summary
This paper addresses the challenge of predicting pass@k metrics at scale when only a limited sampling budget is available. The authors critique prior methods like log-log regression and discretized beta fitting for their statistical shortcomings in sample-limited regimes. They propose a robust estimation framework using a beta-binomial distribution to better model problem difficulty distributions, combined with a dynamic sampling strategy that allocates more resources to harder problems. The approach consistently achieves lower mean squared error than existing methods across three datasets and multiple models, particularly for large k values.

## Method Summary
The method involves dynamic sampling where problems with the fewest successes are selected iteratively, followed by fitting a beta-binomial distribution to the observed success counts using maximum likelihood estimation. The pass@k metric is then predicted by computing the expectation under the fitted beta distribution. This approach models the full distribution of per-problem success probabilities and concentrates sampling budget on harder problems where uncertainty is highest, avoiding the statistical flaws of previous methods that violate regression assumptions or introduce downward bias through discretization.

## Key Results
- Beta-binomial distributional fitting outperforms log-log regression and discretized beta methods for predicting pass@k from limited samples
- Dynamic sampling concentrating compute on harder problems improves prediction accuracy under fixed budgets
- Accurate modeling of the left tail of the problem-difficulty distribution is key to extrapolating pass@k for large k values

## Why This Works (Mechanism)

### Mechanism 1
Models the full distribution of per-problem success probabilities U by maximizing the beta-binomial likelihood directly over observed (successes, attempts) pairs, then computes pass@k as the expectation E[1-(1-p)^k] under the fitted Beta(α̂, β̂). This avoids statistical flaws in prior methods that violate regression assumptions or introduce downward bias through discretization.

### Mechanism 2
Algorithm 1 repeatedly selects the problem with fewest successes (breaking ties by fewest attempts), allocating more samples where uncertainty about near-zero success rates is highest. This is theoretically motivated by Theorem 1, which proves optimal sampling should allocate b_i* ∝ √(p_i(1-p_i)^(2k-1)), heavily favoring low-p problems.

### Mechanism 3
Pass@k for large k is dominated by whether the hardest problems ever succeed. The beta-binomial captures tail shape through α and β parameters, while the discretized beta method's wider bins at low probabilities systematically underweight this critical region.

## Foundational Learning

### Concept: Beta-binomial conjugate prior relationship
- **Why needed**: The method hinges on why maximizing Equation 15 is tractable—beta priors yield closed-form posteriors under binomial likelihoods.
- **Quick check**: If you observe 3 successes in 10 attempts on problem i, what's the posterior mean success probability under a flat Beta(1,1) prior?

### Concept: Pass@k definition and combinatorial estimator
- **Why needed**: Understanding Equation 1 (pass@k = 1-(1-p)^k) and why Equation 3 requires n≥k samples clarifies the extrapolation problem.
- **Quick check**: If pass@1 = 0.1, what's pass@100? What's the minimum samples needed to compute the unbiased estimator?

### Concept: Heteroscedasticity and why OLS assumptions fail
- **Why needed**: Section 3.2's critique of log-log regression relies on understanding non-constant variance across k values.
- **Quick check**: Why does Var(pass@1) differ from Var(pass@n) even with the same total samples?

## Architecture Onboarding

### Component map
Data (problems with binary outcomes) → Sampling controller (dynamic or uniform) → Observation buffer (s_i, b_i per problem) → MLE optimizer (maximizes beta-binomial likelihood) → Predictor (computes expectation under fitted Beta)

### Critical path
The likelihood maximization in Equation 16—numerical instability here cascades to all downstream predictions.

### Design tradeoffs
Dynamic sampling (Algorithm 1) requires online decision-making and doesn't integrate with methods needing uniform sampling (regression, discretized beta); static allocation is simpler but less sample-efficient.

### Failure signatures
(1) Predictions exceeding 1 (log-log regression, clipped in paper's figures); (2) Systematic underestimation at large k (discretized beta); (3) Non-convergence of MLE when all s_i = 0 or all s_i = b_i

### First 3 experiments
1. Reproduce Figure 2 on synthetic uniform data: fit both discretized and beta-binomial to s_i ~ Binomial(100, p_i) where p_i ~ Uniform(0,1); verify discretized produces left-tail bias.
2. Ablate sampling strategy: Run Algorithm 2 with uniform vs. dynamic allocation on a held-out subset of the Code Contests data; isolate sampling contribution by fixing the estimator.
3. Stress-test distributional assumptions: Generate synthetic data from a bimodal difficulty distribution (e.g., mixture of Beta(0.5,5) and Beta(5,0.5)); compare prediction MSE to unimodal ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
Does the proposed adaptive sampling strategy theoretically reduce variance in the Bayesian Beta-Binomial estimation framework? The authors provide a theoretical proof of variance reduction only for a frequentist estimator with oracle access, not for their primary Bayesian estimator.

### Open Question 2
Can the statistical corrections provided for pass@k extrapolation be successfully applied to improve the sample efficiency of other neural scaling laws? The paper focuses exclusively on inference-time scaling and does not test if these robust estimation techniques apply to pre-training or architecture scaling laws.

### Open Question 3
To what extent does the dynamic sampling strategy alone improve prediction accuracy independently of the Beta-Binomial estimation method in real-world regimes? Real-world experiments combine the new estimator with the new sampling strategy, making it difficult to decouple the specific gains from sampling versus modeling.

## Limitations
- The beta-binomial distributional assumption may struggle with multi-modal or heavy-tailed difficulty distributions
- The dynamic sampling strategy's optimality for the beta-binomial setting follows from frequentist theory but hasn't been rigorously proven for the Bayesian case
- The method requires careful numerical implementation of the likelihood optimization with potential failure modes

## Confidence

**High Confidence**: The critique of log-log regression's statistical violations and the discretized beta method's systematic downward bias is well-supported by both theoretical analysis and empirical demonstration.

**Medium Confidence**: The dynamic sampling allocation strategy's optimality for the beta-binomial setting follows from frequentist theory but hasn't been rigorously proven for the Bayesian case.

**Low Confidence**: The claim that the beta-binomial family is sufficiently expressive for real-world problem difficulty distributions remains untested against alternatives.

## Next Checks

1. Generate synthetic data from multi-modal difficulty distributions (e.g., mixture of Betas) and evaluate prediction MSE compared to unimodal ground truth to stress-test the beta-binomial's expressive capacity.

2. Implement a hybrid sampling strategy that combines uniform and dynamic allocation and measure whether a mixed approach outperforms either extreme, particularly for datasets with heterogeneous difficulty distributions.

3. Apply the method to a new dataset with known difficulty characteristics (e.g., synthetic programming problems with controlled difficulty gradients) and compare performance against baseline methods across multiple data regimes to establish generalizability.