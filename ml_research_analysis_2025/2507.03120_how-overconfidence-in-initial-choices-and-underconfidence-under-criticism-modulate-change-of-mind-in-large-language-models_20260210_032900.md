---
ver: rpa2
title: How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate
  Change of Mind in Large Language Models
arxiv_id: '2507.03120'
source_url: https://arxiv.org/abs/2507.03120
tags:
- advice
- answer
- confidence
- change
- mind
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study reveals that LLMs exhibit conflicting behaviors: they
  are both overconfident in their initial answers and overly sensitive to contradictory
  feedback. Using a novel experimental paradigm, the researchers showed that LLMs
  (Gemma 3, GPT4o, and o1-preview) display a choice-supportive bias, reinforcing confidence
  in their initial answers and resisting change, even when faced with opposing advice.'
---

# How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models

## Quick Facts
- arXiv ID: 2507.03120
- Source URL: https://arxiv.org/abs/2507.03120
- Reference count: 36
- Key outcome: LLMs exhibit both overconfidence in initial answers and excessive sensitivity to contradictory feedback

## Executive Summary
This study investigates how large language models (LLMs) handle changes of mind when confronted with contradictory advice. Using a novel experimental paradigm, researchers found that LLMs display conflicting behaviors: they are both overconfident in their initial answers and overly sensitive to criticism. The study reveals that LLMs demonstrate a choice-supportive bias, reinforcing confidence in their initial responses and resisting change even when presented with opposing viewpoints. Additionally, LLMs overweight inconsistent advice compared to consistent advice, deviating from normative Bayesian updating principles.

## Method Summary
The researchers employed a novel experimental paradigm involving multiple-choice questions across various domains. Participants (LLMs including Gemma 3, GPT-4o, and o1-preview) were presented with questions, provided answers, and then received varying types of advice (consistent, inconsistent, or no advice). The study measured changes in confidence levels before and after receiving advice, analyzing patterns of decision-making and belief updating. The experimental design allowed for systematic examination of how LLMs process contradictory information and update their beliefs.

## Key Results
- LLMs show choice-supportive bias, reinforcing confidence in initial answers and resisting change
- LLMs overweight inconsistent advice compared to consistent advice, deviating from Bayesian norms
- The conflicting behaviors of overconfidence and excessive sensitivity to criticism were observed across multiple models

## Why This Works (Mechanism)
The study suggests that LLMs' conflicting behaviors stem from underlying confidence mechanisms that are not optimally calibrated. The choice-supportive bias appears to be an inherent property of how these models process and reinforce their initial decisions, while the excessive sensitivity to contradictory feedback indicates a lack of robust belief-updating mechanisms. These dual tendencies create a paradox where models are simultaneously stubborn and overly reactive to criticism.

## Foundational Learning
- **Choice-supportive bias**: The tendency to retroactively enhance the attractiveness of chosen options while devaluing rejected ones. Why needed: To understand how LLMs reinforce their initial decisions. Quick check: Compare confidence changes in chosen vs. unchosen options.
- **Bayesian updating**: The normative framework for rationally incorporating new evidence into existing beliefs. Why needed: To establish a benchmark for evaluating LLMs' belief-updating behavior. Quick check: Calculate whether confidence changes follow Bayesian predictions.
- **Confidence calibration**: The alignment between stated confidence and actual accuracy. Why needed: To identify systematic deviations from optimal confidence expression. Quick check: Correlate confidence levels with actual accuracy rates.
- **Feedback sensitivity**: The degree to which new information influences belief revision. Why needed: To measure how LLMs respond to contradictory advice. Quick check: Compare confidence changes following consistent vs. inconsistent feedback.
- **Decision inertia**: The resistance to changing an initial choice despite new evidence. Why needed: To quantify the stubbornness of LLMs in maintaining their positions. Quick check: Measure the frequency of maintained initial choices across conditions.
- **Normative vs. descriptive models**: The distinction between ideal rational behavior and actual observed behavior. Why needed: To contextualize LLMs' deviations from optimal decision-making. Quick check: Compare observed behavior against Bayesian benchmarks.

## Architecture Onboarding
Component map: Input Processing -> Confidence Estimation -> Feedback Integration -> Output Generation
Critical path: The confidence estimation and feedback integration components are most critical for the observed behaviors, as they directly determine how initial choices are reinforced and how contradictory advice is weighted.
Design tradeoffs: The models appear to prioritize maintaining coherence with initial responses over accurate belief updating, suggesting a design preference for response consistency over optimal calibration.
Failure signatures: Overconfidence in initial choices manifests as resistance to change, while underconfidence under criticism appears as excessive sensitivity to contradictory feedback.
Three first experiments:
1. Test whether different prompting strategies (e.g., chain-of-thought vs. direct answers) modulate the magnitude of choice-supportive bias
2. Examine whether providing meta-level instructions about rational belief updating reduces the overweighting of inconsistent advice
3. Investigate whether confidence calibration improves with fine-tuning on datasets that emphasize normative belief-updating behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are based on three specific models (Gemma 3, GPT-4o, and o1-preview), limiting generalizability
- The single experimental paradigm may not capture the full range of confidence-related behaviors in LLMs
- The study does not establish clear thresholds for distinguishing between appropriate and excessive sensitivity to criticism

## Confidence
- Confidence in conflicting overconfidence/underconfidence behaviors: High
- Confidence in same underlying mechanism interpretation: Medium
- Confidence in practical implications: Medium-Low

## Next Checks
1. Replicate the experimental paradigm across a broader range of LLM architectures to assess generalizability of the overconfidence/underconfidence patterns
2. Test whether the magnitude of choice-supportive bias and sensitivity to criticism varies with different prompt engineering approaches
3. Design a real-world deployment scenario where these confidence biases could manifest and empirically measure their frequency and impact