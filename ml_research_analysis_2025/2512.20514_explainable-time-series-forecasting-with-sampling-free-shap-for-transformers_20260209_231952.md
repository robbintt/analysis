---
ver: rpa2
title: Explainable time-series forecasting with sampling-free SHAP for Transformers
arxiv_id: '2512.20514'
source_url: https://arxiv.org/abs/2512.20514
tags:
- shap
- feature
- load
- shapformer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHAPformer is a Transformer-based time-series forecasting model
  that enables fast, exact SHAP explanations by using attention manipulation to evaluate
  predictions on feature subsets without sampling. On synthetic data with ground truth
  explanations, SHAPformer closely matches the ground truth in feature importance
  and dependence patterns, outperforming both the SHAP Permutation Explainer and Custom
  Masker.
---

# Explainable time-series forecasting with sampling-free SHAP for Transformers

## Quick Facts
- arXiv ID: 2512.20514
- Source URL: https://arxiv.org/abs/2512.20514
- Reference count: 40
- Key outcome: SHAPformer enables fast, exact SHAP explanations for Transformer-based time-series forecasting by manipulating attention weights, achieving over 50× speedup compared to sampling-based methods while maintaining competitive forecasting accuracy.

## Executive Summary
SHAPformer is a Transformer-based time-series forecasting model that enables fast, exact SHAP explanations by using attention manipulation to evaluate predictions on feature subsets without sampling. On synthetic data with ground truth explanations, SHAPformer closely matches the ground truth in feature importance and dependence patterns, outperforming both the SHAP Permutation Explainer and Custom Masker. Applied to real-world electrical load data, it achieves competitive forecasting accuracy (RMSE ≈ 265.9 MW) and provides meaningful global and local insights, such as identifying past load as the dominant predictor and revealing distinct model behavior during the Christmas period. Inference time for explanations is under one second, over 50× faster than the Permutation Explainer on synthetic data and over 800× faster on real-world data.

## Method Summary
SHAPformer introduces a novel approach to explainable time-series forecasting by modifying Transformer attention mechanisms to compute exact SHAP values without sampling. The method works by manipulating attention weights to evaluate model predictions on different feature subsets, enabling efficient computation of feature attributions. This is achieved through a custom implementation that modifies the attention mechanism to simulate feature masking directly in the attention computation, rather than requiring multiple model evaluations as in traditional SHAP methods. The approach is validated on both synthetic data with known ground truth explanations and real-world electrical load forecasting data, demonstrating both accuracy and computational efficiency.

## Key Results
- On synthetic data, SHAPformer closely matches ground truth explanations in feature importance and dependence patterns, outperforming SHAP Permutation Explainer and Custom Masker
- For real-world electrical load forecasting, SHAPformer achieves RMSE ≈ 265.9 MW while providing meaningful global and local insights
- Inference time for explanations is under one second, over 50× faster than Permutation Explainer on synthetic data and over 800× faster on real-world data

## Why This Works (Mechanism)
The core mechanism leverages the attention mechanism's inherent ability to weight feature importance. By manipulating attention weights to simulate feature masking, SHAPformer can compute exact SHAP values without requiring multiple model evaluations. This approach exploits the fact that attention weights directly represent feature contributions, allowing the model to evaluate predictions under different feature subsets through weight manipulation rather than re-computation.

## Foundational Learning
- Transformer attention mechanisms - needed for understanding how feature importance is encoded; quick check: verify attention weights sum to 1 across features
- SHAP (SHapley Additive exPlanations) - needed for understanding feature attribution methodology; quick check: confirm SHAP values sum to prediction difference from baseline
- Time-series forecasting - needed for context of sequential prediction task; quick check: validate forecast horizon matches evaluation metrics
- Exact vs. sampling-based explanations - needed to understand computational trade-offs; quick check: compare variance in explanations between methods
- Attention weight manipulation - needed for understanding the core innovation; quick check: verify manipulated attention produces expected masked predictions
- Computational complexity analysis - needed for understanding efficiency claims; quick check: benchmark explanation time vs. prediction time

## Architecture Onboarding

**Component Map:** Input features -> Transformer Encoder -> Attention Manipulation Module -> SHAP Value Computation -> Explanations

**Critical Path:** Data preprocessing → Transformer encoding → Attention manipulation → SHAP computation → Visualization/interpretation

**Design Tradeoffs:** Exact explanations via attention manipulation sacrifice some flexibility compared to sampling methods but gain significant speed. The approach requires custom implementation of attention mechanisms rather than using standard libraries.

**Failure Signatures:** Poor attention weight distributions may lead to inaccurate SHAP values; model architecture changes may break the attention manipulation assumptions; numerical instability in attention manipulation could produce invalid explanations.

**First Experiments:** 1) Verify attention manipulation produces correct masked predictions on synthetic data with known ground truth; 2) Benchmark explanation speed vs. standard SHAP methods on small datasets; 3) Test explanation quality degradation with increasing sequence length.

## Open Questions the Paper Calls Out
The paper identifies several open questions, including how the approach generalizes to more complex attention mechanisms beyond standard self-attention, whether the exact computation remains valid with longer sequences, and how the method performs on datasets with different characteristics than the electrical load data tested.

## Limitations
- Reliance on synthetic data for ground truth validation may not fully capture real-world time-series complexity
- Exact SHAP computation via attention manipulation may have edge cases that break down with more complex attention mechanisms
- Generalization to diverse real-world datasets beyond electrical load forecasting remains uncertain

## Confidence
- Exact SHAP computation claims: High confidence (supported by synthetic data results)
- Generalization to real-world data: Medium confidence (limited to electrical load dataset)
- Speed improvement claims: High confidence (well-supported by reported inference times)
- Robustness to complex attention mechanisms: Low confidence (not extensively tested)

## Next Checks
1. Test SHAPformer on additional real-world datasets with known ground truth explanations (e.g., simulated sensor data with injected anomalies) to verify explanation accuracy beyond the electrical load dataset.
2. Evaluate the model's performance with varying attention mechanisms (e.g., sparse attention, cross-attention) to ensure the exact SHAP computation remains valid.
3. Conduct ablation studies to quantify the trade-off between explanation speed and accuracy when using attention manipulation versus sampling-based methods on larger datasets.