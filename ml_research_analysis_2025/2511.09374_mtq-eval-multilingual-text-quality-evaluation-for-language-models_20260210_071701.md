---
ver: rpa2
title: 'MTQ-Eval: Multilingual Text Quality Evaluation for Language Models'
arxiv_id: '2511.09374'
source_url: https://arxiv.org/abs/2511.09374
tags:
- text
- quality
- latn
- languages
- mtq-eval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MTQ-Eval, a novel framework for multilingual
  text quality evaluation that trains models to distinguish high- and low-quality
  text using Direct Preference Optimization (DPO). The method automatically generates
  synthetic quality preference data across 115 languages without human annotations
  by creating paired normal and degraded text samples.
---

# MTQ-Eval: Multilingual Text Quality Evaluation for Language Models

## Quick Facts
- arXiv ID: 2511.09374
- Source URL: https://arxiv.org/abs/2511.09374
- Reference count: 11
- This paper introduces MTQ-Eval, a novel framework for multilingual text quality evaluation that trains models to distinguish high- and low-quality text using Direct Preference Optimization (DPO).

## Executive Summary
MTQ-Eval presents a framework for training multilingual language models to evaluate text quality across 115 languages without human annotations. The method uses Direct Preference Optimization to align models with quality preferences, automatically generating synthetic quality preference data through paired normal and degraded text samples. By word-shuffling passages to create degraded versions, the approach enables training on binary preference pairs that capture quality differences. The framework demonstrates significant improvements in text quality assessment performance compared to baseline approaches, achieving better Matthew's Correlation Coefficient, KL divergence, and F1 scores across both high- and low-resource languages.

## Method Summary
The MTQ-Eval framework creates synthetic quality preference data by generating paired normal and degraded text samples from the Belebele dataset (115 languages, 20 samples each). High-quality text consists of original passages, while low-quality text is created by randomly swapping 3-6 words within each passage. This creates 4,600 passages total with binary labels (1 for normal, 0 for degraded). The method employs Direct Preference Optimization fine-tuning on Llama-3.1-8B-Instruct or Aya-Expanse-8B models with LoRA adapters (alpha=128, rank=64) using a specific prompt template that includes chosen and rejected responses for binary classification. Evaluation is performed on both the MELA dataset (10 languages with human annotations) and a Belebele-derived test set covering all 115 languages.

## Key Results
- MTQ-Eval achieves significantly better Matthew's Correlation Coefficient, KL divergence, and F1 scores compared to baseline approaches across both high- and low-resource languages
- The enhanced text quality evaluation capability translates to improved performance in downstream tasks including sentiment analysis and summarization, particularly for low-resource languages
- Word-shuffling creates sufficiently distinct degraded text that receives lower quality scores from all models, validating the degradation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Preference Optimization (DPO) aligns models with text quality judgments more effectively than supervised fine-tuning (SFT) for multilingual evaluation.
- Mechanism: DPO optimizes a log-ratio loss function that increases likelihood of preferred (high-quality) responses while reducing probability of rejected (low-quality) ones. Unlike SFT which learns absolute labels, DPO learns relative rankings through pairwise comparisons, creating more robust quality discrimination across diverse linguistic structures.
- Core assumption: Text quality can be effectively captured through binary preference pairs, and this preference learning transfers across languages without requiring language-specific annotations.
- Evidence anchors: [abstract] "uses Direct Preference Optimization to align models with quality preferences"; [section 2.2] "DPO fine-tunes the model by increasing the likelihood of preferred responses while reducing the probability of less preferred ones"; [corpus] Limited corpus evidence for DPO specifically for quality evaluation.

### Mechanism 2
- Claim: Word-shuffling creates sufficiently distinct degraded text to serve as negative examples for quality preference learning across 115 languages without language-specific NLP tools.
- Mechanism: Randomly swapping 3-6 words within each passage disrupts grammatical structure and semantic flow. This controlled degradation preserves enough surface similarity to normal text that the model learns fine-grained quality distinctions rather than trivial pattern matching.
- Core assumption: Word order disruption correlates with quality degradation across typologically diverse languages, and the degree of shuffling (3-6 words) creates appropriately challenging training signal.
- Evidence anchors: [section 2.1] "This straightforward technique...randomly rearranges a few words within a passage, disrupting the grammatical structure and semantic flow"; [Table 1] Shows word-shuffled text receiving lower scores from all models; [corpus] No direct corpus evidence.

### Mechanism 3
- Claim: Learning to evaluate text quality creates transferable representations that improve downstream task performance even without task-specific training data.
- Mechanism: Quality evaluation training encourages models to develop stronger internal representations of coherence, fluency, and linguistic acceptability. These representations become useful for tasks like sentiment analysis and summarization that implicitly require understanding text quality.
- Core assumption: Text quality evaluation and downstream tasks share underlying linguistic competencies; improvements in one transfer to others.
- Evidence anchors: [abstract] "This method also improves downstream performance in sentiment analysis and summarization tasks"; [section 5] "models trained to assess text quality also develop stronger representations"; [Table 12] Shows MTQ-Eval producing coherent summaries for Nepali (low-resource) where SFT outputs are noisy.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed here: DPO is the core training method replacing RLHF for aligning models with quality preferences. Understanding its loss function and how it differs from SFT is essential.
  - Quick check question: Can you explain why DPO's relative ranking objective might outperform SFT's absolute classification for learning quality judgments?

- **Text Quality Dimensions**
  - Why needed here: The paper operationalizes quality through four dimensions (coherence, fluency, simplicity, linguistic acceptability). These inform both the evaluation prompt and degradation strategy.
  - Quick check question: Which quality dimension would word-shuffling most directly affect, and which might it leave relatively intact?

- **Multilingual Resource Grading**
  - Why needed here: The paper uses Joshi et al.'s 0-5 resource level taxonomy to analyze performance differences. Understanding this helps interpret why results vary across languages.
  - Quick check question: Why might DPO-trained models show greater improvements for high-resource (levels 3-5) versus low-resource (levels 0-2) languages?

## Architecture Onboarding

- **Component map:**
  Belebele passages -> Word shuffler -> Preference pairs (prompt, chosen, rejected)
  -> Training: Llama 3.1 / Aya Expanse 8B -> LoRA adapters (rank=64, alpha=128) -> DPO loss
  -> Evaluation: MELA (10 languages, human annotations) + Belebele-derived test (115 languages, synthetic)
  -> Downstream: MMS sentiment (28 languages) + XL-Sum (45 languages)

- **Critical path:**
  1. Data creation dominates iteration speedâ€”ensure word-shuffling pipeline handles Unicode correctly across scripts
  2. DPO hyperparameters (learning rate 5e-7, batch size 2, 1 epoch) were tuned conservatively; aggressive settings may destabilize
  3. Prompt template (Figure 2) must be consistent between training and inference

- **Design tradeoffs:**
  - Synthetic degradation vs. human annotations: Scales to 115 languages but captures limited quality failure modes (no semantic errors, factual inaccuracies)
  - Binary ratings vs. scalar: Simpler training signal but loses granularity
  - Word shuffling exclusion of 7 languages trades coverage for degradation reliability

- **Failure signatures:**
  - Negative MCC on SFT baselines (aya SFT: -0.23 on Belebele) indicates miscalibrated confidence
  - High variance across language families (Nilo-Saharan: 0.33-0.37 F1) suggests underrepresentation issues
  - Script-specific degradation (Orya, Tamil, Bengali show negative improvement with Llama) indicates tokenization issues

- **First 3 experiments:**
  1. Reproduce the degradation pipeline on 5 diverse languages (pick from different resource levels and scripts), manually verify shuffled samples appear degraded to native speakers or use GPT-4 as proxy judge.
  2. Compare DPO vs. SFT on a held-out language not in training to test zero-shot transfer; measure MCC gap.
  3. Ablate degradation intensity (2-3 vs. 5-6 word swaps) on 3 languages to find sweet spot between too-easy and too-distorted negative examples.

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic nature of training data captures only grammatical and fluency-related quality failures through word shuffling, missing semantic errors, factual inaccuracies, or context-dependent quality issues
- The exclusion of 7 languages with flexible word order acknowledges a fundamental limitation in the degradation approach
- While MTQ-Eval improves performance for high-resource languages (levels 3-5), gains for low-resource languages (levels 0-2) are more modest

## Confidence
- Core methodological contribution: High confidence based on robust experimental design and comprehensive evaluation across 115 languages
- Downstream task transfer benefits: Medium confidence as evidence comes primarily from quantitative improvements rather than qualitative analysis
- Claims about multilingual generalization: Medium confidence due to reliance on synthetic test sets for 105 of 115 languages

## Next Checks
1. Conduct qualitative analysis of degraded samples across diverse language families to verify that word-shuffling creates perceptibly lower-quality text, not just different word orders.
2. Test zero-shot transfer to languages outside the 115-language training set to evaluate true multilingual generalization capabilities.
3. Perform ablation studies varying degradation intensity (2-3 vs. 5-6 word swaps) to identify optimal balance between creating challenging negative examples and maintaining realistic quality failures.