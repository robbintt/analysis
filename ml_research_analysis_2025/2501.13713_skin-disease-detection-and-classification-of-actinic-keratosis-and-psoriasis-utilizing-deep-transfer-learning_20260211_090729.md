---
ver: rpa2
title: Skin Disease Detection and Classification of Actinic Keratosis and Psoriasis
  Utilizing Deep Transfer Learning
arxiv_id: '2501.13713'
source_url: https://arxiv.org/abs/2501.13713
tags:
- skin
- vgg16
- learning
- dataset
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a deep learning-based approach for detecting\
  \ and classifying two specific skin diseases\u2014Actinic Keratosis and Psoriasis\u2014\
  using a modified VGG16 convolutional neural network. The model employs transfer\
  \ learning with ImageNet pre-trained weights, fine-tuned with custom top layers\
  \ including fully connected and softmax activation layers."
---

# Skin Disease Detection and Classification of Actinic Keratosis and Psoriasis Utilizing Deep Transfer Learning

## Quick Facts
- arXiv ID: 2501.13713
- Source URL: https://arxiv.org/abs/2501.13713
- Authors: Fahud Ahmmed; Md. Zaheer Raihan; Kamnur Nahar; D. M. Asadujjaman; Md. Mahfujur Rahman; Abdullah Tamim
- Reference count: 21
- Primary result: Modified VGG16 achieves 90.67% accuracy on skin disease classification with precision, recall, and F1-scores ranging from 0.83 to 1.00 per class

## Executive Summary
This study presents a deep learning-based approach for detecting and classifying two specific skin diseases—Actinic Keratosis and Psoriasis—using a modified VGG16 convolutional neural network. The model employs transfer learning with ImageNet pre-trained weights, fine-tuned with custom top layers including fully connected and softmax activation layers. Data augmentation techniques such as rotation, shifting, and zooming were applied to enhance training robustness. Evaluated on a balanced dataset of 2,400 images, the proposed model achieved 90.67% accuracy, with precision, recall, and F1-scores ranging from 0.83 to 1.00 per class. Results demonstrate that the modified VGG16 architecture effectively supports early and accurate skin disease diagnosis, offering a reliable tool for clinical applications.

## Method Summary
The methodology employs a modified VGG16 convolutional neural network with ImageNet pre-trained weights. The original classification head is replaced with custom layers: two dense layers (1024 and 512 units) with 0.5 dropout regularization, followed by a softmax layer for three-class output. The convolutional base remains frozen during training, and data augmentation including rotation, shifting, and zooming is applied during preprocessing. The model is trained using categorical crossentropy loss and the Adam optimizer (learning rate 0.0001) on a balanced dataset of 800 images per class.

## Key Results
- Modified VGG16 achieves 90.67% overall accuracy on three-class skin disease classification
- Class-specific precision ranges from 0.83 (Normal) to 1.00 (Psoriasis)
- ROC-AUC scores range from 0.9496 to 0.9997 per class
- Data augmentation ablation confirms its contribution to model robustness

## Why This Works (Mechanism)

### Mechanism 1
Transfer learning from ImageNet enables effective feature extraction for dermatological images despite domain shift. The pre-trained VGG16 convolutional base has learned hierarchical edge, texture, and shape detectors from 1M+ natural images. These low-level and mid-level features (edges, color gradients, texture patterns) transfer to skin lesion classification because dermatological patterns share structural regularities with natural image features. Only the classification head requires retraining. Core assumption: Features learned from natural images generalize to clinical skin imaging despite significant domain difference.

### Mechanism 2
Geometric data augmentation improves model robustness and reduces overfitting on the modest 1,950-image training set. Applying rotation, shifting, and zooming during preprocessing artificially expands training diversity, forcing the model to learn invariant representations rather than memorizing exact pixel configurations. This is particularly important for skin lesions where orientation and scale vary naturally. Core assumption: Augmentation transformations preserve disease-relevant features while introducing irrelevant variation the model should ignore.

### Mechanism 3
The custom classification head with dropout regularization enables effective 3-class discrimination from VGG16 bottleneck features. Replacing the original 1,000-class ImageNet head with two dense layers (1024 → 512 units) and 0.5 dropout compresses high-dimensional features (8,192 from 150×150 input through flatten) into class-appropriate representations. Dropout randomly zeros 50% of activations during training, preventing co-adaptation and improving generalization. Core assumption: The bottleneck features contain sufficient discriminative information for the three target classes.

## Foundational Learning

- Concept: Transfer Learning in CNNs
  - Why needed here: Understanding why frozen pre-trained weights work for feature extraction and when to fine-tune vs. freeze layers is critical for reproducing results.
  - Quick check question: If you observe high training accuracy but low validation accuracy with frozen convolutional layers, what adjustment should you consider first?

- Concept: Class Imbalance and Macro vs. Weighted Metrics
  - Why needed here: The paper uses a balanced dataset (800 per class), but real-world deployments face imbalance. Understanding why macro-F1 equals weighted-F1 here (0.91) helps assess generalizability.
  - Quick check question: Why do macro and weighted averages coincide in this study, and what would happen if Actinic Keratosis had 2,400 images while others had 800?

- Concept: ROC-AUC for Multi-class Classification
  - Why needed here: The paper reports AUC range 0.9496–0.9997. Understanding one-vs-rest ROC curves is essential for interpreting per-class discriminative ability.
  - Quick check question: A Psoriasis AUC of 0.9997 indicates near-perfect separation from other classes. Does this guarantee clinical reliability if the test set is not representative of population prevalence?

## Architecture Onboarding

- Component map:
  Input: 150×150×3 RGB image → VGG16 convolutional base (13 conv layers, 5 max-pool stages, pre-trained ImageNet weights, frozen) → Flatten → Dense(1024, ReLU) → Dropout(0.5) → Dense(512, ReLU) → Dropout(0.5) → Dense(3, Softmax)

- Critical path:
  1. Verify VGG16 ImageNet weights load correctly and convolutional base is frozen
  2. Ensure preprocessing matches ImageNet normalization (mean subtraction, channel ordering)
  3. Validate custom head dimensionality: flattened output size → 1024 → 512 → 3
  4. Confirm augmentation pipeline runs only on training data, not validation/test

- Design tradeoffs:
  - Small input resolution (150×150) reduces computational cost but may lose fine-grained lesion texture
  - Freezing convolutional base speeds training but prevents adaptation to dermatological features; partial fine-tuning (last 2-4 conv blocks) was not explored
  - Aggressive dropout (0.5) may under-regularize if training data increases, or over-regularize with current dataset—no ablation cited

- Failure signatures:
  - Training loss plateaus early with frozen base: pre-trained features may be insufficient; consider unfreezing later conv blocks
  - High Psoriasis precision (1.00) but lower Actinic Keratosis precision (0.90): class confusion likely between visually similar lesions; check confusion matrix for specific error patterns
  - Validation accuracy oscillates: batch size of 8 may cause noisy gradients; try increasing to 16 or 32

- First 3 experiments:
  1. Reproduce baseline with exact hyperparameters (lr=0.0001, batch=8, epochs=150) and verify 90.67% accuracy on the same test split. Log training/validation curves to identify overfitting onset.
  2. Run ablation: train without augmentation (remove rotation/shift/zoom) to quantify augmentation contribution. Expect potential drop in generalization metrics.
  3. Compare frozen vs. partially fine-tuned VGG16 (unfreeze last 2 convolutional blocks) with same hyperparameters. Monitor for overfitting and accuracy changes; this tests whether domain-specific adaptation improves results.

## Open Questions the Paper Calls Out

### Open Question 1
Can the modified VGG16 model maintain its reported 90.67% accuracy when validated against external, multi-center clinical datasets distinct from the Kaggle source? The study relies on a single publicly available dataset ("Skin Disease Dataset"), which may contain biases or image qualities that do not reflect diverse real-world clinical settings. Reporting performance metrics (F1-score, AUC) obtained by testing the trained model on separate, heterogeneous datasets such as ISIC or HAM10000 would resolve this.

### Open Question 2
How does the model's classification performance scale when expanding the diagnostic scope beyond the current binary disease classification (Actinic Keratosis and Psoriasis)? The methodology restricts the problem to three classes (two diseases + normal), whereas the literature review notes existing frameworks that classify up to eleven different skin conditions. It is undetermined if the specific "modified top layers" are robust enough to distinguish a wider array of dermatological pathologies without significant degradation in precision. Re-training the architecture on a dataset containing >10 disease classes and analyzing the resulting confusion matrix and per-class accuracy would resolve this.

### Open Question 3
Is the VGG16 architecture the most efficient choice for deployment given its computational heaviness compared to modern alternatives? The paper acknowledges that VGG16 is "computationally demanding, possessing a substantial number of parameters... in comparison to newer designs such as ResNet and MobileNet." While the authors achieved high accuracy, they did not evaluate if similar accuracy could be achieved with more efficient models, which is critical for the mentioned goal of "accessible" diagnosis. A comparative ablation study measuring inference time and resource usage against lighter models like EfficientNet or MobileNet on the same dataset would resolve this.

## Limitations
- Model performance untested on real-world imbalanced datasets reflecting actual disease prevalence
- No external validation on independent clinical datasets from multiple hospitals
- Limited architectural comparison with more efficient modern CNN alternatives

## Confidence

- **High Confidence**: VGG16 transfer learning framework and overall methodology (well-established in computer vision literature)
- **Medium Confidence**: Class-specific performance metrics (precision/recall/F1) given balanced test set
- **Medium Confidence**: Data augmentation contribution (supported by ablation but no comparative analysis)

## Next Checks

1. Test model performance on an imbalanced dataset reflecting actual disease prevalence to assess robustness
2. Validate on external clinical datasets from multiple hospitals to evaluate real-world generalization
3. Compare against alternative CNN architectures (ResNet50, Inception v3) with same transfer learning protocol to benchmark relative performance