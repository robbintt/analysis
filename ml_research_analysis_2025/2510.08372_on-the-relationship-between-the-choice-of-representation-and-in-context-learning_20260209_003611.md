---
ver: rpa2
title: On the Relationship Between the Choice of Representation and In-Context Learning
arxiv_id: '2510.08372'
source_url: https://arxiv.org/abs/2510.08372
tags:
- learning
- label
- accuracy
- demonstrations
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores the relationship between representation and
  learning in in-context learning (ICL). The authors propose that ICL performance
  depends on two independent factors: the choice of label representation and the ability
  to learn from demonstrations.'
---

# On the Relationship Between the Choice of Representation and In-Context Learning

## Quick Facts
- arXiv ID: 2510.08372
- Source URL: https://arxiv.org/abs/2510.08372
- Authors: Ioana Marinescu; Kyunghyun Cho; Eric Karl Oermann
- Reference count: 40
- Primary result: ICL performance is determined by label representation quality (baseline accuracy) and learning efficiency from demonstrations, with consistent relative ranking across demonstration counts

## Executive Summary
This study investigates how the choice of label representation affects in-context learning (ICL) performance. The authors propose that ICL outcomes depend on two independent factors: the semantic quality of label representations and the model's ability to learn from demonstrations. Through an optimization algorithm that generates label sets with varying semantic relevance for sentiment classification, they demonstrate that representation quality sets the baseline accuracy while learning from demonstrations improves performance above this baseline. The findings reveal that larger models learn more efficiently from demonstrations, but learning occurs regardless of label set quality, with efficiency depending on both representation and model size.

## Method Summary
The authors developed an optimization algorithm to generate label sets with varying semantic relevance for sentiment classification tasks. They evaluated ICL performance using these label sets across different model sizes (1B, 8B, and 70B Llama models) and varying numbers of demonstrations. The methodology involved creating label sets optimized for semantic similarity or dissimilarity to the actual sentiment labels, then measuring zero-shot and few-shot ICL performance. By comparing accuracy across different label set qualities and demonstration counts, they could isolate the effects of representation quality versus learning efficiency.

## Key Results
- Label representation quality determines the baseline accuracy of ICL performance
- Learning from demonstrations improves accuracy above the baseline, with larger models learning more efficiently
- The relative ranking of label sets in terms of performance remains consistent across different numbers of demonstrations
- Learning occurs regardless of label set quality, but efficiency depends on both representation quality and model size

## Why This Works (Mechanism)
None provided

## Foundational Learning
- In-context learning (ICL): A learning paradigm where models learn from demonstrations without parameter updates during inference
  - Why needed: Understanding how models can adapt to new tasks without fine-tuning
  - Quick check: Verify model can perform new classification tasks given labeled examples

- Semantic similarity in label representations: The relationship between label tokens and their meaning in embedding space
  - Why needed: Labels with higher semantic relevance should provide better performance baselines
  - Quick check: Measure cosine similarity between label embeddings and true sentiment representations

- Optimization algorithms for label generation: Methods to create label sets with controlled semantic properties
  - Why needed: Allows systematic study of how different representations affect learning
  - Quick check: Validate generated labels have expected semantic properties through human evaluation

## Architecture Onboarding
Component map: Optimization Algorithm -> Label Set Generation -> ICL Evaluation -> Performance Analysis
Critical path: Generate semantically optimized labels → Measure baseline accuracy → Add demonstrations → Measure learning efficiency
Design tradeoffs: Single-token labels vs. multi-token representations; semantic relevance vs. task difficulty
Failure signatures: Inconsistent ranking across demonstration counts would indicate confounding factors; no baseline effect would suggest representation doesn't matter
First experiments:
1. Replicate zero-shot accuracy measurements with different label set qualities
2. Test learning curves across demonstration counts for each label set
3. Compare performance across model sizes to verify learning efficiency differences

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the orthogonality between representation quality and learning efficiency observed in classification tasks hold for complex reasoning tasks, such as mathematical problem solving or code generation?
- Basis in paper: [explicit] The conclusion states that extending these findings to more complex reasoning tasks "could offer a more nuanced understanding about memorization vs. reasoning in LLMs."
- Why unresolved: The experiments were restricted to 3-way and 5-way sentiment classification; reasoning tasks involve multi-step logic rather than single-token categorization.
- What evidence would resolve it: Replicating the optimization protocol (generating semantically relevant vs. irrelevant "labels" or instructions) on reasoning benchmarks like GSM8K or HumanEval to see if the baseline/learning decoupling persists.

### Open Question 2
- Question: How does the relationship between representation and learning change when label representations are expanded from single tokens to multi-token phrases or natural language definitions?
- Basis in paper: [inferred] The methodology (Section 3.2) explicitly restricts the label space to "a single token in the vocabulary" to optimize the objective function, leaving the impact of complex, multi-token representations unexplored.
- Why unresolved: Single tokens have distinct prior probabilities in the model's softmax layer; phrases distribute probability differently, potentially altering the "baseline" effect observed.
- What evidence would resolve it: Adapting the optimization algorithm to allow multi-token candidates and evaluating if the ranking consistency of label sets is maintained across demonstration counts.

### Open Question 3
- Question: Does the independent effect of representation on baseline accuracy persist in generative tasks with open-ended output spaces, or is it specific to the bounded label space of classification?
- Basis in paper: [inferred] The paper focuses exclusively on classification (sentiment), formulating ICL as `arg max p(y|x, D)` where `y` is a class from a fixed set `C`.
- Why unresolved: Generative tasks lack a fixed set `C`, so the concept of a "semantically relevant label set" determining a baseline may not translate directly to semantic relevance of output prefixes or styles.
- What evidence would resolve it: Applying the representation optimization concept to generative tasks (e.g., summarization style or translation formality) to measure if representation quality similarly caps the performance baseline.

## Limitations
- Study focuses exclusively on sentiment classification, limiting generalizability to other NLP tasks
- Optimization algorithm for generating label sets is not publicly available, making exact replication challenging
- Does not explore impact of varying label set sizes beyond tested configurations
- Does not investigate how different task complexities interact with representation quality

## Confidence
- Relationship between label representation and ICL baseline accuracy: **High**
- Learning from demonstrations improves accuracy above baseline: **High**
- Relative ranking of label sets remains consistent across demonstration counts: **Medium**
- Model size affects learning efficiency: **High**

## Next Checks
1. Replicate the study across multiple task types (e.g., text classification, relation extraction, question answering) to test generalizability
2. Conduct ablation studies varying label set sizes to understand optimal configuration ranges
3. Implement the optimization algorithm and test with alternative semantic similarity metrics to verify robustness of results