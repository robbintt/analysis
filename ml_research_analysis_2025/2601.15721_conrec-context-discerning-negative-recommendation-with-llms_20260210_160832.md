---
ver: rpa2
title: 'CoNRec: Context-Discerning Negative Recommendation with LLMs'
arxiv_id: '2601.15721'
source_url: https://arxiv.org/abs/2601.15721
tags:
- negative
- feedback
- user
- item
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CoNRec introduces the first large language model framework specifically\
  \ designed to model users\u2019 negative preferences in recommendation systems.\
  \ Unlike existing methods that primarily use negative feedback as an auxiliary signal,\
  \ CoNRec directly addresses the challenges of modeling sparse negative feedback\
  \ data and distinguishing true negative preferences from noise influenced by recommendation\
  \ systems."
---

# CoNRec: Context-Discerning Negative Recommendation with LLMs

## Quick Facts
- arXiv ID: 2601.15721
- Source URL: https://arxiv.org/abs/2601.15721
- Reference count: 39
- Key outcome: First LLM framework to directly model users' negative preferences in recommendation, achieving 11.5% HR@20 and 13.7% FHR@20 improvements over baselines on Taobao data

## Executive Summary
CoNRec introduces a novel large language model framework specifically designed to address the challenge of modeling users' negative preferences in recommendation systems. Unlike existing approaches that treat negative feedback as an auxiliary signal, CoNRec directly tackles the sparsity and noise inherent in negative feedback data by using hierarchical semantic ID representations, item-level alignment tasks, and progressive training paradigms. The framework demonstrates significant improvements in predicting negative user feedback while maintaining strong performance on long-tail users and items, addressing a critical gap in current recommendation system research.

## Method Summary
CoNRec employs a three-stage approach: first, a multimodal RQ-VAE generates hierarchical semantic IDs from item features; second, the model undergoes bidirectional translation fine-tuning and item-level alignment pre-training via LoRA; finally, Progressive GRPO training optimizes the model using future-looking rewards based on 7-day negative feedback and collaborative signals. The framework uses Qwen3-14B as its backbone and is trained on Taobao user behavior data with explicit negative feedback, achieving state-of-the-art performance through its innovative treatment of negative feedback as a primary objective rather than an auxiliary signal.

## Key Results
- Achieves 11.5% improvement in Hit Ratio (HR@20) and 13.7% improvement in Future Hit Ratio (FHR@20) over previous methods
- Demonstrates even larger gains for long-tail users and items through specialized evaluation metrics (LUF@20/LIF@20)
- Introduces novel reward functions and evaluation metrics that extend beyond next-item prediction to future items and collaborative signals

## Why This Works (Mechanism)

### Mechanism 1
Replacing raw text with hierarchical Semantic IDs reduces context noise and enables generative item retrieval. The framework uses RQ-VAE to quantize multimodal item features into discrete, hierarchical tokens (e.g., `a_5b_2c_9`), compressing redundant textual data while preserving semantic similarity. If codebook size is too small or quantization error is high, distinct items collapse to the same ID, causing ambiguity in generation.

### Mechanism 2
Isolating item-level contrastive alignment prevents the model from being overwhelmed by long positive history sequences. CoNRec inserts an intermediate "Item-level Alignment" task (via LoRA tuning) before sequence modeling, forcing the model to learn specific semantic attributes of "negative preference" rather than just memorizing user IDs. If the alignment task data is noisy, the model learns spurious correlations, failing to generalize to real user dispreferences.

### Mechanism 3
Progressive training and future-looking rewards mitigate the bias of "next-item" prediction in negative feedback loops. CoNRec uses Progressive GRPO, starting with short negative-only sequences and expanding to full context, with rewards based on similarity to future (7-day) negative feedback and collaborative signals rather than just the immediate next interaction. If the future horizon is too long, user preferences may drift, making the "future" ground truth irrelevant to the current prediction.

## Foundational Learning

- Concept: **Residual Quantization (RQ-VAE)**
  - Why needed here: You must understand how continuous vectors (images/text) are mapped to discrete tokens (Semantic IDs) to debug why an item might be generated or ignored.
  - Quick check question: Can you explain how a 3-level codebook hierarchy represents an item differently than a single-level codebook?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The core training loop uses GRPO (a variant of PPO/RLHF) to optimize the generation policy against a reward model.
  - Quick check question: How does GRPO differ from standard Supervised Fine-Tuning (SFT) in terms of objective function and data requirements?

- Concept: **Collaborative Filtering (Swing Algorithm)**
  - Why needed here: The reward function uses "collaborative items" derived via Swing to identify items frequently disliked by similar users.
  - Quick check question: What does the Swing algorithm measure that simple co-occurrence counts do not?

## Architecture Onboarding

- Component map: Multimodal Encoder -> RQ-VAE -> Semantic ID -> LLM Backbone -> Alignment Module (LoRA) -> GRPO Trainer
- Critical path: The RQ-VAE Codebook is the linchpin. If poorly trained (low reconstruction quality), Semantic IDs are meaningless, causing both alignment task and GRPO reward to fail.
- Design tradeoffs: Generative vs. Discriminative (Semantic IDs allow generation but rely on codebook mapping); Noise vs. Signal (progressive approach balances context with bias).
- Failure signatures: Mode Collapse (generates same popular ID), Over-suppression (false positive negatives), Reward Hacking (maximizes reward with nonsensical items).
- First 3 experiments:
  1. Reconstruction Validation: Verify RQ-VAE reconstructs item embeddings from Semantic IDs with high cosine similarity (>0.9).
  2. Context Ablation: Train with "Negative Only" vs. "Negative + Positive" context to replicate 45-50% performance drop.
  3. Reward Sensitivity: Test standard "next-item" reward vs. proposed "future-item" reward to measure reduction in prediction noise.

## Open Questions the Paper Calls Out

### Open Question 1
Can CoNRec effectively infer negative preferences for users with absolutely no historical negative feedback? The current framework relies on aligning historical negative items with semantic IDs; without a negative anchor point, the item-level alignment task cannot easily establish a boundary for "dislike" versus "neutral." Appendix D states the model "performs poorly for users who have no historical negative feedback at all and only have historical positive feedback."

### Open Question 2
What are the performance and latency impacts of deploying CoNRec in a live, real-time production environment? Section 6 explicitly lists this as a future direction: "Future work will focus on investigating the feasibility and effectiveness of deploying negative-feedback-aware models in real-world production environments." The current evaluation relies on offline metrics and a simulated offline filtering pipeline.

### Open Question 3
Is the 7-day "future window" used for ground truth and reward calculation optimal across domains with differing user behavior frequencies? The authors empirically select a 7-day horizon to improve ground truth coverage from 17% to 48%, but provide no validation that this window is robust for non-ecommerce settings or varying user activity levels.

## Limitations

- Dataset Access: Primary performance claims rely on proprietary Taobao user behavior data with specific filtering criteria not fully specified, limiting external validation.
- RQ-VAE Training Details: Critical hyperparameters like quantization loss weight and multimodal encoder architecture are not provided, making exact reproduction challenging.
- Reward Function Specificity: Exact Swing algorithm parameters and threshold settings are unspecified, making the reward calculation opaque and difficult to replicate.

## Confidence

- High Confidence: Core architectural design (Semantic IDs + Progressive GRPO) is technically sound with well-designed ablation studies.
- Medium Confidence: Reported performance gains are impressive but depend on proprietary dataset; long-tail improvements are plausible but need independent verification.
- Low Confidence: Exact RQ-VAE training procedure and full Swing algorithm implementation are critical unknowns that could significantly impact reproducibility.

## Next Checks

1. **RQ-VAE Reconstruction Quality**: Implement and train RQ-VAE on a public multimodal dataset (e.g., Amazon product images + titles). Measure cosine similarity between original and reconstructed item embeddings to ensure semantic information preservation.

2. **Context Ablation Replication**: On a public dataset with explicit negative feedback (e.g., Yelp reviews with 1-star ratings), train two models: (a) Negative-only context, and (b) Negative + Positive context (non-progressive). Compare performance to verify the 45-50% degradation mentioned in the motivational study.

3. **Reward Function Ablation**: Using the same public dataset, train models with three different reward functions: (a) Standard next-item reward, (b) Future 7-day reward (as proposed), and (c) Future 7-day + collaborative reward. Measure reduction in false positive negative predictions to quantify benefit of proposed reward design.