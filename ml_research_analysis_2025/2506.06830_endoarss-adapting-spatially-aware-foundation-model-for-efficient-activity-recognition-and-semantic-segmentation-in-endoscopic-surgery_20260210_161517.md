---
ver: rpa2
title: 'EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity
  Recognition and Semantic Segmentation in Endoscopic Surgery'
arxiv_id: '2506.06830'
source_url: https://arxiv.org/abs/2506.06830
tags:
- segmentation
- learning
- surgical
- tasks
- endoarss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EndoARSS introduces a multi-task learning framework for endoscopic
  surgical activity recognition and semantic segmentation, built on the DINOv2 foundation
  model. It employs Task Efficient Shared Low-rank Adapters (TESLA) to mitigate gradient
  conflicts between tasks and Spatially-Aware Multi-Scale Attention (SMA) to enhance
  feature discrimination in complex surgical environments.
---

# EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery

## Quick Facts
- **arXiv ID:** 2506.06830
- **Source URL:** https://arxiv.org/abs/2506.06830
- **Reference count:** 40
- **Primary result:** Achieves 92.16% mIoU on MTLESD and 72.95% on MTLEndovis using multi-task learning with DINOv2 backbone

## Executive Summary
EndoARSS introduces a multi-task learning framework for simultaneous surgical activity recognition and semantic segmentation in endoscopic surgery. Built on the DINOv2 foundation model, it employs Task Efficient Shared Low-rank Adapters (TESLA) to mitigate gradient conflicts between tasks and Spatially-Aware Multi-Scale Attention (SMA) to enhance feature discrimination in complex surgical environments. The framework is evaluated on three novel datasets, demonstrating superior performance over single-task baselines while maintaining strong generalization under image corruptions.

## Method Summary
EndoARSS combines DINOv2 as a frozen backbone with task-specific low-rank adapters (TESLA) and spatial attention enhancement (SMA). The framework processes both classification and segmentation tasks simultaneously through dual decoder heads. TESLA isolates task-specific gradients using low-rank decomposition to prevent interference, while SMA enhances pixel-level attention through multi-scale spatial context encoding. The model is trained with Adam optimizer for 200 epochs using task-weighted losses.

## Key Results
- Achieves 92.16% mIoU on MTLESD and 72.95% on MTLEndovis datasets
- Maintains strong performance under various image corruptions
- Runs at 35 FPS on RTX 4090 GPU
- Demonstrates superior performance over single-task learning baselines

## Why This Works (Mechanism)

### Mechanism 1: TESLA Gradient Isolation
Task Efficient Shared Low-rank Adapters (TESLA) mitigate gradient conflicts between heterogeneous tasks through explicit parameter isolation. Each task receives dedicated low-rank adapter modules (A_i, B_i) that compute task-specific gradients while sharing the frozen backbone. The weighted combination `h = (W' + Σα_i B_i A_i) x_t` allows gradient separation per task identifier, preventing interference in shared representation space. Core assumption: Gradient conflicts arise from data heterogeneity across tasks. Break condition: If tasks are highly correlated, explicit isolation may reduce beneficial transfer (ablation shows 0.28-4.59% mIoU reduction without TESLA).

### Mechanism 2: SMA Spatial Context Encoding
Spatially-Aware Multi-Scale Attention (SMA) enhances discrimination of visually similar features through cross-spatial global context encoding. SMA unfolds input into non-overlapping patches, applies multi-scale attention with parallel 1×1 and 3×3 branches capturing channel and spatial dependencies, then folds back. This provides an H×W receptive field per pixel, encoding both local and global context with spatial inductive bias. Core assumption: Feature ambiguity stems from high visual similarity; global spatial context helps disambiguate. Break condition: Ablation shows SMA contributes 0.68-4.67% mIoU improvement; removal causes performance drop but model remains functional.

### Mechanism 3: MTL Feature Sharing
Multi-task learning improves segmentation performance through shared feature interactions. A unified encoder-decoder architecture processes both tasks simultaneously; classification provides scene-level context that constrains segmentation predictions, reducing cross-activity confusion. Core assumption: Tasks share complementary information; classification provides semantic context beneficial for pixel-level prediction. Break condition: If tasks are unrelated or conflicting, MTL can degrade performance. Figure 2 shows some classes perform similarly across tasks.

## Foundational Learning

- **Low-Rank Adaptation (LoRA):** EndoARSS freezes DINOv2's 1B parameters and only trains lightweight LoRA layers (rank-4 decomposition), reducing memory from prohibitive to 9858MiB training / 1668MiB inference. Quick check: Can you explain why `ΔW ≈ BA` where B and A have rank r << min(d_in, d_out)`?

- **Vision Transformers (ViT) with Self-Attention:** DINOv2 backbone is a ViT; understanding patch embeddings, positional encodings, and multi-head self-attention is prerequisite to understanding where LoRA and TESLA attach (q, v projection layers). Quick check: How does self-attention compute relationships between all patch positions simultaneously?

- **Multi-Task Learning Optimization Conflicts:** Gradient conflicts occur when different tasks' loss gradients point in conflicting directions in shared parameter space; TESLA addresses this via explicit separation. Quick check: Why might minimizing L_class + L_seg simultaneously cause worse performance on either task compared to training separately?

## Architecture Onboarding

- **Component map:**
  Input Image → [SMA: Conv-n×n → Conv-1×1 → Unfold → Multi-Scale Attention → Fold → Conv-1×1] → DINOv2 ViT Encoder (frozen) with LoRA in q,v projections + TESLA (task-specific adapters) → [CLS token] → Classification Decoder → Activity Label → [Patch tokens] → Segmentation Decoder → Segmentation Mask

- **Critical path:**
  1. SMA preprocessing (enables global receptive field before ViT)
  2. Frozen DINOv2 with LoRA+TESLA (core feature extraction, task-specific adaptation)
  3. Dual decoder heads (task outputs)
  4. Joint loss optimization with gradient separation via TESLA

- **Design tradeoffs:**
  - Rank size: Table VI shows rank=4 optimal; rank=1 underfits, rank≥8 introduces noise
  - Frozen vs. full fine-tuning: Freezing reduces memory but limits domain adaptation depth
  - TESLA vs. shared LoRA: Task isolation improves performance (~0.68-4.59% mIoU) but doubles adapter parameters

- **Failure signatures:**
  - Gradient conflicts manifest as unstable training or task imbalance
  - SMA failure mode: Over-smoothing if unfold parameters misconfigured (patch size P too large)
  - Domain shift: MTLEndovis-Gen shows 23% mIoU drop from MTLEndovis (72.95% → 49.10%), indicating generalization limits

- **First 3 experiments:**
  1. Baseline reproduction: Train single-task segmentation with frozen DINOv2 + vanilla LoRA (no TESLA, no SMA) on MTLESD; verify ~88% mIoU
  2. Ablation by component: Add TESLA, then SMA independently; confirm each contributes ~1-4% mIoU improvement
  3. Rank sensitivity: Sweep rank ∈ {1, 4, 8, 16} on MTLEndovis; verify rank=4 peak performance

## Open Questions the Paper Calls Out
1. **Computational optimization:** How can the EndoARSS framework be optimized to reduce computational costs and memory consumption for real-time applicability in resource-constrained clinical settings? The authors aim to explore lightweight architectures and adaptive training mechanisms to enhance real-time applicability.

2. **Cross-modal generalization:** Can the EndoARSS framework generalize effectively to other surgical modalities and tasks beyond endoscopic submucosal dissection and instrument segmentation? The authors seek to extend clinical applications to diverse surgical domains.

3. **Complex interaction stability:** How can the model's performance be stabilized for intricate tool-tissue interactions where current methods exhibit occasional accuracy drops? The authors plan to integrate temporal or geometric inductive biases to improve performance on complex interaction frames.

## Limitations
- TESLA mechanism lacks direct validation against standard LoRA baselines in ablations
- Significant performance drop (72.95% → 49.10% mIoU) on generalization dataset suggests domain adaptation limitations
- Claims about robustness under image corruptions lack specification of corruption types and severity levels tested

## Confidence
- **High confidence:** SMA module effectiveness and overall MTL framework superiority over single-task baselines
- **Medium confidence:** TESLA's gradient conflict mitigation mechanism (mechanism described but not directly validated; gains are incremental)
- **Low confidence:** Claims about robustness under image corruptions without specifying corruption types or severity levels tested

## Next Checks
1. **TESLA ablation comparison:** Implement EndoARSS without TESLA using standard shared LoRA across both tasks, then compare performance to verify gradient isolation benefits claimed (~0.68-4.59% mIoU improvement)

2. **Domain adaptation evaluation:** Test framework on additional cross-dataset scenarios beyond MTLEndovis-Gen to quantify generalization limits and identify failure modes

3. **Corruption robustness validation:** Apply standardized corruption suites (e.g., ImageNet-C) to evaluate robustness claims, specifying corruption types, severity levels, and performance metrics across the full corruption spectrum