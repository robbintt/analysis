---
ver: rpa2
title: 'IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information
  Entropy for EEG Classification of Neurological Disorders'
arxiv_id: '2509.15259'
source_url: https://arxiv.org/abs/2509.15259
tags:
- gradients
- feature
- iefs-gmb
- information
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces IEFS-GMB, an Information Entropy-based Feature
  Selection method guided by a Gradient Memory Bank for EEG classification of neurological
  disorders. It addresses the challenge of low signal-to-noise ratio in EEG signals
  by optimizing feature selection for deep learning models.
---

# IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders

## Quick Facts
- **arXiv ID**: 2509.15259
- **Source URL**: https://arxiv.org/abs/2509.15259
- **Reference count**: 6
- **Primary result**: Information Entropy-based Feature Selection guided by a Gradient Memory Bank improves EEG classification accuracy by 0.64% to 6.45% across four public neurological disorder datasets.

## Executive Summary
IEFS-GMB addresses the challenge of low signal-to-noise ratio in EEG signals by introducing a gradient memory bank-guided feature selection method for deep learning models. The approach stores historical gradients from multiple training iterations and uses information entropy to calculate feature importance, providing more stable and comprehensive feature selection than single-iteration methods. Experiments demonstrate consistent accuracy improvements across four public EEG datasets when integrated into diverse encoder architectures including CNN-based SpikeNet and transformer-based TimeXer and iTransformer models.

## Method Summary
IEFS-GMB is a plug-and-play feature weighting module that can be integrated at multiple depths of diverse encoder architectures for EEG classification. The method uses a queue-based memory bank to store sampled gradients from previous mini-batches, retrieves top-K most similar historical gradients via cosine similarity, and applies decay rate γ to balance recent and historical information. Information entropy of activation probabilities quantifies feature importance, with lower entropy indicating more predictive features. The entropy-derived weighting coefficients are applied via linear weighting with residual connection to refine feature representations before classification.

## Key Results
- Accuracy improvements of 0.64% to 6.45% over baseline models across four public EEG datasets
- Outperforms four competing feature selection techniques including attention mechanisms and ADSEL
- Demonstrates robust integration across CNN and transformer architectures with consistent improvements
- Enhances model interpretability through entropy-based feature importance quantification

## Why This Works (Mechanism)

### Mechanism 1
Historical gradients from multiple training iterations provide a more stable and comprehensive view of feature importance than single-iteration gradients. A queue-based memory bank stores sampled gradients from the previous q mini-batches, retrieves top-K most similar historical gradients via cosine similarity, and applies decay rate γ so older gradients contribute less. Experiments show m=0.2 outperforms both m=0 and m=1 configurations.

### Mechanism 2
Information entropy of activation probabilities quantifies feature importance for EEG classification, where lower entropy indicates more predictive features. For each local feature representation, a probability distribution is computed via Grad-CAM-style heating map using accumulated gradients, and information entropy measures uncertainty. Features with lower entropy receive higher weighting coefficients.

### Mechanism 3
A plug-and-play feature weighting module can be integrated at multiple depths of diverse encoder architectures to improve classification accuracy. Entropy-derived weighting coefficients are applied via linear weighting to heating maps with residual connection, preserving original feature information while emphasizing informative dimensions. Consistent improvements observed across shallow, middle, and deep layer integrations.

## Foundational Learning

- **Concept: Gradients in Backpropagation**
  - **Why needed here**: The method is fundamentally based on extracting, storing, and reusing gradients from training iterations. Understanding what gradients represent is essential to grasp why historical gradients could indicate feature importance.
  - **Quick check question**: Can you explain why a feature with consistently large gradient magnitudes across iterations might be more "important" than one with small or unstable gradients?

- **Concept: Information Entropy**
  - **Why needed here**: The core feature selection criterion uses Shannon entropy to measure prediction confidence. Lower entropy indicates the model is more certain about its output given a particular feature.
  - **Quick check question**: Given a probability distribution [0.9, 0.1], would you expect its entropy to be higher or lower than [0.5, 0.5]? Which distribution suggests a more "informative" feature?

- **Concept: Grad-CAM and Heating Maps**
  - **Why needed here**: The probability calculation in IEFS-GMB adapts Grad-CAM's approach: using gradients flowing into a layer to generate a "heating map" that highlights important regions.
  - **Quick check question**: How does Grad-CAM differ from raw gradient visualization in terms of what it highlights in a neural network's decision process?

## Architecture Onboarding

- **Component map**: Input EEG clips xi → Encoder E producing feature representation h → Gradient Memory Bank storing historical gradients → IEFS Module computing heating maps and entropy weights → Feature refinement with residual connection → Classification Head producing logits y

- **Critical path**: 
  1. Forward pass through encoder to layer l → feature representation h
  2. Backward pass extracts gradients Gj-1
  3. GMB updates: enqueue new gradients, sample top-K similar historical gradients, apply decay γ
  4. IEFS computes heating maps, entropy H(pr), and weighting λr
  5. Feature refinement: hfinal = h + {λr · vr}
  6. Continue through remaining encoder/classifier → loss → repeat

- **Design tradeoffs**:
  - Memory bank size (q): Larger q provides more historical context but increases memory; optimal q=8 in experiments
  - Top-K samples: K=1 performed best; higher K adds redundancy without gains
  - Momentum (m): Controls balance between historical (m→1) and recent (m→0) gradients; optimal m=0.2
  - Decay rate (γ): Must stay <0.5; higher values over-weight distant iterations
  - Integration depth: Middle layers generally performed best, but improvements observed across all depths

- **Failure signatures**:
  - Accuracy degradation vs. baseline: Check if m=0 or m=1 configurations underperform m=0.2
  - No improvement despite correct hyperparameters: Verify insertion layer has sufficient spatial-temporal resolution
  - Memory overflow: Reduce q or batch size; bank stores (q×b, c, h, w) gradients
  - NaN values in entropy: Check for zero probabilities in activation outputs

- **First 3 experiments**:
  1. Baseline validation: Run chosen encoder on TUSZ or TUEV without IEFS-GMB; confirm baseline accuracy (e.g., 78.50% for TimeXer on TUSZ)
  2. Hyperparameter sweep: Fix encoder and dataset; vary q ∈ {4, 8, 12}, K ∈ {1, 2, 4}, m ∈ {0.1, 0.2, 0.5}, γ ∈ {0.1, 0.3, 0.5}
  3. Layer depth comparison: Integrate IEFS-GMB at shallow, middle, and deep layers of same encoder; compare accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Does simultaneously applying IEFS-GMB to multiple layers of the EEG encoder yield further performance gains compared to single-layer application? The methodology restricts operation to "a specified layer l" and shows performance varies significantly by layer depth, but cascading across multiple stages remains untested.

### Open Question 2
What is the computational overhead (training time and memory footprint) introduced by the Gradient Memory Bank relative to baseline models? While accuracy improves, the resource cost of maintaining the memory bank and performing gradient sampling is not quantified.

### Open Question 3
Can the criterion for sampling historical gradients be optimized beyond cosine similarity to improve feature selection robustness? The paper acknowledges cosine similarity was chosen based on belief that recent gradients have most significant effect, but alternative sampling strategies remain untested.

## Limitations
- Entropy-based feature weighting mechanism lacks direct validation from corpus papers specifically for EEG applications
- Performance highly sensitive to hyperparameter choices with specific configurations required for optimal results
- Preprocessing pipeline for TUEV dataset mentions fixing labeling errors without providing exact scripts

## Confidence
- **High Confidence**: Gradient memory bank mechanism supported by ablation study showing m=0.2 outperforms alternatives
- **Medium Confidence**: Entropy-based feature importance calculation has theoretical grounding but lacks EEG-specific corpus validation
- **Medium Confidence**: Plug-and-play integration approach demonstrated across architectures but optimal depth varies significantly

## Next Checks
1. **Gradient Memory Bank Validation**: Implement ablation tests comparing m=0, m=1, and m=0.2 on a single dataset to verify critical importance of balancing historical and recent gradients
2. **Entropy Weighting Sensitivity**: Test alternative probability calculation methods (Sigmoid vs Softmax) on binary classification to determine activation function impact on performance
3. **Layer Integration Robustness**: Systematically test IEFS-GMB integration at shallow, middle, and deep layers across multiple architectures on same dataset to confirm architecture-specific optimal integration points