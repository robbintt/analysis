---
ver: rpa2
title: 'PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box
  LLMs'
arxiv_id: '2510.25808'
source_url: https://arxiv.org/abs/2510.25808
tags:
- optimization
- score
- soft
- instruction
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of optimizing instructions for black-box
  large language models (LLMs) whose internal parameters are inaccessible. The core
  method, PRESTO, reinterprets the many-to-one mapping between soft prompts and instructions
  as a useful structure rather than a redundancy.
---

# PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs

## Quick Facts
- arXiv ID: 2510.25808
- Source URL: https://arxiv.org/abs/2510.25808
- Authors: Jaewon Chu; Seunghun Lee; Hyunwoo J. Kim
- Reference count: 40
- Key outcome: PRESTO achieves state-of-the-art performance on 33 tasks by obtaining 14× more scored data under the same query budget through preimage-informed optimization

## Executive Summary
PRESTO addresses the challenge of optimizing instructions for black-box large language models (LLMs) by reinterpreting the many-to-one mapping between soft prompts and instructions as a useful structure rather than a redundancy. The method leverages preimage analysis to propagate scores across soft prompts mapping to the same instruction, enabling more efficient instruction optimization without additional black-box queries. By combining score sharing, preimage-based initialization, and score consistency regularization, PRESTO achieves significant improvements in both sample efficiency and final task performance compared to existing approaches.

## Method Summary
PRESTO builds on the NeuralUCB framework for instruction optimization, introducing three key innovations that exploit the preimage structure of soft prompts. First, it shares evaluation scores across all soft prompts mapping to the same instruction, effectively multiplying the amount of labeled data available for training the score predictor. Second, it uses a greedy algorithm to select initial queries that maximize coverage of the soft prompt embedding space, ensuring diverse exploration. Third, it adds a consistency regularization term that enforces identical score predictions for soft prompts within the same preimage, encoding structural knowledge into the learning process. The method requires a white-box LLM to generate candidate instructions from soft prompts, then uses these to construct preimages and guide the optimization process for the black-box target LLM.

## Key Results
- PRESTO achieves state-of-the-art performance across 33 tasks, including 30 instruction induction and 3 arithmetic reasoning tasks
- The method obtains 14× more scored data points (2,300 vs 165) under the same 165-query budget through score sharing
- Experimental results show PRESTO improves instruction optimization efficiency while maintaining or exceeding baseline accuracy
- Preimage-based initialization achieves denser and more comprehensive coverage of the search space compared to random initialization

## Why This Works (Mechanism)

### Mechanism 1: Preimage-Based Score Sharing Amplifies Query Efficiency
Evaluating one soft prompt provides supervisory signal for all prompts in its preimage without additional black-box queries. When multiple soft prompts map to the same instruction, querying the black-box LLM once yields a score that applies to the entire preimage set. This transforms the many-to-one mapping from a redundancy problem into a data augmentation strategy, effectively enlarging the training data for the score predictor without additional calls to the black-box LLMs.

### Mechanism 2: Preimage-Based Initialization Maximizes Embedding Space Coverage
Selecting initial queries based on preimage structure improves score predictor training by ensuring diverse coverage of the soft prompt embedding space. The method computes a coverage score combining representativeness via MMD between selected and total embeddings, and preimage size. A greedy algorithm iteratively selects preimages that maximize coverage, ensuring initial data spans the search space rather than clustering in narrow regions.

### Mechanism 3: Score Consistency Regularization Enforces Preimage Structure as Inductive Bias
Regularizing the score predictor to produce identical predictions within unscored preimages improves generalization by encoding known structure. The loss adds an unsupervised term for all z, z' in the same unscored preimage, constraining the predictor to respect the many-to-one mapping even before those preimages receive ground-truth scores. A linear warmup schedule prevents premature convergence to incorrect predictions.

## Foundational Learning

- **Soft Prompts vs. Hard Prompts**: Why needed here - The method optimizes continuous embeddings rather than discrete text tokens, enabling gradient-free optimization but introducing the many-to-one mapping problem. Quick check: Can you explain why optimizing a 10-dimensional continuous vector is tractable while optimizing a 10-token discrete sequence is combinatorially hard?

- **Neural Upper Confidence Bound (NeuralUCB)**: Why needed here - PRESTO builds on INSTINCT's NeuralUCB framework for exploration-exploitation trade-offs, where a neural network predicts scores and uncertainty estimates guide query selection. Quick check: How does the UCB formula balance predicted mean score against uncertainty, and why does this matter for sample-efficient optimization?

- **Preimage (Mathematical)**: Why needed here - The core insight reinterprets f_w^(-1)(v) = {z : f_w(z) = v} as useful structure rather than redundancy. Understanding set-valued inverse mappings is essential. Quick check: Given a function f: A → B, what is the preimage of a singleton {b} ∈ B, and how does its size relate to whether f is injective?

## Architecture Onboarding

- **Component map**: Preprocessing (Sobol samples → white-box instructions → embeddings → preimages) → Initialization (greedy coverage selection → black-box evaluation → score sharing) → Optimization (MLP training + consistency + NeuralUCB → query selection → repeat)

- **Critical path**: Preprocessing (6.72 min for 10k candidates) → Initialization → Optimization iterations (~10 min). Preimage construction is one-time cost; MLP training per iteration is ~2 seconds.

- **Design tradeoffs**: Larger candidate sets (N) improve coverage but increase preprocessing time linearly. Higher intrinsic dimensions reduce duplicate instructions (more unique preimages) but may reduce score sharing benefits. Consistency regularization weight (γ=0.1) with warmup prevents early collapse.

- **Failure signatures**: Low unique instruction count (<50% of N) suggests intrinsic dimension too low. Score predictor RMSE not decreasing indicates incorrect preimage assignments or aggressive regularization. Final instructions worse than baselines suggests black-box evaluation issues.

- **First 3 experiments**: 
  1. Reproduce preimage size distribution (Figure 1b) on your target white-box LLM to confirm many-to-one structure exists
  2. Ablate score sharing alone on 3 tasks to isolate its contribution; expect ~7-8 point accuracy gain
  3. Run full PRESTO vs. INSTINCT comparison on held-out task from instruction induction benchmark, tracking both accuracy and wall-clock time

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, based on the limitations section and experimental scope, several implicit questions emerge regarding the method's robustness and generalization capabilities.

## Limitations

- The method assumes a deterministic white-box LLM for preimage construction, which may not hold in practice with temperature sampling
- Performance depends on the degree of instruction redundancy; low redundancy reduces score sharing benefits
- Computational overhead of preimage construction may outweigh benefits when candidate sets are small
- The 165-query budget may not be sufficient for tasks requiring highly specialized instructions

## Confidence

- **High confidence**: The 14× query efficiency improvement and consistent accuracy gains across 33 tasks are directly measurable outcomes with clear methodology
- **Medium confidence**: The theoretical justification for consistency regularization relies on assumptions about preimage structure stability that aren't fully validated
- **Low confidence**: Claims about preimage-based initialization's contribution are supported by qualitative visualizations but lack quantitative ablation studies

## Next Checks

1. **Preimage stability test**: Run preimage construction across 10 different seeds of the white-box LLM and measure Jaccard similarity between preimage sets to assess whether score sharing introduces noise from unstable mappings

2. **Budget sensitivity analysis**: Evaluate PRESTO's performance with query budgets of 50, 100, 165, and 250 on held-out tasks to reveal whether the 14× efficiency claim holds across different budget regimes

3. **Cross-model generalization**: Implement PRESTO using a different white-box LLM (e.g., Mistral-7B) for preimage construction while keeping the black-box model constant to test whether benefits depend on specific instruction generation characteristics