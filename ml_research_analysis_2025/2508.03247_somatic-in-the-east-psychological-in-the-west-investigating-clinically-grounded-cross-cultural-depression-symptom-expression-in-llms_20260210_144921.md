---
ver: rpa2
title: 'Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded
  Cross-Cultural Depression Symptom Expression in LLMs'
arxiv_id: '2508.03247'
source_url: https://arxiv.org/abs/2508.03247
tags:
- symptom
- cultural
- somatic
- eastern
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether LLMs reproduce culturally specific
  depression symptom patterns observed in clinical psychology, where Western individuals
  tend to report psychological symptoms and Eastern individuals report somatic ones.
  Using symptom selection tasks with cultural personas from Western and Eastern countries,
  the study found that LLMs largely fail to replicate these patterns when prompted
  in English.
---

# Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs

## Quick Facts
- arXiv ID: 2508.03247
- Source URL: https://arxiv.org/abs/2508.03247
- Reference count: 40
- Primary result: General-purpose LLMs largely fail to reproduce culturally specific depression symptom patterns observed in clinical psychology

## Executive Summary
This study evaluates whether large language models reproduce culturally specific depression symptom patterns where Western individuals report more psychological symptoms and Eastern individuals report more somatic symptoms. Using symptom selection tasks with cultural personas from Western and Eastern countries, the researchers found that LLMs generally fail to replicate these patterns when prompted in English. While local language prompting (Chinese, Japanese, Hindi) showed some improvement in alignment, results remained limited overall. The analysis revealed low sensitivity to cultural personas and a strong, culturally invariant symptom hierarchy that overrides cultural cues, indicating that current general-purpose LLMs lack the culture-aware capabilities essential for safe mental health applications.

## Method Summary
The researchers conducted controlled experiments using symptom selection tasks with fixed cultural personas from Western and Eastern countries. They tested multiple LLMs with various prompting strategies including English and local language prompts (Chinese, Japanese, Hindi). The study used discrete symptom selection rather than naturalistic language generation to assess whether models could reproduce clinically observed cross-cultural differences in depression symptom expression. Cultural personas were provided as context, and models were asked to select symptoms that would be most relevant for individuals from different cultural backgrounds.

## Key Results
- English-prompted LLMs showed low sensitivity to cultural personas in depression symptom selection tasks
- Local language prompting (Chinese, Japanese, Hindi) improved alignment in some configurations but not consistently
- Models exhibited a strong, culturally invariant symptom hierarchy that overrode cultural cues
- General-purpose LLMs largely failed to replicate clinically observed patterns of somatic symptoms in Eastern populations versus psychological symptoms in Western populations

## Why This Works (Mechanism)
Assumption: The observed failures stem from training data that underrepresents culturally specific symptom expression patterns, leading models to learn a universal symptom hierarchy that ignores cultural context. The lack of cultural sensitivity in symptom selection suggests that current LLMs have not internalized the clinical observations about how depression manifests differently across cultures, instead relying on more generic symptom associations that override cultural persona information.

## Foundational Learning
- Cross-cultural psychology in depression: Understanding how cultural background influences symptom expression is essential for developing culturally competent mental health tools
- Prompt engineering for cultural context: Local language prompting can sometimes improve cultural alignment but effects are inconsistent and depend on specific configurations
- Symptom hierarchy in LLMs: Models appear to prioritize certain symptoms regardless of cultural context, suggesting learned patterns that may not reflect clinical reality
- Discrete selection vs. natural language generation: The choice of evaluation method (selecting from lists vs. generating descriptions) may significantly impact observed cultural sensitivity

## Architecture Onboarding
**Component map:** Data preparation -> Prompt engineering -> Model inference -> Evaluation metrics -> Analysis
**Critical path:** Cultural persona creation → Symptom list preparation → Prompt construction → Model response generation → Cultural alignment scoring
**Design tradeoffs:** Using discrete symptom selection provides controlled measurement but may not capture naturalistic symptom expression; local language prompting improves alignment but requires multilingual model capabilities
**Failure signatures:** Low variance in symptom selection across cultural personas, consistent prioritization of certain symptoms regardless of cultural context, minimal improvement from local language prompting
**First experiments:**
1. Test specialized mental health-focused LLMs with the same cultural personas and symptom selection tasks
2. Compare discrete selection results with naturalistic symptom description prompts to assess method dependency
3. Conduct ablation studies varying symptom list composition and persona detail levels to identify key factors affecting cultural sensitivity

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, but the results suggest several areas requiring further investigation, including whether the observed limitations are fundamental to current LLM architectures or could be addressed through specialized fine-tuning, and how different evaluation methods might reveal different patterns of cultural sensitivity.

## Limitations
- Symptom selection task may not adequately capture clinically relevant symptom expression compared to naturalistic language generation
- Inconsistent effects of local language prompting suggest dependency on specific prompting strategies or model versions
- Strong symptom hierarchy could reflect training data limitations or architectural constraints rather than fundamental inability to represent cultural differences
- Study focuses only on general-purpose LLMs without examining specialized mental health models or fine-tuning approaches

## Confidence
- High confidence: English-prompted LLMs show low sensitivity to cultural personas in symptom selection tasks
- Medium confidence: General-purpose LLMs lack robust culture-aware capabilities, given limited scope to specific models and prompting strategies
- Medium confidence: Observed patterns reflect fundamental model limitations rather than task design issues

## Next Checks
1. Test the same cultural personas and symptom selection tasks with specialized mental health-focused LLMs or fine-tuned versions of the evaluated models
2. Replicate the experiments using naturalistic symptom description prompts rather than discrete selection to assess whether cultural differences emerge in generated language
3. Conduct ablation studies varying symptom list composition, persona detail levels, and prompting strategies to identify which factors most strongly influence cultural sensitivity