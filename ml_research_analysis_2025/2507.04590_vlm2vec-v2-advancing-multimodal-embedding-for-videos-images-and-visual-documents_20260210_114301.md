---
ver: rpa2
title: 'VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual
  Documents'
arxiv_id: '2507.04590'
source_url: https://arxiv.org/abs/2507.04590
tags:
- video
- visual
- tasks
- retrieval
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal embedding models
  being limited to natural images, restricting their applicability in real-world scenarios
  involving videos and visual documents. The authors propose VLM2Vec-V2, a unified
  framework for learning embeddings across diverse visual forms.
---

# VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents

## Quick Facts
- arXiv ID: 2507.04590
- Source URL: https://arxiv.org/abs/2507.04590
- Reference count: 21
- One-line primary result: VLM2Vec-V2 achieves 58.0 average score on 78 MMEB-V2 tasks, outperforming baselines by unifying video, image, and visual document embeddings

## Executive Summary
This paper addresses the limitation of existing multimodal embedding models that are primarily designed for natural images, restricting their applicability to real-world scenarios involving videos and visual documents. The authors propose VLM2Vec-V2, a unified framework that extends embedding capabilities across diverse visual forms including videos and visual documents. They introduce MMEB-V2, a comprehensive benchmark that expands the original MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification, and video question answering. VLM2Vec-V2 demonstrates strong performance across all modalities with an overall average score of 58.0 on 78 tasks, significantly outperforming prior baselines like GME, LamRA, and VLM2Vec.

## Method Summary
VLM2Vec-V2 uses a Qwen2-VL 2B backbone with LoRA (rank=16, α=32) fine-tuning, trained with contrastive learning using InfoNCE loss (temperature=0.02) on a unified dataset mixing video-caption pairs, video-QA, visual documents, and image-text data. The training employs interleaved sub-batching (global batch 1,024 split into 16 sub-batches of 64) with GradCache for memory efficiency, using instruction-guided formatting with modality-specific tokens. The model achieves embeddings from the last token's final layer and evaluates across MMEB-V2 benchmark tasks using Hit@1 for image/video tasks and NDCG@5 for visual document tasks.

## Key Results
- Achieves 58.0 average score across 78 tasks in MMEB-V2 benchmark
- Outperforms baselines (GME, LamRA, VLM2Vec) on unified multimodal embedding tasks
- Ablation studies show best performance when training on all three modalities (image, video, visual documents)
- Interleaved sub-batching improves cross-modal generalization compared to homogeneous batching

## Why This Works (Mechanism)
The paper doesn't explicitly detail the mechanism, but the unified training approach allows the model to learn shared representations across different visual modalities. The contrastive learning framework with instruction-guided formatting helps the model understand task-specific contexts while maintaining modality-agnostic embedding capabilities. The interleaved sub-batching strategy appears to improve cross-modal generalization by exposing the model to diverse data distributions within each training step.

## Foundational Learning
- **Contrastive Learning**: Needed to align multimodal representations by pulling together matching pairs and pushing apart non-matching pairs; quick check: verify embedding similarity for positive vs negative pairs
- **Instruction-guided formatting**: Required to provide task context to the vision-language model; quick check: test model response to different instruction templates
- **Interleaved sub-batching**: Essential for maintaining cross-modal diversity during training; quick check: monitor per-modality validation scores for batch composition effects
- **GradCache optimization**: Critical for enabling large batch training with limited GPU memory; quick check: verify gradient computation matches full-batch equivalent

## Architecture Onboarding
- **Component map**: Data loader -> Qwen2-VL 2B + LoRA -> InfoNCE loss -> GradCache optimizer -> embeddings
- **Critical path**: Input → Instruction formatting → Qwen2-VL forward pass → Contrastive loss → GradCache backward pass → Parameter update
- **Design tradeoffs**: Small 2B backbone vs. larger models (better efficiency but potentially limited capacity); interleaved sub-batching vs. full batch (better generalization but more complex implementation)
- **Failure signatures**: GPU OOM during training (indicates GradCache issues or sub-batch size too large); poor cross-modal performance (suggests insufficient modality mixing or instruction formatting issues)
- **3 first experiments**: 1) Test InfoNCE loss with temperature sweep on validation set, 2) Verify instruction formatting produces expected outputs across modalities, 3) Profile GradCache memory savings vs. standard training

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The ablation study comparing multi-modality vs. single-modality training may be confounded by dataset size differences
- The effectiveness of the interleaving strategy isn't fully characterized in terms of optimal sub-batch size
- The 2B backbone, while effective, may have limitations for scaling to more diverse real-world distributions
- The relative difficulty and distribution of new MMEB-V2 tasks across modalities isn't fully characterized

## Confidence
- High confidence: Training procedure implementation details (InfoNCE loss, GradCache, interleaved sub-batching) are clearly specified and reproducible
- Medium confidence: Claims about unified training improving cross-modal generalization are supported but could benefit from more extensive ablation studies
- Medium confidence: Benchmark extension with five new task types is valuable but task difficulty distribution isn't fully characterized

## Next Checks
1. Replicate the ablation study comparing single-modality vs. multi-modality training with matched dataset sizes to isolate the effect of training diversity from data quantity
2. Test the model's zero-shot generalization to completely unseen visual document formats (e.g., scientific papers, invoices) not represented in the training data
3. Evaluate the impact of different sub-batch sizes in the interleaved strategy (e.g., 32 vs 128) to determine the optimal balance between modality mixing and computational efficiency