---
ver: rpa2
title: 'STAR: Stability-Inducing Weight Perturbation for Continual Learning'
arxiv_id: '2503.01595'
source_url: https://arxiv.org/abs/2503.01595
tags:
- star
- learning
- forgetting
- buffer
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STAR, a stability-inducing weight perturbation
  technique for continual learning. The method addresses catastrophic forgetting by
  promoting stability in the local parameter neighborhood through a novel loss function
  that optimizes the worst-case KL-divergence of model predictions.
---

# STAR: Stability-Inducing Weight Perturbation for Continual Learning

## Quick Facts
- **arXiv ID:** 2503.01595
- **Source URL:** https://arxiv.org/abs/2503.01595
- **Authors:** Masih Eskandar; Tooba Imtiaz; Davin Hill; Zifeng Wang; Jennifer Dy
- **Reference count:** 19
- **Primary result:** STAR consistently improves continual learning performance by up to 15% over baselines, particularly with smaller buffer sizes.

## Executive Summary
STAR introduces a stability-inducing weight perturbation technique that addresses catastrophic forgetting in continual learning by promoting stability in the local parameter neighborhood. The method computes a worst-case parameter perturbation that maximizes KL-divergence of model predictions on correctly classified buffer samples, then optimizes against this perturbation to push parameters toward stable regions of the loss landscape. STAR functions as a plug-and-play component that can be combined with any rehearsal-based continual learning method. Experiments demonstrate significant improvements across varying baselines and achieve superior or competitive accuracy to state-of-the-art methods, with particularly strong performance under memory-constrained scenarios.

## Method Summary
STAR enhances rehearsal-based continual learning by adding a stability-inducing regularizer that optimizes against worst-case parameter perturbations. The method filters buffer samples to include only those correctly classified by the current model, computes a destabilizing perturbation δ that maximizes KL-divergence of predictions, and then takes a gradient step to minimize this worst-case divergence. This process pushes parameters into regions where predictions are locally robust to perturbation, reducing catastrophic forgetting. STAR uses layer-wise normalized perturbations to account for heterogeneous weight scales across layers, with a single hyperparameter γ controlling perturbation magnitude uniformly. The approach can be combined with any rehearsal-based method by adding the STAR loss term to the existing objective.

## Key Results
- STAR consistently improves performance of existing rehearsal methods by up to 15% across varying baselines
- Achieves superior or competitive accuracy to state-of-the-art continual learning methods
- Demonstrates significant gains particularly with smaller buffer sizes, making it effective for memory-constrained scenarios
- The correctly classified sample filtering (x*) consistently outperforms using all buffer samples by 1-2% on smaller buffers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing against worst-case parameter perturbations reduces future forgetting by guiding parameters toward stable regions of the loss landscape.
- **Mechanism:** STAR computes a destabilizing perturbation δ that maximizes KL-divergence of model outputs on correctly classified buffer samples, then takes a gradient step to minimize this worst-case divergence. This pushes parameters into regions where the model's predictions are locally robust to perturbation, meaning future gradient updates (which are bounded in magnitude) are less likely to change outputs catastrophically.
- **Core assumption:** The direction of future parameter updates lies within the local neighborhood defined by the perturbation constraint ‖δ‖₂ ≤ d.
- **Evidence anchors:**
  - [abstract] "exploits the worst-case parameter perturbation that reduces the KL-divergence of model predictions with that of its local parameter neighborhood"
  - [Section 4.1, Eq. 7] LSTAR(θt) := max_δ LFG(θt, θt + δ) s.t. ‖δ‖₂ ≤ d
  - [corpus] Weak direct evidence; related work (EWGN, EWC) addresses stability through regularization but not via worst-case perturbation.

### Mechanism 2
- **Claim:** Using only correctly classified buffer samples (x*) in the STAR loss focuses stability regularization on knowledge at risk of being forgotten, rather than already-forgotten or unlearned samples.
- **Mechanism:** The loss LFG is computed only over M*t = {(x,y) ∈ Mt | fθt(x) = y}. Samples already misclassified contribute zero to the stability objective since their outputs cannot be "preserved" through regularization. This concentrates optimization capacity on retaining correct predictions.
- **Core assumption:** Correctly classified buffer samples are representative of retained knowledge from previous tasks.
- **Evidence anchors:**
  - [Section 4.1, Eq. 6] M*t = {(x,y) ∈ Mt | fθt(x) = y}
  - [Table 3, ablation] Using x* vs. all x shows consistent improvement (e.g., DER++ buffer 500: 75.11 → 76.52)
  - [corpus] No direct comparison; most CL methods (EWC, DER++) regularize over all available samples without this filtering.

### Mechanism 3
- **Claim:** Layer-wise normalized perturbation accounts for heterogeneous weight scales across layers, enabling a single hyperparameter γ to control perturbation magnitude uniformly.
- **Mechanism:** Instead of a global norm constraint, δ(l) is computed per-layer with normalization: δ(l) = δ(l)₀ + γ(‖θ(l)‖²/‖g(l)‖²)g(l). This ensures the perturbation ratio ‖δ(l)‖₂/‖θ(l)‖₂ ≈ γ is consistent across layers with different weight magnitudes.
- **Core assumption:** Layer-wise normalization preserves the functional relationship between perturbation and output change better than global normalization.
- **Evidence anchors:**
  - [Section 4.3] "we normalize the gradients of the model layer-by-layer. This is due to the different numeric distributions of the weights of each layer, as well as the scale invariance of the weights"
  - [Section 4.3, Eq. 11] Explicit per-layer formula
  - [corpus] No corpus evidence; layer-wise normalization is adapted from prior work (Li et al. 2018, Wu et al. 2020) on sharpness-aware minimization, not CL-specific.

## Foundational Learning

- **Concept: KL-Divergence as a distributional distance metric**
  - **Why needed here:** STAR measures the change in model output distributions (not just accuracy) using KL-divergence. Understanding that KL(q||p) ≠ KL(p||q) and that it penalizes q assigning probability mass where p has near-zero mass is essential.
  - **Quick check question:** Given two probability distributions p = [0.9, 0.1] and q = [0.5, 0.5], which direction of KL-divergence is larger: KL(p||q) or KL(q||p)?

- **Concept: Gradient ascent for worst-case perturbation**
  - **Why needed here:** Computing δ requires maximizing (not minimizing) a loss. Understanding that gradient ascent moves parameters toward higher loss values, and that this finds a local worst-case within the constrained neighborhood, is critical.
  - **Quick check question:** If you perform gradient ascent on a loss L(θ + δ) starting from δ = 0, will you find the global maximum of L within the constraint ‖δ‖ ≤ d? Why or why not?

- **Concept: Rehearsal-based continual learning**
  - **Why needed here:** STAR is explicitly a plug-in for rehearsal methods. You need to understand what a buffer Mt stores, how reservoir sampling maintains a balanced distribution, and why rehearsal alone is insufficient (buffer is small, distribution shifts).
  - **Quick check question:** A rehearsal buffer of size 200 is trained on a 5-task split where each task has 10,000 samples. After all tasks, what fraction of data from task 1 remains in the buffer under reservoir sampling?

## Architecture Onboarding

- **Component map:**
  Input Stream (Xt) -> Buffer Manager (reservoir sampling) -> Mt
                      -> Current Batch (xt, yt)
                                |
                                ▼
                    ┌───────────────────────────┐
                    │   Forward Pass (fθ)        │
                    └───────────────────────────┘
                                |
                    ┌───────────┴───────────┐
                    ▼                       ▼
           LCL Computation         STAR Perturbation Module
           (baseline method)       ─────────────────────────
                    |               1. Filter to x* (correct preds)
                    |               2. Init δ₀ ~ N(0, ε‖θ‖²I)
                    |               3. Compute g = ∇θ+δ₀ KL(qθ||qθ+δ₀)
                    |               4. δ(l) = δ₀(l) + γ(‖θ(l)‖²/‖g(l)‖²)g(l)
                    |               5. LSTAR = KL(qθ(x*)||qθ+δ(x*))
                    |               6. ∇θLSTAR ≈ ∇θ+δ LSTAR (gradient at perturbed point)
                    ▼                       ▼
                    └───────────┬───────────┘
                                ▼
                    Combined Gradient: u = ∇LCL + λ∇LSTAR
                                |
                                ▼
                    Parameter Update: θ ← θ - αu

- **Critical path:**
  1. **Filtering to correctly classified samples** — If this step is omitted or buggy, STAR will regularize outputs on samples already misclassified, wasting capacity.
  2. **Per-layer perturbation computation** — Errors here (e.g., using global norm, incorrect gradient accumulation) will produce unstable δ that doesn't reflect true worst-case.
  3. **Gradient approximation** — The paper uses ∇θLFG(θ,θ+δ) ≈ ∇θ+δLFG(θ,θ+δ). If you compute the exact gradient via Hessian-vector products, computational cost increases significantly with marginal accuracy gain.

- **Design tradeoffs:**
  - **λ (STAR loss weight):** Higher λ increases stability but may reduce plasticity on new tasks. Paper uses λ ∈ [0.01, 0.1] depending on dataset/baseline.
  - **γ (perturbation ratio):** Larger γ considers a wider neighborhood, giving stronger robustness but potentially overly conservative updates. Paper uses γ ∈ [0.001, 0.05].
  - **Number of gradient steps for δ:** Paper shows 1 step is usually sufficient; 3 steps can slightly improve performance but increases compute ~2-3×.

- **Failure signatures:**
  - **Accuracy drops on new tasks:** λ too high; STAR is over-constraining plasticity.
  - **No improvement over baseline:** γ too small (perturbation neighborhood trivial) or buffer sampling not providing representative x*.
  - **Training instability/NaN:** Perturbation magnitude causing numerical issues; check layer-wise normalization and ε initialization.
  - **Forgetting unchanged but training slower:** Likely using all samples (x) instead of only correctly classified (x*); ablation in Table 3 shows this reduces gains.

- **First 3 experiments:**
  1. **Sanity check on a single task:** Train on Split-CIFAR10 with buffer size 200, comparing ER vs. ER+STAR. Expected: STAR should show ~5-10% higher average accuracy, especially on earlier tasks. This validates the implementation without multi-task complexity.
  2. **Ablation on correctly classified filtering:** Run STAR with x* vs. all x (as in Table 3). If performance gap doesn't match paper (~1-2% for smaller buffers, more for larger), debug the filtering logic.
  3. **Hyperparameter sweep on γ and λ:** Fix buffer size 200 on Split-CIFAR100, sweep γ ∈ {0.001, 0.01, 0.05} and λ ∈ {0.01, 0.05, 0.1}. Plot a 3×3 grid of average accuracy. This establishes the sensitivity landscape before committing to longer experiments.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the failure of STAR when using current task data (instead of or in addition to buffer data) stem primarily from the shift in objective minima or the instability of outputs on current data?
- **Basis in paper:** [explicit] In Section 5.4, the authors state, "We hypothesize two reasons as to why... if we only use current task data... the consistency of predictions for previous data is no longer considered... Secondly... output distribution of current samples are not fully trained."
- **Why unresolved:** The paper presents the failure mode and offers two plausible hypotheses but does not isolate the contributing factor through specific ablation or theoretical analysis.
- **What evidence would resolve it:** An ablation study analyzing the gradient alignment between the STAR loss on current vs. buffer data, or a theoretical analysis of the loss landscape curvature when current data is included.

### Open Question 2
- **Question:** How does increasing the number of inner maximization gradient steps impact the accuracy of the gradient approximation and overall convergence?
- **Basis in paper:** [explicit] In Appendix D, the authors note that while 3 steps can improve performance, further steps reduce it. They state, "We hypothesize that this is due to the gradient approximation... As we take more and more optimization steps, the gradient approximation becomes less accurate."
- **Why unresolved:** The paper relies on a single-step approximation for the main results and attributes the degradation in multi-step experiments to approximation error without quantifying the error bound.
- **What evidence would resolve it:** A theoretical analysis of the error bounds for the Hessian-vector product approximation as the number of steps increases, or empirical measurement of the divergence between the approximated gradient and the true gradient.

### Open Question 3
- **Question:** How does STAR compare to stochastic weight perturbation methods like DPCL specifically within the Online Continual Learning (OCL) setting?
- **Basis in paper:** [explicit] In Section 5.1, the authors exclude DPCL from comparisons, stating, "the experimental setting in their paper is the online CL setting (which is different than ours)... we cannot compare against DPCL in a fair and reasonable fashion."
- **Why unresolved:** While STAR is empirically validated in class-incremental scenarios, its efficacy relative to methods designed specifically for the stricter constraints of online learning (single pass) is unknown.
- **What evidence would resolve it:** A comparative study evaluating STAR and DPCL on standard OCL benchmarks (e.g., Split-CIFAR100 in a single-epoch setting) using identical hardware and hyperparameter search protocols.

### Open Question 4
- **Question:** Can the computational overhead of calculating worst-case perturbations be reduced without compromising the stability-inducing benefits of STAR?
- **Basis in paper:** [inferred] Table 7 in Appendix C shows that adding STAR increases training time significantly (e.g., from 47 to 106 minutes on ER for Split-CIFAR10). The authors acknowledge it requires "two extra optimization steps per batch."
- **Why unresolved:** The implementation requires a backward pass to compute the perturbation gradient and another for the update, which may be prohibitive for resource-constrained applications.
- **What evidence would resolve it:** Experiments utilizing efficient gradient calculation methods (e.g., Jacobian-vector products or cheap approximations) to determine if the overhead can be minimized while retaining the accuracy gains shown in Table 1.

## Limitations
- The paper does not provide the exact initialization scale ε for δ₀, which may affect stability and reproducibility
- Layer-wise normalization effectiveness is adapted from sharpness-aware minimization literature rather than CL-specific validation
- The claim of "plug-and-play" compatibility across all rehearsal methods requires broader testing beyond three baselines

## Confidence
- **High confidence**: STAR improves over baselines in reported experiments (Tables 1-2); correctly classified sample filtering consistently helps (Table 3); layer-wise normalization is computationally necessary given heterogeneous weight scales
- **Medium confidence**: The worst-case perturbation framework provides meaningful stability (mechanism assumption reasonable but not exhaustively validated); 1-3 gradient steps for δ are sufficient (empirically observed but not theoretically bounded)
- **Low confidence**: The claim of "plug-and-play" compatibility across all rehearsal methods (requires broader testing beyond three baselines); layer-wise normalization being optimal for all architectures (only tested on ResNet/EfficientNet)

## Next Checks
1. **Extended compatibility test**: Apply STAR to two additional rehearsal methods (e.g., A-GEM, MIR) on Split-CIFAR10 with buffer 200. Verify consistent accuracy improvements and check if λ/γ ranges need adjustment.
2. **Perturbation neighborhood sensitivity**: Systematically vary γ from 0.0001 to 0.1 on Split-CIFAR100 buffer 500. Plot average accuracy vs. γ to confirm the paper's claim that γ=0.01-0.05 is optimal and identify if a sharp drop-off exists beyond 0.05.
3. **Buffer representativeness stress test**: Create an imbalanced buffer scenario where task 1 has only 20 samples and task 5 has 180 samples (out of 200). Train with and without STAR, then measure forgetting of task 1. This tests whether correctly classified filtering (x*) is robust to severe sampling imbalance.