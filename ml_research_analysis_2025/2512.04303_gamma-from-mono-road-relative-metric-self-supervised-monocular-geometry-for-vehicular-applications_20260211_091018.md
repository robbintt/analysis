---
ver: rpa2
title: 'Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry
  for Vehicular Applications'
arxiv_id: '2512.04303'
source_url: https://arxiv.org/abs/2512.04303
tags:
- depth
- road
- monocular
- height
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of accurately reconstructing near-field
  road geometry, including slopes, bumps, and surface irregularities, which is critical
  for safe autonomous driving and robotics navigation. Standard monocular depth estimation
  methods often oversmooth these fine-scale features, missing small obstacles and
  height variations essential for vehicle control.
---

# Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications

## Quick Facts
- arXiv ID: 2512.04303
- Source URL: https://arxiv.org/abs/2512.04303
- Reference count: 40
- First self-supervised monocular approach evaluated on RSRD for road surface reconstruction

## Executive Summary
Gamma-from-Mono (GfM) is a self-supervised monocular geometry estimation method that predicts road-plane normal and per-pixel height-to-depth ratio (gamma) for accurate metric depth recovery in vehicular applications. Unlike standard depth estimation that oversmooths near-field features, GfM prioritizes fine-scale road geometry like bumps and slopes critical for safe autonomous driving. The method uses a lightweight 8.88M-parameter model that achieves state-of-the-art near-field accuracy on KITTI and RSRD benchmarks while naturally handling the metric scale ambiguity through a known camera height parameter.

## Method Summary
GfM predicts a global road-plane normal and per-pixel gamma (height-to-depth ratio) using a LiteMono-8M backbone with specialized heads. The gamma prediction is transformed through a signed log-space mapping to stabilize self-supervised training. Metric depth is recovered in closed form using only the camera height, avoiding full extrinsic calibration. Training employs dual-stream view synthesis with both standard depth-based and planar-parallax warps to enforce consistency for global structure and fine local geometry. The model is trained on KITTI and RSRD datasets using photometric reconstruction losses with attention-pooled normal estimation.

## Key Results
- State-of-the-art near-field depth accuracy on KITTI and RSRD benchmarks
- Achieves 8.88M parameters while maintaining high accuracy
- Successfully adapts to diverse camera setups and viewpoints
- First self-supervised monocular method evaluated on road surface reconstruction dataset

## Why This Works (Mechanism)

### Mechanism 1: Dimensionless Height-to-Depth Ratio (γ)
The network predicts γ = h/d (height above road / depth from camera) rather than absolute depth, preserving near-field geometric details and reducing scale ambiguity to a single camera-height parameter. This amplifies the error signal for small near-field obstacles that would otherwise be negligible in absolute depth maps.

### Mechanism 2: Signed Log-Space Transform
Mapping sigmoid output to γ via signed log-space transform stabilizes self-supervised training by expanding the dynamic range near zero, allowing the network to learn fine-grained road irregularities without being overwhelmed by large off-plane objects.

### Mechanism 3: Dual-Stream View Synthesis
Supervising with both standard depth-based warp and planar-parallax warp enforces consistency for global structure and fine local geometry. The parallax-specific warp isolates residual geometry deviations from the road plane, forcing accurate prediction of fine-scale features.

## Foundational Learning

- **Concept:** Planar Parallax Geometry
  - Why needed: Core mathematical engine decomposing 2D image motion into global homography plus residual parallax flow proportional to height above reference plane
  - Quick check: If a camera moves forward, how does image velocity of a point on the road differ from a point floating 1 meter above the road?

- **Concept:** Metric Scale Ambiguity
  - Why needed: Problem GfM solves—why single camera cannot recover absolute meters without external reference like camera height
  - Quick check: Why does knowing camera height (hc) allow model to convert dimensionless ratio γ into absolute depth d?

- **Concept:** Self-Supervised View Synthesis
  - Why needed: Training paradigm where warping source image to target view using predicted geometry creates reconstruction loss as supervisory signal
  - Quick check: In loss function L_photo, why is synthesized image Î compared to target image It rather than source Is?

## Architecture Onboarding

- **Component map:** Image -> Encoder -> γ Decoder & Normal Head -> Post-Process (Calc Depth/Flow) -> Image Warp (Synthesize Target) -> Photometric Loss

- **Critical path:** Image → Encoder → γ Decoder & Normal Head → Post-Process (Calc Depth/Flow) → Image Warp (Synthesize Target) → Photometric Loss

- **Design tradeoffs:** Global normal prediction stabilizes plane estimation but assumes locally planar road; γ range [-0.1, 5.0] must be tuned to avoid truncation or precision loss

- **Failure signatures:** Normal collapse causes ⃗Npred to flip or degenerate (γ Abs Diff spikes to ~2.28); infinite depth occurs when γ→0 and ⃗N·p→0 simultaneously

- **First 3 experiments:**
  1. Baseline ablation: Train on KITTI with only standard depth loss vs. GfM's dual-stream loss to verify improvement in near-field "bump" reconstruction
  2. Normal stability test: Visualize ⃗Npred on video sequence to check if it remains stable on flat roads
  3. Metric sensitivity: Perturb input camera height hc by ±10% during inference and plot resulting error in absolute depth vs. γ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the GfM framework be effectively scaled to a foundation model for general vehicular perception using large-scale monocular driving videos?
- Basis: Authors state intent to extend framework toward foundation model for vehicular perception by self-supervising γ on large-scale monocular driving videos
- Why unresolved: Current lightweight model evaluated on specific datasets; foundation models require massive diverse data and larger backbones
- What evidence would resolve: Successful training of large-capacity GfM variant on diverse datasets demonstrating superior zero-shot transfer

### Open Question 2
- Question: How can the dimensionless γ parameter be integrated directly into motion planning and control stacks without requiring explicit 3D reconstruction?
- Basis: Authors propose exploring using γ directly for on-road control and planning where full 3D reconstruction is unnecessary
- Why unresolved: Modern planning algorithms typically require metric state estimates; interface for converting relative γ signals into control commands remains undefined
- What evidence would resolve: Control theoretic study or simulation where planner consumes γ maps to successfully navigate complex road topographies

### Open Question 3
- Question: What is the quantitative sensitivity of the model's metric depth accuracy to errors or drift in the assumed camera height (hc)?
- Basis: Limitations section notes metric scale depends on camera-height accuracy, as miscalibration introduces global scale factor while preserving relative geometry
- Why unresolved: Paper identifies dependency but does not quantify magnitude of depth errors from specific miscalibrations
- What evidence would resolve: Ablation study plotting correlation between synthetic camera height perturbations and resulting Abs Rel depth error

## Limitations
- Assumes locally planar road geometry; performance degrades on highly undulating surfaces
- Metric depth accuracy is sensitive to camera height calibration
- Limited evaluation to KITTI and RSRD datasets; generalization to other environments unknown

## Confidence
- High confidence: Dual-stream view synthesis improves near-field accuracy; signed log-space transform stabilizes training
- Medium confidence: Attention pooling effectively predicts stable global normal; method generalizes to diverse camera setups
- Low confidence: Performance on non-planar terrain; behavior with incorrect camera height calibration

## Next Checks
1. Test model on dataset with pronounced road undulations (e.g., KITTI-360 or nuScenes) to evaluate planar assumption breakdown
2. Systematically vary camera height calibration error (±5%, ±10%, ±20%) and measure impact on absolute depth vs. relative gamma accuracy
3. Replace attention pooling with simpler global average pooling to quantify architectural contribution to normal stability