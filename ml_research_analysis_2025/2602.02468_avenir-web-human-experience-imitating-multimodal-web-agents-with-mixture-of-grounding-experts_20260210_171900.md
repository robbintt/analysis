---
ver: rpa2
title: 'Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture
  of Grounding Experts'
arxiv_id: '2602.02468'
source_url: https://arxiv.org/abs/2602.02468
tags:
- agent
- grounding
- agents
- https
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AVENIR-WEB introduces a robust autonomous web agent that achieves
  a new open-source state of the art on the Online-Mind2Web benchmark by addressing
  key reliability bottlenecks. The agent integrates Mixture of Grounding Experts (MoGE)
  for precise cross-modal element interaction, Experience-Imitation Planning (EIP)
  to incorporate external procedural knowledge from online guides, and a Task-Tracking
  Checklist with Adaptive Memory for resilient long-term state management.
---

# Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts

## Quick Facts
- arXiv ID: 2602.02468
- Source URL: https://arxiv.org/abs/2602.02468
- Authors: Aiden Yiliu Li; Xinyue Hao; Shilong Liu; Mengdi Wang
- Reference count: 35
- Primary result: 53.7% task success rate on Online-Mind2Web benchmark

## Executive Summary
AVENIR-WEB introduces a robust autonomous web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark by addressing key reliability bottlenecks. The agent integrates Mixture of Grounding Experts (MoGE) for precise cross-modal element interaction, Experience-Imitation Planning (EIP) to incorporate external procedural knowledge from online guides, and a Task-Tracking Checklist with Adaptive Memory for resilient long-term state management. This modular architecture enables effective navigation of complex, dynamic web interfaces while preventing navigational drift and repetitive failure loops. The system demonstrates a 53.7% task success rate on Online-Mind2Web, representing a 23.7% absolute improvement over existing open-source baselines and achieving performance parity with top-tier proprietary models. Additionally, a fully open-source configuration using a lightweight 8B model attains a 25.7% success rate, showcasing the framework's ability to empower compact models with industry-standard agentic capabilities.

## Method Summary
AVENIR-WEB employs a modular architecture that combines visual grounding, procedural knowledge retrieval, and structured state management. The system uses Mixture of Grounding Experts (MoGE) to handle cross-modal element interaction through a visual-first approach with semantic fallback. Experience-Imitation Planning (EIP) retrieves and synthesizes human-authored online guides before execution. A Task-Tracking Checklist maintains atomic outcome states, while Adaptive Memory performs chunked recursive summarization to prevent context saturation. The framework operates through a multi-stage pipeline: initialization phase for strategic roadmap synthesis, execution loop for perception and action, and auxiliary failure detection mechanisms.

## Key Results
- Achieves 53.7% task success rate on Online-Mind2Web benchmark
- Demonstrates 23.7% absolute improvement over existing open-source baselines
- 8B model configuration achieves 25.7% success rate while maintaining core capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A visual-first grounding path with semantic fallback improves element interaction reliability on complex DOM structures.
- Mechanism: MoGE prioritizes direct coordinate-based grounding using multimodal models, treating the viewport as a unified visual canvas. Only when visual cues are insufficient does it fall back to semantic structural reasoning (DOM parsing). This bypasses issues with nested iframes, shadow DOMs, and canvas elements that paralyze purely DOM-centric agents.
- Core assumption: Visual grounding generalizes better across heterogeneous UI paradigms than structural parsing alone.
- Evidence anchors:
  - [abstract] "integrates Mixture of Grounding Experts (MoGE) for precise cross-modal element interaction"
  - [section 3.3] "prioritizes direct visual grounding... while leveraging semantic structural reasoning to resolve complex interface elements"
  - [corpus] Related work on GUI grounding (e.g., OmniParser, ShowUI) supports hybrid visual-semantic approaches, though direct comparison to MoGE is not available.
- Break condition: Fails when visual coordinates map to non-interactive overlays or when elements lack visual differentiation (e.g., identically styled buttons with different functions).

### Mechanism 2
- Claim: Retrieving and synthesizing human-authored online guides before execution reduces trial-and-error exploration.
- Mechanism: EIP queries external resources (forums, help centers, guides) via search capabilities, then synthesizes 2-4 high-level imperative directives. This procedural prior is injected into the execution context, decoupling strategic knowledge from low-level action generation.
- Core assumption: External guides accurately reflect current site workflows and are retrievable for most target domains.
- Evidence anchors:
  - [abstract] "Experience-Imitation Planning (EIP) to incorporate external procedural knowledge from online guides"
  - [section 3.1] "Without such human-derived experience, agents are often forced into open-ended exploration"
  - [corpus] Web-CogReasoner similarly emphasizes knowledge acquisition before cognitive reasoning for web tasks, providing convergent evidence.
- Break condition: Fails when guides are outdated, domain-specific terminology is ambiguous, or no documentation exists for niche sites.

### Mechanism 3
- Claim: Structured state tracking with recursive memory distillation mitigates navigational drift in long-horizon tasks.
- Mechanism: The Task-Tracking Checklist maintains atomic outcome states with status updates (pending, in_progress, completed, failed) after each action. Adaptive Memory performs chunked recursive summarization over sliding windows (W=5), distilling execution traces while preserving failure reflections to prevent context saturation.
- Core assumption: Lightweight models can reliably update checklist status and summarize traces without losing critical signal.
- Evidence anchors:
  - [abstract] "Task-Tracking Checklist with Adaptive Memory for resilient long-term state management"
  - [section 3.4] "balances tactical interaction history with strategic awareness via Chunked Recursive Summarization"
  - [corpus] Evidence weak; related work on memory architectures for agents (e.g., Reflexion) supports reflection loops but not the specific recursive distillation approach.
- Break condition: Fails when summary distillation drops critical error signals or when checklist items are misclassified as complete prematurely.

## Foundational Learning

- Concept: **Visual Grounding vs. DOM Parsing**
  - Why needed here: MoGE's hybrid approach requires understanding when coordinate-based interaction suffices versus when structural context is necessary.
  - Quick check question: Given a webpage with a calendar widget inside an iframe, which grounding path would you prioritize and why?

- Concept: **Procedural Knowledge Retrieval**
  - Why needed here: EIP depends on effectively querying and synthesizing external documentation into actionable plans.
  - Quick check question: How would you handle a task on a website with no publicly available guides or documentation?

- Concept: **State Management in Long-Horizon Agents**
  - Why needed here: Understanding sliding window limitations and recursive summarization tradeoffs is critical for debugging drift.
  - Quick check question: If an agent repeats the same failed action 3 times, which memory component should detect this and how?

## Architecture Onboarding

- Component map:
  - Initialization Phase: EIP (strategic roadmap synthesis) → Checklist Generator (atomic subgoals)
  - Execution Loop: Core Agent (perception + intent) → Adaptive Memory (context buffer) → MoGE (grounding + action) → Checklist Synchronizer (state update)
  - Auxiliary: Failure Detection (4-layer: execution, state-change, action-specific, pattern analysis)

- Critical path:
  1. User instruction + URL → EIP retrieves guides → generates high-level plan
  2. Plan + instruction → Checklist Generator creates initial Ct
  3. Per-step: Core Agent receives (plan, checklist, memory, perception) → outputs intent → MoGE grounds to action
  4. Action executed → state observation → checklist updated → memory distilled if window full

- Design tradeoffs:
  - Single-inference grounding (faster) vs. iterative Chain-of-Ground (more accurate for complex elements)
  - Lightweight Qwen-3-VL-8B for checklist updates (lower latency) vs. larger model (higher reliability)
  - Fixed window W=5 (prevents hallucination) vs. W=∞ (full context risks saturation)

- Failure signatures:
  - **Grounding loop**: Repeated "No Operation" or non-responsive clicks → likely iframe/shadow DOM issue, MoGE fallback not triggered
  - **Checklist stall**: All items remain "pending" despite progress → synchronizer model failing to detect state changes
  - **Memory hallucination**: Agent references actions that never occurred → context window exceeded, distillation lost critical trace

- First 3 experiments:
  1. Run MoGE-only (disable EIP, checklist, adaptive memory) on 20 tasks with heavy iframe usage to isolate grounding contribution.
  2. Ablate EIP on tasks from unfamiliar domains to measure exploration efficiency (steps taken, token consumption).
  3. Compare W=5 vs. W=10 vs. W=∞ on tasks exceeding 15 steps to identify context saturation thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agent latency be optimized without compromising the complex reasoning capabilities required for reliable web navigation?
- Basis in paper: [explicit] The conclusion states that "Subsequent research will explore further optimization of agent latency."
- Why unresolved: The current reliance on large-scale MLLMs and multi-step modules (EIP, MoGE) introduces significant computational overhead.
- What evidence would resolve it: Comparative benchmarks showing reduced time-to-completion or token efficiency while maintaining the 53.7% success rate.

### Open Question 2
- Question: Does Experience-Imitation Planning (EIP) scale effectively to digital applications that lack abundant online documentation or human-authored guides?
- Basis in paper: [explicit] The conclusion suggests the need to "investigate the scalability of experience-guided planning across a broader range of digital applications."
- Why unresolved: EIP relies on retrieving external procedural knowledge; its robustness is unproven on niche interfaces with sparse online resources.
- What evidence would resolve it: Evaluation results on a benchmark of obscure or custom enterprise web applications with limited public guides.

### Open Question 3
- Question: Can "white-listed" evaluation environments or cooperative protocols effectively resolve the friction between ethical transparency and anti-bot mechanisms?
- Basis in paper: [explicit] Appendix E notes that "Future work may need to explore 'white-listed' evaluation environments or cooperative protocols between agents and website operators."
- Why unresolved: The ethical commitment to non-evasion resulted in 10% of tasks being blocked by security systems (e.g., Cloudflare), limiting real-world deployment.
- What evidence would resolve it: Demonstration of a standardized cooperative protocol where agents achieve high success rates without triggering security blocks.

## Limitations

- Heavy reliance on visual grounding may struggle with non-visual or dynamically rendered elements like WebGL canvases and encrypted media
- EIP component's effectiveness unproven when documentation is absent or domain-specific jargon prevents effective synthesis
- Adaptive Memory's recursive summarization lacks empirical validation on preventing both context saturation and critical signal loss

## Confidence

- **High confidence**: The modular architecture design and component integration are well-documented and technically coherent. The reported 23.7% absolute improvement over open-source baselines on Online-Mind2Web is verifiable through the benchmark's public leaderboard.
- **Medium confidence**: The core mechanisms (MoGE grounding, EIP planning, adaptive memory) are plausible given the literature, but the paper lacks rigorous ablation studies to isolate each component's contribution. The 25.7% success rate with 8B models is promising but untested on the full benchmark.
- **Low confidence**: Claims about MoGE's superiority over purely DOM-based approaches are unsupported by direct comparisons. The effectiveness of recursive summarization in preventing memory saturation is asserted rather than empirically demonstrated.

## Next Checks

1. **Grounding Path Isolation Test**: Run MoGE-only on a curated dataset of 50 tasks specifically designed to contain nested iframes, shadow DOMs, and canvas elements. Measure whether the visual-first approach achieves higher success rates than a purely DOM-based baseline, and quantify fallback frequency.

2. **EIP Robustness Evaluation**: Conduct a systematic ablation where EIP is disabled on 30 tasks from unfamiliar domains. Compare step counts, token consumption, and success rates against the full system to measure exploration efficiency and identify whether guide synthesis provides meaningful strategic advantage.

3. **Memory Distillation Threshold Analysis**: Execute a controlled experiment varying W from 3 to 10 on tasks exceeding 20 steps. Track checklist update accuracy, hallucination rates (agent referencing non-existent actions), and success rates to empirically determine the optimal window size that balances context preservation with computational efficiency.