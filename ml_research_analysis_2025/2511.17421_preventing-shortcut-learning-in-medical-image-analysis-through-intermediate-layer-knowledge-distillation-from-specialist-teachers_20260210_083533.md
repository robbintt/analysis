---
ver: rpa2
title: Preventing Shortcut Learning in Medical Image Analysis through Intermediate
  Layer Knowledge Distillation from Specialist Teachers
arxiv_id: '2511.17421'
source_url: https://arxiv.org/abs/2511.17421
tags:
- data
- shortcut
- training
- teacher
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel knowledge distillation approach to
  prevent shortcut learning in medical image analysis by leveraging intermediate-layer
  guidance from a teacher network fine-tuned on a small subset of unbiased, task-relevant
  data. The authors demonstrate that different types of shortcuts manifest distinctly
  across network layers, with diffuse shortcuts emerging in earlier layers and localized
  shortcuts appearing in later layers.
---

# Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers

## Quick Facts
- arXiv ID: 2511.17421
- Source URL: https://arxiv.org/abs/2511.17421
- Reference count: 40
- Key outcome: Intermediate-layer knowledge distillation with specialist teacher achieves comparable performance to bias-free models while requiring only small unbiased data subsets

## Executive Summary
This paper addresses shortcut learning in medical image analysis by introducing an intermediate-layer knowledge distillation approach that leverages specialist teachers fine-tuned on small, unbiased datasets. The method demonstrates that different shortcut types manifest distinctively across network layers, enabling more effective mitigation through targeted intermediate-layer guidance. Extensive experiments on CheXpert, ISIC 2017, and SimBA datasets show the approach consistently outperforms standard ERM, augmentation-based bias mitigation, and group-based approaches, often achieving performance comparable to models trained on completely bias-free data.

## Method Summary
The approach involves training a teacher network on a small, curated subset of bias-free data, then using this teacher to guide a student network through intermediate-layer knowledge distillation. Classification probes are attached to multiple layers of both networks, and KL divergence between their probability distributions provides regularization during student training. The teacher is frozen after training, while student probes are fine-tuned after each weight update. This intermediate-layer guidance helps the student learn task-relevant features while avoiding spurious correlations, with the teacher providing directional signals that are more informative than final-layer outputs alone.

## Key Results
- Intermediate-layer distillation consistently outperforms final-layer-only distillation, reducing shortcut reliance (∆TPR) from ~0.66 to ~0.08 on localized shortcuts
- Cross-architecture distillation is effective, with AlexNet teachers guiding ResNet-18 students to achieve AUC 0.68-0.75 on CheXpert
- Teachers fine-tuned on just 5% of bias-free data (56 ISIC images) substantially reduce ∆TPR compared to ERM baselines
- The method achieves comparable performance to models trained on completely bias-free data in many cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate-layer distillation is more effective than final-layer-only distillation for shortcut mitigation.
- Mechanism: KL divergence between teacher and student classification probes at intermediate layers guides feature learning away from spurious correlations before they become entrenched in network representations.
- Core assumption: The teacher has learned robust, task-relevant features from bias-free data that can serve as a meaningful reference signal.
- Evidence anchors:
  - [abstract]: "we demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers."
  - [section 4.2.1, Table 4]: When distillation is applied solely at the final classification head (n=0), performance declines significantly—∆TPR increases from ~0.08 to ~0.66 for localized shortcuts on CheXpert.
  - [corpus]: "On Measuring Localization of Shortcuts in Deep Networks" discusses shortcut impact on feature representations but does not directly validate intermediate-layer KD.
- Break condition: If the teacher's intermediate representations are themselves biased or task-irrelevant, distillation may propagate rather than mitigate shortcuts.

### Mechanism 2
- Claim: Compact teacher architectures can effectively guide larger student networks.
- Mechanism: Layer-wise probability matching via classification probes allows cross-architecture knowledge transfer; the teacher provides directional guidance rather than exact feature replication.
- Core assumption: The student has sufficient capacity to learn features the teacher cannot represent directly.
- Evidence anchors:
  - [section 4.2.2, Table 5]: AlexNet teacher → ResNet-18 student achieves AUC 0.68–0.75 on CheXpert vs. ERM baseline of 0.49–0.55; ResNet-18 → DenseNet-121 shows similar gains.
  - [section 3.3.1]: "We randomly sample n layers from the student network each epoch, where n is equal to the number of classification probes in the teacher model."
  - [corpus]: No direct corpus validation for cross-architecture transfer in this specific KD configuration.
- Break condition: If the capacity gap is too large or architectural inductive biases differ substantially (e.g., CNN → Transformer), layer alignment may fail.

### Mechanism 3
- Claim: Task-specific teacher fine-tuning outperforms ImageNet pretraining or pure confidence regularization.
- Mechanism: A teacher fine-tuned on even 5–20% of task-relevant, bias-free data provides domain-specific guidance that general pre-trained features or regularization alone cannot match.
- Core assumption: A small curated subset can be identified that is genuinely bias-free for the task.
- Evidence anchors:
  - [section 4.2.3, Table 6]: Fine-tuned teacher (Ours_f) achieves AUC 0.74–0.81 vs. ImageNet-pretrained (Ours_p) at 0.66–0.78 vs. confidence regularization (Ours_c) at 0.64–0.75.
  - [section 4.3.1, Figure 8]: Teacher trained on 5% of data (56 ISIC images) still substantially reduces ∆TPR compared to ERM.
  - [corpus]: RoentMod paper discusses identifying shortcuts but does not validate specialist teacher approach.
- Break condition: If the "bias-free" subset contains residual shortcuts at ≥5–10% prevalence for simple features (noise, constant markers), teacher effectiveness degrades measurably.

## Foundational Learning

- Concept: **Knowledge Distillation (KD)**
  - Why needed here: This method extends traditional KD from output-only to intermediate-layer alignment; understanding the base formulation is prerequisite.
  - Quick check question: Can you explain how KL divergence between soft probability distributions differs from direct cross-entropy with hard labels?

- Concept: **Shortcut / Spurious Correlation**
  - Why needed here: The core problem being solved; these are features correlated with labels during training that fail to generalize.
  - Quick check question: In medical imaging, what are three examples of shortcuts a model might learn that are clinically irrelevant?

- Concept: **Empirical Risk Minimization (ERM)**
  - Why needed here: This is the baseline training paradigm being improved upon; understanding its limitations motivates the approach.
  - Quick check question: Why does standard ERM fail when training data contains label-correlated but task-irrelevant features?

## Architecture Onboarding

- Component map:
  Teacher network (frozen) → Classification probes → KL divergence → Student network (trainable) → Classification probes → Combined loss

- Critical path:
  1. Curate small bias-free subset (5–20% of training data)
  2. Train teacher to convergence, freeze weights
  3. Train classification probes on teacher (separate optimizer)
  4. Train student on biased data with probe-level KD at each epoch
  5. Fine-tune student probes after each student weight update

- Design tradeoffs:
  - More distillation layers → stronger regularization but may over-constrain
  - Larger teacher subset → better guidance but higher curation cost
  - Using OOD teacher data → viable but requires ~2–3× more data than in-distribution

- Failure signatures:
  - Student matches teacher on training set but degrades on OOD → teacher overfitting
  - High ∆TPR persists → residual shortcuts in teacher data or insufficient distillation layers
  - Training instability → probe learning rate too high or layer sampling too aggressive

- First 3 experiments:
  1. **Baseline comparison**: Train ResNet-18 with ERM on noise-corrupted ISIC data (100% prevalence); measure ∆TPR between bias-aligned and bias-contrasting samples. Expected: ∆TPR ≈ 1.0 (complete shortcut reliance).
  2. **Teacher ablation**: Compare three teachers—(a) fine-tuned on 20% clean data, (b) ImageNet-pretrained only, (c) no teacher (confidence regularization). Expected: (a) >> (b) ≈ (c).
  3. **Layer depth sweep**: Run distillation with n ∈ {0, 5, 9, 17} intermediate layers. Expected: n=0 fails dramatically; n=5–17 performs similarly, suggesting partial distillation is sufficient.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the distinctive intermediate-layer confidence trajectories observed in CNNs (where diffuse shortcuts manifest in early layers and localized shortcuts in later layers) also occur in transformer architectures for medical image analysis?
- Basis in paper: [explicit] The authors state: "we suggest that establishing if the distinctive intermediate-layer confidence trajectories that we see in CNN models (Figure 4) is also mirrored in transformer architectures."
- Why unresolved: All experiments used CNN-based architectures (ResNet-18, AlexNet, DenseNet-121, 3D CNN); transformer architectures were not evaluated despite their increasing prevalence in medical imaging.
- What evidence would resolve it: Replicate the layer-wise confidence analysis (Figure 4) using vision transformer architectures on the same datasets with synthetic shortcuts, comparing confidence trajectories across layer depths.

### Open Question 2
- Question: Can principled methods for identifying the most impactful layers for distillation outperform the random layer sampling approach?
- Basis in paper: [explicit] The authors note: "While our random layer sampling approach proved effective, it represents a naive strategy that does not consider layer-specific contributions to shortcut learning. Future work should explore principled methods for identifying the layers where distillation would be most impactful."
- Why unresolved: Current approach randomly samples layers for partial distillation without considering which layers contribute most to shortcut learning; this may be suboptimal.
- What evidence would resolve it: Develop and test methods that analyze layer-specific bias contributions (e.g., using probe accuracy gradients or attention patterns) to select distillation targets, comparing against random sampling.

### Open Question 3
- Question: Can generative models create synthetic, bias-free training data for the teacher that eliminates the need for manually curated datasets?
- Basis in paper: [explicit] The authors propose: "One interesting avenue for possible future work would be the use of generative models to create clean, synthetic training data for the teacher model."
- Why unresolved: The current approach requires a small but manually curated, bias-free subset for teacher training, which imposes practical limitations despite reduced annotation burden.
- What evidence would resolve it: Train teacher models using synthetically generated medical images (from diffusion models or GANs) conditioned on disease labels without bias features, then evaluate student mitigation efficacy against the current curated-data approach.

### Open Question 4
- Question: How effective is the method against real-world demographic biases and other complex spurious correlations in clinical settings?
- Basis in paper: [explicit] The authors acknowledge: "While our synthetic bias features provide a controlled experimental environment, a critical next step is the investigation of the effectiveness of our approach against a broader range of real-world medical image shortcuts, such as those related to patient demographics."
- Why unresolved: Experiments used synthetic shortcuts (noise, squares) and one dataset with morphological biases; real-world biases like demographics, acquisition protocols, and hospital site effects remain untested.
- What evidence would resolve it: Apply the method to datasets with known demographic biases (e.g., underdiagnosis across patient subgroups), measuring TPR disparity across protected attributes with and without distillation.

## Limitations

- The method requires manual curation of a bias-free subset, limiting scalability despite reduced annotation burden
- Cross-architecture distillation performance varies significantly between teacher-student pairs
- Effectiveness against real-world biases remains untested beyond synthetic shortcuts and morphological biases
- The approach may over-constrain student learning if too many intermediate layers are used for distillation

## Confidence

- High confidence: The fundamental claim that intermediate-layer distillation outperforms final-layer-only approaches is well-supported by controlled experiments
- Medium confidence: The generalizability across different shortcut types (diffuse vs localized) is demonstrated but could benefit from more diverse synthetic shortcut designs
- Medium confidence: The teacher-student architecture compatibility findings are based on limited combinations (AlexNet, ResNet, DenseNet)

## Next Checks

1. Test the method's robustness when the "bias-free" subset contains unknown residual shortcuts at 5-10% prevalence
2. Evaluate performance degradation when teacher and student have substantially different architectural inductive biases (e.g., CNN → Transformer)
3. Assess the method's effectiveness on real-world shortcuts rather than synthetic ones by comparing with domain-expert-annotated shortcut detection