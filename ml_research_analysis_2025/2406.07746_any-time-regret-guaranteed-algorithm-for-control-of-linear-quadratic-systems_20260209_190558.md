---
ver: rpa2
title: Any-Time Regret-Guaranteed Algorithm for Control of Linear Quadratic Systems
arxiv_id: '2406.07746'
source_url: https://arxiv.org/abs/2406.07746
tags:
- algorithm
- regret
- bound
- control
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes any-time regret-guaranteed algorithms for controlling
  unknown Linear Quadratic Regulator (LQR) systems, addressing the computational inefficiency
  and horizon-dependency of existing methods. The core idea is to build on the SDP-based
  framework of Cohen et al.
---

# Any-Time Regret-Guaranteed Algorithm for Control of Linear Quadratic Systems

## Quick Facts
- **arXiv ID**: 2406.07746
- **Source URL**: https://arxiv.org/abs/2406.07746
- **Reference count**: 6
- **Primary result**: Any-time regret-guaranteed algorithms for LQR systems with optimal O(∥P*∥¹²) dependence on the DARE solution norm

## Executive Summary
This paper addresses the computational inefficiency and horizon-dependency of existing online learning algorithms for Linear Quadratic Regulator (LQR) control. The authors propose any-time regret-guaranteed algorithms that provide explicit high-probability bounds on state trajectories, making them suitable for safety-critical systems. The approach builds on the SDP-based framework of Cohen et al. (2019) but introduces confidence ellipsoid construction, input perturbation, and dwell-time-inspired policy updates to balance exploration and exploitation.

## Method Summary
The method uses a two-phase approach: a warm-up phase that gathers initial data using a stabilizing policy to estimate system parameters within an error bound, followed by the main control phase. The main algorithm (ARSLO+ or ARSLO) estimates parameters via regularized least squares, constructs confidence ellipsoids, solves relaxed primal/dual SDPs to determine policy and perturbation noise covariance, and applies control with perturbation. The key innovation is relaxing the sequential stability requirement while using dwell-time concepts to maintain stability, achieving optimal regret with explicit state-norm bounds.

## Key Results
- Introduces ARSLO and ARSLO+ algorithms that achieve O(√T) regret with explicit high-probability bounds on state trajectories
- Eliminates need for a priori bounds on ∥P*∥, a limitation of existing computationally efficient OFU-based algorithms
- Achieves optimal regret with O(∥P*∥¹²) dependence on DARE solution norm (vs O(∥P*∥¹⁶) for ARSLO)
- Provides explicit characterization of trade-off between state amplification and regret through parameter ρ̅

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Constructing confidence ellipsoids around unknown system parameters and solving relaxed SDPs enables computationally efficient optimistic control without a priori bounds on ∥P*∥.
- **Mechanism**: Regularized least-squares estimation builds confidence ellipsoid Ct(δ) = {Θ | Tr((Θ-Θ̂t)⊤Vt(Θ-Θ̂t)) ≤ rt}. This ellipsoid is incorporated into relaxed primal/dual SDPs (Equations 9, 11) via perturbation bounds. The dual SDP solution P(Ct) provides an upper bound on ∥P*∥ through the inequality P(Ct) ⪯ P* ⪯ P(Ct) + α₀/(4κ²t)I (Lemma 15), eliminating external bound requirements.
- **Core assumption**: The true system Θ* lies within the constructed confidence ellipsoid with high probability; the initial estimate Θ₀ is sufficiently accurate (∥Θ₀ - Θ*∥F ≤ ε̅(κ̅₁)).
- **Evidence anchors**:
  - [abstract]: "Our approach builds on the SDP-based framework of Cohen et al. (2019), using an appropriately tuned regularization and a sufficiently accurate initial estimate to construct confidence ellipsoids for control design."
  - [Section 2.2]: Defines confidence ellipsoid construction via regularized least squares and relaxed SDP formulations.
  - [corpus]: Neighbor paper "The Confusing Instance Principle for Online Linear Quadratic Control" addresses similar OFU limitations, suggesting this is an active research direction with unresolved practical issues.

### Mechanism 2
- **Claim**: The parameter ρ̅ ∈ [0,8) controls a continuum between strong sequential stability and dwell-time-only stability, with optimal regret achieved at ρ̅* = 2.
- **Mechanism**: The relaxation condition μt∥P(Ct)∥Vt⁻¹ ⪯ (α₀)/(4κ^(ρ̅)t)I replaces the conservative strong sequential stability requirement (ρ̅ = 8). Smaller ρ̅ allows larger policy jumps but requires longer dwell-time β(ρ̅) between updates via the criterion det(Vt) > (1 + β(ρ̅))det(Vτ). This trades off two regret terms: R4(t) which grows with β(ρ̅), and R6(t) which grows with perturbation noise scaling (inversely related to ρ̅).
- **Core assumption**: Switched systems theory applies—slowing policy updates sufficiently prevents state explosion even without strong sequential coupling between policies.
- **Evidence anchors**:
  - [abstract]: "Our analysis explicitly characterizes the trade-off between state amplification and regret, and shows that partially relaxing the sequential-stability requirement yields optimal regret."
  - [Section 4]: "We observe a continuum of behaviors... The optimal performance arises at an intermediate relaxation level, where stability and exploration are balanced. We identify a specific tuning that guarantees an O(∥P*∥⁶) dependence in the regret bound."
  - [corpus]: Evidence is weak—neighbor papers don't explicitly analyze this stability-regret trade-off spectrum, suggesting this is a novel contribution.

### Mechanism 3
- **Claim**: Input perturbation with covariance Γt ∝ (K(Ct)K(Ct)⊤ + ∥P(Ct)∥/α₀ · I)/√(t+c̅) achieves O(√t) regret while ensuring stability.
- **Mechanism**: Perturbation ηt ~ N(0, Γt) provides exploration. The specific structure uses both primal SDP solution K(Ct) (for directed exploration) and dual solution ∥P(Ct)∥ (for magnitude). Scaling as O(1/√t) is critical: larger (O(t^(-1/2+α̅))) fails stability conditions; smaller (O(t^(-1/2-α̅))) degrades regret to O(t^(1/2+α̅)) (Proposition 2, Appendix M).
- **Core assumption**: Gaussian perturbation is sufficient for exploration; the system can tolerate additional noise within stability margins.
- **Evidence anchors**:
  - [Section 3.1]: "The rationale for choosing Gaussian perturbation noise with a covariance matrix Γt such that ∥Γt∥ = O(1/√t) is to ensure stability and achieve an overall regret of order O(√t)."
  - [Appendix M]: Provides formal justification for the O(1/√t) scaling via cases showing deviations break either stability or regret bounds.
  - [corpus]: No direct corpus evidence on this specific perturbation structure—appears to be a novel design using both primal and dual SDP solutions.

## Foundational Learning

- **Concept**: Linear Quadratic Regulator (LQR) and Discrete Algebraic Riccati Equation (DARE)
  - **Why needed here**: The entire approach assumes knowledge that optimal LQR control is u*_t = K*x_t where K* derives from the DARE solution P*. Without this, the SDP formulations (Equations 47, 49) and the meaning of "optimistic" control are opaque.
  - **Quick check question**: Given system matrices A*, B* and cost matrices Q, R, can you derive the optimal infinite-horizon controller via the DARE? Can you explain why P* bounds appear in regret expressions?

- **Concept**: Self-normalized confidence bounds for martingales
  - **Why needed here**: The confidence ellipsoid radius rt (Equation 8) uses results from Abbasi-Yadkori & Szepesvári (2011) on self-normalized processes. Understanding why Tr((Θ̂t - Θ*)⊤Vt(Θ̂t - Θ*)) ≤ rt holds with high probability is essential for the perturbation lemma.
  - **Quick check question**: Given a martingale difference sequence ωk with conditional sub-Gaussianity, can you explain why the self-normalized bound ∥S^i_t∥²_(Vt⁻¹) ≤ 2σ²ω log(det(Vt)/(δ·det(λI))) holds?

- **Concept**: Switched systems and dwell-time stability
  - **Why needed here**: The paper explicitly adapts dwell-time concepts from switched systems control (Liberzon 2003) to balance the exploration-exploitation trade-off. Understanding why switching between individually stabilizing policies can destabilize a system is crucial.
  - **Quick check question**: If policies K₁ and K₂ each stabilize system (A*, B*) individually, can you explain why switching between them might cause instability? What role does dwell-time play in preventing this?

## Architecture Onboarding

- **Component map**: Warm-up Phase -> Confidence Ellipsoid Builder -> Relaxed SDP Solver -> Perturbation Generator -> Policy Update Controller -> Control Application

- **Critical path**: Warm-up phase duration → Initial estimate quality Θ₀ → Confidence ellipsoid tightness → SDP solution accuracy → Perturbation magnitude → Exploration quality → Next-step ellipsoid → Loop. The first ~O(1/ε²) steps in warm-up are rate-limiting; for ARSLO+(ρ̅*), this is O((n+m)⁵∥P*∥²⁴) steps.

- **Design tradeoffs**:
  - **ρ̅ choice**: ρ̅ = 8 (ARSLO) gives tighter state bounds O(∥P*∥^1.5) but worse regret O(∥P*∥¹⁶); ρ̅ = 2 (ARSLO+) gives optimal regret O(∥P*∥¹²) but looser state bounds O(∥P*∥².5). Choose based on safety-criticality vs. efficiency.
  - **λ regularization**: Larger λ stabilizes early estimates but slows ellipsoid shrinkage; paper sets λ = σ²ω c̅/(80(n+m)log(1/δ)) for ARSLO+ (Equation 26).
  - **Warm-up controller K₀**: Must be strongly stabilizing; choice affects warm-up duration quadratically via κ₀²/γ₀² term in bound (Equation 177).

- **Failure signatures**:
  1. **State norm explosion**: If ∥x_t∥ exceeds bound in Proposition 1/Theorem 4, check: (a) ρ̅ and β(ρ̅) mismatch, (b) perturbation covariance too large, (c) initial estimate Θ₀ outside ε̅(κ̅₁)-neighborhood.
  2. **Regret degrades to O(t^(2/3))**: Suggests perturbation scaling is O(t^(-1/2-α̅)) instead of O(t^(-1/2)); verify Γt computation.
  3. **No policy updates occurring**: det(Vt) criterion never met; check if λ is too large or if exploration noise is too small.
  4. **Confidence ellipsoid doesn't contain Θ***: Check rt computation (Equation 8); verify noise assumption (sub-Gaussian martingale difference) holds.

- **First 3 experiments**:
  1. **Scalar system validation**: Implement on n=1, m=1 system with known A*, B*. Set ρ̅ = 8 (ARSLO) and verify: (a) regret scales as O(√t), (b) state bound holds, (c) ∥P(Ct)∥ approaches ∥P*∥ from below. Then test ρ̅ = 2 and compare regret coefficients.
  2. **Warm-up phase calibration**: For fixed n, m, measure actual warm-up duration tw vs. theoretical bound O(1/ε²). Vary initial stabilizer K₀ quality and observe correlation with κ₀/γ₀. Confirm that dual SDP correctly detects when Θ₀ enters required neighborhood.
  3. **Stability-regret trade-off sweep**: Implement ARSLO+(ρ̅) for ρ̅ ∈ {0, 2, 4, 6, 8} on a controllable 2D system. Plot: (a) maximum state norm ∥x_t∥max, (b) cumulative regret at t=10⁴, (c) number of policy updates. Verify that ρ̅ = 2 minimizes regret while state bound degrades smoothly as ρ̅ decreases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed dwell-time-inspired techniques be extended to general nonlinear control schemes while preserving stability guarantees?
- Basis in paper: [explicit] The conclusion states the techniques developed "may be extended to more general learning-based nonlinear control schemes" where closed-loop dynamics can be interpreted as switched systems.
- Why unresolved: The current analysis relies heavily on the linearity of the system and the specific structure of the LQR cost and SDP formulations.
- What evidence would resolve it: Derivation of similar any-time regret bounds and high-probability state-norm bounds for a defined class of nonlinear systems.

### Open Question 2
- Question: Is it possible to improve the regret dependence on ∥P*∥ to approximately 4.5 without incurring an exponential runtime dependence on ∥P*∥?
- Basis in paper: [explicit] The paper notes that "A tighter analysis could further reduce this dependence to a power of approximately 4.5, but at the cost of an exponential dependence on ∥P*∥ in the runtime."
- Why unresolved: The current proof techniques for the algorithm's specific update rule seem to require a trade-off between computational efficiency and parameter dependence.
- What evidence would resolve it: A modified algorithm or analysis that achieves the tighter ∥P*∥^{4.5} regret bound with polynomial time complexity.

### Open Question 3
- Question: Can the regret dependence on ∥P*∥ be reduced to match the optimal scaling of Certainty-Equivalence methods (e.g., ∥P*∥^{5.5}) while retaining the explicit, high-probability bounds on the state trajectory?
- Basis in paper: [inferred] The paper compares its results to Simchowitz and Foster (2020), noting its own dependence (∥P*∥^6) is "slightly higher" than the CE method's (∥P*∥^{5.5}), while emphasizing the trade-off that its method provides explicit state-norm bounds.
- Why unresolved: Enforcing the strict stability and sequentiality properties required for state-norm bounds currently introduces additional conservativeness in the regret analysis.
- What evidence would resolve it: A theoretical derivation matching the CE regret bounds within the SDP-based/OFU framework or a new algorithm bridging the two performance gaps.

## Limitations
- The warm-up phase duration scales poorly with ∥P*∥, potentially requiring O((n+m)⁵∥P*∥²⁴) steps for high-precision initialization
- The method assumes full state observation, limiting applicability to partially observed systems
- The confidence ellipsoid construction assumes sub-Gaussian noise and accurate initial estimates, with failures causing state instability

## Confidence
- **High**: The SDP relaxation framework and its connection to confidence ellipsoids (Mechanism 1) is well-established theoretically with explicit bounds
- **Medium**: The switched systems analysis for policy updates and stability-regret trade-off (Mechanism 2) relies on non-trivial assumptions about dwell-time effectiveness that warrant empirical validation
- **Medium**: The perturbation design achieving O(1/√t) scaling with both primal and dual SDP solutions (Mechanism 3) is theoretically justified but depends on precise implementation of covariance matrices

## Next Checks
1. Implement ARSLO+(ρ̅) on a 3-state, 2-input system with varying ρ̅ values (0, 2, 4, 8) to empirically verify the theoretical regret bound improvement at ρ̅ = 2 while confirming the state norm degradation as ρ̅ decreases.
2. Systematically vary the regularization parameter λ in the warm-up phase to measure its impact on warm-up duration and final estimation accuracy, validating whether the theoretical scaling with (n+m) and ∥P*∥ matches practice.
3. Test the sensitivity of the method to initial stabilizing policy quality by deliberately choosing K₀ with poor γ₀ (near-unstable) and measuring the resulting warm-up duration and early regret performance.