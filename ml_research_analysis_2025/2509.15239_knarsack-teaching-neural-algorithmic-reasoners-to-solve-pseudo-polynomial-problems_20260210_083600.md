---
ver: rpa2
title: 'KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial
  Problems'
arxiv_id: '2509.15239'
source_url: https://arxiv.org/abs/2509.15239
tags:
- neural
- capacity
- item
- table
- algorithmic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KNARsack, a neural algorithmic reasoning
  (NAR) approach for solving the Knapsack problem, a pseudo-polynomial optimization
  problem omitted from standard NAR benchmarks. The authors develop a two-phase pipeline
  that first constructs the dynamic programming (DP) table and then reconstructs the
  solution, achieving better generalization to larger problem instances than direct-prediction
  baselines.
---

# KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems

## Quick Facts
- arXiv ID: 2509.15239
- Source URL: https://arxiv.org/abs/2509.15239
- Reference count: 40
- Primary result: Introduces NAR approach for Knapsack problem with edge length encoding and homogeneous processor achieving strong OOD generalization

## Executive Summary
This paper introduces KNARsack, a neural algorithmic reasoning (NAR) approach for solving the Knapsack problem, a pseudo-polynomial optimization problem omitted from standard NAR benchmarks. The authors develop a two-phase pipeline that first constructs the dynamic programming (DP) table and then reconstructs the solution, achieving better generalization to larger problem instances than direct-prediction baselines. Key innovations include edge length encoding, which enables the model to identify correct past states influencing the current one, and a homogeneous processor that enforces scale invariance with respect to item values. The approach achieves strong out-of-distribution performance, with micro-F1 scores reaching 0.917 and exact-match accuracy of 0.495 on instances with 16 items and capacity 64.

## Method Summary
KNARsack implements a two-phase NAR pipeline for 0-1 Knapsack: construction (DP table prediction) followed by reconstruction (solution extraction). The construction phase uses edge length encoding to identify relevant past DP states and a homogeneous processor for scale invariance. The reconstruction phase operates on predicted decision tables without access to item values to prevent shortcut learning. The model is trained separately on small instances (n≤16, C≤16) and evaluated on larger out-of-distribution instances (n=64, C=64). The approach builds on CLRS-30 with Triplet-GMPNN processors and demonstrates that modeling intermediate states through DP supervision enables NAR to handle pseudo-polynomial problems.

## Key Results
- Achieves micro-F1 0.917 and exact-match 0.495 on 16 items, capacity 64 (vs no-hint baseline: micro-F1 0.785, exact-match 0.438)
- Homogeneous processor maintains micro-F1 0.861-0.989 on 10× scaled values vs regular processor collapse to 0.263-0.646
- Edge length encoding produces sharper, more accurate DP tables compared to standard positional encoding
- Two-phase separation prevents shortcut learning, with reconstruction trained without item values achieving better OOD generalization

## Why This Works (Mechanism)

### Mechanism 1: Edge Length Encoding for DP State Localization
Categorical edge features based on node distance enable the model to identify which historical states influence current DP transitions. The encoding assigns a categorical vector to each edge based on |i - j| (absolute difference in capacity indices). Since the Knapsack DP recurrence `dp[i][c] = max(dp[i-1][c], dp[i-1][c-w[i]] + v[i])` requires accessing state `c - w[i]`, the model can learn to route messages along edges where the encoded distance matches the current item's weight. Core assumption: GNN message passing can exploit categorical edge features to selectively aggregate information from structurally-relevant neighbors.

### Mechanism 2: Homogeneous Processor for Scale Invariance
Constraining the processor to be homogeneous (f(αx) = αf(x)) prevents systematic under-prediction of DP values on out-of-distribution instances. Standard processors with bias terms and layer normalization learn value scales specific to training data. Homogeneity removes these, forcing linear scaling behavior that transfers to larger values. Core assumption: The DP computation should be invariant to the absolute scale of item values—only relative differences matter.

### Mechanism 3: Two-Phase Separation with Information Ablation
Splitting into construction and reconstruction phases, while withholding item values during reconstruction, prevents shortcut learning and forces algorithmic simulation. When reconstruction receives item values, it learns to predict selections directly without consulting the decision table. Removing values forces the model to learn the backtracking traversal. Core assumption: The decision table encodes sufficient information for reconstruction without access to original values.

## Foundational Learning

- **Pseudo-polynomial complexity**: Needed to understand why Knapsack is NP-hard yet admits O(nC) algorithms. Quick check: Explain why 0-1 Knapsack is NP-hard yet admits an O(nC) algorithm. What does "pseudo-polynomial" mean regarding how capacity is represented?

- **Dynamic programming state transitions and dependencies**: Essential for designing edge encoding and interpreting model behavior. Quick check: For `dp[i][c] = max(dp[i-1][c], dp[i-1][c-w[i]] + v[i])`, which states does (i, c) depend on? How does this relate to the edge length encoding choice?

- **Scale invariance and homogeneous functions**: Recognizing when neural components should preserve input scale helps diagnose generalization failures. Quick check: If you double all inputs to a linear layer with bias, what happens to outputs? What about a homogeneous layer (no bias, no normalization)?

## Architecture Onboarding

- **Component map**: NAR Construction (edge length encoding + homogeneous processor) -> DP table prediction -> NAR Reconstruction (item weights only) -> Final selection
- **Critical path**: Train construction model (30,000 epochs) → Generate soft decision tables on training data → Train reconstruction model using TRUE decision tables with item weights only → Inference: construction → soft decision table → reconstruction → final selection
- **Design tradeoffs**: Split vs joint reconstruction steps (alternating steps outperform combining), neural vs deterministic reconstruction (deterministic slightly better but unstable), oracle vs predicted tables during training (ground truth stabilizes learning but creates train-test gap)
- **Failure signatures**: DP value under-prediction in larger instances (use homogeneous processor), high micro-F1 but low exact-match (small errors compound), reconstruction generalization collapse (verify values excluded), training instability with unified model (separate training)
- **First 3 experiments**: 1) Edge encoding ablation (train with standard positional only, expect blurrier predictions), 2) Scale invariance test (evaluate regular vs homogeneous on 10× scaled values, expect regular collapse), 3) Reconstruction input study (train with/without values, expect value-inclusive to fail OOD)

## Open Questions the Paper Calls Out

### Open Question 1
Can the construction and reconstruction phases be jointly trained in an end-to-end manner while maintaining stability and generalization? The authors state joint training "suffers from instability, failing to converge properly within the first 10,000 epochs" and currently use separate training as a pragmatic workaround.

### Open Question 2
What architectural modifications would enable stable gradient flow when training the homogeneous construction model with deterministic (differentiable) reconstruction? The authors report being "unable to train (homo. c. + det. r.) due to exploding/vanishing gradients" and hypothesize this is due to algorithm chaining multiplications of probabilities.

### Open Question 3
Can edge length encoding improve generalization on CLRS-30 algorithms that involve relative positional reasoning? The authors plan to investigate applying edge length encoding to problems from CLRS-30 to shed light on their potential beyond the current setting.

## Limitations
- Training instability of unified models requires careful separate training procedures rather than inherent architectural advantages
- Edge length encoding cutoff (M=10) is arbitrary and may not generalize to algorithms with different dependency ranges
- The approach is demonstrated only on Knapsack, not other pseudo-polynomial problems like subset sum or partition

## Confidence
- **High confidence**: Core empirical results showing edge length encoding and homogeneous processors improve OOD generalization are well-supported by controlled ablations
- **Medium confidence**: Mechanism explanations for why edge encoding works and why homogeneity helps are plausible but not rigorously proven
- **Low confidence**: Claim that two-phase separation "prevents shortcut learning" is somewhat circular and the deeper reasons aren't fully explained

## Next Checks
1. Reconstruct with predicted vs true decision tables to test whether the train-test gap significantly impacts real-world performance
2. Edge encoding sensitivity analysis by systematically varying M and testing on algorithms with different state dependency ranges
3. Cross-problem transfer by applying the same techniques to subset sum or partition problems to test mechanism generality