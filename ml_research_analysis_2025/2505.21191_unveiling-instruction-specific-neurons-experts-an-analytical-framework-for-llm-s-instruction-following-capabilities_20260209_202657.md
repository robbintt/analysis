---
ver: rpa2
title: 'Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework
  for LLM''s Instruction-Following Capabilities'
arxiv_id: '2505.21191'
source_url: https://arxiv.org/abs/2505.21191
tags:
- arxiv
- instructions
- experts
- preprint
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates how fine-tuning reconfigures
  LLM computations to enhance instruction-following capabilities. The researchers
  introduce HEXA INST, a balanced dataset spanning six instruction categories (classification,
  code, general QA, generation, math, and summarization), and propose SPARCOM, a framework
  for identifying and analyzing instruction-specific sparse components (neurons in
  dense models and both neurons and experts in Mixture-of-Experts architectures).
---

# Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities

## Quick Facts
- arXiv ID: 2505.21191
- Source URL: https://arxiv.org/abs/2505.21191
- Reference count: 20
- Introduces SPARCOM framework to systematically identify and analyze instruction-specific neurons and experts in LLMs

## Executive Summary
This study introduces SPARCOM, a systematic framework for identifying and analyzing Instruction-Specific Neurons (ISNs) in dense LLMs and both ISNs and Instruction-Specific Experts (ISEs) in Mixture-of-Experts (MoE) architectures. The framework reveals that fine-tuning reconfigures neuron activation patterns within experts rather than altering expert selection, with ISNs following a three-phase layer-wise distribution pattern that varies by instruction type. Using the balanced HEXA INST dataset spanning six instruction categories, the research demonstrates that ISNs exhibit both category-specific uniqueness and cross-category generality, with fine-tuning significantly altering ISN activation patterns while ISE selection remains stable.

## Method Summary
The SPARCOM framework operates through three sequential steps: (1) Identification of ISNs by computing neuron activation frequencies across tokens for each instruction and selecting those above a threshold percentile, with ISEs identified as top-k experts per token in MoE models; (2) Evaluation of functional generality and uniqueness through overlap analysis using Jaccard similarity between same-type versus different-type instructions; (3) Alteration comparison between vanilla and fine-tuned models using Jaccard similarity for ISNs and Pearson correlation for ISEs. The method employs vllm and Transformers library hooks to capture post-activation FFN values (gate_up_proj layers) for frequency computation across the HEXA INST dataset's 1,200 balanced instructions.

## Key Results
- Fine-tuning significantly alters ISN activation patterns (Jaccard similarity 0.38-0.62) while ISE selection remains highly stable (Pearson correlation 0.91-0.94)
- ISNs exhibit both general and unique characteristics with high within-category overlap, particularly for classification, code, and math tasks
- ISN distribution follows consistent three-phase layer-wise behavior: high early-layer counts for instruction parsing, reduced middle-layer counts for generalization, and sharp final-layer increases for output generation

## Why This Works (Mechanism)

### Mechanism 1: Three-Phase Layer-Wise ISN Distribution
- Claim: ISNs follow a consistent three-phase pattern across layers, with functional specialization varying by depth.
- Mechanism: Early layers show high ISN counts for initial instruction parsing (encoding shallow concepts). Middle layers exhibit reduced counts as the model generalizes instruction understanding. Final layers show sharp ISN increases to decode content into output tokens.
- Core assumption: Layer-wise activation distribution reflects functional specialization rather than arbitrary activation patterns.
- Evidence anchors:
  - [abstract] "three-phase mechanistic behavior: early layers show high ISN counts for initial instruction parsing, middle layers have reduced counts for generalization, and final layers show sharp increases for output generation"
  - [section] §5.2 describes distinct phase patterns for both non-MoE and MoE architectures
  - [corpus] Limited direct corpus support; "Unveiling Hidden Collaboration within Mixture-of-Experts" examines expert routing but not layer-wise neuron distribution
- Break condition: If different instruction types show radically divergent layer-wise distributions, the three-phase framework may not generalize across categories.

### Mechanism 2: Fine-Tuning Refines Neurons Within Experts Rather Than Expert Selection
- Claim: Instruction-following improvements from fine-tuning primarily emerge from ISN activation pattern changes within experts, not from altering which experts are selected.
- Mechanism: Fine-tuning substantially changes which neurons activate for specific instructions (Jaccard similarity 0.38-0.62) while expert selection patterns remain highly stable (Pearson correlation 0.91-0.94), suggesting within-expert refinement drives capability gains.
- Core assumption: The disparity between ISN change magnitude and ISE stability indicates that neuron-level adaptation, not routing adaptation, underlies performance improvements.
- Evidence anchors:
  - [abstract] "Fine-tuning significantly alters ISN activation patterns (Jaccard similarity coefficients ranging from 0.38 to 0.62 across categories) while ISEs maintain high correlation (0.91-0.94)"
  - [section] Table 1 and Table 2 quantitatively demonstrate ISN alteration vs. ISE stability; §5.3 concludes "ISNs of the experts may have played a more significant role"
  - [corpus] "Dropping Experts, Recombining Neurons" supports neuron-level manipulation importance in MoE architectures
- Break condition: If intervention studies demonstrate that modifying expert selection improves performance more than neuron-level editing, this mechanism would be incomplete.

### Mechanism 3: Instruction-Specific Neurons Exhibit Category Uniqueness and Cross-Category Generality
- Claim: ISNs encode both category-specific patterns (uniqueness) and shared instruction-processing functions (generality).
- Mechanism: Unique ISNs show high overlap within instruction categories—particularly classification, code, and math—enabling specialized processing. General ISNs appear across categories, handling common instruction language and overlapping vocabulary.
- Core assumption: The diagonal dominance observed in ISN overlap matrices reflects functional specialization, not methodological artifact.
- Evidence anchors:
  - [abstract] "ISNs and ISEs exhibit both general and unique characteristics, with high overlap within instruction categories (particularly for classification, code, and math tasks) indicating specialized functionality"
  - [section] Figure 3 shows darker diagonal coloration for same-type comparisons; §5.1 discusses generality vs. uniqueness
  - [corpus] "Sparse Activation Editing for Reliable Instruction Following" provides supporting evidence that editing instruction-relevant neurons improves following behavior
- Break condition: If controlling for token overlap eliminates category-specific ISN patterns, uniqueness may stem from vocabulary distribution rather than semantic specialization.

## Foundational Learning

- Concept: Feed-Forward Network (FFN) as Key Site for Neuron Analysis
  - Why needed here: ISN identification operates specifically on FFN gate_up_proj activations after the activation function, not attention outputs. Understanding FFN's role in information storage and transformation is essential.
  - Quick check question: Why does the paper analyze FFN activations rather than attention outputs for ISN identification?

- Concept: Jaccard Similarity for Set Overlap Measurement
  - Why needed here: The paper quantifies ISN overlap using Jaccard similarity (|A∩B|/|A∪B|). Interpreting the 0.38-0.62 range for fine-tuning changes requires understanding this metric.
  - Quick check question: What does a Jaccard similarity of 0.5 between two ISN sets indicate about their overlap?

- Concept: Mixture-of-Experts Top-k Routing
  - Why needed here: The framework distinguishes ISNs (individual neurons) from ISEs (activated experts). Understanding how MoE models select top-k experts per token is critical for interpreting ISE correlation results.
  - Quick check question: In Qwen-MoE selecting top-4 experts from 60 per token, what does a Pearson correlation of 0.93 between pre- and post-fine-tuning ISEs suggest?

## Architecture Onboarding

- Component map:
  - HEXA_INST dataset -> SPARCOM framework -> ISN/ISE identification -> Overlap/alteration analysis

- Critical path:
  1. Feed instructions through LLM, capture FFN activations after activation function (Eq. 5)
  2. Compute per-neuron activation frequency: proportion of tokens where activation > 0 (Eq. 6)
  3. Threshold at top-ε percentile to identify ISNs (Eq. 7)
  4. Compute Jaccard similarity for ISN overlap and Pearson correlation for ISE patterns
  5. Compare vanilla vs. fine-tuned models using Eq. 15-16

- Design tradeoffs:
  - **Threshold ε selection**: Higher values yield more specific but fewer ISNs; lower values increase coverage but reduce discrimination
  - **Six-category taxonomy**: Simplified from real-world instruction diversity; may miss nuanced intermediate types
  - **Token-level aggregation**: Eq. 6 averages across all tokens, potentially conflating instruction and response activation patterns

- Failure signatures:
  - Low diagonal dominance in ISN overlap heatmap → identification method may not capture category-specific patterns
  - Jaccard similarity >0.8 between vanilla/fine-tuned ISNs → fine-tuning may not have substantially altered neuron behavior
  - ISE correlation <0.7 before/after fine-tuning → contradicts paper's finding of expert selection stability

- First 3 experiments:
  1. Replicate ISN identification on held-out instructions to test generalization beyond HEXA_INST
  2. Ablate top-k% ISNs for one category and measure performance drop on that category vs. others to validate causal role
  3. Extend MoE analysis beyond Qwen-MoE (e.g., Mixtral, DeepSeek-MoE) to test ISE stability generalization

## Open Questions the Paper Calls Out

- **Active leveraging of ISNs/ISEs**: Developing effective strategies to leverage these components to enhance the model's instruction-following capabilities is an important direction for future work.

- **Generalization to broader instruction categories**: Exploring larger and more diverse datasets to identify a broader range of ISN/ISE components is necessary for validating the framework's applicability.

- **Efficiency and performance optimization**: Investigating how ISNs and ISEs can be utilized to improve model efficiency or performance remains an open research direction.

## Limitations

- **Threshold parameter uncertainty**: The paper does not specify the exact ε percentile threshold used for ISN identification, which is critical for reproducibility.

- **Limited architecture generalization**: The three-phase layer-wise distribution pattern and expert selection stability (Pearson correlation 0.91-0.94) were primarily validated on Qwen-MoE, with unclear generalizability to other MoE architectures.

- **Semantic vs. surface effect ambiguity**: High within-category ISN overlap could reflect genuine semantic specialization or merely vocabulary distribution artifacts, with no control for instruction content overlap.

## Confidence

**High Confidence (HL-Mech)**:
- Three-phase layer-wise ISN distribution pattern (Mechanism 1)
- Expert selection stability during fine-tuning (Mechanism 2)

**Medium Confidence (SL-Mech)**:
- ISN activation pattern changes driving performance improvements (Mechanism 2)
- Category-specific vs. general ISN overlap patterns (Mechanism 3)

**Low Confidence (LL-Mech)**:
- Causal relationship between specific ISN alterations and instruction-following capability
- Whether findings generalize across different MoE architectures

## Next Checks

1. **Ablation Study**: Remove top-5% ISNs for one instruction category and measure performance degradation specifically on that category versus others to establish causal role of ISNs in category-specific processing.

2. **Architecture Generalization**: Replicate the expert selection stability analysis (Pearson correlation 0.91-0.94) on Mixtral and DeepSeek-MoE models to test whether this pattern holds across different MoE implementations with varying top-k and expert counts.

3. **Content Control**: Compute ISN overlap matrices while controlling for instruction content similarity (e.g., using semantic embeddings) to distinguish between genuine functional specialization and vocabulary distribution effects.