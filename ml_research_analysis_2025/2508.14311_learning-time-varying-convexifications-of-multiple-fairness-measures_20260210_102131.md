---
ver: rpa2
title: Learning Time-Varying Convexifications of Multiple Fairness Measures
arxiv_id: '2508.14311'
source_url: https://arxiv.org/abs/2508.14311
tags:
- fairness
- action
- feedback
- vertices
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning time-varying convexifications
  of multiple fairness measures in sequential decision-making, where the relative
  weights of fairness regularizers are unknown and may change over time. The authors
  propose a framework that incorporates graph-structured feedback, where actions and
  fairness measures are represented as vertices in a graph, and edges indicate relationships
  between them.
---

# Learning Time-Varying Convexifications of Multiple Fairness Measures

## Quick Facts
- arXiv ID: 2508.14311
- Source URL: https://arxiv.org/abs/2508.14311
- Authors: Quan Zhou; Jakub Marecek; Robert Shorten
- Reference count: 40
- One-line primary result: O(√T log T) regret bound for learning time-varying convexifications of multiple fairness measures with graph-structured feedback

## Executive Summary
This paper addresses the challenge of learning to balance multiple fairness measures in sequential decision-making when their relative weights are unknown and may change over time. The authors propose a novel framework that leverages graph-structured feedback, where actions and fairness measures are represented as vertices in a graph, and edges indicate relationships between them. By executing one action, the learner can observe rewards for related actions and fairness measures, enabling more efficient learning with limited feedback.

The core contribution is an algorithm that combines exponentially-weighted updates with linear programming to make decisions in each round while updating weights based on observed rewards. The framework handles both time-invariant and time-varying graph-structured feedback, providing theoretical guarantees on regret. The primary result is a regret bound of O(√T log T) for the time-varying case, demonstrating that the algorithm achieves sublinear regret. A numerical illustration using advertising revenue data demonstrates the effectiveness of the approach in balancing fairness measures and revenue.

## Method Summary
The method combines exponentially-weighted algorithms with linear programming to handle sequential decision-making with multiple fairness measures under graph-structured feedback. In each round, the algorithm constructs a probability distribution that mixes exploitation (based on exponential weights) with exploration (derived from a linear program that ensures fair information gathering). The linear program solves for a distribution that maximizes the minimum probability of observing any action's reward, preventing information starvation. Weights are updated using exponential formulas based on observed rewards from the chosen action and its neighbors in the graph. The framework provides theoretical regret bounds that scale with the structural properties of the feedback graph, specifically the size of its maximal acyclic subgraph.

## Key Results
- Proposed framework incorporates graph-structured feedback where actions and fairness measures are represented as vertices with edges indicating relationships
- Algorithm achieves O(√T log T) regret bound for time-varying case, demonstrating sublinear regret
- Numerical illustration using advertising revenue dataset demonstrates effectiveness in balancing fairness measures and revenue
- Framework handles both time-invariant and time-varying graph-structured feedback scenarios
- Theoretical analysis shows regret is controlled by graph structural properties, specifically the maximal acyclic subgraph parameter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A graph-structured feedback model enables generalization of reward information from one action to related actions, reducing the need for exhaustive sampling
- Mechanism: The core of this method is the "compatibility graph" ($G_t$). Vertices represent actions ($V_a$) and fairness regularizers ($V_f$). A directed edge from action $a_i$ to $a_j$ ($a_i \rightarrow a_j$) means choosing $a_i$ reveals the reward for $a_j$. An edge from $a_i$ to $f_j$ means action $a_i$ impacts regularizer $f_j$. By executing one action, the learner observes rewards for its out-neighbors, effectively getting partial feedback on related decisions without taking them. This information updates the weights ($\phi(i,t)$) of all observed actions
- Core assumption: Relationships between actions and fairness regularizers can be meaningfully modeled as a graph; this structure is known or disclosed at the beginning of each time step $t$
- Evidence anchors:
  - [abstract]: "...framework that incorporates graph-structured feedback, where actions and fairness measures are represented as vertices in a graph..."
  - [Page 2-3, Section 3]: Defines the graph $G_t$, vertices $V_a, V_f$, and edges $E_t$. Describes in-neighbours $N_{in}^t(f_j)$ and out-neighbours $N_{out}^t(a_i)$
  - [Page 6, Section 5]: "In the graph, a directed edge starts vertex $a_i$ and ends vertex $a_j$ implies that by choosing vertex $a_i$... the reward associated with both vertices $a_i, a_j$ will be revealed immediately."
  - [corpus]: Evidence for this specific graph-structured bandit mechanism is weak in the provided related papers
- Break condition: Fails if the graph structure is unknown, time-varying without disclosure, or mis-specified such that the revealed reward information is misleading

### Mechanism 2
- Claim: An exponentially-weighted algorithm with a linear program component balances exploration and exploitation while ensuring a baseline level of information gathering
- Mechanism: The algorithm maintains a weight $\phi(i,t)$ for each action. In each round $t$, it constructs a probability distribution $p(i,t)$. This distribution mixes exponential weights (exploitation) with a term $\gamma_t \xi(i)$ derived from a linear program (exploration). The LP solves for a distribution $\xi$ that maximizes the minimum probability of observing any action's reward, ensuring no action is starved of information. The egalitarianism factor $\gamma_t$ forces exploration based on the graph's structure
- Core assumption: Regret can be minimized by combining exponential weighting (Hedge/Exp3 family) with a linear program for fair exploration; graph parameters provide a meaningful basis for setting the exploration rate
- Evidence anchors:
  - [abstract]: "The core method idea involves using exponentially-weighted algorithms with linear programming to make decisions..."
  - [Page 6-7, Algorithm 1]: Describes the probability distribution $p(i,t) = (1-\gamma_t)\frac{\phi(i,t)}{\Phi_t} + \gamma_t\xi(i)$. The linear program to find $\xi$ is shown in equation (10)
  - [corpus]: Evidence for this specific LP-based bandit algorithm is weak in the provided related papers
- Break condition: Underperforms if the exploration forced by the linear program ($\xi(i)$) is too aggressive or insufficient for the problem dynamics

### Mechanism 3
- Claim: Regret is bounded by the structural properties of the feedback graph, specifically the size of its maximal acyclic subgraph ($mas(G_t)$)
- Mechanism: The theoretical analysis shows that the algorithm's performance (weak regret) is controlled by the information-theoretic complexity of the graph. A more informative graph has a smaller "maximal acyclic subgraph" parameter. The regret bound scales with the sum of $mas(G_t)$ over all rounds, yielding sublinear guarantees when the graph is informative
- Core assumption: Theoretical regret bounds derived from graph parameters like $mas(G_t)$ hold for the adversarial or non-stochastic setting considered; learning rate $\eta$ and mixing factor $\gamma_t$ are chosen appropriately
- Evidence anchors:
  - [abstract]: "The primary result is a regret bound of O(√T log T) for the time-varying case..."
  - [Page 7, Theorem 1]: States the regret is $\tilde{O}(\sqrt{\log(I/\delta)\sum_{t\in[T]} mas(G_t)})$
  - [Page 3, Table 1]: An overview table listing algorithms and regret bounds parameterized by $mas(G_t)$
  - [corpus]: Evidence for this theoretical guarantee is weak
- Break condition: Claimed bounds are not achievable if the graph is time-varying without disclosure at each round or if analysis assumptions are violated

## Foundational Learning

- Concept: **Online Convex Optimization**
  - Why needed here: The paper frames the problem as a sequence of decisions with a changing or unknown objective function, the standard setting for online convex optimization
  - Quick check question: Can you explain the "Bandit" setting versus the "Expert" setting within online learning, and how graph-structured feedback provides a middle ground?

- Concept: **Multi-Objective Optimization and Pareto Frontiers**
  - Why needed here: The core challenge is balancing multiple, conflicting fairness measures alongside a primary objective like revenue
  - Quick check question: If you have to maximize two conflicting goals, like revenue and fairness, why can't you find a single point that maximizes both simultaneously?

- Concept: **Fairness Definitions in Machine Learning**
  - Why needed here: The "fairness regularizers" are abstract functions but are instantiated from real concepts like demographic parity and equalized odds
  - Quick check question: What is the difference between "group fairness" (e.g., demographic parity) and "individual fairness"?

## Architecture Onboarding

- Component map:
  - Inputs: Time horizon $T$, action/regularizer counts $I, J$, and for each round $t$: graph $G_t$, real reward $r_t$ for chosen action and its neighbors
  - Internal State: Weight vector $\phi(i,t)$ for each action; last chosen action
  - Core Logic (per round):
    1. Linear Program Solver: Uses $G_t$ to solve for exploration distribution $\xi$
    2. Probability Mixer: Combines exploitation (from weights $\phi$) and exploration (from $\xi$) to get final action probabilities $p(i,t)$
    3. Action Selector: Samples an action $a_i$ from $p$
    4. Feedback Processor: Receives rewards for $a_i$ and its out-neighbors, computes estimated reward $\hat{r}(n,t)$
    5. Weight Updater: Updates weights $\phi$ using exponential formula
  - Outputs: Chosen action, updated cumulative reward

- Critical path: Performance hinges on the LP solver finding an effective $\xi$ and the weight updater correctly propagating feedback from neighbors

- Design tradeoffs: Exploration vs. Exploitation (controlled by $\gamma_t$), Graph Complexity vs. Regret (more complex graphs lead to weaker bounds), Dynamic vs. Weak Regret (guarantees are for the latter)

- Failure signatures: Stagnation (weights for some actions never update), Oscillating Action Choice ($\gamma_t$ or $\eta$ too high), Consistent Suboptimal Performance (mis-specified graph or reward)

- First 3 experiments:
  1. Toy Problem Validation: Replicate the simple 2-party political advertising example from Section 4. Goal: Verify basic loop and action trace
  2. Ablation on Graph Structure: Run the algorithm with a full graph, an empty graph, and a sparse graph. Goal: Confirm performance scales with graph connectivity
  3. Time-Varying Graph Stress Test: Run a simulation where $G_t$ changes randomly each round (Case II). Goal: Measure robustness and impact of re-solving the LP every round

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption of known graph structure at each round is critical and may not hold in many real-world scenarios where relationships between actions and fairness measures are complex or evolve without clear notification
- Theoretical analysis relies heavily on specific structural properties of the feedback graph, particularly the maximal acyclic subgraph parameter, which may not capture all relevant aspects of graph informativeness
- The numerical illustration on advertising revenue is limited to a single dataset without comparisons to baselines or ablation studies on key parameters

## Confidence

- High confidence: The core algorithmic framework (exponential weighting + linear programming) is clearly described and theoretically sound under stated assumptions
- Medium confidence: The O(√T log T) regret bound is derived from established techniques, but its practical tightness depends heavily on the graph structure parameter mas(Gt)
- Low confidence: The numerical illustration on advertising revenue is limited to a single dataset without comparisons to baselines or ablation studies on key parameters

## Next Checks

1. Graph Structure Sensitivity: Systematically vary the density and structure of the feedback graph in simulations to empirically verify how mas(Gt) affects regret and compare against theoretical predictions

2. Unknown Graph Extension: Implement a variant where the graph structure is estimated from data rather than assumed known, measuring the degradation in performance

3. Multi-Fairness Measure Calibration: Extend the advertising revenue experiment to explicitly show how the algorithm trades off between different fairness metrics (e.g., demographic parity vs. equalized odds) under varying weight configurations