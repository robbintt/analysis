---
ver: rpa2
title: 'Rethinking Federated Graph Learning: A Data Condensation Perspective'
arxiv_id: '2505.02573'
source_url: https://arxiv.org/abs/2505.02573
tags:
- graph
- condensed
- subgraph
- federated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of federated graph learning
  (FGL) where existing methods relying on model parameters or gradients struggle with
  subgraph heterogeneity and introduce privacy risks. The authors propose FedGM, a
  novel paradigm using condensed graphs as optimization carriers instead of traditional
  model parameters.
---

# Rethinking Federated Graph Learning: A Data Condensation Perspective

## Quick Facts
- arXiv ID: 2505.02573
- Source URL: https://arxiv.org/abs/2505.02573
- Reference count: 15
- Key outcome: Proposed FedGM achieves up to 4.3% performance improvement over FedAvg and 4.1% over Fed-PUB, with average accuracy of 74.56% on six datasets

## Executive Summary
This paper addresses the challenge of federated graph learning (FGL) where existing methods relying on model parameters or gradients struggle with subgraph heterogeneity and introduce privacy risks. The authors propose FedGM, a novel paradigm using condensed graphs as optimization carriers instead of traditional model parameters. FedGM operates in two stages: (1) clients perform local subgraph condensation through gradient matching and upload condensed subgraphs to the server; (2) the server optimizes the condensed features using class-wise gradients from real subgraphs to improve quality. Extensive experiments on six datasets (Cora, CiteSeer, Photo, Actor, Tolokers, CS) demonstrate that FedGM consistently outperforms state-of-the-art baselines while reducing communication costs through a single transmission round.

## Method Summary
FedGM is a dual-stage federated learning framework that uses condensed graphs as optimization carriers. In Stage 1, each client locally generates a condensed subgraph through gradient matching, optimizing a learnable condensed graph to match gradients computed on the real subgraph. Clients upload these condensed subgraphs to the server, which assembles them into a global condensed graph. In Stage 2, the server distributes a gradient generation model to clients, who compute class-wise gradients on their real subgraphs and upload them back. The server then optimizes the global condensed features by matching them to aggregated class-wise gradients, improving minority class representations. The final model is trained on the optimized condensed graph and distributed to all clients.

## Key Results
- FedGM achieves up to 4.3% performance improvement over FedAvg baseline
- Outperforms Fed-PUB by 4.1% on average accuracy
- Demonstrates robust performance across different client numbers and condensation ratios
- Reduces communication costs through single transmission round of condensed data
- Average accuracy of 74.56% across six benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient matching enables synthetic graphs that preserve training dynamics without exposing raw data.
- Mechanism: A randomly initialized GNN computes gradients on both the real subgraph and a learnable condensed subgraph. The condensed graph's features and structure (parameterized via MLP) are optimized to minimize the distance between these gradient vectors. This aligns the "loss landscape" induced by the synthetic data with that of real data, so a model trained on the condensed graph generalizes to the original distribution.
- Core assumption: Gradient similarity implies functional equivalence for downstream training; the training trajectory depends more on gradient direction than data fidelity.
- Evidence anchors:
  - [abstract]: "we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data."
  - [section 4.1]: "the client generate the condensed subgraph through one-step gradient matching... where a GNN is updated using real subgraph and condensed subgraph, respectively, and their resultant gradients are encouraged to be consistent"
  - [corpus]: Related work "DANCE" (FMR=0.53) confirms gradient-based condensation is an active research direction, but comparative validation remains limited.
- Break condition: If condensed node count is too small relative to class complexity, gradient matching cannot preserve class boundaries; if topology is critical and MLP parameterization fails to capture it, mechanism degrades.

### Mechanism 2
- Claim: Condensed graphs transfer topological-feature interplay more effectively than model parameters or gradients alone.
- Mechanism: Unlike CV-based federated carriers (parameters/gradients) which ignore graph structure, condensed graphs explicitly encode node features and adjacency via Eq.7. This captures the homophily/heterophily patterns unique to each client's subgraph. The server concatenates features and labels (Eq.8) while preserving disconnected components (Eq.9), maintaining local topological signatures.
- Core assumption: Subgraph heterogeneity is primarily expressed through feature-topology coupling; preserving this coupling at the data level outperforms parameter aggregation.
- Evidence anchors:
  - [section 3]: "Using model parameters or gradients as primary information carriers overlooks the complex interplay between features and topology within local heterogeneous subgraphs, resulting in sub-optimal performance"
  - [section 1]: "condensed graphs can effectively capture the complex relationships between nodes and topology, providing a more suitable means of information transmission"
  - [corpus]: "A Comprehensive Data-centric Overview of Federated Graph Learning" (FMR=0.49) reinforces that data-centric approaches are gaining attention, but no direct comparative studies validate topology preservation claims.
- Break condition: If clients have highly overlapping subgraphs with similar topology, the overhead of condensation exceeds its benefit; if topology is irrelevant to the task, simpler carriers suffice.

### Mechanism 3
- Claim: Two-stage federated gradient matching addresses class imbalance across clients by aggregating global intra-class gradients.
- Mechanism: Stage 1 produces local condensed graphs but minority classes suffer from sparse representation. Stage 2 distributes a gradient generation model θt to clients, who compute class-wise gradients on real data (Eq.10-11). The server aggregates these by class-weighted averaging (Eq.12) and optimizes the global condensed features to match aggregated gradients (Eq.14). This reinforces minority class representations using knowledge from clients where those classes are majority.
- Core assumption: Class imbalance is non-uniform across clients; minority classes on one client are majority classes elsewhere, enabling cross-client knowledge transfer.
- Evidence anchors:
  - [section 4.2]: "majority classes on one client often correspond to minority classes on other clients in scenarios with subgraph heterogeneity"
  - [section 5.3]: "After Stage 1, our method achieves an average accuracy of 72.92%... our method consistently achieves superior performance compared to its single-stage variant across all datasets (e.g., increasing from 84.91% to 90.85%)"
  - [corpus]: "Federated Prototype Graph Learning" (FMR=0.53) explores prototype-based aggregation for heterogeneity, suggesting class-level representations are a broader trend, but comparative evidence is thin.
- Break condition: If all clients have identical class imbalance patterns, global aggregation provides no benefit; if communication rounds are severely limited, Stage 2 may not converge.

## Foundational Learning

- Concept: **Message Passing in GNNs**
  - Why needed here: The paper uses GCN as backbone; understanding Eq.1 ( ˆAH(ℓ−1)W(ℓ)) is essential to grasp why topology matters for gradient computation.
  - Quick check question: Can you explain why normalizing the adjacency matrix ( ˆA = ˜D−1/2 ˜A ˜D−1/2) affects gradient flow differently for high-degree vs. low-degree nodes?

- Concept: **Gradient Matching for Dataset Condensation**
  - Why needed here: Core to both stages; Eq.4-6 define the optimization objective. Without this, the two-stage design is opaque.
  - Quick check question: Why does matching gradients across random initializations (θk ∼ Pθk) prevent overfitting to a specific model?

- Concept: **Federated Averaging (FedAvg)**
  - Why needed here: The paper positions itself against FedAvg; understanding parameter aggregation (¯θ ← Nk/N Σθk) clarifies the contrast with condensation-based carriers.
  - Quick check question: What happens to FedAvg convergence when clients have non-IID label distributions and how does FedGM's approach differ fundamentally?

## Architecture Onboarding

- Component map:
  - Client-side (Stage 1): Local subgraph Gk → Condensation model (GCN + MLP for adjacency) → Condensed subgraph Sk = {A′k, X′k, Y′k}
  - Server-side (Stage 1): Receives K condensed subgraphs → Concatenates features/labels (Eq.8) → Assembles block-diagonal adjacency (Eq.9) → Initial global condensed graph Sglo
  - Client-side (Stage 2): Receives gradient generation model θt → Computes class-wise gradients ∇θt LGk,c on real subgraph → Uploads gradients + sample counts
  - Server-side (Stage 2): Aggregates class-wise gradients (Eq.12) → Optimizes X′glo via gradient matching (Eq.14) → Trains final GNN on optimized Sglo → Distributes model

- Critical path:
  1. Each client runs 1000 condensation epochs locally (Stage 1) — this is compute-heavy.
  2. Single upload of condensed graphs to server (one-shot communication).
  3. Server initializes global condensed graph.
  4. T rounds of federated gradient matching (Stage 2) — lightweight communication (gradients only).
  5. Server trains final model and broadcasts once.

- Design tradeoffs:
  - **Condensation ratio r**: Lower r reduces communication and privacy risk but may lose class-specific information. Paper shows insensitivity (Fig.6), but this is dataset-dependent.
  - **Stage 2 rounds (T)**: More rounds improve condensed quality but increase communication. Paper uses 100 rounds; diminishing returns likely.
  - **Condensed topology preservation**: Block-diagonal adjacency (Eq.9) preserves local structure but prevents cross-client edge learning. Alternative: learn cross-client edges, but this increases parameters and privacy risk.

- Failure signatures:
  - Stage 1 fails silently if condensation ratio is too low (minority classes underrepresented).
  - Stage 2 diverges if class-wise gradient aggregation weights (N′k,c/N′c) are unstable due to extreme imbalance.
  - Final model underperforms if condensed graph overfits to gradient generation model distribution Pθt.
  - Communication bottleneck shifts from parameter upload to gradient generation model distribution in Stage 2.

- First 3 experiments:
  1. **Ablation on condensation ratio**: Vary r from 0.1 to 1.0 on Cora/CiteSeer; monitor accuracy and per-class F1 to detect minority class degradation.
  2. **Stage 1 vs. Stage 2 isolation**: Run Stage 1 only and compare against full FedGM; quantify the contribution of global intra-class gradient matching.
  3. **Scalability test with K=20 clients**: Replicate Fig.5 setup; measure communication cost (bytes transferred) and wall-clock time against FedAvg baseline to validate claimed efficiency gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the condensation-based federated learning paradigm be effectively extended to graph-level tasks, such as graph classification or link prediction, given the current focus on node classification?
- Basis in paper: [explicit] The Introduction explicitly states: "this paper focuses on subgraph-FL, the instance of FGL on a semi-supervised node classification paradigm."
- Why unresolved: The method relies on condensing subgraphs of a larger implicit graph, whereas graph-level tasks often involve disjoint graphs, requiring a different condensation strategy for feature and topology alignment.
- What evidence would resolve it: Successful application of FedGM or a similar condensation framework to benchmark graph classification datasets (e.g., molecular property prediction) within a federated setting.

### Open Question 2
- Question: Does the transmission of class-wise gradients from real subgraphs in Stage 2 introduce new privacy vulnerabilities that negate the privacy benefits of sharing condensed graphs in Stage 1?
- Basis in paper: [inferred] While the paper claims reduced privacy risk via condensed data, Section 4.2 requires clients to upload gradients $\nabla L_{G_{k,c}}$ calculated on real data. The analysis in Section 5.3 asserts reduced risk but does not prove robustness against gradient inversion attacks on these specific updates.
- Why unresolved: Gradient inversion attacks have been shown to reconstruct data from gradients; the paper does not provide a theoretical or empirical privacy analysis (e.g., differential privacy guarantees) for the multi-round gradient matching phase.
- What evidence would resolve it: A formal privacy audit or differential privacy analysis quantifying the information leakage from the gradients shared during the federated optimization stage.

### Open Question 3
- Question: Is the local subgraph condensation step computationally feasible for resource-constrained edge devices when scaling to graphs with millions of nodes?
- Basis in paper: [inferred] The paper tests on datasets with up to 18,333 nodes (CS dataset). The computational complexity of condensation (Section 5.3, Result 4) is $O(rM)$, which involves bi-level optimization that can be significantly slower than standard training, potentially limiting adoption on standard edge hardware.
- Why unresolved: The experimental validation is limited to relatively small benchmark datasets, and the wall-clock time for condensation versus standard local training is not reported.
- What evidence would resolve it: Experiments reporting wall-clock time and memory usage for the condensation phase on large-scale industrial graphs (e.g., >100k nodes per client) on hardware representative of edge devices.

## Limitations
- Limited ablation studies on condensation ratio sensitivity across all datasets - Fig.6 only covers Cora/CiteSeer
- No privacy analysis of condensed graphs (membership inference risk assessment)
- Communication cost comparison only against FedAvg, missing comparison to FedNova or FedProx
- No evaluation on large-scale graphs (>100k nodes) to validate scalability claims

## Confidence
- **High confidence**: Performance improvements over FedAvg (empirical results consistent across 6 datasets)
- **Medium confidence**: Two-stage design effectiveness (Stage 2 contribution shown but not rigorously isolated)
- **Low confidence**: Claims about superiority for "heterogeneous subgraphs" (subgroup analysis limited to class-level)

## Next Checks
1. Implement membership inference attack on condensed subgraphs to quantify privacy-utility tradeoff
2. Test FedGM on graphs with extreme label imbalance (>90% majority class) to validate Stage 2 robustness
3. Measure wall-clock time and memory usage for condensation stage on graphs with 50K+ nodes