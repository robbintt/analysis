---
ver: rpa2
title: Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt
  Evaluator
arxiv_id: '2510.08524'
source_url: https://arxiv.org/abs/2510.08524
tags:
- prompt
- proxy
- prompts
- performance
- mcts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a prompt optimization framework for detecting
  unfair clauses in Terms of Service (ToS) agreements using Large Language Models
  (LLMs). The approach combines Monte Carlo Tree Search (MCTS) for efficient prompt
  exploration with a proxy prompt evaluator to reduce computational costs.
---

# Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt Evaluator

## Quick Facts
- arXiv ID: 2510.08524
- Source URL: https://arxiv.org/abs/2510.08524
- Reference count: 5
- Key outcome: MCTS-based prompt optimization with proxy evaluator achieves 0.90 accuracy and 0.73 macro F1 on CLAUDETTE dataset while reducing computational costs

## Executive Summary
This paper addresses the challenge of optimizing prompts for Large Language Models (LLMs) in legal text classification tasks, specifically detecting unfair clauses in Terms of Service (ToS) agreements. The authors propose a novel framework that combines Monte Carlo Tree Search (MCTS) with a proxy prompt evaluator to efficiently explore the prompt space while minimizing expensive LLM evaluations. The proxy evaluator, trained on a dataset of correctness labels, predicts whether an LLM would correctly classify a clause under a given prompt, significantly reducing computational overhead. Experiments on the CLAUDETTE dataset demonstrate that this approach achieves comparable performance to traditional classifiers and outperforms baseline prompt optimization methods.

## Method Summary
The proposed framework uses Monte Carlo Tree Search (MCTS) to efficiently explore the prompt space by iteratively selecting, expanding, simulating, and backpropagating through prompt variations. A proxy prompt evaluator, trained on a dataset of correctness labels (whether the LLM correctly classifies clauses under given prompts), predicts the performance of new prompts without requiring actual LLM evaluations. The proxy model takes as input the concatenation of prompt embeddings (using Sentence-BERT) and clause embeddings, outputting a probability of correctness. MCTS uses these predictions to guide its search toward high-performing prompts. The best prompt discovered through MCTS is then evaluated on a held-out test set using the actual LLM to measure final performance.

## Key Results
- MCTS with proxy evaluator achieved 0.90 accuracy and 0.73 macro F1 on the CLAUDETTE dataset
- Outperformed baseline prompt optimization methods including genetic algorithms and random search
- Best-performing proxy model used MLP classifier with Sentence-BERT embeddings
- Computational efficiency improved by reducing LLM calls through proxy predictions
- Performance comparable to traditional classifiers while requiring less computational resources

## Why This Works (Mechanism)
The approach works by decoupling prompt evaluation from actual LLM inference through a learned proxy model. The proxy evaluator captures the relationship between prompt structure and LLM performance by learning from a dataset of prompt-clause pairs with correctness labels. MCTS efficiently navigates the prompt space by using these proxy predictions to focus search on promising regions, avoiding exhaustive exploration. The MCTS framework naturally balances exploration of new prompts with exploitation of known good prompts through its UCT (Upper Confidence bounds applied to Trees) selection mechanism.

## Foundational Learning
- Monte Carlo Tree Search: Needed for efficient exploration of prompt space; Quick check: Verify UCT formula implementation and exploration-exploitation balance
- Prompt optimization: Needed for improving LLM performance on specific tasks; Quick check: Test prompt variations on held-out validation set
- Proxy model training: Needed to reduce computational costs of LLM evaluations; Quick check: Validate proxy accuracy against ground truth LLM outputs
- Legal clause classification: Needed for the specific application domain; Quick check: Verify label consistency across annotators
- Sentence-BERT embeddings: Needed for capturing semantic meaning of prompts and clauses; Quick check: Test embedding quality on semantic similarity tasks

## Architecture Onboarding

Component map: CLAUDETTE dataset -> Prompt generation -> Proxy evaluator training -> MCTS search -> Best prompt evaluation -> Final performance metrics

Critical path: The most critical components are the proxy evaluator (quality of predictions directly impacts MCTS efficiency) and the MCTS implementation (search strategy determines prompt quality). The proxy evaluator must be trained on representative data, and the MCTS must effectively balance exploration and exploitation.

Design tradeoffs: The main tradeoff is between proxy evaluator accuracy and training data requirements versus computational savings. Using simpler proxy models reduces training complexity but may sacrifice prediction quality. The MCTS search depth and breadth must balance thoroughness against computational feasibility.

Failure signatures: If the proxy evaluator is poorly trained, MCTS will converge to suboptimal prompts. If the MCTS implementation has bugs in the UCT calculation or backpropagation, the search may get stuck in local optima or fail to converge. Poor prompt embeddings will lead to inability to distinguish between effective and ineffective prompts.

First experiments:
1. Test proxy evaluator prediction accuracy on a held-out validation set of prompt-clause pairs
2. Run MCTS with a simplified proxy (e.g., random predictions) to verify search infrastructure works
3. Evaluate a known good prompt through the full pipeline to verify end-to-end functionality

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can proxy prompt evaluators trained on small LLMs (e.g., Llama-3-8B) transfer effectively to larger models without retraining?
- Basis in paper: The limitations section states: "The effectiveness of the proxy model is likewise tied to the LLM it was trained on and the error patterns specific to that model. Further research is needed to determine whether our findings generalize to larger models."
- Why unresolved: The experiments used only Llama-3-8B; error patterns and prompt sensitivity likely differ across model scales.
- What evidence would resolve it: Train proxy evaluators on small LLMs, evaluate their prediction accuracy when scoring prompts for larger models (e.g., GPT-4, Claude) on the same legal classification task.

### Open Question 2
- Question: Can proxy evaluators accurately predict correctness for multi-label unfairness category classification, not just binary fairness labels?
- Basis in paper: The limitations section notes: "Predicting correctness on multi-label classification is more difficult and it is left to see whether the proxy could sufficiently predict correctness to draw useful conclusions about the performance of a multi-label prompt."
- Why unresolved: Multi-label prediction requires capturing more nuanced prompt-clause interactions; binary correctness may not generalize.
- What evidence would resolve it: Extend the correctness dataset to include the nine unfairness categories, train proxy models, and compare their predicted vs. actual multi-label F1 scores during MCTS search.

### Open Question 3
- Question: Would active learning or curriculum learning strategies for score set selection improve proxy evaluator stability and reduce the number of training samples required?
- Basis in paper: The conclusion proposes: "optimizing the score set using active or curriculum learning strategies. By selectively including the most informative or representative clauses, active learning can reduce the number of evaluations required while maintaining reliable performance estimates."
- Why unresolved: Current approach uses random sampling; informed selection may improve efficiency but remains untested.
- What evidence would resolve it: Compare proxy evaluator performance (validation accuracy, downstream prompt quality) when trained on actively sampled vs. randomly sampled clause sets of varying sizes.

### Open Question 4
- Question: Can alternative proxy architectures (e.g., Transformers, Bayesian models) better capture prompt-task interactions than MLP and logistic regression?
- Basis in paper: The conclusion suggests: "experimenting alternative proxy model architectures, including Transformers or Bayesian Models to better capture the interaction between prompt and task performance."
- Why unresolved: Current proxy models use simple concatenation of embeddings; richer interaction modeling may improve correctness prediction.
- What evidence would resolve it: Train Transformer-based or Bayesian proxy models on the same correctness dataset and compare their prediction accuracy, calibration, and impact on final prompt quality against MLP/logistic regression baselines.

## Limitations
- Proxy evaluator effectiveness is tied to the specific LLM it was trained on, limiting generalizability to larger models
- Current approach only handles binary classification, with multi-label prediction remaining an open challenge
- MCTS search strategy is relatively simple and may miss complex, high-performing prompts requiring multiple simultaneous modifications
- Evaluation limited to single dataset (CLAUDETTE), raising questions about broader applicability across legal domains

## Confidence
- High confidence: Computational efficiency gains from using proxy evaluator instead of direct LLM calls
- Medium confidence: Claim of comparable performance to traditional classifiers requires more extensive validation across diverse datasets
- Medium confidence: Superiority over baseline optimization methods demonstrated within experimental setup but may not generalize

## Next Checks
1. Evaluate the framework on multiple legal text classification datasets beyond CLAUDETTE, including different clause types and legal domains, to assess generalizability.

2. Conduct ablation studies to determine the impact of proxy evaluator quality on MCTS performance, including testing with synthetically corrupted training data to understand robustness.

3. Compare the MCTS search strategy against alternative prompt optimization methods (genetic algorithms, reinforcement learning approaches) using the same proxy evaluator to isolate the contribution of the search algorithm versus the evaluation proxy.