---
ver: rpa2
title: Choices Speak Louder than Questions
arxiv_id: '2502.18798'
source_url: https://arxiv.org/abs/2502.18798
tags:
- choice
- question
- answer
- choices
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how much model predictions in multiple-choice
  question answering are driven by the answer choices themselves rather than by the
  questions. To measure this, it defines choice sensitivity as the frequency with
  which the model's decision is more influenced by the answer choices than by the
  question.
---

# Choices Speak Louder than Questions

## Quick Facts
- **arXiv ID**: 2502.18798
- **Source URL**: https://arxiv.org/abs/2502.18798
- **Reference count**: 40
- **Primary result**: 20-60% of MCQA predictions are more influenced by answer choices than by questions, with NPSQ metric isolating question impact more effectively than traditional scoring

## Executive Summary
This paper investigates choice sensitivity in multiple-choice question answering (MCQA), revealing that 20-60% of model predictions are driven more by answer choices than by the questions themselves. The authors decompose model scores into choice-driven and question-driven components, finding that traditional metrics like accuracy and length-normalized accuracy are highly vulnerable to superficial choice properties. To address this, they introduce NPSQ (Normalized Probability Shift by the Question), which isolates the question's impact by normalizing probability shifts relative to baseline choice probabilities. Experiments across different formats (cloze, symbols, hybrid) and models (Qwen 2.5, Llama 3.1, Mistral) demonstrate that NPSQ is much more stable when faced with adversarial choices designed to exploit superficial choice features.

## Method Summary
The paper measures choice sensitivity by decomposing model scores into additive components: one driven by answer choices alone (computed by replacing the question with an empty string), and another driven by the question itself (the residual difference). They introduce NPSQ to isolate question impact by normalizing the probability shift attributable to the question relative to the baseline choice probability. The method evaluates three input formats (cloze, symbols, hybrid) across multiple datasets (HellaSwag, ARC-Challenge, MMLU) and models (Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3). Adversarial choices are used to test metric robustness by replacing distractors with engineered options designed to maximize superficial choice features.

## Key Results
- 20-60% of predictions show choice sensitivity (Δ_choice > Δ_question), with cloze format most affected
- Traditional metrics (accuracy, length-normalized accuracy) are highly vulnerable to adversarial choices, showing >90% drops in some cases
- NPSQ remains largely unaffected by adversarial choices, with <1% prediction changes in cloze format
- Instruction-tuned models show reduced choice sensitivity compared to base models
- Increasing few-shot examples can exacerbate choice sensitivity in symbols and hybrid formats

## Why This Works (Mechanism)

### Mechanism 1: Score Decomposition into Choice-Driven and Question-Driven Components
- Claim: Model predictions can be decomposed into additive choice-driven and question-driven components in log-probability space
- Core assumption: The two components are approximately independent and additive
- Evidence: Models can solve MCQA without questions (Balepur et al., 2024), supporting decomposability
- Break condition: Non-linear interactions between choice-driven and question-driven components invalidate the decomposition

### Mechanism 2: NPSQ Normalization Isolates Question Impact
- Claim: NPSQ neutralizes choice-driven bias by construction, leaving only question-driven signal
- Core assumption: Normalizing by -logP(x|C) appropriately scales shifts across choices with different baseline probabilities
- Evidence: NPSQ equals zero when question is absent, eliminating choice-driven component
- Break condition: Near-uniform baseline probabilities cause numerical instability; question-choice interactions in symbols/hybrid formats

### Mechanism 3: Adversarial Choices Exploit High Choice-Driven Components
- Claim: Traditional scoring methods select adversarial choices because these choices have high choice-driven components
- Core assumption: Adversarial choices have higher choice-driven component scores than original distractors
- Evidence: 93.19% of predictions favor simple adversarial choice in HellaSwag; NPSQ changes <0.17%
- Break condition: If adversarial choices also affect question-driven component, NPSQ may not fully isolate their effect

## Foundational Learning

- **Log-likelihood scoring in MCQA**: Models assign logP(x|Q,C) to each answer candidate; highest-scoring candidate is selected
  - Why needed: Paper's entire analysis decomposes and reinterprets this scoring mechanism
  - Quick check: Given logP(A)=-2.1, logP(B)=-1.8, logP(C)=-3.5, which answer does a standard log-likelihood scorer select? (Answer: B)

- **Length-normalized log-likelihood**: Divides logP by token count to reduce bias toward shorter outputs
  - Why needed: Paper shows this normalization fails to address choice sensitivity and may worsen it
  - Quick check: If logP("short")=-1.0 (1 token) and logP("a much longer phrase")=-1.5 (4 tokens), which has higher length-normalized score? (Answer: "short" at -1.0 vs. -0.375)

- **Probability shift and baseline normalization**: Comparing conditional probabilities under different contexts to isolate causal effects
  - Why needed: NPSQ is fundamentally a normalized probability shift measuring the question's contribution
  - Quick check: If P(x|C)=0.1 and P(x|Q,C)=0.4, what is the raw probability shift in log-space? (Answer: log(0.4)-log(0.1) ≈ 1.39 nats)

## Architecture Onboarding

- **Component map**: Question Q, choices C → logP(x|Q,C) for each candidate x → compute logP(x|C) → apply NPSQ normalization → rank choices by NPSQ
- **Critical path**: Accuracy of NPSQ depends critically on correctly computing logP(x|C) - the baseline without the question
- **Design tradeoffs**: Cloze format shows highest choice sensitivity (50-60%) but NPSQ provides largest correction; symbols/hybrid formats show lower sensitivity (20-40%) but NPSQ yields slightly lower accuracy because choice-driven components can be beneficial
- **Failure signatures**: Near-uniform baseline probabilities causing NPSQ numerical instability; adversarial choices causing >10% accuracy drops indicating traditional metrics are compromised; significant acc vs. acc_npsq divergence indicating substantial choice-driven contamination
- **First 3 experiments**:
  1. Replicate choice sensitivity measurement: For your target model on HellaSwag, compute Δchoice and Δquestion for top-2 candidates; report percentage where Δchoice > Δquestion
  2. Adversarial robustness test: Replace one distractor with "Hello, everyone." (simple choice); compare acc, acc_norm, and acc_npsq stability
  3. Format comparison: Run the same model on ARC-Challenge in cloze vs. symbols format; verify that cloze shows higher choice sensitivity and that NPSQ reverses the accuracy direction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increasing the number of few-shot examples exacerbate choice sensitivity in the symbols and hybrid formats?
- Basis: Observation 5 states that while cloze-based sensitivity is stable, symbols and hybrid formats show higher choice sensitivity as few-shot examples increase
- Why unresolved: Paper demonstrates correlation but doesn't investigate whether this is due to in-context learning prioritizing pattern matching over reasoning
- What evidence would resolve it: Ablation study analyzing attention weights or probability contributions of few-shot demonstrations relative to target question and choices

### Open Question 2
- Question: Does instruction tuning reduce choice sensitivity by enhancing instruction adherence or by fundamentally altering the model's priors on answer distributions?
- Basis: Observation 6 notes instruction-tuned models consistently show reduced choice sensitivity compared to base models
- Why unresolved: Paper establishes reduction but doesn't isolate whether model learns to weight "question-solving instruction" more heavily or if training data suppresses spurious choice correlations
- What evidence would resolve it: Mechanistic interpretability analysis comparing internal representation of choice-driven component in base versus instruction-tuned models

### Open Question 3
- Question: Can the NPSQ metric be refined to fully decouple the interdependence of answer choices in symbols and hybrid formats?
- Basis: Section 5.1 admits that for symbols and hybrid formats, altering one choice impacts NPSQ values of other choices, causing small but non-zero sensitivity
- Why unresolved: Current formulation computes NPSQ for all choices together in these formats, introducing conditional dependencies
- What evidence would resolve it: Derivation of a conditional NPSQ variant that normalizes probability shift of a specific candidate independent of distractor set's composition

## Limitations

- The additive decomposition assumes independence between choice-driven and question-driven components without empirical validation
- NPSQ shows numerical instability when baseline probabilities are near-uniform, though frequency of occurrence is unreported
- The paper doesn't explore whether adversarial patterns generalize across different model architectures or training paradigms

## Confidence

- **High confidence**: Empirical observation that 20-60% of predictions show choice sensitivity is well-supported by direct measurements across multiple datasets and models
- **Medium confidence**: Additive score decomposition mechanism is theoretically sound but relies on assumptions about component independence that aren't empirically validated
- **Low confidence**: Assertion that NPSQ provides the "correct" evaluation metric is not fully justified - paper shows it's more stable against adversarial choices but doesn't demonstrate it better measures genuine question understanding

## Next Checks

1. **Decomposability validation**: For a subset of instances where choice sensitivity is high, empirically test whether Score(Q,C,x) ≈ Score_choice(Q,C,x) + Score_question(Q,C,x) holds by measuring correlation and residuals across the three score components

2. **Numerical stability analysis**: Characterize the distribution of baseline probabilities P(x|C) across all choices in the test sets, reporting the frequency of cases where -logP(x|C) falls below thresholds (e.g., 10, 5, 1) that would cause NPSQ instability

3. **Cross-architecture robustness**: Test whether observed choice sensitivity patterns and NPSQ effectiveness transfer to decoder-only models (like GPT-4) and to models trained with different objectives (e.g., reward models versus standard language models)