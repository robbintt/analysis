---
ver: rpa2
title: 'Cache-to-Cache: Direct Semantic Communication Between Large Language Models'
arxiv_id: '2510.03215'
source_url: https://arxiv.org/abs/2510.03215
tags:
- cache
- receiver
- sharer
- kv-cache
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of text-based communication
  between large language models (LLMs) in multi-LLM systems, where rich semantic information
  is lost and latency is introduced. The authors propose Cache-to-Cache (C2C), a new
  paradigm that enables direct semantic communication between LLMs by projecting and
  fusing KV-caches from a source model into a target model.
---

# Cache-to-Cache: Direct Semantic Communication Between Large Language Models

## Quick Facts
- **arXiv ID:** 2510.03215
- **Source URL:** https://arxiv.org/abs/2510.03215
- **Reference count:** 33
- **Primary result:** C2C achieves 8.5-10.5% higher average accuracy than individual models and 2.0× speedup in latency compared to text-based communication.

## Executive Summary
This paper addresses the limitations of text-based communication between large language models (LLMs) in multi-LLM systems, where rich semantic information is lost and latency is introduced. The authors propose Cache-to-Cache (C2C), a new paradigm that enables direct semantic communication between LLMs by projecting and fusing KV-caches from a source model into a target model. C2C uses a neural cache fuser with learnable gating to select which layers benefit from cache communication, avoiding explicit intermediate text generation. Experiments show C2C achieves 8.5-10.5% higher average accuracy than individual models and outperforms text communication by 3.0-5.0% while delivering 2.0× speedup in latency. The method is validated across diverse model combinations and benchmarks, demonstrating its effectiveness for scalable, low-latency multi-LLM systems.

## Method Summary
C2C introduces a novel approach to direct semantic communication between heterogeneous LLMs by operating on their KV-caches rather than text outputs. The method consists of a Cache-to-Cache Fuser that projects KV-cache from a source (Sharer) model into the representation space of a target (Receiver) model, then selectively fuses the projected cache into the Receiver's own cache using learnable per-layer gates. The fuser uses a 3-layer MLP for projection, dynamic weighting for fusion, and Gumbel-sigmoid gates for layer selection. Token alignment is achieved through string-based decoding and re-encoding across different tokenizers. The system is trained on OpenHermes2.5 Dataset with next-token prediction loss, freezing the LLM parameters while optimizing only the fuser. The approach is evaluated on MMLU-Redux, ARC-Challenge, OpenBookQA, and C-Eval benchmarks across multiple model combinations.

## Key Results
- C2C achieves 8.5-10.5% higher average accuracy than individual models across benchmarks
- C2C outperforms text communication by 3.0-5.0% in accuracy while delivering 2.0× speedup in latency
- Gated layer-selective fusion prevents destructive overwriting, with some layers benefiting (+2% accuracy) while others degrade
- Cross-model representation space alignment is successful, with projected caches occupying the target model's space

## Why This Works (Mechanism)

### Mechanism 1: KV-Cache as a Higher-Bandwidth Semantic Medium
KV-cache carries richer semantic information than text output, enabling better cross-model knowledge transfer. Oracle experiments show that enriching a model's KV-cache (via few-shot examples) and discarding exemplar tokens while keeping only the question-aligned cache slice improves accuracy at the same cache length. This suggests internal representations encode semantic traces that are lost during tokenization and text generation.

### Mechanism 2: Cross-Model Representation Space Alignment via Learned Projection
A lightweight neural projector can map KV-cache from one model's representation space into another's, enabling semantic transfer across heterogeneous architectures. A 3-layer MLP transforms source cache vectors, and t-SNE visualization shows raw caches are distant but transformed caches occupy a subset of the target's space. The transformation is trained via next-token prediction loss on the receiver model.

### Mechanism 3: Gated Layer-Selective Fusion Prevents Destructive Overwriting
Not all layers benefit from external cache injection; learnable gating selectively enables layers where fusion improves predictions. Each layer has a trainable gate (Gumbel-sigmoid with temperature annealing). Layer-wise enrichment experiments show some layers benefit (layers 4, 16: +2% accuracy) while others degrade. During inference, gates become binary, selecting which layers receive fused cache.

## Foundational Learning

- **Concept: KV-Cache in Autoregressive Transformers**
  - Why needed here: C2C operates directly on KV-cache during prefill and decode; without understanding what's cached and why, you can't debug fusion failures.
  - Quick check question: If a model has 32 layers and processes a 100-token input, how many cache entries exist after prefill?

- **Concept: Attention Head Dynamics and Residual Integration**
  - Why needed here: The fuser uses "residual integration" and "head modulation"—these require understanding how multi-head attention combines signals and how residual connections preserve original information.
  - Quick check question: Why might directly replacing (rather than residual-adding) target cache with projected source cache destroy information?

- **Concept: Gumbel-Sigmoid and Differentiable Discrete Choices**
  - Why needed here: C2C uses Gumbel-sigmoid for gates to enable differentiable training while producing binary decisions at inference.
  - Quick check question: What happens to gradient flow if you use a hard binary gate during training?

## Architecture Onboarding

- **Component map:** Sharer Model → produces S-Cache during prefill → Token Alignment Module → Cache Projection Network → Feature Fusion + Dynamic Weighting → Learnable Gates → Receiver Model (uses fused cache C^F for decoding)

- **Critical path:** Input tokenization → Sharer prefill (S-Cache) + Receiver prefill (R-Cache) → Token alignment → Layer-wise projection → Gated fusion → Receiver decode with C^F

- **Design tradeoffs:**
  - Terminal vs. depth-normalized layer alignment: Terminal aligns output-side layers first (chosen for simplicity); depth-normalized spreads alignment uniformly
  - Maximal-coverage vs. first-occurrence token alignment: Maximal-coverage preserves more information in one-to-many tokenization cases
  - Simple C2C vs. C2C-C (complex): C2C-C adds projection stage before fusion, achieving 76-86% PGR but increases parameter count

- **Failure signatures:**
  - Accuracy drops below receiver-only baseline → likely token alignment failure or gate overfitting
  - Inference time not improved → source model prefill not parallelized or fuser overhead dominates
  - Gates all-open but no accuracy gain → dynamic weights may be near-zero; check weighting module

- **First 3 experiments:**
  1. **Token alignment validation:** Run C2C with identical model as both sharer and receiver (Identical setting from Table 5); if accuracy doesn't improve, tokenization alignment is broken.
  2. **Single-layer gate ablation:** Manually enable fusion only on layer 16 (high-performer in Table 9) and disable all others; compare to all-gates-open to validate gate utility.
  3. **Cross-family stress test:** Pair models from different families (e.g., Gemma3-1B sharer → Qwen3-0.6B receiver); verify t-SNE shows projected cache within target space before full benchmark runs.

## Open Questions the Paper Calls Out

- **Privacy-aware cloud–edge collaboration:** Can transmitting fused KV-Caches provide robust privacy protection compared to text-based communication? The authors suggest C2C limits exposure but don't evaluate whether the KV-Cache itself is vulnerable to reconstruction attacks or if sensitive information can be extracted from the fused representations.

- **Performance Gap Recovered (PGR) with complex fusers:** To what extent does increasing the architectural complexity of the Cache Fuser improve PGR? The paper mentions a "C2C-C (Complex)" variant with an additional projection stage that "attains stronger performance" (e.g., 60.63% vs 44.40% on C-Eval) but leaves systematic investigation of more elaborate architectures to future work.

- **Multimodal integration:** Can C2C effectively align and fuse caches across heterogeneous modalities, such as between Vision-Language Models (VLMs) and text-only LLMs? The paper suggests the alignment of caches among language LLMs, VLMs, and Vision-Language-Action (VLA) policies as a potential scenario, but the current work focuses exclusively on text-based LLMs.

## Limitations
- Limited empirical validation across diverse model architectures and scales, with testing restricted to 3-4 models total
- Heavy dependency on token alignment through string-based decoding and re-encoding, with no analysis of alignment failure rates or information loss
- Training on OpenHermes2.5 Dataset raises questions about generalizability to domains outside the training distribution

## Confidence
- **High Confidence (0.85):** Core architectural design (projection + gating + fusion) is technically sound and implementable; empirical improvements in accuracy and latency are reproducible; t-SNE visualization results showing successful cache space alignment are credible
- **Medium Confidence (0.65):** Semantic richness preservation in KV-cache versus text output; assertion that C2C enables "scalable, low-latency multi-LLM systems" for arbitrary combinations; generalizability of layer-wise gating benefits across diverse model architectures
- **Low Confidence (0.45):** Scalability claims to very large models (70B+) or highly heterogeneous architectures; robustness of token alignment under extreme tokenization differences; long-term stability of learned gates across diverse downstream applications

## Next Checks
- **Validation Check 1:** Pair a byte-level tokenizer model (e.g., GPT-2) with a word-piece tokenizer model (e.g., BERT) as sharer and receiver; measure alignment success rate and information loss using BLEU score between original and remapped sequences.
- **Validation Check 2:** Implement C2C between Qwen2.5-0.5B as sharer and Llama3.2-3B as receiver; measure accuracy improvements and compare gate activation patterns to same-scale pairs.
- **Validation Check 3:** Train the fuser on OpenHermes2.5, then evaluate on specialized domains (medical, legal, code) without fine-tuning; compare performance to models fine-tuned on these domains to measure transfer capability.