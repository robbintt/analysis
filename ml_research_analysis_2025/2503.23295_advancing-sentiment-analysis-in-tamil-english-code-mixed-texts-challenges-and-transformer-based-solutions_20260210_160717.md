---
ver: rpa2
title: 'Advancing Sentiment Analysis in Tamil-English Code-Mixed Texts: Challenges
  and Transformer-Based Solutions'
arxiv_id: '2503.23295'
source_url: https://arxiv.org/abs/2503.23295
tags:
- sentiment
- texts
- data
- code-mixed
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates transformer-based models for sentiment analysis\
  \ in Tamil-English code-mixed texts, addressing challenges such as grammatical inconsistencies,\
  \ orthographic variations, and phonetic ambiguities. Four models\u2014XLM-RoBERTa,\
  \ mT5, IndicBERT, and RemBERT\u2014were tested on three datasets (SAIL, CMD-Tamil,\
  \ DravidianCodeMix)."
---

# Advancing Sentiment Analysis in Tamil-English Code-Mixed Texts: Challenges and Transformer-Based Solutions

## Quick Facts
- arXiv ID: 2503.23295
- Source URL: https://arxiv.org/abs/2503.23295
- Reference count: 1
- Primary result: RemBERT achieved 87.5% accuracy and 86.4% F1-score on Tamil-English code-mixed sentiment classification

## Executive Summary
This study evaluates transformer-based models for sentiment analysis in Tamil-English code-mixed texts, addressing challenges such as grammatical inconsistencies, orthographic variations, and phonetic ambiguities. Four models—XLM-RoBERTa, mT5, IndicBERT, and RemBERT—were tested on three datasets (SAIL, CMD-Tamil, DravidianCodeMix). RemBERT achieved the highest accuracy at 87.5% and an F1-score of 86.4%, outperforming other models. mT5 also showed strong performance (86.3% accuracy). Errors were mainly due to sarcasm, idiomatic expressions, and phonetic variations. The study highlights the need for expanded datasets, phonetic normalization, and hybrid modeling approaches to improve sentiment analysis in low-resource, code-mixed environments.

## Method Summary
The study fine-tuned four transformer models (XLM-RoBERTa, mT5, IndicBERT, RemBERT) on three Tamil-English code-mixed sentiment datasets. Text normalization and SentencePiece tokenization were applied as preprocessing steps, with back-translation augmentation used to enhance training data. Models were trained with batch_size=16, learning_rate=3×10⁻⁵, and early stopping on validation loss for up to 10 epochs. Performance was evaluated using accuracy and F1-score metrics across positive, neutral, and negative sentiment classes.

## Key Results
- RemBERT achieved the highest accuracy (87.5%) and F1-score (86.4%) among all tested models
- mT5 showed strong performance with 86.3% accuracy
- Latin-script code-mixed text was processed more accurately than Tamil-script text due to better pretraining alignment
- Sarcasm and idiomatic expressions caused 35% of classification errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RemBERT outperforms other multilingual transformers on Tamil-English code-mixed sentiment classification under the tested conditions.
- Mechanism: RemBERT's embedding architecture combines improved multilingual representations with multitask learning, enabling better handling of code-switched syntactic structures compared to models pretraining primarily on monolingual or less diverse multilingual corpora.
- Core assumption: The observed performance gap reflects architectural differences rather than random variation or hyperparameter tuning alone.
- Evidence anchors:
  - [abstract] "RemBERT achieved the highest accuracy at 87.5% and an F1-score of 86.4%, outperforming other models."
  - [section] Table 5: RemBERT 87.5% accuracy vs. XLM-RoBERTa 84.2%, IndicBERT 83.8%
  - [corpus] Weak direct corroboration; neighboring papers focus on different language pairs (Sinhala, Persian, Hindi-Bengali) but similarly find transformer selection significantly impacts code-mixed tasks.
- Break condition: Performance advantage disappears on larger or more balanced datasets; gains may stem from dataset-specific patterns rather than architecture.

### Mechanism 2
- Claim: Latin-script code-mixed text is processed more accurately than Tamil-script text by pretrained transformers.
- Mechanism: Pretrained multilingual models have seen substantially more Latin-script data during pretraining, resulting in better-aligned embeddings for Romanized Tamil compared to native Tamil script tokens.
- Core assumption: Script-specific embedding quality drives the accuracy gap rather than annotation quality differences.
- Evidence anchors:
  - [abstract] "orthographic variations, and phonetic ambiguities" identified as key challenges.
  - [section] Table 6: "Use of Latin script Increases accuracy due to better adaptation of pre-trained embeddings"
  - [corpus] Paper 77363 on Romanized Hindi/Bengali similarly emphasizes transliteration normalization as critical, indirectly supporting script sensitivity.
- Break condition: Models pretrained with balanced Tamil-script exposure eliminate this gap; current findings may reflect pretraining data imbalance, not inherent script difficulty.

### Mechanism 3
- Claim: Sarcasm and idiomatic expressions cause the largest proportion of sentiment classification errors.
- Mechanism: Transformer token-level attention captures surface sentiment words (e.g., "comedy") without integrating the broader pragmatic or cultural context required to detect irony.
- Core assumption: Error distribution generalizes beyond these three datasets.
- Evidence anchors:
  - [abstract] "Errors were mainly due to sarcasm, idiomatic expressions, and phonetic variations."
  - [section] Table 7: 35% of errors from sarcasm/idioms; example misclassification provided (sarcastic "comedy" → false positive)
  - [corpus] Weak direct support; corpus papers do not specifically quantify sarcasm error rates in code-mixed settings.
- Break condition: Dedicated sarcasm detection modules or culturally-informed training data substantially reduce this error category.

## Foundational Learning

- **Code-mixing vs. code-switching**
  - Why needed here: The paper conflates these terms but they differ; code-mixing involves intra-sentential language alternation, while code-switching can occur at sentence boundaries. Understanding this affects tokenization strategy.
  - Quick check question: Given "Indha padam sema comedy," can you identify which language each token belongs to and whether this is intra-sentential mixing?

- **Subword tokenization (SentencePiece/BPE)**
  - Why needed here: The preprocessing pipeline relies on SentencePiece to handle Tamil's agglutinative morphology and phonetic spelling variations.
  - Quick check question: How would the subword token "sema" (Tamil "very" transliterated) be segmented differently from a purely English word?

- **F1-score for imbalanced classes**
  - Why needed here: Dataset class distributions are imbalanced (e.g., SAIL: 6000/5000/4000); accuracy alone is misleading.
  - Quick check question: If a model predicts "positive" for all samples in a dataset with 70% positive labels, what would the accuracy be? What would the F1-score for the minority class likely show?

## Architecture Onboarding

- **Component map:**
  Input layer → Text normalization (transliteration unification) → SentencePiece tokenizer → Transformer backbone (RemBERT/mT5/IndicBERT/XLM-RoBERTa) → Classification head (3 classes: positive/neutral/negative) → Output
  - Augmentation module (back-translation) operates as a preprocessing step, not inline.

- **Critical path:**
  1. Data preprocessing (normalization + tokenization quality determines downstream success)
  2. Model selection (RemBERT recommended based on reported results, but requires validation on your data)
  3. Fine-tuning with early stopping on validation loss

- **Design tradeoffs:**
  - RemBERT: Best reported accuracy, but larger memory footprint; verify inference latency requirements.
  - mT5: Strong generative capabilities, useful if future tasks require text generation; higher computational cost than encoder-only models.
  - IndicBERT: Smaller, optimized for Indian languages; underperformed in this study—may still be viable for resource-constrained deployments.
  - Script handling: Supporting both Tamil and Latin scripts increases vocabulary size and tokenization complexity.

- **Failure signatures:**
  - Sarcasm misclassification (positive words in negative pragmatic context)
  - Phonetic spelling variations causing OOV or incorrect subword segmentation
  - Long sentences with mixed syntax losing sentiment signal in attention diffusion

- **First 3 experiments:**
  1. **Baseline replication:** Replicate Table 5 results using the same three datasets and hyperparameters; verify RemBERT superiority holds on your infrastructure.
  2. **Script ablation:** Train separate models on Latin-only subsets vs. mixed-script data; quantify the script effect suggested in Table 6.
  3. **Error-focused sampling:** Extract sarcasm/idiom examples from validation set; evaluate whether adding explicit contrastive examples improves performance on this subset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can transformer architectures be optimized to effectively detect sarcasm and idiomatic expressions in Tamil-English code-mixed texts?
- Basis in paper: [explicit] The error analysis identified sarcasm and idioms as the source of 35% of errors (Table 7), and Section 6.3 explicitly calls for training models with cultural context to handle these linguistic constructs.
- Why unresolved: Current models like RemBERT rely on lexical cues (e.g., identifying "comedy" as positive) and lack the mechanisms to interpret pragmatic intent or cultural nuances in code-mixed scenarios.
- What evidence would resolve it: A modified architecture or attention mechanism that demonstrates a statistically significant reduction in sarcasm-related misclassification compared to the baseline RemBERT model.

### Open Question 2
- Question: To what extent does integrating explicit phonetic normalization modules into the preprocessing pipeline improve sentiment classification accuracy?
- Basis in paper: [explicit] Section 5.4 and Section 6.3 list "Developing phonetics-aware models" and "phonetic normalization" as key strategies to address the 22% of errors caused by phonetic variations and spelling ambiguities.
- Why unresolved: The study implemented standard tokenization (SentencePiece), but the results show that orthographic variability and "phonetic errors" remain significant hurdles for models trained on Romanized text.
- What evidence would resolve it: Ablation studies comparing standard tokenization against pipelines utilizing phonetic normalization, showing a measurable increase in Precision and Recall for phonetically varied inputs.

### Open Question 3
- Question: Can semi-supervised or unsupervised learning approaches effectively bridge the performance gap caused by the limited size of annotated Tamil-English code-mixed corpora?
- Basis in paper: [explicit] Section 6.3 proposes "Applying semi-supervised and unsupervised methods" to reduce dependency on annotated data, addressing the limitation noted in Section 6.2 regarding insufficient dataset sizes (e.g., SAIL has only 15,000 samples).
- Why unresolved: The current study relied entirely on supervised learning with limited datasets (SAIL, CMD-Tamil, DravidianCodeMix), which restricts the models' ability to generalize to the vast variability of code-mixed syntax.
- What evidence would resolve it: Experiments demonstrating that self-supervised pre-training on unlabeled code-mixed data achieves comparable or superior F1-scores to the current supervised baselines while using fewer labeled examples.

## Limitations
- Dataset dependency: Performance advantages based on three specific code-mixed datasets; generalization to other domains remains untested
- Script-specific pretraining gap: Latin-script advantage likely reflects pretraining data imbalance rather than inherent script difficulty
- Error characterization scope: Sarcasm/idiom error quantification based on limited manual inspection, may not capture systematic issues

## Confidence
- **High confidence**: RemBERT achieves the highest accuracy (87.5%) among tested models on the specific datasets used
- **Medium confidence**: Latin-script code-mixed text is processed more accurately than Tamil-script text due to better-aligned embeddings
- **Low confidence**: Sarcasm and idiomatic expressions cause the largest proportion of sentiment classification errors

## Next Checks
1. **Dataset generalization test**: Evaluate the same models on a held-out Tamil-English code-mixed dataset from a different source to verify whether RemBERT's accuracy advantage persists across domains and annotation styles
2. **Script exposure ablation**: Create two versions of the existing datasets - one with Latin-script only and one with Tamil-script only - and train separate models on each to quantify the true impact of script-specific pretraining exposure
3. **Sarcasm detection module integration**: Extract all false positive examples where models misclassified sarcastic content as positive, create a dedicated sarcasm detection module, and measure performance improvement specifically on this error subset