---
ver: rpa2
title: A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization
arxiv_id: '2511.19829'
source_url: https://arxiv.org/abs/2511.19829
tags:
- prompt
- optimization
- evaluation
- evaluator
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified evaluation-instructed framework
  for query-dependent prompt optimization. Unlike static-template approaches, it dynamically
  tailors prompts to individual queries while leveraging multi-dimensional evaluation
  metrics.
---

# A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization

## Quick Facts
- arXiv ID: 2511.19829
- Source URL: https://arxiv.org/abs/2511.19829
- Reference count: 36
- Key outcome: Dynamic, query-dependent prompt optimization framework using multi-metric evaluation and diagnosers achieves 5-10% accuracy gains across 8 datasets and 3 models.

## Executive Summary
This paper introduces a unified evaluation-instructed framework for query-dependent prompt optimization. Unlike static-template approaches, it dynamically tailors prompts to individual queries while leveraging multi-dimensional evaluation metrics. The method first constructs a diverse prompt corpus, selects performance-reflective metrics (NLL, stability, MI, query entropy), and trains a lightweight evaluator to predict these scores without prompt execution. During optimization, metric-specific diagnosers identify failure modes and generate targeted revisions guided by evaluator gradients. The framework is evaluated across eight datasets and three backbone models, consistently outperforming both static-template and query-dependent baselines. The evaluator achieves 83.7% accuracy in prompt performance prediction and delivers 5-10% average accuracy gains across tasks. Notably, the method generalizes to unseen domains like medical QA and maintains performance across different LLMs, demonstrating model-agnostic portability. The approach addresses limitations of unstable textual feedback and black-box reward models by grounding optimization in interpretable, metric-aligned signals.

## Method Summary
The framework optimizes prompts through a two-phase process: training and inference. During training, it generates 11,530 diverse prompts from static templates, LLM generation, and evolutionary recombination. Each prompt is executed 10 times to compute four metrics (NLL, stability, MI, query entropy) and a binary quality label based on accuracy >50%. A LLaMA-3-8B-Instruct evaluator with LoRA adaptation is trained to predict these metrics and the quality score from text alone, using a bi-level objective where metric regression supports classification. During inference, given a query and candidate prompt, the evaluator predicts quality and metric scores. If quality is low, gradient attribution identifies problematic metric dimensions, triggering corresponding diagnosers that generate targeted revisions. The process iterates up to three times, producing query-specific optimized prompts.

## Key Results
- Evaluator achieves 83.7% accuracy in predicting prompt quality vs. 69% baseline using embedding+XGBoost
- Framework delivers 5-10% average accuracy gains across eight datasets (BBH, GPQA, LegalBench) and three backbone models (Llama-3-8B, Llama-3-70B, GPT-4o)
- Cross-model generalization: optimizer trained on Llama-3-8B generalizes to GPT-4o with consistent performance improvements
- Outperforms static-template baselines and query-dependent baselines in both accuracy and robustness across domains

## Why This Works (Mechanism)

### Mechanism 1: Execution-Free Multi-Dimensional Quality Prediction
The framework predicts prompt quality from text alone using a learned evaluator that estimates four performance-correlated metrics (NLL, stability, MI, query entropy) and a binary quality score. This execution-free approach reduces computational cost while maintaining prediction accuracy (83.7%). The evaluator uses a shared encoder with metric regression heads that feed into a fusion MLP before classification, creating semantics-aware quality assessment.

### Mechanism 2: Gradient-Informed Metric Weighting for Dynamic Prioritization
During training, the framework dynamically adjusts the relative importance of each metric based on its gradient impact on the classification objective. After each forward pass, gradients of classification loss with respect to predicted metrics are computed, and weights are updated to prioritize metrics that exert stronger gradients. This creates closed-loop alignment between regression supervision and the end task, with learned weights (query entropy 32.7%, NLL 26.4%, stability 22.3%, MI 18.6%) aligning with feature importance from ablation studies.

### Mechanism 3: Metric-Specific Failure Diagnosis and Targeted Revision
When a prompt is classified as low-quality, the framework uses gradient attribution to identify which metric dimensions contributed most negatively to the quality score. Metric-specific diagnosers then inspect the prompt for issues related to each metric (e.g., instruction-conflict for NLL, format guard for stability) and generate targeted corrective edits. This decomposition of failure modes into interpretable dimensions enables precise, stable optimization rather than monolithic improvement signals.

## Foundational Learning

- **Concept: Negative Log-Likelihood (NLL) for Prompt Quality**
  - Why needed here: NLL measures model confidence in the correct answer under a given prompt. High NLL indicates the prompt fails to constrain the output distribution toward correctness.
  - Quick check question: Given a prompt and query, can you explain why a higher NLL suggests poorer prompt guidance?

- **Concept: Mutual Information (MI) Between Query and Response**
  - Why needed here: MI quantifies how much the prompt influences the response beyond the query alone. Low MI indicates the prompt is "hollow"—adding tokens without actionable guidance.
  - Quick check question: Why might a verbose, polite prompt have lower MI than a concise, schema-driven one?

- **Concept: LoRA (Low-Rank Adaptation) for Efficient Fine-Tuning**
  - Why needed here: The evaluator uses LoRA to adapt only ~5.5% of LLaMA-8B parameters, maintaining efficiency and portability while learning task-specific evaluation behavior.
  - Quick check question: What are the tradeoffs of LoRA vs. full fine-tuning for a multi-task evaluator?

## Architecture Onboarding

- **Component map**: Prompt Corpus Generator → Metric Computation Module → Evaluator (E_θ) → Gradient-Based Weighting → Metric-Specific Diagnosers → Optimizer → Improved Prompt
- **Critical path**: Training phase: Corpus generation → metric computation → evaluator training with bi-level objective. Inference phase: Query + candidate prompt → evaluator predicts quality and metrics → if low-quality, gradient attribution identifies problem dimensions → corresponding diagnosers generate revisions → new prompt is re-evaluated (max 3 iterations).
- **Design tradeoffs**: Diversity vs. efficiency in corpus generation (evolutionary recombination adds diversity but increases size); Metric selection vs. comprehensiveness (four metrics reduce noise but may miss task-specific dimensions); Execution-free prediction vs. accuracy (evaluator removes execution cost but achieves 83.7% accuracy).
- **Failure signatures**: Evaluator underconfidence (all ŷ near 0.5 → weak gradients); Diagnoser conflicts (revised prompts perform worse → conflicting suggestions); Cross-model degradation (gains disappear on new LLM → overfitting to training model).
- **First 3 experiments**: 1) Metric importance validation: replicate XGBoost feature importance analysis on your dataset. 2) Evaluator ablation: train with/without predicted metrics, feature fusion, dynamic weighting. 3) Cross-model transfer test: train on one LLM, evaluate on different LLM without retraining.

## Open Questions the Paper Calls Out

- **Question**: How can the unified evaluation framework be extended to incorporate non-performance dimensions such as safety, token efficiency, and user-centric factors like readability?
  - Basis in paper: The Limitation section explicitly states that the current framework "focuses primarily on performance-related metrics" and "does not account for other important dimensions such as safety, token efficiency, or user-centric factors."

- **Question**: Can the evaluator be trained to explicitly distinguish between "prompt-induced failure" and "model capacity limits" to prevent ineffective optimization loops?
  - Basis in paper: The authors note in the Limitation section that for MATH500 on smaller models, "errors arise from the backbone model's inability... making prompt optimization largely ineffective."

- **Question**: Does the specific selection of performance-reflective metrics (NLL, stability, MI, query entropy) generalize to open-ended generation tasks where ground-truth accuracy is undefined?
  - Basis in paper: The paper validates its approach exclusively on reasoning and QA benchmarks (BBH, GPQA, LegalBench) which have definitive ground truths, leaving open-ended tasks unexplored.

## Limitations
- The framework requires executing each prompt 10 times during training to compute metrics, creating substantial upfront computational burden
- Evaluator achieves 83.7% accuracy, meaning approximately 16% of quality predictions are incorrect and could compound during iterative optimization
- Metric-specific diagnosers' implementation details remain underspecified, particularly how gradient information translates into actionable revision suggestions
- The framework focuses exclusively on accuracy as the downstream objective, explicitly excluding safety, token efficiency, and computational cost considerations

## Confidence

- **High Confidence**: The evaluator's architecture and training methodology are clearly specified and technically sound. The 83.7% prediction accuracy vs. 69% baseline provides strong empirical support for the execution-free quality prediction mechanism.

- **Medium Confidence**: The gradient-informed metric weighting mechanism produces interpretable results (weights align with XGBoost importance), but the exact implementation details and hyperparameter sensitivity are not fully disclosed. The cross-model generalization claims rest on limited empirical validation.

- **Low Confidence**: The metric-specific diagnosers' effectiveness is asserted through aggregate performance gains (5-10%) rather than systematic ablation studies. The conflict resolution strategy for competing diagnostic suggestions is not described.

## Next Checks

1. **Metric Correlation Validation**: Compute Pearson/Spearman correlations between each predicted metric (NLL, stability, MI, query entropy) and actual task accuracy on a held-out dataset. Confirm that correlation coefficients exceed 0.3 for at least three metrics to validate the foundational assumption that these signals predict performance.

2. **Diagnoser Ablation Study**: Implement an ablation where (a) all diagnosers are active, (b) only the top-2 metrics by weight are used, and (c) no diagnosers (random prompt selection). Measure not just final accuracy but also the stability of improvements across multiple optimization rounds to determine whether diagnosers provide consistent guidance or introduce noise.

3. **Cross-Domain Transfer Test**: Evaluate the framework on a domain outside the training distribution (e.g., medical QA, code generation, or sentiment analysis) using prompts generated from a different LLM family (e.g., Claude or GPT-4) without any fine-tuning. Measure both evaluator accuracy and downstream task performance to assess true model-agnostic portability.