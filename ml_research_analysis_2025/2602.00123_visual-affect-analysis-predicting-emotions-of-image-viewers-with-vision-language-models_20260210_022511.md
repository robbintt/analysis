---
ver: rpa2
title: 'Visual Affect Analysis: Predicting Emotions of Image Viewers with Vision-Language
  Models'
arxiv_id: '2602.00123'
source_url: https://arxiv.org/abs/2602.00123
tags:
- affective
- emotion
- image
- ratings
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks nine state-of-the-art vision-language models
  on predicting human affective ratings using three psychometrically validated image
  datasets: the International Affective Picture System, the Nencki Affective Picture
  System, and the Library of AI-Generated Affective Images. Models performed emotion
  classification and continuous rating prediction in a zero-shot setting, and the
  impact of rater-conditioned prompting was evaluated on the LAI-GAI dataset.'
---

# Visual Affect Analysis: Predicting Emotions of Image Viewers with Vision-Language Models

## Quick Facts
- **arXiv ID:** 2602.00123
- **Source URL:** https://arxiv.org/abs/2602.00123
- **Reference count:** 40
- **Primary result:** VLMs achieve 60-80% accuracy in emotion classification and r > 0.75 correlation with human ratings for valence and dominance, but systematically overestimate intensity and struggle with arousal and surprise dimensions

## Executive Summary
This paper presents a comprehensive benchmark of nine state-of-the-art vision-language models (VLMs) for predicting human affective responses to images. The study evaluates models across three psychometrically validated datasets - IAPS, NAPS, and LAI-GAI - in a zero-shot setting for both emotion classification and continuous rating prediction. Models demonstrate strong performance in emotion classification (60-80% accuracy) and moderate-to-strong alignment with human ratings for valence and dominance (r > 0.75), but show systematic overestimation of rating intensity and weaker performance on arousal and surprise dimensions. The minimal impact of rater-conditioned prompting suggests current personalization strategies may need refinement for affective computing applications.

## Method Summary
The study benchmarks nine VLMs (GPT-4V, Gemini Pro Vision, Claude 3, LLaVA, InstructBLIP, BLIP-2, MMBT, VinVL, and METER) across three established affective image datasets. Models perform zero-shot prediction for both discrete emotion classification and continuous valence, arousal, and dominance ratings. The evaluation uses established psychometrics metrics including Spearman correlation, mean absolute error, and classification accuracy. A key experimental component tests rater-conditioned prompting on the LAI-GAI dataset to assess whether providing model information about the rater's characteristics improves prediction accuracy.

## Key Results
- Emotion classification accuracy ranges from 60-80% across VLMs, with GPT-4V and Claude 3 showing top performance
- Continuous rating predictions achieve strong correlation with human ratings (r > 0.75) for valence and dominance, but weaker performance for arousal (r ~ 0.6-0.7)
- All models systematically overestimate the intensity of ratings compared to human responses, with mean absolute errors of 0.5-1.0 on 9-point scales
- Rater-conditioned prompting shows minimal impact on prediction accuracy, suggesting current personalization strategies are ineffective

## Why This Works (Mechanism)
Vision-language models leverage their cross-modal training to map visual features to linguistic descriptions of emotions, enabling zero-shot affect prediction. The strong performance on valence and dominance reflects the models' ability to capture semantic and contextual cues in images that align with human emotional perception. The systematic overestimation suggests models may rely on more extreme emotional associations learned during pretraining. The weaker performance on arousal and surprise indicates these dimensions may require more nuanced understanding of dynamic or subtle emotional cues that are less prominent in standard pretraining data.

## Foundational Learning

**Vision-Language Models** - Multimodal AI systems trained on paired image-text data to perform cross-modal tasks. *Why needed:* Form the core technology enabling zero-shot affect prediction without specialized training. *Quick check:* Verify model can correctly caption images before testing affect prediction.

**Psychometrically Validated Datasets** - Standardized image collections with human-rated emotional responses (valence, arousal, dominance). *Why needed:* Provide ground truth for evaluating affect prediction accuracy. *Quick check:* Confirm dataset ratings follow expected distributions and inter-rater reliability.

**Zero-Shot Learning** - Model evaluation without task-specific fine-tuning, using only pretrained knowledge. *Why needed:* Tests generalization capability and practical applicability of VLMs. *Quick check:* Ensure prompts are consistent across all models being compared.

## Architecture Onboarding

**Component Map:** Image input -> Vision encoder -> Text encoder -> Cross-attention layers -> Text decoder -> Emotion/rating output

**Critical Path:** Vision encoder extracts visual features → Cross-attention layers fuse visual and linguistic representations → Text decoder generates affect predictions based on learned mappings from pretraining

**Design Tradeoffs:** Zero-shot evaluation vs. fine-tuned performance; comprehensive benchmarking across multiple datasets vs. focused analysis of specific affect dimensions; standard prompting vs. rater-conditioned personalization

**Failure Signatures:** Systematic overestimation of rating intensity; weaker performance on arousal and surprise dimensions; minimal improvement from rater-conditioned prompting; performance drops on images with subtle emotional content

**First Experiments:**
1. Test basic image captioning to verify vision-language model functionality
2. Evaluate emotion classification on a small subset of images to establish baseline performance
3. Compare correlation between model predictions and human ratings for valence vs. arousal

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions for future research, but the findings suggest several important areas: whether fine-tuning on affective datasets would significantly improve performance, how to develop more effective personalization strategies beyond simple rater-conditioned prompting, and whether cross-cultural validation would reveal systematic biases in VLM affect recognition capabilities.

## Limitations

- Zero-shot evaluation without fine-tuning may underestimate the potential performance of VLMs for affect prediction
- Systematic overestimation of rating intensity suggests calibration issues that could impact real-world deployment
- Performance on arousal and surprise dimensions is notably weaker than for other emotion categories
- Rater-conditioned prompting shows minimal impact, but only limited prompting strategies were tested

## Confidence

**High confidence:** Emotion classification accuracies (60-80%) and correlation values (r > 0.75) for valence and dominance ratings are based on well-established psychometrically validated datasets and robust evaluation protocols.

**Medium confidence:** Findings regarding systematic overestimation of rating intensity and weaker performance on arousal and surprise dimensions, while methodologically sound, may vary with different prompting strategies or model fine-tuning approaches not explored in this study.

**Low confidence:** Conclusion about minimal impact of rater-conditioned prompting may be premature, as the study only tested limited prompting variations without exploring more sophisticated personalization techniques.

## Next Checks

1. Evaluate the same models with fine-tuning on affective datasets to establish performance ceilings and compare against zero-shot results.

2. Test more diverse and sophisticated rater-conditioned prompting strategies, including few-shot examples and multi-turn dialogue approaches, to assess whether personalized affect prediction can be improved.

3. Conduct cross-cultural validation using datasets from different cultural contexts to evaluate the generalizability of observed performance patterns and potential cultural biases in VLM affect recognition.