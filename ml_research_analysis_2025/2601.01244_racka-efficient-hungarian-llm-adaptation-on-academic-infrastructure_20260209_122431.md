---
ver: rpa2
title: 'Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure'
arxiv_id: '2601.01244'
source_url: https://arxiv.org/abs/2601.01244
tags:
- hungarian
- language
- training
- https
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Racka, a 4-billion-parameter Hungarian language
  model developed through continual pretraining of Qwen-3 4B with Low-Rank Adaptation
  (LoRA) on academic infrastructure. The authors address the resource gap for Hungarian
  by adapting both the model and tokenizer to better handle the morphologically rich,
  agglutinative nature of the language.
---

# Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure

## Quick Facts
- arXiv ID: 2601.01244
- Source URL: https://arxiv.org/abs/2601.01244
- Reference count: 0
- 4-billion-parameter Hungarian language model developed through continual pretraining of Qwen-3 4B with LoRA on academic infrastructure

## Executive Summary
Racka is a 4-billion-parameter Hungarian language model developed through continual pretraining of Qwen-3 4B with Low-Rank Adaptation (LoRA) on academic infrastructure. The authors address the resource gap for Hungarian by adapting both the model and tokenizer to better handle the morphologically rich, agglutinative nature of the language. A new tokenizer is created, increasing subword fertility efficiency by 46% for Hungarian while maintaining performance on English and German. Training leverages a 160B-token multilingual corpus (44% Hungarian, 24% English, 21% German, 11% code) to mitigate catastrophic forgetting.

## Method Summary
The methodology involves three key components: (1) tokenizer adaptation through training a new BPE tokenizer on Hungarian corpus and merging with Qwen vocabulary, adding 32,000 new tokens initialized via VIPI; (2) LoRA-based continual pretraining with rank=64, alpha=128, dropout=0.1 on all linear layers, training 0.52B parameters (12.5%); (3) data replay strategy using 44% Hungarian, 43% high-resource languages, and 11% code mix to prevent catastrophic forgetting. Training ran for 326K steps on 64 A100 (40GB) GPUs using DDP with gradient checkpointing, achieving 2.9-3.0 sec/iteration.

## Key Results
- Subword fertility reduced from 3.13 to 1.66 for Hungarian (-46.96%)
- Competitive performance on Hungarian benchmarks relative to larger models
- 160B-token training corpus with strategic language mix (44% Hungarian, 24% English, 21% German, 11% code)
- Training completed in 287 hours on academic infrastructure (64× A100 40GB GPUs)

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Expansion with Language-Specific Subwords
- **Claim:** Vocabulary expansion with language-specific subwords reduces tokenization overhead for morphologically rich languages.
- **Mechanism:** New BPE tokens trained on Hungarian corpus replace multi-token fragments, lowering sequence length; VIPI initialization averages constituent subword embeddings to provide semantically grounded starting points for new tokens.
- **Core assumption:** Hungarian words fragmented by English-centric tokenizers carry recoverable semantic structure when reconstituted as single tokens.
- **Evidence anchors:**
  - Subword fertility reduced from 3.13 to 1.66 for Hungarian (-46.96%)
  - English increased 23.44%, German 12.62%
- **Break condition:** If target language has high orthographic overlap with base vocabulary, gains diminish; if OOV rate is already low, vocabulary expansion adds parameters without efficiency benefit.

### Mechanism 2: LoRA-Based Continual Pretraining
- **Claim:** LoRA-based continual pretraining preserves base capabilities while adapting to new language distribution.
- **Mechanism:** Frozen base weights retain original knowledge; low-rank adapters learn language-specific modifications. This constrains the solution space, reducing overfitting risk on new data.
- **Core assumption:** Language adaptation is a low-rank perturbation of the original weight space.
- **Evidence anchors:**
  - LoRA rank=64, alpha=128, dropout=0.1; 0.52B parameters trained (12.5% of base)
  - 44% Hungarian, 43% high-resource languages, 11% code mix to mitigate catastrophic forgetting
- **Break condition:** If adaptation requires substantial restructuring of representations, low-rank constraint may underfit; full fine-tuning or higher rank needed.

### Mechanism 3: DDP vs FSDP Communication Strategy
- **Claim:** DDP outperforms FSDP on clusters with limited inter-node bandwidth when model fits in single-GPU memory.
- **Mechanism:** DDP synchronizes once per backward pass (bulk all-reduce); FSDP requires frequent all-gather operations sensitive to latency. With 4B params fitting in 40GB, sharding overhead dominates.
- **Core assumption:** Gradient synchronization bandwidth scales linearly with model size; communication latency is the bottleneck.
- **Evidence anchors:**
  - DDP: 2.9–3.0 sec/iteration vs. FSDP: 5.0–5.1 sec/iteration on 2-node setup
  - FSDP mixed-precision impossible; full precision caused OOM with 4k context
- **Break condition:** For larger models exceeding single-GPU memory, FSDP becomes necessary regardless of interconnect quality.

## Foundational Learning

- **Concept:** **Catastrophic forgetting in continual learning**
  - Why needed here: Motivates data replay strategy (43% non-Hungarian data) and LoRA's frozen-base approach.
  - Quick check question: Can you explain why mixing original distribution data prevents performance degradation on previously learned tasks?

- **Concept:** **Subword tokenization and fertility**
  - Why needed here: Core metric for evaluating tokenizer efficiency; 46% reduction claimed as key contribution.
  - Quick check question: Why does higher fertility (more tokens per word) hurt efficiency and potentially model performance?

- **Concept:** **Low-Rank Adaptation (LoRA) mechanics**
  - Why needed here: Parameter-efficient training method enabling 4B model adaptation on limited hardware.
  - Quick check question: How does LoRA's rank parameter control the tradeoff between adaptation capacity and overfitting risk?

## Architecture Onboarding

- **Component map:** Qwen-3 4B (36 transformer layers, GQA, SwiGLU, RMSNorm, 32K context) -> Extended vocabulary (original + 32K Hungarian-optimized tokens) -> LoRA modules on all linear layers (rank=64, alpha=128) -> Training data (160B tokens across Hungarian web/news/academic, English/German filtered corpora, code)

- **Critical path:**
  1. Train Hungarian BPE tokenizer → merge with Qwen vocab → initialize new embeddings via VIPI
  2. Insert LoRA adapters → configure per-device batch size=2, grad_accum=4 (effective batch=512)
  3. Run 326K steps with warmup (1%) + linear decay → monitor validation perplexity per language

- **Design tradeoffs:**
  - DDP vs. FSDP: Choose DDP when model fits in GPU memory and interconnect is slow; accept higher memory usage for lower communication overhead
  - Tokenizer expansion: 46% Hungarian efficiency gain vs. 12-24% English/German fertility increase (larger sequences)
  - Data mix: More Hungarian improves target performance; less high-resource data increases forgetting risk

- **Failure signatures:**
  - Validation perplexity diverges by language (catastrophic forgetting) → increase data replay ratio
  - Training loss plateaus but validation PPL keeps increasing → overfitting, check Hungarian data quality
  - OOM with FSDP → switch to DDP or reduce context length
  - New tokenizer tokens have near-zero gradients → check VIPI initialization

- **First 3 experiments:**
  1. **Tokenizer ablation:** Compare base Qwen tokenizer vs. adapted tokenizer on Hungarian text (fertility, downstream task scores)
  2. **LoRA rank sweep:** Test rank=32, 64, 128 on held-out Hungarian benchmarks to validate 64 choice
  3. **Data mix sensitivity:** Train with 60% Hungarian vs. 44% Hungarian; measure forgetting on English/German validation sets

## Open Questions the Paper Calls Out

- **Optimal hyperparameters for Hungarian adaptation:** What are the optimal hyperparameters (LoRA rank, scaling factor α, learning rates) for continual pretraining of Hungarian language models, and how sensitive is adaptation quality to these choices?

- **Interaction between reasoning-tuned backbone and tokenizer adaptation:** How does the interaction between the reasoning-tuned backbone (Qwen3-4B) and tokenizer adaptation affect downstream task performance, given the mixed results where Racka outperforms the base on some tasks but lags on others?

- **Optimal data mixture ratio:** What is the optimal data mixture ratio for Hungarian continual pretraining that maximizes target language performance while minimizing catastrophic forgetting of high-resource languages?

- **Tokenizer healing phase effectiveness:** Does employing a specialized tokenizer healing phase improve convergence and final performance when adapting LLMs with vocabulary expansion?

## Limitations

- Limited empirical validation beyond Hungarian language adaptation
- Infrastructure-specific findings (DDP vs FSDP performance) may not generalize
- Claims of "competitive performance" lack absolute metric values for direct comparison
- No systematic hyperparameter optimization was performed

## Confidence

**High Confidence:**
- 46% reduction in Hungarian subword fertility is directly measurable
- Methodology for tokenizer adaptation is clearly specified and reproducible
- LoRA training configuration and data mixing strategy are explicitly documented

**Medium Confidence:**
- LoRA preserves base capabilities while adapting to Hungarian (lacks controlled ablation)
- Racka outperforms larger models on Hungarian tasks (based on benchmark rankings)
- Infrastructure-specific DDP vs FSDP finding (may not generalize)

**Low Confidence:**
- Applicability to other morphologically rich languages (not empirically validated)
- Optimal Hungarian-to-high-resource data ratio (lacks sensitivity analysis)
- Claims of "state-of-the-art performance" (not directly supported by comparative results)

## Next Checks

1. **Reproduce tokenizer efficiency gains:** Train the proposed Hungarian BPE tokenizer on the same corpus and measure subword fertility reduction compared to the base Qwen-3 tokenizer.

2. **Validate catastrophic forgetting mitigation:** Train two versions - one with the 44% Hungarian data mix and one with 60% Hungarian - then measure performance degradation on English and German validation sets.

3. **Benchmark comparison with absolute metrics:** Run Racka on the full suite of Hungarian benchmarks and report absolute scores alongside the relative rankings to enable proper comparison with existing models.