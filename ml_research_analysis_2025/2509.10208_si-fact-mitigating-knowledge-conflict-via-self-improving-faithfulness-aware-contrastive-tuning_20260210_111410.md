---
ver: rpa2
title: 'SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware
  Contrastive Tuning'
arxiv_id: '2509.10208'
source_url: https://arxiv.org/abs/2509.10208
tags:
- si-fact
- knowledge
- context
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses knowledge conflict in Large Language Models,
  where LLMs prefer their internal parametric knowledge over provided context, leading
  to unfaithful generation. It proposes SI-FACT, a self-improving framework that uses
  contrastive learning with automatically generated data.
---

# SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning

## Quick Facts
- arXiv ID: 2509.10208
- Source URL: https://arxiv.org/abs/2509.10208
- Reference count: 0
- Key outcome: Improves contextual recall rate by 6.2% over baselines while maintaining general capabilities

## Executive Summary
SI-FACT addresses knowledge conflict in LLMs where models prefer their internal parametric knowledge over provided context, leading to unfaithful generation. The method uses self-instruct to automatically generate high-quality contrastive learning data including anchor samples, semantically equivalent positive samples, and negative samples simulating unfaithful scenarios. Through InfoNCE contrastive loss, the model learns to pull faithful responses closer and push unfaithful responses farther apart in representation space. Experiments on ECARE_KRE and COSE_KRE benchmarks show significant improvements in contextual faithfulness while preserving general capabilities.

## Method Summary
SI-FACT uses Llama3-8B-Instruct as the base model and SQuAD dataset as the anchor source. The method extracts (Context, Question, Golden Answer) triplets as anchors, then uses the base LLM with self-instruct prompts to generate positive samples (semantic rewrites) and three types of negative samples (external information injection, context conflict, irrelevant answers). Representations are extracted from the last token's hidden state of the final transformer layer, and the model is trained with InfoNCE loss to maximize similarity between anchor and positive representations while minimizing similarity with negative representations. The approach demonstrates high data efficiency, reaching peak performance at only 1,000 training samples.

## Key Results
- Improves Contextual Recall Rate by 6.2% over best baseline on ECARE_KRE and COSE_KRE benchmarks
- Achieves lowest Parametric Recall Rate and Memorization Ratio among tested methods
- Maintains general capabilities with minimal performance drops on TriviaQA, GSM8K, Hellaswag, and ARC-Challenge
- Demonstrates high data efficiency with peak performance at only 1,000 training samples

## Why This Works (Mechanism)

### Mechanism 1
Self-generated structured contrastive data enables targeted learning of contextual faithfulness without manual annotation. The base LLM generates semantically equivalent positive samples via paraphrasing and three types of hard negatives: external information injection, context conflict, and irrelevant answers. This directly targets the failure modes of knowledge conflict. Core assumption: The model's internal language capabilities are sufficient to generate meaningful contrastive pairs without quality degradation.

### Mechanism 2
InfoNCE contrastive loss encodes faithfulness as a separable dimension in representation space. By maximizing cosine similarity between anchor and positive representations while minimizing similarity with negatives, the model develops an internal representation where "faithful to context" becomes a discoverable semantic feature. Core assumption: The last token representation encodes the model's judgment about answer quality relative to context.

### Mechanism 3
High data efficiency emerges from targeted contrastive signal rather than volume. Self-generated contrastive pairs are inherently task-aligned and directly encode the faithfulness distinction the model needs to learn. This creates rapid convergence without requiring large-scale data. Core assumption: Quality and targeting of contrastive pairs matters more than quantity for this specific capability.

## Foundational Learning

- **Concept: Contrastive Learning and InfoNCE Loss**
  - Why needed here: The entire SI-FACT training loop centers on InfoNCE loss. Without understanding how positive/negative pair selection shapes representation spaces, you cannot debug convergence issues or tune temperature τ.
  - Quick check question: Given an anchor with one positive and three negatives, if similarity(anchor, positive) = 0.8 and similarities to negatives are [0.6, 0.4, 0.3] with τ = 0.1, what is the approximate InfoNCE loss? Can you compute it manually?

- **Concept: Knowledge Conflict Taxonomy (Context-Memory, Inter-Context, Intra-Memory)**
  - Why needed here: SI-FACT specifically targets context-memory conflict. Understanding why the model defaults to parametric knowledge helps you evaluate whether the three negative types adequately cover failure modes.
  - Quick check question: If a user asks about recent news not in the model's training data, but RAG retrieves contradictory articles, which conflict type dominates? How would SI-FACT's negative types address this?

- **Concept: Representation Extraction from Transformer Hidden States**
  - Why needed here: The method uses the last token's hidden state as the sequence representation. This choice affects what semantic information is captured and how contrastive learning operates.
  - Quick check question: Why might the last token be preferred over mean pooling or [CLS] token for this task? What tradeoffs does this introduce for longer contexts?

## Architecture Onboarding

- **Component map:**
SQuAD Dataset → Anchor Triplet Extraction (C, Q, A_golden) → Self-Instruct Data Generation Engine (Base LLM as teacher) → Positive Sample (Paraphrase), Negative Type 1-3 → Contrastive Dataset (12K samples max) → InfoNCE Loss Computation (h_anchor, h_pos, h_neg, τ) → Backprop on Base LLM → SI-FACT Tuned Model

- **Critical path:**
1. Data generation quality → If negatives are not truly unfaithful or positives are not semantically equivalent, contrastive signal collapses. Inspect generated samples manually before training.
2. Representation extraction → Last-token hidden state must be correctly extracted from the final transformer layer. Verify tensor shapes and indexing.
3. Loss scaling → InfoNCE loss with temperature τ. Default not specified in paper; start with τ ∈ [0.05, 0.2] and monitor gradient norms.

- **Design tradeoffs:**
Targeted capability vs. general performance: Table 4 shows -0.8% on GSM8K, -1.93% on ARC-Challenge, but +0.4% on TriviaQA. Acceptable tradeoff for faithfulness-critical applications, but quantify your tolerance before deployment. Self-generation vs. human annotation: Eliminates annotation cost but introduces potential quality drift. Paper does not report human evaluation of generated contrastive pairs. Last-token representation vs. alternatives: Simple and effective but may miss nuanced semantic information in longer sequences. Consider experimenting with pooling strategies for production use.

- **Failure signatures:**
Collapsed representations: If t-SNE shows no separation between positive and negative clusters after training, check: (a) negative quality, (b) temperature τ too high, (c) learning rate too aggressive. Overfitting to SQuAD patterns: If model performs well on ECARE_KRE/COSE_KRE but fails on domain-specific contexts, the contrastive signal may not generalize. Expand anchor source diversity. Catastrophic forgetting: If general benchmarks degrade >5%, reduce training samples or add regularization. Paper shows <2% degradation, but monitor your specific use case.

- **First 3 experiments:**
1. Reproduce data efficiency curve: Train with 250, 500, 1000, 2000, 4000 samples. Verify that performance peaks near 1000 as claimed. Document saturation point for your compute budget.
2. Ablate negative types: Train three variants, each using only one negative type. Measure CRR/PRR to identify which negative type contributes most to faithfulness gains. Paper does not provide this analysis.
3. Temperature sensitivity sweep: Test τ ∈ [0.05, 0.1, 0.15, 0.2, 0.5]. Plot final CRR vs. τ. Identify stable operating range for your hardware and batch size.

## Open Questions the Paper Calls Out

### Open Question 1
Does the self-improving mechanism scale effectively to models significantly larger than 8B parameters (e.g., 70B+), and does the observed data saturation point of 1,000 samples hold true for higher-capacity architectures? Basis in paper: [explicit] The conclusion states: "Future research will extend SI-FACT in three directions: applying it to larger-scale models..." Why unresolved: The current study is restricted to Llama3-8B-Instruct. It is unknown if the representation geometry in larger models requires more contrastive data or different hyperparameters to achieve the same faithfulness separation.

### Open Question 2
How does SI-FACT perform when integrated into Retrieval-Augmented Generation (RAG) systems where the external context is noisy or partially relevant, rather than the specifically misleading contexts used in current benchmarks? Basis in paper: [explicit] The authors list "...integrating it with RAG to optimize faithfulness in complex, open environments" as a primary direction for future work. Why unresolved: The current evaluation uses benchmarks (ECARE_KRE, COSE_KRE) with machine-generated, deliberately contradictory contexts. Real-world RAG retrieves diverse documents that may be irrelevant rather than explicitly conflicting.

### Open Question 3
Can the self-improving contrastive paradigm be successfully adapted to optimize orthogonal capabilities like safety and bias mitigation without causing interference or "capability forgetting" in the model's faithfulness? Basis in paper: [explicit] The paper proposes extending "...the self-improvement paradigm to other critical capabilities such as safety enhancement and bias mitigation." Why unresolved: The paper demonstrates success with faithfulness, but the interaction between different contrastive objectives (e.g., pulling apart "unsafe" representations vs. "unfaithful" representations) in the same representation space is unexplored.

## Limitations

- Data generation quality control: The paper relies entirely on self-generated contrastive data without human validation, creating potential for false training signals.
- Temperature hyperparameter: The critical temperature τ value is not specified, making exact reproduction impossible.
- Generalization boundaries: All experiments are conducted on commonsense reasoning benchmarks, with no demonstration of faithfulness improvements in other domains like long-form generation or open-ended dialogue.

## Confidence

**High Confidence**: The core methodology (self-instruct data generation + contrastive learning) is technically sound and well-documented. The mechanism by which InfoNCE loss separates faithful from unfaithful representations is clearly explained and supported by the t-SNE visualization. The data efficiency claim (peak at 1000 samples) is reproducible based on the described procedure.

**Medium Confidence**: The benchmark results showing CRR improvements (6.2% over baselines) are likely accurate given the controlled experimental setup, but the evaluation protocol details are sparse. The claim that SI-FACT "effectively mitigates knowledge conflict" is supported but could benefit from more diverse evaluation scenarios.

**Low Confidence**: Claims about the model's ability to generalize faithfulness improvements to unseen domains are not empirically validated. The assumption that self-generated data quality is sufficient without human oversight is not tested. The long-term stability of the learned representations is not addressed.

## Next Checks

**Validation Check 1: Human Evaluation of Generated Contrastive Data**
Manually evaluate 100 randomly sampled contrastive pairs (anchors with their positive and three negatives) for: (a) whether positive samples are truly semantically equivalent, (b) whether each negative type correctly represents the intended unfaithfulness category, and (c) whether any negatives are accidentally faithful or positives are unfaithful. This validates the foundational assumption of self-generated supervision quality.

**Validation Check 2: Cross-Domain Faithfulness Transfer**
Test the SI-FACT tuned model on faithfulness-critical tasks outside commonsense reasoning: (a) long-form document summarization where context-memory conflict is common, (b) multi-hop question answering requiring complex context integration, and (c) open-ended dialogue with external knowledge injection. Measure whether CRR improvements transfer or if the model overfits to SQuAD-style contexts.

**Validation Check 3: Temperature Sensitivity Analysis**
Conduct a systematic sweep of temperature τ values (e.g., [0.05, 0.1, 0.15, 0.2, 0.5]) while training with fixed hyperparameters and data quantity. Plot CRR, PRR, and MR against τ to identify the optimal operating range and determine whether the model is sensitive to this critical hyperparameter. Document whether performance degrades significantly for τ values outside the optimal range.