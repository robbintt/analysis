---
ver: rpa2
title: Figurative-cum-Commonsense Knowledge Infusion for Multimodal Mental Health
  Meme Classification
arxiv_id: '2501.15321'
source_url: https://arxiv.org/abs/2501.15321
tags:
- memes
- mental
- health
- meme
- figurative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M3H, a framework for classifying mental health
  symptoms in memes by combining figurative reasoning and commonsense knowledge. The
  method retrieves cause-effect relationships, figurative meanings, and mental states
  using GPT-4, then fuses this knowledge with OCR-extracted text through a RAG module
  before classifying symptoms with a BART-based model.
---

# Figurative-cum-Commonsense Knowledge Infusion for Multimodal Mental Health Meme Classification

## Quick Facts
- arXiv ID: 2501.15321
- Source URL: https://arxiv.org/abs/2501.15321
- Reference count: 40
- Outperforms 20 baseline variations, achieving 4.20–4.66% improvement in weighted-F1 and 4.94–5.79% in macro-F1

## Executive Summary
This paper introduces M3H, a framework for classifying mental health symptoms in memes by combining figurative reasoning and commonsense knowledge. The method retrieves cause-effect relationships, figurative meanings, and mental states using GPT-4, then fuses this knowledge with OCR-extracted text through a RAG module before classifying symptoms with a BART-based model. On two datasets (AxiOM for anxiety and RESTORE for depression), M3H outperforms 20 baseline variations, achieving 4.20–4.66% improvement in weighted-F1 and 4.94–5.79% in macro-F1. Human evaluation confirms the quality of generated reasoning, and ablation studies show each module's contribution to performance. Limitations include reliance on trending online content and increased computational cost from LLMs.

## Method Summary
The M3H framework processes mental health memes through a pipeline: (1) OCR extracts text from meme images, (2) GPT-4o generates structured reasoning across three attributes (cause-effect, figurative understanding, mental state), (3) OCR text and reasoning are embedded jointly for RAG retrieval of similar training examples, and (4) a BART-based encoder-decoder model classifies symptoms using concatenated input of OCR, reasoning, and retrieved exemplars. The approach uses few-shot learning with retrieved context to compensate for limited training data across fine-grained symptom categories.

## Key Results
- Outperforms 20 baseline variations on AxiOM and RESTORE datasets
- Achieves 4.20–4.66% improvement in weighted-F1 and 4.94–5.79% in macro-F1
- Human evaluation confirms quality of generated reasoning triples
- Ablation studies demonstrate contribution of each module to performance

## Why This Works (Mechanism)

### Mechanism 1
Structured figurative reasoning extraction provides explicit commonsense knowledge that MLMs otherwise miss. GPT-4o generates reasoning across three dimensions—cause-effect (grounding scenarios in real-world consequences), figurative understanding (metaphors, irony, satire), and mental state (psychological states)—surfacing implicit meanings in memes that standard multimodal encoders fail to capture. The quality of LLM-generated reasoning must faithfully represent human-level commonsense interpretation of meme content.

### Mechanism 2
Retrieval-Augmented Generation (RAG) provides contextual exemplars that improve few-shot classification. OCR text and figurative reasoning are embedded jointly using a sentence transformer. During inference, the top-n most similar training examples are retrieved via cosine similarity, providing the classifier with relevant precedent cases. This compensates for limited training data and the nuanced nature of mental health expressions. Similar meme representations (based on OCR + reasoning embeddings) must correlate with similar symptom categories.

### Mechanism 3
Domain-enriched encoder-decoder models outperform standard multimodal baselines when augmented with structured commonsense. BART receives concatenated input: OCR text (literal content) + figurative reasoning (commonsense interpretation) + retrieved exemplars (contextual knowledge). The few-shot formulation helps generalize from limited data across the fine-grained symptom categories. The text-based representation (OCR + reasoning + exemplars) must sufficiently capture the multimodal meme content without direct image encoding in the classifier.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: The framework relies on RAG to provide the classifier with similar training examples during inference. Without understanding how embedding similarity, retrieval, and context fusion work, you cannot debug retrieval quality or optimize the knowledge base. *Quick check question*: Given a meme about "difficulty relaxing," what would constitute a good retrieved exemplar—surface text similarity or symptom-level semantic similarity?

- **Figurative Language Interpretation (Metaphor, Irony, Sarcasm)**: Mental health memes heavily rely on figurative expression. The framework's three-attribute reasoning (cause-effect, figurative understanding, mental state) is designed to decompose these layers. Understanding what makes figurative language computationally hard helps you evaluate whether the LLM reasoning module is succeeding. *Quick check question*: If a meme shows a calm image with text "Everything is fine" but the context suggests otherwise, which reasoning attribute would surface this irony?

- **Few-Shot Learning with In-Context Examples**: The final classifier uses retrieved exemplars as in-context examples. Understanding how models leverage few-shot prompts—and their sensitivity to example selection and ordering—is essential for optimizing the RAG retrieval strategy. *Quick check question*: What happens to classification performance if retrieved examples are all from the same symptom class?

## Architecture Onboarding

- **Component map**: Image → OCR + GPT-4o Reasoning → Joint Embedding → RAG Retrieval → BART Classifier → Symptom Label
- **Critical path**: The reasoning quality from GPT-4o and the relevance of RAG retrieval are the two highest-leverage points
- **Design tradeoffs**:
  - LLM reasoning vs. direct multimodal encoding: Using GPT-4o for reasoning adds computational cost and latency but provides interpretable, structured commonsense
  - RAG vs. larger training set: RAG enables few-shot learning with limited data but introduces retrieval complexity
  - BART vs. Mental-BART: Mental-BART is domain-adapted but may overfit; vanilla BART may generalize better with reasoning augmentation
- **Failure signatures**:
  1. Misclassification between "Lack of Worry Control" and "Impending Doom" (16% confusion)
  2. OCR failures on stylized meme text causing pipeline degradation
  3. Cultural/slang mismatch in reasoning leading to noisy signals
- **First 3 experiments**:
  1. Ablate each reasoning attribute: Run inference with only cause-effect, only figurative understanding, only mental state, and all three
  2. RAG retrieval analysis: For 50 held-out test samples, manually inspect top-3 retrieved exemplars and score relevance
  3. Error analysis on high-confusion pairs: Extract misclassifications between "Lack of Worry Control" and "Impending Doom"

## Open Questions the Paper Calls Out

- Can the M3H framework effectively generalize to other domains of figurative communication beyond memes, such as political cartoons or digital art? The authors note this as a future direction in the Discussion.
- How can the factuality and faithfulness of the generated figurative reasoning be improved to ensure alignment with human interpretation? The paper acknowledges this as a limitation and future focus area.
- Can the figurative and commonsense knowledge be distilled into smaller, more computationally efficient models without significant performance degradation? The reliance on GPT-4o and its computational cost is identified as a primary limitation.

## Limitations

- LLM-generated reasoning quality is opaque and potentially variable across cultural contexts and evolving internet slang
- RAG module's retrieval relevance is unquantified with no retrieval accuracy metrics or manual relevance scoring
- Computational cost of GPT-4o reasoning generation is expensive and slow, limiting real-time deployment
- Absence of direct image encoding means OCR failures cascade through the entire pipeline

## Confidence

- **High Confidence**: The framework architecture is clearly specified and the 4-5% performance improvements over 20 baselines are statistically significant and reproducible
- **Medium Confidence**: The three-attribute reasoning structure logically addresses the problem, but LLM-generated reasoning quality is not independently verified against human annotations for mental health memes
- **Low Confidence**: The claim that RAG provides meaningful contextual exemplars lacks empirical support - no retrieval accuracy metrics or exemplar relevance scoring are provided

## Next Checks

1. **LLM Reasoning Quality Validation**: Conduct human evaluation where annotators score GPT-4o-generated reasoning triples against ground truth symptom understanding for 100 randomly selected memes
2. **RAG Retrieval Quality Analysis**: For 50 test samples, manually inspect top-3 retrieved exemplars and score relevance (relevant/irrelevant), correlating with classification correctness
3. **OCR Robustness Testing**: Systematically degrade text quality in memes (blur, stylized fonts, overlapping visuals) and measure performance degradation