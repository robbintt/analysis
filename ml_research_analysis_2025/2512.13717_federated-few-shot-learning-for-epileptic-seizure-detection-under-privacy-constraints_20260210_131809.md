---
ver: rpa2
title: Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints
arxiv_id: '2512.13717'
source_url: https://arxiv.org/abs/2512.13717
tags:
- seizure
- learning
- federated
- detection
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints

## Quick Facts
- arXiv ID: 2512.13717
- Source URL: https://arxiv.org/abs/2512.13717
- Reference count: 15
- Primary result: FFSL achieves 0.77 balanced accuracy with 5-shot support, outperforming federated fine-tuning (0.43) while preserving privacy

## Executive Summary
This paper presents a two-stage federated few-shot learning approach for personalized EEG seizure detection under privacy constraints. The method first performs federated fine-tuning of a pre-trained biosignal transformer (BIOT) across multiple simulated hospital sites, then adapts patient-specific classifiers using only 5 labeled samples per patient. The approach addresses the challenge of building personalized seizure detection models while keeping patient data localized, achieving strong performance on heterogeneous EEG datasets.

## Method Summary
The method uses a two-stage process: (1) Federated fine-tuning of a pre-trained BIOT encoder across 4 simulated hospital sites using FedAvg, and (2) Patient-specific few-shot adaptation with frozen encoder and lightweight classifiers trained on 5-shot support sets. The system processes bipolar EEG montages at 200Hz, extracts 5-second tokenized segments, and uses weighted interpolation (α=0.8) between local and global parameters during federated training to preserve patient-specific information while benefiting from cross-site knowledge.

## Key Results
- Stage 1 (Federated fine-tuning): Balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), weighted F1 of 0.69 (0.74)
- Stage 2 (Few-shot learning): Average balanced accuracy of 0.77, Cohen's kappa of 0.62, weighted F1 of 0.73 across 4 sites
- Client-specific models preserve local adaptation through weighted interpolation while benefiting from global knowledge
- Performance degrades when seizure types are underrepresented across clients, highlighting sensitivity to data distribution

## Why This Works (Mechanism)

### Mechanism 1
- Pre-trained biosignal representations enable effective few-shot adaptation for seizure detection.
- The BIOT encoder, pre-trained on 5 million EEG samples, learns generalizable signal features that transfer across patients. When frozen and reused in Stage 2, these embeddings provide semantically rich representations, allowing a lightweight classifier to separate seizure from non-seizure patterns using only 5 labeled segments per patient.
- Core assumption: Pre-training captures transferable features that generalize across recording conditions, electrode configurations, and patient populations.
- Evidence anchors:
  - [abstract] "In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning."
  - [section 3.2] "The encoder employs linear self-attention for scalable modeling of long biosignal sequences... pre-trained on 5 million EEG samples."
- Break condition: If pre-training data distribution differs substantially from target patient population (e.g., pediatric vs. adult EEG), representations may not transfer effectively.

### Mechanism 2
- Federated aggregation of local updates produces a global model that captures cross-site seizure variability without data sharing.
- FedAvg performs element-wise averaging of client model parameters after local training epochs. This exposes the model to heterogeneous seizure morphologies and recording conditions distributed across institutions, improving generalization beyond any single site's data while preserving privacy by exchanging only parameters.
- Core assumption: Non-IID data distributions across clients still share sufficient common structure for averaging to improve upon local-only training.
- Evidence anchors:
  - [abstract] "Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74)."
  - [section 3.3.1] "Training followed the standard Federated Averaging (FedAvg) algorithm... all clients initialize with the same model and perform local updates for several epochs using their respective data."
- Break condition: If client data distributions are extremely heterogeneous (highly non-IID), standard FedAvg may suffer from client drift, degrading global model quality.

### Mechanism 3
- Weighted interpolation between local and global parameters preserves patient-specific information across federated rounds.
- After each aggregation round, each client computes new parameters as θ_new = αθ_local + (1-α)θ_global with α=0.8. This retains 80% of locally-adapted weights while incorporating 20% from the global model, preventing complete overwriting of patient-specific signals that would occur with standard FedAvg reset.
- Core assumption: Local patient adaptations contain valuable signal that should persist across rounds; global model provides complementary regularization rather than replacement.
- Evidence anchors:
  - [abstract] "In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions."
  - [section 3.3.2] "To retain local patient-specific information while still benefiting from shared knowledge, we apply a weighted integration strategy after each communication round."
- Break condition: If α is too high, clients may overfit to local data and fail to benefit from cross-site knowledge; if too low, personalization signal is lost.

## Foundational Learning

- **Federated Averaging (FedAvg)**: Core optimization algorithm enabling distributed training across simulated hospital sites without sharing EEG data. Why needed here: Standard method for aggregating distributed model updates while preserving privacy. Quick check question: Can you explain why FedAvg performs local gradient descent before aggregation rather than aggregating gradients directly?

- **Few-shot learning with support/query sets**: Stage 2 personalization uses 5-shot support sets to adapt classifiers to new patients with minimal labeled data. Why needed here: Enables personalized models when labeled data is scarce per patient. Quick check question: What is the difference between a support set and a query set in episodic few-shot training?

- **Non-IID data distribution**: EEG seizure patterns vary substantially across patients and sites; the paper explicitly evaluates under heterogeneous distributions. Why needed here: Real-world EEG data is inherently non-IID across patients and institutions. Quick check question: Why does non-IID data make federated learning more challenging than IID settings?

## Architecture Onboarding

- **Component map**: EEG signals (bipolar montage, 200Hz) → BIOT Encoder (frozen) → 256-dim embeddings → Lightweight Classifier (ELU + Linear) → Seizure/Background classification

- **Critical path**:
  1. Preprocess EEG (bipolar montage → 200Hz resampling → normalization → 5-second tokenization)
  2. Stage 1: Federated fine-tune BIOT encoder + classifier across 4 sites using TUEV I
  3. Stage 2: Freeze encoder, extract embeddings for new patients from TUEV II
  4. Train lightweight classifier per patient using 5-shot support sets with α=0.8 interpolation
  5. Evaluate on per-patient query sets

- **Design tradeoffs**:
  - α parameter (0.8): Higher values preserve local personalization but reduce cross-site knowledge transfer
  - 5-shot support set (4 seizure + 1 background): Over-represents seizure to improve sensitivity but may bias predictions
  - FedAvg vs. advanced FL methods: Standard FedAvg provides interpretable baseline but may underperform under extreme non-IID conditions

- **Failure signatures**:
  - Low balanced accuracy with moderate weighted F1: Model biased toward majority class (seen in E1 results)
  - High inter-client variance in performance: Indicates heterogeneous embedding separability; Client 3's unique seizure type 4 may limit global knowledge benefit
  - Embedding space overlap in PCA: Client 3 showed diffuse clusters correlating with 0.533 balanced accuracy

- **First 3 experiments**:
  1. **Baseline replication**: Run E1 with centralized BIOT on TUEV I to verify balanced accuracy ~0.52 matches paper baseline
  2. **Ablation on α**: Test α ∈ {0.5, 0.7, 0.8, 0.9, 1.0} in E2 to characterize personalization-regularization tradeoff
  3. **Cross-dataset validation**: Apply frozen Fed-BIOT encoder from E1 to a different EEG seizure dataset to assess transferability of federated representations

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gap between federated (0.43 balanced accuracy) and centralized (0.52) training indicates communication/computation efficiency trade-offs
- Sensitivity to data distribution: Client 3 with unique seizure type 4 showed poor performance, highlighting limitations with highly heterogeneous data
- Limited evaluation on truly distributed hospital environments rather than simulated partitions of a single dataset

## Confidence
- Method reproducibility: Medium - Requires access to BIOT pretrained weights and specific TUEV dataset partitioning
- Performance claims: High - Well-documented metrics with clear baselines
- Mechanism explanations: High - Detailed architectural descriptions and hyperparameter specifications

## Next Checks
1. Verify that BIOT pretrained weights are accessible and compatible with the described architecture
2. Confirm that TUEV dataset can be partitioned into 4 non-overlapping client subsets matching the paper's distribution
3. Test whether the 5-shot learning approach generalizes to datasets with different seizure type distributions than TUEV