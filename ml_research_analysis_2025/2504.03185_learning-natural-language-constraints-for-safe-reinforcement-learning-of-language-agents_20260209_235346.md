---
ver: rpa2
title: Learning Natural Language Constraints for Safe Reinforcement Learning of Language
  Agents
arxiv_id: '2504.03185'
source_url: https://arxiv.org/abs/2504.03185
tags:
- learning
- language
- constraint
- arxiv
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework for learning natural language
  constraints from demonstrations to improve safety and generalization in language
  agents. By combining inverse reinforcement learning with constrained Markov decision
  processes, the method learns both task rewards and safety constraints directly from
  positive and negative text demonstrations.
---

# Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents

## Quick Facts
- arXiv ID: 2504.03185
- Source URL: https://arxiv.org/abs/2504.03185
- Reference count: 28
- Primary result: Learns safety constraints from demonstrations to improve robustness under domain shift

## Executive Summary
This work introduces a framework for learning natural language constraints from demonstrations to improve safety and generalization in language agents. By combining inverse reinforcement learning with constrained Markov decision processes, the method learns both task rewards and safety constraints directly from positive and negative text demonstrations. This enables adaptation to novel safety requirements and robustness under domain shifts and adversarial inputs. Experiments in a text-based navigation environment demonstrate improved constraint satisfaction and zero violations when applied to a distilled BERT model, offering a promising path toward safer and more generalizable large language models.

## Method Summary
The framework, called CLIRL (Constrained Inverse Reinforcement Learning), extends Maximum Causal Entropy IRL to simultaneously infer task rewards and safety constraints from demonstrations. It uses positive demonstrations (desirable behavior) and negative demonstrations (undesirable behavior) to learn reward function R_θ and constraint functions C_{k,φk}. These are then used in a Constraint-Aware Policy Optimization (CAPO) algorithm that maintains safety while pursuing task reward. The method includes domain sampling and CVaR minimization to improve robustness to distribution shifts.

## Key Results
- Improved constraint satisfaction under distribution shift compared to implicit preference learning
- Zero violations achieved when applied to a distilled BERT model in text-based navigation
- Demonstrated adaptation to novel safety requirements through demonstration-based learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simultaneously learning reward functions and safety constraints from demonstrations improves constraint satisfaction under distribution shift compared to implicit preference learning.
- Mechanism: CLIRL extends Maximum Causal Entropy IRL by jointly inferring R_θ (task reward) and C_{k,φk} (constraint costs) from positive trajectories D_pos (desirable behavior) and negative trajectories D_neg (undesirable behavior). The objective maximizes likelihood of positive demonstrations while minimizing likelihood of negative ones under a combined reward-cost model, with a penalty term for constraint violations exceeding threshold H_k.
- Core assumption: Negative demonstrations reliably signal constraint violations that can be disentangled from task failures; safety constraints are learnable from trajectory-level signal rather than explicit annotation.
- Evidence anchors:
  - [abstract] "extending inverse reinforcement learning to simultaneously infer task rewards and safety constraints"
  - [section 3.4, equation 3] Formal CLIRL objective with separate positive/negative demonstration terms
  - [corpus] Weak direct evidence on constraint learning from IRL; related work (HC-RLHF) assumes known constraints rather than learning them
- Break condition: If negative demonstrations primarily reflect task failure rather than safety violation, learned C functions may conflate incompetence with unsafety.

### Mechanism 2
- Claim: Constraint-aware policy optimization with dynamic Lagrange multipliers maintains safety while pursuing task reward.
- Mechanism: CAPO modifies Constrained Policy Optimization by incorporating learned constraint functions directly into the objective: J_CAPO = E[Σγ^t R_θ] - Σ_k β_k E[Σγ^t C_{k,φk}]. Trust region updates ensure each policy iteration improves expected return while satisfying cost constraints. Lagrange multipliers β_k adapt during training to balance reward-constraint tradeoff.
- Core assumption: Learned constraint functions generalize sufficiently to guide policy updates; constraint satisfaction during training transfers to deployment conditions.
- Evidence anchors:
  - [section 3.5, equation 1] CAPO objective formulation with Lagrange multipliers
  - [table 2] SAIL-CaRL shows lower violation rates than No Constraint baseline post-shift (1.523 vs 2.588)
  - [corpus] Constrained Policy Optimization foundations validated in related safe RL work (semi-infinite safe RL, CH-MARL)
- Break condition: If learned C functions have high false-positive rates on safe actions, policy optimization becomes overly conservative and task performance degrades unacceptably.

### Mechanism 3
- Claim: Training with stochastic domain transitions and CVaR minimization improves robustness to distribution shift.
- Mechanism: Domain parameter θ ∈ Θ is sampled from distribution P(θ) during training, simulating adversarial perturbations, topic changes, and style variations. CVaR_α minimization over cumulative constraint costs ensures the policy satisfies constraints even in worst-case scenarios (tail of violation distribution), rather than just in expectation.
- Core assumption: The training distribution P(θ) adequately covers the space of deployment variations; worst-case training scenarios transfer to real domain shifts.
- Evidence anchors:
  - [section 3.6, equation 2] CVaR minimization objective for constraint violations
  - [section 4.1] Domain shift simulated by adding new danger zone at epoch 100; SAIL-CaRL adapts constraint function
  - [corpus] Limited direct evidence; related work (Boundary-to-Region Supervision) addresses offline safe RL but not explicit domain shift modeling
- Break condition: If deployment shifts involve constraint types or state structures not represented in P(θ), CVaR optimization provides no guarantee.

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: Core formal framework distinguishing this work from standard RL; separates reward (helpfulness) from costs (constraint violations) with explicit threshold H.
  - Quick check question: Can you write the CMDP objective and explain why decoupling R and C matters for safety?

- Concept: Inverse Reinforcement Learning (IRL) and Maximum Causal Entropy
  - Why needed here: CLIRL builds on IRL principles; understanding how IRL infers rewards from demonstrations is prerequisite for understanding joint reward-constraint inference.
  - Quick check question: Why does IRL infer rewards rather than directly learning a policy?

- Concept: Conditional Value at Risk (CVaR)
  - Why needed here: Used to ensure safety under worst-case scenarios; requires understanding tail-risk measures versus expected-value optimization.
  - Quick check question: How does CVaR differ from constraining expected violations, and when would you prefer one over the other?

## Architecture Onboarding

- Component map:
  - **Demonstration Dataset**: D_pos (safe trajectories) and D_neg (violation trajectories) → input to CLIRL
  - **CLIRL Module**: Neural networks R_θ and C_{k,φk} trained via max-entropy IRL objective → outputs reward and constraint functions
  - **CAPO Optimizer**: Policy network π_ψ updated via trust-region method using R_θ and C_{k,φk} → outputs safe policy
  - **Domain Sampler**: Samples θ ~ P(θ) during training for robustness
  - **Distillation Target**: Learned constraints applied to fine-tune BERT-family models

- Critical path:
  1. Collect/curate positive and negative demonstrations (quality and coverage are bottlenecks)
  2. Train CLIRL to convergence on R_θ and C_{k,φk}
  3. Run CAPO with domain sampling until constraint satisfaction stabilizes
  4. Validate on held-out domain shift before deployment

- Design tradeoffs:
  - Negative demonstration quantity vs. quality: More D_neg improves constraint learning but noisy negatives conflate failure modes
  - Number of constraint functions K: More constraints capture finer-grained safety but increase optimization complexity and hyperparameter burden
  - CVaR α parameter: Lower α optimizes for rarer worst-cases but may sacrifice average performance

- Failure signatures:
  - High variance in violation rates across trials (see table 2: 0.833 ± 0.477) → CLIRL not converging to stable constraint representation
  - Constraint function heatmap shows diffuse activation rather than sharp danger zones → insufficient negative demonstration coverage or network capacity
  - Post-shift violation spike without adaptation → P(θ) not covering actual shift distribution

- First 3 experiments:
  1. **Baseline sanity check**: Implement tabular SAIL-CaRL on 3×3 grid (simpler than paper's 5×5) with single danger zone; verify CLIRL learns correct C values (near 1.0 for dangerous state-actions, near 0.0 for safe)
  2. **Ablation on demonstration composition**: Vary D_pos/D_neg ratio (80/20, 50/50, 20/80) and measure constraint violation rate and task success; identify minimum viable negative demonstration quantity
  3. **Domain shift generalization test**: Train on θ_1 distribution, evaluate on held-out θ_2 (e.g., different danger zone patterns); compare SAIL-CaRL vs. hand-coded constraint baseline to isolate learning contribution

## Open Questions the Paper Calls Out

- **Scalability**: Can CLIRL scale to high-dimensional language tasks without computational instability associated with IRL in large action spaces?
- **Demonstration noise**: How robust is constraint learning to noise or ambiguity in positive and negative demonstration datasets?
- **Semantic grounding**: Does the learned constraint function capture semantic safety concepts or merely surface-level statistical patterns?

## Limitations

- Demonstration quality dependency: CLIRL performance heavily relies on the quality and representativeness of negative demonstrations
- Domain shift coverage: Training distribution must adequately represent potential deployment shifts
- Scalability: Results are demonstrated on a simple 5×5 grid world, with unproven scalability to complex environments

## Confidence

- Mechanism 1 (CLIRL learning): Medium - Strong theoretical foundation but limited empirical validation beyond simple domains
- Mechanism 2 (CAPO optimization): High - Built on well-established Constrained Policy Optimization with clear empirical support
- Mechanism 3 (CVaR robustness): Medium - Theoretical justification solid, but empirical coverage of shift scenarios is limited

## Next Checks

1. **Negative demonstration sensitivity analysis**: Systematically vary the quality and quantity of D_neg (e.g., introduce noise, change ratio to D_pos) and measure impact on learned constraint accuracy and policy safety
2. **Out-of-distribution constraint testing**: Design domain shifts that introduce constraint types or state structures not present in P(θ) to test CVaR robustness limits
3. **Scalability benchmark**: Apply the framework to a more complex text-based environment (e.g., multi-room navigation with varied constraints) and measure performance degradation relative to tabular results