---
ver: rpa2
title: 'From Local to Global: Revisiting Structured Pruning Paradigms for Large Language
  Models'
arxiv_id: '2510.18030'
source_url: https://arxiv.org/abs/2510.18030
tags:
- pruning
- gisp
- local
- accuracy
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of deploying large language
  models (LLMs) by improving structured pruning techniques. The core method, GISP
  (Global Iterative Structured Pruning), introduces a global, iterative approach that
  removes attention heads and MLP channels based on first-order, loss-based importance
  scores, with block-wise normalization and a ratio-scheduled pruning process.
---

# From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models

## Quick Facts
- arXiv ID: 2510.18030
- Source URL: https://arxiv.org/abs/2510.18030
- Authors: Ziyan Wang; Enmao Diao; Qi Le; Pu Wang; Minwoo Lee; Shu-ping Yeh; Evgeny Stupachenko; Hao Feng; Li Yang
- Reference count: 40
- Core method: GISP (Global Iterative Structured Pruning) - global, iterative approach using first-order loss-based importance scores with block-wise normalization

## Executive Summary
This paper addresses the inefficiency of deploying large language models (LLMs) by improving structured pruning techniques. The core method, GISP (Global Iterative Structured Pruning), introduces a global, iterative approach that removes attention heads and MLP channels based on first-order, loss-based importance scores, with block-wise normalization and a ratio-scheduled pruning process. Unlike local pruning methods that optimize layer-wise reconstruction, GISP defines importance at the model level, enabling task-specific objectives and better downstream accuracy. Experiments across Llama2, Llama3, and Mistral models show that GISP consistently improves WikiText-2 perplexity and downstream accuracy, with notable gains at 40-50% sparsity.

## Method Summary
GISP implements global iterative structured pruning through first-order Taylor expansion importance estimation, block-wise normalization, and task-specific loss alignment. The method aggregates per-weight importance to attention heads and MLP channels, normalizes scores separately for each block type, and uses a linear iterative schedule to gradually increase sparsity. Unlike local methods, GISP defines importance via model-level loss rather than layer-wise reconstruction, enabling better preservation of task performance through task-aligned calibration data and objectives.

## Key Results
- GISP improves WikiText-2 perplexity and downstream accuracy across Llama2, Llama3, and Mistral models
- Notable gains at 40-50% sparsity, with 32 iterations reducing 50%-sparsity PPL by 92.82 on Llama2-7B
- Task-aligned calibration significantly boosts exact-match accuracy for DeepSeek-R1-Distill-Llama-3-8B on GSM8K (67.93% vs 25.25% at 20% sparsity)

## Why This Works (Mechanism)

### Mechanism 1: Global Loss-Guided Importance Redistribution
First-order Taylor expansion estimates per-weight importance as |⟨∇W L, W⟩|; these are aggregated to structure level (attention heads, MLP channels) with separate normalization per block type to handle magnitude disparities. Defining importance via model-level loss enables non-uniform sparsity that better preserves task performance than layer-wise reconstruction.

### Mechanism 2: Iterative Schedule Stabilizes High-Sparsity Regimes
A linear scheduler increases sparsity incrementally; each step recomputes importance on the already-pruned model, allowing iterative feedback to refine decisions. Gradual pruning with small per-step ratios mitigates perplexity collapse compared to one-shot global pruning.

### Mechanism 3: Task-Specific Loss Alignment
For multi-choice tasks, importance becomes |(∂L+/∂W − ∂L−/∂W) · W|, explicitly preserving the gap between correct and incorrect candidates rather than just minimizing next-token loss. Replacing generic perplexity loss with task-aligned objectives preserves decision boundaries during pruning.

## Foundational Learning

- **Local vs. Global Pruning Objectives**
  - Why needed: GISP's central claim is that global loss-based importance outperforms layer-wise reconstruction. You must understand what each optimizes to interpret results.
  - Quick check: Given a 4-layer model, would a local method ever prune layer 2 to 60% while keeping layer 1 at 10% if the global loss sensitivity suggests the opposite?

- **Taylor Expansion for Importance Estimation**
  - Why needed: GISP relies on first-order approximations (Eq. 3) to estimate pruning impact without computing full Hessians.
  - Quick check: If the loss landscape is highly non-linear near a weight, would first-order importance under- or over-estimate true importance?

- **Block-Wise Normalization**
  - Why needed: Attention and MLP blocks exhibit different importance magnitude scales (Fig 3(c)); without normalization, one block dominates ranking.
  - Quick check: If attention importance scores average 10× larger than MLP scores, what happens to MLP pruning without normalization?

## Architecture Onboarding

- **Component map:**
  Calibration loader -> Gradient computer -> Structure aggregator -> Block normalizer -> Global ranker -> Checkpoint recorder

- **Critical path:**
  Calibration data → Forward/backward pass → Per-weight gradients → Aggregate to structures → Normalize per block type → Global rank → Prune → Repeat until target ratio

- **Design tradeoffs:**
  - Iteration steps vs. compute: More steps stabilize high sparsity but increase wall-clock (Table 3: GISP 125 min vs. Wanda 7 min total; amortized 1.12 min/subnetwork)
  - Calibration choice: Task-specific data helps GISP but not local methods (Table 1, Table 14)
  - Loss type: Perplexity preserves generative quality; margin preserves decision boundaries—choose based on deployment goal

- **Failure signatures:**
  - Perplexity collapse at >40% sparsity with one-shot global pruning (Table 2: 50% → PPL 159.47)
  - Downstream accuracy stagnation with local methods despite task-specific calibration (Table 1: CMQA calibration yields <1% gain for Wanda)
  - Zero accuracy on reasoning tasks (GSM8K) with standard pruning on distilled models (Table 7: Wanda-sp 20% → 0%)

- **First 3 experiments:**
  1. Run GISP with 1, 32, 64, 128 steps on Llama2-7B at 50% sparsity; plot PPL vs. steps to verify stabilization claim (replicate Fig 3a)
  2. Compare C4 vs. task-specific calibration (CMQA) for both GISP and Wanda-sp; measure downstream accuracy gap to confirm GISP's task-specific property
  3. On GSM8K, compare perplexity loss vs. margin loss (if applicable) or in-domain calibration; verify Table 7 gains (67.93% vs. 25.25% at 20% with GSM8K calibration)

## Open Questions the Paper Calls Out

### Open Question 1
How can GISP be effectively adapted for Mixture-of-Experts (MoE) architectures, given their sparse activation patterns and significantly larger scale? The authors explicitly state in the Limitations section: "we have not yet evaluated it on Mixture-of-Experts (MoE) models due to their significantly larger scale. Extending GISP to MoE architectures remains a valuable direction for future exploration."

### Open Question 2
Can parameter-efficient fine-tuning (PEFT) techniques be integrated into the GISP framework to reduce the high memory footprint associated with gradient-based importance estimation? The authors note: "due to its reliance on gradient-based weight importance estimation, it can incur relatively high memory and computational costs. To address this, one could integrate parameter-efficient fine-tuning (PEFT) techniques... a direction we leave for future work."

### Open Question 3
Does optimizing for task-specific objectives (e.g., margin loss) inadvertently degrade the model's general language modeling capabilities or performance on unrelated tasks? The paper demonstrates that using a margin-based loss improves downstream accuracy for specific tasks like CMQA (Table 6). However, it does not rigorously analyze if this "task-alignment" acts as a form of overfitting that harms general perplexity or zero-shot performance on out-of-domain tasks.

### Open Question 4
Is the linear pruning ratio scheduler optimal, or would an adaptive scheduler based on loss sensitivity improve efficiency and stability? The method section states, "we use a linear scheduler that gradually increases the pruning ratio," ensuring a fixed number of structures are removed per step. The paper does not explore if dynamic schedules could better handle varying layer sensitivities.

## Limitations
- Reliance on first-order Taylor approximations may not capture highly non-linear loss landscapes in deep LLM layers
- Task-specific calibration benefits are demonstrated on limited datasets (C4, CMQA, GSM8K) and may not generalize across diverse reasoning benchmarks
- Iterative pruning schedule creates significant computational overhead that may limit practical deployment in resource-constrained scenarios

## Confidence

**High Confidence:** The iterative pruning schedule's superiority over one-shot methods at high sparsity levels (50%) is well-supported by perplexity metrics and directly demonstrated in Table 2 and Figure 3a. The block-wise normalization's necessity is empirically validated through relative importance distributions.

**Medium Confidence:** The global loss-based importance's advantage over local reconstruction metrics is supported by cross-model comparisons, though the absolute performance gap varies significantly across model sizes and sparsity levels. The task-specific calibration benefits for GISP versus local methods are demonstrated but may be dataset-dependent.

**Low Confidence:** The margin-based loss formulation's general applicability to multi-choice tasks beyond GSM8K remains unproven, and the first-order importance approximation's sufficiency for structured pruning in LLMs lacks comprehensive theoretical grounding or extensive empirical validation across diverse architectures.

## Next Checks

1. **Multi-Benchmark Reasoning Validation:** Test the margin-based loss formulation and task-specific calibration on at least two additional reasoning datasets (e.g., HumanEval, BBH) across different model families to assess generalizability beyond GSM8K.

2. **Non-Linear Landscape Analysis:** Conduct ablation studies comparing first-order importance estimates against second-order approximations or learned importance scores on a subset of weights to quantify the approximation error and identify scenarios where first-order methods fail.

3. **Resource-Constrained Iteration Scheduling:** Experiment with adaptive iteration schedules that terminate early when importance score convergence is detected, measuring the trade-off between computational efficiency and pruning quality to establish practical deployment guidelines.