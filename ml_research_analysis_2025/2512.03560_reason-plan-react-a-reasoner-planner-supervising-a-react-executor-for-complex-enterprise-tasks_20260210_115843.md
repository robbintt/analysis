---
ver: rpa2
title: 'Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex
  Enterprise Tasks'
arxiv_id: '2512.03560'
source_url: https://arxiv.org/abs/2512.03560
tags:
- agent
- react
- which
- arxiv
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RP-ReAct introduces a multi-agent architecture that decouples high-level
  planning from low-level execution to solve complex enterprise tasks involving multiple
  tools and diverse data sources. The Reasoner Planner Agent (RPA) handles strategic
  planning and error analysis, while Proxy-Execution Agents (PEAs) manage tool interactions
  using a ReAct approach.
---

# Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks

## Quick Facts
- **arXiv ID:** 2512.03560
- **Source URL:** https://arxiv.org/abs/2512.03560
- **Reference count:** 6
- **Primary result:** Multi-agent architecture with decoupled planning/execution achieves superior performance on complex ToolQA tasks, especially hard ones requiring intensive reasoning and multiple tool calls.

## Executive Summary
RP-ReAct introduces a multi-agent architecture that decouples high-level planning from low-level execution to solve complex enterprise tasks involving multiple tools and diverse data sources. The Reasoner Planner Agent (RPA) handles strategic planning and error analysis, while Proxy-Execution Agents (PEAs) manage tool interactions using a ReAct approach. A context-saving strategy mitigates context window overflow by offloading large tool outputs to external storage with on-demand access. Evaluated on the ToolQA benchmark across five domains using six open-weight reasoning models, RP-ReAct achieves superior performance compared to state-of-the-art baselines, particularly in hard tasks requiring intensive reasoning and numerous sequential tool calls. The architecture demonstrates enhanced robustness and stability across different model scales, validating its effectiveness for deployable enterprise automation solutions.

## Method Summary
RP-ReAct implements a hierarchical multi-agent system where a Reasoner Planner Agent (RPA) formulates strategic plans and sub-questions, which are delegated to Proxy-Execution Agents (PEAs) that handle tool interactions using ReAct methodology. The RPA maintains a clean reasoning context focused on high-level planning, while PEAs manage tool execution, error handling, and syntax details. Communication occurs through explicit tag-based protocols (`<|begin search query|>`, `<|begin search result|>`) to prevent context contamination. A context-saving mechanism offloads tool outputs exceeding 100 tokens to temporary variables, allowing the RPA to request full data only when needed. The system was evaluated on ToolQA benchmark across five domains (Airbnb, Flight, Coffee, Scirex, Yelp) with easy/hard splits, comparing six open-weight models against standard ReAct baselines.

## Key Results
- RP-ReAct achieves 88.7% accuracy on hard ToolQA tasks versus 76.8% for standard ReAct
- Performance gains are most pronounced for tasks requiring 7+ sequential tool calls
- Smaller models (<10B parameters) fail regardless of architecture, indicating minimum viable scale is ~14B+
- Context-saving mechanism reduces token consumption by 37% while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Role Decoupling
Separating strategic planning (RPA) from tool execution (PEA) reduces cognitive load and prevents trajectory deviation in complex multi-step tasks. The RPA maintains a clean context focused on high-level reasoning and sub-question formulation. Tool interactions, error handling, and syntax details are offloaded to PEA(s). When execution fails, the RPA receives only abstract success/failure signals and can re-plan without being polluted by low-level execution noise. Core assumption: Models perform better when reasoning and execution are cognitively isolated rather than interleaved in a single monolithic loop.

### Mechanism 2: Context-Saving with Threshold-Based Offloading
Offloading large tool outputs to external storage with on-demand access prevents context window overflow while preserving data accessibility. When tool output exceeds threshold T (set to 100 tokens), only the first T tokens enter the PEA's context. The full output is stored in a temporary variable in the execution environment. The PEA informs the RPA of the variable name and that Python execution is required for full analysis. Core assumption: Agents often need only structural previews of large tabular data to generate correct analysis code; full data injection is unnecessary and harmful.

### Mechanism 3: Structured Communication Protocol with Explicit Tags
Delimiting sub-questions and results with explicit tags (`<|begin search query|>`, `<|begin search result|>`) enables reliable parsing and maintains clean separation between planning and execution contexts. The RPA outputs sub-questions within search query tags. The PEA parses these, executes tool calls, and returns results within search result tags. The RPA then evaluates success/failure and either proceeds or triggers re-planning. Core assumption: Tag-based delimiters provide more reliable parsing than implicit natural language handoffs between agents.

## Foundational Learning

- **Concept: ReAct (Reasoning + Acting) paradigm**
  - **Why needed here:** The PEA implements ReAct's Think-Act-Observe loop. Understanding this foundational pattern is essential for configuring the executor agent's behavior and debugging its traces.
  - **Quick check question:** Can you trace a single PEA step showing Thought → Action → Observation for a simple database query?

- **Concept: Large Reasoning Models (LRMs) vs standard LLMs**
  - **Why needed here:** The RPA leverages LRMs (e.g., DeepSeek-R1, Qwen3) specifically for their enhanced deliberative reasoning. Selecting appropriate models for each role is critical.
  - **Quick check question:** What distinguishes an LRM's output structure from a standard LLM when given a multi-step planning task?

- **Concept: Context window economics in open-weight models**
  - **Why needed here:** Enterprise deployments often require open-weight models with smaller context windows (vs. proprietary models). The context-saving mechanism directly addresses this constraint.
  - **Quick check question:** Given a 4K token context window and a 3K token SQL result, how would RP-ReAct's threshold mechanism alter what enters the PEA's context?

## Architecture Onboarding

- **Component map:** User Task → RPA (decomposition → sub-question → `<|begin search query|>`) → PEA (ReAct loop → tool execution) → RPA (evaluation → next sub-question OR re-plan OR `<Finish>answer</Finish>`)

- **Critical path:**
  1. User task → RPA decomposes into sub-question → `<|begin search query|>`
  2. PEA receives query → ReAct loop (Think → Act → Observe) → Tool execution
  3. If output > T: partial preview + variable name → RPA
  4. RPA evaluates → next sub-question OR re-plan OR `<Finish>answer</Finish>`

- **Design tradeoffs:**
  - Step limits: RPA max 10 steps; PEA max 10 steps per sub-question (100 worst-case total). ReAct baseline has 20 steps. RP-ReAct's structured planning uses steps more efficiently on hard tasks.
  - Temperature: Set to 0.6 for both agents. Paper suggests future exploration of RPA temperature = 0.0 for deterministic planning.
  - Model selection: Smaller models (<10B) fail regardless of architecture due to premature responses and trajectory deviation. Minimum viable scale appears to be ~14B+.

- **Failure signatures:**
  - PEA ignores protocol: Returns immediate answers instead of delegating (common in <10B models).
  - RPA redundant planning: Repeats unnecessary steps like database re-loading (observed on easy tasks).
  - Context overflow: Without threshold mechanism, large SQL/CSV outputs consume context window, causing trajectory drift.

- **First 3 experiments:**
  1. Baseline comparison: Run standard ReAct vs. RP-ReAct on 5 ToolQA domains (easy + hard) with Qwen3-14B. Expect RP-ReAct advantage on hard tasks, ReAct advantage on easy tasks.
  2. Threshold sensitivity: Test T ∈ {50, 100, 200, 500} on tasks with large tabular outputs. Measure accuracy vs. token consumption.
  3. Model scale ablation: Compare gpt-oss-20B vs. gpt-oss-120B as RPA (with same PEA). Isolate whether RPA reasoning capability or PEA execution is the bottleneck for hard tasks.

## Open Questions the Paper Calls Out

- **Question:** Does scaling the number of Proxy-Execution Agents (PEAs) improve performance on complex reasoning benchmarks beyond ToolQA, such as OfficeBench and Mint?
  - **Basis:** Authors plan to expand evaluation to OfficeBench and Mint using increased PEAs.
  - **Why unresolved:** Current setup limited to ToolQA with 1 RPA and 1 PEA.
  - **Evidence needed:** Empirical results from multi-PEA configuration on OfficeBench/Mint.

- **Question:** Can Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL) specifically optimized for the RPA and PEA reduce redundant planning steps and incorrect re-planning actions?
  - **Basis:** Paper notes no post-training optimization techniques were employed.
  - **Why unresolved:** Results rely on baseline model capabilities without domain-specific weight updates.
  - **Evidence needed:** Comparison of trajectory efficiency and accuracy between baseline and post-trained versions.

- **Question:** What is the optimal token threshold (T) for context offloading, and does integrating token summarization offer superior performance over simple variable storage?
  - **Basis:** Authors acknowledge need for comprehensive investigation of various T thresholds and summarization mechanisms.
  - **Why unresolved:** Study used fixed threshold (T=100) without ablation studies.
  - **Evidence needed:** Ablation study measuring context retention and task accuracy across varying thresholds and summarization techniques.

- **Question:** Does setting the Reasoner Planner Agent's (RPA) temperature to 0.0 improve planning determinism and reliability compared to the static 0.6 setting used in the study?
  - **Basis:** Authors list as limitation: "Our experiments used a static temperature of 0.6. Future iterations will explore agent-specific temperature tuning, such as lowering the RPA's temperature to 0.0 to prioritize deterministic outputs over creativity."
  - **Why unresolved:** High-level planning may require deterministic outputs for stable trajectories, but this was not tested.
  - **Evidence needed:** Comparative analysis of trajectory stability and success rates when running RPA at temperature 0.0 versus 0.6.

## Limitations
- Evaluation confined to ToolQA benchmark with five database-centric domains, limiting generalizability to diverse enterprise tasks
- Performance advantage over ReAct only pronounced on hard tasks; architectural overhead introduces inefficiency on simple tasks
- Implementation relies on open-weight models whose accessibility and reproducibility may be challenging for practitioners

## Confidence
- **High Confidence (0.85-0.95):** Architectural separation improves performance on complex multi-step tasks; context-saving prevents overflow with minimal accuracy impact; smaller models (<10B) consistently fail regardless of architecture
- **Medium Confidence (0.65-0.85):** Tag-based communication provides reliable parsing; performance gains justify complexity for enterprise deployment; temperature 0.6 represents optimal balance
- **Low Confidence (0.45-0.65):** 100-token threshold is optimal across scenarios; architecture maintains advantages with hundreds of tool types; CPS metric provides meaningful differentiation

## Next Checks
1. **Cross-Benchmark Validation:** Evaluate RP-ReAct on multiple enterprise-oriented benchmarks (e.g., HotpotQA, DROP, or industry-specific datasets) to assess generalization beyond ToolQA's database-centric tasks.

2. **Context-Saving Sensitivity Analysis:** Systematically vary the threshold parameter T across a broader range (e.g., 50-500 tokens) and measure the trade-off between token efficiency and accuracy degradation across different data types.

3. **Real-World Enterprise Deployment Simulation:** Create a synthetic enterprise environment with realistic constraints including API rate limits, heterogeneous data sources, and concurrent task execution to measure RP-ReAct's performance under these conditions.