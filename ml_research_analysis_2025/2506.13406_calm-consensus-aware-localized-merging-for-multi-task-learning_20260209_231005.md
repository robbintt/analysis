---
ver: rpa2
title: 'CALM: Consensus-Aware Localized Merging for Multi-Task Learning'
arxiv_id: '2506.13406'
source_url: https://arxiv.org/abs/2506.13406
tags:
- merging
- task
- tasks
- calm
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively integrating multiple
  independently fine-tuned models into a unified multi-task learning (MTL) model.
  Existing global-aware methods suffer from parameter interference, while local-aware
  methods struggle to maintain the effectiveness of task-specific details.
---

# CALM: Consensus-Aware Localized Merging for Multi-Task Learning

## Quick Facts
- arXiv ID: 2506.13406
- Source URL: https://arxiv.org/abs/2506.13406
- Authors: Kunda Yan; Min Zhang; Sen Cui; Zikun Qu; Bo Jiang; Feng Liu; Changshui Zhang
- Reference count: 29
- Primary result: CALM achieves 87.7% average accuracy across eight vision tasks, outperforming state-of-the-art merging methods by 1-3%

## Executive Summary
This paper addresses the challenge of integrating multiple independently fine-tuned models into a unified multi-task learning (MTL) model. The authors identify limitations in both global-aware methods (which suffer from parameter interference) and local-aware methods (which struggle to maintain task-specific details). They propose CALM (Consensus-Aware Localized Merging), a method that combines localized information with global task consensus to overcome these limitations. The approach achieves performance close to traditional MTL while being more efficient and scalable.

## Method Summary
CALM introduces three key components to address multi-task model merging: class-balanced entropy minimization sampling that leverages unsupervised data flexibly; an efficient-aware framework that selects a small set of tasks for sequential merging with high scalability; and consensus-aware mask optimization that aligns localized binary masks with global task consensus for conflict-free merging. The method balances the need for global consensus with task-specific details through a novel mask optimization approach that ensures smooth integration across tasks.

## Key Results
- Achieves 87.7% average accuracy across eight vision classification tasks
- Outperforms state-of-the-art methods by 1-3% in accuracy
- Performance approaches that of traditional multi-task learning approaches
- Demonstrates superior scalability through efficient task selection strategy

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental tension between global consistency and local specificity in multi-task learning. By using consensus-aware mask optimization, CALM can preserve task-specific features while ensuring that the merged model maintains coherent representations across all tasks. The class-balanced entropy minimization sampling ensures that unsupervised data is leveraged effectively, preventing bias toward tasks with more labeled examples. The efficient task selection strategy enables scalable merging without sacrificing performance.

## Foundational Learning
- Multi-task learning fundamentals: Understanding how models learn multiple tasks simultaneously and the challenges of interference between tasks. Why needed: To appreciate the problem CALM solves. Quick check: Review standard MTL architectures and their limitations.
- Model merging techniques: Knowledge of existing approaches to combine independently trained models. Why needed: To understand the innovation CALM brings. Quick check: Compare gradient-based vs. parameter-sharing approaches.
- Entropy minimization: Understanding how entropy-based sampling can improve model training. Why needed: To grasp the sampling strategy in CALM. Quick check: Implement basic entropy-based sampling on a simple dataset.

## Architecture Onboarding
Component map: Data sampling -> Task selection -> Mask optimization -> Model merging

Critical path: The core innovation lies in the consensus-aware mask optimization component, which takes as input the selected tasks and their associated parameters, then generates conflict-free masks that balance global consensus with local specificity.

Design tradeoffs: The sequential task selection strategy trades off between computational efficiency and potential loss of information from non-selected tasks. The mask optimization balances between preserving task-specific details and maintaining global coherence.

Failure signatures: Potential failures include mask conflicts when tasks have highly divergent feature representations, or suboptimal performance when the task selection strategy misses critical tasks for the final merged model.

First experiments:
1. Replicate the eight-task vision classification benchmark to verify reported performance improvements
2. Perform ablation study removing the consensus-aware component to measure its contribution
3. Test the method on a new task distribution to evaluate generalization beyond the original experiments

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to vision classification tasks, raising questions about cross-domain effectiveness
- No detailed analysis of computational overhead or memory requirements during merging
- Absence of comparison with more recent model merging approaches that may have emerged

## Confidence
- Performance claims: Medium (based on limited task scope)
- Methodology soundness: Medium (lacks computational complexity analysis)
- Generalizability: Low-Medium (domain-specific evaluation)

## Next Checks
1. Conduct experiments on diverse task types beyond vision classification (e.g., NLP, audio processing) to assess CALM's cross-domain effectiveness and identify potential limitations in different application areas.

2. Perform detailed computational complexity analysis comparing CALM with existing methods, including runtime, memory usage, and scalability measurements across varying numbers of tasks and model sizes.

3. Investigate the sensitivity of CALM's performance to different hyperparameter settings, particularly the task selection strategy and mask optimization parameters, through systematic ablation studies and robustness analysis.