---
ver: rpa2
title: Benchmarking global optimization techniques for unmanned aerial vehicle path
  planning
arxiv_id: '2501.14503'
source_url: https://arxiv.org/abs/2501.14503
tags:
- optimization
- methods
- path
- planning
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks global optimization techniques for UAV path
  planning. A problem instance generator creates 56 diverse terrains with obstacles
  and threats.
---

# Benchmarking global optimization techniques for unmanned aerial vehicle path planning

## Quick Facts
- arXiv ID: 2501.14503
- Source URL: https://arxiv.org/abs/2501.14503
- Reference count: 40
- Top-performing methods: EA4eig, APGSK, and ELSHADE from recent IEEE CEC competitions

## Executive Summary
This paper benchmarks 12 global optimization techniques on UAV path planning problems across varying dimensions and computational budgets. The study generates 56 diverse terrain instances with obstacles and threats, comparing both deterministic and stochastic methods. Results show evolutionary computation methods, particularly recent IEEE CEC competition winners, consistently outperform other approaches. The research reveals that lower-dimensional problem formulations yield better solutions due to computational efficiency, and that the UAV problem landscape differs substantially from standard benchmark suites.

## Method Summary
The study compares 12 global optimization algorithms (deterministic and stochastic) on a 3D UAV path planning problem with 56 hand-picked terrain instances. Each instance contains cylindrical threat obstacles (15 or 30) on 900×900 pixel terrains. The composite cost function combines path length, obstacle avoidance, altitude, and smoothness with a finite collision penalty (Jpen = 10⁴). Methods are tested across 4 decision vector dimensions (DV = 5, 10, 15, 20) and 3 budget levels (B = 10³, 10⁴, 10⁵), with maximum evaluations = 3 × DV × B. Performance is measured via mean relative error, number of wins, and Friedman ranks.

## Key Results
- Evolutionary computation methods from recent IEEE CEC competitions (EA4eig, APGSK, ELSHADE) outperform both classical metaheuristics and deterministic methods
- Lower-dimensional formulations (DV=5) yield better solutions than higher dimensions under equivalent computational budgets
- The UAV path planning problem exhibits a unique landscape distinct from standard benchmark suites, as shown through Exploratory Landscape Analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evolutionary computation methods from recent IEEE CEC competitions outperform both classical metaheuristics and deterministic methods on UAV path planning problems.
- Mechanism: These methods combine adaptive parameter control (SHBA), population size reduction (LPSR), and covariance-based search direction adaptation (CMA-ES elements), which better suit the multi-modal, constraint-heavy objective function landscape of UAV problems.
- Core assumption: The combination of adaptive strategies transfers effectively from numerical benchmarks to the structured terrain-based cost landscape of UAV path planning.
- Evidence anchors: The abstract states top-ranking methods were "top-performing evolutionary techniques from recent competitions on numerical optimization at the Institute of Electrical and Electronics Engineers Congress on Evolutionary Computation."

### Mechanism 2
- Claim: Lower-dimensional problem formulations consistently produce better solutions than higher-dimensional formulations under equivalent computational budgets.
- Mechanism: Lower-dimensional search spaces allow more thorough exploration per dimension with fixed evaluation budgets. Initial coarse paths capture major constraint-avoidance requirements, while refinement through additional waypoints provides marginal improvements at high computational cost.
- Core assumption: The optimal path can be reasonably approximated with relatively few waypoints (5-10 intermediate points), and penalty-based constraint handling doesn't require dense waypoint placement.
- Evidence anchors: Table 7 shows DV=5 achieving the most "wins" (45-51 across budgets) and lowest mean relative errors (0.005-0.020) compared to DV=20 (0-2 wins, 0.222-0.579 mean relative error).

### Mechanism 3
- Claim: The composite cost function formulation creates a landscape distinct from standard benchmark suites.
- Mechanism: The UAV cost function combines continuous optimization with hard constraint penalties, creating discontinuous "cliffs" where small waypoint movements trigger large cost jumps when crossing threat boundaries. Standard benchmarks lack this hybrid continuous-discrete structure.
- Evidence anchors: PCA, t-SNE, and UMAP visualizations show UAV problem instances clustering separately from BBOB, CEC, and ABS benchmark functions in ELA feature space.

## Foundational Learning

- Concept: **Black-box optimization and function evaluation budgets**
  - Why needed here: The paper evaluates algorithms based on fixed function evaluation limits (3 × DV × B). Understanding that algorithms must balance exploration vs. exploitation within these budgets is essential for interpreting results.
  - Quick check question: Given a budget of 30,000 evaluations for a 30-dimensional problem, can you explain why a population-based method with population size 100 can only run ~300 generations?

- Concept: **Composite objective functions with penalty methods**
  - Why needed here: The UAV cost function combines multiple objectives with weights and uses penalty values (Jpen = 10⁴) for constraint violation. This structure affects algorithm behavior differently than unconstrained benchmarks.
  - Quick check question: Why might setting Jpen = ∞ prevent some optimization algorithms from making progress, compared to using a large finite penalty?

- Concept: **Exploratory Landscape Analysis (ELA) features**
  - Why needed here: The paper uses ELA to demonstrate that generated UAV instances differ from standard benchmark suites. ELA features quantify landscape properties like multimodality, separability, and global structure.
  - Quick check question: If two optimization problems have similar ELA feature vectors, what does this suggest about the likely relative performance of algorithms tested on them?

## Architecture Onboarding

- Component map: Terrain Generator -> Cost Function Evaluator -> Optimization Algorithm Interface -> Statistical Analysis Pipeline
- Critical path: 1. Generate terrain instances with threat placement -> 2. Configure optimization method with DV and budget -> 3. Run optimization with fixed random seed -> 4. Compute relative error against best-found solution -> 5. Aggregate statistics across 56 instances
- Design tradeoffs:
  - Hand-picked 28 terrains from 5000 generated provides diverse challenges but introduces selection bias
  - Jpen = 10⁴ allows algorithms to explore infeasible regions temporarily; Jpen = ∞ blocks progress but guarantees feasible-only search
  - maxevaluations = 3 × DV × B scales budget with dimensionality; alternative is fixed budget regardless of dimension
- Failure signatures:
  - Convergence plot shows large step discontinuities: indicates algorithm finding paths that pass through fewer threats in discrete jumps
  - Method achieves low Friedman rank but high mean relative error: suggests method finds good solutions on easy instances but fails catastrophically on difficult ones
  - Best method varies across evaluation metrics: indicates non-robust performance
  - Negative improvement when increasing budget: suggests algorithm implementation issues or hyperparameter sensitivity
- First 3 experiments:
  1. Run all 12 methods on 5 randomly selected terrain instances with DV=10, B=10⁴, tracking convergence curves to verify correctness against Table 3
  2. For a single terrain instance, compare path quality and computation time across DV ∈ {5, 10, 15, 20} using EA4eig to understand where additional waypoints help vs. add computational burden
  3. Test whether the gap between top CEC methods and standard methods persists when reducing the computational budget to B=500 evaluations

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on hand-selected terrain instances introduces potential selection bias (only 56 cases from 5000 generated)
- Specific weight coefficients for the composite cost function are not explicitly stated in the text
- While ELA analysis shows UAV problems differ from standard benchmarks, practical implications for algorithm transfer remain qualitative rather than quantified

## Confidence
- High confidence in the comparative rankings of the 12 optimization methods
- Medium confidence in absolute performance values due to potential implementation differences
- Medium confidence in the variable dimension finding, as this characteristic appears under-explored in the broader literature
- Medium confidence in the landscape analysis conclusions, as the ELA feature interpretation relies on qualitative clustering patterns

## Next Checks
1. Verify the relative error distributions of EA4eig, APGSK, and ELSHADE on 3-5 randomly selected terrain instances match the reported rankings
2. Test whether the performance gap between top CEC methods and standard methods persists when reducing the computational budget to B=500 evaluations
3. Conduct a sensitivity analysis by varying the obstacle penalty value Jpen between 10⁴ and 10⁶ to assess the impact on algorithm behavior and solution quality