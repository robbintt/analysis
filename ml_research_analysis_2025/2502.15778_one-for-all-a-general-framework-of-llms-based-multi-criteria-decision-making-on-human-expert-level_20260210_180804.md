---
ver: rpa2
title: 'One for All: A General Framework of LLMs-based Multi-Criteria Decision Making
  on Human Expert Level'
arxiv_id: '2502.15778'
source_url: https://arxiv.org/abs/2502.15778
tags:
- evaluation
- mcdm
- quality
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a general framework leveraging large language
  models (LLMs) for multi-criteria decision making (MCDM) tasks, addressing the limitations
  of traditional methods in high-dimensional problems. The framework employs few-shot
  learning, chain-of-thought reasoning, and LoRA-based fine-tuning to enhance LLM
  performance.
---

# One for All: A General Framework of LLMs-based Multi-Criteria Decision Making on Human Expert Level

## Quick Facts
- arXiv ID: 2502.15778
- Source URL: https://arxiv.org/abs/2502.15778
- Reference count: 15
- Key outcome: Basic LLMs achieve ~60% accuracy on MCDM tasks, improving to ~70% with few-shot/CoT prompting and ~95% with LoRA fine-tuning

## Executive Summary
This paper presents a general framework for using large language models (LLMs) to perform multi-criteria decision making (MCDM) at human expert level. The authors address the limitations of traditional MCDM methods in high-dimensional problems by leveraging LLMs with few-shot learning, chain-of-thought reasoning, and LoRA-based fine-tuning. Experiments across three MCDM applications demonstrate that while basic LLMs achieve only ~60% accuracy, incorporating few-shot examples or CoT prompting improves performance to ~70%. LoRA fine-tuning further boosts accuracy to ~95% across different model sizes, achieving human-expert-level performance while reducing model-specific differences.

## Method Summary
The framework combines prompt engineering (zero-shot, few-shot, and chain-of-thought prompting with weighted criteria) with parameter-efficient LoRA fine-tuning. The approach uses 500 labeled examples per MCDM application to fine-tune open-source models while freezing pre-trained weights. Ground truth labels are derived from traditional AHP-FCE expert evaluation methods. The system evaluates multiple models including API-based (ChatGPT, Claude) and open-source (LLaMA, Qwen, ChatGLM) variants across supply chain evaluation, customer satisfaction, and air quality assessment tasks.

## Key Results
- Zero-shot LLMs achieve only ~55-60% F1 score on MCDM tasks
- Few-shot or CoT prompting improves accuracy to ~70%, with performance varying by model
- LoRA fine-tuning achieves ~95% accuracy across all tested models and tasks
- Model-specific performance differences are significantly reduced after LoRA fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Few-shot Pattern Transfer with Weighted Criteria
Providing few-shot examples combined with domain-specific weight information improves MCDM performance by teaching the model evaluation patterns while guiding attention to more important criteria. Expert-derived weights from traditional AHP-FCE methods encode meaningful decision priorities that transfer to LLM reasoning, reducing noise from less relevant dimensions.

### Mechanism 2: LoRA Fine-tuning for Domain-Specific Decision Encoding
LoRA-based parameter-efficient fine-tuning enables human-expert-level MCDM performance (~95% accuracy) with only 500 labeled examples. Low-rank adaptation matrices capture domain-specific decision patterns while freezing pre-trained weights, allowing efficient knowledge injection without catastrophic forgetting.

### Mechanism 3: Chain-of-Thought Decomposition for Multi-Criteria Integration
Explicit step-by-step reasoning instructions improve performance on complex MCDM tasks by decomposing criterion evaluation before synthesis. CoT prompts force the model to generate intermediate reasoning for each criterion, reducing shortcut reasoning and improving calibration on multi-dimensional inputs.

## Foundational Learning

- **Concept: Multi-Criteria Decision Making (MCDM) and AHP-FCE Methods**
  - Why needed here: The paper assumes familiarity with traditional MCDM methods to understand what LLMs are replacing and how weights are derived
  - Quick check question: Can you explain how AHP derives criterion weights and how FCE aggregates fuzzy evaluations into a final decision?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The core technical contribution relies on understanding how LoRA enables efficient fine-tuning with rank-constrained updates
  - Quick check question: Given a 7B parameter model and LoRA rank r=8, how many trainable parameters are added versus full fine-tuning?

- **Concept: Prompt Engineering Paradigms (Zero-shot, Few-shot, CoT)**
  - Why needed here: The experimental design systematically compares these approaches; understanding their tradeoffs is essential for interpreting results
  - Quick check question: Why might few-shot + CoT not always outperform few-shot alone (as observed in some model/task combinations)?

## Architecture Onboarding

- **Component map**: Input Data → Weight Extraction (AHP-FCE) → Prompt Construction (role + task + criteria + weights + examples + CoT template) → Model Selection → If Open-source: LoRA Fine-tuning → Inference → Evaluation Output → Metrics

- **Critical path**:
  1. Ground truth labeling aligned with expert evaluations (AHP-FCE derived labels)
  2. Prompt template design incorporating weighted criteria and evaluation rubric
  3. LoRA fine-tuning with learning rate and rank selection
  4. Validation on held-out test set comparing to expert ground truth

- **Design tradeoffs**:
  - API vs. Open-source: API models (70% best case) vs. LoRA-tuned open-source (95%); trade API convenience for fine-tuning capability
  - Data efficiency vs. generality: 500 examples sufficient for specific MCDM task but may not generalize to new domains
  - Model size vs. performance gap: Paper shows LoRA reduces model-specific differences, suggesting smaller models become viable after fine-tuning

- **Failure signatures**:
  - Zero-shot F1 ~50-60%: Model lacks task understanding; needs prompt engineering or fine-tuning
  - High variance across models with same prompt: Indicates prompt brittleness; consider CoT or fine-tuning
  - Fine-tuned model overfits to training distribution: Check confusion matrix for class imbalance issues
  - Weighted prompts underperform unweighted: Possible weight misalignment with actual task importance

- **First 3 experiments**:
  1. Baseline characterization: Run all API and open-source models in zero-shot mode on one dataset; record F1 scores to confirm ~55-60% baseline
  2. Prompt engineering sweep: Systematically test (few-shot only, CoT only, few-shot + CoT) on the same dataset; identify best prompt configuration per model; target ~70% F1
  3. LoRA fine-tuning validation: Fine-tune one open-source model with 500 examples; verify ~95% F1 and analyze confusion matrix to identify remaining failure modes

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the proposed framework maintain human-expert-level accuracy when applied to a wider range of real-world MCDM applications beyond the three tested domains?
  - Basis in paper: The conclusion states, "For future work, applying the proposed framework to more real applications is promising."
  - Why unresolved: The current study validates the method only on supply chain, customer satisfaction, and air quality datasets, leaving performance in other high-dimensional or domain-specific scenarios unverified.

- **Open Question 2**: Can LLMs be leveraged to generate and tag synthetic fine-tuning data that is as effective as costly human-annotated data?
  - Basis in paper: The authors note, "it is also interesting to generate and tag fine-tuning MCDM data on LLMs, since... collecting and tagging real data is costly."
  - Why unresolved: While LoRA fine-tuning proved effective, it relied on existing datasets. The viability of using LLMs to bootstrap their own training data for MCDM remains untested.

- **Open Question 3**: How robust is the framework when processing unstructured, noisy, or conflicting natural language inputs rather than the structured key-value pairs used in the experiments?
  - Basis in paper: The provided examples utilize structured input strings (e.g., "Temperature: 26.3°C").
  - Why unresolved: Real-world expert decision-making often involves ambiguous text or conflicting qualitative descriptions. The model's ability to parse and weigh criteria from unstructured text is not demonstrated.

## Limitations
- The claimed ~95% accuracy across diverse MCDM tasks appears highly optimistic given the complexity of real-world decision scenarios
- Limited transparency around LoRA hyperparameter selection makes it difficult to assess reproducibility and generalizability
- The paper assumes AHP-FCE derived ground truth labels are definitive benchmarks, but expert judgment methods can contain inherent subjectivity

## Confidence
- **High confidence**: LoRA fine-tuning improves MCDM performance significantly (95% accuracy vs baseline ~60%)
- **Medium confidence**: Few-shot and CoT prompting reliably achieve ~70% accuracy; effectiveness may depend on task complexity and model capacity
- **Low confidence**: The framework achieves truly "human-expert-level" performance; claims require independent validation on held-out expert-verified datasets

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary LoRA rank (r=4, 8, 16) and learning rates (1e-4, 5e-5, 1e-5) to identify optimal settings and assess robustness
2. **Cross-dataset generalization test**: Train LoRA on supplier evaluation data and evaluate on customer satisfaction and air quality datasets to measure domain transfer capability
3. **Expert ground truth validation**: Conduct blind expert evaluations on model predictions to verify alignment between model outputs and human judgment, particularly for cases where models disagree with AHP-FCE labels