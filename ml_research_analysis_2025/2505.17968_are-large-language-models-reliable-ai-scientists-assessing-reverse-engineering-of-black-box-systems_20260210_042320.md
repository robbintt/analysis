---
ver: rpa2
title: Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering
  of Black-Box Systems
arxiv_id: '2505.17968'
source_url: https://arxiv.org/abs/2505.17968
tags:
- black-box
- language
- black
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how well large language models (LLMs)\
  \ can reverse-engineer black-box systems by inferring their underlying structure\
  \ from observed behavior. The authors explore three types of black-box systems\u2014\
  list-mapping programs, formal languages, and math equations\u2014and compare LLM\
  \ performance using passive observations versus active interventions (where the\
  \ LLM generates queries to test hypotheses)."
---

# Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems

## Quick Facts
- arXiv ID: 2505.17968
- Source URL: https://arxiv.org/abs/2505.17968
- Reference count: 40
- Key outcome: LLMs significantly improve reverse-engineering performance when allowed to actively intervene and query black-box systems, rather than passively observing

## Executive Summary
This paper investigates how well large language models (LLMs) can reverse-engineer black-box systems by inferring their underlying structure from observed behavior. The authors explore three types of black-box systems—list-mapping programs, formal languages, and math equations—and compare LLM performance using passive observations versus active interventions (where the LLM generates queries to test hypotheses). Across all three domains, LLMs perform significantly worse than Bayesian inference models when relying solely on passive observations, often plateauing at suboptimal performance. However, allowing LLMs to actively intervene and query the black-box substantially improves their ability to correctly infer the underlying rules. The authors find that this improvement stems from the LLM refining its own beliefs through testing, rather than simply collecting higher-quality data. They identify two common failure modes—overcomplication (making overly complex hypotheses) and overlooking (failing to incorporate observations)—which active intervention helps mitigate. Additionally, they show that intervention data generated by one LLM is less useful to another, highlighting model-specific benefits of active learning.

## Method Summary
The paper uses GPT-4o to reverse-engineer three types of black-box systems: list-mapping programs (100 instances from Rule et al. [58]), formal languages (46 instances from Yang & Piantadosi [79]), and CES utility functions with parameters a_i and ρ. For observation-only, the LLM outputs hypotheses after seeing N={2,5,10,20,50,100} random input-output pairs. For observation-intervention, the LLM iterates through query/test/stop actions after 10 initial observations, with M={5,10,20,50} intervention rounds. Performance is evaluated descriptively (0-10 scale by LLM judge) for Program/Formal Language, and with 1-RMSE for Math Equation parameters, comparing against Bayesian inference baselines.

## Key Results
- LLMs perform significantly worse than Bayesian inference models with passive observations, plateauing at suboptimal performance
- Active intervention substantially improves LLM reverse-engineering across all three domains
- The improvement comes from the query-generation process itself, not just data quality (demonstrated by intervention-yoked control experiments)
- Two failure modes dominate: overcomplication (Program task) and overlooking (Math Equation task), both mitigated by intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active intervention improves reverse-engineering performance because the process of generating queries engages hypothesis refinement, not merely because the collected data is more informative.
- Mechanism: When an LLM constructs a query, it must verbalize or implicitly commit to a hypothesis about the black-box. The act of selecting which edge case to test forces the model to confront its uncertainty. When the black-box response contradicts expectations, the model revises its hypothesis. This iterative belief-updating loop is where learning occurs.
- Core assumption: The LLM's internal hypothesis is meaningfully shaped by the query-generation process, and the model can revise beliefs when predictions fail.
- Evidence anchors:
  - [abstract]: "this improvement is partly a result of engaging in the process of generating effective interventions, paralleling results in the literature on human learning"
  - [section 4.3]: Intervention-yoked GPT-4o (passively receiving another model's intervention data) underperforms observation-intervention across all three black-box types, demonstrating that generating queries—not just receiving informative data—drives improvement.
  - [corpus]: Limited corpus support; related work (PentestEval, Black-Box Guardrail Reverse-engineering) addresses LLM probing but not the mechanism of query-generation-as-learning.
- Break condition: If the black-box provides sparse or ambiguous feedback (e.g., Board Game in Appendix I), intervention fails to improve performance because the hypothesis-testing loop cannot converge.

### Mechanism 2
- Claim: Intervention mitigates the overcomplication failure mode by forcing the LLM to falsify unnecessarily complex hypotheses through targeted queries.
- Mechanism: Overcomplication occurs when the LLM imports prior knowledge that doesn't apply to the specific black-box (e.g., assuming a list-mapping program includes sorting when it doesn't). Active intervention lets the model test whether these assumed operations are present. When queries designed to detect complexity return simpler results, the model abandons the overcomplicated hypothesis.
- Core assumption: The LLM can generate queries specifically designed to discriminate between simple and complex hypotheses.
- Evidence anchors:
  - [section 5.1]: For Program black-boxes, 17 of 20 failed examples were classified as overcomplication. Figure 5 shows intervention gains diminish at higher complexity levels for Program, suggesting intervention helps most when the true hypothesis is simpler than the LLM's prior assumption.
  - [Table 2]: Example shows LLM inferring a sorting operation that doesn't exist in the ground truth—a classic overcomplication error.
  - [corpus]: No direct corpus evidence for this specific mechanism.
- Break condition: If task complexity is genuinely high (Program complexity level 5), intervention provides less marginal benefit because the true hypothesis is closer to the LLM's complex prior.

### Mechanism 3
- Claim: Intervention mitigates the overlooking failure mode by requiring the LLM to engage with specific data patterns rather than defaulting to generic conclusions.
- Mechanism: Overlooking occurs when the LLM ignores observed data and produces under-specified hypotheses (e.g., "the utility function depends on quantities" without inferring parameters). Intervention forces the model to make predictions that can be falsified, exposing gaps in generic hypotheses and requiring incorporation of observed patterns.
- Core assumption: The LLM's query-generation process surface latent inattention to specific data patterns.
- Evidence anchors:
  - [section 5.1]: For Math Equation black-boxes, 16 of 20 failed examples were classified as overlooking. Figure 5 shows intervention gains increase at higher complexity levels for Math Equation, where overlooking is dominant.
  - [Table 5]: Example shows LLM stating the utility function form without actually fitting parameters to observations—intervention would require parameter-specific predictions.
  - [corpus]: No direct corpus evidence for this specific mechanism.
- Break condition: If the intervention budget is insufficient or the model generates uninformative queries, overlooking persists because key patterns remain untested.

## Foundational Learning

- Concept: **Bayesian posterior updating**
  - Why needed here: The paper uses Bayesian inference as the optimal reference. Understanding how priors combine with likelihoods to form posteriors is essential to interpret the performance gap between LLMs and Bayesian models.
  - Quick check question: Given observations O and hypothesis space H, can you write the posterior P(H|O) ∝ P(O|H)P(H)?

- Concept: **Active learning vs. passive observation**
  - Why needed here: The core distinction in the paper is between receiving random observations (passive) and selecting queries to maximize information (active). The intervention-yoked control isolates the value of query selection.
  - Quick check question: Why might a learner who selects their own training examples outperform one who receives the same examples passively?

- Concept: **Inductive bias and Occam's Razor**
  - Why needed here: The evaluation criteria (Section D.2) penalize hypotheses with "extra unused parts." Both failure modes relate to inductive bias: overcomplication is an overly strong bias toward complexity; overlooking is a weak bias that under-constrains hypotheses.
  - Quick check question: In hypothesis selection, how does a simplicity prior (Occam's Razor) trade off against fit to data?

## Architecture Onboarding

- Component map:
  - Black-box environment -> Reverse-engineer LLM -> Judge LLM -> Bayesian oracle
  - Black-box environment: Three APIs (Program, Formal Language, Math Equation) exposing observation and intervention modes
  - Reverse-engineer LLM: GPT-4o (or alternative) prompted to output hypothesis or iterate through query/test/stop actions
  - Judge LLM: Separate GPT-4o instance scoring descriptive hypotheses against ground truth on 0-10 scale
  - Bayesian oracle: Domain-specific inference algorithms providing optimal reference performance

- Critical path:
  1. Initialize black-box instance with hidden ground truth
  2. Generate N passive observations (random queries)
  3. For observation-only: LLM outputs hypothesis after seeing observations
  4. For observation-intervention: LLM iteratively constructs queries, receives responses, optionally verbalizes hypothesis, and stops when confident
  5. Judge LLM scores output hypothesis against ground truth

- Design tradeoffs:
  - **Descriptive vs. functional evaluation**: Descriptive (verbalized rule) captures communicative correctness; functional (prediction accuracy on held-out queries) captures behavioral equivalence. Appendix H shows these can diverge—choose based on deployment context.
  - **Intervention budget**: More rounds improve performance but increase cost and latency. Figure 3 shows diminishing returns after ~30-40 datapoints for most tasks.
  - **Reasoning strategy**: Section 5.2 compares four intervention strategies. "Analyze-then-Query" (free-form hypothesis verbalization) outperforms structured functional intervention, suggesting reverse-engineering differs from formal reasoning tasks.

- Failure signatures:
  - **Overcomplication**: Hypothesis includes operations not implied by observations (e.g., sorting, extra constraints). Signature: hypothesis score penalized for complexity; performance gap largest at low complexity levels.
  - **Overlooking**: Hypothesis is generic, missing specific parameters or patterns. Signature: hypothesis score penalized for incomplete specification; performance gap largest at high complexity levels.
  - **Intervention-yoked degradation**: When LLM receives another model's intervention data passively, performance drops relative to self-intervention. This signals that the learning benefit is process-dependent, not data-dependent.

- First 3 experiments:
  1. **Baseline replication**: Run observation-only experiments (N = 2, 5, 10, 20, 50, 100) on all three black-box types with GPT-4o. Verify performance plateau against Bayesian oracle. This establishes the core deficit the paper identifies.
  2. **Intervention ablation**: Compare observation-intervention against intervention-yoked on 20 instances per black-box type. Confirm that query-generation process—not just data quality—drives improvement. This validates Mechanism 1.
  3. **Failure mode profiling**: For each black-box type, sample 20 failed examples (score < 2/10) from observation-only. Manually classify as overcomplication, overlooking, or other. Verify correlation with complexity levels per Figure 5. This establishes which failure mode dominates which domain.

## Open Questions the Paper Calls Out
None

## Limitations
- The findings are limited to three specific black-box domains that may not generalize to real-world scientific discovery
- LLM performance metrics depend on human-constructed evaluation criteria and LLM judges, which may not fully capture scientific validity
- The study uses GPT-4o as the sole LLM implementation, limiting generalizability to other model families

## Confidence
- **High Confidence**: The core finding that active intervention substantially improves LLM reverse-engineering performance compared to passive observation is well-supported by multiple experiments across all three domains
- **Medium Confidence**: The failure mode analysis (overcomplication vs. overlooking) is based on manual classification of failed examples and may be subjective
- **Low Confidence**: The comparison to Bayesian inference models as "optimal" baselines assumes these models capture the true posterior distribution over hypotheses

## Next Checks
1. **Domain Generalization Test**: Apply the same observation-intervention framework to a fourth black-box domain (e.g., cellular automata or physical simulations) to verify whether the active learning benefit generalizes beyond the three studied domains.

2. **Mechanistic Validation**: Conduct a within-subjects experiment where the same LLM instance performs both observation-only and observation-intervention tasks on identical black-box instances, with detailed logging of hypothesis evolution to directly observe the belief-updating mechanism.

3. **Cost-Benefit Analysis**: Measure the performance-cost tradeoff across different intervention budgets and compare against alternative active learning strategies (e.g., uncertainty sampling, query-by-committee) to determine the most efficient approach for different black-box complexities.