---
ver: rpa2
title: 'SoundBreak: A Systematic Study of Audio-Only Adversarial Attacks on Trimodal
  Models'
arxiv_id: '2601.16231'
source_url: https://arxiv.org/abs/2601.16231
tags:
- attack
- audio
- attacks
- attention
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SOUNDBREAK investigates audio-only adversarial attacks on trimodal
  audio-video-language models, a previously underexplored threat model. The study
  proposes six complementary attack objectives targeting different stages of multimodal
  processing, including encoder representations, attention mechanisms, and hidden
  states.
---

# SoundBreak: A Systematic Study of Audio-Only Adversarial Attacks on Trimodal Models

## Quick Facts
- **arXiv ID**: 2601.16231
- **Source URL**: https://arxiv.org/abs/2601.16231
- **Reference count**: 40
- **Primary result**: Audio-only adversarial perturbations can induce severe multimodal failures in trimodal models, achieving up to 96% attack success rate with low perceptual distortion.

## Executive Summary
SOUNDBREAK systematically investigates audio-only adversarial attacks on trimodal audio-video-language models, a previously underexplored threat model. The study proposes six complementary attack objectives targeting different stages of multimodal processing, including encoder representations, attention mechanisms, and hidden states. Experiments across three state-of-the-art models and multiple benchmarks demonstrate that audio-only perturbations can cause severe multimodal failures while maintaining low perceptual distortion. The findings reveal that audio encoder representations constitute the primary vulnerability bottleneck, with attacks achieving high success rates (up to 96%) at perceptually imperceptible levels (LPIPS ≤0.08).

## Method Summary
The study employs Projected Gradient Descent (PGD) to optimize audio perturbations under ℓ∞ constraints, using six complementary loss functions targeting different attack surfaces: encoder representations (L_cos), negative log-likelihood (L_negLM), attention manipulation (visionatt, audioatt, randatt), and hidden states (L_hidden-cos). Attacks are evaluated on trimodal models including VideoLLAMA2, Qwen2.5 Omni, and PANN using benchmarks A VQA, Music-A VQA, and A VSD. The methodology involves mel-filterbank extraction, cyclic perturbation extension, and careful gradient projection, with extensive ablation studies examining attack budgets, optimization budgets, and transferability across models and encoders.

## Key Results
- Audio-only perturbations achieve up to 96% attack success rate across trimodal models while maintaining low perceptual distortion (LPIPS ≤0.08, SI-SNR ≥0)
- Encoder-space attacks (L_cos) consistently outperform other objectives, achieving 89.12% ASR versus 18.72% for attention-based attacks
- Extended optimization (150 epochs) proves more effective than increased data scale, with budget 1.0 achieving 73.5% ASR versus 51.9% at budget 0.7
- Transferability across models and encoders remains limited (<5% ASR), indicating attacks exploit model-specific vulnerabilities
- Speech recognition systems like Whisper primarily respond to perturbation magnitude, achieving >97% attack success under severe distortion

## Why This Works (Mechanism)

### Mechanism 1
Audio encoder representations constitute the primary vulnerability bottleneck in trimodal models. Small directional shifts in encoder embedding space propagate through downstream cross-modal fusion, corrupting reasoning without requiring output-level supervision. This works because encoder representations establish the foundation for all subsequent multimodal processing; corrupting them early induces cascading failures. The core assumption is that encoder representations are the critical integration point for all downstream processing. Evidence shows L(cos) achieves 89.12% ASR on A VQA, substantially exceeding attention-based and hidden-state objectives. This mechanism would weaken if encoder representations were isolated from downstream fusion through modality-specific preprocessing.

### Mechanism 2
Attention manipulation can redirect cross-modal grounding by forcing over-reliance on the perturbed audio channel. Minimizing attention allocated to vision tokens while maximizing attention to audio tokens creates a compounding effect—the model both receives corrupted audio and is compelled to weight it heavily. This works because cross-modal attention is the primary mechanism for integrating multimodal evidence; manipulating it shifts reasoning without altering content. Evidence shows combined attacks increase audio attention from 9.01 to 29.23 while maintaining video attention around 13-14. This mechanism would be less effective if attention patterns were decoupled from output generation through attention regularization during training.

### Mechanism 3
Lower transformer layers (1-10) capture audio-specific vulnerabilities that diminish in higher layers. Early layers process raw audio embeddings before cross-modal integration; perturbations here affect the entire downstream computation pipeline. This works because layer-wise vulnerability is non-uniform due to different layers specializing in different processing stages. Evidence shows audio attention attacks achieve 39.75% ASR on layers 1-10 versus 2.04% on layers 19-28. This mechanism would differ if architectures used uniform layer processing or early modality fusion.

## Foundational Learning

- **Projected Gradient Descent (PGD)**: Iterative gradient optimization with ℓ∞ constraint projection. Needed because all attacks optimize perturbations through iterative gradient updates with constraint enforcement. Quick check: Given a loss L and constraint set C, what is the projection operation Π_C?

- **LPIPS for audio**: Perceptual similarity metric adapted from images to log-mel spectrograms. Needed because the paper uses LPIPS on log-mel spectrograms to measure perceptual distortion of adversarial audio. Quick check: Why use LPIPS instead of MSE for measuring audio perturbation perceptibility?

- **Cross-modal attention**: Transformer mechanism for integrating multimodal evidence. Needed because attacks manipulate attention weights between audio, video, and text tokens to affect reasoning. Quick check: In a multimodal transformer, how do attention weights determine which modality influences the output?

## Architecture Onboarding

- **Component map**: Audio input → mel-filterbank extraction (128-dim) → audio encoder (BEATs/Whisper-style) → projection layer → concatenated tokens → cross-modal attention (28 layers) → LLM backbone (Qwen2-7B-Instruct) → answer distribution

- **Critical path**: Audio input → mel-filterbank extraction → audio encoder → projection layer → concatenated tokens → cross-modal attention → LLM → output

- **Design tradeoffs**: Encoder-space attacks (L_cos) offer high ASR (89%) with low distortion (LPIPS 0.08) but low transferability across encoders. Combined loss achieves highest ASR (96%) with moderate distortion (LPIPS 0.14) but requires more compute. Extended optimization proves more effective than larger datasets.

- **Failure signatures**: High attack success with maintained confidence (0.93 clean → 0.85 adversarial) indicates internal misalignment rather than uncertainty. Non-monotonic budget effects show budget 1.0 achieving 73.5% ASR versus budget 0.7 at 51.9%, indicating constraint space geometry matters. Low cross-model transfer (<5% ASR) suggests attacks exploit encoder-specific geometries.

- **First 3 experiments**:
  1. Replicate L(cos) attack on VideoLLAMA2 with ε=1.0, 150 epochs, natural audio initialization; measure ASR on A VQA validation set
  2. Analyze layer-wise attention patterns under clean vs. L(audioatt) attack to verify early-layer concentration
  3. Test transferability: Train L(cos) on VideoLLAMA2 encoder, evaluate cosine similarity on Qwen 2.5 Omni and PANN encoders (expect: 0.75, 0.62 vs 0.50 on source)

## Open Questions the Paper Calls Out

1. **Physical-world robustness**: How effective are audio-only adversarial perturbations under real-world conditions including over-the-air propagation, room reverberation, environmental noise, and lossy compression? The paper notes these factors "require further investigation" as all experiments apply perturbations digitally to waveforms without physical-world testing.

2. **Defense mechanisms**: Can cross-modal consistency monitoring, input preprocessing, or adversarial training effectively defend against audio-only adversarial attacks while maintaining model utility? The paper states findings "motivate defenses that enforce cross-modal consistency" but explicitly notes no defensive mechanisms were evaluated.

3. **Optimization landscape properties**: Why does attack effectiveness vary non-monotonically with perturbation budget, and what optimization landscape properties cause this behavior? The paper documents this phenomenon but doesn't explain the underlying cause, whether local optima, gradient dynamics, or feature space geometry.

4. **Architectural vulnerability sources**: What architectural or representational properties determine why encoder-space attacks substantially outperform attention-based and hidden-state attacks? The paper demonstrates this dominance empirically without theoretical or mechanistic explanation.

## Limitations
- All experiments apply perturbations digitally to waveforms without testing physical-world conditions including over-the-air propagation and environmental interference
- No defensive mechanisms were evaluated despite the paper's findings motivating cross-modal consistency defenses
- The study focuses exclusively on untargeted attacks without exploring targeted variants that might reveal different failure modes
- Limited cross-model transferability suggests attacks exploit model-specific encoder geometries rather than universal vulnerabilities

## Confidence
- **High confidence**: Encoder-space attacks (L_cos) consistently outperform other objectives across models and tasks, with ASR reaching 89.12% and perceptual distortion remaining low (LPIPS ≤0.08). Extended optimization outperforming larger datasets is well-supported.
- **Medium confidence**: Attention manipulation mechanisms show measurable effects on cross-modal grounding, with combined attacks achieving 96% ASR, though lacking direct corpus validation in trimodal settings.
- **Low confidence**: Layer-wise vulnerability claims have limited external validation since no corpus papers examine layer-wise vulnerability in audio-visual models. Whisper-specific findings may reflect model-specific sensitivity rather than generalizable principles.

## Next Checks
1. Compute cosine similarity between clean and adversarial embeddings across different audio encoders (BEATs, Qwen, PANN) to quantify model-specific attack surfaces and explain low transferability.

2. Generate heatmaps of cross-modal attention distributions under clean vs. L_audioatt attack conditions to empirically verify the mechanism of audio-over-reliance and vision-suppression.

3. Systematically evaluate attacks across model pairs (VideoLLAMA2→Qwen, Qwen→VideoLLAMA2) with varying perturbation magnitudes to establish transfer bounds and identify universal vs. model-specific attack components.