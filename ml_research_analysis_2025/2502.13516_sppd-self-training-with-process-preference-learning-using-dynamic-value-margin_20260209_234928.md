---
ver: rpa2
title: 'SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin'
arxiv_id: '2502.13516'
source_url: https://arxiv.org/abs/2502.13516
tags:
- step
- sppd
- arxiv
- preference
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving numerical and logical
  reasoning capabilities in Large Language Models (LLMs), which existing methods struggle
  with due to limitations in step-wise correctness, dependence on stronger models
  for distillation, and high computational costs. The proposed method, SPPD (Self-training
  with Process Preference Learning using Dynamic Value Margin), introduces a novel
  approach that leverages a process-based Markov Decision Process (MDP) and Bellman
  optimality equation to derive dynamic value margins for step-level preference optimization.
---

# SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin

## Quick Facts
- arXiv ID: 2502.13516
- Source URL: https://arxiv.org/abs/2502.13516
- Authors: Hao Yi; Qingyang Li; Yulan Hu; Fuzheng Zhang; Di Zhang; Yong Liu
- Reference count: 19
- One-line primary result: Improves mathematical reasoning in 7B-scale LLMs using self-training without distillation, achieving state-of-the-art performance on benchmarks.

## Executive Summary
This paper addresses the challenge of improving numerical and logical reasoning capabilities in Large Language Models (LLMs), which existing methods struggle with due to limitations in step-wise correctness, dependence on stronger models for distillation, and high computational costs. The proposed method, SPPD (Self-training with Process Preference Learning using Dynamic Value Margin), introduces a novel approach that leverages a process-based Markov Decision Process (MDP) and Bellman optimality equation to derive dynamic value margins for step-level preference optimization. Instead of relying on distillation from stronger models, SPPD employs tree-based self-sampling on the model's own responses to generate preference pairs. Experiments on 7B-scale models demonstrate significant improvements across in-domain and out-domain mathematical benchmarks, achieving state-of-the-art performance without requiring distillation from stronger models.

## Method Summary
SPPD is a three-stage pipeline that improves mathematical reasoning in LLMs without distillation. First, it generates a dataset of reasoning trajectories using tree-based self-sampling, where the model explores its own responses with a branching factor of C=2 and K=64 trajectories per problem. Second, it trains a Process Reward Model (PRM) to score each reasoning step. Third, it optimizes the policy model using a modified DPO loss that incorporates a dynamic value margin derived from the PRM scores, following the Bellman optimality equation. The method theoretically proves equivalence to on-policy policy gradient methods and demonstrates strong empirical performance on mathematical reasoning benchmarks.

## Key Results
- Achieves 79% accuracy on MATH500 and 94.7% on GSM8k using the ORM_VOTE@64 voting strategy
- Demonstrates state-of-the-art performance on 7B-scale models without requiring distillation from stronger models
- Shows robust generalization capabilities across in-domain and out-domain mathematical benchmarks
- Ablation study confirms the dynamic margin is the most critical component for performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPPD improves step-level reasoning by optimizing preferences with a dynamic margin that accounts for the future value difference between preferred and dispreferred reasoning paths.
- Mechanism: The framework defines a step-level Markov Decision Process (MDP). It derives an optimal reward function (Lemma 4.1) composed of an "implicit reward" from the policy and a "value gain." This value gain, the difference between the optimal value of the next state and the current state, is used as a dynamic margin in the step-level Bradley-Terry preference model (Theorem 4.2). This contrasts with standard DPO which implicitly uses a fixed margin of zero.
- Core assumption: The reasoning process can be modeled effectively as a Markov Decision Process, and a Process Reward Model (PRM) score can accurately approximate the optimal value function $V^*(s)$ required to calculate the dynamic margin.
- Evidence anchors:
  - [abstract] "SPPD leverages a process-based Markov Decision Process (MDP) and Bellman optimality equation to derive dynamic value margin on step-level preference optimization".
  - [section 4.1] Equation (3) and Theorem 4.2 detail the derivation of the dynamic margin term: $V^*(s^w_{t+1}) - V^*(s^l_{t+1})$.
  - [corpus] Corpus evidence for this specific "dynamic value margin" technique is weak; related papers like "Dual-Stage Value-Guided Inference" suggest growing interest in value-guided methods, but do not confirm this mechanism.
- Break condition: The mechanism's benefit may degrade if the PRM used to approximate $V^*$ is inaccurate or if the underlying process violates the MDP assumption (e.g., if state transitions are not Markovian).

### Mechanism 2
- Claim: The method achieves self-improvement without external supervision by generating its own training data through a tree-based sampling process.
- Mechanism: Instead of using a stronger model or human annotations, the model performs tree search over its own possible reasoning steps. This creates trajectories with common prefixes, which allows for meaningful step-level preference pairs (chosen vs. rejected) to be constructed based on their PRM scores.
- Core assumption: The policy model's own generation distribution, when explored via tree search, contains enough signal (via PRM scoring) to create high-quality preference pairs for self-distillation.
- Evidence anchors:
  - [abstract] "...employs tree-based self-sampling on model responses without any distillation from other models."
  - [section 4.2] Describes the four-step process: "Selection, Expansion, Collection and Scoring" to build the step-level dataset $D_{step}$.
  - [corpus] Weak connection. While some corpus papers mention self-training or preference optimization, none detail this specific tree-based self-sampling for step-level data generation.
- Break condition: Performance gains will plateau if the model's own generations are insufficiently diverse or consistently incorrect, leading to noisy or uninformative preference signals from the PRM.

### Mechanism 3
- Claim: The theoretical foundation shows that SPPD's offline optimization is equivalent to an on-policy policy gradient update.
- Mechanism: By defining a specific reward function and a "preference decoding model," the paper proves that the gradient of SPPD's loss function is mathematically equivalent to the gradient of the expected long-term reward in an online RL setting (Theorem 5.2). This provides a theoretical justification for why a static, offline dataset can produce strong, stable policy improvements.
- Core assumption: The theoretical equivalence relies on a specific reward definition, $r(\tau) = \prod_{i=1}^T \frac{\pi_{ref}(a_t|s_t)}{\pi^p_\theta(a_t|s_t)}$, which must hold.
- Evidence anchors:
  - [abstract] "...we theoretically prove that SPPD is equivalent to on-policy policy gradient methods under reward constraints."
  - [section 5] Theorem 5.2 states: $\nabla_\theta J(\theta) = -\nabla_\theta L_{every-step}$.
  - [corpus] The paper "Implicit Reward as the Bridge" presents a related unified view of SFT and DPO connections, supporting this line of theoretical analysis.
- Break condition: The theoretical equivalence may not translate to practical gains if the optimization process is unstable or if the assumptions for the proof do not hold in practice due to finite samples.

## Foundational Learning

- Concept: **Markov Decision Process (MDP)** and **Bellman Optimality Equation**
  - Why needed here: The core innovation is formulating reasoning as a step-level MDP. Understanding state transitions and value functions is essential to grasp how the "dynamic value margin" is derived.
  - Quick check question: In a step-level MDP for reasoning, what represents a *state* and what represents an *action*? (Answer: A state is the problem plus the sequence of reasoning steps so far; an action is the next reasoning step).

- Concept: **Bradley-Terry (BT) Preference Model**
  - Why needed here: The paper uses the BT model to define the probability of preferring one reasoning step over another based on their rewards. This is the foundation of the preference learning loss.
  - Quick check question: According to the BT model, what does the sigmoid function $\sigma(r(s, a^w) - r(s, a^l))$ represent? (Answer: The probability that action $a^w$ is preferred over action $a^l$ given state $s$).

- Concept: **KL Divergence and Reward Equivalence**
  - Why needed here: The method's loss function minimizes the KL divergence between preference distributions. The concept of reward equivalence is used to introduce the $\gamma$ scaling factor, making the optimization more controllable.
  - Quick check question: What is the role of the $\beta$ parameter in the optimization objective? (Answer: It acts as a KL penalty, preventing the policy model $\pi_\theta$ from deviating too far from the reference model $\pi_{ref}$).

## Architecture Onboarding

- Component map: Base Model -> Tree-Based Sampling -> PRM Scoring -> Preference Pair Construction -> SPPD Training
- Critical path: Base Model $\rightarrow$ Tree-Based Sampling $\rightarrow$ PRM Scoring $\rightarrow$ Preference Pair Construction $\rightarrow$ SPPD Training. The ablation study (Section 6.3) confirms that the performance gain is most sensitive to the dynamic margin, making the PRM scoring and the margin calculation the most critical steps for correctness.
- Design tradeoffs:
  - **Self-Sampling vs. Distillation**: The method trades the potentially higher quality of a stronger teacher model for the scalability and lack of dependency offered by self-generated data.
  - **Offline vs. Online RL**: SPPD is an offline method. It avoids the high GPU memory and instability of online RL (like PPO), but its effectiveness is bounded by the quality of the data it generates from its own policy.
- Failure signatures:
  - **No Improvement**: May occur if the PRM is a poor proxy for step correctness, leading to noisy preference labels. Monitor PRM accuracy on a held-out validation set.
  - **Training Instability**: Could arise from an improperly tuned $\gamma$ scaling factor in the dynamic margin term. The ablation study shows performance varies with $\gamma$.
  - **Overfitting**: The paper notes that using a minimum PRM score threshold (0.5) for pairs is crucial to mitigate noise and overfitting. A too-low threshold may cause the model to overfit to noisy preferences.
- First 3 experiments:
  1. **Ablation on Margin Strategy**: Compare SPPD (dynamic margin) against no-margin ($\gamma=0$) and fixed-margin step DPO. This validates the core mechanism (Table 4).
  2. **Varying the $\gamma$ Scaling Factor**: Run a parameter sweep for $\gamma \in \{0.1, 0.5, 1.0, 2.0, 5.0\}$ on a downstream task like MATH to find the optimal value (Figure 2).
  3. **Iterative Self-Training (Stage 2)**: After one round of SPPD, use the improved model to generate a new dataset and train again (SPPD-Stage2). This tests the robustness of the self-training loop (Table 1).

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the Process Reward Model (PRM) be dynamically updated or synchronized with the policy model to prevent the reward signal from becoming stale as the policy iterates? The authors explicitly state in the Limitations section that this work neglects PRM updates, creating a risk of the reward model becoming ineffective as the policy iterates.
- **Open Question 2**: How can the step-level Markov Decision Process (MDP) modeling used in SPPD be theoretically integrated with existing on-policy reinforcement learning methods like PPO and GRPO? The paper notes that both PPO and GRPO are modeled based on bandit, and integrating MDP modeling with on-policy methods remains an important subject for future research.
- **Open Question 3**: Is SPPD robust to scenarios where the Process Reward Model provides noisy or incorrect scores, and can it be adapted to tasks where step-level verification is difficult? The authors mention that some PRMs may fail under specific tasks, but the method relies heavily on precise PRM scores to calculate the dynamic value margin.

## Limitations
- The method's effectiveness is heavily dependent on the quality of the Process Reward Model (PRM), which may not generalize well to non-mathematical reasoning tasks.
- The computational overhead of tree-based self-sampling with K=64 trajectories may become prohibitive at larger scales.
- The long-term stability of iterative self-training and the method's performance on cross-domain transfer tasks are not thoroughly explored.

## Confidence
- **High Confidence**: The empirical improvements on mathematical reasoning benchmarks (MATH, GSM8k) are well-documented and reproducible. The ablation study clearly demonstrates the contribution of the dynamic margin.
- **Medium Confidence**: The theoretical equivalence to on-policy policy gradient methods is mathematically sound under the stated assumptions, but its practical impact depends on the quality of the PRM and the stability of the optimization.
- **Low Confidence**: The long-term stability of iterative self-training (Stage 2+) and the method's performance on non-mathematical reasoning tasks are not thoroughly explored.

## Next Checks
1. **PRM Quality Validation**: Before full training, evaluate the PRM's accuracy in ranking correct vs. incorrect reasoning steps on a held-out validation set. If the PRM's accuracy is below 80%, investigate alternative PRM architectures or training data.
2. **Dynamic Margin Sensitivity Analysis**: Extend the $\gamma$ parameter sweep beyond the reported range (e.g., $\gamma \in \{0.05, 0.2, 0.8, 1.5, 3.0, 10.0\}$) on a downstream task like MATH to identify the optimal value and confirm the robustness of the reported results.
3. **Cross-Domain Transfer Test**: Apply SPPD to a different reasoning domain (e.g., commonsense reasoning with StrategyQA or ARC) using the same training pipeline. This will test whether the MDP-based dynamic margin is a general mechanism or specific to mathematical reasoning.