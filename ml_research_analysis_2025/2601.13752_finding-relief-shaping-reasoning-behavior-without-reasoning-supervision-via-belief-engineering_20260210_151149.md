---
ver: rpa2
title: 'Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via
  Belief Engineering'
arxiv_id: '2601.13752'
source_url: https://arxiv.org/abs/2601.13752
tags:
- reasoning
- belief
- relief
- hint
- faithfulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RELIEF introduces a novel approach to shaping Large Reasoning Models
  (LRMs) by targeting their internal reasoning beliefs rather than relying on explicit
  reasoning trace supervision. The method leverages the observation that LRMs maintain
  latent beliefs about their own reasoning traits, which can be probed and systematically
  shifted.
---

# Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering

## Quick Facts
- arXiv ID: 2601.13752
- Source URL: https://arxiv.org/abs/2601.13752
- Reference count: 40
- Primary result: Achieves LRM behavior shaping via belief engineering, matching baselines with 75% fewer training tokens

## Executive Summary
RELIEF introduces a novel approach to shaping Large Reasoning Models (LRMs) by targeting their internal reasoning beliefs rather than relying on explicit reasoning trace supervision. The method leverages the observation that LRMs maintain latent beliefs about their own reasoning traits, which can be probed and systematically shifted. RELIEF shapes behavior by aligning the model's self-concept with a target belief blueprint through fine-tuning on synthesized, self-reflective question-answering pairs, bypassing the need for costly reasoning traces.

Evaluated across efficiency and faithfulness dimensions, RELIEF achieves performance on par with behavior-supervised and preference-based baselines while using significantly fewer training tokens (around 0.4M vs. 3.9M-8.2M). On efficiency tasks, it reduces reasoning length by up to 71% without sacrificing accuracy, outperforming trace-based methods in both compression and stability. On faithfulness tasks, it consistently improves honesty in attributing decisions to external hints. Analysis confirms that RELIEF induces genuine shifts in the model's reasoning beliefs, resulting in structural changes to reasoning patterns and increased meta-cognitive vocabulary. The approach offers a scalable, data-efficient alternative for controlling LRM behavior by targeting internal beliefs rather than surface-level behaviors.

## Method Summary
RELIEF operates through a three-phase pipeline: (1) expand a seed belief into a detailed blueprint using a teacher LLM, (2) synthesize 1,000 diverse self-reflective QA pairs affirming the target belief, and (3) perform supervised fine-tuning on these pairs with empty reasoning blocks. The method uses logit probing to measure latent reasoning beliefs via contrastive token groups, correlating these beliefs with external metrics. During training, the model learns to affirm the target belief through self-descriptive statements, which then acts as a prior that biases actual reasoning behavior. The approach uses LoRA fine-tuning with 0.4M training tokens, significantly less than behavior-supervised or preference-based alternatives.

## Key Results
- Reduces reasoning token count by up to 71% while maintaining accuracy on GSM8K and MATH-500
- Improves faithfulness (honest hint attribution) by 5-10 points on MMLU-Redux and GPQA-Diamond
- Uses 75% fewer training tokens than behavior-supervised baselines (0.4M vs 3.9M-8.2M tokens)

## Why This Works (Mechanism)

### Mechanism 1: Latent Reasoning Belief Probing
- Claim: LRMs maintain internal beliefs about their reasoning traits that can be extracted via logit analysis.
- Mechanism: A self-reflective prompt ("I consider my previous reasoning to be ___") is appended to reasoning traces. Logit differences between contrastive token groups capture the model's self-assessment, correlating with external metrics.
- Core assumption: The model's next-token distribution at the final position reflects a coherent internal belief state, not merely surface-level pattern matching.
- Evidence anchors: [Section 2]: "LRMs' internal logits correlate remarkably with external measurements of their reasoning traits"; [Figure 1]: Logit differences systematically track token count and faithfulness scores.
- Break condition: If probed beliefs do not correlate with external metrics across multiple reasoning dimensions, the premise fails.

### Mechanism 2: Belief Internalization via Self-Reflective SFT
- Claim: Fine-tuning on synthesized QA pairs where the model affirms target beliefs shifts its internal self-concept.
- Mechanism: A target belief blueprint is expanded by a teacher LLM. QA pairs are synthesized where the model discusses its reasoning style without generating actual CoT. Standard SFT on these pairs internalizes the belief.
- Core assumption: Self-descriptive statements about reasoning generalize to shape actual reasoning behavior, even without reasoning trace supervision.
- Evidence anchors: [Section 3]: "The internalized belief then acts as a prior that biases the model's actual reasoning traces"; [Section 5.3]: Post-training logit analysis shows large belief shifts (Δ up to 12.4 for efficient target).
- Break condition: If internalized beliefs do not manifest in actual reasoning traces (no structural change in patterns or vocabulary), the transfer fails.

### Mechanism 3: Belief-to-Behavior Translation via Structural Reasoning Shifts
- Claim: Shifted beliefs induce qualitative changes in reasoning structure, not just output length.
- Mechanism: Internalized beliefs alter rhetorical style distribution (reduced "Exploration" markers, increased "Directness") and vocabulary (more meta-cognitive terms like "honest," "metadata"). This structural shift propagates through inference.
- Core assumption: The model's internal belief state influences token-by-token generation dynamics beyond explicit prompting effects.
- Evidence anchors: [Section 5.2]: "RELIEF structurally alters reasoning style by prioritizing Directness over Exploration"; [Figure 7-8]: Rhetorical style and vocabulary shifts differentiate RELIEF from SimPO.
- Break condition: If behavioral changes are explained entirely by format bias or SFT length artifacts, the belief mechanism is not causal.

## Foundational Learning

- **Concept: Logit Probing with Contrastive Token Groups**
  - Why needed here: The method requires measuring latent beliefs via logit differences (Eq. 1); understanding aggregation over token sets is essential.
  - Quick check question: Given V+ = {concise, brief} and V- = {verbose, lengthy}, what does a positive Δ score indicate?

- **Concept: Supervised Fine-Tuning with Empty Reasoning Context**
  - Why needed here: RELIEF trains on [Xi, 画作] → Yi where 画作 is an empty CoT block; this differs from standard instruction tuning.
  - Quick check question: Why might training with empty reasoning blocks help transfer belief to actual reasoning traces?

- **Concept: Reasoning Faithfulness vs. Rationalization**
  - Why needed here: The faithfulness task requires distinguishing honest hint attribution from post-hoc logical fabrication.
  - Quick check question: If a model acknowledges a hint at the end of reasoning but derived the answer independently first, is this faithful per the paper's rubric?

## Architecture Onboarding

- **Component map**: Seed belief → Blueprint expansion → QA synthesis → SFT with empty reasoning block → Belief probe validation
- **Critical path**: Seed belief → Blueprint expansion → QA synthesis (N=1000, diverse tones/domains) → SFT with empty reasoning block → Belief probe validation on held-out set
- **Design tradeoffs**:
  - SFT vs. RL: SFT chosen for efficiency; paper notes RL/preference optimization could reinforce beliefs further (unexplored)
  - Blueprint richness vs. synthesis complexity: More detailed blueprints yield better behavioral alignment but require more careful synthesis
  - Token budget: 0.4M tokens sufficient for behavioral shaping; unclear if scales to larger models or more complex traits
- **Failure signatures**:
  - "None" baseline: Plain SFT without belief injection shortens traces but loses accuracy (format bias artifact)
  - SimPO collapse: Preference optimization on faithfulness pairs can cause infinite repetition if length and faithfulness are confounded
  - Unidirectional control: Qwen3-8B resists length expansion (already RL-maximized), suggesting capability ceiling
- **First 3 experiments**:
  1. Replicate efficiency probing on your target LRM: append self-reflective template, compute Δ across reasoning traces of varying lengths, verify correlation with token count
  2. Run control analysis: train with "Efficient," "Inefficient," and "None" targets on same data, compare Δ shifts and Pass@1 to isolate belief vs. format effects
  3. Test cross-domain transfer: synthesize QA pairs in one domain (math), evaluate belief shaping on another (code reasoning) to probe generalization bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the efficacy of belief engineering transfer to significantly larger models (beyond 8B parameters) and open-ended generation domains?
- Basis in paper: [explicit] The authors state in the Limitations section: "our experiments were restricted to 7B-8B parameter models... whether these findings transfer to significantly larger models or open-ended generation domains requires further verification."
- Why unresolved: The paper only validates the approach on smaller, specific reasoning models (Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B) using mathematical and multiple-choice benchmarks. It is unknown if latent beliefs are as easily probe-able or steerable in larger architectures with different training distributions.
- What evidence would resolve it: Successful application of RELIEF on 70B+ parameter models and open-ended creative writing tasks, showing consistent behavioral shifts without capability degradation.

### Open Question 2
- Question: Can reinforcement learning or preference optimization strengthen internalized beliefs more effectively than standard supervised fine-tuning (SFT)?
- Basis in paper: [explicit] The authors note: "we rely exclusively on SFT for internalization of belief... we have not explored whether reinforcement learning or preference optimization could further reinforce these beliefs."
- Why unresolved: The current method uses SFT on synthetic QA pairs, but it is untested whether using RL to maximize the belief probe score (Δ) directly would result in deeper internalization or better stability compared to the current proxy method.
- What evidence would resolve it: A comparative study where models are trained using RL based on the belief metric itself versus the current SFT approach, measuring the durability and strength of the behavioral shift.

### Open Question 3
- Question: Can belief engineering enhance intrinsic capabilities (e.g., accuracy), or is it strictly limited to modulating behavioral styles?
- Basis in paper: [explicit] The authors hypothesize a boundary: "we do not expect it to improve intrinsic capabilities; for instance, shaping a belief of 'being accurate' is unlikely to spontaneously improve mathematical reasoning ability if the underlying knowledge is absent."
- Why unresolved: This is a theoretical claim based on the nature of "style" vs. "competence." It remains unverified if a strong "accuracy" belief could act as a retrieval cue or consistency check that actually improves Pass@1 scores on knowledge-heavy tasks.
- What evidence would resolve it: Experiments applying a "competence-oriented" belief blueprint (e.g., "I am highly accurate") to a model with sufficient but latent knowledge, testing for performance gains over a neutral baseline.

## Limitations

- Limited to 7B-8B parameter models; scalability to larger architectures untested
- Only validates two specific behaviors (efficiency, faithfulness) on mathematical reasoning tasks
- Theoretical framework of "reasoning beliefs" lacks mechanistic validation beyond behavioral observation

## Confidence

- **High confidence**: The efficiency results (71% reduction in reasoning tokens while maintaining accuracy) and the faithfulness improvements are empirically well-supported. The probing method shows stable correlations, and the SFT procedure is clearly specified.
- **Medium confidence**: The claim that belief shifts are "genuine" and causally drive behavioral change. While logit analysis shows large belief score changes and structural reasoning shifts are observed, alternative explanations cannot be fully excluded without ablation studies.
- **Low confidence**: The broader theoretical framework of "reasoning beliefs" as a latent construct in LRMs. The paper provides correlational evidence but lacks mechanistic validation or cross-domain generalization studies.

## Next Checks

1. **Cross-domain belief transfer test**: Synthesize QA pairs in one domain (e.g., mathematical reasoning) and evaluate belief shaping on a different domain (e.g., code reasoning or commonsense QA). This will test whether the belief mechanism generalizes beyond the training distribution or is task-specific.

2. **Ablation on synthesis quality**: Compare RELIEF trained on high-quality vs. lower-quality self-reflective QA pairs (e.g., using a weaker teacher model or adversarial filtering). If belief scores and behavioral shifts drop significantly with lower-quality data, it would confirm the importance of synthesis fidelity.

3. **Long-term stability analysis**: Evaluate the persistence of belief shifts over multiple inference sessions and fine-tuning-free adaptation tasks. If the internalized beliefs degrade or revert to baseline behavior over time or with exposure to diverse reasoning contexts, it would question the robustness of the approach.