---
ver: rpa2
title: 'Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety
  Moderation Classifiers'
arxiv_id: '2501.13302'
source_url: https://arxiv.org/abs/2501.13302
tags:
- fairness
- moderation
- openai
- unsafe
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates fairness and robustness of four closed-source
  AI Safety Moderation (ASM) classifiers used for content moderation and LLM safety
  guardrails. The study finds that OpenAI Moderation API exhibits the highest fairness
  issues, particularly for sexual orientation, while Google Cloud Natural Language
  API performs most fairly.
---

# Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers

## Quick Facts
- **arXiv ID**: 2501.13302
- **Source URL**: https://arxiv.org/abs/2501.13302
- **Reference count**: 27
- **Primary result**: This work evaluates fairness and robustness of four closed-source AI Safety Moderation (ASM) classifiers used for content moderation and LLM safety guardrails. The study finds that OpenAI Moderation API exhibits the highest fairness issues, particularly for sexual orientation, while Google Cloud Natural Language API performs most fairly. The robustness analysis reveals that LLM-based input perturbations significantly reduce moderation effectiveness, with OpenAI API showing the highest vulnerability. Small changes in input text can bypass ASM filters, potentially allowing unsafe content to evade detection. The findings highlight critical gaps in fairness and robustness that need to be addressed in future ASM model development.

## Executive Summary
This study provides a comprehensive black-box audit of four leading commercial AI Safety Moderation classifiers, revealing significant fairness disparities and robustness vulnerabilities. The researchers found that all evaluated models exhibit fairness issues, with sexual orientation groups facing the highest bias rates, particularly in OpenAI's moderation system. The robustness analysis demonstrates that simple LLM-based paraphrasing can bypass moderation filters in 20-32% of cases, converting unsafe content to safe classifications while preserving harmful semantic content. The Google Cloud Natural Language API emerged as the most fair-performing model, though it exhibits higher false positive rates, suggesting inherent tradeoffs between fairness, robustness, and accuracy in content moderation systems.

## Method Summary
The researchers conducted a black-box audit of four closed-source ASM classifiers (OpenAI Moderation API, Perspective API, Google Cloud Natural Language API, and Clarifai API) using input-output analysis. They evaluated fairness through demographic parity and conditional statistical parity metrics across Jigsaw Toxicity dataset subsets (gender, ethnicity, disability, sexual orientation) and a custom Reddit-Ideology dataset. Robustness was tested using two perturbation strategies: backtranslation via nlpaug library and LLM-based paraphrasing with GPT-3.5-Turbo. The study also employed a BERT regard classifier to control for legitimate sentiment differences when computing conditional statistical parity. All evaluations used a 0.5 threshold for binary classification, with runtime and API-specific batching strategies documented.

## Key Results
- OpenAI Moderation API exhibits the highest fairness issues, particularly for sexual orientation groups with significantly higher unsafe classification rates
- Google Cloud Natural Language API demonstrates the most fair performance across all protected groups, though with higher overall false positive rates
- LLM-based input perturbations can bypass moderation filters, converting unsafe to safe classifications in 20-32% of cases while preserving harmful semantic content
- The robustness bypass rates vary by ASM model, with OpenAI showing the highest vulnerability to adversarial perturbations
- Threshold selection significantly impacts fairness metrics, with 0.7 threshold improving Perspective API fairness but worsening GCNL API performance

## Why This Works (Mechanism)

### Mechanism 1: Group-Dependent Classification Disparities
ASM classifiers exhibit measurable fairness gaps where content associated with minority protected groups receives higher unsafe classification rates compared to majority groups, with sexual orientation showing the largest disparities. This occurs because ASM models trained on web-scale data internalize corpus-level associations between demographic terms and toxicity labels. When input text contains identity markers (e.g., "homosexual," "transgender"), the model's learned representations activate toxicity-associated features even when the semantic content is identical to majority-group examples. The GCNL API's better fairness performance likely stems from its broader label taxonomy (16 categories) distributing classification pressure across more dimensions, reducing overfitting to specific identity-toxicity correlations.

### Mechanism 2: Semantic Perturbation Bypass via Lexical Substitution
LLM-based paraphrasing converts unsafe classifications to safe at rates of 20-32% while preserving harmful semantic content, exploiting ASM models' reliance on surface-level lexical cues rather than deep semantic understanding. ASM classifiers disproportionately weight specific lexical tokens (profanity, slurs, explicit threat language) as toxicity signals. GPT-3.5-Turbo paraphrasing substitutes flagged terms with semantically equivalent but lexically distinct alternatives (e.g., "NASTY WOMAN" → "fierce woman"; "bad man" → "not a good person"), reducing activation of toxicity-associated features without altering underlying harmful intent. Backtranslation shows lower bypass rates (1-6%) because it preserves more lexical overlap with original text.

### Mechanism 3: Fairness-Robustness-Accuracy Tradeoff Surface
GCNL API achieves superior fairness metrics but at the cost of elevated false positive rates, while OpenAI shows the inverse pattern—revealing an inherent tradeoff where stricter moderation improves robustness but exacerbates fairness disparities. GCNL's PaLM2-based architecture with 16 moderation categories applies a broader definition of "unsafe," flagging more content overall. This higher baseline unsafe rate reduces relative disparities between groups (if both majority and minority content is flagged more often, the ratio converges). Conversely, OpenAI's more targeted moderation creates sharper decision boundaries that amplify subtle biases in the training distribution.

## Foundational Learning

- **Concept: Demographic Parity and Conditional Statistical Parity**
  - **Why needed here:** These metrics quantify fairness by measuring whether model outcomes are independent of protected group membership. DP measures raw disparity; CSP controls for legitimate factors (here, sentiment via BERT regard classification) to distinguish valid from biased classification differences.
  - **Quick check question:** If a model flags 30% of comments mentioning "gay" as unsafe but only 10% of comments mentioning "straight," does this violate demographic parity? What additional information would CSP require?

- **Concept: Black-Box Auditing via Input Perturbation**
  - **Why needed here:** Closed-source ASM models provide only input-output access, preventing gradient-based or internal-state analysis. Perturbation-based auditing infers robustness properties by observing output changes under controlled input modifications, treating the model as an opaque function.
  - **Quick check question:** Why is backtranslation a weaker robustness probe than LLM-based paraphrasing for this audit? What does the difference in bypass rates reveal about the ASM models' decision boundaries?

- **Concept: Semantic Preservation in Adversarial Perturbations**
  - **Why needed here:** Not all perturbations are valid robustness tests—changing "hate speech" to "love speech" trivially changes classification but doesn't reveal a model flaw. Valid perturbations must preserve harmful intent while altering surface features, requiring either human verification or semantic similarity metrics.
  - **Quick check question:** The paper manually verified 300 perturbed samples. What automation approaches could scale this verification while maintaining confidence that semantic harm is preserved?

## Architecture Onboarding

- **Component map:**
  Input Text → [ASM APIs: OpenAI | Perspective | GCNL | Clarifai]
                    ↓
              Score outputs (per-label probabilities)
                    ↓
              Threshold Application (0.5 or 0.7)
                    ↓
              Binary Classification (Safe/Unsafe)
                    ↓
         [Fairness Metrics: DP, CSP]  [Robustness: f_robust]

- **Critical path:**
  1. Dataset preparation with protected attribute labels (Jigsaw subsets, custom Reddit collection)
  2. Batch API calls with rate limiting (Perspective/GCNL slowest at ~24k seconds for full evaluation)
  3. Score aggregation and threshold application
  4. Fairness metric computation stratified by protected group
  5. Perturbation generation and before/after classification comparison

- **Design tradeoffs:**
  - **GCNL vs. OpenAI:** GCNL offers better fairness but higher false positives (over-moderation risk); OpenAI provides targeted moderation with fairness gaps
  - **Threshold selection:** 0.5 enables cross-API comparison but may not reflect production thresholds; 0.7 recommended for Perspective but worsens GCNL fairness
  - **Perturbation strategy:** LLM-based more effective for robustness testing but requires cost management; backtranslation cheaper but less diagnostic

- **Failure signatures:**
  - High DP/CSP values for sexual orientation subset → model has internalized corpus-level identity-toxicity correlations
  - Asymmetric bypass rates (unsafe→safe higher than safe→unsafe) → model relies on lexical triggers rather than semantic understanding
  - Large threshold-dependent fairness swings → decision boundary positioned near cluster of identity-associated content
  - Runtime exceeding 20k seconds → batch size limitations or rate limiting (use Clarifai's 128-batch for faster iteration)

- **First 3 experiments:**
  1. **Baseline fairness profiling:** Run all four ASM APIs on the Jigsaw-Gender subset with 0.5 threshold, compute DP and CSP. Compare against the Always Fair random baseline to establish unfairness magnitude. Expected: OpenAI > Clarifai > Perspective > GCNL in DP error.
  2. **Robustness stress test:** Apply GPT-3.5-Turbo paraphrasing to 100 samples classified as "unsafe" by OpenAI Moderation API. Measure conversion rate to "safe." Manually verify semantic preservation on 20 random samples. Expected: 20-32% conversion rate.
  3. **Threshold sensitivity analysis:** For Perspective API, compare fairness metrics at 0.5 vs. 0.7 thresholds on Jigsaw-Sexual_Orientation subset. Identify whether threshold tuning can mitigate fairness gaps without unacceptable accuracy loss.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do fairness metrics fluctuate when evaluating ASM models on datasets featuring intersecting protected attributes (e.g., gender combined with sexual orientation)?
  - **Basis in paper:** [explicit] The "Discussion" section states: "Future research in this direction can focus on larger scale intersectional studies on ASM fairness."
  - **Why unresolved:** The current study primarily isolated single protected attributes to analyze individual effects, noting only in the appendix that intersectional analysis introduces complex bias interactions.
  - **What evidence would resolve it:** Evaluation results from large-scale datasets annotated for multiple demographic attributes simultaneously, showing demographic parity across intersectional subgroups.

- **Open Question 2:** Do the observed fairness disparities and robustness vulnerabilities persist in low-resource languages and multimodal content (e.g., images/video)?
  - **Basis in paper:** [explicit] The "Limitations" section notes: "It is of paramount importance to consider low-resource languages and specialized domains in future work" and suggests "future work can consider fairness for multimodal data."
  - **Why unresolved:** The experimental scope was restricted to English textual data, leaving the performance of ASM models in other languages and data modalities unknown.
  - **What evidence would resolve it:** Benchmark results replicating the fairness and robustness methodology on non-English corpora and multimodal datasets.

- **Open Question 3:** To what extent can sophisticated adversarial attacks (e.g., AutoDAN) exploit the identified sensitivity to perturbations to systematically bypass ASM guardrails?
  - **Basis in paper:** [explicit] The "Discussion" section highlights that the observed robustness issues "open up possibilities for adversarial attacks such as AutoDAN (Liu et al., 2023) and persuasively adversarial prompts (PAP)."
  - **Why unresolved:** The study relied on GPT-3.5 Turbo paraphrasing for robustness testing rather than optimized, malicious adversarial strategies.
  - **What evidence would resolve it:** Success rates of automated adversarial attacks against the ASM classifiers compared to the paraphrasing baselines used in this study.

## Limitations

- The study relies on black-box API access without visibility into model architectures, training data, or decision boundaries, limiting mechanistic explanations for observed fairness disparities and robustness vulnerabilities.
- Perturbation effectiveness assumes semantic preservation, but automated verification remains challenging at scale; manual validation of 300 samples may not capture distribution-wide behavior.
- Results are threshold-dependent; the 0.5 threshold enables cross-API comparison but may not reflect actual deployment settings, with substantial metric variations observed across thresholds.

## Confidence

- **High confidence:** ASM models exhibit measurable fairness gaps, particularly for sexual orientation groups; LLM-based perturbations can bypass moderation filters; GCNL API demonstrates superior fairness performance compared to other evaluated APIs.
- **Medium confidence:** The mechanism of lexical substitution enabling bypass is well-supported by qualitative examples and quantitative results, though broader semantic attack vectors may exist; the fairness-robustness-accuracy tradeoff surface is plausible given architectural differences but requires more systematic exploration.
- **Low confidence:** Specific ranking of APIs by fairness/robustness may shift with different thresholds or deployment contexts; generalizability to non-English content and other protected attribute categories remains unverified.

## Next Checks

1. **Threshold sensitivity validation:** Replicate fairness and robustness experiments across multiple thresholds (0.3, 0.5, 0.7, 0.9) for all four ASM APIs to quantify ranking stability and identify optimal deployment points for different fairness/robustness priorities.
2. **Intersectional fairness analysis:** Extend the fairness evaluation to examine compound protected attributes (e.g., Black women, disabled LGBTQ+ individuals) using the original Jigsaw dataset with multi-label annotations to detect potentially hidden disparities.
3. **Adversarial perturbation scalability test:** Implement automated semantic preservation verification using sentence similarity metrics (e.g., BERTScore, BLEURT) combined with targeted semantic probes to scale robustness testing beyond manual validation while maintaining confidence in perturbation validity.