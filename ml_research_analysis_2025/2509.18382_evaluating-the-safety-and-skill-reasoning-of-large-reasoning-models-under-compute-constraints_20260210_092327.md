---
ver: rpa2
title: Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute
  Constraints
arxiv_id: '2509.18382'
source_url: https://arxiv.org/abs/2509.18382
tags:
- reasoning
- performance
- safety
- compute
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints
## Quick Facts
- arXiv ID: 2509.18382
- Source URL: https://arxiv.org/abs/2509.18382
- Reference count: 5
- Key outcome: None

## Executive Summary
This paper investigates how compute constraints—specifically reasoning length limits and model quantization—affect both the safety and skill performance of Large Reasoning Models (LRMs). The authors introduce Length Controlled Policy Optimization (LCPO) to enforce reasoning length targets during fine-tuning, and evaluate the impact of GPTQ quantization (INT8/INT4) on safety (using StrongReject) and skill (using GPQA, MATH500, AIME2025). They find that INT8 quantization allows models to generate longer reasoning traces within fixed compute budgets, partially compensating for precision loss, while INT4 quantization severely degrades safety performance.

## Method Summary
The study fine-tunes L1-Qwen-1.5B and L1-8B models using LCPO on the SafeChain dataset, incorporating a reward function that combines safety scoring (via Llama-Guard-3) with length penalties to enforce user-defined Chain-of-Thought (CoT) lengths. The models are then quantized using GPTQ to INT8 and INT4 precision. Evaluation is performed at target reasoning lengths of 512, 1024, 2048, and 3600 tokens on skill benchmarks (GPQA, MATH500, AIME2025) and safety benchmark (StrongReject), measuring pass@1 and safe@1 scores respectively.

## Key Results
- INT8 quantization increases inference throughput by ~1.6x, allowing longer reasoning traces that can compensate for precision loss.
- INT4 quantization causes severe safety degradation, with safe@1 scores dropping from 40% to 10% at 512 tokens.
- LCPO fine-tuning enables models to adhere to user-defined reasoning length targets without significant performance degradation.
- Safety and skill performance trade-offs vary significantly with quantization level and reasoning length.

## Why This Works (Mechanism)
### Mechanism 1
- **Claim:** Controlling reasoning length via reinforcement learning allows models to adhere to compute budgets while retaining capability.
- **Mechanism:** Length Controlled Policy Optimization (LCPO) modifies the reward function to include a penalty for deviating from a user-defined token count. By fine-tuning on a CoT dataset (SafeChain) augmented with target length instructions, the model learns to compress or expand its reasoning trace to hit a specific token window.
- **Core assumption:** The model has sufficient foundational knowledge to solve the problem; the bottleneck is primarily the allocation of inference compute (tokens), not the fundamental lack of knowledge.
- **Evidence anchors:**
  - [abstract] "fine-tuning reasoning models using a length controlled policy optimization (LCPO)... to satisfy a user-defined CoT reasoning length"
  - [PAGE 3] "We modify the reward function to combine the (1) safety reward and (2) the length penalty."
  - [corpus] *L1: Controlling How Long...* confirms LCPO as the base method for this control.
- **Break condition:** If a task requires a minimum number of steps to solve that exceeds the token limit, the model may fail or hallucinate a shortcut answer (inverse scaling).

### Mechanism 2
- **Claim:** Quantization (INT8) increases throughput (tokens/second), allowing a model to generate longer reasoning chains within the same wall-clock budget, offsetting precision loss.
- **Mechanism:** Reducing weight precision from BF16 to INT8 lowers memory bandwidth requirements, increasing inference speed. In a fixed time window, the quantized model generates *more* tokens than the full-precision model. If performance scales with reasoning length (test-time compute), the extra tokens compensate for the slight degradation in per-token logic quality due to lower precision.
- **Core assumption:** The relationship between reasoning length and accuracy is monotonic (more tokens = better performance) for the specific task.
- **Evidence anchors:**
  - [PAGE 6] "a quantized reasoning model can perform at par with a full-precision model... [by generating] more reasoning tokens and hence compensate for the loss."
  - [PAGE 5] Table 1 shows INT8 throughput is ~1.6x higher than BF16 for the 1.5B model.
  - [corpus] *Inverse Scaling in Test-Time Compute* suggests this assumption fails for certain tasks where longer reasoning introduces noise.
- **Break condition:** If the task relies heavily on precise numerical representation or complex logical nuance degraded by INT8, the increased token count will not recover accuracy.

### Mechanism 3
- **Claim:** Aggressive quantization (INT4) disproportionately degrades safety alignment compared to skill reasoning.
- **Mechanism:** Safety refusal behaviors often rely on specific decision boundaries learned during alignment. INT4 quantization introduces significant error into weight matrices, potentially distorting these boundaries enough to disable refusal mechanisms (safety collapse) while preserving the broader linguistic capabilities required for math/coding.
- **Core assumption:** Safety classifiers/refusal circuits are more sensitive to weight perturbation than general reasoning circuits.
- **Evidence anchors:**
  - [PAGE 5] "We observe a significant drop in performance of the 4-bit quantized model... safe@1 score drops from 40% to 10%."
  - [PAGE 6] "...making the quantized model far more susceptible to jailbreak queries."
  - [corpus] Evidence regarding INT4 specific safety collapse is weak in the provided neighbors; this observation appears specific to this paper's evaluation.
- **Break condition:** If the safety alignment was reinforced via quantization-aware training (QAT), this degradation might be mitigated (not tested in paper).

## Foundational Learning
- **Concept:** **Test-Time Compute Scaling**
  - **Why needed here:** The paper is predicated on the idea that performance improves as the model "thinks longer" (generates more tokens) at inference time. Understanding this trade-off is essential before applying constraints.
  - **Quick check question:** Does doubling the inference token budget always yield a linear improvement in accuracy, or are there diminishing returns?

- **Concept:** **Post-Training Quantization (PTQ)**
  - **Why needed here:** The paper uses GPTQ to compress models. You must understand that PTQ reduces precision (BF16 -> INT8) without retraining the model on data, trading accuracy for speed/memory.
  - **Quick check question:** Why might a model's "refusal" behavior be more fragile to weight compression than its ability to solve a math equation?

- **Concept:** **Reinforcement Learning from verifiable rewards**
  - **Why needed here:** The authors use LCPO (RL) to enforce length constraints. Understanding that the model is "rewarded" for hitting a token count helps explain why it can control its output length.
  - **Quick check question:** In LCPO, if a model solves a problem correctly in 100 tokens but is rewarded for 1000 tokens, what behavior might emerge? (e.g., repetition/over-analysis).

## Architecture Onboarding
- **Component map:** Base L1 Model -> SafeChain Data Augmentation -> LCPO Fine-tuning (SL1) -> GPTQ Quantization (Q8-SL1/Q4-SL1) -> Evaluation (StrongReject/GPQA/MATH500/AIME2025)
- **Critical path:**
  1. Start with Base L1 Model.
  2. Format SafeChain data: `Prompt + "Think for n tokens"`.
  3. Run LCPO training (Reward = Safety Score + Length Penalty).
  4. Save model (SL1).
  5. Apply GPTQ to SL1 -> Q8-SL1 or Q4-SL1.
  6. Evaluate on StrongReject (Safety) and AIME/GPQA (Skill).

- **Design tradeoffs:**
  - **INT4 vs. INT8:** INT4 is significantly faster (higher throughput) but the paper shows it "breaks" safety (40% -> 10% safe@1). **Do not use INT4 for safety-critical apps** based on this evidence.
  - **Length vs. Compute:** For a fixed compute budget, it is often better to run a quantized (INT8) model for 2048 tokens than a full-precision (BF16) model for 1024 tokens.

- **Failure signatures:**
  - **Safety Collapse:** Sudden drop in `safe@1` score (observed in INT4 models).
  - **Length Violation:** Model ignoring the "Think for n tokens" instruction (happens if LCPO training is insufficient).
  - **Inverse Scaling:** Performance dropping as reasoning length increases (possible in specific logic tasks, per corpus).

- **First 3 experiments:**
  1. **Baseline Calibration:** Run the baseline L1 model on StrongReject with varying token limits (512, 1024, 2048) to confirm safety scores are low.
  2. **Quantization Stress Test:** Compare INT8 vs INT4 on a safety benchmark. Confirm INT4 is unsafe before proceeding with INT8 as the primary compression target.
  3. **Fixed-Budget Parity Check:** For a fixed wall-clock time (e.g., 15 seconds), compare the accuracy of BF16 (short CoT) vs. INT8 (long CoT) to validate the "Compute Equivalence" claim.

## Open Questions the Paper Calls Out
- **Question:** Can the severe safety degradation observed in INT4 quantized models be recovered using mixed-precision or quantization-aware training techniques?
  - **Basis in paper:** [explicit] The authors state they "limit our study to INT8 quantized models" because of the "significant drop in performance of the 4-bit quantized model (SL1) for safety" (e.g., safe@1 dropping from 40% to 10%).
  - **Why unresolved:** The paper identifies INT4 as detrimental to safety but does not investigate methods to mitigate this specific failure mode.
  - **What evidence would resolve it:** Evaluating INT4 models trained with Quantization-Aware Training (QAT) or mixed-precision on the StrongReject benchmark to see if safety scores recover to baseline levels.

- **Question:** Do the trade-offs between compute constraints and safety hold for models with significantly larger parameter counts (e.g., 70B+)?
  - **Basis in paper:** [inferred] The study restricts experiments to L1-1.5B and L1-8B models.
  - **Why unresolved:** Reasoning and safety capabilities may scale differently with parameter size; a 1.5B model may rely on different features for safety than a 70B model, altering the impact of quantization.
  - **What evidence would resolve it:** Replicating the LCPO and GPTQ experiments on larger foundation models (e.g., Llama-3-70B) and comparing the safety/performance slopes.

- **Question:** How does quantizing activations, rather than just weights, affect the safety reasoning capabilities of LRMs?
  - **Basis in paper:** [inferred] The methodology specifies the use of "weight-only quantization (GPTQ)... leaving the activation operation at the original 16-bit."
  - **Why unresolved:** Activations carry the intermediate reasoning states; compressing them might destroy safety-critical features more aggressively than weight compression.
  - **What evidence would resolve it:** A comparative study of weight-only (W4A16) versus weight-and-activation (W4A4 or W4A8) quantization on the safety benchmarks.

## Limitations
- The study only evaluates INT4 and INT8 quantization, leaving open the question of whether other quantization strategies (e.g., mixed-precision) could mitigate safety degradation.
- Experiments are limited to 1.5B and 8B parameter models, so scalability to larger models is unknown.
- The paper does not investigate activation quantization, which may have a different impact on safety reasoning than weight quantization.

## Confidence
- **Mechanism 1 (LCPO length control):** High
- **Mechanism 2 (INT8 throughput compensation):** High
- **Mechanism 3 (INT4 safety degradation):** Medium (based on this paper's evidence, but needs independent verification)

## Next Checks
1. Validate that INT4 quantization consistently causes severe safety degradation (safe@1 ~10%) on StrongReject before proceeding with INT8 experiments.
2. Confirm that LCPO fine-tuning successfully enforces the "Think for n tokens" instruction by measuring length compliance rates across different target lengths.
3. Test the compute equivalence claim by running a fixed-time benchmark comparing BF16 (1024 tokens) vs. INT8 (2048 tokens) and measuring pass@1 scores.