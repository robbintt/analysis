---
ver: rpa2
title: 'Understanding Code Agent Behaviour: An Empirical Study of Success and Failure
  Trajectories'
arxiv_id: '2511.00197'
source_url: https://arxiv.org/abs/2511.00197
tags:
- agents
- trajectories
- agent
- success
- swe-bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comparative analysis of agent trajectories
  on the SWE-Bench benchmark, examining execution traces from three state-of-the-art
  code agents (OpenHands, SWE-agent, and Prometheus). The study reveals that successful
  agents employ distinct problem-solving strategies, including defensive programming
  and context gathering.
---

# Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories

## Quick Facts
- arXiv ID: 2511.00197
- Source URL: https://arxiv.org/abs/2511.00197
- Reference count: 37
- Primary result: Failed code agent trajectories are consistently longer and more variable than successful ones, with fault localization accuracy at file level being necessary but insufficient for success

## Executive Summary
This paper presents the first comparative analysis of agent trajectories on the SWE-Bench benchmark, examining execution traces from three state-of-the-art code agents (OpenHands, SWE-agent, and Prometheus). The study reveals that successful agents employ distinct problem-solving strategies, including defensive programming and context gathering. Failed trajectories are consistently longer and exhibit higher variance than successful ones, with failure patterns differing significantly between agents. Fault localization analysis shows that while most trajectories correctly identify problematic files (72-81% even in failures), success depends more on achieving approximate rather than exact code modifications.

## Method Summary
The study analyzes execution trajectories from three code agents on SWE-Bench Lite and Verified datasets, using a unified thought-action-result framework to parse heterogeneous agent outputs. The analysis segments trajectories into five phases (diagnosis, reproduction, hypothesis, implementation, verification) and computes step count distributions, fault localization accuracy at file/function/hunk levels with lenient ±5 line matching, and unique issue resolution patterns. Statistical comparisons reveal systematic differences between successful and failed trajectories across agents.

## Key Results
- Failed trajectories are consistently longer (20-70 more steps) and more variable than successful ones
- Agents correctly identify target files in 79-81% of failures but still fail to produce working patches
- Only 3-8% of successful patches match gold hunks exactly, while 9-12% achieve close proximity (within 5 lines)
- Each agent's architectural biases create distinct problem-solving profiles for uniquely-solved issues

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Length as an Early Warning Signal
- Claim: Failed trajectories exhibit systematically longer and more variable step counts than successful ones, suggesting agents enter unproductive reasoning loops they cannot exit.
- Mechanism: As agents fail, they generate additional diagnostic steps attempting to recover, creating a compounding effect where length becomes a proxy for "being lost."
- Core assumption: Step count reflects reasoning overhead rather than verbosity differences alone.
- Evidence anchors:
  - [section 4.2]: "Failed trajectories are longer on average and have a wider distribution than successful ones"
  - [section 4.2, Finding 5]: "A higher absolute number of steps in successful trajectories correlates with less divergent failures."

### Mechanism 2: Approximate Localization Sufficiency
- Claim: File-level fault localization is necessary but not sufficient; agents succeed through approximate hunk-level modifications rather than exact matches.
- Mechanism: Agents correctly identify target files even in failures, but success depends on finding workable modification points within those files—not necessarily the gold patch locations.
- Core assumption: Gold patches represent one valid solution among multiple possibilities.
- Evidence anchors:
  - [section 4.3, Finding 6]: "Successful patches find the correct patch file over 90% of the time, but potential solutions within that file can be at multiple places."

### Mechanism 3: Strategy-Problem Fit Determines Uniquely Solved Issues
- Claim: Each agent's architectural biases create distinct problem-solving profiles; success on uniquely-solved issues traces to strategy-problem alignment rather than general capability.
- Mechanism: Prometheus' knowledge-graph context gathering enables repository-pattern recognition, SWE-agent's defensive programming compensates for domain knowledge gaps, and OpenHands' single-commit approach succeeds when early hypothesis is correct.
- Core assumption: Uniquely-solved issues represent strategy-fit rather than noise.
- Evidence anchors:
  - [section 4.1.1, Finding 1]: "Repository-aware context gathering enables knowledge of existing architectural patterns."

## Foundational Learning

- **Agent Trajectory Structure (Thought-Action-Result)**
  - Why needed here: The entire analysis framework depends on normalizing disparate agent outputs into comparable step sequences.
  - Quick check question: Given a raw log excerpt, can you classify each line as thought, action, or result?

- **Fault Localization Granularity Hierarchy**
  - Why needed here: RQ3's contribution is distinguishing file→function→hunk precision and showing they have different relationships to success.
  - Quick check question: Why might file-level localization be necessary but not sufficient for patch success?

- **SWE-Bench Evaluation Protocol**
  - Why needed here: Understanding that success = test passage (not gold-patch match) is critical for interpreting approximate localization findings.
  - Quick check question: What does it mean when a patch passes tests but doesn't match the gold patch hunks?

## Architecture Onboarding

- **Component map:**
  - Trajectory parsers -> unified step arrays -> step count distributions
  - FL analyzer -> modified entities extraction -> match frequency computation
  - Statistical aggregator -> joint frequency analysis -> uniqueness set computation

- **Critical path:**
  1. Parse raw trajectories → unified step arrays
  2. Compute step counts per trajectory
  3. Extract modified entities from patches
  4. Compare against gold patches at three granularities
  5. Correlate FL outcomes with success/failure labels

- **Design tradeoffs:**
  - **Parser granularity**: Counting individual steps vs iterations trades precision for complexity
  - **Lenient hunk matching**: ±5 line tolerance captures "close enough" but may mask systematic offset errors
  - **Gold patch as reference**: Using gold patches as ground truth assumes they represent canonical solutions

- **Failure signatures:**
  - **Runaway trajectories**: Prometheus failures showing 3x std dev increase, max 691 steps vs 192 success max
  - **File-miss loops**: In SWE-Bench Verified, 35-40% of failures explore wrong files for equivalent time as correct ones
  - **Early abandonment failure**: Prometheus hitting recursion limits before implementation

- **First 3 experiments:**
  1. **Replicate trajectory length analysis**: Parse 50 trajectories per agent, compute success/failure length distributions
  2. **Implement lenient hunk matching**: Build the ±5 line tolerance comparator, test on 20 patches
  3. **Manual trajectory phase classification**: Apply the 5-phase framework to 3 unique-issue trajectories

## Open Questions the Paper Calls Out

- **Early stopping mechanisms**: Can trajectory monitoring reduce wasted computation on failed attempts without sacrificing success rates?
- **Unproductive reasoning loop detection**: What signals can reliably detect unproductive reasoning loops in real-time?
- **Approximate hunk localization improvement**: Does improving approximate hunk localization directly increase patch success rates?
- **Generalizability to other agents**: Do the observed trajectory patterns and fault localization findings generalize beyond the three studied agents?

## Limitations

- Trajectory data availability issues: Prometheus trajectories collected from experimental evaluation rather than public logs
- Gold patch assumption: Assumes gold patches represent optimal solutions without validation
- Unified framework potential information loss: Normalizing diverse agent behaviors may obscure diagnostically valuable characteristics

## Confidence

- **High Confidence**: Trajectory length findings showing failed trajectories are systematically longer with higher variance
- **Medium Confidence**: Fault localization sufficiency claims, depending on untested gold patch assumptions
- **Low Confidence**: Strategy-specific success claims for uniquely-solved issues, with weak direct evidence linking architectural choices to outcomes

## Next Checks

1. **Gold Patch Validation Study**: Manually review 20 successful patches where candidate doesn't match gold hunks exactly to assess whether these represent alternative valid solutions
2. **Cross-Agent Trajectory Comparison**: Implement normalized trajectory comparison framework to quantify information loss from unified format
3. **Failure Pattern Classification**: Develop machine learning classifier to automatically categorize failure trajectories based on step sequence patterns