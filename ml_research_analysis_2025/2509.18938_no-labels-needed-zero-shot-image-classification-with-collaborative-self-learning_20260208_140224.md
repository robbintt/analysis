---
ver: rpa2
title: 'No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning'
arxiv_id: '2509.18938'
source_url: https://arxiv.org/abs/2509.18938
tags:
- image
- images
- clip
- classifier
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot image classification framework
  that combines a vision-language model (CLIP) with a pre-trained visual model (ViT-G-14)
  in a self-learning cycle. The method requires only class names and no labeled training
  data, using a confidence-based pseudo-labeling strategy to train a lightweight classifier
  directly on test data.
---

# No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning

## Quick Facts
- arXiv ID: 2509.18938
- Source URL: https://arxiv.org/abs/2509.18938
- Authors: Matheus Vinícius Todescato; Joel Luís Carbonera
- Reference count: 39
- Primary result: Achieves 3-10 percentage point accuracy improvements over CLIP baseline and AdaptCLIPZS across ten diverse datasets using zero-shot learning with class names only

## Executive Summary
This paper presents a zero-shot image classification framework that combines CLIP's semantic alignment with a pre-trained visual model (ViT-G-14) through a self-learning cycle. The method requires only class names and no labeled training data, using confidence-based pseudo-labeling to train a lightweight classifier directly on test data. CLIP identifies high-confidence samples while the pre-trained visual model provides enhanced feature representations, enabling the system to capture complementary semantic and visual cues without supervision. Experiments demonstrate superior performance compared to baseline zero-shot methods and state-of-the-art approaches like AdaptCLIPZS.

## Method Summary
The framework operates in three stages: (1) Seed Selection uses CLIP to rank images by semantic similarity and selects high-confidence candidates through neighborhood consensus filtering; (2) Initial classifier training extracts features with ViT-G-14 and trains a linear classifier on the seed set; (3) Self-learning iteratively refines the classifier by pseudo-labeling and fine-tuning on increasingly confident predictions until stopping criteria are met. The decoupled approach separates semantic selection from visual feature extraction, reducing bias propagation while leveraging the strengths of both components.

## Key Results
- Outperforms CLIP baseline by 3-10 percentage points depending on backbone
- Achieves state-of-the-art results compared to AdaptCLIPZS
- Demonstrates strong performance across diverse domains including fine-grained classification tasks
- Shows robustness through modular design allowing integration of different backbone models
- Maintains accuracy improvements across ten tested datasets including ImageNet, Stanford Cars, and Aircraft

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Semantic-Visual Processing
The framework separates semantic alignment (CLIP) from visual feature extraction (ViT-G-14), using CLIP solely for sample selection while the visual-only model provides discriminative features. This prevents overfitting to noise in the VLM's joint embedding space. Core assumption: the visual-only model provides more discriminative features than the VLM's image encoder. Break condition: if the visual-only feature extractor lacks semantic awareness, the classifier will fail to converge.

### Mechanism 2: Neighborhood Consensus Filtering
Accuracy in pseudo-labeling is improved by aggregating similarity scores from neighboring samples rather than relying on single image-text pairs. For each candidate image, the method retrieves k nearest neighbors and uses their average similarity as the confidence score. This acts as a noise filter ensuring selection only when local clusters align with semantic labels. Break condition: fails in "modal ambiguity" scenarios where legitimate images have visually dissimilar neighbors incorrectly down-weighted by consensus.

### Mechanism 3: Iterative Error Regularization
The self-learning loop iteratively refines the decision boundary but is constrained by stopping criteria to prevent confirmation bias. A lightweight classifier is trained on the initial high-confidence seed, then iteratively tuned on lower-confidence data if the classifier's prediction matches the ranking label. This gradually expands the decision boundary while relying on the visual feature space to self-correct. Break condition: if confidence threshold is too low, label noise accumulates faster than the model can correct it, leading to model collapse.

## Foundational Learning

- **Concept: Vision-Language Contrastive Learning (CLIP)**
  - Why needed here: This is the "Selector" component. You must understand how CLIP maps images and text to a shared latent space to grasp how the system retrieves initial candidates without training data.
  - Quick check question: Can you explain why CLIP can classify an image of a "dax" if "dax" is just a text prompt provided at inference time?

- **Concept: Transfer Learning & Feature Extraction**
  - Why needed here: This is the "Backbone" component. The method relies on ViT-G-14 not to learn, but to project raw pixels into a high-dimensional space where classes are linearly separable.
  - Quick check question: Why would a feature extractor pre-trained on a massive dataset (like LAION-2B) perform better on a zero-shot task than a randomly initialized one?

- **Concept: Pseudo-Labeling & Confirmation Bias**
  - Why needed here: This is the "Risk" component. The entire self-learning cycle is a battle against confirmation bias—where the model reinforces its own initial mistakes.
  - Quick check question: In a self-training loop, if a "cat" is mislabeled as "dog" and added to the training set, what happens to the decision boundary for "dog" in the next iteration?

## Architecture Onboarding

- **Component map:** CLIP Encoder (Frozen) -> Visual Encoder (Frozen, ViT-G-14) -> Lightweight Classifier (Trainable)
- **Critical path:** The Seed Selection (Step A) determines the upper bound of system performance. If the neighborhood consensus selects noisy samples, the Self-Learning Loop (Step B) will propagate these errors. Do not optimize the classifier learning rate until the Seed accuracy is verified.
- **Design tradeoffs:** 
  - Modularity vs. Latency: Using two separate models (CLIP + ViT-G) doubles the inference overhead for feature extraction compared to single-stream methods
  - k-shot selection: A small k (e.g., 5) ensures high precision but limits diversity; a large k (e.g., 50) increases diversity but introduces label noise
- **Failure signatures:**
  - Semantic Drift: Loss stabilizes but accuracy degrades on specific fine-grained classes
  - Overfitting to Seed: Classifier achieves 100% accuracy on SEED set but near-random accuracy on full dataset
  - High Variance: If standard deviation across runs is high, check stability of CLIP similarity scores for edge cases
- **First 3 experiments:**
  1. Seed Purity Audit: Run Step A on labeled validation set to calculate SEED precision vs k
  2. Ablation on Backbone: Swap ViT-G-14 for CLIP image encoder as feature extractor
  3. Loop Stability: Run Step B with varying Loss_limit and plot accuracy vs cycle count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the collaborative self-learning framework be effectively adapted to the few-shot setting by utilizing labeled samples during seed selection?
- Basis in paper: [explicit] The authors state in the conclusion, "In future works, we plan to investigate the use of our approach in a few-shot setting, evaluating the seed selection process with labeled samples."
- Why unresolved: The current study evaluates the method strictly in a zero-shot environment. It is unknown how the self-learning cycle interacts with a small, verified set of labeled data versus pseudo-labeled data.
- What evidence would resolve it: Comparative experiments showing performance when initial SEED set uses ground-truth labels vs purely zero-shot baseline.

### Open Question 2
- Question: How can confirmation bias in the self-learning cycle be mitigated for datasets with large numbers of classes?
- Basis in paper: [inferred] The authors observe complete approach underperforms "Seed" baseline on ImageNet and hypothesize this is due to "error accumulation" where "even a small error rate in pseudo-labeling can introduce significant noise."
- Why unresolved: Current stopping criteria and pseudo-labeling strategy fail to prevent overfitting when label space is large (1000 classes), causing iterative refinement to degrade accuracy.
- What evidence would resolve it: A modification to training dynamics or filtering strategy that results in iterative approach consistently outperforming seed-only baseline on high-cardinality datasets.

### Open Question 3
- Question: Is the framework's performance robust against class names that lack semantic distinctiveness?
- Basis in paper: [inferred] The authors note the approach is "dependent on... the semantic condition of the dataset" and highlight difficulties with fine-grained datasets like Aircraft where seed selection accuracy was low.
- Why unresolved: While visual model reduces dependence on VLM, critical seed selection phase still relies on CLIP's text embeddings. It is unclear if visual features can recover from poor semantic initialization.
- What evidence would resolve it: Experiments using datasets with deliberately ambiguous or synonymous class names to determine if visual feature extraction can correct for semantic confusion in initial selection phase.

## Limitations
- Performance heavily depends on CLIP's initial ranking quality, which may degrade in domains with limited semantic overlap to CLIP's training corpus
- The stopping criteria (particularly Loss_limit=0.1) may be dataset-specific and require tuning for optimal performance
- Fixed hyperparameters (k=5 for most datasets, I_epochs=100, R_epochs=20) may not generalize well across all domain types

## Confidence
- **High Confidence:** The decoupled semantic-visual processing mechanism and overall zero-shot performance claims are well-supported by experimental results across ten diverse datasets
- **Medium Confidence:** The neighborhood consensus filtering mechanism is theoretically sound but its effectiveness in highly ambiguous visual domains remains to be fully validated
- **Low Confidence:** The specific hyperparameter values (especially stopping thresholds) may be overfit to the tested datasets and require domain-specific tuning

## Next Checks
1. **Domain Transfer Validation:** Test the framework on medical imaging or satellite imagery datasets where CLIP's semantic coverage may be limited
2. **Hyperparameter Sensitivity Analysis:** Systematically vary k, I_epochs, R_epochs, and Loss_limit to determine optimal ranges for different dataset characteristics
3. **Noise Robustness Test:** Intentionally introduce varying levels of label noise in the SEED selection phase to quantify the method's resilience to initial contamination