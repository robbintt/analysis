---
ver: rpa2
title: Correlating and Predicting Human Evaluations of Language Models from Natural
  Language Processing Benchmarks
arxiv_id: '2502.18339'
source_url: https://arxiv.org/abs/2502.18339
tags:
- mmlu
- dialogue
- metric
- additional
- char
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between human evaluations
  and NLP benchmarks for chat-finetuned language models. The authors conducted a large-scale
  study comparing four Chat Llama 2 models on 160 NLP benchmarks and extensive human
  evaluations across 11k single-turn and 2k multi-turn dialogues.
---

# Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks

## Quick Facts
- **arXiv ID**: 2502.18339
- **Source URL**: https://arxiv.org/abs/2502.18339
- **Reference count**: 40
- **Primary result**: NLP benchmarks can reliably predict human evaluations for chat-finetuned language models, with some safety-related exceptions

## Executive Summary
This paper investigates the relationship between human evaluations and NLP benchmarks for chat-finetuned language models. The authors conducted a large-scale study comparing four Chat Llama 2 models on 160 NLP benchmarks and extensive human evaluations across 11k single-turn and 2k multi-turn dialogues. They found that most NLP benchmarks are highly correlated with human evaluations, suggesting they can serve as reliable predictors. However, three safety-related evaluations were anticorrelated, and two were uncorrelated. Through overparameterized linear regressions with leave-one-out cross-validation, the authors demonstrated that NLP scores can accurately predict human evaluations across different model scales.

## Method Summary
The authors evaluated four Chat Llama 2 models across 160 NLP benchmarks and conducted extensive human evaluations through both single-turn (11k dialogues) and multi-turn (2k dialogues) assessments. They employed overparameterized linear regression models with leave-one-out cross-validation to predict human evaluation scores from benchmark performance. The correlation analysis examined relationships between individual benchmark scores and human judgments, while the predictive modeling tested whether benchmark suites could accurately forecast human evaluation outcomes across different model scales.

## Key Results
- Most NLP benchmarks show high correlation with human evaluations for chat-finetuned models
- Three safety-related evaluations showed anticorrelation with benchmark performance
- Two evaluations were uncorrelated with benchmark scores
- Overparameterized linear regression with leave-one-out cross-validation accurately predicted human evaluations from NLP scores

## Why This Works (Mechanism)
The correlation between NLP benchmarks and human evaluations works because chat-finetuned language models retain their underlying capabilities while adapting to conversational contexts. The benchmarks, primarily designed to measure fundamental language understanding and generation capabilities, capture many of the core competencies that humans value in chat interactions. The linear relationship suggests that these fundamental capabilities translate relatively directly to human-perceived quality in conversational settings, though exceptions exist for safety-specific dimensions that may require specialized evaluation approaches.

## Foundational Learning

**Linear Regression with Leave-One-Out Cross-Validation**
- Why needed: To predict human evaluation scores from benchmark performance while validating robustness
- Quick check: Ensure model coefficients are stable across folds and prediction errors are minimized

**Correlation Analysis**
- Why needed: To quantify relationships between benchmark scores and human judgments
- Quick check: Verify correlation coefficients are statistically significant and examine scatter plots for linear patterns

**Chat-Finetuning**
- Why needed: To understand the specific model variant being evaluated
- Quick check: Confirm models were properly fine-tuned on conversational data and retain base model capabilities

## Architecture Onboarding

**Component Map**: Human Evaluations -> Correlation Analysis -> Linear Regression -> Predictive Modeling -> Benchmark Suite Validation

**Critical Path**: Model Selection → Benchmark Evaluation → Human Evaluation → Correlation Analysis → Predictive Modeling → Validation

**Design Tradeoffs**: The study prioritized breadth of benchmark coverage (160 benchmarks) over depth of model diversity (4 models), trading model variety for comprehensive evaluation of existing benchmarks' predictive power.

**Failure Signatures**: Anticorrelation in safety evaluations indicates potential blind spots where benchmarks may not capture human safety concerns; uncorrelated evaluations suggest benchmarks may miss specific capabilities entirely.

**First Experiments**:
1. Replicate correlation analysis with a single additional model to test initial generalizability
2. Perform ablation study removing one benchmark category at a time to identify critical predictors
3. Conduct temporal analysis comparing correlation strength across different evaluation time periods

## Open Questions the Paper Calls Out
None

## Limitations
- Limited model diversity with only four Chat Llama 2 variants tested
- Potential non-linear relationships between benchmarks and human judgments not fully explored
- Safety-related evaluations show complex relationships that may not be captured by standard benchmarks

## Confidence
- **High**: Correlation patterns observed within tested models and benchmarks
- **Medium**: Predictive utility claims for benchmark-based forecasting
- **Low**: Assertion that benchmarks can fully replace human annotation in safety-critical contexts

## Next Checks
1. Replicate the correlation analysis across at least 10 additional chat-finetuned models from different architectures (e.g., GPT, Claude, Mistral) to test generalizability
2. Conduct ablation studies to identify which specific benchmarks contribute most to predictive accuracy versus redundancy
3. Design a longitudinal study comparing benchmark-based predictions against human evaluations over time as models continue to evolve