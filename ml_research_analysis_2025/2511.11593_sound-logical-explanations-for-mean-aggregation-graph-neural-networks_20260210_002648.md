---
ver: rpa2
title: Sound Logical Explanations for Mean Aggregation Graph Neural Networks
arxiv_id: '2511.11593'
source_url: https://arxiv.org/abs/2511.11593
tags:
- rules
- sound
- rule
- dataset
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining mean-aggregation
  graph neural networks (MAGNNs) for knowledge graph completion. The authors prove
  which monotonic rules can be sound for MAGNNs and provide a restricted fragment
  of first-order logic to explain any MAGNN prediction.
---

# Sound Logical Explanations for Mean Aggregation Graph Neural Networks

## Quick Facts
- arXiv ID: 2511.11593
- Source URL: https://arxiv.org/abs/2511.11593
- Reference count: 40
- Primary result: Provides sound logical explanations for mean-aggregation graph neural networks by identifying ELUQ rules as the only monotonic rules that can be proven sound

## Executive Summary
This paper tackles the challenge of explaining mean-aggregation graph neural networks (MAGNNs) used for knowledge graph completion. The authors establish theoretical foundations by proving which monotonic rules can be sound for MAGNNs and develop a procedure to extract these explanations. Their work bridges the gap between the opacity of neural networks and the need for interpretable reasoning in knowledge graph applications.

The research demonstrates that while MAGNNs can be constrained to produce sound monotonic explanations, this comes with significant limitations. Experiments on benchmark datasets show that non-negative weight constraints maintain or improve performance while enabling sound rule extraction, but the explanatory power is restricted to a specific fragment of first-order logic (ELUQ). This work represents an important step toward verifiable AI systems in knowledge graph reasoning.

## Method Summary
The authors develop a theoretical framework for explaining MAGNN predictions through monotonic rules. They first prove that only ELUQ (Existential Logic with Union and Quantification) rules can be sound for MAGNNs, establishing the limited class of explainable rules. The paper then constructs a procedure to extract sound explanatory rules from any MAGNN prediction by analyzing the model's learned weights and structure. To ensure sound explanations, the authors modify MAGNNs to use non-negative weights, which constrains the model to monotonic behavior. The approach is validated on benchmark knowledge graph datasets where the extracted rules are evaluated for soundness and the modified MAGNNs are compared against standard MAGNNs in terms of performance.

## Key Results
- Identified ELUQ rules as the only monotonic rules that can be proven sound for MAGNNs
- Developed a procedure to extract sound explanatory rules from MAGNN predictions
- Non-negative weight constraints maintain or improve performance while enabling sound explanations
- MAGNNs with non-negative weights recovered sound monotonic rules on YAGO26K-906 but failed on NELL995

## Why This Works (Mechanism)
The approach works by constraining MAGNNs to monotonic behavior through non-negative weight constraints, which enables the extraction of sound logical explanations. The theoretical foundation proves that only ELUQ rules can be sound for MAGNNs due to the properties of mean aggregation. The explanation procedure works by analyzing the learned weights and structure of the constrained MAGNN to construct ELUQ rules that provably explain each prediction.

## Foundational Learning

**Knowledge Graph Completion** - Predicting missing links in knowledge graphs using existing triples. Needed to understand the application domain. Quick check: Can identify subject-predicate-object triples and missing link prediction task.

**Graph Neural Networks** - Neural networks that operate on graph-structured data through message passing. Needed to understand MAGNN architecture. Quick check: Can explain node aggregation and feature propagation.

**Mean Aggregation** - Averaging neighbor features during message passing in MAGNNs. Needed to understand the specific model being explained. Quick check: Can describe how neighbor information is combined.

**First-Order Logic Rules** - Logical statements with variables, quantifiers, and connectives. Needed to understand the explanation format. Quick check: Can write simple Horn clauses.

**Monotonicity** - Property where adding information cannot invalidate existing conclusions. Needed to understand sound explanations. Quick check: Can identify monotonic vs non-monotonic rules.

**ELUQ Logic** - Fragment of first-order logic with existential quantification, union, and quantification. Needed to understand the expressive limitations. Quick check: Can write valid ELUQ formulas.

## Architecture Onboarding

**Component Map**: Input KG triples -> MAGNN layers (with non-negative weights) -> Node embeddings -> Prediction layer -> Rule extraction module

**Critical Path**: Training MAGNN with non-negative weights → Extracting weights → Analyzing weight patterns → Constructing ELUQ rules → Validating rule soundness

**Design Tradeoffs**: Non-negative weights ensure monotonicity but limit expressiveness; ELUQ restriction guarantees soundness but reduces explanation quality; mean aggregation enables efficient computation but restricts rule class.

**Failure Signatures**: Inability to extract sound rules indicates non-monotonic behavior or complex patterns beyond ELUQ; poor performance with non-negative weights suggests the need for non-monotonic reasoning; inconsistent explanations across similar examples indicate model instability.

**First Experiments**: 1) Train MAGNN on small synthetic knowledge graph with known monotonic rules; 2) Apply rule extraction procedure and verify extracted rules match ground truth; 3) Test performance impact of non-negative weight constraint on a benchmark dataset.

## Open Questions the Paper Calls Out
The paper highlights that the restricted class of ELUQ rules significantly limits the expressiveness of explanations, raising questions about whether this is sufficient for practical applications. It also notes that MAGNNs with non-negative weights can recover sound rules on some datasets but fail on others, suggesting the approach may not generalize well across different knowledge graph characteristics.

## Limitations
- ELUQ restriction severely limits the expressiveness of explanations that can be extracted
- Mixed empirical results show the approach works on some datasets but fails on others
- Assumes binary relations and does not address more complex relational structures

## Confidence
- Theoretical claims about ELUQ soundness: High
- Empirical performance results: Medium
- Generalizability across knowledge graph datasets: Medium

## Next Checks
1) Test the explanation procedure on additional knowledge graph datasets with varying characteristics to assess robustness
2) Evaluate whether the ELUQ restriction significantly impacts the quality of explanations in practical applications
3) Investigate whether alternative aggregation mechanisms or weight constraints could enable more expressive yet still sound explanations