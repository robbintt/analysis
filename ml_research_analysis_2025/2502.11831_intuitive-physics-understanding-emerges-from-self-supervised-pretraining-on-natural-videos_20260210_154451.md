---
ver: rpa2
title: Intuitive physics understanding emerges from self-supervised pretraining on
  natural videos
arxiv_id: '2502.11831'
source_url: https://arxiv.org/abs/2502.11831
tags:
- video
- videos
- performance
- v-jepa
- physics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether intuitive physics understanding
  can emerge from self-supervised pretraining on natural videos. The authors compare
  three approaches: (1) V-JEPA, which predicts missing parts of video in a learned
  representation space; (2) VideoMAEv2, which predicts pixel space; and (3) multimodal
  large language models (MLLMs) that reason through text.'
---

# Intuitive physics understanding emerges from self-supervised pretraining on natural videos

## Quick Facts
- arXiv ID: 2502.11831
- Source URL: https://arxiv.org/abs/2502.11831
- Reference count: 40
- Self-supervised video pretraining in representation space yields physics understanding, with V-JEPA achieving 98% accuracy on IntPhys benchmark

## Executive Summary
This paper investigates whether intuitive physics understanding can emerge from self-supervised pretraining on natural videos. The authors compare three approaches: V-JEPA (predicting missing video parts in learned representation space), VideoMAEv2 (pixel-space prediction), and multimodal LLMs (text-based reasoning). Using violation-of-expectation evaluation on three benchmarks (IntPhys, GRASP, InfLevel-lab), they find that V-JEPA significantly outperforms both pixel prediction models and MLLMs, achieving 98% accuracy on IntPhys compared to chance-level performance for alternatives. The key insight is that predicting in abstract representation space, rather than pixel space, forces the model to capture physical structure and regularities.

## Method Summary
The method employs V-JEPA (Video Joint-Embedding Predictive Architecture), which jointly learns an abstract representation space while predicting missing parts of sensory input. The architecture consists of a context encoder, target encoder (with EMA weights), and predictor. During training, videos are corrupted with block masking, encoded, and the predictor attempts to reconstruct target representations. At test time, the model computes surprise as the L1 distance between predicted and actual encoded frames, using higher surprise to indicate physically implausible events. The approach is evaluated zero-shot on violation-of-expectation benchmarks without task-specific training.

## Key Results
- V-JEPA achieves 98% accuracy on IntPhys benchmark, significantly outperforming VideoMAEv2 (pixel prediction) and MLLMs at chance level
- Physics understanding emerges from predicting in learned representation space rather than pixel space
- The capability is robust across model sizes (even 115M parameters) and training data sources (including just one week of unique video)
- V-JEPA struggles with color constancy, solidity, and collision detection properties, performing near chance on these tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Predicting video content in a learned representation space yields abstract representations that capture physical structure, whereas pixel-space prediction does not.
- **Mechanism:** The encoder is trained jointly with a predictor to minimize prediction error in representation space. This forces the encoder to retain predictable, semantic information (object permanence, continuity) while discarding unpredictable low-level details (texture noise, pixel variations).
- **Core assumption:** Physical regularities are more predictable in abstract representation than in raw pixels.
- **Evidence anchors:**
  - [abstract] "video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties"
  - [Page 3] "jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient"
- **Break condition:** If pixel-space models (VideoMAEv2) matched V-JEPA performance, the mechanism would be invalidated—but they perform near chance (Page 4).

### Mechanism 2
- **Claim:** Prediction error in representation space functions as a "surprise" signal that detects physical violations without task-specific training.
- **Mechanism:** At inference, the model encodes past frames, predicts future representations, and computes L1 distance to actual encoded frames. Higher distance indicates greater surprise at implausible events.
- **Core assumption:** The model's learned representations encode expectations about normal physical behavior; violations produce larger prediction errors.
- **Evidence anchors:**
  - [Page 3] "By recording the prediction error—the distance between the predicted video representations and the actual encoded video representations—we obtain a temporally aligned quantitative measure of surprise"
  - [Page 5] V-JEPA shows significantly higher surprise for impossible vs. possible videos across object permanence, continuity, shape constancy (p < 0.001)
- **Break condition:** If untrained random networks showed similar surprise patterns, the mechanism would be learning-independent—but they perform at chance (Page 4).

### Mechanism 3
- **Claim:** The specific pretraining masking strategy matters less than the prediction-in-representation-space objective itself.
- **Mechanism:** V-JEPA was trained with block masking but tested on causal future prediction. Random masking (90% of patches) still yields >85% accuracy on IntPhys, only ~5 points below block masking.
- **Core assumption:** The key is learning to predict representations, not the precise masking pattern.
- **Evidence anchors:**
  - [Page 7] "Random Masking seems to perform worse...the drop on IntPhys is only around 5 points...the effective performance of Random Masking...suggests that the understanding of intuitive physics does not require a tailored objective"
- **Break condition:** If different masking strategies produced dramatically different physics understanding (e.g., >30 point gaps), the mechanism would be masking-specific rather than objective-specific.

## Foundational Learning

- **Concept: Violation-of-Expectation (VoE) paradigm**
  - Why needed here: The entire evaluation methodology relies on measuring model "surprise" to physically impossible vs. possible videos, borrowed from infant psychology.
  - Quick check question: If a model sees two nearly identical videos where one shows a ball disappearing behind an occluder and reappearing, and the other shows it vanishing, which should produce higher surprise?

- **Concept: Predictive coding hypothesis**
  - Why needed here: V-JEPA's design is motivated by the neuroscience idea that brains constantly predict sensory input and learn from prediction errors.
  - Quick check question: What does a predictive system learn when its predictions consistently match observations vs. when they don't?

- **Concept: Joint Embedding Predictive Architectures (JEPAs)**
  - Why needed here: Understanding why V-JEPA has separate context encoder, target encoder (EMA), and predictor is essential for reproducing the approach.
  - Quick check question: Why predict in representation space rather than reconstructing pixels directly?

## Architecture Onboarding

- **Component map:** Context encoder (ViT) -> Context representation -> Predictor -> Predicted representation; Target encoder (ViT, EMA) -> Target representation; L1 loss between predicted and target representations

- **Critical path:**
  1. Video V → corrupt with mask → V_corrupted
  2. f_θ(V_corrupted) → context representation
  3. f_EMA(V) → target representation (for unmasked regions)
  4. p_ϕ(context_repr) → predicted representation
  5. L1 loss between predicted and target representations
  6. At test time: encode past → predict future → measure distance to actual

- **Design tradeoffs:**
  - Larger models improve performance but 115M params still achieves >85% on IntPhys
  - Training data diversity matters: HowTo100M (tutorials) outperforms SSv2 (fine motion)
  - Short clips (16 frames / 3 seconds) limit memory for complex interactions
  - Assumption: Random masking works nearly as well as block masking for physics understanding

- **Failure signatures:**
  - Color constancy, solidity, and collision detection remain near chance across all tested properties
  - Occluded events produce lower performance (e.g., 8.2% error vs 0.6% for visible object permanence)
  - Contextualization events (requiring memory beyond clip length) cause failures on InfLevel-lab

- **First 3 experiments:**
  1. **Baseline sanity check:** Load pretrained V-JEPA, evaluate on IntPhys dev set with provided surprise computation code; verify ~98% pairwise accuracy.
  2. **Ablation on prediction space:** Compare V-JEPA vs VideoMAEv2 (pixel prediction) on same benchmark using identical evaluation protocol; expect ~50% vs ~98%.
  3. **Data efficiency test:** Train V-JEPA-L on subset of HowTo100M (e.g., 128 hours unique video) and evaluate whether physics understanding emerges with minimal data (target: >70% on IntPhys).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does intuitive physics understanding emerge in models trained on videos that mimic an infant's visual experience?
- Basis in paper: [explicit] The authors ask whether an understanding of physics "emerges in models trained on videos that mimic what infants see."
- Why unresolved: Current models are trained on curated or web-scraped adult-centric data (e.g., HowTo100M), which differs significantly from the developmental visual diet of biological agents.
- What evidence would resolve it: Training V-JEPA on longitudinal, egocentric infant datasets (e.g., SayCam, BabyView) and evaluating performance on IntPhys and GRASP benchmarks.

### Open Question 2
- Question: Is active interaction required to learn physical properties involving object interactions, such as collision and solidity?
- Basis in paper: [explicit] The discussion suggests "an agent has to be able to interact with objects themselves in order to learn about interactions, suggesting the need to add action channels."
- Why unresolved: V-JEPA currently acts as a passive observer and performs close to chance on collision and solidity tasks.
- What evidence would resolve it: Integrating action-conditioning into the JEPA architecture to test if agency improves accuracy on interaction-heavy physical reasoning tasks.

### Open Question 3
- Question: Do higher-order object interactions require a hierarchical predictive architecture?
- Basis in paper: [explicit] The authors hypothesize that "interactions between objects require higher-order representations, and that a more powerful hierarchical version of JEPA is needed."
- Why unresolved: The non-hierarchical V-JEPA captures scene content well but fails to model complex inter-object dynamics.
- What evidence would resolve it: Ablation studies comparing standard V-JEPA against a hierarchical variant on specific subsets of benchmarks focusing on collisions and solidity.

## Limitations
- Performance on color constancy, solidity, and collision detection properties remains near chance across all tested methods
- Understanding is limited to visible events, with significantly degraded performance on occluded scenarios
- The specific pretraining masking strategy matters less than the prediction objective, but optimal configuration for different physical properties remains unclear

## Confidence
- **High confidence**: The core finding that V-JEPA achieves ~98% accuracy on IntPhys while pixel-space models and MLLMs perform at chance level. The comparative methodology and benchmark results are robust.
- **Medium confidence**: The claim that prediction in representation space (rather than pixel space) is the key mechanism for physics understanding. While supported by comparisons, the underlying representational differences warrant deeper analysis.
- **Medium confidence**: The assertion that physics understanding emerges from prediction objectives rather than task-specific training. The masking strategy ablation supports this, but alternative pretraining objectives need exploration.

## Next Checks
1. **Cross-domain generalization test**: Evaluate V-JEPA on videos from domains very different from HowTo100M (e.g., cartoons, synthetic physics simulations, or surveillance footage) to determine if the learned physics understanding transfers beyond training distribution.

2. **Temporal reasoning capacity**: Test whether increasing clip length (beyond 16 frames) significantly improves performance on InfLevel-lab contextualization events, which require longer-term memory for complex physical interactions.

3. **Ablation on prediction targets**: Replace the EMA target encoder with a static pretrained encoder or random targets to determine whether the predictive objective alone is sufficient, or if the dynamic target encoding is critical for physics understanding emergence.