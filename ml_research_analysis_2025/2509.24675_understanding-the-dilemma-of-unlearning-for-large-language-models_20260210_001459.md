---
ver: rpa2
title: Understanding the Dilemma of Unlearning for Large Language Models
arxiv_id: '2509.24675'
source_url: https://arxiv.org/abs/2509.24675
tags:
- unlearning
- arxiv
- knowledge
- preprint
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness and interpretability
  of unlearning in large language models (LLMs) through a framework called UNPACT,
  which quantifies the contribution of each prompt token to the model's output. The
  study reveals that unlearning primarily disrupts the model's focus on key prompt
  tokens associated with the correct answer.
---

# Understanding the Dilemma of Unlearning for Large Language Models

## Quick Facts
- arXiv ID: 2509.24675
- Source URL: https://arxiv.org/abs/2509.24675
- Authors: Qingjie Zhang; Haoting Qian; Zhicong Huang; Cheng Hong; Minlie Huang; Ke Xu; Chao Zhang; Han Qiu
- Reference count: 21
- Primary result: Unlearning primarily disrupts focus on key prompt tokens rather than erasing knowledge, leaving knowledge recoverable through keyword emphasis or causing catastrophic forgetting through indiscriminate token penalization.

## Executive Summary
This paper investigates the effectiveness and interpretability of unlearning in large language models (LLMs) through a framework called UNPACT, which quantifies the contribution of each prompt token to the model's output. The study reveals that unlearning primarily disrupts the model's focus on key prompt tokens associated with the correct answer. However, the knowledge is often not truly erased and can be recovered by simply emphasizing these keywords in the prompt. Catastrophic forgetting arises from indiscriminate penalization of all tokens, including those essential for general performance. The findings suggest that existing unlearning methods face a dilemma: they either leave knowledge recoverable or cause excessive performance degradation, leaving a gap to achieving reliable unlearning.

## Method Summary
The study introduces UNPACT (Understanding through UNderstanding the Prompt token Contribution) to analyze unlearning by quantifying each token's contribution to model outputs. The method computes token contribution as the difference in log-probability when a token is masked versus present. Key tokens are identified using thresholds on these contributions. The framework evaluates six unlearning methods (GA, NPO, TV, RMU) with and without regularization (GD, KL) on three datasets (News, Books, WMDP) using three LLMs (Llama-2-7B, Llama-3.1-8B-Instruct, Qwen3-14B). Recovery is tested through FOCUSONKEY (emphasizing key tokens in prompts), while destructive rates measure catastrophic forgetting. The study compares methods based on their ability to balance forgetting unwanted knowledge while preserving general capabilities.

## Key Results
- Unlearning disrupts focus on key prompt tokens rather than erasing underlying knowledge
- Knowledge can be recovered by 30.7%–54.4% through simple keyword emphasis in prompts
- Catastrophic forgetting occurs when unlearning indiscriminately penalizes all tokens, degrading general performance
- No current method reliably minimizes both recovery rate and destructive rate simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Keyword Focus Disruption
When unlearning appears successful, it may function by reducing model attention on KEYTOKENS that predict correct answers, rather than erasing underlying knowledge. UNPACT quantifies each prompt token's contribution to the output via perturbation: C(x_i, y) = LP(x, y) - LP(x\{x_i}, y). When knowledge is forgotten post-unlearning, cosine similarity between pre- and post-unlearning KEYTOKEN sets drops significantly (34.2% average decrease on News dataset). If models rely on distributed representations where no single token is critical, KEYTOKEN identification becomes unstable and the mechanism's explanatory power weakens.

### Mechanism 2: Prompt-Based Knowledge Recovery
"Unlearned" knowledge often remains recoverable if KEYTOKENS are explicitly emphasized in prompts, suggesting incomplete erasure. FOCUSONKEY appends minimal emphasis phrases (e.g., "Focus on [K_s] to answer") to prompts. Recovery rates range from 30.7%–54.4% across methods, nearly double the probabilistic baseline (PROBAB) using multinomial sampling. If models have genuinely erased parametric knowledge, prompt emphasis should not improve recovery; persistent recovery suggests knowledge persists in weights or activations.

### Mechanism 3: Indiscriminate Token Penalization
Catastrophic forgetting may arise because unlearning objectives penalize all tokens in forget-set text, including common words unrelated to target knowledge. When catastrophic forgetting occurs, UNPACT shows models ignore all prompt tokens, generating nonsense. Since unlearning losses (e.g., GA, NPO) don't distinguish between knowledge-relevant tokens (e.g., "Samuel Paty") and irrelevant tokens (e.g., "did", "get"), common-word representations degrade. If regularization (GD, KL) sufficiently protects common-token representations, catastrophic forgetting may be avoided; current methods fail to achieve this reliably.

## Foundational Learning

**Token Attribution Methods (e.g., LIME, SHAP, perturbation-based)**
- Why needed here: UNPACT relies on perturbation-based contribution scoring. Understanding attribution fundamentals clarifies why this approach works for both open- and closed-source models.
- Quick check question: How would masking a token change log-probability if the token is irrelevant vs. critical for an answer?

**Catastrophic Forgetting in Neural Networks**
- Why needed here: The paper positions catastrophic forgetting as a core unlearning tradeoff. Grasping its causes (interference, representational overlap) helps interpret why indiscriminate penalization is damaging.
- Quick check question: Why does training on a new task sometimes degrade performance on a previously learned task?

**Regularization for Utility Preservation (KL Divergence, Gradient Descent on Retain Set)**
- Why needed here: The paper compares six methods, four combining unlearning with regularization. Understanding how KL and GD losses constrain model drift is essential for interpreting the dilemma.
- Quick check question: How does minimizing KL divergence between pre- and post-unlearning models differ from fine-tuning on a retain set?

## Architecture Onboarding

**Component Map:**
Input Prompt → UNPACT (Token Contribution Scoring) → KEYTOKEN Identification → Compare KEYTOKEN Sets → Focus Shift Analysis → Recovery via FOCUSONKEY

**Critical Path:**
1. Define forget set (D_F) and retain set (D_R)
2. Apply unlearning method (e.g., GA, NPO, TV, RMU) with optional regularization
3. Run UNPACT pre- and post-unlearning to extract KEYTOKENS
4. Compute cosine similarity between KEYTOKEN sets to measure focus shift
5. Apply FOCUSONKEY to test recoverability; measure destructive rate for catastrophic forgetting

**Design Tradeoffs:**
- Open- vs. Closed-Source Compatibility: UNPACT uses only input-output log-probabilities, enabling closed-source analysis but limiting mechanistic insight compared to activation-based methods
- KEYTOKEN Threshold Selection (α, β): Grid-searched values (α=0.22, β=0.24) balance sensitivity and noise; different domains may require retuning
- Recovery vs. Destructive Rate Tradeoff: Aggressive unlearning reduces recovery rate but increases destructive rate; no current method reliably minimizes both

**Failure Signatures:**
- High Recovery Rate + Low Destructive Rate: Unlearning insufficient; knowledge persists
- Low Recovery Rate + High Destructive Rate: Catastrophic forgetting; model utility collapses
- Unstable KEYTOKEN Overlap: Suggests model relies on distributed representations; UNPACT may produce noisy interpretations

**First 3 Experiments:**
1. Baseline Reproduction: Run GA, NPO, TV, and RMU on News dataset with Llama-2-7B; measure recovery and destructive rates using UNPACT. Compare to Table 2/6 benchmarks.
2. Hyperparameter Sensitivity Analysis: Vary regularization strength (KL, GD) and training epochs; plot recovery vs. destructive rate trajectories to identify the "gap" region.
3. Cross-Domain KEYTOKEN Stability: Compare KEYTOKEN overlap consistency across News, Books, and WMDP; assess whether the mechanism generalizes beyond news articles.

## Open Questions the Paper Calls Out

### Open Question 1
Can the UNPACT framework's token attribution metrics be operationalized into a loss function that selectively penalizes high-contribution "KeyTokens" while explicitly preserving common tokens, thereby resolving the trade-off between knowledge erasure and catastrophic forgetting? The authors identify that catastrophic forgetting arises from the "indiscriminate penalization of all tokens" and conclude that current methods leave a "gap to reliable unlearning" by failing to distinguish target keywords from essential common words. An unlearning algorithm that weights gradients based on token contribution scores, demonstrating significantly lower destructive rates while maintaining high forgetting rates compared to uniform penalization, would resolve this question.

### Open Question 2
Does the "FocusOnKey" recovery vulnerability generalize to more sophisticated adversarial prompt engineering or semantic paraphrasing, or is it strictly limited to explicit keyword emphasis? While the paper proves knowledge is not "truly erased" via simple prompts, it does not test whether the knowledge is accessible via indirect prompt injection, semantic obfuscation, or multi-turn dialogue strategies. Testing unlearned models against a comprehensive suite of adversarial attacks (e.g., jailbreaking, indirect prompts) to see if recovery rates rise or fall compared to the "FocusOnKey" baseline would resolve this question.

### Open Question 3
Do the mechanisms of the "unlearning dilemma" apply to non-factual knowledge domains (e.g., reasoning capabilities, style transfer) where "KeyTokens" are less distinct or distributed differently than in the textual fact-retrieval benchmarks tested? The study relies on benchmarks (News, Books, WMDP) heavily grounded in factual recall, where keywords are highly predictive; the authors acknowledge the framework's dependence on "KeyTokens," which may be ill-defined for abstract tasks. Application of UNPACT to unlearning tasks involving reasoning or stylistic generation to analyze if "KeyTokens" can be identified and disrupted without causing disproportionate utility collapse would resolve this question.

## Limitations

- The UNPACT framework assumes stable token contribution mappings that may not generalize across different architectures or domains
- KEYTOKEN identification uses fixed thresholds determined on News dataset that may not transfer to other domains or languages
- The study conflates parametric knowledge retention with in-class learning capabilities when measuring recovery through prompt manipulation
- Results primarily apply to instruction-tuned models, leaving applicability to base models and other architectures uncertain

## Confidence

**High Confidence**: The empirical observations about recovery rates and destructive rates across different unlearning methods are well-supported by the presented experiments. The systematic comparison of six methods on three datasets provides robust evidence for the identified tradeoff between knowledge recoverability and catastrophic forgetting.

**Medium Confidence**: The interpretation of UNPACT results as evidence for keyword focus disruption is plausible but depends on untested assumptions about token contribution stability. While the method shows consistent patterns across experiments, alternative explanations have not been ruled out.

**Low Confidence**: The mechanism explanations for why unlearning works at a fundamental level remain speculative. The paper identifies symptoms but the causal mechanisms linking these observations to underlying model behavior changes are not definitively established.

## Next Checks

1. **Cross-Architecture Validation**: Test whether UNPACT findings generalize to different model families (e.g., Mistral, GPT models) and architectures (base vs. instruction-tuned, decoder-only vs. encoder-decoder). This would validate whether keyword focus disruption is a universal unlearning failure mode or specific to the studied models.

2. **Alternative Attribution Methods**: Replicate key experiments using different token attribution methods (SHAP, integrated gradients) to verify that UNPACT's perturbation-based approach captures the same phenomena. This would test the robustness of the KEYTOKEN identification process.

3. **Knowledge Tracing Experiments**: Design experiments to distinguish between parametric knowledge retention and in-context learning by testing recovery after prompt modifications that should prevent in-context learning (e.g., adding distracting context or changing prompt formats). This would validate whether FOCUSONKEY truly measures erased knowledge versus model adaptability.