---
ver: rpa2
title: Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?
arxiv_id: '2601.09433'
source_url: https://arxiv.org/abs/2601.09433
tags:
- data
- which
- training
- coins
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first application of Vision Transformer
  (ViT) architectures to the task of identifying semantic elements on ancient Roman
  coins, comparing their performance against traditional CNNs. The authors trained
  models to recognize eight common coin motifs (e.g., cornucopia, eagle, horse) using
  a large dataset of 100,000 coin images and descriptions.
---

# Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?
## Quick Facts
- arXiv ID: 2601.09433
- Source URL: https://arxiv.org/abs/2601.09433
- Reference count: 40
- CNNs vs. ViT: ViT models consistently outperformed CNNs, achieving accuracy scores 2-5% higher across most semantic elements.

## Executive Summary
This paper presents the first application of Vision Transformer (ViT) architectures to the task of identifying semantic elements on ancient Roman coins, comparing their performance against traditional CNNs. The authors trained models to recognize eight common coin motifs (e.g., cornucopia, eagle, horse) using a large dataset of 100,000 coin images and descriptions. ViT models consistently outperformed CNNs, achieving accuracy scores 2-5% higher across most semantic elements. The ViT models also demonstrated better generalization, particularly for rare motifs like Hercules. Saliency mapping revealed that CNNs focused on narrow regions while ViTs used more diverse visual cues. Training ViTs was more computationally expensive but yielded superior results. This work advances automated ancient coin analysis and demonstrates the potential of transformer architectures for fine-grained visual recognition tasks.

## Method Summary
The study compared ViT and CNN architectures for recognizing eight semantic elements on ancient Roman coin reverse sides. The dataset consisted of 100,000 coin images with multi-language text descriptions. Images were preprocessed to extract reverse sides and convert to grayscale, with 12.7% rejected. Labels were weakly assigned via keyword search across five languages. ViT models used ViT-Large architecture with ImageNet-21K pretraining, freezing all layers except the classification head. A single multi-label model was trained with weighted BCE loss. CNNs followed the Cooper & Arandjelović architecture, trained separately per concept with balanced data and cross-entropy loss. Both used SGD optimization with different learning rates.

## Key Results
- ViT models achieved 2-5% higher accuracy than CNNs across most semantic elements
- ViTs demonstrated superior generalization, especially for rare motifs like Hercules (0.83 vs 0.67 accuracy)
- Saliency maps showed ViTs used more diverse visual cues while CNNs focused on invariant small regions
- ViT training was more computationally expensive but yielded superior results

## Why This Works (Mechanism)
### Mechanism 1
- Claim: ViT's global self-attention enables learning spatially distributed features across the entire coin image from early layers, whereas CNNs require sufficient depth to build adequate receptive fields.
- Mechanism: Self-attention computes relationships between all patch embeddings simultaneously, allowing the model to associate distant visual elements (e.g., a figure and its associated attributes) without sequential propagation through convolutional layers.
- Core assumption: Ancient coin motifs contain meaningful long-range spatial dependencies that benefit from global context integration.
- Evidence anchors: [section 2.2.2] "ViTs can model long-range dependencies in their lowest layers"; [section 4.3.4-4.3.11] Saliency maps show ViT salient regions were "more diffuse" and "highly variable between images" while CNN models "tended to focus on a small region that was relatively invariant between image samples."

### Mechanism 2
- Claim: Pretraining on large-scale datasets (ImageNet-21K) transfers more effectively to ViT than CNN because ViT's weaker inductive bias requires more data to learn equivalent visual primitives.
- Mechanism: Frozen pretrained encoder layers provide general feature representations; only the classification head is fine-tuned, leveraging learned visual primitives while adapting to domain-specific classes.
- Core assumption: Features learned from natural images transfer meaningfully to stylized numismatic imagery despite significant domain shift.
- Evidence anchors: [section 3.2.2] "The ViT models that were trained in this work were initialized using weight parameters taken from a ViT model that was pretrained on ImageNet-21K"; [section 2.2.1] "Since Transformer needs a large amount of training data to outperform other models, transfer learning is typically used."

### Mechanism 3
- Claim: Training on unbalanced datasets with class weighting enables ViT to learn rare motifs better than undersampling approaches used for CNNs.
- Mechanism: Per-sample loss weighting gives rare positive examples disproportionate influence on gradient updates, allowing the model to learn from limited examples without discarding negative sample diversity.
- Core assumption: Rare classes contain learnable patterns that benefit from seeing more negative sample variation rather than artificial balance.
- Evidence anchors: [section 3.2.1] "To counter this data set imbalance during training, larger weights were applied to positive samples in the ViT model's per-concept loss functions"; [section 4.3.11] For Hercules, "the ViT model was trained on an unbalanced data set, containing far more negative samples, which may have contributed to its far superior performance" (0.83 vs 0.67 accuracy).

## Foundational Learning
- Concept: **Self-Attention and Multi-Head Attention**
  - Why needed here: Core differentiator between ViT and CNN architectures; explains why ViT can capture global relationships CNNs miss.
  - Quick check question: Can you explain why self-attention has O(n²) complexity and how patch embeddings reduce n for images?

- Concept: **Transfer Learning and Fine-Tuning Strategies**
  - Why needed here: The paper relies entirely on pretrained ViT; understanding frozen vs. trainable layers is critical for replication.
  - Quick check question: Why would freezing all layers except the classification head preserve useful features while still enabling domain adaptation?

- Concept: **Saliency Mapping for Model Interpretability**
  - Why needed here: Primary evidence for mechanism claims comes from comparing what each architecture attends to visually.
  - Quick check question: What does it mean when a model's saliency map highlights regions unrelated to the labeled concept?

## Architecture Onboarding
- Component map:
  Raw image -> Reverse-side extraction -> Greyscale conversion -> 16×16 patches -> Linear projection -> Positional embeddings + class token -> 12-layer Transformer encoder -> Linear classification head

- Critical path:
  1. Data preprocessing (Sections 3.1.1-3.1.2) — automatic reverse extraction and weak label assignment
  2. Model initialization with ImageNet-21K pretrained weights
  3. Freeze encoder, train head only with SGD + momentum (lr=10⁻⁴)
  4. Validate on stratified holdout; checkpoint best model per epoch

- Design tradeoffs:
  - **Patch size 16 vs. 32**: Smaller patches = more tokens = better accuracy but higher compute (chose 16)
  - **ViT-Large vs. Huge**: Large chosen for compute efficiency; Huge offered <1% accuracy gain
  - **Single multi-label model vs. one model per class**: Single model more efficient; enabled by frozen encoder

- Failure signatures:
  - **"Dying ReLU" in CNN**: CNN trained with Adam optimizer produced constant outputs; switched to SGD
  - **Noisy labels from descriptions**: Labels applied to whole image but descriptions reference both sides; causes systematic false positives/negatives
  - **GPU memory overflow in saliency mapping**: Required algorithm modification to merge loops and free intermediate memory

- First 3 experiments:
  1. Reproduce preprocessing pipeline on 1,000 images; manually verify reverse extraction accuracy and identify rejection patterns
  2. Train ViT on single concept (e.g., "eagle") with frozen encoder to validate fine-tuning pipeline before scaling to 8 concepts
  3. Compare saliency maps on identical test images between ViT and CNN to visually confirm attention pattern differences described in Section 4.3

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can preprocessing coin descriptions to isolate text relevant to the reverse side reduce label noise and improve model accuracy?
- Basis in paper: [explicit] The authors note that descriptions relate to both sides of the coin, causing "noisy" labels for reverse-only images, and suggest "filtering out parts that do not relate to the coin side of interest" as future work.
- Why unresolved: The current weak supervision method applies text describing the whole coin to an image of only the reverse, leading to many false negatives/positives.
- What evidence would resolve it: A comparison of model performance between the current dataset and a dataset cleaned via targeted text mining or "rev"/"obverse" boundary detection.

### Open Question 2
- Question: Can multi-modal Transformers improve semantic understanding by jointly learning from coin images and their associated text descriptions?
- Basis in paper: [explicit] The authors propose exploring multi-modal Transformers to "learn from both images and text" and potentially predict descriptions from images.
- Why unresolved: The current approach treats text merely as a source for binary labels (weak supervision), discarding the rich contextual relationships between the visual motifs and the unstructured descriptions.
- What evidence would resolve it: Implementing a multi-modal model (e.g., ViLBERT) and evaluating its performance on semantic retrieval or description generation tasks against the current ViT baseline.

### Open Question 3
- Question: Do specific Vision Transformer variants or hybrids offer a significant advantage over the standard ViT architecture for this specific domain?
- Basis in paper: [explicit] The authors state that since Dosovitskiy et al. proposed ViT, "many variants have been developed," and an evaluation of these was "outside the scope of this work."
- Why unresolved: The study only tested the original ViT, finding it performed similarly to prior state-of-the-art CNNs (Cooper and Arandjelović), leaving the potential of newer architectures unexplored.
- What evidence would resolve it: Benchmarking modern variants (e.g., Swin Transformer) on the Roman coin dataset to see if they surpass the accuracy plateau observed with the standard ViT.

## Limitations
- The CNN architecture's underspecification prevents faithful reproduction and direct architectural comparison
- Weak supervision from coin descriptions introduces systematic label noise that limits achievable accuracy
- Performance differences may be implementation-dependent rather than architecture-specific

## Confidence
- **High confidence**: ViT consistently outperforms CNN across most semantic elements (2-5% accuracy improvement, supported by Table 1)
- **Medium confidence**: Mechanisms explaining ViT superiority (self-attention global context, transfer learning benefits, class imbalance handling) — supported by saliency maps and hyperparameter choices but not experimentally isolated
- **Low confidence**: Exact performance gap magnitude — depends on CNN architecture fidelity which cannot be verified

## Next Checks
1. Train simplified CNN and ViT models with identical architecture (e.g., both using standard ResNet backbone) to determine whether observed performance differences are architecture-specific or implementation-dependent.

2. Manually annotate 100 randomly selected test images with ground truth semantic element presence to measure the gap between weak labels and actual content, establishing an upper bound on model performance.

3. Train ViT from scratch on the coin dataset versus using pretrained weights to quantify the contribution of ImageNet-21K initialization to the observed performance advantage.