---
ver: rpa2
title: A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete
  Optimization with Epistasis among Parameters
arxiv_id: '2504.21338'
source_url: https://arxiv.org/abs/2504.21338
tags:
- search
- local
- proposed
- population
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses black-box discrete optimization problems with
  epistasis among parameters, where multiple variables must be modified simultaneously
  for effective improvement. The authors propose a memetic algorithm that combines
  Variational Autoencoder (VAE)-based sampling with bit-flip local search.
---

# A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete Optimization with Epistasis among Parameters

## Quick Facts
- arXiv ID: 2504.21338
- Source URL: https://arxiv.org/abs/2504.21338
- Authors: Aoi Kato; Kenta Kojima; Masahiro Nomura; Isao Ono
- Reference count: 18
- Primary result: Proposed memetic algorithm combining VAE sampling with local search outperforms state-of-the-art methods on high-dimensional NK landscapes with epistasis

## Executive Summary
This paper addresses black-box discrete optimization problems where parameters exhibit epistasis, requiring simultaneous modification of multiple variables for effective improvement. The authors propose a memetic algorithm that leverages Variational Autoencoders (VAEs) for intelligent sampling combined with bit-flip local search refinement. The method demonstrates superior performance on NK landscapes with up to 1000 dimensions and varying epistasis levels, showing that VAE-guided sampling can effectively navigate complex variable dependencies that challenge traditional optimization approaches.

## Method Summary
The proposed method combines VAE-based sampling with First Improvement Hill Climber (FIHC) local search to solve black-box discrete optimization problems with epistatic interactions. The algorithm operates in two phases: first, a VAE is trained on promising solutions to learn the underlying variable dependencies; second, offspring are generated by sampling from the trained VAE and then refined through FIHC local search. This memetic approach leverages the VAE's ability to capture complex variable interactions for exploration while using local search for exploitation, addressing the challenge that epistatic problems require coordinated changes across multiple parameters rather than incremental single-variable modifications.

## Key Results
- Outperforms state-of-the-art methods (DSMGA-II, VAE-EDA-Q, P3, multi-start local search) on NK landscapes with N=300 and N=1000
- Achieved fitness improvements of 0.0042 to 0.0314 in mean fitness compared to best-performing baselines
- Demonstrated effectiveness across multiple epistasis levels (k=4,5,6)
- Shows particular advantage on high-dimensional problems where traditional methods struggle with variable dependencies

## Why This Works (Mechanism)
The method succeeds by addressing the fundamental challenge of epistasis through a two-pronged approach. VAEs learn the joint distribution of promising solutions, capturing the complex dependencies between variables that simple univariate models miss. This learned model then guides sampling toward promising regions of the search space that respect these dependencies. The subsequent local search phase refines these samples, ensuring exploitation of local structure. This combination allows the algorithm to escape the limitations of methods that assume independent variable contributions and to navigate the rugged fitness landscapes typical of epistatic problems.

## Foundational Learning

**Variational Autoencoders (VAEs)**: Generative models that learn to encode and decode data distributions, needed for capturing complex variable dependencies in promising solutions. Quick check: Verify the VAE learns meaningful representations by inspecting latent space structure.

**Epistasis**: The phenomenon where the effect of one variable depends on the state of other variables, needed to understand why traditional single-variable optimization fails. Quick check: Analyze variable correlation matrices to quantify epistatic strength.

**First Improvement Hill Climber (FIHC)**: A local search method that accepts the first improving move found, needed for efficient exploitation after VAE-guided exploration. Quick check: Compare FIHC against best-improvement variants on sample problems.

**NK Landscapes**: Benchmark problems with tunable epistasis (k parameter) and ruggedness, needed for controlled evaluation of optimization algorithms. Quick check: Verify landscape ruggedness metrics align with expected theoretical values.

## Architecture Onboarding

Component map: Problem instance -> Initial population -> VAE training -> VAE sampling -> FIHC refinement -> Offspring population -> Selection

Critical path: VAE training -> Sampling -> FIHC refinement -> Fitness evaluation

Design tradeoffs: VAE training overhead vs. improved sampling quality; exploration breadth vs. exploitation depth; model complexity vs. generalization capability

Failure signatures: Poor VAE training leading to unrealistic samples; local search getting trapped in local optima; premature convergence to suboptimal regions

First experiments:
1. Test on low-dimensional NK landscapes (N=50, k=2) to verify basic functionality
2. Compare VAE sampling against random sampling with FIHC to isolate VAE contribution
3. Analyze the impact of VAE architecture depth on optimization performance

## Open Questions the Paper Calls Out

None identified in the provided material.

## Limitations

- Restricted experimental scope to only NK landscapes with specific epistasis levels (k=4,5,6)
- Evaluation metrics focus primarily on mean fitness without statistical significance tests or confidence intervals
- Computational overhead of VAE training not thoroughly analyzed or compared against baseline methods

## Confidence

- Performance comparison claims: Medium confidence - Consistent improvement across multiple instances, but limited baseline diversity and lack of statistical validation
- VAE effectiveness for sampling: Medium confidence - Theoretically sound, but superiority over other generative approaches not demonstrated
- Scalability claims: Low confidence - Tested on N=1000 problems but lacks systematic scaling analysis or runtime comparisons

## Next Checks

1. Conduct statistical significance testing (e.g., Wilcoxon signed-rank test) on performance differences between proposed method and baselines across multiple runs
2. Test algorithm on additional problem types beyond NK landscapes, such as MAX-SAT or real-world combinatorial optimization benchmarks, to assess generalizability
3. Perform runtime analysis comparing total computational cost (including VAE training time) against baseline methods to evaluate practical efficiency tradeoffs