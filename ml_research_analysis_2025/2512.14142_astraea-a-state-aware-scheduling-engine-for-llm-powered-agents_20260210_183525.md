---
ver: rpa2
title: 'Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents'
arxiv_id: '2512.14142'
source_url: https://arxiv.org/abs/2512.14142
tags:
- uni00000014
- uni00000013
- uni00000011
- uni00000018
- scheduling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of existing LLM serving systems
  in handling agentic workflows, which alternate between local computation and external
  API calls. The key problem is that traditional schedulers optimize per-segment response
  times, leading to poor end-to-end Job Completion Time (JCT) and head-of-line blocking
  during long I/O waits.
---

# Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents

## Quick Facts
- arXiv ID: 2512.14142
- Source URL: https://arxiv.org/abs/2512.14142
- Reference count: 40
- One-line primary result: Reduces average Job Completion Time (JCT) by up to 25.5% for LLM agentic workflows by state-aware scheduling

## Executive Summary
This paper addresses the inefficiency of existing LLM serving systems in handling agentic workflows, which alternate between local computation and external API calls. The key problem is that traditional schedulers optimize per-segment response times, leading to poor end-to-end Job Completion Time (JCT) and head-of-line blocking during long I/O waits. To overcome this, the authors propose Astraea, a state-aware scheduling engine that optimizes global JCT by unifying a request's historical state with future predictions. Experimental results show that Astraea reduces average JCT by up to 25.5% compared to baseline methods, with superior stability and robustness under high load across different model scales.

## Method Summary
Astraea implements a stateful Multi-Level Feedback Queue (MLFQ) scheduler that dynamically classifies requests by their I/O and compute intensity. The system employs priority migration based on token-cost thresholds, uses Highest Response Ratio Next (HRRN) for intra-queue ordering, and includes an aging mechanism to prevent starvation. It also features an adaptive KV cache manager that balances latency and throughput under memory pressure by selecting between Preserve, Swap, and Discard strategies based on predicted API durations and system memory availability. The system is built on vLLM and uses a lifecycle-aware prediction module combining offline profiling, segment-level oracles, and category-based API latency statistics.

## Key Results
- Reduces average JCT by up to 25.5% compared to baseline methods
- Demonstrates superior stability and robustness under high load across different model scales
- Ablation studies confirm the effectiveness of the Stateful-MLFQ scheduling algorithm itself
- Shows negligible computational overhead in overhead analysis

## Why This Works (Mechanism)

### Mechanism 1: Stateful-MLFQ with Dynamic Priority Migration
- **Claim:** If requests are classified by their runtime behavior (I/O vs. Compute) and prioritized accordingly, global Job Completion Time (JCT) improves over myopic per-segment scheduling.
- **Mechanism:** The system implements a Multi-Level Feedback Queue (MLFQ). Requests start in a high-priority queue. They are demoted if they exhaust a "Token Cost" quota (indicating compute-heavy) and promoted if they yield for an API call before hitting the limit (indicating I/O-heavy). This prevents long API waits from blocking short compute tasks.
- **Core assumption:** The paper assumes that "Token Cost" is a more stable metric for scheduling granularity than time slices, which fluctuate with batch composition.
- **Evidence anchors:**
  - [abstract] "...dynamically classifies requests by their I/O and compute intensive nature..."
  - [section 4.3.2] "We adopt Token Cost instead of a time slice as the migration threshold... [which] solely measures the computational work a request has received."
  - [corpus] Related work (Autellix) focuses on program-level scheduling; Astraea differentiates by using prediction-based, proactive priority migration.
- **Break condition:** If the variance in API latency is extreme (unpredictable), the "I/O-intensive" classification may fail, causing priority inversion where a blocked request holds a high priority unnecessarily.

### Mechanism 2: Adaptive KV Cache Management
- **Claim:** If KV caches are managed dynamically based on memory pressure during I/O waits, the system can trade off single-request latency for higher system throughput without manual tuning.
- **Mechanism:** When a request triggers an API call, the manager selects a strategy (Preserve, Swap, Discard) by minimizing a "memory waste" cost function ($W$). If memory is low, it prefers Discard/Swap to free space for new requests; if high, it preserves to avoid recomputation latency.
- **Core assumption:** The system assumes accurate prediction of API duration ($T_{api}$) and Swap time ($T_{swap}$) to calculate the cost function correctly.
- **Evidence anchors:**
  - [abstract] "...adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure."
  - [section 4.4] "The system evaluates the potential waste for three candidate policies... choosing the one with the minimum cost."
  - [corpus] Corpus indicates memory pressure is a common bottleneck in LLM serving (SuperInfer, Apt-Serve), validating the need for adaptive policies over static allocation.
- **Break condition:** If the Swap I/O bandwidth is saturated (e.g., PCIe bottleneck), the predicted $T_{swap}$ will underestimate real latency, causing the "Swap" strategy to degrade performance more than "Discard."

### Mechanism 3: Lifecycle-Aware Prediction
- **Claim:** Scheduling effectiveness relies on predicting the duration of both local computation and external API calls, rather than just measuring historical compute time.
- **Mechanism:** A "Service Time Predictor" combines offline profiling for prefill, a segment-level oracle for generation length, and category-based statistical averages for API latency.
- **Core assumption:** The paper assumes API latencies follow stable distributions based on functional categories (e.g., "Image Generation" vs. "Math").
- **Evidence anchors:**
  - [section 4.2] "...we observe that APIs within the same functional category exhibit stable latency distributions."
  - [section 3.1] Defines the problem as optimizing over the sum of compute, API, and wait times.
- **Break condition:** If the distribution of API latencies has "heavy tails" (high variance within categories), the mean-based prediction will frequently misestimate the blocking time, leading to suboptimal scheduling.

## Foundational Learning

- **Concept: Multi-Level Feedback Queue (MLFQ)**
  - **Why needed here:** This is the structural backbone of Astraea. You must understand how processes move between queues based on "time slices" (or Token Costs) to grasp how Astraea prioritizes I/O vs. Compute.
  - **Quick check question:** If a request consistently uses its full time slice without blocking, does it move to a higher or lower priority queue in standard MLFQ? (Astraea modifies this logic).

- **Concept: KV Cache Offloading/Streaming**
  - **Why needed here:** Astraea manages the trade-off between keeping the KV cache in expensive GPU memory vs. swapping it to CPU. Understanding the bandwidth cost of this data movement is essential.
  - **Quick check question:** Why is "Discarding" a KV cache expensive for latency, even if it saves memory?

- **Concept: Highest Response Ratio Next (HRRN)**
  - **Why needed here:** Astraea uses HRRN for intra-queue sorting to prevent "starvation" of long tasks while still favoring short ones.
  - **Quick check question:** How does the HRRN score change as a request waits longer in the queue?

## Architecture Onboarding

- **Component map:** Request Pool -> Predictor -> Scheduler (Stateful-MLFQ) -> KV Cache Manager -> Inference Engine
- **Critical path:** The `BuildNextBatch` function in Algorithm 1. This is where priority aging is checked, the highest priority non-empty queue is selected, and segments are sorted by HRRN score before being packed into a batch.
- **Design tradeoffs:**
  - **Prediction Accuracy vs. Overhead:** The paper uses an oracle for token generation and category stats for APIs. In deployment, complex predictors may introduce latency.
  - **Responsiveness vs. Throughput:** The "Preserve" strategy minimizes latency for one request but risks Out-Of-Memory (OOM) errors that crash the system.
- **Failure signatures:**
  - **Starvation:** Long compute-heavy requests stuck in $Q_{m-1}$ never run. (Mitigated by the "Aging" mechanism).
  - **HoL Blocking:** The system stalls waiting for a long API call if the scheduler fails to preempt or context switch effectively.
  - **Cache Thrashing:** Rapid swapping of KV caches due to fluctuating memory pressure, reducing throughput to near zero.
- **First 3 experiments:**
  1.  **Baseline Reproduction:** Run the GPT-J/Vicuna experiments against vLLM-FCFS to reproduce the 25.5% JCT reduction (Fig 4/5).
  2.  **Ablation on Cache Manager:** Isolate the scheduler by fixing the KV cache policy to "Preserve" to measure the gains from Stateful-MLFQ alone (Fig 6).
  3.  **Stress Test:** Increase QPS until JCT spikes (knee of the curve) to find the breaking point of Astraea vs. baselines (Appendix C analysis).

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- The prediction system's accuracy under real-world noisy data is unverified, particularly for API latencies with heavy-tail distributions.
- The effectiveness of the Stateful-MLFQ parameters (token cost thresholds, aging threshold) may not transfer to workloads with different compute-to-I/O ratios.
- The full system overhead (CPU cycles, GPU memory beyond KV cache) is not quantified, and the cost of context switching during priority migration is not discussed.

## Confidence
- **High Confidence:** The core architectural contribution (Stateful-MLFQ with priority migration) is well-specified and the experimental methodology is reproducible.
- **Medium Confidence:** The adaptive KV cache manager is a novel contribution, but its effectiveness is heavily dependent on the accuracy of the underlying predictions and the stability of the memory pressure model.
- **Low Confidence:** The "Lifecycle-Aware Prediction" component is the weakest link, using oracles and pre-tabulated statistics not available for faithful reproduction.

## Next Checks
1. **Stress-Test API Latency Distributions:** Run Astraea under synthetic workloads where API latencies have high variance or heavy tails within categories. Measure the degradation in JCT and compare the scheduler's performance to a simpler First-Come-First-Served policy to quantify the cost of misprediction.

2. **Parameter Sensitivity Analysis:** Systematically vary the Stateful-MLFQ's token cost thresholds and aging threshold on a held-out test set of the Infercept dataset. Identify the range of parameters where Astraea outperforms baselines and where it breaks down, to understand the brittleness of the design.

3. **Overhead Profiling:** Instrument a complete deployment of Astraea (including the prediction engine and KV cache manager) to measure the total CPU and GPU memory overhead. Run a microbenchmark to quantify the latency introduced by context switching during priority migration and cache swapping.