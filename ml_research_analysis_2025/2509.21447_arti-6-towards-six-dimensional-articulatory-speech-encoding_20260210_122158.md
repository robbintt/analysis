---
ver: rpa2
title: 'ARTI-6: Towards Six-dimensional Articulatory Speech Encoding'
arxiv_id: '2509.21447'
source_url: https://arxiv.org/abs/2509.21447
tags:
- speech
- articulatory
- inversion
- arti-6
- vocal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARTI-6, a six-dimensional articulatory speech
  encoding framework that captures key vocal tract regions (lips, tongue tip/body,
  velum, tongue root, and larynx) using real-time MRI data. The framework combines
  articulatory inversion, predicting articulatory features from speech acoustics using
  speech foundation models with a correlation of 0.87, and articulatory synthesis,
  reconstructing intelligible speech from these low-dimensional features.
---

# ARTI-6: Towards Six-dimensional Articulatory Speech Encoding

## Quick Facts
- arXiv ID: 2509.21447
- Source URL: https://arxiv.org/abs/2509.21447
- Reference count: 0
- Primary result: Six-dimensional articulatory speech encoding achieving 0.87 correlation in inversion and ~12% WER in synthesis

## Executive Summary
ARTI-6 introduces a six-dimensional articulatory speech encoding framework that captures key vocal tract regions using real-time MRI data. The system combines articulatory inversion (predicting articulatory features from speech acoustics using speech foundation models) with articulatory synthesis (reconstructing speech from these low-dimensional features). By focusing on six key regions—lips, tongue tip, tongue body, velum, tongue root, and larynx—ARTI-6 offers a compact, interpretable alternative to high-dimensional speech representations while maintaining reasonable intelligibility and naturalness.

## Method Summary
ARTI-6 uses real-time MRI data from the USC-TIMIT corpus to extract six region-of-interest (ROI) pixel intensities as articulatory features. A speech foundation model (WavLM or HuBERT) is fine-tuned via LoRA adapters to predict these six-dimensional features from speech audio. The inversion model achieves 0.87 correlation with ground truth articulatory features. For synthesis, a HiFi-GAN vocoder conditioned on speaker embeddings from ECAPA-TDNN reconstructs speech from the predicted six-dimensional features, achieving word error rates around 12% and character error rates around 7% on the LibriTTS-R dataset.

## Key Results
- Inversion correlation of 0.87 between predicted and ground truth articulatory features
- Word error rate of approximately 12% and character error rate of approximately 7% in synthesis
- Mean opinion score of 3.947 for naturalness compared to ground truth score of 4.604
- Computationally efficient representation requiring only six dimensions versus hundreds in traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Speech foundation models encode articulatory-relevant information extractable via lightweight fine-tuning
- **Mechanism:** LoRA adapters in frozen pre-trained models aggregate conv and transformer hidden states, projecting to 6 articulatory dimensions via 1D-pointwise convolution
- **Core assumption:** Pre-trained speech representations contain latent articulatory structure correlating with vocal tract constrictions
- **Evidence anchors:** 0.87 prediction correlation; related work on articulatory inversion confirms ongoing interest but doesn't validate specific LoRA approach
- **Break condition:** If pre-training data lacks articulatory diversity, learned representations may not generalize to unseen configurations

### Mechanism 2
- **Claim:** Six ROI pixel intensities from midsagittal rtMRI serve as sufficient statistics for vocal tract state
- **Mechanism:** Hand-defined ROIs (LA, TT, TB, VL, TR, LX); pixel intensity within each ROI proxies constriction degree
- **Core assumption:** Selected six regions are minimally sufficient; other kinematic points highly correlate with these
- **Evidence anchors:** Coverage of velum, tongue root, larynx unlike EMA; related rtMRI work supports fuller coverage but doesn't prove six dimensions are sufficient
- **Break condition:** Noisy pixel intensity or inconsistent ROI placement degrades proxy measure, especially for velum and larynx

### Mechanism 3
- **Claim:** 6-dimensional articulatory trace generates intelligible speech via neural vocoding conditioned on speaker embeddings
- **Mechanism:** HiFi-GAN v1 upsamples 50Hz features to 16kHz audio; speaker embeddings from ECAPA-TDNN condition generator
- **Core assumption:** Speaker identity disentangled from articulatory content and injected via conditioning; upsampling ratios sufficient for temporal structure recovery
- **Evidence anchors:** HiFi-GAN architecture with upsampling rates [8,5,4,2]; WER ~12%, CER ~7%, MOS 3.947 for synthesis
- **Break condition:** Insufficient temporal resolution or failure to capture coarticulatory dynamics results in muffled or phonemically confusable synthesis

## Foundational Learning

- **Concept: Self-supervised speech representations (WavLM, HuBERT, Wav2Vec2, Whisper)**
  - Why needed here: Provide acoustic encoder backbone; understanding pre-training objectives helps diagnose articulatory information encoding
  - Quick check question: Can you explain why a model trained for ASR (Whisper) might encode different articulatory features than a model trained with contrastive self-supervision (Wav2Vec2)?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Fine-tunes large transformers efficiently by inserting low-rank matrices; understanding rank selection and target layers is critical
  - Quick check question: If LoRA rank is too low, what artifact might you observe in the predicted articulatory trajectories?

- **Concept: HiFi-GAN vocoding with multi-scale upsampling**
  - Why needed here: Maps low-frame-rate articulatory features to high-sample-rate audio; upsampling stack and adversarial training determine audio fidelity
  - Quick check question: What happens to phase coherence if the upsampling kernel sizes are mismatched to the input frame rate?

## Architecture Onboarding

- **Component map:** Speech audio (16kHz) -> Noise reduction (Miipher) -> Speech foundation model with LoRA adapters -> Linear aggregation -> 1D-conv projection -> 6D articulatory features (50Hz) -> HiFi-GAN vocoder with speaker embeddings -> 16kHz audio

- **Critical path:** 1) rtMRI ROI extraction → target articulatory features 2) Speech foundation model fine-tuning (L2 loss on articulatory features) 3) Pseudo-articulatory feature generation on LibriTTS-R using trained inversion model 4) HiFi-GAN training on (pseudo-articulatory, audio) pairs

- **Design tradeoffs:** Dimensionality vs. interpretability (6D compact but sacrifices some naturalness); ROI pixel intensity vs. hand-crafted features (simpler but noisier); single-speaker training vs. multi-speaker generalization (current model is speaker-specific)

- **Failure signatures:** Velum and larynx prediction correlation lower (~0.73-0.82) than lips/tongue (~0.90-0.95); noisy pixel intensity during velum closure; midsagittal rtMRI may miss internal larynx geometry

- **First 3 experiments:**
  1. Baseline correlation check: Train inversion on LSS dataset with each foundation model (Wav2Vec2, Whisper, HuBERT, WavLM); report Pearson correlation per ROI
  2. Ablate LoRA rank: Test rank ∈ {4, 8, 16, 32}; observe impact on correlation and training stability
  3. Synthesis intelligibility probe: Generate speech from ARTI-6 features on LibriTTS-R test-clean; compute WER/CER using Whisper-large-v3; compare against mel-spectrogram (80D) and EMA+pitch+loudness (14D) baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on pixel intensity within hand-defined ROIs introduces measurement noise, particularly for velum and larynx regions
- Currently validated on single speaker (S18 from USC-TIMIT), raising questions about inter-speaker generalization
- Six-dimensional representation may lack sufficient detail for full acoustic reproduction (MOS 3.95 vs 4.60 ground truth)

## Confidence
**High confidence:** Articulatory inversion mechanism and synthesis architecture are well-supported by correlation results and align with established literature
**Medium confidence:** Sufficiency of six ROI pixel intensities as complete articulatory representation, limited by noise and absence of diverse speaker validation
**Low confidence:** Claim that ARTI-6 provides "physiologically grounded" alternative, as six regions may not capture full articulatory space needed for all phonetic contexts

## Next Checks
1. Cross-speaker generalization test: Apply trained ARTI-6 inversion model to rtMRI data from different speakers in USC-TIMIT; measure correlation degradation and synthesis intelligibility
2. Phonetic context ablation: Generate speech using ARTI-6 features for sentences rich in velar stops, fricatives, and vowel-consonant transitions; compare WER/CER against ground truth, focusing on regions with lower prediction correlation
3. Dimensionality sensitivity analysis: Systematically vary number of ROIs (e.g., 4, 8, 12 regions) and measure trade-off between prediction correlation, synthesis intelligibility, and computational cost