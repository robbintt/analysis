---
ver: rpa2
title: 'Oogiri-Master: Benchmarking Humor Understanding via Oogiri'
arxiv_id: '2512.21494'
source_url: https://arxiv.org/abs/2512.21494
tags:
- prompt
- response
- humor
- features
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic study of humor in the Japanese
  creative response game Oogiri, aiming to understand what makes responses funny to
  humans. The authors construct a novel dataset, Oogiri-Corpus, which pairs prompts
  with approximately 100 diverse candidate responses rated independently by 100 judges
  each, addressing fairness and structural biases in existing datasets.
---

# Oogiri-Master: Benchmarking Humor Understanding via Oogiri

## Quick Facts
- **arXiv ID:** 2512.21494
- **Source URL:** https://arxiv.org/abs/2512.21494
- **Authors:** Soichiro Murakami; Hidetaka Kamigaito; Hiroya Takamura; Manabu Okumura
- **Reference count:** 0
- **Primary result:** A novel dataset and benchmark evaluating LLM humor understanding via Japanese Oogiri prompts, showing GPT-5 achieves near-human performance when augmented with humor-relevant linguistic features.

## Executive Summary
This paper presents a systematic study of humor in the Japanese creative response game Oogiri, constructing a novel dataset (Oogiri-Corpus) with ~100 independent human ratings per prompt to mitigate popularity bias. Through quantitative linguistic analysis, they identify features—such as brevity, ambiguity exploitation, and perspective shift—that correlate with funniness. Building on these insights, they introduce Oogiri-Master, a benchmark evaluating LLMs across relative and absolute judgment tasks. Experiments show that state-of-the-art models like GPT-5 approach human-level performance, that incorporating humor-relevant features improves accuracy, and that continued pretraining on Japanese data and careful prompt design further boost results.

## Method Summary
The authors construct Oogiri-Corpus from the Japanese Oogiri Sogo platform, filtering for prompts with ≥100 votes and pairing each with ~96 responses. Each prompt–response pair is independently rated by ~100 judges without visibility of others' ratings, reducing popularity bias. They extract a rich set of linguistic features: basic statistics (length, POS ratios), semantic/NLI embeddings, surprisal/nPMI scores, and LLM-scored higher-order features (e.g., ambiguity, perspective shift) via GPT-5. Oogiri-Master is a benchmark of 600 items across 5 tasks (4 MCQA relative judgment tasks, 1 binary absolute judgment), with accuracy as the metric. Models are evaluated with baseline prompts and insight-augmented prompts embedding precomputed feature values; GPT-5 achieves 70.7% accuracy with features versus 67.6% baseline.

## Key Results
- Oogiri-Master enables systematic evaluation of LLM humor understanding, with GPT-5 approaching human-level accuracy (~68.7% baseline).
- Incorporating humor-relevant linguistic features into prompts improves GPT-5 performance by ~3 points; weaker models may degrade due to over-reliance on feature magnitudes.
- Features such as shorter response length, ambiguity exploitation, and perspective shift show statistically significant differences (Cohen's d up to 0.5) between high- and low-rated responses.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Independent rating with a large pool of ~100 judges per prompt reduces popularity bias and enables more robust funniness aggregation than social-voting platforms where votes are visible.
- **Mechanism:** By hiding other judges' ratings during evaluation, each judgment is independent, reducing conformity effects; aggregating ~100 independent ratings yields a more stable funniness signal.
- **Core assumption:** Judges vote sincerely and have comparable interpretations of funniness; demographic mismatch between judges and target audience is limited.
- **Evidence anchors:**
  - [abstract]: "funniness is rated independently by approximately 100 human judges without access to others' ratings, reducing popularity bias and enabling robust aggregation."
  - [section] §3: "Unlike other platforms (e.g., Bokete), vote counts are not displayed during the voting phase, which helps to mitigate popularity bias and supports fairer evaluation."
  - [corpus]: No directly comparable mechanism validation found in neighboring papers.
- **Break condition:** If judges systematically differ from the target demographic or if independence is violated (e.g., external signals influence judgments), the aggregation no longer reflects intended funniness.

### Mechanism 2
- **Claim:** Certain quantifiable linguistic features—shorter length, ambiguity exploitation, perspective shift, incongruity resolution—correlate with higher funniness ratings and can be used to guide LLM evaluation.
- **Mechanism:** The authors extract measurable features (e.g., character counts, LLM-scored higher-order aspects) and find statistically significant differences between high- and low-rated responses via t-tests and Cohen's d.
- **Core assumption:** These features generalize beyond the analyzed dataset and are causally relevant, not merely correlates, to funniness.
- **Evidence anchors:**
  - [abstract]: "quantitative analysis of the linguistic factors associated with funniness, such as text length, ambiguity, and incongruity resolution."
  - [section] §4.4, Table 2: Perspective shift (d≈0.50), ambiguity exploitation (d≈0.42), incongruity resolution (d≈0.36), and shorter response length show statistically significant differences with small-to-medium effect sizes.
  - [corpus]: Related work on humor often references incongruity theory and perspective shifts, but direct quantitative replication in other corpora is limited.
- **Break condition:** If features are overfitted to this specific dataset or culture-specific humor (e.g., Japanese Oogiri), they may not transfer to other humor domains or languages.

### Mechanism 3
- **Claim:** Incorporating humor-relevant linguistic features into prompts improves LLM judgment accuracy for strong reasoners (e.g., GPT-5), while weaker models may over-rely on feature magnitudes and degrade.
- **Mechanism:** Insight-augmented prompts provide explicit feature values (basic and LLM-scored) alongside instructions, allowing capable models to weigh them appropriately; models with limited reasoning may misinterpret or overfit.
- **Core assumption:** LLMs can correctly interpret and selectively use feature information; instruction style (e.g., "consult only when uncertain") moderates over-reliance.
- **Evidence anchors:**
  - [abstract]: "insight-augmented prompting improves the model performance."
  - [section] §5.3.2, Table 3: GPT-5 improves from 67.6% to 70.7% with features; weaker models like LLM-jp-3.1-13bja degrade.
  - [section] §5.3.3, Table 5: Instructing to use features "only when uncertain" yields best performance (+3.1 points for GPT-5).
  - [corpus]: Neighboring papers (e.g., "Bridging the Creativity Understanding Gap") also report that small-scale human alignment can improve humor ranking, consistent with the value of explicit guidance, but do not directly test feature-augmented prompting.
- **Break condition:** If a model cannot reliably judge its own uncertainty or misinterprets feature semantics, the augmented prompt may add noise rather than signal.

## Foundational Learning

- **Concept: Popularity bias in social rating systems**
  - Why needed here: Understanding why visible vote counts distort aggregated funniness judgments is essential to appreciate the dataset design.
  - Quick check question: Why might hiding vote counts during rating lead to a more reliable funniness signal?

- **Concept: Effect size vs. statistical significance**
  - Why needed here: The analysis uses both p-values and Cohen's d; distinguishing statistical significance from practical importance is critical for interpreting which features matter.
  - Quick check question: If a feature has a significant p-value but Cohen's d < 0.2, what does that imply for its practical relevance?

- **Concept: Prompt engineering with explicit features**
  - Why needed here: The benchmark experiments show that how features are presented (baseline vs. insight-augmented vs. uncertain-only instructions) materially affects performance.
  - Quick check question: What risk arises when weaker models are given complex feature-augmented prompts?

## Architecture Onboarding

- **Component map:**
  - Oogiri-Corpus (prompt–response pairs with vote counts, filtered for ≥100 votes per prompt).
  - Feature extraction pipeline (basic linguistic, semantic/NLI, surprisal/PMI, LLM-scored higher-order).
  - Oogiri-Master benchmark (5 tasks: 4 MCQA relative judgment, 1 binary absolute judgment).
  - Evaluation harness (accuracy metrics, baseline vs. insight-augmented prompts, temperature=0 inference).

- **Critical path:**
  1. Ingest and filter raw platform data into Oogiri-Corpus.
  2. Extract and store linguistic features for each prompt–response pair.
  3. Construct benchmark items (positive/negative pairs per task).
  4. Run LLM inference with chosen prompting strategy.
  5. Aggregate accuracy across tasks and compare to human baseline.

- **Design tradeoffs:**
  - Feature complexity vs. model reasoning capability: richer features help strong models but hurt weaker ones.
  - Instruction style: direct use of features vs. uncertain-only consultation trades off clarity vs. over-reliance risk.
  - Dataset scale: requiring ≥100 votes per prompt improves reliability but reduces prompt count to 908.

- **Failure signatures:**
  - Models over-selecting very short responses when length is provided as a feature (suggesting over-reliance on heuristics).
  - Performance drop on insight-augmented prompts for smaller models (indicating instruction comprehension limits).
  - Large variance in human crowdworker judgments vs. platform users (demographic mismatch signal).

- **First 3 experiments:**
  1. Replicate the baseline vs. insight-augmented comparison on a held-out slice of Oogiri-Master to confirm feature utility for your target LLM.
  2. Ablate feature groups (basic only vs. LLM-scored only vs. both) to identify which category drives gains for your model.
  3. Test the "uncertain-only" instruction style and compare to direct feature-use instructions; monitor for over-reliance patterns (e.g., disproportionate selection based on a single feature).

## Open Questions the Paper Calls Out
None

## Limitations
- The ~68.7% human baseline may reflect demographic mismatch between crowdworkers and platform users, or genuine humor subjectivity.
- Effect sizes for linguistic features (Cohen's d ≤ 0.5) are statistically significant but small-to-medium, suggesting limited practical impact.
- Analysis is culture-specific (Japanese Oogiri), raising questions about cross-domain and cross-cultural generalization.

## Confidence
- **High confidence:** Independent rating design mitigates popularity bias; hiding vote counts is a well-established anti-conformity technique.
- **Medium confidence:** Linguistic features correlate with funniness but effect sizes are small-to-medium; causal relevance is unproven.
- **Medium confidence:** Feature-augmented prompting benefits strong reasoners but degrades weaker models; results may depend on prompt phrasing and model reasoning capacity.

## Next Checks
1. Replicate the insight-augmented prompting effect using a different SOTA model (e.g., GPT-4o) and verify whether performance gains persist and generalize to a held-out test slice.
2. Conduct a demographic survey of crowdworkers to quantify potential mismatch with platform users, and measure how this affects the human baseline relative to known high-voted responses.
3. Run ablation studies isolating each feature category (basic, semantic/NLI, surprisal/nPMI, LLM-scored) to determine which subset yields the most robust gains for strong reasoners and least degradation for weaker models.