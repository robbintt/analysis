---
ver: rpa2
title: 'Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review
  of NLP Approaches in Genomics, Transcriptomics, and Proteomics'
arxiv_id: '2506.02212'
source_url: https://arxiv.org/abs/2506.02212
tags:
- protein
- language
- sequences
- transformer
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review systematically examines the application of natural
  language processing (NLP) techniques to biological sequences across genomics, transcriptomics,
  and proteomics. It surveys a wide range of methods from classical word2vec embeddings
  to advanced transformer and Hyena architectures, detailing how each approach handles
  tokenization of biological sequences and their respective strengths and limitations.
---

# Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics

## Quick Facts
- arXiv ID: 2506.02212
- Source URL: https://arxiv.org/abs/2506.02212
- Reference count: 0
- This review surveys NLP methods for analyzing biological sequences across genomics, transcriptomics, and proteomics

## Executive Summary
This comprehensive review examines how natural language processing techniques have been applied to analyze biological sequences, treating DNA, RNA, and proteins as text. The authors systematically survey methods ranging from classical word2vec embeddings to advanced transformer and Hyena architectures, detailing how different tokenization strategies handle biological sequences and their respective strengths and limitations. The review demonstrates that NLP approaches can extract meaningful insights from large-scale genomic data, addressing tasks from protein structure prediction to gene expression analysis and regulatory element identification.

## Method Summary
The review systematically examines NLP approaches to biological sequences through a comprehensive literature survey, cataloging over 70 models across three main categories: genomics, transcriptomics, and proteomics. The methodology involves analyzing model architectures (transformers, Hyena operators, hybrids), tokenization strategies (character-level, k-mers, sub-word), and biological applications. The authors evaluate these approaches based on their ability to handle different sequence types, capture biological patterns, and address specific tasks like structure prediction, expression analysis, and functional annotation. The review synthesizes findings from individual studies while identifying gaps in current approaches and opportunities for future research.

## Key Results
- NLP methods effectively analyze biological sequences across genomics, transcriptomics, and proteomics
- Tokenization strategy significantly impacts model performance and what biological patterns can be detected
- Transformer models currently dominate applications, but Hyena and other efficient architectures show promise for long sequences
- Genomic context information provides valuable functional insights but remains underutilized in current models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Tokenization strategy conditions what biological patterns a model can access.
- **Mechanism:** Character-level tokens preserve position-specific information and enable single-mutation sensitivity, but produce long sequences computationally challenging for attention-based models. K-mer tokens capture local patterns but create vocabulary sparsity and, when overlapping, leak information about masked positions during pre-training. Sub-word methods (BPE, SentencePiece) learn variable-length tokens from data, potentially revealing motifs without pre-specified lengths.
- **Core assumption:** No "true" vocabulary exists for biological sequences, making tokenization a design choice rather than a discovery problem.
- **Evidence anchors:** [Page 7] "In the overlapping version, a sliding window of size k and stride t (commonly t=1) is used to create a sequence of k-mers. However, this approach can lead to data leakage since each k-mer contains information about adjacent tokens." [Page 7] "Sub-word tokenization creates tokens of variable length, making the training prediction tasks more challenging and potentially enhancing the model's performance."
- **Break condition:** If biological function primarily depends on structural motifs rather than sequential patterns, sub-word tokenization may not recover meaningful units without structural supervision.

### Mechanism 2
- **Claim:** Self-attention mechanisms may capture biologically meaningful long-range dependencies, but this is architecture-dependent and not universally demonstrated.
- **Mechanism:** Transformers compute pairwise attention across all positions, enabling models to learn dependencies between distant sequence elements. In proteins, this may correspond to residue-residue contacts in 3D structure. In DNA, this may capture enhancer-promoter interactions. Hyena operators use long convolutions with multiplicative gating to approximate attention at sub-quadratic cost, extending context length but potentially trading off precise pairwise modeling.
- **Core assumption:** Attention patterns correspond to biological interactions (structure, regulation) rather than statistical correlations alone.
- **Evidence anchors:** [Page 3] "This self-attention mechanism learns the representation for each position while 'attending to' every other position in the same sequence, thus helping to address the polysemy issue." [Page 11] "ESM-1b was shown to produce embeddings useful for remote homology detection, prediction of secondary structure, long-range residue-residue contacts, and mutational effects." [Page 10] "HyenaDNA was pre-trained on the human genome using a context length of up to a million tokens, offering long-range capabilities that were shown to benefit multiple downstream tasks."
- **Break condition:** If attention heads predominantly learn positional heuristics or local patterns, interpretability claims about "capturing biological interactions" overstate what the mechanism provides.

### Mechanism 3
- **Claim:** Genomic context embeddings can predict gene function beyond sequence alone, particularly in prokaryotes.
- **Mechanism:** Prokaryotic genomes organize functionally related genes into operons and conserved clusters. Models treating gene neighborhoods as "sentences" learn that genes appearing in similar contexts share functional annotations. This leverages conserved gene order from horizontal gene transfer and selection for co-regulation.
- **Core assumption:** Genomic context conservation reflects functional coupling across diverse organisms and that gene order encodes predictive signal beyond sequence homology.
- **Evidence anchors:** [Page 5] "Analogously, research has shown that the genomic context, i.e., the set of genes located near a given gene, provides crucial insights into the gene's function." [Page 12-13] "gLM demonstrated enhanced performance in enzyme classification compared to standard pLM embeddings, and its attention patterns were utilized for operon prediction."
- **Break condition:** If function-coupled gene clusters are rare outside specific bacterial lineages or specialized pathways, context-based predictions will not generalize to eukaryotes or poorly characterized taxa.

## Foundational Learning

- **Concept: Tokenization of biological sequences**
  - **Why needed here:** Unlike natural language, biological sequences lack explicit word boundaries. Choosing how to segment DNA/RNA/proteins into tokens determines vocabulary size, sequence length, and the patterns accessible to the model. This choice precedes all downstream architecture decisions.
  - **Quick check question:** Given a 1000-nucleotide DNA sequence, estimate the number of tokens under: (a) single-nucleotide tokenization, (b) non-overlapping 6-mers, (c) overlapping 6-mers with stride 1.

- **Concept: Context window vs. receptive field**
  - **Why needed here:** Biological tasks differ dramatically in their dependency ranges. Promoter detection may require ~100bp context; enhancer-promoter interactions span 10-100kb; protein folding involves residue contacts across the full sequence. Architecture choice (transformer, Hyena, CNN-transformer hybrid) must match the biological scale.
  - **Quick check question:** A standard transformer with 512 token context processes DNA at single-nucleotide resolution. What genomic feature might it fail to capture? What architectural modification could address this?

- **Concept: Pre-training objectives in biological LMs**
  - **Why needed here:** Masked language modeling (MLM) predicts masked tokens from context, learning bidirectional representations. Next-token prediction (autoregressive) learns generative models. The paper notes both are used, but their relative merits for different biological tasks remain unclear.
  - **Quick check question:** For a protein stability prediction task, would you prefer embeddings from an MLM-trained model (like ESM-2) or an autoregressive model (like ProtGPT2)? What tradeoffs might apply?

## Architecture Onboarding

- **Component map:** Tokenizer (character/k-mer/BPE) -> embedding lookup -> Encoder (Transformer blocks OR Hyena operators OR hybrid) -> Task heads (classification/regression/generation/retrieval) -> Training (pre-training on unlabeled sequences, fine-tuning on labeled task data)

- **Critical path:**
  1. Define biological task and identify required context window
  2. Select tokenization matching task granularity (single-nucleotide for variant effects, k-mers for motifs, BPE for unknown patterns)
  3. Choose architecture based on sequence length vs. computational budget
  4. Identify pre-trained checkpoint or determine pre-training corpus needs
  5. Design fine-tuning protocol with appropriate evaluation benchmark

- **Design tradeoffs:**
  - **Accuracy vs. efficiency:** Transformers are most validated but scale quadratically; Hyena/Mamba offer linear scaling with less empirical coverage
  - **Generality vs. specificity:** Multi-species pre-training (DNABERT-2, Nucleotide Transformer) generalizes better but may underperform on specialized tasks vs. domain-specific models
  - **Context vs. resolution:** Long context (HyenaDNA: 1M tokens) enables full-genome analysis but may dilute local signal; short context with dilated convolutions (Enformer: 100kb receptive field) offers a middle ground

- **Failure signatures:**
  - Tokenization mismatch: Model fails on sequences with insertions/deletions if trained only on substitutions; non-overlapping k-mers shift all positions after an indel
  - Context truncation: Predictions for distal regulatory elements fail when context window excludes enhancer regions
  - Vocabulary leakage: Overlapping k-mers with MLM pre-training show artificially high masking accuracy but poor generalization
  - Pre-training domain gap: Model trained on human genome underperforms on bacterial sequences, and vice versa

- **First 3 experiments:**
  1. **Tokenization ablation:** Take a pre-trained model (e.g., DNABERT-2) and evaluate on a fixed task (promoter prediction) using GUE benchmark. Re-tokenize the same data with different schemes (single nucleotide, 3-mers, 6-mers) and measure performance gap to isolate tokenization effects.
  2. **Context window sensitivity:** Using HyenaDNA (supports up to 1M context), evaluate the same task with progressively truncated input sequences. Plot performance vs. context length to identify the minimal sufficient context for your biological question.
  3. **Embedding probing:** Extract embeddings from a pre-trained pLM (e.g., ESM-2) at different layers. Train linear probes for structural properties (secondary structure, contact maps) and functional properties (enzyme class, GO terms). Compare which properties emerge at which layers to understand what the model has learned.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can genomic language models effectively integrate a gene's sequence with its broader genomic context to improve functional predictions?
- **Basis in paper:** [explicit] "Only a few models currently consider the genomic context of genes... Further research on the study of the 'genomic language' and the integration of a gene's sequence with its genomic context could be highly beneficial."
- **Why unresolved:** Most existing models treat individual genes or proteins as sentences, ignoring syntenic relationships and operon structures that provide crucial functional insights, particularly in prokaryotes.
- **What evidence would resolve it:** Development of models that outperform current pLMs on functional annotation tasks by jointly encoding sequence and genomic context, validated on diverse organisms.

### Open Question 2
- **Question:** What biological insights can be extracted from attention maps in DNA and genomic language models?
- **Basis in paper:** [explicit] "Similar studies on DNA and genomic language models are lacking. Analysis of attention maps from these models might provide insights into splicing or transcription signals in DNA sequences, or operons and regulatory elements in genome-based languages."
- **Why unresolved:** While attention analysis has yielded structural insights from protein LMs, systematic attention analysis for DNA/genomic LMs remains unexplored.
- **What evidence would resolve it:** Studies demonstrating that attention heads consistently attend to known regulatory motifs, splice sites, or operon boundaries, with predictive power comparable to established annotation methods.

### Open Question 3
- **Question:** Which efficient architectures (e.g., Hyena, Mamba, sparse attention) will prove most effective for processing long biological sequences compared to standard transformers?
- **Basis in paper:** [explicit] "Standard transformers currently remain the most widely used architecture in genomic NLP studies, highlighting the need to explore more efficient algorithms for biological applications."
- **Why unresolved:** Hyena and other sub-quadratic architectures show promise on limited tasks, but comprehensive evaluation across diverse biological tasks and sequence lengths is lacking.
- **What evidence would resolve it:** Systematic benchmarking of efficient architectures against standard transformers across tasks (gene expression, variant effects, structure prediction) with varying sequence lengths and computational budgets.

### Open Question 4
- **Question:** How can benchmark datasets be expanded to better evaluate genomic language models across diverse organisms and underrepresented biological tasks?
- **Basis in paper:** [explicit] "Future efforts should prioritize expanding these benchmarks to address currently underrepresented biological research areas and include a more diverse range of organisms... It is also critical to create benchmarking datasets tailored toward genomic LMs."
- **Why unresolved:** Current benchmarks (GUE, ProteinGym, PETA) focus primarily on human/bacterial data and specific task types, limiting assessment of generalizability.
- **What evidence would resolve it:** Curated benchmark suites spanning diverse taxonomic groups, genomic tasks (e.g., operon detection, regulatory syntax), and standardized evaluation protocols.

## Limitations
- Limited empirical comparison between different architectures (transformer vs. Hyena vs. hybrid) on identical tasks and datasets
- Insufficient evidence to determine optimal tokenization strategies for specific biological problems
- Lack of systematic validation that attention patterns actually capture biological dependencies rather than spurious correlations
- Limited exploration of domain adaptation challenges and generalizability across species

## Confidence
- **High Confidence:** The fundamental premise that NLP techniques can be applied to biological sequences is well-established. The review accurately catalogs existing approaches, tokenization strategies, and the diversity of biological applications.
- **Medium Confidence:** Claims about specific model performances are supported by the literature but lack systematic benchmarking across comparable datasets.
- **Low Confidence:** Assertions that attention patterns directly correspond to biological interactions are speculative and require additional experimental validation.

## Next Checks
1. **Architecture ablation study:** Select three representative tasks (promoter prediction, protein structure prediction, variant effect prediction) and systematically compare performance of transformer, Hyena, and hybrid architectures using identical tokenization and training protocols.

2. **Tokenization impact assessment:** Using a pre-trained transformer model (e.g., DNABERT-2), evaluate performance across three biological tasks using different tokenization schemes (single nucleotide, overlapping/non-overlapping k-mers, BPE) on the same datasets.

3. **Context window sufficiency analysis:** For HyenaDNA and comparable long-context models, systematically evaluate performance on distal regulatory element prediction by truncating input sequences at different lengths (10kb, 50kb, 100kb, full context).