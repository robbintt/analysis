---
ver: rpa2
title: Automatic Fine-grained Segmentation-assisted Report Generation
arxiv_id: '2507.16623'
source_url: https://arxiv.org/abs/2507.16623
tags:
- report
- segmentation
- features
- generation
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ASaRG, a method that enhances the LLaVA architecture
  for medical report generation by integrating intermediate visual features and fine-grained
  segmentation maps from specialist radiological models. This approach aims to improve
  both performance and explainability of AI-generated reports.
---

# Automatic Fine-grained Segmentation-assisted Report Generation

## Quick Facts
- arXiv ID: 2507.16623
- Source URL: https://arxiv.org/abs/2507.16623
- Reference count: 40
- Primary result: +2.77% CE F1 score improvement over LLaVA baseline on MIMIC-CXR dataset

## Executive Summary
This paper introduces ASaRG, a method that enhances the LLaVA architecture for medical report generation by integrating intermediate visual features and fine-grained segmentation maps from specialist radiological models. The approach concatenates these additional modalities into LLaVA's multi-modal projection layer, requiring minimal added parameters while achieving significant performance gains. Experiments on the MIMIC-CXR dataset show a +2.77% performance gain in CE F1 score compared to the LLaVA baseline, and a 6.28-6.98% improvement over existing methods like COMG and ORID. The use of segmentation maps also enables traceability of report content to specific image regions, enhancing groundedness.

## Method Summary
ASaRG extends LLaVA by adding two new input streams to the multi-modal projection layer: intermediate features from the LVM-Med segmentation model and fine-grained segmentation maps from the CXAS model. The method uses simple concatenation fusion to preserve pre-trained alignment while adding new information. Intermediate features are extracted from the final layer before LVM-Med's output head and embedded using a learnable class token. Segmentation maps (212 fine-grained classes) are pooled and convolved before fusion. The model employs a two-stage training approach: full fine-tuning in epoch 1, then projector-only training in epoch 2 with randomly initialized segmentation parameters. The approach is validated on the MIMIC-CXR dataset converted to VQA format.

## Key Results
- +2.77% CE F1 score improvement over LLaVA baseline (p<0.05)
- 6.28-6.98% improvement over existing methods (COMG and ORID)
- Concatenation fusion outperforms replacement and learned mixing alternatives
- 20 superclasses achieve nearly identical performance to 212 fine-grained classes (p=0.989)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenation-based fusion of multi-modal features outperforms alternative fusion strategies for medical report generation.
- Mechanism: Concatenation preserves pre-trained alignment between vision tower embeddings and the LLM embedding space while adding new information streams. Unlike learned mixing or replacement approaches—which disrupt this alignment and require costly re-alignment—concatenation allows the original projector layer to receive unmodified baseline features alongside new inputs, letting the LLM learn to leverage all sources without destroying pre-trained representations.
- Core assumption: The pre-trained LLaVA vision-language alignment contains valuable general visual-semantic knowledge that should not be discarded when adding domain-specific features.
- Evidence anchors:
  - [abstract]: "ASaRG proposes to fuse intermediate features and fine-grained segmentation maps created by specialist radiological models into LLaVA's multi-modal projection layer via simple concatenation."
  - [section 5, Results]: "Replacement and fusion via Learned Mixing do not work well for our purposes... Concatenation achieves the most significant performance boost by far (+0.87%, p=0.021), with the added benefit of maintaining any previous feature alignment."
  - [corpus]: Weak direct corpus support; related work on medical VLMs (MedVL-SAM2) focuses on different fusion paradigms.

### Mechanism 2
- Claim: Combining intermediate visual features (global) with fine-grained segmentation maps (local) provides complementary information that improves clinical efficacy.
- Mechanism: LVM-Med intermediate features encode condensed global representations from a specialist segmentation model, while CXAS segmentation maps provide explicit spatial local cues about 212 anatomical, pathological, and foreign object regions. The interaction between these sources—similar to skip connections in U-Net decoders—allows the model to leverage both coarse semantic understanding and fine-grained spatial localization.
- Core assumption: Specialist models (LVM-Med, CXAS) capture clinically relevant visual information that general-purpose vision encoders miss.
- Evidence anchors:
  - [abstract]: "The use of segmentation maps also enables traceability of report content to specific image regions, enhancing groundedness."
  - [section 3.1]: "Segmentation models are capable of decoding all relevant segmentations from an intermediate latent space representation of sufficient size."
  - [section 6.1]: "The remaining performance could be attributed to the interaction between intermediate features and full segmentation maps, similar to how skip connections in a U-Net decoder add additional performance."
  - [corpus]: Limited; neighboring papers don't directly evaluate this global-local complementarity mechanism.

### Mechanism 3
- Claim: Explicit segmentation map inputs enable post-hoc grounding verification of generated report content.
- Mechanism: Since segmentation maps are provided as explicit inputs during generation, downstream users can trace which report statements correspond to which anatomical/pathological regions. When the segmentation model agrees with the generated finding (e.g., both predict atelectasis), confidence increases; when they disagree, skepticism is warranted and the discrepancy can be used to identify model weaknesses.
- Core assumption: Segmentation maps provide interpretable, clinically meaningful spatial regions that correspond to reportable findings.
- Evidence anchors:
  - [abstract]: "The use of an arbitrary number of segmentations as part of the input demonstrably allows tracing elements of the report to the corresponding segmentation maps and verifying the groundedness of assessments."
  - [section 6.2]: Shows concrete examples where heart segmentation quality correlates with report trustworthiness, and where pathology segmentation disagreement flags skepticism.
  - [corpus]: No direct corpus validation; grounding verification is task-specific.

## Foundational Learning

- Concept: **LLaVA multi-modal projector architecture**
  - Why needed here: ASaRG is explicitly an extension of LLaVA's projector layer. Understanding that LLaVA maps vision features to LLM embedding space via a learned projector is essential before modifying it.
  - Quick check question: Can you explain what happens to image features between the vision encoder output and the LLM input in standard LLaVA?

- Concept: **Feature alignment in multi-modal models**
  - Why needed here: The paper's core finding is that concatenation preserves alignment while replacement/mixing disrupts it. Understanding why alignment matters explains the performance differences.
  - Quick check question: Why would replacing vision tower features with intermediate features from another model cause training instability?

- Concept: **Segmentation map representations**
  - Why needed here: The method uses 212 fine-grained segmentation maps pooled and convolved before fusion. Understanding how spatial masks become embeddings clarifies the information flow.
  - Quick check question: How does AdaptiveAveragePooling followed by 1D-convolution transform a spatial segmentation map into a sequence-compatible embedding?

## Architecture Onboarding

- Component map:
  - Image → Vision Tower → FI (vision features)
  - Image → LVM-Med → R (intermediate features) → Linear → Rstack (repeated along channel dim)
  - Image → CXAS → S (212 segmentation maps) → AdaptiveAveragePooling + Conv1D → Sloc
  - Learnable class embedding C + Rstack → Linear → RI
  - RI + Sloc → Linear → SI
  - Concatenate: CAT(FI, RI, SI) → Original Projector P → LLM

- Critical path:
  1. Image → Vision Tower → FI (vision features)
  2. Image → LVM-Med → R (intermediate features) → Linear → Rstack (repeated along channel dim)
  3. Image → CXAS → S (212 segmentation maps) → AdaptiveAveragePooling + Conv1D → Sloc
  4. Learnable class embedding C + Rstack → Linear → RI
  5. RI + Sloc → Linear → SI
  6. Concatenate: CAT(FI, RI, SI) → Original Projector P → LLM

- Design tradeoffs:
  - **Concatenation vs. Addition vs. Replacement**: Concatenation preserves alignment but increases sequence length; addition is parameter-efficient but may lose information; replacement fails entirely
  - **212 fine-grained classes vs. 20 superclasses**: Paper finds nearly identical performance (p=0.989), suggesting either (a) current segmentation quality limits usefulness, or (b) superclass aggregation removes noise without losing signal
  - **Two-stage training vs. full finetuning**: Freezing backbone reduces VRAM but may limit performance gains

- Failure signatures:
  - **Replacement divergence**: Model doesn't converge; intermediate features alone lack information for report generation
  - **Learned mixing instability**: Initial performance drops due to misalignment; slow recovery
  - **Segmentation VRAM overflow**: 212 full-size maps exceed GPU memory; requires freezing backbone and gradient accumulation

- First 3 experiments:
  1. **Baseline validation**: Run standard LLaVA-v1.5 on your MIMIC-CXR VQA split for 1-2 epochs; confirm you can reproduce ~0.378 CE F1 baseline
  2. **Feature-only concatenation**: Add LVM-Med intermediate features via concatenation; expect +0.89% CE F1 improvement (p≈0.012) with minimal parameter overhead
  3. **Segmentation map integration**: Add CXAS maps with two-stage training (epoch 1: all trainable; epoch 2: projector-only with new segmentation parameters); expect +2.77% CE F1 vs. baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ASaRG framework maintain performance gains when applied to different medical imaging modalities, specifically CT scans?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that it remains an "open question whether our method would achieve similar success on other report generation datasets or different modalities, such as CTs."
- Why unresolved: The study restricted all experiments exclusively to the MIMIC-CXR chest X-ray dataset.
- What evidence would resolve it: Successful implementation and evaluation of the ASaRG fusion strategy on a 3D volumetric dataset (e.g., a CT report generation benchmark).

### Open Question 2
- Question: Would specialized architectures like MAIRA benefit from ASaRG to the same degree as the baseline LLaVA model?
- Basis in paper: [explicit] The authors note it is "unclear whether all LLaVA offshoots will benefit... to the same degree," specifically citing MAIRA as a potential candidate for diminished returns due to its RAD-DINO encoder.
- Why unresolved: The study used a general LLaVA baseline, leaving the interaction with domain-specific vision encoders (which may already capture the features ASaRG adds) untested.
- What evidence would resolve it: Applying the ASaRG concatenation method to the MAIRA architecture and comparing the relative CE F1 score improvement against the LLaVA baseline.

### Open Question 3
- Question: Does the equivalence in performance between 212 fine-grained maps and 20 superclasses indicate a limit in model utility or a need for higher quality segmentation?
- Basis in paper: [inferred] In the Results section, the authors posit that the matching performance implies "either there is a limit to the usefulness of extremely fine-grained segmentations... or that the quality of the fine-grained segmentations still needs to improve."
- Why unresolved: The experiment showed no statistical difference between the two approaches (p=0.989), making the source of the benefit (granularity vs. noise reduction) ambiguous.
- What evidence would resolve it: Re-evaluating the model using a higher-accuracy segmentation model for the 212 classes to see if fine-grained performance surpasses the superclass aggregation.

## Limitations

- VRAM Constraints: The method requires substantial memory (212 segmentation maps × full resolution), necessitating frozen backbones and two-stage training rather than full fine-tuning.
- Data Availability: CXAS extension code and training data for the additional 54 pathological/foreign object classes are not publicly available, limiting exact reproduction.
- Evaluation Scope: Grounding verification relies on qualitative examples rather than systematic measurement of hallucination reduction or clinical utility.

## Confidence

- **High Confidence**: The concatenation fusion mechanism works better than alternatives (replacement, learned mixing) - directly demonstrated with p=0.021 statistical significance.
- **Medium Confidence**: The 212 fine-grained segmentation maps provide meaningful information beyond intermediate features - while the paper shows this works, the 20-superclass ablation suggests coarse-grained features may be sufficient.
- **Medium Confidence**: The grounding capability meaningfully improves interpretability - the mechanism is sound but validation is limited to qualitative examples rather than systematic user studies.

## Next Checks

1. **Statistical Robustness**: Perform k-fold cross-validation on MIMIC-CXR to verify the CE F1 improvement holds across different data splits, not just the single split used in the paper.
2. **Generalization Test**: Evaluate ASaRG on an external chest X-ray dataset (e.g., PadChest or CheXpert) to assess whether the segmentation-assisted approach generalizes beyond MIMIC-CXR.
3. **User Study**: Conduct a controlled study with radiologists comparing reports from LLaVA vs. ASaRG, measuring both diagnostic accuracy and trust calibration when segmentation maps are provided for verification.