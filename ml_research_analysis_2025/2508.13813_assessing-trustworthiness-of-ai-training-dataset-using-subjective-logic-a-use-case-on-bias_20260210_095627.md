---
ver: rpa2
title: Assessing Trustworthiness of AI Training Dataset using Subjective Logic --
  A Use Case on Bias
arxiv_id: '2508.13813'
source_url: https://arxiv.org/abs/2508.13813
tags:
- dataset
- trust
- uncertainty
- bias
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first formal framework for assessing
  the trustworthiness of AI training datasets using Subjective Logic. The method quantifies
  trust in global properties such as bias, fairness, and representativeness, particularly
  when evidence is incomplete or distributed.
---

# Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias

## Quick Facts
- arXiv ID: 2508.13813
- Source URL: https://arxiv.org/abs/2508.13813
- Reference count: 29
- Introduces first formal framework for assessing trustworthiness of AI training datasets using Subjective Logic

## Executive Summary
This paper introduces the first formal framework for assessing the trustworthiness of AI training datasets using Subjective Logic. The method quantifies trust in global properties such as bias, fairness, and representativeness, particularly when evidence is incomplete or distributed. It extends prior work focused on individual data trustworthiness to dataset-level properties that only emerge from collective data patterns.

The framework was instantiated on the property of bias and evaluated using a Traffic Sign Recognition dataset. Experimental results showed that the method successfully detected class imbalance, handled both centralized and federated contexts, and produced interpretable, uncertainty-aware trust assessments. Accuracy on warning signs improved from 95.19% to 95.40% with data augmentation, while trust belief increased from 0.50 to 0.64.

## Method Summary
The framework combines Subjective Logic with data augmentation and clustering techniques to quantify trust in training datasets. It uses a Dual-Step Strategy: Step 1 augments data via rotation, brightness, and affine transformations; Step 2 clusters augmented data to balance class representation. Trust is then computed using Subjective Logic to handle uncertainty in dataset properties. The method was tested on a Traffic Sign Recognition dataset, measuring trust belief, uncertainty, and distrust before and after augmentation. It also evaluates trust in federated settings by aggregating local evidence using two fusion methods.

## Key Results
- Trust belief increased from 0.50 to 0.64 with data augmentation
- Accuracy on warning signs improved from 95.19% to 95.40% with augmentation
- In federated settings, Method 1 showed sharper belief decreases under imbalance, while Method 2 offered gradual, evidence-weighted adjustments

## Why This Works (Mechanism)
The framework leverages Subjective Logic's ability to model uncertainty and incomplete evidence when assessing trustworthiness. By combining data augmentation and clustering, it balances class representation, reducing bias and increasing trust belief. The method adapts to both centralized and federated contexts by aggregating local evidence through fusion operators. This approach ensures that trust assessments are interpretable and robust to varying data distributions.

## Foundational Learning
- **Subjective Logic**: A probabilistic logic framework for reasoning under uncertainty; needed for modeling incomplete or distributed evidence in trust assessments; quick check: verify understanding of belief, disbelief, and uncertainty mass.
- **Data Augmentation**: Techniques like rotation, brightness adjustment, and affine transformations; needed to balance class representation and reduce bias; quick check: ensure augmented data maintains semantic consistency.
- **Federated Learning**: Decentralized model training across distributed datasets; needed to evaluate trust in scenarios with imbalanced local data; quick check: confirm aggregation methods preserve trust evidence.

## Architecture Onboarding
- **Component Map**: Data Augmentation -> Clustering -> Subjective Logic Trust Assessment -> Federated Aggregation (if applicable)
- **Critical Path**: Augmentation -> Clustering -> Trust Calculation -> Validation
- **Design Tradeoffs**: Augmentation increases trust belief but adds computational overhead; clustering improves balance but may introduce redundancy; Subjective Logic handles uncertainty but requires careful evidence aggregation.
- **Failure Signatures**: Sharp drops in trust belief indicate severe class imbalance; high uncertainty suggests insufficient evidence; federated aggregation failures may stem from poor local evidence weighting.
- **First Experiments**:
  1. Apply framework to a simple dataset (e.g., MNIST) to validate trust assessment mechanics.
  2. Test data augmentation techniques on a small-scale bias scenario.
  3. Evaluate federated aggregation with synthetic imbalanced datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on a single global property (bias) limits generalizability to other trustworthiness dimensions.
- Experimental scope is constrained to one dataset (Traffic Sign Recognition) and specific augmentation techniques.
- No benchmarking against alternative bias detection or trust quantification methods.

## Confidence
- Framework generalizability: **Medium** (limited to bias, needs validation on other properties)
- Accuracy improvements: **High** (clear before-and-after comparison with augmentation)
- Trust assessment improvements: **Medium** (limited dataset and evaluation conditions)

## Next Checks
1. Apply the framework to multiple trustworthiness properties (fairness, representativeness) across diverse datasets to test generalizability.
2. Compare Subjective Logic-based trust assessments against established bias detection metrics and alternative quantification approaches.
3. Evaluate the framework in real-world federated learning scenarios with naturally occurring data imbalances rather than synthetic distributions.