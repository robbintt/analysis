---
ver: rpa2
title: Tensorization is a powerful but underexplored tool for compression and interpretability
  of neural networks
arxiv_id: '2505.20132'
source_url: https://arxiv.org/abs/2505.20132
tags:
- tensor
- networks
- neural
- network
- tnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that tensorizing neural networks by
  reshaping dense weight matrices into higher-order tensors and approximating them
  using low-rank tensor network decompositions is a powerful yet underexplored approach
  for both model compression and interpretability. The authors highlight that tensorized
  neural networks (TNNs) introduce structured inductive biases, enable novel scaling
  directions (e.g., bond dimension inflation), and offer new interpretability avenues
  through "bond indices" that serve as latent feature channels.
---

# Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks

## Quick Facts
- arXiv ID: 2505.20132
- Source URL: https://arxiv.org/abs/2505.20132
- Authors: Safa Hamreras; Sukhbinder Singh; Román Orús
- Reference count: 40
- Key outcome: Tensorizing neural networks by reshaping dense weight matrices into higher-order tensors and approximating them using low-rank tensor network decompositions is a powerful yet underexplored approach for both model compression and interpretability.

## Executive Summary
This position paper argues that tensorizing neural networks by reshaping dense weight matrices into higher-order tensors and approximating them using low-rank tensor network decompositions is a powerful yet underexplored approach for both model compression and interpretability. The authors highlight that tensorized neural networks (TNNs) introduce structured inductive biases, enable novel scaling directions (e.g., bond dimension inflation), and offer new interpretability avenues through "bond indices" that serve as latent feature channels. TNNs can also be seamlessly combined with other compression methods like quantization and pruning, and they show promise for accelerating both forward and backward passes. The paper identifies key challenges—such as hardware limitations, hyperparameter complexity, and integration with other methods—and outlines future research directions toward fully tensorized networks.

## Method Summary
The method involves reshaping dense weight matrices into higher-order tensors and approximating them using low-rank tensor network decompositions. For fully connected layers, this typically means using Matrix Product Operators (MPO) or Tensor Train decompositions, while convolutional layers can be decomposed using Tucker or CP decompositions. The compression level is controlled by bond dimensions, which capture the correlation structure between weights. TNNs can be trained from scratch or applied to compress pretrained models. The paper also discusses "stack views" that interpret tensorized layers as sequences of sparse linear layers, offering interpretability insights through intermediate bond features.

## Key Results
- TNNs can compress neural networks with minimal performance loss by exploiting structured correlations in learned representations
- Bond indices introduce new latent spaces that serve as interpretable feature channels for mechanistic interpretability
- TNNs enable novel scaling directions like bond dimension inflation that differ from traditional model scaling approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank tensor network decompositions can compress neural network weights with minimal performance loss by exploiting structured correlations in learned representations.
- Mechanism: Dense weight matrices are reshaped into higher-order tensors (e.g., an m×n matrix into an 8-index tensor), then decomposed via iterative SVD into tensor networks (MPO/Tucker/CP). The bond dimensions capture "spatial" correlations between weights; small bonds imply strong compressibility.
- Core assumption: Weights of networks trained on structured real-world data inherit correlation structure amenable to low-rank tensor approximation. Assumption: This structure generalizes across modalities (text, images, audio) and architectures.
- Evidence anchors:
  - [abstract] "Tensorizing a neural network involves reshaping some or all of its dense weight matrices into higher-order tensors and approximating them using low-rank tensor network decompositions. This technique has shown promise as a model compression strategy."
  - [section 2, p.4] "Notably, pre-trained convolution kernels frequently exhibit low-rank structure under Tucker and CP decompositions [28]."
  - [corpus] Neighbor paper "Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks" focuses on tensorization for CNN compression, suggesting active empirical exploration; corpus FMR scores are moderate (avg=0.412) with low citation counts, indicating this is an evolving but not yet established area.
- Break condition: If bond dimensions must be inflated near-full-rank to maintain accuracy, compression benefit vanishes and computational overhead dominates.

### Mechanism 2
- Claim: Tensor network structure injects an inductive bias constraining weight correlations, which can improve training efficiency or accuracy when the bias aligns with data structure.
- Mechanism: The TN geometry (MPO, Tensor Ring, etc.) enforces constraints on which weight correlations are expressible. Small bond dimensions restrict correlations across hypercube dimensions; larger bonds allow more complex interactions. The "stack view" shows an MPO layer as a sequence of sparse fully-connected layers interleaved with identity operators.
- Core assumption: Real-world data exhibits structured correlations that gradient descent propagates into weight structure. Assumption: A well-chosen TN geometry a priori can match this structure better than dense unconstrained weights.
- Evidence anchors:
  - [section 3, p.5] "The inductive bias introduced by TNNs appears to have a different origin, not one rooted in obvious symmetries. Instead, it can be understood as resulting from constraint on the 'spatial' correlations between the weights."
  - [section 4, p.8] "Though well-understood in physics, the inductive bias of TNNs in deep learning is still poorly characterized."
  - [corpus] Weak direct corpus evidence on inductive bias characterization; neighbor papers focus on compression rather than theoretical bias analysis.
- Break condition: If the imposed correlation structure conflicts with the actual weight correlations needed for the task, performance degrades unless bond dimensions are inflated (negating the bias).

### Mechanism 3
- Claim: Bond indices in tensorized layers create interpretable latent channels that decompose layer-wise feature transformations into finer-grained, temporally-resolved stages.
- Mechanism: The "stack view" reinterprets an MPO layer as a composition of sparse linear layers. Bond indices become input/output dimensions of these internal layers. Multiple equivalent stacks exist via gauge transformations and matricization variants, offering different "temporal resolutions" of the same computation. These intermediate activations can be probed for interpretability.
- Core assumption: Bond-space representations develop task-relevant structure during training. Assumption: Different stack decompositions reveal complementary interpretability insights.
- Evidence anchors:
  - [abstract] "A central feature of TNNs is the presence of bond indices, which introduce new latent spaces not found in conventional networks."
  - [section 3, p.7-8] "These intermediate bond features can thus be viewed as a 'temporal' resolution of the total features output by the MPO layer... Studying how these bond features relate to the output features... may provide new tools for mechanistic interpretability."
  - [corpus] Neighbor "Tensorization of neural networks for improved privacy and interpretability" (FMR=0.57) suggests emerging interest in interpretability applications, but empirical validation remains limited.
- Break condition: If bond features remain entangled or semantically opaque despite analysis, interpretability claims do not materialize in practice.

## Foundational Learning

- Concept: **Tensor notation and contraction (einsum)**
  - Why needed here: The entire paper uses graphical tensor notation and describes TN layers as contracted tensor sets. Without understanding indices, bond vs. open indices, and contraction order, the mechanisms are opaque.
  - Quick check question: Given tensors A_{ij} and B_{jk}, write the contraction yielding C_{ik} and explain why contraction order affects FLOP count.

- Concept: **Singular Value Decomposition and low-rank approximation**
  - Why needed here: MPO decomposition is constructed via iterative SVD; bond dimensions relate to truncated singular values. Understanding compression requires grasping how SVD reveals and truncates correlation structure.
  - Quick check question: How does truncating small singular values in an SVD approximation affect reconstruction error vs. parameter count?

- Concept: **Inductive bias in deep learning**
  - Why needed here: The paper frames TN structure as a novel inductive bias source. Distinguishing this from symmetry-based biases (e.g., CNN translation equivariance) is central to the theoretical contribution.
  - Quick check question: Name two common inductive biases in CNNs and explain how they differ from the correlation-constraint bias of TNNs.

## Architecture Onboarding

- Component map:
  - Input: Dense weight matrix W (m×n) or convolution kernel K (x×y×w×h)
  - Reshape: Factor indices into multi-indices (e.g., m → m₁m₂m₃)
  - TN Layer: MPO/Tucker/CP decomposition with bond dimensions {χ}
  - Forward pass: Contract input with TN tensors (sequence matters for FLOPs)
  - Stack view: Interpret TN layer as composition of sparse FC layers (for interpretability)

- Critical path:
  1. Identify layer(s) to tensorize (start with large FC or conv layers)
  2. Choose TN architecture (MPO for FC; Tucker/CP for conv)
  3. Determine index factorization and initial bond dimensions
  4. Implement forward pass with efficient contraction ordering (use `opt_einsum`)
  5. Validate: compare dense vs. tensorized output, monitor accuracy loss

- Design tradeoffs:
  - Smaller bond dimensions → higher compression but risk underfitting
  - Larger bond dimensions → better expressivity but diminished compression
  - TN geometry choice (MPO vs. Tensor Ring vs. Tucker) trades parameter efficiency vs. correlation expressiveness
  - Training from scratch vs. post-hoc tensorization: former leverages native bias; latter is safer for pretrained models

- Failure signatures:
  - Accuracy collapse with aggressive bond truncation → bond dimensions too small for task complexity
  - Slower inference than dense baseline → poor contraction ordering or unnecessary densification
  - Training instability with local activations → naive ReLU application zeros useful components prematurely

- First 3 experiments:
  1. **Baseline tensorization**: Take a pretrained MLP or CNN, tensorize one FC/conv layer via MPO/Tucker with moderate bond dimensions, measure compression ratio and accuracy drop.
  2. **Contraction benchmark**: Implement two contraction orderings for the same TN forward pass; profile FLOPs and wall-clock time on CPU vs. GPU.
  3. **Bond sensitivity sweep**: Systematically vary bond dimensions for a single layer and plot accuracy vs. parameter count to identify the knee point for your task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the TNN inductive bias generally useful across diverse tasks, or is it limited to specific problem domains?
- Basis in paper: [explicit] Listed in "Open Questions" section (p. 9): "Is the TNN inductive bias generally useful, or limited to specific tasks?"
- Why unresolved: The paper notes the inductive bias of TNNs in deep learning is "still poorly characterized" (p. 8), with unclear origins unlike symmetry-based biases in CNNs.
- What evidence would resolve it: Systematic benchmarking of TNNs across diverse tasks (vision, language, audio, tabular) with controlled comparisons to dense baselines, identifying where tensorization consistently helps or hurts.

### Open Question 2
- Question: Can optimal tensor network decompositions be inferred directly from the structure of training data and task?
- Basis in paper: [explicit] Listed in "Open Questions" section (p. 9): "Is it possible to infer optimal tensor decompositions directly from the structure of the data and task?"
- Why unresolved: Current selection of TN architectures and bond dimensions "is challenging and often done heuristically" (p. 9) without theoretical guidance.
- What evidence would resolve it: Methods that analyze data correlation structure and predict effective TN geometries/ranks before training, validated against empirical performance.

### Open Question 3
- Question: How should tensorization be optimally combined with quantization and pruning for maximum compression?
- Basis in paper: [explicit] Listed in "Open Questions" section (p. 9): "How should tensorization be combined with quantization or pruning? For instance, what is the best way to quantize MPO layers?"
- Why unresolved: "Integrating them remains difficult. Most quantization methods target dense weights and don't generalize well to tensorized formats" (p. 9).
- What evidence would resolve it: Systematic studies comparing ordering strategies (quantize-then-tensorize vs. tensorize-then-quantize) with quantization schemes specifically designed for TN bond indices.

### Open Question 4
- Question: Can "fully tensorized neural networks" with tensorized activations and local nonlinearities match dense network performance?
- Basis in paper: [inferred] The paper proposes fully tensorized networks (p. 9) but notes naive local activations "can lead to unstable gradients and harm training" (p. 9, footnote 9).
- Why unresolved: Standard nonlinearities operate pointwise on dense tensors and are incompatible with TN representations; local alternatives remain unexplored.
- What evidence would resolve it: Demonstrating training dynamics and final performance of fully tensorized networks with designed local activation functions across standard benchmarks.

## Limitations

- Lack of standardized evaluation protocols for tensorized networks makes comparison difficult
- Performance highly sensitive to bond dimension selection and contraction ordering choices
- Limited empirical evidence for interpretability benefits beyond theoretical framing
- Hardware constraints and poor tensor operation optimization in existing frameworks limit practical deployment

## Confidence

- **High** confidence: Core claim that tensorization offers a powerful compression tool, grounded in established tensor network theory
- **Medium** confidence: Assertion that tensorization enables novel interpretability via bond indices, mechanism is clear but practical validation limited
- **Low** confidence: Claim about achieving significant speedups due to unresolved contraction ordering challenges and hardware constraints

## Next Checks

1. Benchmark contraction ordering algorithms on tensorized layers across different hardware (CPU/GPU/TPU) to identify optimal paths for various TN architectures
2. Systematically evaluate bond dimension sensitivity across multiple tasks and architectures to establish practical guidelines for compression-accuracy tradeoffs
3. Design interpretability experiments that probe bond-space representations in trained TNNs and compare against traditional feature visualization methods