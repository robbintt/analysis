---
ver: rpa2
title: Translating the Rashomon Effect to Sequential Decision-Making Tasks
arxiv_id: '2512.17470'
source_url: https://arxiv.org/abs/2512.17470
tags:
- rashomon
- policies
- policy
- identical
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Rashomon effect describes multiple models making identical
  predictions while differing internally. While extensively studied in classification,
  it remained unexplored in sequential decision-making, where policies learn to achieve
  objectives through actions in stochastic environments.
---

# Translating the Rashomon Effect to Sequential Decision-Making Tasks

## Quick Facts
- **arXiv ID**: 2512.17470
- **Source URL**: https://arxiv.org/abs/2512.17470
- **Reference count**: 31
- **Key outcome**: Demonstrates the Rashomon effect exists in sequential decision-making: 82 out of 100 trained policies produced identical induced DTMCs for completing 5 jobs in a taxi environment while exhibiting different feature importance rankings.

## Executive Summary
This paper translates the Rashomon effect from classification to sequential decision-making tasks, where multiple policies can produce identical observable behavior while differing in internal structure. The authors define this as multiple policies generating identical induced discrete-time Markov chains (DTMCs) for a specified property, verified through probabilistic model checking, while exhibiting different feature attributions. Experiments in a taxi environment demonstrate that 82% of trained policies formed such a behavioral equivalence class, and a permissive ensemble constructed from this set maintained 100% success rate under distribution shifts while drastically reducing the state space from 72,064 to 6,209 states.

## Method Summary
The authors trained 100 neural network policies via behavioral cloning on an expert dataset extracted from an optimal policy in a taxi environment. They used probabilistic model checking to construct induced DTMCs for each policy and verify behavioral equivalence for completing 5 jobs. Feature importance rankings were computed using gradient-based saliency maps to identify internal structural differences. The permissive ensemble policy was created by allowing any action selected by at least one member of the Rashomon set, maintaining performance while reducing computational complexity.

## Key Results
- 82 out of 100 trained policies produced identical induced DTMCs for completing 5 jobs
- Policies within the Rashomon set exhibited significantly different feature importance rankings
- Permissive ensemble maintained 100% success rate under distribution shifts while reducing state space from 72,064 to 6,209 states
- Ensemble outperformed random selection of individual policies and saved time compared to sequential model checking

## Why This Works (Mechanism)

### Mechanism 1: Behavioral Equivalence via Induced DTMCs
- **Claim:** Multiple distinct neural policies may produce exactly identical observable behavior in stochastic environments, forming a behavioral equivalence class.
- **Mechanism:** Probabilistic model checking resolves policy-environment interactions into an induced Discrete-Time Markov Chain (DTMC). If two policies select the same action in every reachable state relative to a specific property, they generate isomorphic DTMCs despite different neural weights.
- **Evidence anchors:** The abstract defines the Rashomon effect as "multiple policies that exhibit identical behavior... visiting the same states and selecting the same actions." Section 4.2 explains how COOL-MC constructs the induced Markov chain for precise comparison.

### Mechanism 2: Internal Divergence via Feature Attribution
- **Claim:** Policies within a behavioral equivalence class can rely on significantly different internal feature hierarchies to select identical actions.
- **Mechanism:** Gradient-based saliency maps aggregate feature importance scores across the dataset. Even if Policy_A and Policy_B output the same action in a state, they may rank different features as most important.
- **Evidence anchors:** The abstract mentions "...differing in their internal structure, such as feature attributions." Table 1 in section 5.2 shows policies with identical attribution patterns share the same row color while others differ internally.

### Mechanism 3: Robustness & Efficiency via Permissive Ensembles
- **Claim:** A permissive policy derived from the Rashomon set can maintain optimality under distribution shift while drastically reducing state space.
- **Mechanism:** A permissive policy allows any action selected by at least one member of the Rashomon set, creating an induced MDP that encompasses the union of robust behaviors. This MDP is smaller than the full environment model because it prunes states unreachable by the good policies.
- **Evidence anchors:** The abstract states "A permissive ensemble policy... maintained 100% success rate... while reducing state space from 72,064 to 6,209 states." Section 5.2 confirms this approach saves time compared to sequential model checking.

## Foundational Learning

- **Concept:** Induced Discrete-Time Markov Chain (DTMC)
  - **Why needed here:** This is the ground truth for "behavior" in this paper. You cannot verify the Rashomon effect without understanding how a policy freezes an MDP's non-determinism into a specific chain of probabilistic states.
  - **Quick check question:** If Policy A and Policy B both have a 100% success rate, do they necessarily have the same induced DTMC? (Answer: No, they might take different paths to the goal).

- **Concept:** Probabilistic Computation Tree Logic (PCTL)
  - **Why needed here:** PCTL is the language used to define "identical behavior." You need this to specify what property the policies must satisfy.
  - **Quick check question:** How would you express "The system eventually reaches a collision state with probability < 0.01" in this context?

- **Concept:** Behavioral Cloning (BC)
  - **Why needed here:** The paper uses BC to generate the Rashomon set. Understanding BC explains why the policies differ internally despite seeing the same data.
  - **Quick check question:** Why is BC sufficient for this study rather than online Reinforcement Learning? (Answer: The study requires fixed training data to isolate the Rashomon effect).

## Architecture Onboarding

- **Component map:** Expert Source (Storm) -> Dataset (State-Action pairs) -> Learners (100 Neural Networks) -> Verifier (COOL-MC) -> Divergence Meter (Saliency Maps) -> Synthesis (Permissive Policy Builder)
- **Critical path:** Generate expert data → Train policy population → Verify DTMCs (Filter for behavioral equivalence) → Compute Saliency (Filter for internal divergence) → Construct Permissive Ensemble
- **Design tradeoffs:**
  - Exactness vs. Scalability: Using PMC gives formal guarantees but struggles with massive state spaces; the Permissive Policy mitigates this but requires a valid Rashomon set first.
  - Metric Sensitivity: The definition of the Rashomon set depends on the chosen internal metric (e.g., saliency). Different metrics might yield different sets.
- **Failure signatures:**
  - False Negative: Policies are discarded because the PMC engine runs out of memory before verifying the property.
  - False Positive (Behavior): Policies appear identical on a simple property (5 jobs) but diverge on harder shifts (10 jobs) if not ensemble.
  - False Positive (Structure): Saliency maps show different rankings, but they are just noise; the networks are effectively functionally identical.
- **First 3 experiments:**
  1. Validation Run: Train 10 policies; verify if they achieve Probability 1.0 for the base property. (Confirms setup).
  2. Equivalence Check: Pick 2 successful policies and compare their induced DTMC state counts. (Checks for behavioral identity).
  3. Internal Check: Run saliency maps on the two behaviorally identical policies. (Confirms or denies Rashomon effect).

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the Rashomon effect manifest in multi-agent sequential decision-making systems?
  - **Basis in paper:** "Future work could investigate the Rashomon effect in multi-agent systems"
  - **Why unresolved:** The current work only addresses single-agent settings. Multi-agent systems introduce strategic interactions, non-stationarity, and joint policy spaces that fundamentally change how behavioral equivalence and internal structure divergence might interact.

- **Open Question 2:** How does the choice of internal structure metric affect which policies belong to the Rashomon set?
  - **Basis in paper:** "investigate how different internal structure metrics affect Rashomon set composition" and "The choice of metric may influence which policies are included in the Rashomon set, as two policies might appear identical under one metric but differ under another."
  - **Why unresolved:** Only saliency-based feature attributions were tested; the methodology is claimed metric-agnostic but sensitivity to metric choice remains unquantified.

- **Open Question 3:** What determines whether internal differences manifest as robustness or fragility under specific types of distribution shifts?
  - **Basis in paper:** "explore how differences in feature attribution relate to specific types of distribution shifts"
  - **Why unresolved:** The paper demonstrated divergence under one shift type (increasing job count) but did not characterize which feature attribution patterns predict robustness to which shift types.

## Limitations
- Claims are contingent on accurate MDP modeling and effective saliency metrics
- Use of behavioral cloning limits generalizability to online RL scenarios where policies actively explore and update
- Rashomon set definition depends heavily on the chosen internal metric (saliency)
- State space explosion remains a fundamental challenge for probabilistic model checking

## Confidence
- **High Confidence**: The existence of behavioral equivalence classes in sequential decision-making (verified through induced DTMCs)
- **Medium Confidence**: The claim that internal feature hierarchies differ within equivalence classes (based on saliency map analysis)
- **Medium Confidence**: The efficiency and robustness benefits of permissive ensembles under distribution shift (demonstrated in specific Taxi environment)
- **Low Confidence**: Generalization to more complex environments or different RL paradigms (not tested)

## Next Checks
1. **Metric Sensitivity Analysis**: Apply alternative XAI methods (e.g., Integrated Gradients, SHAP) to verify if the same internal divergence is detected across different attribution techniques.
2. **Robustness Stress Test**: Systematically vary the distribution shift (e.g., different job counts, dynamic environment changes) to evaluate the permissive policy's performance boundaries.
3. **Architecture Ablation Study**: Vary the neural network architecture (depth, width, activation functions) during behavioral cloning to determine if the Rashomon effect persists across different model families.