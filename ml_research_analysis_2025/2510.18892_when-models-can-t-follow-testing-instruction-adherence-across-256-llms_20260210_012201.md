---
ver: rpa2
title: 'When Models Can''t Follow: Testing Instruction Adherence Across 256 LLMs'
arxiv_id: '2510.18892'
source_url: https://arxiv.org/abs/2510.18892
tags:
- instruction-following
- evaluation
- instruction
- output
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a focused evaluation framework using 20 diagnostic
  prompts to assess instruction-following capabilities across 256 verified large language
  models. The methodology involves verifying model functionality before evaluation
  and employs objective, verifiable success criteria for each prompt.
---

# When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs

## Quick Facts
- **arXiv ID:** 2510.18892
- **Source URL:** https://arxiv.org/abs/2510.18892
- **Authors:** Richard J. Young; Brandon Gillins; Alice M. Matthews
- **Reference count:** 37
- **Primary result:** String manipulation tasks show near-universal failure (2.7% success) while constraint compliance tasks achieve 66.9% success, revealing fundamental gaps in instruction-following capabilities

## Executive Summary
This study presents a focused evaluation framework using 20 diagnostic prompts to assess instruction-following capabilities across 256 verified large language models. The methodology involves verifying model functionality before evaluation and employs objective, verifiable success criteria for each prompt. The comprehensive evaluation reveals significant variability in instruction-following performance, ranging from 0% to 100% across models, with an overall pass rate of 43.7%. String manipulation tasks proved particularly challenging (12.0% average success), while constraint compliance tests showed the highest success rate (66.9%). Provider-level analysis revealed performance ranging from 33.3% to 79.3%, with x-ai models achieving the highest average (79.3%). The evaluation framework offers a practical diagnostic tool that efficiently identifies specific instruction-following weaknesses across the contemporary LLM landscape.

## Method Summary
The evaluation framework employs 20 diagnostic prompts targeting sequential execution, format compliance, and constraint satisfaction. Each prompt has objective success criteria defined by strict matching rules with semantic equivalence allowances. The study verified 256 endpoints by confirming basic functionality (capital of France question) before evaluation, eliminating 22.7% of candidates. Models were tested using OpenRouter API with standardized parameters (temperature=0.0, max_tokens=150, seed=42 where supported) and a 10-second timeout. Two-stage verification checked for strict output matching first, then semantic equivalence for acceptable variations. Results were aggregated by model, provider, and task type to identify performance patterns and failure modes.

## Key Results
- String manipulation tasks achieved only 12.0% average success rate, with Test 5 (String Manipulation Chain) at 2.7% pass rate
- Constraint compliance tests showed highest success at 66.9%, with Test 2 (exact output compliance) at 96.1%
- Provider performance ranged from 33.3% to 79.3%, with x-ai models achieving highest average (79.3%)
- Overall instruction-following success rate was 43.7% across all models and tasks
- Mathematical operations performed better (44.9% success) than string manipulation despite higher computational complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** String manipulation tasks fail at higher rates than mathematical operations because tokenization misaligns with character-level operations.
- **Mechanism:** Modern tokenizers operate at subword/word granularity, not character granularity. When models must reverse characters, remove specific letters, or apply character-level transformations, they cannot directly access the atomic units being manipulated. Multi-step string operations compound this—each transformation introduces error that propagates.
- **Core assumption:** Tokenization strategy is the primary bottleneck, not reasoning capacity.
- **Evidence anchors:** Test 5 (String Manipulation Chain) proved to be the hardest test with only a 2.7% pass rate; String manipulation averaged 12.0% success vs. mathematical operations at 44.9%; Neighbor paper notes models struggle with "compositions" of simple instructions.

### Mechanism 2
- **Claim:** Constraint compliance shows binary outcomes because constraint-awareness is a training property, not an emergent capability.
- **Mechanism:** Models either learned to check outputs against constraints during instruction-tuning or they didn't. The paper observes that models "succeeded completely or failed catastrophically, with few intermediate cases." This suggests constraint-satisfaction is not something models discover at inference time—it must be encoded in training.
- **Core assumption:** Constraint-checking is not recoverable through scale alone; it requires explicit training signal.
- **Evidence anchors:** Successful models appeared to implement robust checking mechanisms, while failing models showed no awareness of the constraint violation; Test 2 (exact output compliance) achieved 96.1% pass rate—binary distribution; Weak/missing neighbor papers don't directly address constraint binary outcomes.

### Mechanism 3
- **Claim:** Provider-level performance variation reflects differences in instruction-tuning methodology, not just base model capability.
- **Mechanism:** The 46-percentage-point spread between providers (33.3% to 79.3%) cannot be explained by architecture alone. Instruction-following requires specific training on verifiable constraints. Providers prioritizing instruction-tuning (x-ai at 79.3%, Anthropic at 56.8%) outperform those focused on other objectives.
- **Core assumption:** Instruction-tuning data quality and methodology vary substantially across providers.
- **Evidence anchors:** x-ai models achieved the highest mean success rate (79.3% across 7 models); instruction-tuned variants consistently outperformed their base model counterparts; "The Instruction Gap" paper confirms enterprise deployment reveals "inconsistent adherence to custom instructions."

## Foundational Learning

- **Concept: Verifiable instructions**
  - **Why needed here:** The entire framework depends on instructions with objective success criteria. Without verifiability, you cannot automate evaluation at scale.
  - **Quick check question:** Can you write a test for "be creative" that a script can grade? If no, it's not verifiable.

- **Concept: Tokenization misalignment**
  - **Why needed here:** Understanding why "simple" string operations (2.7% success) fail harder than "complex" math (44.9% success) requires knowing that models don't see characters—they see tokens.
  - **Quick check question:** How many tokens does "strategic" produce in your tokenizer? If you don't know, you can't predict character-level task difficulty.

- **Concept: Binary vs. gradient failure modes**
  - **Why needed here:** Some failures are continuous (getting 3/5 steps right); others are catastrophic (violating a single constraint invalidates everything). The paper shows both patterns—string manipulation has gradient failures, constraint compliance has binary ones.
  - **Quick check question:** If your output has one wrong character, is the whole response wrong? Depends on the task type.

## Architecture Onboarding

- **Component map:** Prompt bank -> Verification layer -> Model interface -> Pre-filter -> Batch execution -> Automated scoring -> Aggregation
- **Critical path:** Prompt design → Verification criteria definition → Endpoint verification → Batch execution → Automated scoring → Aggregation by model/provider/task-type
- **Design tradeoffs:** 20 prompts vs. 500+ in IFEval: Sacrifices statistical robustness for practical deployability; Temperature 0.0: Eliminates response variability but may underrepresent some models' capabilities; 10-second timeout: May penalize reasoning-heavy models; study acknowledges this limitation
- **Failure signatures:** Sequential tasks: Fail at transition points (complete step 1, skip step 2); Format tasks: Produce valid structure but wrong formatting details (valid JSON with extra spaces); Constraint tasks: Either fully comply or completely ignore—no partial credit
- **First 3 experiments:**
  1. Reproduce the string manipulation finding: Run Tests 1, 3, 5, 17 on 5 models from different providers. Confirm if string tasks cluster at bottom across all providers (paper suggests yes—universal challenge).
  2. Test the constraint-training hypothesis: Fine-tune a small model on constraint-satisfaction examples (e.g., "output without letter E"), then compare binary distribution pre/post training. Paper predicts shift from catastrophic failure to success.
  3. Probe the tokenization mechanism: Create character-level versions of Test 5 (spelling out operations like "take character 1, then character 2...") vs. token-level phrasing. If tokenization is the bottleneck, character-explicit prompts should improve performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do large language models exhibit significantly higher failure rates on low-complexity string manipulation tasks than on cognitively demanding mathematical operations?
- **Basis in paper:** The Discussion notes a counterintuitive "dissociation between computational complexity and instruction-following success rates," stating this pattern "warrants further investigation."
- **Why unresolved:** The study quantifies the performance gap (12.0% vs. 44.9% success) but does not isolate the root cause, such as tokenization limitations or a lack of character-level training data.
- **What evidence would resolve it:** A causal analysis correlating string manipulation performance with different tokenization strategies or an ablation study on character-level attention mechanisms.

### Open Question 2
- **Question:** Which specific training data characteristics or fine-tuning objectives most effectively improve performance on sequential, multi-step instruction adherence?
- **Basis in paper:** The Future Directions section calls for "understanding whether specific data characteristics, training objectives, or architectural choices contribute to superior instruction adherence."
- **Why unresolved:** While the authors observe that instruction-tuned variants outperform base models, they note the "degree of improvement varied significantly," leaving the specific drivers of success unidentified.
- **What evidence would resolve it:** Controlled training experiments comparing models trained on procedural data against those trained on single-step instructions to isolate the impact on sequential task coherence.

### Open Question 3
- **Question:** Do the failure modes identified in single-turn diagnostic prompts persist or degrade when applied to multi-turn conversational contexts?
- **Basis in paper:** The Limitations section lists "Single-Turn Focus" as a constraint, noting the study "does not evaluate multi-turn conversations."
- **Why unresolved:** It is unclear if the observed inability to maintain state across sequential steps (e.g., in Test 1) translates to an inability to maintain constraints over a longer dialogue history.
- **What evidence would resolve it:** Extending the diagnostic framework to multi-turn scenarios where the model must retain and apply the initial diagnostic constraints across several conversational exchanges.

## Limitations

- **Endpoint reliability:** 22.7% failure rate during basic functionality checks raises concerns about endpoint stability, with some models passing initial checks but failing during actual evaluation
- **Task representation:** 20-prompt diagnostic set may miss critical capabilities and cannot determine whether universal string manipulation difficulty reflects fundamental architectural limitations or training data absence
- **Contextual performance:** All evaluations used fixed parameters (temperature=0.0), which may systematically underestimate models' capabilities in practical deployments where parameter tuning is possible

## Confidence

- **High Confidence:** Instruction-following capability varies substantially across models (0% to 100% success rates) and providers (33.3% to 79.3% average), supported by verification framework and objective success criteria
- **Medium Confidence:** Tokenization misalignment explains string manipulation failures (2.7% success on Test 5) mechanistically, but direct testing of this mechanism is absent; Binary failure pattern in constraint compliance (96.1% success on Test 2) suggests training presence rather than emergent capability
- **Low Confidence:** Provider-level performance differences (x-ai at 79.3% vs. average 43.7%) are documented but causal attribution to instruction-tuning methodology lacks direct evidence

## Next Checks

1. **Endpoint Reliability Validation:** Conduct longitudinal study where endpoints that pass initial functionality checks are re-evaluated at multiple time points during main assessment. Measure intermittent failure rates and correlate with instruction-following performance to quantify endpoint instability contribution.

2. **Parameter Sensitivity Analysis:** Re-run 20 diagnostic prompts across subset of high-performing models (x-ai, Anthropic) using variable temperature settings (0.0, 0.3, 0.7) and max_tokens ranges. Compare success rates to quantify performance gap between standardized evaluation and practical deployment scenarios.

3. **Tokenization Mechanism Isolation:** Design controlled experiment comparing character-level versus token-level prompt phrasing for string manipulation tasks. Create parallel versions of Test 5 where operations are described at character granularity versus token granularity. Measure performance differences to test whether tokenization misalignment explains universal difficulty with character-level operations.