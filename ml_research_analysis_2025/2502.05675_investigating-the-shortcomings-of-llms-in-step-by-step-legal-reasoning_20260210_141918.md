---
ver: rpa2
title: Investigating the Shortcomings of LLMs in Step-by-Step Legal Reasoning
arxiv_id: '2502.05675'
source_url: https://arxiv.org/abs/2502.05675
tags:
- reasoning
- legal
- error
- errors
- conclusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a detailed, step-by-step analysis of errors
  made by large language models (LLMs) in legal reasoning. The authors use a college-level
  multiple-choice question-answering dataset from civil procedure and develop a novel
  error taxonomy through manual evaluation of LLM-generated reasoning chains.
---

# Investigating the Shortcomings of LLMs in Step-by-Step Legal Reasoning

## Quick Facts
- arXiv ID: 2502.05675
- Source URL: https://arxiv.org/abs/2502.05675
- Reference count: 40
- Key outcome: This paper conducts a detailed, step-by-step analysis of errors made by large language models (LLMs) in legal reasoning.

## Executive Summary
This paper conducts a detailed, step-by-step analysis of errors made by large language models (LLMs) in legal reasoning. The authors use a college-level multiple-choice question-answering dataset from civil procedure and develop a novel error taxonomy through manual evaluation of LLM-generated reasoning chains. They introduce two metrics—soundness and correctness—to evaluate the quality of these chains and build an automated evaluation framework using LLMs as evaluators. Their findings show that while most premises are error-free, most reasoning chains still lead to conclusions from false premises, often due to misinterpretation of legal context. Accuracy is significantly higher than correctness, indicating LLMs often guess correctly without fully sound reasoning. Incorporating error taxonomy feedback into prompting strategies marginally improves performance, suggesting that explicit error guidance can enhance legal reasoning. The work provides a scalable framework for diagnosing and analyzing reasoning errors in complex, logic-intensive tasks.

## Method Summary
The study evaluates LLM reasoning on a college-level Civil Procedure dataset (175 samples) using zero-shot Chain-of-Thought prompting. Four models (Mistral-7B, Llama-3-8B, GPT-3.5, GPT-4) generate step-by-step reasoning chains. An automated evaluation framework using GPT-4o detects premise-level errors (misinterpretation, irrelevant premise, factual hallucination) and maps them to conclusion-level error categories. Two metrics are introduced: soundness (error-free premises/total premises) and correctness (binary: fully sound reasoning chain with correct conclusion). The framework is validated against human annotations and used to test prompting strategies with error feedback incorporation.

## Key Results
- Soundness scores are high (~62-78% across models) while correctness scores are substantially lower (~13-44%), revealing LLMs often reach correct conclusions through flawed reasoning paths
- Misinterpretation of legal context is the dominant failure mode, accounting for the majority of premise-level errors and propagating to wrong conclusions
- Incorporating explicit error taxonomy definitions as feedback into prompting strategies yields marginal accuracy improvements (up to ~4%), with effectiveness varying by model and prompting method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soundness scores (error-free premises) can be high while correctness scores (fully valid reasoning chains) remain low, revealing that LLMs often reach correct conclusions through flawed reasoning paths.
- Mechanism: Most generated premises reiterate provided context correctly, but errors concentrate in the smaller subset of premises requiring contextual interpretation and inference. This produces high soundness (~62-78% across models) but substantially lower correctness (~13-44%), as conclusions derived from even one faulty premise invalidate the entire chain.
- Core assumption: The error taxonomy comprehensively captures the relevant failure modes for legal reasoning; the auto-evaluator's ~87% recall on error detection sufficiently approximates human judgment.
- Evidence anchors:
  - [abstract] "Accuracy is significantly higher than correctness, indicating LLMs often guess correctly without fully sound reasoning."
  - [Section 5.1] "Table 4 and Figure 4 show a sharp decrease (an average of ~27%) in the scores of accuracy to correctness across all LLMs."
  - [Section 5.1] "The highest fall in percentage is observed in Llama-3-8B-Instruct (31.4% decrease)... suggests that LLMs often rely on superficial correlations and patterns... to arrive at correct conclusions, rather than through genuine reasoning."

### Mechanism 2
- Claim: Misinterpretation of legal context is the dominant failure mode, accounting for the majority of premise-level errors and propagating to wrong conclusions.
- Mechanism: LLMs struggle to integrate nuanced legal distinctions—such as "indefinite intent" versus temporary residence—when multiple facts must be jointly considered. The error propagates through subsequent steps, producing "Wrong Conclusion from False Premises" as the most common conclusion-level error for smaller models.
- Core assumption: Zero-shot CoT prompting does not inherently elicit systematic legal analysis; the Civil Procedure dataset's ambiguity and "trick" scenarios fairly represent real legal complexity.
- Evidence anchors:
  - [Section 5.2] "'Misinterpretations' are the dominant category of errors at premise-level... highlights that LLMs struggle to fully grasp the nuanced complexities of legal scenarios requiring the demonstration of critical analysis in zero-shot CoT settings."
  - [Figure 12 description] "an average of ~96% of reasoning chains leading to conclusions from false premises have one or more misinterpretation errors in the intermediate premises."
  - [corpus] Related work on legal reasoning (LegalBench, LawBench) focuses on final accuracy rather than step-by-step chain evaluation, leaving this error-propagation mechanism underexplored.

### Mechanism 3
- Claim: Incorporating explicit error taxonomy definitions as feedback into prompting strategies yields marginal accuracy improvements (up to ~4%), but effectiveness varies by model and prompting method.
- Mechanism: Error definitions guide the model toward self-verification by naming failure modes (misinterpretation, hallucination, irrelevant premises). However, some models exhibit "self-doubting" behavior when over-corrected, reducing accuracy on certain strategies (e.g., Self-Correct, Self-Discovery for Gemini-1.5-Flash).
- Core assumption: The model can interpret and apply abstract error descriptions to its own outputs; feedback does not require task-specific fine-tuning.
- Evidence anchors:
  - [Section 5.3] "adding the error definitions as feedback showed improvement in accuracy up to 4%... for Llama-3-8B-instruct, accuracy improved across all prompting techniques, whereas for Gemini-1.5-Flash, the accuracy increased only for the Chain-of-Thought and Plan-and-Solve methods."
  - [Section 5.3] "the decrease in accuracy for these strategies with Gemini resulted due to self-doubting nature of LLMs."
  - [corpus] Related work (DIVERSE, Self-Correct methods) shows step-aware verification improves reasoning, but effectiveness is task-dependent; weak or missing corpus evidence on legal-specific feedback loops.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The entire evaluation framework assumes zero-shot CoT output as the reasoning substrate; understanding how CoT elicits intermediate steps is prerequisite to diagnosing where those steps fail.
  - Quick check question: Given a legal scenario, can you prompt a model to produce numbered reasoning steps before its final answer, and identify which step first diverges from the provided legal context?

- Concept: **Error Propagation in Sequential Reasoning**
  - Why needed here: The paper's central finding—that one misinterpretation cascades to wrong conclusions—requires understanding how deductive chains transmit errors forward.
  - Quick check question: If Premise 4 incorrectly equates "two-year program" with "not indefinite," and Premise 6 draws a conclusion based on Premise 4, what type of conclusion-level error results?

- Concept: **Soundness vs. Correctness Distinction**
  - Why needed here: These are the paper's two core metrics; soundness measures step-level validity, correctness measures end-to-end validity with ground-truth alignment.
  - Quick check question: A reasoning chain has 5 premises, all error-free, but selects the wrong final option. What is its soundness score? What is its correctness score?

## Architecture Onboarding

- Component map: Reasoning Chain Generator -> Auto-Evaluator Pipeline -> Conclusion-Level Classifier -> Metric Calculator -> Feedback-Augmented Prompting Module
- Critical path:
  1. Dataset sample → LLM reasoning chain generation (zero-shot CoT)
  2. Auto-evaluator analyzes each premise; hallucination detector uses verification-question generation + context-grounded answering
  3. Summarizer aggregates multi-detector outputs per premise
  4. Conclusion classifier maps premise errors to conclusion-level error label
  5. Metrics computed; optional feedback loop for prompting strategy comparison
- Design tradeoffs:
  - **Single-call vs. multi-call hallucination detection**: Multi-call improves factual verification recall but increases latency and cost
  - **Granularity of error taxonomy**: Finer categories (e.g., 7 misinterpretation subtypes) improve diagnostic precision but complicate auto-evaluator agreement
  - **Human vs. auto evaluation**: Human annotation achieves high inter-annotator agreement (κ≈0.82) but doesn't scale; auto-evaluator achieves ~87% recall but produces false negatives on rare error types (e.g., "Correct Conclusion from Incomplete Premises")
- Failure signatures:
  - **High accuracy, low correctness**: Model is pattern-matching to correct options without valid reasoning (observed across all models, most pronounced in Llama-3-8B with 31.4% drop)
  - **Correct Conclusion from False Premises (CCFP)**: Dominant error in GPT-4 variants, suggesting reliance on training-data patterns rather than context-grounded inference
  - **Self-doubting degradation**: Feedback-augmented prompting reduces accuracy when model over-corrects (observed in Gemini with Self-Correct/Self-Discovery strategies)
- First 3 experiments:
  1. **Baseline soundness-correctness gap measurement**: Run zero-shot CoT on 30-sample subset with human annotation; compute soundness, accuracy, correctness for Mistral-7B, Llama-3-8B, GPT-3.5, GPT-4-turbo. Confirm ~25-30% accuracy-to-correctness drop.
  2. **Auto-evaluator validation**: Run GPT-4o auto-evaluator on same 30-sample subset; compute recall against human annotations for premise-level error detection (target: >85% recall). Identify systematic mislabeling patterns (e.g., misinterpretation ↔ irrelevant premise confusion).
  3. **Feedback-augmented prompting ablation**: Test Plan-and-Solve with and without error taxonomy feedback on Llama-3-8B-Instruct across full 175-sample dataset; measure accuracy delta (target: +3-4%). Separately test on Gemini-1.5-Flash to observe self-doubting degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can agent-based frameworks specifically targeting "Misinterpretation" errors achieve significant performance gains over the marginal improvements seen with error-taxonomy prompting?
- **Basis in paper:** [explicit] The authors state in the Discussion: "there is a need to develop more effective frameworks beyond prompting, such as agent-based methods, that account for these errors and enhance the model's legal reasoning capabilities."
- **Why unresolved:** The integration of error-taxonomy feedback into prompting strategies only resulted in a marginal accuracy improvement (up to ~4%).
- **What evidence would resolve it:** Developing an agentic system that iteratively validates reasoning steps against the error taxonomy and measuring the increase in Correctness scores compared to the prompting baselines.

### Open Question 2
- **Question:** Would integrating auto-formalization (converting natural language to formal logic) enhance the detection of reasoning errors compared to LLM-based semantic evaluation?
- **Basis in paper:** [explicit] The Limitations section notes: "the systematic evaluation could benefit from converting natural language to formal language through auto-formalization."
- **Why unresolved:** The current framework relies entirely on natural language analysis via LLMs, which may struggle with logical consistency in a way that formal logic validation would not.
- **What evidence would resolve it:** A comparative study measuring the error detection recall of a formal logic parser versus the current GPT-4o based auto-evaluator on the Civ. Pro. dataset.

### Open Question 3
- **Question:** How can the auto-evaluation framework be improved to distinguish "Correct Conclusion from Incomplete Premise(s)" (CCIP) from valid reasoning?
- **Basis in paper:** [explicit] Appendix J states: "The current auto-evaluator system cannot detect 'Correct Conclusion from Incomplete Premise(s)' (CCIP), as it cannot distinguish it from a 'Correct Conclusion from Correct Premises' scenario."
- **Why unresolved:** The current LLM-based evaluator can verify if premises are factually correct, but lacks the capability to assess if the premises are *sufficient* to justify the conclusion.
- **What evidence would resolve it:** Implementing a specific module that generates "necessary premise" verification questions and checks if the reasoning chain addresses them before labeling the conclusion as error-free.

## Limitations
- The error taxonomy, while comprehensive for Civil Procedure, may not capture all reasoning errors across diverse legal domains
- The automated evaluation pipeline, despite achieving ~87% recall, still relies on a single auto-evaluator model (GPT-4o) and may propagate systematic biases
- The study focuses on zero-shot prompting without exploring fine-tuning or domain adaptation, which could significantly impact error patterns

## Confidence

- **High Confidence**: The soundness-correctness gap finding (Section 5.1) is well-supported by consistent data across multiple models showing 25-31% drops. The auto-evaluator's ~87% recall provides sufficient reliability for the main conclusions about error distribution.
- **Medium Confidence**: The dominance of misinterpretation errors (Section 5.2) is well-documented but relies on the taxonomy's completeness. The auto-evaluator may miss nuanced legal distinctions, potentially underestimating certain error types.
- **Medium Confidence**: The marginal effectiveness of error feedback (Section 5.3) is observed but the small effect size (up to 4%) and model-dependent variability (e.g., self-doubting in Gemini) suggest the mechanism needs further investigation before generalization.

## Next Checks

1. **Taxonomy Completeness Validation**: Apply the current error taxonomy to legal reasoning chains from a different domain (e.g., criminal law or contract law) and document any novel error types that emerge, requiring taxonomy expansion.

2. **Auto-Evaluator Robustness Test**: Perform ablation studies varying the auto-evaluator model (e.g., test with GPT-3.5, Claude-3) and measure agreement with GPT-4o on premise-level error detection to assess whether conclusions depend on a single evaluator's judgment.

3. **Feedback Strategy Optimization**: Systematically vary the granularity and presentation of error definitions in feedback (e.g., test 5-level vs. 2-level taxonomies) across all four prompting strategies to identify optimal feedback configurations that maximize improvement while minimizing self-doubt.