---
ver: rpa2
title: 'From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of
  Images'
arxiv_id: '2511.22805'
source_url: https://arxiv.org/abs/2511.22805
tags:
- image
- score
- cognitive
- visual
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We present CogIP-Bench, a comprehensive benchmark for evaluating
  Multimodal Large Language Models (MLLMs) on four subjective image cognitive properties:
  aesthetics, funniness, emotional valence, and memorability. Our evaluation reveals
  that current MLLMs show poor alignment with human perception of these nuanced properties,
  with memorability being particularly challenging.'
---

# From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images

## Quick Facts
- arXiv ID: 2511.22805
- Source URL: https://arxiv.org/abs/2511.22805
- Reference count: 40
- Authors: Yiming Chen; Junlin Han; Tianyi Bai; Shengbang Tong; Filippos Kokkinos; Philip Torr
- Primary result: Presents CogIP-Bench to evaluate MLLMs on subjective image properties, showing poor alignment with human perception that can be improved via supervised fine-tuning and transferred to image generation

## Executive Summary
This paper introduces CogIP-Bench, a comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on four subjective image cognitive properties: aesthetics, funniness, emotional valence, and memorability. The benchmark reveals that current MLLMs perform poorly in aligning with human perception of these nuanced properties, with memorability being particularly challenging. The authors propose a post-training pipeline using supervised fine-tuning to improve MLLMs' alignment with human cognitive judgments. They demonstrate that this cognitive alignment can be transferred to downstream tasks, specifically showing that integrating their fine-tuned MLLM into an image generation pipeline enables the synthesis of images that better embody desired traits like memorability or aesthetics.

## Method Summary
The paper introduces CogIP-Bench as a new evaluation framework for assessing MLLMs on subjective image properties. The benchmark covers four dimensions: aesthetics, funniness, emotional valence, and memorability. The authors then employ a supervised fine-tuning pipeline to improve MLLM alignment with human cognitive judgments on these properties. Finally, they demonstrate the transferability of this alignment by integrating the fine-tuned model into an image generation pipeline (Qwen-Image), showing improved synthesis of images with desired cognitive properties.

## Key Results
- Current MLLMs show poor alignment with human perception of subjective image properties
- Memorability is identified as the most challenging property for MLLMs to capture
- Supervised fine-tuning significantly improves MLLM alignment with human cognitive judgments
- Cognitive alignment is transferable to downstream image generation tasks, enabling synthesis of images with desired properties

## Why This Works (Mechanism)
The mechanism works by first establishing a benchmark that captures the gap between MLLM outputs and human subjective judgments on image properties. This gap identification allows for targeted supervised fine-tuning that aligns the model's outputs with human cognitive patterns. The fine-tuning process effectively recalibrates the MLLM's internal representations to better match how humans perceive and evaluate images across the four dimensions. When this aligned model is integrated into downstream tasks like image generation, it can synthesize outputs that reflect the learned human cognitive preferences rather than the model's default generation patterns.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: AI systems that process and generate both text and visual information; needed because the work bridges visual perception with language-based judgment
- **Supervised Fine-Tuning**: A training approach where models learn from labeled examples; needed to align MLLM outputs with human subjective judgments
- **Image Cognitive Properties**: Subjective attributes of images including aesthetics, funniness, emotional valence, and memorability; needed as the target properties for alignment
- **Human Perception Alignment**: The process of making AI outputs match human subjective evaluations; needed to bridge the gap between model outputs and human preferences
- **Transfer Learning**: Applying knowledge gained in one task to improve performance in another; needed to demonstrate the practical utility of the alignment work

## Architecture Onboarding
- **Component Map**: CogIP-Bench dataset -> MLLM evaluation -> Supervised fine-tuning pipeline -> Fine-tuned MLLM -> Image generation pipeline integration
- **Critical Path**: Benchmark creation → MLLM evaluation → Fine-tuning → Downstream task validation
- **Design Tradeoffs**: The choice of supervised fine-tuning over other alignment methods balances precision with computational efficiency, though it may limit the model's ability to generalize beyond the specific properties targeted
- **Failure Signatures**: Poor alignment may manifest as inconsistent judgments across similar images, inability to capture cultural context, or generation of images that technically meet criteria but fail to resonate with human perception
- **First Experiments**: 1) Evaluate baseline MLLM performance on CogIP-Bench, 2) Apply supervised fine-tuning and re-evaluate, 3) Integrate fine-tuned model into image generation pipeline and assess output quality

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, though several emerge from the work. One implicit question is how well the CogIP-Bench would generalize across different cultural contexts, given that subjective image properties can vary significantly between cultures. Another potential open question is whether the supervised fine-tuning approach would be effective for other subjective properties beyond the four tested. The paper also leaves open questions about the scalability of this approach to larger, more diverse image datasets and whether the alignment improvements would transfer equally well to other types of downstream tasks beyond image generation.

## Limitations
- The evaluation relies heavily on the quality and representativeness of the CogIP-Bench dataset, with limited information on rater diversity or cultural biases
- The claim that memorability is "particularly challenging" lacks clear definition of poor alignment metrics
- Transferability to downstream tasks is demonstrated but not rigorously validated across diverse use cases or other MLLM architectures
- The paper does not address potential issues with dataset bias, such as whether the benchmark images represent a balanced cross-section of visual content
- There is limited discussion of how the fine-tuning process might affect the model's performance on other unrelated tasks

## Confidence
- **High Confidence**: CogIP-Bench as a comprehensive benchmark for evaluating MLLMs on subjective image properties
- **Medium Confidence**: Supervised fine-tuning can improve MLLM alignment with human cognitive judgments
- **Low Confidence**: Cognitive alignment transferability to downstream tasks is demonstrated but not thoroughly validated

## Next Checks
1. Conduct cross-cultural validation of CogIP-Bench to assess impact of cultural biases on subjective image property judgments
2. Perform ablation studies comparing supervised fine-tuning with alternative alignment techniques like reinforcement learning from human feedback
3. Extend transferability validation to multiple MLLM architectures and downstream tasks beyond image generation, such as video summarization or recommendation systems