---
ver: rpa2
title: A Hybrid Search for Complex Table Question Answering in Securities Report
arxiv_id: '2511.09179'
source_url: https://arxiv.org/abs/2511.09179
tags:
- table
- question
- cell
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting accurate answers
  from complex financial tables in securities reports using Large Language Models
  (LLMs). The proposed hybrid search method avoids relying on manually identified
  table headers by combining TF-IDF with language model-based vector retrieval to
  estimate the most relevant row and column cells, then selecting their intersection
  as the answer.
---

# A Hybrid Search for Complex Table Question Answering in Securities Report

## Quick Facts
- **arXiv ID**: 2511.09189
- **Source URL**: https://arxiv.org/abs/2511.09189
- **Reference count**: 37
- **Primary result**: Hybrid search achieves 74.6% accuracy on TQA dataset, outperforming GPT-4o mini (63.9%) and narrowing gap with human baseline (93.1%)

## Executive Summary
This paper tackles the challenge of extracting accurate answers from complex financial tables in securities reports using Large Language Models. The proposed hybrid search method combines TF-IDF with language model-based vector retrieval to estimate the most relevant row and column cells, then selects their intersection as the answer. By avoiding reliance on manually identified table headers, the approach handles structurally complex tables with merged cells and nested structures. The method is evaluated on the TQA dataset from the U4 shared task at NTCIR-18, demonstrating superior performance compared to GPT-4o mini while highlighting the remaining gap with human expertise.

## Method Summary
The approach uses a four-stage pipeline: table cleaning (removing decorative HTML while preserving colspan/rowspan), value extraction via hybrid search, unit extraction using GPT-4o mini, and value normalization through rule-based scaling. The hybrid search scores all cells using a weighted combination of TF-IDF and Sentence-BERT cosine similarity (optimal α=0.21), selects the top-2 cells from different rows and columns, and returns their intersection as the answer cell. Contrastive learning fine-tunes the language model on 1,000 question-header pairs to improve retrieval accuracy. The method avoids explicit header identification, making it robust to complex table structures.

## Key Results
- Hybrid search achieves 74.6% accuracy on TQA dataset, outperforming GPT-4o mini (63.9%)
- Human annotator without financial expertise reaches 93.1% accuracy, highlighting gap between automated and human performance
- Contrastive learning with 1,000 questions and 31,325 pseudo-pairs improves vector retrieval accuracy
- Intersection-based cell selection handles complex tables with merged cells and nested structures effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval combining lexical and semantic matching outperforms either method alone for financial table cell identification.
- Mechanism: A weighted score s_H(q,d) = (1-α)s_v(q,d) + αs_t(q,d) balances TF-IDF (surface term matching via MeCab tokenization) and Sentence-BERT vector retrieval (semantic similarity). Optimal α=0.21 was determined through validation set tuning.
- Core assumption: Questions contain both exact terminology from table headers AND semantic relationships requiring embedding-based similarity; these signals are complementary but partially independent.
- Evidence anchors:
  - [abstract] "hybrid retrieval mechanism that integrates a language model and TF-IDF"
  - [section III-B1] Formula and hyperparameter search described; α=0.21 selected
  - [corpus] Jin et al. (2023) showed dense retrieval improved recall over keyword-only methods for table QA
- Break condition: If TF-IDF and vector scores become highly correlated (no complementary information), or if optimal α approaches 0 or 1 (one signal dominates).

### Mechanism 2
- Claim: Contrastive learning on pseudo question-header pairs enables effective domain adaptation with limited data (1,000 questions).
- Mechanism: Row and column cells are concatenated into text strings. If a row/column contains the correct cell, it forms a positive pair with the question; otherwise negative. Sentence-BERT is fine-tuned to pull positive pairs closer and push negatives apart in embedding space.
- Core assumption: Concatenating cells along rows/columns creates meaningful textual representations that capture the "header-like" information needed to match question semantics.
- Evidence anchors:
  - [abstract] "language model is trained using contrastive learning on a small dataset of question-header pairs"
  - [section IV-E + Figure 2] Pseudo-pair construction details; 31,325 pairs (2,000 positive)
  - [corpus] Limited direct evidence; related work uses dense retrieval but not this specific pseudo-labeling approach
- Break condition: If pseudo-labels are noisy (correct cell doesn't imply row/column text is relevant), or if concatenation loses cell-level granularity needed for matching.

### Mechanism 3
- Claim: Answering via row-column intersection avoids explicit header identification, which is critical for complex tables with merged cells and nested structures.
- Mechanism: Two highest-scoring cells c1 and c2 are selected from different rows AND columns. Their intersection is the answer cell. The lower-right cell indicates the row axis; the other indicates the column axis.
- Core assumption: Questions implicitly specify BOTH a row criterion (e.g., fiscal year) and a column criterion (e.g., metric name), and top-2 cells will capture these distinct axes.
- Evidence anchors:
  - [abstract] "selecting their intersection as the answer"
  - [section III-B2] Detailed selection logic for c1, c2 and intersection
  - [corpus] Sun et al. (2016) proposed intersection-based cell search but required exact term matching and pre-identified headers
- Break condition: If both top cells lie on the same axis (e.g., both are row labels), or if questions reference only one dimension, intersection logic fails.

## Foundational Learning

- Concept: **TF-IDF and Cosine Similarity**
  - Why needed here: Understanding how lexical matching contributes to the hybrid score; why surface term overlap matters in financial tables where terminology is standardized.
  - Quick check question: Given query "fiscal year 2018 revenue" and document "2018年度 売上高", would TF-IDF capture the match if tokenized separately vs. together?

- Concept: **Contrastive Learning with Sentence-BERT**
  - Why needed here: The paper fine-tunes embeddings using positive/negative pairs; understanding Siamese network architecture and contrastive loss is essential to diagnose training issues.
  - Quick check question: If all pairs are treated as negatives except the single positive, what happens to the embedding space as training progresses?

- Concept: **HTML Table Structure (rowspan, colspan, merged cells)**
  - Why needed here: The cleaning step preserves colspan/rowspan; the intersection mechanism must handle tables where visual and logical structures diverge.
  - Quick check question: In a table with rowspan=2 on the first column, which logical cells share the same row index?

## Architecture Onboarding

- Component map: Table Cleaning -> Value Extraction (Hybrid Search) -> Unit Extraction (GPT-4o mini) -> Value Normalization (Rule-based)
- Critical path: Hybrid search scoring → Top-2 cell selection → Intersection logic. Errors here cascade to final answer. Unit extraction is a separate failure mode.
- Design tradeoffs:
  - Lightweight encoder (Sentence-BERT) vs. full LLM: Chose encoder for efficiency and fine-tunability with 1K examples; trades off general reasoning ability
  - Small fine-tuning dataset (1,000 questions) vs. generalization: Sufficient for domain adaptation but may not cover all table structures
  - Rule-based normalization vs. learned: Chose deterministic rules for unit handling; trades off flexibility for reliability
- Failure signatures:
  - Both top cells from same row/column → Intersection undefined or wrong
  - Unit extraction hallucinates scale → Correct cell, wrong normalized value
  - Merged cells in HTML not properly flattened → Cell ID mismatch with ground truth
- First 3 experiments:
  1. **Baseline TF-IDF only** (α=1.0): Establishes lexical matching upper bound; expect ~58.7% accuracy per Table I
  2. **Baseline vector retrieval only** (α=0.0, no fine-tuning): Expect ~31% accuracy; confirms need for domain adaptation
  3. **Ablation: Intersection vs. single top cell**: Test whether intersection logic is necessary or if top-1 cell suffices for simpler questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the integration of more efficient text-search models, such as E5 or Ruri, outperform the current traditional encoder models (Sentence-BERT/UBKE-LUKE) and narrow the performance gap with human evaluation?
- Basis in paper: [explicit] The conclusion states, "we plan to incorporate more efficient text-search models such as E5 and Ruri to improve performance and narrow the gap with human evaluation results."
- Why unresolved: The current study was limited to traditional encoder models and did not test whether modern, highly efficient retrieval models could better capture the semantics of financial table cells.
- What evidence would resolve it: Experimental results comparing the accuracy of the proposed pipeline when the vector retrieval component is swapped for E5 or Ruri models against the current benchmark of 74.6%.

### Open Question 2
- Question: What specific structural features or reasoning mechanisms enable non-expert humans to achieve 93.1% accuracy, and how do automated systems fail on these specific instances?
- Basis in paper: [explicit] The conclusion notes, "We will also conduct a detailed analysis of how humans versus automated systems understand complex tables."
- Why unresolved: While the paper quantifies the performance gap (automated 74.6% vs. human 93.1%), it does not qualitatively analyze the types of complex structures or logical inferences where humans succeed and the model fails.
- What evidence would resolve it: A detailed error analysis or ablation study identifying the specific table layouts (e.g., specific types of merged cells, multi-level headers) or question types that contribute to the 18.5% accuracy gap.

### Open Question 3
- Question: Does the heuristic of concatenating all cell values in a row or column to create "pseudo-pairs" for contrastive learning limit the model's ability to distinguish fine-grained cell semantics?
- Basis in paper: [inferred] Section IV-E states, "We build the pseudo-pairs of a question and a text obtained by concatenating all the cell values in each row and column," because the dataset lacks direct header-question pairs.
- Why unresolved: Concatenating cells into a single string may introduce noise or dilute the semantic signal needed for the model to precisely locate a specific cell, but this approximation was not compared against a ground-truth header dataset.
- What evidence would resolve it: A comparative experiment where the model is trained on manually verified question-header pairs versus the proposed pseudo-pairs to measure the loss in retrieval accuracy.

### Open Question 4
- Question: How does the pipeline perform on questions that require aggregating data from multiple cells rather than extracting a single intersection point?
- Basis in paper: [inferred] Section III-B describes the method as selecting "cells at the intersection of the most relevant row and column," implying a single-cell output assumption.
- Why unresolved: Financial QA often requires summing rows or comparing columns. The paper evaluates accuracy based on "extraction," but does not report performance on questions requiring multi-hop reasoning or aggregation across the intersections found.
- What evidence would resolve it: Evaluation results on a subset of TQA questions specifically annotated as requiring aggregation or comparison, measuring if the relevant cells are retrieved even if a single intersection is insufficient for the final answer.

## Limitations

- The hybrid search mechanism assumes top-2 cells will always come from different rows and columns; no fallback strategy exists if this fails.
- Contrastive learning depends on pseudo-labels that may not capture all relevant semantic relationships in complex financial tables.
- Unit extraction using GPT-4o mini is treated as a black box without error analysis or confidence scoring.

## Confidence

- **High confidence**: Hybrid search mechanism's effectiveness (α=0.21 validated on held-out set, 10.7% accuracy improvement over GPT-4o mini is statistically significant)
- **Medium confidence**: Contrastive learning approach (limited direct evidence, potential noise in pseudo-labels)
- **Low confidence**: Robustness of intersection logic for all table structures, particularly those with complex merged cells or irregular layouts

## Next Checks

1. **Ablation Study on Intersection Logic**: Compare accuracy when using only the top-1 cell versus the intersection of top-2 cells to determine if the intersection assumption holds across all question types.
2. **Analysis of Top-2 Cell Selection**: Log cases where both top cells come from the same row or column to quantify how often the intersection logic fails and whether a fallback mechanism is needed.
3. **Unit Extraction Error Analysis**: Manually annotate a sample of unit extraction outputs to determine the error rate and whether the normalization rules introduce systematic biases in the final values.