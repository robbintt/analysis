---
ver: rpa2
title: Decomposition of Small Transformer Models
arxiv_id: '2511.08854'
source_url: https://arxiv.org/abs/2511.08854
tags:
- decomposition
- causal
- subcomponents
- layer
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Decomposition of Small Transformer Models

## Quick Facts
- **arXiv ID**: 2511.08854
- **Source URL**: https://arxiv.org/abs/2511.08854
- **Reference count**: 40
- **Key outcome**: Successfully decomposes transformer weights into sparse rank-1 subcomponents that faithfully reconstruct original model behavior while exposing interpretable mechanisms

## Executive Summary
This paper introduces Stochastic Parameter Decomposition (SPD), a method for decomposing transformer weight matrices into interpretable, sparse rank-1 subcomponents. The approach learns causal importance scores for each subcomponent, enabling faithful reconstruction of original model outputs while identifying the mechanisms responsible for specific behaviors. The method is validated on a toy induction-head model and a small GPT-2 model, demonstrating both reconstruction fidelity and the ability to identify and manipulate interpretable concepts like "golf" and "basketball."

## Method Summary
The method decomposes each weight matrix W^l into C rank-1 subcomponents W^l_c = ũ^l_c ⊗ ṽ^l_c. A causal importance network computes position-aware relevance scores g^l_c,n for each subcomponent using attention with relative positional encodings. During forward passes, stochastic masks are sampled from U(g^l_c(x), 1) to select active subcomponents. The decomposition is trained to minimize a weighted sum of faithfulness (weight reconstruction), minimality (sparsity), and stochastic reconstruction losses. For intervention, identified subcomponents are suppressed via orthogonal projection.

## Key Results
- Induction head model successfully decomposed with faithfulness loss of 3e-9
- K0 subcomponent aligns token representations with positional encodings; Q1/K1 implements copy circuit
- GPT-2-small decomposition identifies golf/basketball concepts that can be selectively suppressed via orthogonal projection
- Golf suppression reduces golf probability from 0.620 to 0.090 while basketball stays high at 0.630

## Why This Works (Mechanism)

### Mechanism 1: Rank-1 Subcomponent Decomposition
Weight matrices are decomposed into C rank-1 outer products with learned activation coefficients. This allows faithful reconstruction while exposing interpretable mechanisms. The approach works because neural network mechanisms can be meaningfully represented as rank-1 parameter-space directions. Break condition: when information requires higher rank to represent (V1 needs 11 unique subcomponents for 128 tokens).

### Mechanism 2: Attention-Based Causal Importance for Sequential Data
Attention across tokens before computing causal importance enables position-aware assignment of subcomponent relevance. This is necessary because identical tokens at different positions have different causal roles. The method uses minimal attention networks with relative positional encodings. Break condition: computational overhead at very long sequences (6 iter/sec at 10240 tokens).

### Mechanism 3: Orthogonal Projection for Targeted Intervention
Identified rank-1 directions are suppressed via orthogonal projection to selectively reduce targeted capabilities while preserving unrelated model outputs. This works because subcomponents are sufficiently independent. Break condition: when concepts are entangled across multiple tokens/directions (requires multi-token suppression).

## Foundational Learning

- **Concept: Rank-1 Matrices and Outer Products**
  - Why needed here: SPD represents all subcomponents as outer products ũ ⊗ ṽ; understanding this constraint is essential for interpreting what mechanisms can/cannot be captured.
  - Quick check question: Why can a rank-1 matrix only represent a single "input direction → output direction" mapping?

- **Concept: Induction Heads**
  - Why needed here: The paper's primary validation is a 2-layer induction head model; you must understand this circuit to evaluate whether SPD recovers correct mechanisms.
  - Quick check question: In an induction circuit, how does layer 1's attention pattern enable layer 2 to "copy the token that followed the previous occurrence"?

- **Concept: KL Divergence for Distribution Matching**
  - Why needed here: All reconstruction losses use D_KL to measure decomposed vs. original model alignment.
  - Quick check question: Why is KL divergence asymmetric (D_KL(P||Q) ≠ D_KL(Q||P)), and which direction does SPD use?

## Architecture Onboarding

- **Component map:** Decomposition module → Causal importance network → Mask sampling → Loss aggregator → Intervention module
- **Critical path:** Initialize subcomponents → Run original model → Compute causal importances → Sample stochastic masks → Run decomposed model → Compute loss → Backprop
- **Design tradeoffs:** Higher C = finer granularity but more compute; faithfulness β too low = poor reconstruction; attention-based importance = position-sensitive but O(n²)
- **Failure signatures:** L_faithfulness > 0.1 = poor reconstruction; single layer concentrates components = model cheating; ablation affects unrelated concepts = insufficient disentanglement
- **First 3 experiments:** 1) Reproduce induction head toy model and verify K0→s1 positional alignment, Q1/K1→s2 pattern; 2) Compare scalar vs. vector vs. attention causal importance on repeated-token sequences; 3) Decompose GPT-2-small on 2-sentence fact dataset and validate via orthogonal projection ablation

## Open Questions the Paper Calls Out

### Open Question 1
Can SPD be scaled efficiently to modern, large-scale language models (e.g., 7B+ parameters)?
Basis in paper: [explicit] "experiments are restricted to small-scale models, leaving open questions about scalability to larger architectures."
Why unresolved: Current method introduces computational overhead through attention networks and rank-1 decomposition, tested only on 2-layer toy model and GPT-2 small.
What evidence would resolve it: Successful decomposition of significantly larger model within reasonable computational constraints, demonstrating interpretable subcomponent recovery.

### Open Question 2
How can the definition of "causal importance" be improved to handle complex feature interactions within transformer circuits?
Basis in paper: [explicit] Listed as future work to "improve our definition of causal importance for computations with feature interactions."
Why unresolved: Current method relies on ablations subject to model interactions, and independent γ-MLPs may struggle with non-linear feature bundling.
What evidence would resolve it: Modified importance function successfully decomposing circuits relying on multiplicative interaction or non-linear feature bundling.

### Open Question 3
What quantitative benchmarks are required to evaluate the generality and faithfulness of parameter-space decompositions across different tasks?
Basis in paper: [explicit] "our evaluation is qualitative and task-specific, so further benchmarks are needed to establish generality across architectures and tasks."
Why unresolved: Paper relies on case studies to prove validity but lacks standardized metric to verify "true" mechanisms vs. loss function artifacts.
What evidence would resolve it: Standardized benchmark suite with known ground-truth circuits where SPD's recovered subcomponents are quantitatively compared against ground truth.

## Limitations
- Rank-1 constraint cannot fully capture higher-rank mechanisms (V1 requires 11 components for 128 tokens)
- Computational overhead limits practical applicability to long sequences
- Only validated on small models and limited datasets (2-layer toy, GPT-2-small on 2-sentence facts)

## Confidence

- **High confidence**: Rank-1 decomposition works for simple circuits (induction head with 3e-9 faithfulness loss), orthogonal projection effectively suppresses identified concepts
- **Medium confidence**: Attention-based causal importance provides position-sensitive scoring, but comparative ablation studies are limited
- **Medium confidence**: Framework identifies interpretable concepts in GPT-2-small, but dataset is quite limited

## Next Checks

1. **Ablation on Causal Importance Mechanisms**: Systematically compare attention-based, scalar, and vector causal importance methods on synthetic task requiring position-sensitive token importance. Measure faithfulness reconstruction loss and active component sparsity.

2. **Scalability Validation**: Apply decomposition framework to 6-layer transformer on WikiText-2 or similar corpus. Evaluate faithfulness reconstruction and concept identification while monitoring computational overhead scaling.

3. **Entanglement Stress Test**: Design dataset where concepts are intentionally entangled across multiple tokens/directions (e.g., "red apple"). Test whether framework can correctly identify and disentangle relevant subcomponents or fails when concepts span multiple rank-1 directions.