---
ver: rpa2
title: Should We Always Train Models on Fine-Grained Classes?
arxiv_id: '2509.05130'
source_url: https://arxiv.org/abs/2509.05130
tags:
- coarse
- training
- fine-grained
- labels
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether training machine learning models
  on fine-grained class labels consistently improves performance on coarse-grained
  classification tasks. Using both real and synthetic datasets, the authors demonstrate
  that the benefit of fine-grained training depends critically on data geometry, label
  hierarchy structure, and the relationship between model capacity and dataset size.
---

# Should We Always Train Models on Fine-Grained Classes?

## Quick Facts
- arXiv ID: 2509.05130
- Source URL: https://arxiv.org/abs/2509.05130
- Reference count: 40
- Primary result: Fine-grained training improves coarse generalization only in data-scarce, over-parameterized regimes with low boundary redundancy between class hierarchies.

## Executive Summary
This study investigates whether training machine learning models on fine-grained class labels consistently improves performance on coarse-grained classification tasks. Using both real and synthetic datasets, the authors demonstrate that the benefit of fine-grained training depends critically on data geometry, label hierarchy structure, and the relationship between model capacity and dataset size. Fine-grained training is advantageous only when the data is limited relative to model parameters, and when the fine and coarse classification tasks share similar decision boundaries (low boundary redundancy). The authors introduce a synthetic dataset to quantitatively control boundary redundancy, showing that fine-grained training outperforms coarse training only in cases of low boundary redundancy or small datasets. They also provide a theoretical decomposition of the loss function, revealing that fine-grained training minimizes both coarse and intra-class losses.

## Method Summary
The paper compares coarse vs. fine-grained training using minimal 1-hidden-layer MLPs with ReLU activation on MNIST, K-MNIST, F-MNIST, and CIFAR-10 datasets. Models are trained with SGD (LR decay 0.01→0.001) and early stopping. The fine model uses K-class softmax outputs while the coarse model uses binary sigmoid. To evaluate the fine model on coarse tasks, subclass probabilities are aggregated per Equation 9. Experiments vary dataset size (from 500 to 10,000 samples) and model capacity (10-100 hidden neurons) to test three hypotheses: fine-grained training acts as regularization in over-parameterized regimes, benefits depend on boundary redundancy between fine and coarse tasks, and the trade-off between optimization burden and boundary alignment determines performance. A synthetic concentric circles dataset controls boundary redundancy by varying the geometric separation between subclass boundaries.

## Key Results
- Fine-grained training improves coarse accuracy only when data is limited relative to model parameters (small datasets).
- The performance gain depends on boundary redundancy: fine-trained models excel when fine and coarse boundaries align (low redundancy).
- Fine-grained loss decomposes into coarse loss plus intra-class loss, suggesting regularization benefits in over-parameterized regimes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained training may improve coarse generalization by acting as a regularizer in data-scarce, over-parameterized regimes.
- Mechanism: The fine-grained loss function decomposes into the coarse loss plus an intra-class loss term ($L_{fine} = L_{coarse} + L_{intra-class}$). When data is limited relative to model parameters, minimizing this additional term forces the model to learn specific features that prevent overfitting, effectively guiding the optimizer toward better representations than the coarse loss alone.
- Core assumption: The intra-class term provides a useful learning signal rather than noise in this specific regime.
- Evidence anchors:
  - [abstract] "advantageous only when the data is limited relative to model parameters"
  - [section 2.4] "Lintra-class can effectively act as a regularizer... pushing the model... towards one that also separates subclasses"
  - [corpus] Weak/No direct corpus support for this specific loss-decomposition regularization mechanism.
- Break condition: Fails when the dataset size is sufficiently large, as the regularization effect becomes unnecessary and the optimization burden of the extra term may degrade performance.

### Mechanism 2
- Claim: The utility of fine-grained labels depends on the geometric alignment of decision boundaries between fine and coarse tasks (boundary redundancy).
- Mechanism: If the decision boundaries required to separate fine classes are distinct from the coarse boundaries (high redundancy), the model wastes capacity learning irrelevant distinctions. If boundaries align (low redundancy), the fine task provides direct structural information for the coarse task.
- Core assumption: The "boundary redundancy" accurately captures the geometric relationship between class hierarchies.
- Evidence anchors:
  - [abstract] "advantageous... when the fine and coarse classification tasks share similar decision boundaries (low boundary redundancy)"
  - [section 2.3] "fine-trained models... accuracy scales with the boundary redundancy... only label structures with low boundary redundancy continue to offer an advantage"
  - [corpus] Weak/No direct corpus support for "boundary redundancy" as a control variable.
- Break condition: Fails in high-redundancy scenarios (e.g., grouping dissimilar subclasses) where fine-grained features are irrelevant to the coarse separation.

### Mechanism 3
- Claim: Increasing label granularity imposes an optimization burden that can degrade performance if the model capacity is insufficient to satisfy both fine and coarse constraints.
- Mechanism: Fine-grained training forces the optimizer to solve a more complex objective. If the model lacks sufficient parameters or the dataset is large (constraining optimization), the model may fail to find a solution that satisfies the fine constraints without violating the coarse ones, leading to lower training accuracy on the coarse task.
- Core assumption: The difficulty in optimization stems from the conflict between minimizing coarse error and the strictness of fine-grained classification.
- Evidence anchors:
  - [section 2.1] "the more challenging task of distinguishing fine-grained classes can hinder optimization and, as a result, degrade final performance"
  - [section 3] "performance gain depends on a trade-off between the additional optimization burden... and the degree of alignment"
  - [corpus] Distant relation to "Learning from Ambiguous Data," which notes optimization difficulties with hard labels, but no direct link.
- Break condition: Fails when the model is under-parameterized relative to the complexity of the fine-grained task.

## Foundational Learning

- Concept: **Cross-Entropy Loss Decomposition**
  - Why needed here: The paper's theoretical argument relies on decomposing the fine-grained cross-entropy into a coarse term and an intra-class term.
  - Quick check question: Can you mathematically derive how $L_{fine} = L_{coarse} + L_{intra-class}$?
- Concept: **Over-parameterization Regime**
  - Why needed here: The paper distinguishes between small and large dataset regimes relative to model parameters; the benefits of fine-graining are specific to the former.
  - Quick check question: How does the ratio of parameters to data points typically affect generalization in neural networks?
- Concept: **Hierarchical Label Aggregation**
  - Why needed here: To evaluate a fine-grained model on a coarse task, one must understand how to map fine probability outputs (softmax) to coarse predictions.
  - Quick check question: How do you aggregate the softmax outputs of a 10-class model to evaluate a 2-class parent task?

## Architecture Onboarding

- Component map:
  - **Input Layer**: Flattened image features (e.g., 28x28 for MNIST)
  - **Hidden Layer**: Fully connected layer with ReLU activation (variable neuron count to control capacity)
  - **Output Layer (Fine)**: K neurons with Softmax (for fine-grained training)
  - **Aggregation Layer**: Sums specific output probabilities (e.g., $\sum_{i \in C_0} \hat{y}_i$) to form coarse predictions at inference time
- Critical path: Training uses the full K-class Softmax and Cross-Entropy loss. Evaluation requires the Aggregation Layer to map K outputs to the coarse binary prediction before calculating accuracy.
- Design tradeoffs: Increasing hidden neurons (capacity) favors fine-grained training; reducing capacity favors coarse training. You must balance model width against dataset size.
- Failure signatures:
  - **Drop in Training Accuracy**: If fine-grained training results in lower training accuracy (on coarse labels) compared to a coarse model, the optimization is over-constrained.
  - **Negative Δ Accuracy**: If `Acc_fine < Acc_coarse` on large datasets, you have exceeded the beneficial regularization regime.
- First 3 experiments:
  1. **Capacity Scaling**: Train fixed architectures on a small subset of data while increasing the number of hidden neurons to observe the shift from coarse-preferred to fine-preferred regimes (reproducing Fig 2d).
  2. **Redundancy Check**: Construct a synthetic "circles" dataset or modify CIFAR labels to create High vs. Low boundary redundancy groupings and compare coarse vs. fine test accuracy (reproducing Fig 3c).
  3. **Loss Correlation**: Track both $L_{coarse}$ and $L_{intra-class}$ during training to verify if the improvement in generalization correlates with the minimization of the intra-class term.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a weighted combination of coarse and intra-class loss ($L = L_{coarse} + \beta L_{intra-class}$) optimize the trade-off between boundary information and optimization complexity better than standard fine-grained training?
- Basis in paper: [explicit] The authors state that the loss decomposition "suggests an alternative training objective to test: $L=L_{coarse} + \beta L_{intra-class}$," proposing that tuning $\beta$ could control the trade-off.
- Why unresolved: The paper theoretically derives the decomposition of the fine-grained loss into coarse and intra-class components but only empirically tests the binary choice (pure coarse vs. pure fine) rather than the proposed hybrid approach.
- What evidence would resolve it: Empirical results from training models using the modified loss function with varying $\beta$ values on datasets with different levels of boundary redundancy and data scarcity.

### Open Question 2
- Question: Do the findings regarding boundary redundancy and model capacity hold for deep architectures like Convolutional Neural Networks (CNNs) and Transformers?
- Basis in paper: [explicit] The authors note that a "natural next step is to test our findings across a wider range of architectures (such as deeper networks, convolutional models, and attention-based transformers) and datasets."
- Why unresolved: The study intentionally employed "minimalistic" shallow fully-connected networks to establish general principles, leaving the behavior of modern, complex architectures unverified.
- What evidence would resolve it: Replicating the boundary redundancy and dataset scaling experiments using standard deep learning architectures (e.g., ResNets, ViTs) on complex datasets like ImageNet.

### Open Question 3
- Question: Can the principles of fine-grained training be extended to unsupervised learning settings where hierarchical structures must be inferred rather than explicitly labeled?
- Basis in paper: [explicit] The authors suggest the approach should be "extendable to unsupervised learning settings, particularly when handling compositional data," citing hidden stratification.
- Why unresolved: The paper focuses exclusively on supervised classification where the label hierarchy is known a priori; it does not test whether "hidden" or inferred hierarchies provide the same geometric benefits.
- What evidence would resolve it: Experiments where models cluster data to infer fine-grained subclasses unsupervised, followed by an analysis of whether these inferred subclasses improve downstream coarse-grained generalization.

## Limitations
- The theoretical loss decomposition lacks rigorous empirical validation across diverse architectures and datasets.
- The synthetic dataset construction for boundary redundancy may not fully capture real-world label hierarchy complexity.
- The parameter matching between coarse and fine models is an approximation that may not guarantee equal capacity across all scenarios.

## Confidence
- **High Confidence**: The empirical observation that fine-grained training benefits are dataset-size-dependent and tied to boundary redundancy structure (Fig 2, Fig 3).
- **Medium Confidence**: The theoretical loss decomposition and its interpretation as a regularizer in over-parameterized regimes.
- **Low Confidence**: The general applicability of boundary redundancy as a predictive metric for all real-world hierarchical classification tasks.

## Next Checks
1. **Architecture Transfer**: Replicate the study using deeper CNNs or transformers instead of MLPs to test if the regularization mechanism holds across architectures.
2. **Boundary Redundancy Quantification**: Implement a quantitative metric for boundary redundancy (e.g., normalized Hamming distance between coarse and fine decision boundaries) and test it on non-synthetic datasets.
3. **Multi-Level Hierarchy Test**: Extend the experiments to three or more hierarchical levels (e.g., super-class → sub-class → fine-class) to see if the benefits scale with hierarchy depth.