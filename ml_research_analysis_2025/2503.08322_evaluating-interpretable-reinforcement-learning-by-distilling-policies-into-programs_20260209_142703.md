---
ver: rpa2
title: Evaluating Interpretable Reinforcement Learning by Distilling Policies into
  Programs
arxiv_id: '2503.08322'
source_url: https://arxiv.org/abs/2503.08322
tags:
- interpretability
- policy
- learning
- policies
- interpretable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating the interpretability
  of reinforcement learning policies without relying on human studies. It introduces
  a new methodology that uses proxies for "simulatability" to assess interpretability,
  focusing on inference time and memory consumption.
---

# Evaluating Interpretable Reinforcement Learning by Distilling Policies into Programs

## Quick Facts
- **arXiv ID**: 2503.08322
- **Source URL**: https://arxiv.org/abs/2503.08322
- **Reference count**: 16
- **Primary result**: A methodology using simulatability proxies (inference time and memory) to evaluate interpretable RL policies without human studies, showing interpretable policies can match or exceed neural network performance.

## Executive Summary
This paper introduces a systematic methodology for evaluating interpretable reinforcement learning policies by distilling complex neural network policies into simpler, interpretable models like linear policies, decision trees, and oblique decision trees. The key innovation is using "simulatability" proxies—specifically inference time and memory consumption—to assess interpretability without relying on human studies. The authors demonstrate that increasing interpretability does not necessarily reduce performance and can sometimes enhance it, while also showing that no single policy class universally offers the best trade-off across different tasks.

## Method Summary
The methodology involves distilling pre-trained neural network policies (DQN, PPO, SAC) into interpretable classes using imitation learning (behavior cloning and DAgger variants). The distilled policies are then "unfolded" into sequential Python code to ensure fair measurement of inference time and memory size. This unfolding process converts vectorized operations and loops into explicit sequential steps that can be measured consistently. The approach is validated across classic control, MuJoCo, and object-centric Atari environments, with performance measured using episodic rewards and interpretability measured using CPU inference time and policy size in bytes.

## Key Results
- Interpretable policies can achieve performance comparable to or better than neural network experts on multiple tasks
- Different policy classes excel on different interpretability metrics (trees faster to infer, compact MLPs sometimes smaller)
- DAgger consistently outperforms behavior cloning in the distillation process
- No single policy class provides the best interpretability-performance trade-off across all environments

## Why This Works (Mechanism)

### Mechanism 1: Imitation Learning Distillation Cascade
Complex neural network policies can be systematically reduced to interpretable forms while preserving task performance. The distillation process iteratively collects state-action pairs from either the expert or student policy, always labeling with expert actions. DAgger variants outperform behavior cloning because they train on states the student actually visits, reducing distribution shift.

### Mechanism 2: Policy Unfolding for Cross-Class Comparison
Fair interpretability evaluation requires grounding all policy classes in a common execution representation. Policies are "unfolded" into sequential Python code that explicitly computes each operation without vectorization, loops, or library optimizations. This mimics how a human would mentally trace through the policy.

### Mechanism 3: Dual Simulatability Proxies
Inference time and policy size capture distinct aspects of interpretability that align with human study findings. Two complementary metrics—step inference time measures action-computation difficulty; policy size measures full-policy comprehensibility. Different policy classes excel on different metrics.

## Foundational Learning

- **Concept: Imitation Learning (DAgger/BC/Q-DAgger)**
  - Why needed here: This is the mechanism for transferring knowledge from neural experts to interpretable students.
  - Quick check question: Given an expert that visits states A and a student that visits states B, which dataset should be used for training the student at iteration i>1?

- **Concept: Simulatability vs. Other Interpretability Notions**
  - Why needed here: The paper specifically targets simulatability (can a human mentally execute the policy?) rather than post-hoc explanations or concept-level understanding.
  - Quick check question: Would a policy that is easy to simulate step-by-step but impossible to summarize globally be considered interpretable under this definition?

- **Concept: Policy Class Inductive Biases**
  - Why needed here: Linear policies, trees, oblique trees, and MLPs have different expressivity-interpretability tradeoffs; understanding when each excels is essential for methodology application.
  - Quick check question: Why might an oblique decision tree (splits on linear combinations of features) outperform a standard axis-aligned tree on continuous control tasks?

## Architecture Onboarding

- **Component map**: Expert Policy Zoo -> Imitation Learning Distillation -> Policy Class Fitters -> Unfolder -> Evaluation Suite
- **Critical path**: Expert selection → Imitation learning variant choice → Policy class fitting → Unfolding → Dual-metric evaluation
- **Design tradeoffs**:
  - More DAgger iterations: Better student performance but higher sample cost
  - Importance sampling (Q-DAgger): Focuses on critical states but requires Q-function access from expert
  - Larger policy capacity: Better performance but degraded interpretability on both metrics
- **Failure signatures**:
  - High variance in inference time across runs → Check CPU throttling, ensure dedicated hardware
  - Folded and unfolded metrics identical → Unfolding not working (e.g., hidden vectorization remains)
  - Student fails to match expert on simple tasks → Check dataset collection (expert vs. student state distribution)
- **First 3 experiments**:
  1. Validate unfolding necessity: Compare interpretability rankings from folded vs. unfolded policies on CartPole; verify that different-sized trees from the same class show different unfolded metrics.
  2. Establish environment-specific baselines: Run full distillation pipeline on one classic control, one MuJoCo, and one Atari task to identify which policy class performs best per environment.
  3. Probe the performance-interpretability frontier: For a single environment, systematically vary tree depth and MLP width to map the Pareto curve; identify the "knee point" where interpretability gains plateau relative to performance loss.

## Open Questions the Paper Calls Out

- How can the evaluation methodology be adapted to measure interpretability for programmatic policies with state-dependent control flow, where policy size fluctuates during execution?
- To what extent does parameter sparsity in linear maps and MLPs influence the simulatability proxies of inference time and memory consumption?
- Do episodic metrics of interpretability (total inference time over an episode) yield different optimal policy classes compared to step-wise metrics?

## Limitations

- The assumption that sequential Python execution reflects human cognitive effort for policy simulation may not fully capture human interpretability across diverse expertise levels
- The methodology focuses on discrete and continuous control tasks but may not generalize to high-dimensional or partially observable environments
- The unfolding procedure introduces implementation complexity that may affect reproducibility

## Confidence

- **High Confidence**: The distillation mechanism (Algorithm 1) and its empirical validation across multiple environments and imitation learning variants are well-supported by the results
- **Medium Confidence**: The unfolding methodology provides a novel solution to fair comparison, but its correlation with human cognitive effort remains theoretical without direct human studies
- **Medium Confidence**: The dual proxy approach (inference time and policy size) captures different interpretability aspects, but the relative importance of these metrics for practical applications is not fully established

## Next Checks

1. Conduct a small-scale human study comparing folded vs. unfolded policies on simple tasks to validate that unfolding metrics correlate with actual human simulation effort
2. Apply the methodology to a partially observable environment (e.g., Atari games with frame stacking) to test whether interpretable policies can still achieve reasonable performance
3. For policies with state-dependent loop lengths, implement a dynamic unfolding approach and analyze how policy size varies across different execution paths and episodes