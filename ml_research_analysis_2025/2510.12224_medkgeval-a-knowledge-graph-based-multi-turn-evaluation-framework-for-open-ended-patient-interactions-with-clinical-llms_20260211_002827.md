---
ver: rpa2
title: 'MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended
  Patient Interactions with Clinical LLMs'
arxiv_id: '2510.12224'
source_url: https://arxiv.org/abs/2510.12224
tags:
- medical
- knowledge
- agent
- patient
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MedKGEval, a multi-turn evaluation framework\
  \ for clinical LLMs grounded in structured medical knowledge graphs. The core method\
  \ uses four specialized agents\u2014Director, Patient, Doctor, and Judge\u2014to\
  \ simulate realistic doctor-patient dialogues and assess LLM performance turn-by-turn\
  \ on medication consultation and disease diagnosis tasks."
---

# MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs

## Quick Facts
- arXiv ID: 2510.12224
- Source URL: https://arxiv.org/abs/2510.12224
- Reference count: 36
- Large general-purpose LLMs outperform smaller medical-specific models in multi-turn clinical dialogue evaluation

## Executive Summary
This paper introduces MedKGEval, a multi-turn evaluation framework for clinical LLMs grounded in structured medical knowledge graphs. The core method uses four specialized agents—Director, Patient, Doctor, and Judge—to simulate realistic doctor-patient dialogues and assess LLM performance turn-by-turn on medication consultation and disease diagnosis tasks. Evaluation metrics focus on correctness, comprehensiveness, and history-taking ability, enabling fine-grained detection of clinical reasoning flaws. Experiments with eight state-of-the-art LLMs show that general-purpose models (e.g., DeepSeek-R1-671B) generally outperform smaller medical-specific models, with correctness often exceeding comprehensiveness. The framework uncovers nuanced performance differences across languages and tasks, revealing that most models struggle with synthesizing multi-turn clinical reasoning into accurate diagnoses.

## Method Summary
MedKGEval evaluates clinical LLMs through a multi-agent simulation framework where a Director Agent retrieves disease-centered subgraphs from a knowledge graph (MedKG) and controls patient symptom disclosure, a Patient Agent generates natural-language patient utterances based on Director guidance, a Doctor Agent (the evaluated LLM) engages in dialogue and provides responses/diagnoses, and a Judge Agent scores each Doctor response immediately using dialogue history and KG-grounded references. The framework uses two task-specific evaluation modes: Medication Consultation (correctness + comprehensiveness) and Disease Diagnosis (correctness + history-taking), with scores aggregated turn-by-turn. Knowledge graph cleaning involves semantic-consistency filtering to remove medically implausible triples and discriminative-symptom selection to enhance clinical realism.

## Key Results
- Large general-purpose LLMs (DeepSeek-R1-671B, GPT-4o, LLaMA3.1-70B) outperform smaller or specialized medical models in multi-turn clinical dialogue
- Correctness scores typically exceed comprehensiveness scores, indicating models often arrive at correct answers but miss important information
- Model performance degrades as dialogue turns increase, with general-purpose models showing greater resilience than specialized ones
- Cross-lingual differences exist, with English evaluations showing more premature diagnoses (1.11 turns) compared to Chinese (2.78 turns)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graph grounding produces more clinically coherent patient simulations than unguided generation.
- Mechanism: The Director Agent retrieves symptom-disease-medication relationships from MedKG and filters medically implausible combinations (e.g., gender-inconsistent conditions) before passing them to the Patient Agent. This constrains the patient's utterances to entities and relations that exist in the curated graph.
- Core assumption: The knowledge graph contains sufficiently complete and accurate medical relationships for the target tasks, and filtering rules correctly identify implausible combinations.
- Evidence anchors:
  - [abstract]: "a knowledge graph-driven patient simulation mechanism, where a dedicated control module retrieves relevant medical facts from a curated knowledge graph"
  - [Section 3]: "To mitigate such noise and enhance the clinical realism of simulated interactions, we design a domain-specific cleaning process consisting of: (1) semantic-consistency filtering, which removes medically invalid triples that conflict with basic constraints such as gender, anatomy, or pathophysiology"
  - [Section 5.3, Table 6]: Ablation shows patient utterance entity alignment with KG drops from 91.4% to 63.4% (Medication Consultation) and 95.26% to 59.6% (Disease Diagnosis) when Director Agent is removed
- Break condition: If the knowledge graph has poor coverage of the target diseases/drugs, or if filtering rules are mis-specified, patient simulations may diverge from realistic clinical presentations or omit critical discriminative features.

### Mechanism 2
- Claim: Turn-level evaluation detects reasoning degradation that post-hoc transcript review misses.
- Mechanism: The Judge Agent evaluates each Doctor Agent response immediately using dialogue history and KG-derived reference knowledge. This captures context drift, premature diagnosis, and incomplete history-taking as they occur, rather than aggregating scores after dialogue completion.
- Core assumption: The scoring rubric (Table 2) validly operationalizes clinical appropriateness, and the Judge Agent can reliably apply these rubrics across diverse dialogue contexts.
- Evidence anchors:
  - [abstract]: "an in-situ, turn-level evaluation framework, where each model response is assessed by a Judge Agent for clinical appropriateness, factual correctness, and safety as the dialogue progresses"
  - [Section 4, Table 2]: Scoring rubric for history-taking explicitly rewards "structured inquiry; covers key disease-specific features" and penalizes "partial core info collected; misses discriminative/exclusionary features"
  - [corpus]: Related work (ClinDEF, MedDialogRubrics) similarly emphasizes dynamic evaluation of clinical reasoning, but corpus papers do not directly validate turn-level vs. post-hoc comparison—evidence is primarily from this paper's framework design
- Break condition: If the Judge Agent's scoring diverges from human clinical judgment (MAE data in Table 3 shows imperfect agreement), turn-level scores may not reflect true clinical quality.

### Mechanism 3
- Claim: Multi-turn dialogue length differentially impacts models based on capacity and specialization.
- Mechanism: As dialogue turns increase, Doctor Agents must maintain context, accumulate symptoms, and defer premature conclusions. Larger general-purpose models (DeepSeek-R1-671B, GPT-4o, LLaMA3.1-70B) show more resilience; smaller or specialized models more often jump to early diagnoses with insufficient information gathering.
- Core assumption: Turn count is a proxy for reasoning complexity, and performance degradation reflects genuine reasoning limitations rather than evaluation artifact.
- Evidence anchors:
  - [Section 5.2]: "In the Chinese scenario... both diagnostic correctness and history-taking ability tend to degrade as the number of turns increases. Large-parameter, general-purpose models—DeepSeek-R1-671B, GPT-4o and LLaMA3.1-70B—display greater resilience"
  - [Section 5.2, Figure 4]: Visualization shows score distributions by turn count; smaller models cluster at lower scores with more turns
  - [Section 5.2]: DeepSeek-R1-671B averages 1.11 turns before diagnosis in English vs. 2.78 in Chinese, indicating premature convergence potentially linked to training data composition
  - [corpus]: TriMediQ and related work note similar multi-turn reasoning challenges, but do not provide direct comparative validation of this turn-length mechanism
- Break condition: If test cases are not balanced across turn counts, or if early termination is incentivized by dialogue design, degradation patterns may reflect task design rather than model capability.

## Foundational Learning

- Concept: **Knowledge Graph Construction and Filtering**
  - Why needed here: MedKGEval's entire pipeline depends on a cleaned, clinically coherent KG. Understanding how triples are extracted, validated, and filtered (semantic-consistency, discriminative symptom selection) is prerequisite to debugging patient simulation quality.
  - Quick check question: Given a set of disease-symptom triples, can you identify at least one medically implausible combination that would be caught by gender/anatomy constraints?

- Concept: **Multi-Agent Coordination Protocols**
  - Why needed here: The framework relies on four agents with distinct roles communicating in sequence. Understanding state passing (who holds dialogue history, when KG retrieval occurs, how Judge receives context) is essential for extending or debugging the pipeline.
  - Quick check question: In a single evaluation run, which agent(s) access the knowledge graph, and at what points in the dialogue loop?

- Concept: **Turn-Level vs. Holistic Evaluation Metrics**
  - Why needed here: The paper's core claim is that turn-level assessment surfaces flaws invisible to post-hoc methods. Understanding the difference—and why history-taking is scored per-turn rather than only at diagnosis—is critical for interpreting results and designing new metrics.
  - Quick check question: If a model gives a correct final diagnosis but asks irrelevant questions in turns 1-3, how would MedKGEval's scoring differ from a single-turn benchmark?

## Architecture Onboarding

- Component map:
  - MedKG (knowledge graph) -> Director Agent (DeepSeek-R1-671B) -> Patient Agent (QwQ-32B) -> Doctor Agent (evaluated LLM) -> Judge Agent (QwQ-32B)

- Critical path:
  1. Director extracts disease/drug-centered subgraph from MedKG and constructs patient profile
  2. Patient Agent generates initial utterance based on Director guidance
  3. Doctor Agent responds; Judge scores response immediately
  4. Director updates disclosure state; Patient generates next utterance
  5. Loop continues until diagnosis or symptom exhaustion
  6. Aggregate turn-level scores into final metrics

- Design tradeoffs:
  - **Judge model selection**: QwQ-32B chosen for highest human agreement (lowest MAE), but 32B model may have limitations vs. larger judges. Paper does not compare against human expert panels at scale.
  - **Director as separate agent**: Adds orchestration complexity but enables controlled, reproducible patient behavior. Ablation shows significant quality drop without it.
  - **Language-specific KGs**: Chinese (CMeKG) and English (PrimeKG) have different coverage; cross-lingual performance differences may reflect KG quality, not just model capability.

- Failure signatures:
  - Patient utterances drift from KG entities (check entity alignment % without Director)
  - Judge scores inconsistent with human review (monitor MAE periodically)
  - Doctor Agent terminates dialogue prematurely (track turn count distributions per model)
  - History-taking scores high but diagnosis scores low (indicates good questioning, poor synthesis)

- First 3 experiments:
  1. **Agent configuration validation**: Reproduce Table 3 and Table 4 results—confirm QwQ-32B Judge has lowest MAE vs. human raters on a held-out sample; confirm Patient Agent consistency scores.
  2. **Director ablation on new diseases**: Run evaluation on 50 held-out diseases not in original test set, with and without Director Agent; measure entity alignment and Doctor performance degradation.
  3. **Turn-count stratified analysis**: For a single model (e.g., DeepSeek-R1-671B), bin test cases by turn count and plot correctness/history-taking curves; verify resilience pattern holds across both languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MedKGEval be effectively integrated into reinforcement learning (RL) pipelines to improve the multi-turn reasoning capabilities of clinical LLMs?
- Basis in paper: [explicit] The Conclusion states the framework "can be integrated into reinforcement learning (RL) architectures... automatically generating challenging prompts and providing reward signals."
- Why unresolved: The current work focuses solely on evaluation and benchmarking; the feedback loop for training has not been implemented or validated.
- What evidence would resolve it: Experiments demonstrating improved performance in clinical LLMs trained via RLHF using MedKGEval's turn-level scores as reward signals.

### Open Question 2
- Question: How does the choice of the Judge Agent model impact the reliability and bias of the evaluation scores?
- Basis in paper: [inferred] Section 5.1 notes that QwQ-32B was selected as the Judge based on a human agreement test, but Table 3 shows varying Mean Absolute Errors (MAE) across models, suggesting the evaluation itself may be sensitive to the evaluator's architecture.
- Why unresolved: The framework relies on a single "best" model for judgment; the systematic bias introduced by replacing this agent with a different high-performing model (e.g., GPT-4o) remains unquantified.
- What evidence would resolve it: A correlation analysis of Doctor Agent rankings when using different SOTA models (e.g., GPT-4o, Claude 3) as the Judge Agent compared to a large-scale human expert panel.

### Open Question 3
- Question: Can the framework be extended to evaluate complex, real-world clinical scenarios such as surgical decision-making and postoperative management?
- Basis in paper: [explicit] The Limitations section states the current approach only covers Medication Consultation and Disease Diagnosis, noting that the "method could be extended to incorporate a broader spectrum of knowledge... such as surgical decision making."
- Why unresolved: The current agent prompts and subgraph extraction logic are specifically tailored for the two defined tasks; adapting them for procedural or longitudinal care reasoning requires new architectural definitions.
- What evidence would resolve it: Successful construction of specific Director/Patient agent logic for surgical scenarios and benchmarking results demonstrating the framework's ability to distinguish between correct and incorrect postoperative management plans.

## Limitations

- The evaluation framework's reliance on automatically generated patient simulations and automated scoring introduces uncertainty about validity compared to human expert judgment
- Cross-lingual performance differences may conflate model capability with differences in knowledge graph quality and coverage
- The framework currently only evaluates Medication Consultation and Disease Diagnosis, limiting generalizability to other clinical scenarios

## Confidence

- **High Confidence**: The framework's ability to detect turn-level reasoning degradation and its general pattern of larger models outperforming smaller ones on multi-turn tasks.
- **Medium Confidence**: The specific performance rankings of individual models and the claim that general-purpose models outperform medical-specific ones.
- **Low Confidence**: The absolute validity of automated scoring compared to human clinical judgment, and the generalizability of findings to real-world clinical scenarios.

## Next Checks

1. **Human Validation Study**: Conduct blind comparison of Judge Agent scores against 3 independent human clinicians on a stratified sample of 100 dialogues to quantify scoring agreement and identify systematic biases.

2. **Knowledge Graph Coverage Analysis**: Measure the percentage of clinically relevant diseases and symptoms in MedQA/NLPEC that are actually represented in the constructed knowledge graph, and assess whether missing entities systematically affect evaluation outcomes.

3. **Cross-Institutional Reproduction**: Replicate the evaluation pipeline using a different, independently constructed medical knowledge graph (e.g., SNOMED CT or UMLS) to test whether findings are robust to KG variations.