---
ver: rpa2
title: 'Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World
  Data'
arxiv_id: '2506.14704'
source_url: https://arxiv.org/abs/2506.14704
tags:
- capacity
- data
- size
- training
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how transformer architecture and data configuration
  affect memorization capacity using synthetic text datasets derived from the SNOMED
  knowledge graph. The experiments compared triplets (concept-property relationships)
  and sequences (graph traversal patterns) to evaluate memorization performance.
---

# Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data

## Quick Facts
- arXiv ID: 2506.14704
- Source URL: https://arxiv.org/abs/2506.14704
- Authors: Anton Changalidis; Aki Härmä
- Reference count: 40
- Primary result: Embedding size is the primary determinant of transformer memorization capacity on structured real-world data

## Executive Summary
This study investigates how transformer architecture and data configuration affect memorization capacity using synthetic text datasets derived from the SNOMED knowledge graph. The experiments compared triplets (concept-property relationships) and sequences (graph traversal patterns) to evaluate memorization performance. Results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Softmax activation function demonstrated greater stability and capacity compared to ReLU-based alternatives. Increasing dataset complexity improved final memorization outcomes.

## Method Summary
The study utilized synthetic text datasets derived from the SNOMED CT knowledge graph, creating two data types: triplets representing concept-property relationships and sequences representing graph traversal patterns. Experiments systematically varied transformer architecture parameters including embedding size, number of layers, and activation functions (softmax vs ReLU variants). The researchers conducted controlled comparisons to isolate the effects of each architectural component on memorization performance, measuring both learning speed and final capacity across different dataset complexities.

## Key Results
- Embedding size is the primary determinant of learning speed and memorization capacity
- Additional transformer layers provide limited benefits and may hinder performance on simpler datasets
- Softmax activation function demonstrates greater stability and capacity compared to ReLU-based alternatives

## Why This Works (Mechanism)
The study reveals that memorization capacity in transformers is primarily constrained by the model's ability to store and retrieve information rather than its capacity for complex reasoning. Larger embedding sizes provide more representational space for storing distinct patterns, while simpler activation functions like softmax maintain more stable gradient signals during training. The synthetic nature of the datasets allows for precise measurement of memorization by eliminating confounding factors present in natural language data, such as ambiguity and context-dependence.

## Foundational Learning
- **Knowledge Graphs**: Structured representations of relationships between concepts (why needed: provides the basis for synthetic dataset creation; quick check: can be represented as directed graphs with labeled edges)
- **Transformer Architecture**: Neural network design using self-attention mechanisms (why needed: the model being studied; quick check: consists of encoder/decoder stacks with attention layers)
- **Embedding Spaces**: Vector representations that capture semantic relationships (why needed: fundamental to how transformers store information; quick check: dimensionality determines storage capacity)
- **Memorization vs Generalization**: Distinction between rote learning and pattern abstraction (why needed: the core phenomenon being studied; quick check: memorization shows perfect recall of training data)
- **Activation Functions**: Mathematical operations that introduce non-linearity (why needed: critical for gradient flow and model capacity; quick check: softmax maintains probability distribution properties)

## Architecture Onboarding
Component Map: Input -> Embedding Layer -> Transformer Blocks -> Softmax/ReLU Activation -> Output
Critical Path: Embedding size directly determines memorization capacity, with activation function choice affecting training stability
Design Tradeoffs: Wider embeddings provide more capacity but increase computational cost; additional layers may add complexity without proportional benefits on simpler datasets
Failure Signatures: Shallow architectures on complex datasets show poor memorization; ReLU activations exhibit training instability on structured data
First Experiments: 1) Compare embedding sizes (64, 128, 256) on triplet datasets; 2) Test single vs multi-layer architectures on sequence data; 3) Benchmark softmax against ReLU activation functions

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize to natural language corpora with less rigid semantic relationships
- Synthetic dataset structure may artificially constrain pattern complexity, potentially overestimating embedding width benefits
- Limited architectural variations in activation function comparison may not represent general principles

## Confidence
**Major Claim Confidence Labels:**
- Embedding size as primary determinant: **High** - supported by consistent experimental results across multiple configurations
- Additional layers provide limited benefits: **Medium** - based on synthetic datasets that may not reflect real-world complexity
- Softmax superiority over ReLU: **Medium** - demonstrated in controlled settings but may not generalize

## Next Checks
1. Replicate experiments using natural language corpora with varying semantic complexity to test generalizability of architectural recommendations
2. Test the same architectural configurations on non-knowledge-graph structured data (e.g., social network relationships, biological taxonomies) to validate the synthetic dataset findings
3. Conduct ablation studies varying both embedding width and depth simultaneously across multiple real-world datasets to identify optimal trade-offs beyond the observed patterns