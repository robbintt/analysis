---
ver: rpa2
title: 'Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity
  Return Prediction: A Comparative Benchmark Study'
arxiv_id: '2512.06630'
source_url: https://arxiv.org/abs/2512.06630
tags:
- quantum
- stock
- qtcnn
- classical
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of cross-sectional equity return
  prediction in noisy and dynamic financial markets, where classical models often
  struggle with generalization and overfitting. The authors propose a Quantum Temporal
  Convolutional Neural Network (QTCNN) that combines a classical temporal encoder
  with parameter-efficient quantum convolution circuits.
---

# Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study

## Quick Facts
- arXiv ID: 2512.06630
- Source URL: https://arxiv.org/abs/2512.06630
- Authors: Chi-Sheng Chen; Xinyu Zhang; Rong Fu; Qiuzhe Xie; Fan Zhang
- Reference count: 40
- Primary result: QTCNN achieves 0.538 Sharpe ratio, outperforming best classical baseline by 72%

## Executive Summary
This paper proposes Quantum Temporal Convolutional Neural Networks (QTCNN) for cross-sectional equity return prediction, addressing the challenge of noisy, dynamic financial markets where classical models often struggle with generalization and overfitting. The hybrid architecture combines a classical temporal encoder with parameter-efficient quantum convolution circuits, achieving superior out-of-sample performance on the JPX Tokyo Stock Exchange dataset. The model demonstrates that quantum-enhanced feature representation can improve financial forecasting robustness compared to both pure classical and pure quantum baselines.

## Method Summary
QTCNN processes sequential technical indicators through a classical temporal CNN encoder that extracts multi-scale patterns, producing an 8-dimensional representation. This is fed into a parameter-efficient quantum circuit using angle embedding and hierarchical quantum convolution layers with intra-layer parameter sharing to mitigate barren plateaus. The single-qubit measurement output is concatenated with classical features and passed through a small MLP classifier. The model is trained on extreme samples only (top/bottom 200-300 stocks per day) using binary cross-entropy loss, with out-of-sample performance measured by annualized Sharpe ratio.

## Key Results
- QTCNN achieves 0.538 Sharpe ratio on 22 out-of-sample days
- Outperforms best classical baseline (Transformer) by approximately 72% (0.538 vs 0.313)
- Demonstrates superior robustness compared to pure quantum (QCNN: 0.361) and pure classical (LSTM: 0.044) approaches
- Parameter sharing reduces quantum circuit parameters from O(n_q · L) to O(L), mitigating barren plateaus

## Why This Works (Mechanism)

### Mechanism 1: Classical Temporal Encoding Preserves Sequential Structure
A classical temporal CNN encoder preserves sequential dynamics that would be lost through direct PCA dimensionality reduction. Instead of applying PCA directly to tabular features, QTCNN processes the T×F feature sequence through 1D convolutions followed by global average pooling, producing a semantically richer n_q-dimensional representation before quantum encoding. This captures temporal patterns (momentum, volatility regimes) predictive of future returns.

### Mechanism 2: Intra-Layer Parameter Sharing Mitigates Barren Plateaus
Sharing a single parameter set θ(ℓ) ∈ R^6 across all adjacent qubit pairs within each layer improves trainability while maintaining expressibility. Unlike standard QCNN assigning independent parameters to each qubit pair, QTCNN uses only 6 parameters per layer regardless of qubit count, reducing gradient variance and avoiding flat loss landscapes. This reduces quantum circuit parameters from O(n_q · L) to O(L).

### Mechanism 3: Hybrid Quantum-Classical Feature Concatenation
Concatenating quantum circuit outputs with classical feature vectors before final classification improves out-of-sample generalization over pure quantum or pure classical approaches. The single-qubit measurement q ∈ R is concatenated with the classical representation z, then passed through an MLP, allowing the model to learn complementary feature combinations. This enables quantum features to capture non-linear structures while classical features provide linear signals.

## Foundational Learning

### Concept: Variational Quantum Circuits and Parameter-Shift Rule
Why needed: QTCNN uses parameterized quantum circuits with trainable rotation angles; understanding gradient flow through quantum circuits is essential for debugging training. Quick check: Can you explain why measuring Pauli-Z expectation values enables backpropagation, and what the parameter-shift rule computes?

### Concept: Barren Plateau Phenomenon
Why needed: The paper explicitly addresses barren plateaus as motivation for parameter sharing; understanding this explains why shallow circuits with fewer parameters are preferred in NISQ-era models. Quick check: Why do deep, highly parameterized variational quantum circuits suffer from vanishing gradients similar to classical deep networks?

### Concept: Cross-Sectional vs. Time-Series Normalization
Why needed: The prediction task ranks stocks within each day (long-short portfolio), not predicting absolute returns; preprocessing must align with this objective. Quick check: How does cross-sectional z-score normalization (per-day mean/std) differ from rolling normalization, and why does it better serve ranking tasks?

## Architecture Onboarding

### Component Map:
Input (T=20 × F features per stock) → [Classical 1D Temporal CNN + GAP] → z ∈ R^8 → [Angle Embedding: RY rotations] → |ψ(z)⟩ on 8 qubits → [Quantum Conv Layers (L≤3) + Pooling] → hierarchical features → [Single Qubit ⟨Z⟩ Measurement] → q ∈ R → [Concatenation]: [q; z] ∈ R^9 → [MLP: 64→32→1 + Sigmoid] → score ŷ

### Critical Path:
1. Data preprocessing: Adjusted close computation → technical indicators → cross-sectional winsorization (1st/99th percentile) → z-score normalization
2. Training constraint: Binary cross-entropy on "extreme samples" only (top/bottom p=200-300 stocks per day)
3. Circuit depth bound: L_eff ≤ ⌊log₂(n_q)⌋ = 3 for 8 qubits to prevent over-pooling

### Design Tradeoffs:
- 8 qubits vs. more: Provides 256-dim state space (CPU-simulable); 12+ qubits require GPU backends
- Stride sampling (k=11): 9% of trading days enables systematic comparison across 9 architectures but limits regime coverage (22 OOS days)
- Weak supervision: Training on only extreme performers increases signal-to-noise but may miss mid-range patterns

### Failure Signatures:
- Near-zero Sharpe with low variance (e.g., LSTM: 0.044±0.029): Model captured noise, not signal
- Very wide confidence intervals (e.g., QCNN: ±0.298): Unstable training or poor generalization
- Quantum underperforms classical: Check angle embedding collapse or insufficient entanglement

### First 3 Experiments:
1. Temporal encoder ablation: Replace CNN encoder with PCA projection; expect ΔSR ≈ -0.177 (gap between QTCNN 0.538 and QCNN 0.361)
2. Circuit depth sweep: Test L ∈ {1, 2, 3} with fixed parameter sharing to validate shallow-depth design principle
3. Training sample sensitivity: Vary stride k ∈ {5, 11, 21} to test whether gains persist with more data or are subsample artifacts

## Open Questions the Paper Calls Out

### Open Question 1
Can the QTCNN maintain its performance advantage when scaled to full datasets and deployed on noisy intermediate-scale quantum (NISQ) hardware? The authors acknowledge that "computational constraints... necessitated a stride-based subsampling strategy" and that "experiments... assume ideal, noiseless quantum simulation." This remains unresolved due to the exponential cost of simulation limiting the study to 9% of the data, and the noise resilience of the parameter-efficient circuits has not been validated on physical devices.

### Open Question 2
Does the QTCNN architecture possess specific inductive biases that provide a provable theoretical advantage for financial time-series? The conclusion states that "a rigorous theoretical analysis of its expressibility, trainability, and potential quantum advantage remains an open question." This remains unresolved because the current work relies on empirical benchmarking rather than theoretical proofs regarding the model's capacity to capture financial data structures.

### Open Question 3
Is the model robust to diverse market regimes, specifically tail events and structural breaks not captured in the subsampled data? The authors note the subsampled dataset "may not fully capture tail events... or regime transitions" and the 22-day OOS period limits statistical power. This remains unresolved because the dataset reduction and short testing window limit the confidence that the model generalizes through high-volatility periods or market crises.

## Limitations
- Architecture specification gap: Exact temporal CNN architecture and concatenated classical features are unspecified
- Dataset temporal coverage: 22 trading days out-of-sample raises questions about statistical significance of 72% improvement
- Quantum hardware considerations: All experiments use CPU simulators; results may not translate to actual quantum hardware with noise and errors

## Confidence
**High Confidence**: The hybrid architecture concept (classical encoder + quantum circuit + classical classifier) is well-specified and theoretically sound. The barren plateau mitigation through parameter sharing is explicitly justified and mechanistically explained.

**Medium Confidence**: The empirical results showing QTCNN outperforming all classical baselines by substantial margins are internally consistent but limited by small OOS sample size. The 72% improvement claim is statistically fragile.

**Low Confidence**: Claims about quantum advantage mechanisms (superposition/entanglement improving feature representation) are plausible but not empirically isolated from other factors like the classical encoder's quality or hyperparameter tuning.

## Next Checks
1. Temporal encoder ablation study: Implement the identical quantum circuit but replace the temporal CNN encoder with PCA dimensionality reduction. If SR drops from 0.538 to near 0.361 (QCNN performance), this validates the classical encoding contribution.

2. Expanded temporal validation: Re-run the experiment with stride k=5 (doubling the OOS sample size to ~44 days) to assess whether the Sharpe advantage persists with more statistical power and whether confidence intervals narrow appropriately.

3. Circuit depth sensitivity: Systematically test QTCNN with L=1, 2, 3 layers while keeping parameter sharing constant. Verify that performance degrades or plateaus beyond L=3, confirming the theoretical shallow-depth design principle.