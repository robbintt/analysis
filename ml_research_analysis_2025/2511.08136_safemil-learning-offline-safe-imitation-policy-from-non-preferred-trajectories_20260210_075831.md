---
ver: rpa2
title: 'SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories'
arxiv_id: '2511.08136'
source_url: https://arxiv.org/abs/2511.08136
tags:
- cost
- trajectories
- learning
- policy
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SafeMIL, a novel offline safe imitation learning
  method that addresses the challenge of learning safe policies from limited non-preferred
  trajectories and large unlabeled datasets without explicit reward or cost information.
  SafeMIL formulates cost function learning as a multiple instance learning problem,
  using negative and unlabeled trajectory bags to train a cost predictor that identifies
  risky state-action pairs.
---

# SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories

## Quick Facts
- arXiv ID: 2511.08136
- Source URL: https://arxiv.org/abs/2511.08136
- Authors: Returaj Burnwal; Nirav Pravinbhai Bhatt; Balaraman Ravindran
- Reference count: 40
- One-line result: SafeMIL outperforms state-of-the-art baselines in velocity-constrained and navigation tasks, achieving 3.7× better safety metrics while maintaining high reward performance.

## Executive Summary
SafeMIL addresses the challenge of learning safe policies from limited non-preferred trajectories and large unlabeled datasets without explicit reward or cost information. The method formulates cost function learning as a multiple instance learning (MIL) problem, using negative and unlabeled trajectory bags to train a cost predictor that identifies risky state-action pairs. The learned cost function then guides the identification of preferred behaviors in the unlabeled dataset, enabling safe policy learning via behavior cloning. Empirically, SafeMIL demonstrates superior safety performance while maintaining competitive reward metrics across various simulated tasks.

## Method Summary
SafeMIL learns a safe policy from offline data containing both non-preferred trajectories (with known safety issues) and unlabeled trajectories. It first learns a cost function using MIL by contrasting bags of trajectories from non-preferred data against mixed unlabeled data. The cost network is trained via a Bradley-Terry loss to assign higher scores to negative bags. This learned cost function is then used to weight trajectories in the unlabeled dataset, with safer trajectories receiving higher weights. Finally, a policy is trained via weighted behavior cloning on the unlabeled dataset, effectively learning to imitate safe behaviors while avoiding risky ones. The method operates entirely offline without environment interaction or explicit reward/cost labels.

## Key Results
- Achieves 3.7× better safety metrics (Normalized Cost and CVaR Cost) compared to state-of-the-art baselines
- Maintains high reward performance (Normalized Return near 1.0) while improving safety
- Demonstrates robustness to partial trajectory sampling and variations in bag size
- Successfully learns effective policies using partial trajectories (H=5 or 10) rather than requiring full trajectories

## Why This Works (Mechanism)

### Mechanism 1
The system infers a dense cost function from weak trajectory-level labels by contrasting aggregate behaviors using Multiple Instance Learning (MIL). The method constructs "bags" of trajectories, with negative bags sampled entirely from non-preferred data and unlabeled bags from mixed data. A cost network is trained via Bradley-Terry loss to assign higher total cost scores to negative bags than to unlabeled bags. This forces the network to learn that specific state-action patterns appearing frequently in non-preferred data correlate with higher risk. The core assumption is that for sufficiently large bag sizes, an unlabeled bag is statistically likely to contain at least one preferred trajectory, ensuring score separation.

### Mechanism 2
A policy can be tuned for safety without explicit cost annotations by performing weighted behavior cloning on data filtered by the learned cost. Once the cost network is trained, it evaluates all trajectories in the unlabeled dataset. Trajectories predicted to have lower cumulative cost are up-weighted in the imitation loss, while high-cost trajectories are down-weighted. The policy network minimizes this weighted behavior cloning loss, effectively mimicking the actions in low-cost (safe) trajectories while ignoring or down-weighting high-cost (risky) behaviors in the same dataset.

### Mechanism 3
Training remains robust to partial trajectory sampling due to statistical averaging within large bags. Instead of full trajectories, the model can use segments of length H. While a segment from a non-preferred trajectory might look "safe" (mislabeling risk), sampling large bags ensures that the average score of the non-preferred bag remains higher than the unlabeled bag, preserving the loss gradient direction. This statistical smoothing allows the method to work with shorter trajectory segments without sacrificing safety performance.

## Foundational Learning

- **Concept: Multiple Instance Learning (MIL)**
  - Why needed here: This is the core theoretical framework for learning from weak labels. You must understand "bags" (sets of instances) vs. "instances" (single trajectories) and how a single positive instance makes a bag positive.
  - Quick check question: If you sample a bag of 10 trajectories from the non-preferred dataset, is it guaranteed to have a higher learned cost than a bag from the unlabeled dataset? (Answer: Not guaranteed, but statistically probable given Lemma 1).

- **Concept: Constrained Markov Decision Process (CMDP)**
  - Why needed here: The definition of "safety" relies on cost constraints. You need to distinguish between reward (task performance) and cost (safety violation), and how standard RL maximizes the former while Safe RL bounds the latter.
  - Quick check question: Does the SafeMIL agent have access to the ground-truth cost function c_i(s, a) during training? (Answer: No, it learns a proxy ĉ_θ from trajectory labels).

- **Concept: Offline Imitation Learning (BC)**
  - Why needed here: The final policy is derived via behavior cloning. You need to understand that the agent cannot interact with the environment to correct mistakes; it must extract the policy entirely from the static dataset.
  - Quick check question: Why does standard BC fail on the unlabeled dataset in this context? (Answer: Because the unlabeled dataset contains non-preferred trajectories, so naive BC would clone risky behaviors).

## Architecture Onboarding

- **Component map:**
  1. Data Loader: Samples K trajectories to form Negative Bags (D_N) and Unlabeled Bags (D_U)
  2. Cost Network (ĉ_θ): MLP that takes (s, a) pairs and outputs a scalar in (0,1). Shared weights process all pairs in a bag
  3. Aggregator: Sums discounted costs over trajectories and averages over the bag
  4. Policy Network (π): Standard actor network trained via Weighted BC on D_U

- **Critical path:**
  1. Tuning the Bag Size (K). This is the most sensitive hyperparameter. If K is too low, the MIL assumption (Lemma 1) fails, and the cost network cannot converge on a meaningful separation
  2. Validating the Cost Network. Before training the policy, verify that the trained ĉ_θ assigns distinct scores to known non-preferred vs. preferred validation trajectories

- **Design tradeoffs:**
  - Full vs. Partial Trajectories (H): Full trajectories are theoretically cleaner but computationally expensive. Partial (H=5 or 10) is faster and shown to be robust, provided K is large (≥ 64)
  - Hard Filtering vs. Soft Weighting: The paper uses soft weights rather than a hard threshold. Soft weighting is more stable if the cost function is noisy

- **Failure signatures:**
  - Cost Collapse: The cost network outputs constant values for both Negative and Unlabeled bags (Loss ≈ log(0.5)). Usually caused by K being too small or learning rate instability
  - Safe but Immobile: The policy learns to avoid risk by doing nothing (low cost, zero reward). This implies the non-preferred data overlaps too heavily with high-reward regions, causing the cost network to penalize task completion

- **First 3 experiments:**
  1. Baseline Ablation: Run standard BC on D_U. You should see high cost/risk. This confirms the dataset is indeed "contaminated" with non-preferred data
  2. Bag Size Sensitivity (K): Sweep K ∈ {1, 8, 16, 64, 128}. Plot the MIL Loss. You should see loss decrease and safety improve as K increases
  3. Policy Extraction Test: Train π using the ground truth costs (if available in simulation) vs. the learned costs. The gap indicates the error introduced by the MIL approximation

## Open Questions the Paper Calls Out

- **Open Question 1:** How does SafeMIL performance degrade when the assumption that non-preferred trajectories have "similar costs" is violated, specifically in scenarios with heterogeneous failure modes? The paper relies on the explicit assumption that "non-preferred trajectories have similar cost" to guarantee the score relationship between negative and unlabeled bags, but doesn't test datasets where "risky" trajectories vary wildly in cost magnitude.

- **Open Question 2:** Would replacing the additive score function (Equation 6) with attention-based permutation-invariant architectures (e.g., Deep Sets or Transformer encoders) improve the detection of sparse risky state-action pairs? The paper opted for intuitive (identity/sum) choices rather than complex neural architectures, potentially missing opportunities to better isolate sparse critical states.

- **Open Question 3:** Can the method scale to high-dimensional vision-based domains where "risky" features must be learned from pixels rather than low-dimensional state vectors? While the Introduction cites autonomous driving as motivation, the Experiments limit evaluation to state-based MuJoCo and Navigation tasks, leaving open questions about applicability to vision-based domains.

## Limitations

- The method's performance heavily depends on the quality and representativeness of the non-preferred dataset D_N, with contamination from high-reward trajectories potentially causing the cost network to penalize task completion
- The theoretical guarantees are asymptotic and may not hold for all data distributions, particularly when bag size K is insufficient for statistical separation
- The approach is validated primarily on locomotion and navigation tasks, with unclear generalization to other domains like manipulation or multi-agent scenarios

## Confidence

- **High Confidence**: The empirical demonstration that SafeMIL outperforms baselines on the DSRL benchmark tasks with well-defined quantitative metrics (Normalized Cost, CVaR Cost, Normalized Return) that are statistically significant across 5 seeds
- **Medium Confidence**: The theoretical foundation of the MIL formulation (Lemma 1 and Bradley-Terry loss) which relies on assumptions about bag size and trajectory distribution that are only partially validated
- **Low Confidence**: The claim of robustness to partial trajectories (H=5 or 10) without extensive ablation, as the paper shows stability within tested ranges but doesn't explore edge cases where very short segments might contain insufficient context

## Next Checks

1. **Bag Size Sensitivity**: Replicate the sensitivity analysis for K ∈ {1, 8, 16, 32, 64, 128, 256} on a held-out validation task. Plot the MIL loss convergence and safety metrics to identify the minimum K required for reliable performance separation.

2. **Non-Preferred Data Quality Impact**: Create controlled experiments where D_N is gradually mixed with high-reward trajectories (e.g., 0%, 25%, 50%, 75% contamination). Measure how this affects the learned cost function's ability to distinguish safe behaviors and the resulting policy performance.

3. **Cross-Domain Generalization**: Apply SafeMIL to a different task family not in the DSRL benchmark (e.g., robotic manipulation or multi-agent coordination). Compare performance against standard BC and verify whether the bag-based MIL approach generalizes beyond the tested locomotion and navigation domains.