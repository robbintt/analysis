---
ver: rpa2
title: Automatic selection of the best neural architecture for time series forecasting
  via multi-objective optimization and Pareto optimality conditions
arxiv_id: '2501.12215'
source_url: https://arxiv.org/abs/2501.12215
tags:
- architectures
- optimal
- time
- preference
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a flexible automated framework for time series
  forecasting that systematically designs and evaluates diverse neural network architectures
  by integrating LSTM, GRU, multi-head Attention, and State-Space Models (SSMs). Using
  a multi-objective optimization approach, the framework determines the optimal number,
  sequence, and combination of blocks to align with specific requirements and evaluation
  objectives.
---

# Automatic selection of the best neural architecture for time series forecasting via multi-objective optimization and Pareto optimality conditions

## Quick Facts
- arXiv ID: 2501.12215
- Source URL: https://arxiv.org/abs/2501.12215
- Reference count: 40
- This paper introduces a flexible automated framework for time series forecasting that systematically designs and evaluates diverse neural network architectures by integrating LSTM, GRU, multi-head Attention, and State-Space Models (SSMs). Using a multi-objective optimization approach, the framework determines the optimal number, sequence, and combination of blocks to align with specific requirements and evaluation objectives. The selection of the best model is made via a user-defined preference function from the resulting Pareto-optimal architectures.

## Executive Summary
This paper proposes a novel framework for automatically selecting optimal neural architectures for time series forecasting through multi-objective optimization and Pareto optimality. The method treats architecture design as a parameterized search problem, combining LSTM, GRU, Attention, and SSM blocks in various sequences and quantities. By evaluating architectures across multiple objectives (accuracy, training time, parameter count), the framework identifies Pareto-optimal solutions and uses preference functions to select the final model based on user priorities. Results across four diverse applications demonstrate that composite architectures often outperform single-block designs when balancing multiple objectives.

## Method Summary
The framework parameterizes neural network architectures using four hyperparameters: the count of each block type (x1=[#GRU,#LSTM,#Attention,#SSM]), their sequence (x2), hidden dimensions (x3), and number of heads for Attention (x4). A search space is defined by constraining these parameters within specified ranges. All candidate architectures are trained, and their performance is evaluated across three objectives: relative L2 error, training time, and parameter count. The Pareto front is computed by identifying non-dominated architectures, and users select their preferred model through weighted preference functions. The framework also implements a Linear Programming approach to "rediscover" the preference weights that would have selected a given optimal architecture.

## Key Results
- Across four applications (glucose, wave height, VIV, motion prediction), the framework identified composite architectures that outperformed single-block designs when balancing accuracy and efficiency
- For minimizing training time alone, single-layer GRU or LSTM architectures were typically optimal
- The best architectures for maximizing accuracy or balancing multiple objectives were often composite designs incorporating multiple block types in specific configurations
- The results demonstrate that no single neural architecture is universally optimal for time series forecasting

## Why This Works (Mechanism)

### Mechanism 1: Composite Block Sequencing
If specific temporal dependencies (short-term, long-term, or non-linear) are better captured by different base architectures, then a composite sequence of blocks may outperform a monolithic single-block architecture for specific accuracy-cost trade-offs. The framework parameterizes a neural network not as a fixed architecture, but as a sequence of heterogeneous blocks (GRU, LSTM, multi-head Attention, and SSM). The number of blocks x1=[n,m,j,k] and their sequence x2 are treated as tunable hyperparameters. This allows the optimization process to construct hybrid models (e.g., [SSM, Attention, GRU]) that leverage the distinct inductive biases of each block type—e.g., using SSMs for long-range efficiency and Attention for focus. The optimal architecture is not universal but is data-dependent and preference-dependent; the "sum" of different block capabilities can be quantitatively better than a single block type for specific objectives.

### Mechanism 2: Multi-Objective Optimization (MOO) via Pareto Optimality
If optimal architecture selection involves conflicting metrics (e.g., minimizing error vs. minimizing training time), then a Pareto front approach is required to identify the set of viable candidates rather than a single "best" model. The framework models performance as a vector-valued map J(x)→R^N (mapping architecture parameters to [L2 error, training time, parameter count]). It solves for the set of Pareto optimal solutions where no single architecture dominates another in all objectives. This decouples the search for efficiency from the preference for accuracy, revealing the true cost of marginal accuracy gains. Users value different metrics differently (e.g., edge devices value low parameters, while offline research values low error), and a single scalar objective function (like simple accuracy) obscures critical trade-offs.

### Mechanism 3: Preference-Based Resolution via Linear Programming (LP)
If the user's priority is definable as a preference function (e.g., "accuracy is worth 10x more than speed"), then a weighted sum or LP approach can mathematically identify the unique best architecture from the Pareto set. The framework allows users to define a preference function p (often a weighted sum of normalized metrics). It normalizes the Pareto-optimal performance data to [0,1] and selects the architecture minimizing p∘f(x). Crucially, it also performs "rediscovery": given an optimal architecture, it uses Linear Programming to reverse-engineer the weights (λ) that would have selected it, explaining why a specific architecture is optimal. User preferences can be approximated by linear or piecewise-linear functions of the metrics.

## Foundational Learning

### Concept: Pareto Optimality and Non-Dominated Sorting
**Why needed here**: The core of the paper is finding the set of architectures where you cannot improve one metric (accuracy) without degrading another (speed). Understanding "dominance" is required to interpret the scatter plots (Figures 2-8).
**Quick check question**: Given Model A (Error: 5%, Time: 10s) and Model B (Error: 6%, Time: 10s), which one dominates?

### Concept: Inductive Biases of RNN vs. Transformer vs. SSM
**Why needed here**: The framework relies on combining these blocks. One must understand that GRU/LSTM handles recurrence, Attention handles global dependencies (at quadratic cost), and SSMs (like Mamba) handle long sequences efficiently to interpret why the optimizer chose a specific sequence.
**Quick check question**: Why might the optimizer prefer an SSM block over a multi-head Attention block when the lookback window increases from 500 to 900 steps?

### Concept: Hyperparameter Search Spaces and Parameterization
**Why needed here**: The method treats architecture as a hyperparameter vector x=[x1,x2,x3,x4]. Understanding how to define this search space (discrete vs. continuous) is critical for replicating the experiment.
**Quick check question**: In the paper, x1 represents the count of blocks. Why is the combination [0,0,0,0] explicitly removed from the search space?

## Architecture Onboarding

### Component map
Embedding -> Configurable Block Sequence ([SSM, Attention, GRU, LSTM] ordered by x2) -> Output Projection (LN)

### Critical path
1. Define Constraints: Set ranges for hidden dimensions, block counts (0-2 or 0-3), and sequences (Table 1)
2. Execute Search: Train all candidate architectures (e.g., 708 or 1530 designs) on the target dataset
3. Compute Front: Filter results to identify non-dominated architectures (Pareto front)
4. Apply Preference: Select the final model based on user-defined weights (λ1,λ2,λ3)

### Design tradeoffs
- Search Cost vs. Optimality: The paper notes a "significant offline computational cost" because ~95% of trained architectures are non-Pareto-optimal. A full grid search is expensive; sampling strategies are suggested as future work
- Block Complexity: Increasing the number of blocks (x1) increases parameters and time but does not guarantee lower error (risk of overfitting)

### Failure signatures
- Single-Block Dominance: If the Pareto front collapses to a single point (e.g., a 1-layer GRU is always best), the dataset may be too simple for composite architectures
- Instability in Complex Composites: Very deep composite stacks (e.g., [Attention=2, GRU=2, LSTM=2]) might fail to converge if learning rates are not tuned for the combined complexity

### First 3 experiments
1. Baseline Comparison: Run the framework on a simple dataset (e.g., Glucose) with a reduced search space (only 1 block allowed) vs. full composite search to verify that composites actually land on the Pareto front
2. Preference Sensitivity: Use the "rediscovery" LP method (Section 3.4) on a known good architecture. Check if the derived weights (λ) match intuition (e.g., does the high-accuracy model get a high λ1?)
3. Sequence Ablation: Fix the number of blocks but vary the sequence (x2) to verify that order matters (e.g., does [SSM→Attention] perform differently than [Attention→SSM]?)

## Open Questions the Paper Calls Out

### Open Question 1
Can an efficient sampling method be developed to reduce the number of architectures evaluated while preserving the Pareto-optimal solutions?
The authors state in the conclusion that "one interesting direction for future work is to develop an efficient sampling method, in the parameterized space of architectures, that will reduce significantly the total number of architectures while preserving the Pareto-optimal ones." The current framework requires training all candidate architectures, incurring a significant offline computational cost as roughly 95% of trained architectures are non-Pareto-optimal.

### Open Question 2
Does replacing the "training time" objective with "inference latency" significantly alter the resulting Pareto-optimal architectures?
The paper emphasizes "real-time forecasting" in applications like offshore drilling and diabetes management, yet the multi-objective cost function explicitly minimizes training time (f2) rather than inference speed. Architectures that are fast to train (e.g., single-layer GRUs) are not necessarily the fastest during inference, which is the critical constraint for the stated real-time applications.

### Open Question 3
Does the optimal architecture change if the search space is expanded to include convolutional layers or significantly deeper networks?
The methodology limits the composite model to LSTM, GRU, Attention, and SSM blocks, restricting the number of blocks per type to a small range (0-2 or 0-3). It is unclear if the identified "optimal" architectures are global optima within the broader universe of deep learning or if they are artifacts of the restricted search space (e.g., missing CNNs for local feature extraction).

## Limitations
- The exhaustive search approach requires training all candidate architectures, incurring significant computational cost since ~95% of models are non-Pareto-optimal
- The framework assumes user preferences can be adequately captured by linear weighted sums, which may not hold for complex trade-off scenarios
- The specific SSM implementation details are not fully specified, potentially limiting exact reproduction

## Confidence
- **High**: The claim that composite architectures outperform single-block designs for balancing multiple objectives is well-supported by empirical results across glucose and VIV datasets
- **Medium**: The assertion that no single architecture is universally optimal is supported for glucose and VIV, but less clear for wave height and motion prediction where single-layer GRU/LSTM often emerge as optimal
- **Low**: The effectiveness of the preference-based Linear Programming "rediscovery" method lacks validation within the provided corpus

## Next Checks
1. Replicate the glucose experiment with a reduced search space (single-block only) to verify that composite architectures actually improve upon single-block Pareto fronts
2. Apply the LP rediscovery method to a known optimal architecture and verify that the derived weights align with the architecture's observed performance characteristics
3. Perform ablation studies varying the sequence order of blocks while fixing the block counts to quantify the importance of architectural sequencing