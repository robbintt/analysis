---
ver: rpa2
title: Diffusion Models for Inverse Problems in the Exponential Family
arxiv_id: '2502.05994'
source_url: https://arxiv.org/abs/2502.05994
tags:
- distribution
- exponential
- function
- family
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to extend diffusion models to handle
  inverse problems where observations follow distributions from the exponential family
  (e.g., Poisson or Binomial). The key innovation is the "evidence trick," which leverages
  conjugate priors to approximate the intractable likelihood score function in a tractable
  way.
---

# Diffusion Models for Inverse Problems in the Exponential Family

## Quick Facts
- **arXiv ID:** 2502.05994
- **Source URL:** https://arxiv.org/abs/2502.05994
- **Reference count:** 40
- **Primary result:** Introduces the "evidence trick" to extend diffusion models to inverse problems with exponential family likelihoods using conjugate priors for tractable likelihood score approximation.

## Executive Summary
This paper presents a method to extend diffusion models to inverse problems where observations follow distributions from the exponential family (e.g., Poisson or Binomial). The key innovation is the "evidence trick," which leverages conjugate priors to approximate the intractable likelihood score function in a tractable way. The method involves training a neural network to infer posterior hyperparameters and computing gradients via automatic differentiation. Experiments demonstrate that the approach aligns well with ground-truth MCMC results and performs competitively with state-of-the-art methods in real-world applications like malaria prevalence estimation.

## Method Summary
The method extends diffusion models to inverse problems with exponential family likelihoods by approximating the intractable likelihood score using conjugate priors. It introduces an "evidence trick" that computes closed-form gradients via the evidence integral, avoiding Tweedie's formula. The approach uses an inference network to predict variational hyperparameters, transforming variational inference into a supervised denoising task. Sampling is performed via stochastic differential equations with combined prior and likelihood score gradients. The method scales to larger datasets and empirical priors, broadening diffusion model applicability in scientific domains.

## Key Results
- Successfully recovers 1D signals from Poisson observations with GP priors, matching MCMC ground truth.
- Handles high-dimensional score-based Cox processes from sparse Poisson event counts using ImageNet priors.
- Interpolates real-world malaria prevalence rates (Binomial) across Africa, demonstrating practical applicability.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enables tractable approximation of likelihood score for non-Gaussian observations where Tweedie-based approaches fail.
- **Mechanism:** Approximates posterior over parameter θ using conjugate prior, yielding closed-form evidence integral and analytically computable likelihood score.
- **Core assumption:** Observation likelihood belongs to one-parameter exponential family.
- **Evidence anchors:** Abstract states "By leveraging conjugacy properties... we introduce the evidence trick." Section 3.3 derives the variational approximation using conjugate priors.

### Mechanism 2
- **Claim:** Inference network can predict variational posterior hyperparameters without expensive Monte Carlo expectations.
- **Mechanism:** Derives training objective independent of reverse process expectation, transforming VI into supervised denoising task.
- **Core assumption:** Variational hyperparameter function is Lipschitz continuous.
- **Evidence anchors:** Section 3.4 states the objective "can be equivalently expressed as... independent of the reverse process expectation."

### Mechanism 3
- **Claim:** Unconstrained diffusion priors can be repurposed for constrained inverse problems.
- **Mechanism:** Uses deterministic, differentiable link function g to map unconstrained latent x₀ to constrained parameter θ.
- **Core assumption:** Link function is bijective and continuously differentiable.
- **Evidence anchors:** Section 3.2 defines the link function mapping, Appendix D discusses scaling factors.

## Foundational Learning

- **Concept: Exponential Family & Conjugate Priors**
  - **Why needed here:** The entire mechanism relies on the mathematical property that specific likelihoods have matching priors yielding closed-form posteriors.
  - **Quick check question:** Can you derive the closed-form posterior update for a Poisson likelihood with a Gamma prior?

- **Concept: Score-Based Generative Modeling (SDEs)**
  - **Why needed here:** The method integrates a likelihood score into a reverse SDE. One must understand how the score function guides the denoising trajectory.
  - **Quick check question:** How does the variance-preserving (VP) SDE formulation relate to the signal-to-noise ratio at time t?

- **Concept: Amortized Variational Inference**
  - **Why needed here:** The paper uses a neural network to predict distribution parameters globally, rather than optimizing them per sample.
  - **Quick check question:** What is the advantage of using a neural network to predict variational parameters versus optimizing them iteratively for a specific observation?

## Architecture Onboarding

- **Component map:** Prior Score Network (s_φ) -> Inference Network (ζ_ρ) -> Analytic Likelihood Module -> Combined Gradient
- **Critical path:**
  1. Pre-train or load standard Score Network on prior dataset (e.g., ImageNet)
  2. Train Inference Net ζ_ρ using amortized VI objective L̃_AVI
  3. Sample posterior via PC sampling using gradients g = g_prior + g_likelihood

- **Design tradeoffs:**
  - Uses identical U-Nets for both networks but reduces channels (128→64) for inference net
  - Link function requires scaling factor s; too low restricts prior range, too high causes instability
  - Gradients must be clipped (e.g., to [-10, 10]) to prevent destabilization

- **Failure signatures:**
  - Saturation: Posterior samples collapse to mean value; check link function scaling
  - Numerical instability: NaNs in early time steps; usually due to unbounded likelihood gradients
  - Mode collapse: Inference network fails to predict uncertainty; verify KL divergence minimization

- **First 3 experiments:**
  1. 1D Benchmark (Poisson): Recover 1D signal from Poisson observations using GP prior
  2. Score-Based Cox Process: Recover intensity image from sparse Poisson event counts
  3. Malaria Prevalence (Binomial): Interpolate real-world prevalence rates across Africa

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the sampling process be stabilized to remove dependency on manual tuning of SNR and gradient clipping?
- **Basis:** Appendix H states failures occur without proper SNR tuning and suggests developing more robust techniques.
- **Why unresolved:** Current method relies on manually clipping gradients and tuning SNR to prevent numerical instabilities.
- **Evidence:** Modified sampling algorithm that generates valid posterior samples across varying noise levels without manual gradient clipping or hyperparameter tuning.

### Open Question 2
- **Question:** Is it possible to modify the "evidence trick" to avoid computational overhead of training separate inference network?
- **Basis:** Appendix H notes additional cost of training separate neural network for variational distribution.
- **Why unresolved:** Amortized variational inference requires training U-Net to predict hyperparameters, doubling training requirements.
- **Evidence:** Theoretical derivation allowing closed-form optimization of variational hyperparameters using only pre-trained score network, or method unifying score and inference networks.

### Open Question 3
- **Question:** Can methodology be extended to handle multi-parameter exponential family distributions or non-conjugate likelihoods?
- **Basis:** Assumption 3.2 restricts framework to univariate one-parameter exponential family.
- **Why unresolved:** Derivation of tractable likelihood score relies heavily on existence and properties of specific natural conjugate prior.
- **Evidence:** Generalization of evidence trick maintaining tractability for multi-parameter distributions or successful integration of approximate conjugate priors for non-conjugate cases.

## Limitations
- Critically depends on observation likelihood belonging to one-parameter exponential family with tractable conjugate prior.
- Likelihood score gradient can become unbounded, requiring gradient clipping and careful hyperparameter tuning.
- Unconstrained diffusion prior must cover necessary parameter range through link function; improper scaling causes saturation.

## Confidence
- **High Confidence:** Mathematical derivation of evidence trick and training objective for inference network.
- **Medium Confidence:** Empirical results demonstrating competitive performance against MCMC and DPS baselines.
- **Low Confidence:** Method's behavior on complex, high-dimensional inverse problems or with likelihoods outside standard exponential family.

## Next Checks
1. **Validate Conjugate Prior Implementation:** For Poisson case, verify closed-form posterior update with Gamma prior by computing posterior mean/variance analytically and comparing against inference network samples.
2. **Stress Test Numerical Stability:** Systematically evaluate on low-rate Poisson observations (y=0 for most samples) to identify conditions where gradient clipping is insufficient.
3. **Test Beyond Exponential Families:** Apply method to Negative Binomial likelihood (similar form but lacks tractable conjugate prior) to assess breakdown of evidence trick.