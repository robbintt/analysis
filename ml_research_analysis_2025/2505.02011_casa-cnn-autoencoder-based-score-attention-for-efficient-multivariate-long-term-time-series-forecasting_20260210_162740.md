---
ver: rpa2
title: 'CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term
  Time-series Forecasting'
arxiv_id: '2505.02011'
source_url: https://arxiv.org/abs/2505.02011
tags:
- casa
- itransformer
- transformer
- softs
- patchtst
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CASA introduces a CNN Autoencoder-based Score Attention module\
  \ that replaces conventional self-attention in Transformer models for multivariate\
  \ long-term time-series forecasting. The key innovation is approximating the attention\
  \ score QK^T/\u221Adk using a CNN autoencoder, which enables linear complexity scaling\
  \ and better captures cross-dimensional interactions."
---

# CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting

## Quick Facts
- arXiv ID: 2505.02011
- Source URL: https://arxiv.org/abs/2505.02011
- Reference count: 40
- Key outcome: CNN Autoencoder-based Score Attention reduces computational resources by up to 77.7%, accelerates inference by 44.0%, and achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics for multivariate long-term time-series forecasting.

## Executive Summary
CASA introduces a CNN Autoencoder-based Score Attention module that replaces conventional self-attention in Transformer models for multivariate long-term time-series forecasting. The key innovation is approximating the attention score QK^T/√d_k using a CNN autoencoder, which enables linear complexity scaling and better captures cross-dimensional interactions. Experiments on eight real-world datasets show CASA reduces computational resources by up to 77.7%, accelerates inference by 44.0%, and achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics.

## Method Summary
CASA replaces self-attention in Transformer models with a CNN autoencoder that approximates the attention score matrix QK^T/√d_k. The approach treats variates as channels using channel-wise tokenization, allowing convolution operations to mix information across variates during score computation. The score network uses an inverted bottleneck autoencoder structure that expands features into high-dimensional space before compression, enabling the model to capture cross-variates relationships while maintaining linear computational complexity.

## Key Results
- Reduces computational resources by up to 77.7% compared to baseline Transformer models
- Accelerates inference by 44.0% while maintaining or improving prediction accuracy
- Achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics across eight benchmark datasets
- Demonstrates linear memory scaling with number of variates, outperforming quadratic scaling of standard attention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Approximating the attention score matrix QK^T/√d_k with a 1D CNN autoencoder achieves linear complexity while maintaining prediction accuracy.
- Mechanism: Instead of computing the full N×N attention matrix, CASA learns a score network using an inverted bottleneck CNN autoencoder that expands features into high-dimensional space, then compresses to retain essential cross-variable information. The output undergoes softmax and element-wise multiplication with the value projection.
- Core assumption: The attention score matrix can be effectively approximated through compress-decompress operations without losing critical relational information.
- Evidence anchors:
  - [abstract] "The key innovation is approximating the attention score QK^T/√d_k using a CNN autoencoder, which enables linear complexity scaling"
  - [section 3.4] "we designed a score network Score to approximate this operation using 1D CNN Autoencoder... Attention(Z_i+1) = softmax(Score(Z_i)) ⊛ V_i+1"
  - [corpus] Limited corpus evidence for this specific approximation technique in time series; CNN-TFT (arXiv:2510.06840) combines CNNs with transformers but uses hybrid attention rather than approximation

### Mechanism 2
- Claim: Treating variates as channels with convolution enables variate-dependent feature refinement, overcoming the variate-independent limitation of standard attention's query/key embeddings.
- Mechanism: The CNN autoencoder treats each variate as a channel, allowing convolution operations to mix information across variates during score computation. This contrasts with standard attention where query and key embeddings for each variate depend only on that variate's features.
- Core assumption: Multivariate time series require capturing correlations between variates, and standard attention's independent embedding prevents effective cross-dimensional interaction.
- Evidence anchors:
  - [section 3.3, Proposition 1] "Query and key embeddings are variate-independent operations in the conventional Transformer using channel-wise tokenization"
  - [section 4.4, Figure 7] "CASA's correlation matrix most closely approximates the Ground Truth, as evidenced by the lowest MSE loss between their probability density functions"
  - [corpus] CNN-TFT (arXiv:2510.06840) confirms "CNNs excel at capturing local patterns and translational invariances, while transformers effectively model long-range dependencies"

### Mechanism 3
- Claim: The score approximation mechanism is general enough to replace self-attention across different tokenization strategies with consistent improvements.
- Mechanism: CASA operates as a drop-in module that only replaces the attention computation while preserving the surrounding Transformer architecture, making it compatible with point-wise, patch-wise, and channel-wise tokenization approaches.
- Core assumption: Cross-dimensional relationships exist and can be captured regardless of how input data is tokenized.
- Evidence anchors:
  - [abstract] "can be introduced in diverse Transformers model-agnosticically"
  - [section 4.2, Table 3] "CASA consistently enhances the performance of the original and variants models, achieving improvements in 40 out of 42 results across the benchmarks"
  - [corpus] No direct corpus evidence for cross-tokenization attention replacement; related work focuses on single tokenization approaches

## Foundational Learning

- Concept: Self-Attention Computational Complexity
  - Why needed here: Understanding why standard attention is O(N²) or O(L²) motivates CASA's approximation approach. The paper reports 77.7% memory reduction on Traffic dataset.
  - Quick check question: Can you explain why computing QK^T creates a matrix whose size scales quadratically with the number of tokens?

- Concept: Tokenization Strategies (Channel-wise vs Point-wise vs Patch-wise)
  - Why needed here: CASA uses channel-wise tokenization where each variate becomes a token. Figure 2 and Section 3.3 show this choice determines whether query/key embeddings are variate-independent or time-independent.
  - Quick check question: For a dataset with 862 variates (Traffic) and 96 time steps, what would be the number of tokens under each tokenization approach?

- Concept: Inverted Bottleneck Architecture
  - Why needed here: CASA's score network uses an inverted bottleneck (expand-then-compress) inspired by representation learning literature. Section 3.4 states this "improves expressiveness" by embedding into high-dimensional space before compression.
  - Quick check question: How does expand-then-compress differ from compress-then-expand, and why might expansion help capture cross-dimensional relationships?

## Architecture Onboarding

- Component map:
  Input layer: X ∈ R^(N×L) → Linear embedding → Z₀ ∈ R^(N×D)
  CASA Block (repeated M times):
    Score Network: 1D CNN autoencoder with inverted bottleneck → Score matrix ∈ R^(N×N)
    Softmax: Applied row-wise to scores
    Value projection: Linear map f(Z_i) → V ∈ R^(N×D)
    Element-wise multiplication: softmax(Score) ⊛ V
    Add & Norm → FFN → Add & Norm
  Predictor: Z_M → Linear projection → Y ∈ R^(N×H)

- Critical path: Input embedding → [Score Network → Softmax ⊛ Value → FFN] × M layers → Predictor → Output. The Score Network is the only modified component from vanilla Transformer.

- Design tradeoffs:
  - Approximation accuracy vs. computational savings: The autoencoder may lose precision in attention scores, but gains O(NL + NH) vs O(N² + NL + NH) complexity
  - Kernel size vs. cross-dimensional coverage: Larger kernels capture more variate relationships but increase computation; paper uses kernel size k with complexity O(NkD²)
  - Channel-wise tokenization vs. temporal granularity: Treating variates as tokens optimizes for cross-dimensional interactions but may underweight temporal patterns compared to patch-wise approaches

- Failure signatures:
  - Memory scales quadratically with N or L: Check that score network is computing approximation, not full attention
  - Performance drops on datasets with few variates (N < 10): Cross-dimensional mixing may be overfitting or introducing noise
  - Training saturates quickly (Figure 1 shows iTransformer saturation): May need learning rate adjustment; CASA shows "consistent learning"
  - Correlation matrix diverges from ground truth (Section 4.4): Score network may not be capturing real variate relationships

- First 3 experiments:
  1. Baseline comparison: Run CASA on ETTh1 dataset with L=96, H=96, comparing MSE/MAE against iTransformer, PatchTST, and SOFTS. Expect CASA to achieve MSE ~0.376 vs iTransformer's 0.386.
  2. Attention replacement validation: Replace self-attention in vanilla Transformer and PatchTST with CASA on Electricity dataset (321 variates). Expect 40/42 metrics to improve per Table 3.
  3. Scalability test: On Traffic dataset (862 variates), vary input length L ∈ {48, 96, 192, 336, 720} and measure memory usage. Expect linear scaling as in Figure 6(a), not quadratic growth seen with iTransformer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CASA's approximation quality of the attention score QK^T/√d_k theoretically scale with CNN autoencoder depth and width, and what are the error bounds compared to exact attention computation?
- Basis in paper: [inferred] The paper empirically demonstrates that the CNN autoencoder approximates attention scores effectively, but provides no theoretical analysis of approximation error bounds or how architectural choices (kernel size k, hidden dimensions) affect approximation fidelity.
- Why unresolved: The complexity analysis (Section 3.4) addresses computational efficiency but not approximation quality. The inverted bottleneck design is motivated by prior work on expressiveness but lacks formal justification in the attention approximation context.
- What evidence would resolve it: Theoretical bounds on ||Score(Z) - QK^T/√d_k|| under different autoencoder configurations, or empirical analysis measuring approximation error across architectures.

### Open Question 2
- Question: To what extent does CASA generalize to time-series forecasting domains beyond the tested benchmark datasets, particularly those with fundamentally different correlation structures (e.g., financial markets, healthcare vitals)?
- Basis in paper: [explicit] The paper states: "Multivariate long-term time series forecasting is critical for applications such as weather prediction, and traffic analysis" but limits experiments to 8 standard LTSF benchmarks with similar temporal characteristics.
- Why unresolved: All tested datasets (ETT, Traffic, Electricity, Weather, Solar) share relatively regular temporal patterns. The cross-dimensional correlation patterns shown in Figure 7 may not transfer to domains with sparser, noisier, or non-stationary cross-variate dependencies.
- What evidence would resolve it: Experiments on datasets from domains with qualitatively different temporal dynamics and correlation structures.

### Open Question 3
- Question: What is the upper bound on the number of variates N for which CASA maintains its efficiency and performance advantages, particularly as N approaches or exceeds the scale where O(N²) attention becomes prohibitive?
- Basis in paper: [inferred] Section 4.3 shows linear memory scaling up to 862 variates (Traffic dataset), but Figure 6(a) suggests memory still grows with N. The paper claims "CASA exhibits linear complexity" but does not explore performance degradation or efficiency gains at much larger scales (e.g., N > 10,000).
- Why unresolved: Real-world applications like sensor networks or financial markets can involve thousands to millions of variates. Whether CNN convolutions across channels remain efficient at such scales is untested.
- What evidence would resolve it: Scaling experiments with synthetic or real datasets having N in the thousands to tens of thousands, measuring both computational efficiency and forecast accuracy.

### Open Question 4
- Question: Can alternative architectures (e.g., MLP-mixers, frequency-domain transforms, or graph neural networks) more effectively approximate attention scores while maintaining linear complexity?
- Basis in paper: [inferred] Section 3.4 mentions the inverted bottleneck autoencoder was "inspired by previous research" on embedding into high-dimensional spaces, but no comparison is made against other potential score approximation architectures beyond the CNN autoencoder design choice.
- Why unresolved: The design space for approximating QK^T is large. CNNs may not be optimal; other architectures could provide better trade-offs between approximation quality, parameter efficiency, and computational cost.
- What evidence would resolve it: Ablation studies comparing CNN autoencoder against MLP-based, spectral, or hybrid architectures for score approximation on the same benchmarks.

## Limitations
- Lack of detailed ablation studies for CNN autoencoder architecture hyperparameters (expansion ratio, kernel size, number of layers)
- Evaluation focused on datasets with high variate counts where cross-dimensional mixing is most beneficial, with limited analysis of low-variation datasets
- Complexity analysis doesn't account for CNN autoencoder's own computational overhead, particularly for large kernel sizes or deep architectures

## Confidence
- **High confidence**: Computational efficiency claims (77.7% memory reduction, 44.0% faster inference) are well-supported by direct measurements on Traffic dataset. The mechanism for replacing attention with CNN autoencoder approximation is clearly specified.
- **Medium confidence**: Performance improvements across 87.5% of metrics are demonstrated but rely on comparison with specific baseline implementations. The variate-mixing advantage over conventional attention is theoretically sound but needs more diverse dataset validation.
- **Low confidence**: The generalizability claim across different tokenization strategies is based on a single table (Table 3) without architectural details of how CASA adapts to patch-wise or point-wise tokenization. The specific CNN autoencoder hyperparameters remain unspecified.

## Next Checks
1. **Ablation study on CNN autoencoder architecture**: Systematically vary the inverted bottleneck expansion ratio, kernel size, and number of layers while measuring both accuracy and efficiency on the Electricity dataset. This would validate whether the efficiency gains come from the approximation technique itself or specific architectural choices.

2. **Cross-tokenization compatibility test**: Implement CASA with patch-wise tokenization (treating time steps as tokens rather than variates) on a small-variate dataset like Weather (14 variates). Measure whether the variate-mixing advantage persists or degrades when temporal patterns dominate.

3. **Low-variation dataset evaluation**: Apply CASA to a dataset with minimal cross-variates correlation (e.g., artificially decorrelated versions of benchmark datasets) and compare against standard attention. This would test the break condition where variate-independent embeddings might actually be preferable.