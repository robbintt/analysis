---
ver: rpa2
title: 'EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting'
arxiv_id: '2505.12738'
source_url: https://arxiv.org/abs/2505.12738
tags:
- epidemic
- forecasting
- epillm
- learning
- spatio-temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EpiLLM, a novel framework that repurposes
  large language models (LLMs) for spatio-temporal epidemic forecasting. The key innovation
  is a dual-branch architecture that aligns infection cases and human mobility data
  with language tokens, enabling LLMs to model complex epidemic patterns.
---

# EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting

## Quick Facts
- **arXiv ID**: 2505.12738
- **Source URL**: https://arxiv.org/abs/2505.12738
- **Reference count**: 40
- **Primary result**: Achieves up to 30.38% RMSE improvement in COVID-19 forecasting compared to state-of-the-art baselines

## Executive Summary
EpiLLM is a novel framework that repurposes large language models (LLMs) for spatio-temporal epidemic forecasting by reformulating the problem as next-token prediction. The key innovation is a dual-branch architecture that aligns infection cases and human mobility data with language tokens, enabling LLMs to model complex epidemic patterns through autoregressive modeling. The framework introduces spatio-temporal prompt learning techniques to enhance LLM perception of epidemic dynamics, addressing the challenge that traditional LLMs are not designed to process numerical epidemiological data directly.

Extensive experiments on real-world COVID-19 datasets from England, France, Italy, and Spain demonstrate EpiLLM's superior performance compared to state-of-the-art baselines. For direct forecasting, EpiLLM achieves up to 30.38% improvement in RMSE over existing methods, particularly on the Spain dataset. The framework also exhibits strong multi-step forecasting capabilities, with the GEMMA-based variant showing the best performance due to reduced error accumulation in long-sequence generation. The paper validates the importance of each component through ablation studies and demonstrates EpiLLM's scaling behavior characteristic of LLMs, with larger models showing improved performance at the cost of increased computational demands.

## Method Summary
EpiLLM addresses the challenge of adapting large language models (LLMs) for epidemic forecasting by treating numerical epidemic data as sequential tokens. The framework employs a dual-branch architecture where one branch processes mobility data and the other handles infection cases, both aligned with language tokens. Through autoregressive modeling, EpiLLM reformulates epidemic forecasting as a next-token prediction task, leveraging the powerful sequence modeling capabilities of LLMs. The system incorporates spatio-temporal prompt learning to enhance the model's ability to capture complex epidemic dynamics across different regions and time periods.

The framework was evaluated on COVID-19 datasets from England, France, Italy, and Spain, demonstrating superior performance compared to state-of-the-art baselines. The experiments showed that EpiLLM achieved up to 30.38% improvement in RMSE for direct forecasting, with the GEMMA-based variant performing particularly well in multi-step forecasting scenarios. The approach addresses the limitations of traditional epidemic forecasting models by leveraging the strong representation learning capabilities of LLMs while adapting them to handle spatio-temporal epidemic data effectively.

## Key Results
- Achieves up to 30.38% RMSE improvement over state-of-the-art baselines in COVID-19 forecasting
- Demonstrates strong multi-step forecasting capabilities with GEMMA-based variant showing best performance
- Shows characteristic LLM scaling behavior with larger models delivering improved accuracy at higher computational cost

## Why This Works (Mechanism)
EpiLLM works by transforming epidemic forecasting from a traditional time-series prediction problem into a next-token prediction task that leverages the inherent strengths of large language models. The dual-branch architecture enables the model to process and integrate two critical types of epidemic data—infection cases and human mobility patterns—in a way that maintains their temporal and spatial relationships. By aligning these numerical datasets with language tokens, the framework allows LLMs to apply their powerful sequence modeling capabilities to epidemic data, capturing complex non-linear relationships and long-range dependencies that traditional forecasting methods struggle to model.

The spatio-temporal prompt learning component is crucial for helping the LLM understand the geographical and temporal context of the epidemic data. This enables the model to recognize patterns that vary across different regions and time periods, improving its ability to generalize from historical data to future predictions. The autoregressive modeling approach allows EpiLLM to generate forecasts step-by-step, maintaining coherence in long-sequence predictions and reducing error accumulation that typically plagues multi-step forecasting methods.

## Foundational Learning

**Spatio-temporal data alignment**: The process of converting numerical epidemic data (cases and mobility) into a format compatible with language tokens. *Why needed*: LLMs cannot directly process raw numerical time-series data. *Quick check*: Verify that aligned token sequences preserve temporal order and spatial relationships.

**Autoregressive modeling**: A sequence generation approach where each prediction depends on previous outputs. *Why needed*: Enables step-by-step forecasting while maintaining temporal coherence. *Quick check*: Monitor prediction consistency across multiple forecasting steps.

**Prompt learning for LLMs**: Techniques for adapting pre-trained models to specific tasks through learned prompts. *Why needed*: Allows LLMs to specialize in epidemic forecasting without full fine-tuning. *Quick check*: Compare performance with and without spatio-temporal prompts.

**Dual-branch architecture**: A design pattern where separate processing streams handle different data modalities. *Why needed*: Enables specialized processing of mobility and infection data before integration. *Quick check*: Validate that both branches contribute equally to final predictions.

**Token-based numerical representation**: Converting continuous numerical values into discrete token sequences. *Why needed*: Bridges the gap between numerical epidemic data and text-based LLM processing. *Quick check*: Ensure numerical precision is maintained during tokenization.

## Architecture Onboarding

**Component Map**: Mobility Data -> Branch 1 -> Token Alignment -> Integration Layer -> LLM -> Forecast Output
Infection Cases -> Branch 2 -> Token Alignment -> Integration Layer -> LLM -> Forecast Output

**Critical Path**: Data preprocessing (token alignment) → Dual-branch processing → Integration layer → LLM inference → Forecast generation

**Design Tradeoffs**: The framework prioritizes model performance and accuracy over computational efficiency, leveraging large LLMs that require significant computational resources. This tradeoff is justified by the superior forecasting accuracy demonstrated in experiments, though it may limit deployment in resource-constrained settings.

**Failure Signatures**: Poor performance may occur when mobility data is sparse or unavailable, when the quality of textual descriptions for data alignment is low, or when the epidemic patterns deviate significantly from historical patterns in the training data. The model may also struggle with rapid outbreak scenarios where mobility patterns change abruptly.

**First Experiments**:
1. **Baseline comparison**: Run EpiLLM against traditional epidemic forecasting models (e.g., ARIMA, LSTM) on the same COVID-19 datasets to establish performance gains
2. **Component ablation**: Systematically disable each component (dual-branch, spatio-temporal prompts, autoregressive modeling) to quantify individual contributions
3. **Scaling analysis**: Test different LLM sizes (small, medium, large) to verify the scaling behavior and establish computational-performance tradeoffs

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on availability of high-quality textual descriptions for mobility and case data alignment
- Computational demands scale unfavorably with model size, potentially limiting deployment in resource-constrained settings
- Generalizability to non-COVID epidemics and regions with different data availability patterns remains to be thoroughly validated

## Confidence
- **High confidence**: Comparative performance gains over baselines (up to 30.38% RMSE improvement) are well-supported by experimental results across multiple countries
- **Medium confidence**: Scalability claims and scaling behavior observations are based on tested model sizes but may not generalize to significantly larger or smaller architectures
- **Medium confidence**: Transferability to other epidemic scenarios is suggested but not extensively validated beyond COVID-19 case studies in Europe

## Next Checks
1. **Cross-disease validation**: Test EpiLLM's performance on non-COVID epidemics (e.g., seasonal influenza, measles outbreaks) to verify generalizability beyond the studied dataset

2. **Data scarcity simulation**: Systematically evaluate model performance with varying levels of data availability and quality to establish robustness thresholds for real-world deployment

3. **Computational efficiency benchmarking**: Conduct detailed profiling of inference time and memory requirements across different hardware configurations to provide practical deployment guidelines for public health agencies with varying resources