---
ver: rpa2
title: Memorization Dynamics in Knowledge Distillation for Language Models
arxiv_id: '2601.15394'
source_url: https://arxiv.org/abs/2601.15394
tags:
- examples
- memorization
- student
- baseline
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knowledge distillation (KD) significantly reduces training data
  memorization while improving generalization compared to standard fine-tuning. The
  distilled student memorizes only 0.07% of training data versus 0.17% for baseline
  models, inheriting just 0.9% of teacher-specific memorization.
---

# Memorization Dynamics in Knowledge Distillation for Language Models

## Quick Facts
- arXiv ID: 2601.15394
- Source URL: https://arxiv.org/abs/2601.15394
- Reference count: 29
- Primary result: Knowledge distillation reduces training data memorization by 60% while improving generalization compared to standard fine-tuning

## Executive Summary
Knowledge distillation significantly reduces training data memorization while improving generalization compared to standard fine-tuning. The distilled student memorizes only 0.07% of training data versus 0.17% for baseline models, inheriting just 0.9% of teacher-specific memorization. KD acts as a strong regularizer, preventing forced memorization of high-entropy examples. Memorization is predictable before distillation using features like zlib entropy and KL divergence, achieving AUC-ROC of 0.9997. Soft distillation reduces memorization by filtering out "difficult" examples, while hard distillation poses higher risk by inheriting 2.7x more teacher-specific memorization. Across three model families and datasets, distillation consistently achieves better validation loss and perplexity while dramatically reducing privacy risks.

## Method Summary
The study compares memorization in knowledge distillation versus standard fine-tuning across three model families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, WikiText-103, Nemotron-CC-v2). Teachers are fine-tuned with cross-entropy, baselines are trained independently with cross-entropy, and students are distilled via forward KL divergence at temperature T=2.0. Memorization is measured through discoverable extraction—prompting with 50-token prefixes and checking if the model exactly reproduces 50-token suffixes from training data. The study also develops a pre-distillation memorization prediction model using zlib entropy, teacher/baseline perplexity, and KL divergence between teacher and baseline.

## Key Results
- Knowledge distillation reduces training data memorization by 60% compared to standard fine-tuning
- Distilled students memorize only 0.07% of training data versus 0.17% for baseline models
- Distillation inherits just 0.9% of teacher-specific memorization, acting as a strong regularizer
- Memorization is highly predictable before distillation, achieving AUC-ROC of 0.9997
- Pre-filtering predicted memorization examples reduces memorization by 99.8%

## Why This Works (Mechanism)

### Mechanism 1: Soft Distillation as Entropy-Selective Regularizer
Soft (logit-level) distillation using KL divergence reduces memorization by allowing the student to avoid committing to high-entropy sequences that cross-entropy would otherwise force-memorize. Cross-entropy loss enforces one-hot targets regardless of model uncertainty, while KL divergence permits the student to output flatter distributions when it cannot match teacher certainty on complex examples. This creates an implicit filter: student only memorizes sequences where it achieves genuine high confidence (low entropy).

### Mechanism 2: Memorization Concentration in Low-Entropy "Easy" Examples
Memorization is highly concentrated in a predictable subset of training examples characterized by low intrinsic compressibility (zlib entropy) and low perplexity. Easy-to-memorize examples form a distinct cluster in entropy-perplexity space. Distilled students selectively memorize almost exclusively from this cluster (95.7% overlap with teacher+baseline shared examples), effectively raising the bar for what gets memorized.

### Mechanism 3: Pre-Distillation Memorization Prediction via Signal Aggregation
Student memorization can be predicted before distillation begins using zlib entropy, teacher/baseline perplexity, and KL divergence between teacher and baseline. A logistic regression classifier using these four features achieves near-perfect discrimination (AUC-ROC 0.9997, Recall 1.0). Removing predicted examples before distillation reduces memorization by 99.8% (1,698 → 4 examples).

## Foundational Learning

- **KL Divergence vs Cross-Entropy Loss**: Understanding why soft distillation reduces memorization requires distinguishing how soft targets (full probability distributions) differ from hard targets (one-hot labels) in their optimization pressure. Given a 3-class problem where the true label is class 1, what's the optimization difference between cross-entropy (target: [1,0,0]) vs KL divergence when teacher assigns [0.7, 0.2, 0.1]?

- **Discoverable Memorization (Extraction-Based)**: The paper's central evaluation metric; requires understanding the extraction attack setup to interpret results. If a model generates 48/50 matching tokens for a training suffix given the 50-token prefix, is that example classified as "memorized" under the paper's definition?

- **Shannon Entropy as Model Uncertainty**: Section 5's mechanism analysis relies on interpreting average Shannon entropy over sequence positions as a measure of intrinsic model uncertainty vs sequence complexity. A model assigns uniform probability (0.25 each) to 4 tokens at position t. What's the Shannon entropy? What if it assigns [0.97, 0.01, 0.01, 0.01]?

## Architecture Onboarding

- **Component map**: Teacher Model (12B) -> Baseline Model (1.4B) -> Student Model (1.4B) via KL distillation
- **Critical path**: 1) Fine-tune teacher on target dataset D, 2) Train baseline independently on D, 3) Extract features for all training examples using teacher + baseline, 4) Optionally train classifier and filter predicted-high-risk examples, 5) Distill student using soft targets (KL divergence, T≥2.0), 6) Evaluate memorization via discoverable extraction
- **Design tradeoffs**: Temperature higher T reduces memorization but may reduce knowledge transfer fidelity—sweep T ∈ {1.0, 2.0, 3.0, 5.0}; Soft vs Hard distillation—similar overall memorization rates, but hard distillation inherits 2.7× more teacher-specific "difficult" examples; Pre-filtering removes 99.8% of memorization but also removes training examples—evaluate downstream utility impact
- **Failure signatures**: Student memorization rate approaches baseline → Check temperature scaling implementation; High teacher-specific memorization inheritance → Verify you're using soft (logit-level) not hard (sequence-level) distillation; Classifier recall drops → Features may not transfer across architectures
- **First 3 experiments**: 1) Train baseline and student on identical data with matched compute; measure discoverable memorization rates—expect ~2× reduction for student, 2) Sweep T ∈ {1.0, 2.0, 3.0, 5.0} while holding other hyperparameters fixed; plot memorization rate vs validation perplexity to establish tradeoff curve, 3) Train classifier on 70% of data, filter predicted memorization examples from held-out 30%, distill on filtered set; measure memorization reduction and any utility degradation

## Open Questions the Paper Calls Out

**Open Question 1**: Why do different model architectures memorize completely non-overlapping sets of examples despite agreeing on which examples have low intrinsic entropy? The paper identifies the phenomenon and rules out data duplication and entropy differences, but does not isolate which architectural factors drive the selection.

**Open Question 2**: What mechanism causes new memorization to emerge when pre-identified high-risk examples are removed from training? The paper documents the phenomenon but does not investigate whether it stems from capacity reallocation, distribution shift, or interaction effects among remaining examples.

**Open Question 3**: Does the memorization-reducing effect of soft distillation generalize to other distillation objectives such as reverse KL or on-policy distillation? The paper only tests forward KL divergence and hard distillation; whether other divergence measures have different memorization properties remains untested.

**Open Question 4**: Can the pre-distillation memorization classifier transfer to unseen model families or datasets, or is it architecture-specific? The high AUC-ROC relies on features like baseline perplexity that are inherently model-dependent; cross-architecture transfer would require features capturing universal memorization propensity.

## Limitations

- Analysis is confined to auto-regressive language models without attention mechanisms
- Relatively small datasets (1M examples) compared to typical pre-training corpora
- Evaluation framework relies on exact suffix matching from 50-token prefixes, which may underestimate memorization in longer sequences
- Limited to three model families; generalization across architectures not fully established
- No downstream task evaluation to assess trade-offs when memorization is aggressively reduced

## Confidence

**High Confidence**: KD reduces overall memorization by ~60% compared to baseline; Soft distillation achieves better validation loss and perplexity than hard distillation; Memorization prediction achieves AUC-ROC of 0.9997 using zlib entropy, PPL, and KL divergence features

**Medium Confidence**: Distillation acts as a strong regularizer preventing forced memorization of high-entropy examples; Easy-to-memorize examples form predictable cluster with low zlib entropy and PPL; Pre-filtering predicted memorization examples reduces memorization by 99.8%

**Low Confidence**: Memorization concentration mechanism applies universally across all KD settings; KL divergence regularization is the primary mechanism for memorization reduction; Distillation improves generalization without trade-offs

## Next Checks

**Check 1**: Temperature Sensitivity Sweep Validation - Replicate Figure 2 by training students across temperature values T ∈ {1.0, 2.0, 3.0, 5.0} while holding all other hyperparameters constant. Measure both memorization rate and validation perplexity at each temperature to establish the precise tradeoff curve.

**Check 2**: Cross-Dataset Generalization Test - Apply the pre-filtering methodology to a different dataset family (e.g., medical or code data) to validate whether the memorization prediction model generalizes beyond the Common Crawl-derived FineWeb dataset.

**Check 3**: Architecture Transfer Validation - Test whether the easy-to-memorize example concentration mechanism holds across additional model families not included in the original study. Train teachers and baselines for architectures like Llama, Mistral, or Phi on the same datasets, then measure whether the same low-entropy clustering pattern emerges.