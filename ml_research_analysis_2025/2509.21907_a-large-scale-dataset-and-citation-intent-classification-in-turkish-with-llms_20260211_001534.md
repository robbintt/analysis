---
ver: rpa2
title: A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs
arxiv_id: '2509.21907'
source_url: https://arxiv.org/abs/2509.21907
tags:
- citation
- dataset
- classification
- prompt
- turkish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of classifying citation intents
  in Turkish academic texts, a task complicated by Turkish's agglutinative structure
  and the lack of large-scale labeled datasets. The authors introduce the first large-scale
  Turkish citation intent dataset (2,650 examples) and evaluate Large Language Models
  (LLMs) using In-Context Learning (ICL).
---

# A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs

## Quick Facts
- arXiv ID: 2509.21907
- Source URL: https://arxiv.org/abs/2509.21907
- Reference count: 29
- First large-scale Turkish citation intent dataset with 2,650 examples

## Executive Summary
This paper addresses the challenge of classifying citation intents in Turkish academic texts, a task complicated by Turkish's agglutinative structure and the lack of large-scale labeled datasets. The authors introduce the first large-scale Turkish citation intent dataset (2,650 examples) and evaluate Large Language Models (LLMs) using In-Context Learning (ICL). Manual ICL performance was inconsistent and sensitive to prompt design. To overcome this, they employed the DSPy framework to systematically optimize prompts, achieving 86.5% accuracy. Finally, they developed a stacked ensemble model using XGBoost as a meta-learner, which achieved a state-of-the-art accuracy of 91.3%. The study provides a foundational dataset and robust framework for Turkish citation intent classification, enabling future qualitative research in low-resource languages.

## Method Summary
The authors developed a comprehensive approach to Turkish citation intent classification by first creating a novel dataset of 2,650 manually labeled examples. They evaluated Large Language Models using In-Context Learning with manually crafted prompts, but found performance inconsistent due to sensitivity to prompt design. To address this limitation, they implemented the DSPy framework for systematic prompt optimization, improving accuracy to 86.5%. The final contribution was a stacked ensemble model that combined base LLM predictions with XGBoost as a meta-learner, achieving 91.3% accuracy on the Turkish citation intent classification task.

## Key Results
- Created the first large-scale Turkish citation intent dataset with 2,650 manually labeled examples
- Achieved 86.5% accuracy using DSPy framework for systematic prompt optimization
- Developed a stacked ensemble model reaching state-of-the-art 91.3% accuracy

## Why This Works (Mechanism)
The success of this approach stems from addressing the fundamental challenges of low-resource language processing. Turkish's agglutinative morphology creates unique challenges for NLP tasks, as words can contain multiple morphemes that change meaning and grammatical function. The DSPy framework's systematic approach to prompt optimization overcomes the limitations of manual prompt engineering, which is particularly sensitive to design choices in low-resource contexts. The ensemble approach leverages both the contextual understanding of LLMs and the discriminative power of traditional machine learning methods like XGBoost, creating a robust classification system that captures both semantic nuance and statistical patterns.

## Foundational Learning
- **Agglutinative Morphology**: Turkish words combine multiple morphemes, requiring models to understand how suffixes modify meaning and function. Critical for accurate citation intent classification where subtle linguistic differences matter.
- **In-Context Learning**: LLM's ability to learn from examples provided in prompts without parameter updates. Essential for low-resource languages where fine-tuning data is scarce.
- **DSPy Framework**: Systematic approach for optimizing prompts and signatures in LLM applications. Addresses the brittleness of manual prompt engineering in production settings.
- **Ensemble Learning**: Combining multiple models to improve overall performance. Leverages complementary strengths of LLMs and traditional ML methods.
- **Citation Intent Classification**: Understanding why authors cite specific works (e.g., background, methodology, contrast). Fundamental task for bibliometric analysis and information extraction.
- **Low-Resource Language Processing**: Developing NLP solutions when limited labeled data exists. Critical challenge for languages like Turkish with fewer available resources.

## Architecture Onboarding

**Component Map**
Dataset -> Manual ICL -> DSPy Optimization -> Ensemble Model

**Critical Path**
Turkish citation text → Prompt engineering (DSPy optimized) → LLM classification → XGBoost meta-classifier → Final citation intent label

**Design Tradeoffs**
- Manual ICL offers simplicity but suffers from sensitivity to prompt design and inconsistency
- DSPy optimization adds complexity but provides systematic improvement and reproducibility
- Ensemble approach increases computational cost but achieves higher accuracy through complementary modeling

**Failure Signatures**
- Poor prompt design leading to inconsistent LLM outputs
- Insufficient dataset size limiting model generalization
- Morphological ambiguity in Turkish causing classification errors
- Over-reliance on accuracy metric masking performance issues in imbalanced classes

**First Experiments**
1. Baseline manual ICL with various prompt templates to establish performance sensitivity
2. DSPy optimization with different signature designs to evaluate systematic improvement
3. Ablation study comparing ensemble performance with and without XGBoost meta-learner

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Small dataset size (2,650 examples) may limit generalizability to broader academic domains
- Limited comparative benchmarks against existing citation classification methods in other languages
- Evaluation focuses on accuracy without reporting precision, recall, or F1-score metrics

## Confidence
- High confidence in the novelty and significance of the dataset and DSPy optimization approach
- Medium confidence in ensemble model performance due to limited comparative benchmarks
- Low confidence in generalizability to other low-resource languages without further validation

## Next Checks
1. Test the model on a larger, more diverse Turkish academic corpus to assess scalability and generalizability
2. Compare ensemble model performance against citation classification methods in other languages
3. Evaluate model using additional metrics (precision, recall, F1-score) for comprehensive assessment