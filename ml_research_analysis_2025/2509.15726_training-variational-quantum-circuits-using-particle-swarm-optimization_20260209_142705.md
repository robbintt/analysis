---
ver: rpa2
title: Training Variational Quantum Circuits Using Particle Swarm Optimization
arxiv_id: '2509.15726'
source_url: https://arxiv.org/abs/2509.15726
tags:
- quantum
- optimization
- gates
- swarm
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Particle Swarm Optimization (PSO) to train the
  full architecture of Variational Quantum Circuits (VQCs) for biomedical image classification
  on MedMNIST datasets, rather than just optimizing parameters as in prior work. PSO
  is used to select both gate types (Rx, Ry, Rz, CNOT) and their configuration, enabling
  adaptive circuit design.
---

# Training Variational Quantum Circuits Using Particle Swarm Optimization

## Quick Facts
- arXiv ID: 2509.15726
- Source URL: https://arxiv.org/abs/2509.15726
- Reference count: 8
- This paper applies PSO to train full VQC architectures for biomedical image classification, achieving comparable accuracy to gradient descent while using fewer quantum gates.

## Executive Summary
This paper introduces a novel approach to training Variational Quantum Circuits (VQCs) by using Particle Swarm Optimization (PSO) to simultaneously optimize both the circuit architecture and parameters. Unlike previous methods that only optimize parameters on fixed architectures, PSO searches for the optimal combination of gate types (Rx, Ry, Rz, CNOT), target qubits, and rotation angles. The method is evaluated on binary classification tasks using MedMNIST biomedical image datasets, demonstrating comparable or superior accuracy to classical gradient descent while employing significantly fewer quantum gates. The approach also shows promise for avoiding barren plateaus and improving class balance in certain datasets.

## Method Summary
The paper applies PSO to train full VQC architectures for biomedical image classification. Each particle encodes a circuit using groups of 4 parameters: gate type (discretized from continuous value), target qubit, control qubit (for CNOT), and rotation angle. The PSO process uses 50 particles with 40 or 80 dimensions (corresponding to up to 10 or 20 gates) over 100 iterations. Gate type is determined by rounding 1 + 4×parameter, qubits by rounding 1 + n_qubits×parameter, and angles by scaling to [0, 2π]. The method is compared against an Adam baseline using fixed 8-qubit circuits with 32 gates in circular topology. Both methods use PCA to reduce images to 8 features and angle encoding for quantum state preparation.

## Key Results
- PSO achieved comparable or superior accuracy to Adam on most binary MedMNIST tasks while using fewer quantum gates (10-20 vs 32)
- On organA dataset, PSO-80 achieved 74% accuracy matching noise-free simulation results on real quantum hardware (Rigetti Ankaa-3 and IQM Garnet)
- PSO demonstrated better class balance in some datasets compared to Adam, which sometimes collapsed to predicting only one class
- The method avoided the barren plateaus issue that can impair gradient-based optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient-free optimization via PSO circumvents barren plateaus that impair gradient-based VQC training.
- **Mechanism:** PSO evaluates candidate circuits through swarm position updates rather than computing gradients of the cost landscape. Each particle maintains position, velocity, personal best, and swarm best. The update rules use cognitive coefficient c₁ and social coefficient c₂ without requiring gradient information.
- **Core assumption:** The loss landscape contains exploitable structure accessible through stochastic sampling, even when gradients vanish.
- **Evidence anchors:** [abstract] "This approach is motivated by the fact that commonly used gradient-based optimization methods can suffer from the barren plateaus problem."

### Mechanism 2
- **Claim:** Joint optimization of gate selection and parameters yields more compact circuits than fixed-architecture training.
- **Mechanism:** Each particle's 4-parameter group encodes: (1) discrete gate type via discretization formula g = round(1 + #gates × parameter), (2) target qubit, (3) control qubit for CNOT, (4) rotation angle scaled to [0, 2π]. PSO searches this unified space rather than optimizing parameters on a predetermined structure.
- **Core assumption:** The optimal circuit architecture exists within the restricted gate set {Rx, Ry, Rz, CNOT} and is discoverable within 100 iterations.
- **Evidence anchors:** [abstract] "PSO has been used to train the entire structure of VQCs, allowing it to select which quantum gates to apply, the target qubits, and the rotation angle"

### Mechanism 3
- **Claim:** Time-varying coefficient scheduling transitions swarm behavior from exploration to exploitation.
- **Mechanism:** Cognitive coefficient c₁ starts high (particles trust their own discoveries), social coefficient c₂ starts low, then invert over iterations. Inertia weight w decreases to slow movement and aid convergence.
- **Core assumption:** The optimal region is broadly discoverable early, then refinable through social information sharing.
- **Evidence anchors:** [section 2] "At the beginning... c₁ is set high and the social coefficient c₂ low, encouraging exploration... As iterations progress, c₁ decreases, while c₂ increases, shifting the behavior toward exploitation"

## Foundational Learning

- **Concept: Barren Plateaus in Variational Quantum Computing**
  - **Why needed here:** The paper's primary motivation is avoiding this gradient-vanishing phenomenon. Without understanding it, the advantage of PSO over Adam is opaque.
  - **Quick check question:** Can you explain why randomly initialized parameterized quantum circuits tend to have exponentially vanishing gradients as qubit count increases?

- **Concept: Particle Swarm Optimization Dynamics**
  - **Why needed here:** Understanding position/velocity updates, personal/global best, and coefficient roles is essential to modify or debug the architecture search.
  - **Quick check question:** What happens to convergence if inertia weight w remains high throughout optimization?

- **Concept: Discrete-Continuous Search Space Mapping**
  - **Why needed here:** The paper maps continuous PSO parameters to discrete gate choices. Understanding this mapping is critical for extending the gate set or qubit count.
  - **Quick check question:** If you increase from 8 to 12 qubits, how must the discretization formula change?

## Architecture Onboarding

- **Component map:** Input: MedMNIST image → PCA (8 features) → Angle encoding (8 qubits) → PSO Loop → Circuit synthesis → Evaluation → Position/velocity update → Output: Binary classification via measurement

- **Critical path:**
  1. Define parameter-to-gate encoding scheme (4 values per gate: type, target, control, angle)
  2. Implement discretization for gate/qubit selection and scaling for angles
  3. Build circuit synthesis function that constructs VQC from particle position
  4. Wire fitness evaluation (classification accuracy on validation batch)
  5. Configure PSO hyperparameters (c₁, c₂, w schedules, swarm size, iterations)

- **Design tradeoffs:**
  - Swarm size vs. iteration count: 50 particles × 100 iterations worked, but larger swarms may find better circuits at higher computational cost.
  - Dimension (40 vs. 80): More dimensions allow more gates but increase search space. Paper shows 80 dimensions matched or exceeded 40 on most datasets.
  - Gate set restriction: {Rx, Ry, Rz, CNOT} limits expressivity but constrains search; extending to more gate types increases search space exponentially.

- **Failure signatures:**
  - Adam collapses to single-class prediction (breast dataset: precision/recall = 0 for class 0) — monitor class-wise metrics, not just accuracy.
  - PSO underperforms on some datasets (pneumonia: 81% Adam vs. 71-72% PSO) — may indicate dataset-specific landscape structure favoring gradient methods.
  - Redundant gates in evolved circuits (Figure 1 shows non-contributing rotations) — suggests need for post-hoc circuit pruning.

- **First 3 experiments:**
  1. **Reproduce binary classification on organA:** 8 qubits, PSO-80, 50 particles, 100 iterations. Target: ~78% test accuracy matching paper.
  2. **Ablate dimension:** Compare PSO-40 vs. PSO-80 vs. PSO-120 on organA to characterize accuracy-vs-search-cost frontier.
  3. **Extend to 3-class classification:** Modify encoding and measurement for 3-class MedMNIST task. Monitor whether PSO's class-balance advantage persists with more classes (explicitly flagged as future work in paper).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can PSO-based architecture search maintain its convergence speed and accuracy advantages when applied to multiclass classification tasks?
- **Basis in paper:** [explicit] The conclusion states that future work will address "more challenging tasks, such as multiclass classification."
- **Why unresolved:** The current study validates the method only on binary classification tasks (e.g., selecting two classes from MedMNIST), leaving the performance on standard multiclass benchmarks unknown.
- **What evidence would resolve it:** Benchmarking the PSO-trained VQCs on datasets with >2 classes (e.g., BloodMNIST) and comparing accuracy and convergence rates against gradient-based optimizers.

### Open Question 2
- **Question:** Does the observed noise resilience persist when evaluating the full test dataset on quantum hardware?
- **Basis in paper:** [inferred] The hardware validation (Rigetti/IQM) was restricted to a small subset of 100 samples.
- **Why unresolved:** A 100-sample subset may not capture the variance of noise across a full dataset or the generalization error of the optimized circuit in a noisy environment.
- **What evidence would resolve it:** Execution of the optimized VQCs on noisy hardware using the complete test set (thousands of samples) to verify statistical robustness.

### Open Question 3
- **Question:** How does the computational cost of PSO scale with increasing circuit depth and qubit count compared to adaptive gradient methods?
- **Basis in paper:** [inferred] The experiments were limited to 8 qubits and shallow circuits (10-20 gates), while the authors explicitly mention exploring "more complex VQCs" in future work.
- **Why unresolved:** PSO is a population-based method (50 particles used here); as the search space grows with qubits, the simulation overhead may become prohibitive relative to gradient descent.
- **What evidence would resolve it:** Profiling training time and resource usage for PSO versus Adam on circuits with 20+ qubits or deeper ansatzes.

## Limitations

- PSO's barren plateau avoidance is inferred rather than experimentally validated; no systematic comparison of gradient norms or landscape analysis is provided.
- The 80-dimensional PSO results match noise-free simulation accuracy on real quantum hardware, but this is based on a 100-sample subset of organA, which may not generalize to full datasets or other tasks.
- The advantage of joint architecture-parameter optimization is demonstrated empirically but lacks ablation studies isolating the impact of adaptive gate selection versus parameter optimization alone.

## Confidence

- **High confidence:** PSO achieves comparable accuracy to Adam on most binary MedMNIST tasks with fewer gates; PSO avoids Adam's class imbalance failure on breast dataset.
- **Medium confidence:** PSO circumvents barren plateaus and demonstrates better generalization in some cases; claims are supported by empirical results but lack rigorous theoretical or ablation analysis.
- **Low confidence:** PSO's barren plateau avoidance is primarily inferred from motivation; no direct landscape analysis or gradient norm comparison is presented.

## Next Checks

1. **Validate barren plateau avoidance:** Compare gradient norms and trainability of PSO vs. Adam across varying qubit counts to directly test barren plateau claims.
2. **Ablate architecture search:** Compare PSO with fixed architecture (parameters only) to isolate the contribution of adaptive gate selection to performance gains.
3. **Extend to multiclass classification:** Apply PSO to 3+ class MedMNIST tasks to test whether class-balance advantages persist and to validate the method's scalability.