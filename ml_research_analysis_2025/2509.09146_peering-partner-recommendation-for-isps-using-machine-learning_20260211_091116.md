---
ver: rpa2
title: Peering Partner Recommendation for ISPs using Machine Learning
arxiv_id: '2509.09146'
source_url: https://arxiv.org/abs/2509.09146
tags:
- peering
- data
- features
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the feasibility of using publicly available
  ISP data to predict peering relationships between ISPs. The authors extract features
  from PeeringDB and CAIDA datasets, then apply machine learning models (tree-based,
  neural network, and transformer-based) to classify peering pairs.
---

# Peering Partner Recommendation for ISPs using Machine Learning

## Quick Facts
- arXiv ID: 2509.09146
- Source URL: https://arxiv.org/abs/2509.09146
- Authors: Md Ibrahim Ibne Alam; Ankur Senapati; Anindo Mahmood; Murat Yuksel; Koushik Kar
- Reference count: 40
- One-line primary result: XGBoost achieves 98% accuracy in predicting ISP peering relationships using only publicly available data

## Executive Summary
This study demonstrates that publicly available ISP data can effectively predict peering relationships between Autonomous Systems (ASes) using machine learning. The authors extract features from PeeringDB and CAIDA datasets, then apply various machine learning models to classify potential peering pairs. XGBoost emerges as the most effective model, achieving 98% accuracy when trained on an optimized feature set. The framework is robust to data variations, time shifts, and missing values, and can transfer well to new ASes. The results suggest that automated peering partner selection is feasible, offering a scalable solution to optimize Internet connectivity without requiring proprietary traffic data.

## Method Summary
The authors extract features from PeeringDB (net) and CAIDA (AS Rank and AS Relationships) datasets, filtering to 24,475 ASes common to both sources. They construct pairwise feature vectors by concatenating individual AS features with calculated metrics including Cone Overlap (customer cone intersection) and Affinity Score (PoP overlap). The dataset contains 34 features per pair after filtering. An XGBoost classifier is trained on this data using random splits, achieving high accuracy despite class imbalance (83:17). The model is evaluated on multiple datasets including temporal shifts and new AS pairs, demonstrating robustness and transfer capability.

## Key Results
- XGBoost achieves 98% overall accuracy and 94-95% balanced accuracy on the optimal feature set
- Model performs well even with only 2% of training data
- Framework remains robust across 2-year time shifts and transfers effectively to new ASes
- CAIDA AS Rank dataset performs exceptionally well compared to PeeringDB
- The approach requires no proprietary traffic data, relying only on publicly available information

## Why This Works (Mechanism)

### Mechanism 1: Structural Feature Determinism
Publicly available AS-level attributes contain sufficient signal to predict private business decisions (peering) without access to internal traffic logs. The model exploits correlations between structural proxies (e.g., customer cone size, geographic footprint) and peering viability. By concatenating features of two ASes into a pairwise vector, the model learns boundaries where mutual benefit (traffic offload, reachability) likely exists. The core assumption is that PeeringDB and CAIDA data accurately reflects real-world topology and policies. This mechanism would degrade if ISPs massively obfuscate public data or if peering decisions become driven primarily by non-technical political factors not captured in topology data.

### Mechanism 2: Feature Interaction via Tree Ensembles
Gradient-boosted trees (XGBoost) capture the conditional logic of peering decisions better than deep neural networks or transformer models. Peering decisions are likely threshold-based (e.g., "Peer only if traffic ratio is within range X and geographic overlap > Y"). Tree-based models excel at partitioning tabular data along these specific feature thresholds, whereas transformers struggle with the lack of sequential context and DNNs may over-smooth these distinct boundary conditions. The core assumption is that the decision boundary for "peer vs. non-peer" is complex but learnable via axis-aligned splits rather than requiring complex manifold learning. If the true decision function requires processing raw unstructured text rather than categorical/numerical summaries, tree models would fail.

### Mechanism 3: Transferability of Topological Invariants
A model trained on historical or subset data generalizes to new ASes and future timeframes because the underlying "physics" of Internet economics (gravity models, hierarchy) remain stable. The model learns the relationship between features (e.g., size disparity, affinity) rather than memorizing specific AS pairs. This allows it to evaluate a new AS with no historical peering data by comparing its structural features against the learned general rules. The core assumption is that fundamental economic incentives for peering do not shift drastically over short time periods (2 years). A major disruption in transit pricing could shift the economic basis of peering, invalidating the learned historical mappings.

## Foundational Learning

- **Concept: Autonomous Systems (AS) and BGP**
  - Why needed here: The paper predicts links between ASes. You cannot interpret the results without understanding that the Internet is a network of networks, and "peering" is a specific contractual relationship distinct from "transit" (customer-provider).
  - Quick check question: If AS A is a customer of AS B, and AS B is a customer of AS C, can AS A generally reach AS C via transit?

- **Concept: Feature Engineering for Pairwise Classification**
  - Why needed here: The raw data describes single nodes ($AS_i$). To predict a link, you must understand how the authors construct a pairwise feature vector (e.g., concatenation, subtraction, or overlap metrics like Cone Overlap).
  - Quick check question: Why might a model perform better using "Cone Overlap" rather than just raw "Cone Size" for two separate ASes?

- **Concept: Gradient Boosting (XGBoost)**
  - Why needed here: This is the core engine. You need to know that XGBoost builds trees sequentially, where each new tree corrects the errors of the previous ones, making it highly effective for structured/tabular data.
  - Quick check question: Why would a boosted tree model potentially overfit noise in the training data more than a Random Forest if not regularized?

## Architecture Onboarding

- **Component map:** API calls to CAIDA (AS Rank, AS Relationships) and PeeringDB (net) -> Raw JSON/CSVs -> Filtering 56 raw features down to 41 -> Feature Construction (Cone Overlap, Affinity Score) -> "Optimum Dataset" (34 features per pair) -> XGBoost Classifier -> Evaluation Metrics

- **Critical path:** The "Optimum Dataset" generation. The paper notes that simply dumping raw data works, but the "Optimal Dataset" (merging filtered features + Cone Overlap + Affinity Score) yields the 98% accuracy. If the Cone Overlap calculation is wrong, the model performance drops.

- **Design tradeoffs:**
  - **Accuracy vs. Freshness:** The model is robust over 2 years, but retraining takes only ~15 seconds. The tradeoff is strictly computational vs. operational complexity (retraining weekly is cheap and safer).
  - **Recall vs. Precision:** High *Overall* Accuracy (98%) is easy due to class imbalance. High *Balanced* Accuracy (94-95%) is the harder target for identifying the minority class (non-peering).

- **Failure signatures:**
  - **High Accuracy, Low Balanced Accuracy:** The model is biased toward the majority class ("Peer"), predicting "Peer" for everything.
  - **Poor Transfer Performance:** If testing on Dataset C (new ASes peering with each other) fails, the model has memorized specific ASNs rather than learning general topological rules.

- **First 3 experiments:**
  1. **Baseline Replication:** Download CAIDA AS Relationship and PeeringDB snapshots. Train a default XGBoost on the raw 82-feature set to verify the ~97% accuracy baseline.
  2. **Ablation Study:** Implement the "Optimal Dataset" pipeline (calculating Cone Overlap/Affinity) and compare performance against the baseline to quantify the gain from these engineered features.
  3. **Time-Shift Stress Test:** Train on $T_{month-2}$ data and test on $T_{month-0}$ data. Plot the degradation curve to determine the maximum safe interval between model retraining cycles.

## Open Questions the Paper Calls Out

### Open Question 1
How can the data-driven framework be extended to solve the "peering location problem," specifically identifying the optimal Internet Exchange Point (IXP) for a predicted connection? The current study successfully predicts the binary relationship (whether two ISPs should peer) but does not predict the physical or logical location (IXP) where that peering session should occur.

### Open Question 2
Do the model's positive predictions for "unknown" AS pairs align with actual future peering agreements established in the live Internet ecosystem? The model's validity is currently tested against historical data, but its effectiveness as a recommender system for previously unconnected pairs has not been validated by future data.

### Open Question 3
Would the integration of complete, non-missing peering policy data significantly improve the prediction accuracy beyond the current 98%? The current high accuracy (98%) is achieved by discarding textual policy features. It remains unclear if these discarded features contain unique signal that would further refine the model's boundaries.

### Open Question 4
Can the binary classification model be adapted to quantify the economic utility or traffic volume improvement of a predicted peering link? A binary recommendation does not distinguish between a highly beneficial peer and a marginally beneficial one, which is critical for ISP resource allocation.

## Limitations
- Data self-reporting bias: PeeringDB data is self-reported by ISPs, potentially introducing inaccuracies in feature values
- Temporal generalization: Model shows robustness over 2-year gap but may not hold during major market disruptions
- Domain specificity: Performance on entirely new peering scenarios (e.g., emerging technologies) that deviate from historical feature space is unknown

## Confidence
- **High Confidence:** XGBoost model's superior performance on optimal feature set (98% accuracy) is well-supported by ablation study results
- **Medium Confidence:** Model's robustness to time shifts and transfer to new ASes is demonstrated but limited to 2-year test periods
- **Medium Confidence:** Paper asserts model's ability to "optimize connectivity" but this is inferred from classification accuracy rather than direct measurement of real-world peering cost reduction

## Next Checks
1. **Feature Dependency Analysis:** Conduct formal ablation study to quantify exact contribution of engineered features (Cone Overlap, Affinity Score) versus raw features to model's performance
2. **Dynamic Retraining Benchmark:** Implement continuous retraining pipeline and measure performance decay rate over 6-12 months to determine optimal retraining frequency
3. **External Dataset Validation:** Test model on completely independent dataset of AS relationships (different time period or geographic region) to validate true generalization capability