---
ver: rpa2
title: 'Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits
  of LVLMs'
arxiv_id: '2508.17334'
source_url: https://arxiv.org/abs/2508.17334
tags:
- batsman
- bowler
- innings
- reasoning
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMCRICBENCH-3K, a new benchmark for evaluating
  large vision-language models on cricket scorecard images. The dataset includes 1,463
  synthetic scorecards (English and Hindi) with 1,500 English QA pairs, designed to
  test structure-aware, numerical, multi-image, and cross-lingual reasoning.
---

# Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs

## Quick Facts
- arXiv ID: 2508.17334
- Source URL: https://arxiv.org/abs/2508.17334
- Reference count: 28
- Large vision-language models struggle with numerical and cross-lingual reasoning on cricket scorecard images, showing accuracy drops of 6-12% on Hindi vs English scorecards.

## Executive Summary
This paper introduces MMCRICBENCH-3K, a new benchmark for evaluating large vision-language models on cricket scorecard images. The dataset includes 1,463 synthetic scorecards (English and Hindi) with 1,500 English QA pairs, designed to test structure-aware, numerical, multi-image, and cross-lingual reasoning. Experiments show that even state-of-the-art models like GPT-4o and Qwen2.5VL struggle, with accuracy peaking at 57.3% on English and dropping further on Hindi, highlighting weaknesses in cross-lingual and numerical reasoning over structured visual text.

## Method Summary
The paper presents MMCRICBENCH-3K, a benchmark consisting of 1,463 synthetic cricket scorecard images with 1,500 English QA pairs across English and Hindi subsets. The evaluation tests large vision-language models on structure-aware, numerical, multi-image, and cross-lingual reasoning tasks. Models are evaluated zero-shot with a prompt prefix instructing precise, 1-2 word answers in digits when required. The benchmark includes single-image and multi-image splits, with questions categorized into direct retrieval (C1), basic arithmetic (C2), and multi-step reasoning (C3). Experiments are conducted on 3x NVIDIA A6000 GPUs.

## Key Results
- Even state-of-the-art models like GPT-4o and Qwen2.5VL struggle with accuracy peaking at 57.3% on English scorecards
- Performance drops significantly on Hindi scorecards, with a cross-lingual gap of 6-12%
- Chain-of-Thought prompting improves single-image reasoning but degrades multi-image reasoning performance
- OCR+LLM pipelines underperform native LVLMs on structured tabular data, highlighting the importance of spatial feature binding

## Why This Works (Mechanism)

### Mechanism 1: Visual-Spatial Structure Binding
If LVLMs outperform OCR+LLM pipelines on tabular data, it is likely because the LVLM preserves spatial coordinates (row/column alignment) during feature extraction, whereas linearizing text via OCR destroys the structural relationships required for numerical reasoning.

### Mechanism 2: Script-Specific Feature Saturation
If models exhibit a sharp performance drop on Hindi scorecards despite identical logical structure, the visual encoder likely suffers from "script-specific feature saturation," where text-token alignment is optimized for Latin scripts (English) and fails to generalize to Devanagari (Hindi) visual patterns.

### Mechanism 3: Context Drift in Multi-Image Reasoning
If Chain-of-Thought (CoT) prompting improves single-image reasoning but degrades multi-image reasoning, CoT may induce "context drift," where the model focuses on the textual rationale generated from the first image and fails to re-attend to the second image's distinct visual features.

## Foundational Learning

- **Concept: Table Structure Recognition (TSR) vs. OCR**
  - Why needed: The paper highlights that simply reading text (OCR) is insufficient; the model must understand that a number belongs to the "Runs" column and the "Innings 1" row.
  - Quick check: If I present a table as a list of sentences ("Batsman A scored 10 runs"), does the model perform better than if I present it as an image?

- **Concept: Cross-Lingual Visual Grounding**
  - Why needed: Understanding that "visual text" is a modality distinct from "language." A model can be fluent in Hindi text (LLM) but blind to Hindi script in images (ViT).
  - Quick check: Can the model correctly transcribe the Hindi numerals in the scorecard image independent of answering the question?

- **Concept: Synthetic Data Fidelity**
  - Why needed: MMCRICBENCH uses synthetic scorecards (WeasyPrint).
  - Quick check: Are the fonts, borders, and backgrounds perfectly crisp, or is noise added? Perfectly crisp data might lead to "texture bias" where models fail on real, noisy TV broadcast screenshots.

## Architecture Onboarding

- **Component map:** Cricket Scorecard Image -> ViT Encoder -> Visual Tokens -> MLP/Q-Former Projector -> LLM Reasoning Core
- **Critical path:** High-Res Feature Extraction -> Structure Alignment -> Numerical Logic Execution
- **Design tradeoffs:** Synthetic vs. Real (control vs. realism), OCR+LLM vs. End-to-End (cheaper but fails on structure vs. expensive but retains spatial cues)
- **Failure signatures:** Cross-Lingual Drop (~6-12% lower on Hindi), Hallucinated Calculation (CoT multi-image), Duck Blindness (failure on Hindi ducks)
- **First 3 experiments:**
  1. Baseline Replication: Run Qwen2.5VL-7B on MMCRICBENCH-E subset to verify 49.1% C1 accuracy
  2. OCR Ablation: Extract text using Tesseract (Hindi model) and compare against visual model
  3. Prompt Sensitivity Test: Test Q2 ("Who got out for a duck?") with standard vs. CoT prompts

## Open Questions the Paper Calls Out

- Would the observed cross-lingual performance gaps generalize to other non-Latin scripts (e.g., Tamil, Bengali, Urdu) common in cricket-viewing regions?
- Why does Chain-of-Thought prompting improve single-image reasoning but degrade multi-image reasoning?
- How much of the performance drop on Hindi scorecards is attributable to OCR errors versus higher-level reasoning failures in cross-lingual grounding?
- Will models trained or fine-tuned on MMCRICBENCH-3K generalize to real-world cricket scorecard images with noise, varied layouts, and OCR artifacts?

## Limitations

- Synthetic nature of MMCRICBENCH-3K may not capture real-world complexity, noise, and variability
- Cross-lingual evaluation limited to English and Hindi, leaving out other regional scripts prevalent in cricket contexts
- Lack of error attribution analysis between visual perception failure and reasoning failure

## Confidence

- **High Confidence**: Core claim that LVLMs struggle with cross-lingual visual reasoning (6-12% accuracy drops)
- **Medium Confidence**: Visual encoders suffer "script-specific feature saturation" for Hindi
- **Low Confidence**: CoT induces "context drift" in multi-image reasoning

## Next Checks

1. Evaluate models on synthetic scorecards using non-Devanagari scripts (e.g., Arabic, Cyrillic) to test script generalization
2. Test best-performing models from MMCRICBENCH on real broadcast screenshots (10-20 images) to measure real-world degradation
3. Compare LVLM baseline against OCR+LLM pipeline enhanced with explicit bounding box coordinates to isolate spatial binding effects