---
ver: rpa2
title: Reliability Crisis of Reference-free Metrics for Grammatical Error Correction
arxiv_id: '2509.25961'
source_url: https://arxiv.org/abs/2509.25961
tags:
- metrics
- systems
- evaluation
- score
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper exposes significant vulnerabilities in reference-free\
  \ metrics for grammatical error correction (GEC), revealing that adversarial systems\
  \ can exploit these metrics to achieve unjustifiably high scores. The authors propose\
  \ attack strategies for four metrics\u2014SOME, Scribendi, IMPARA, and LLM-based\
  \ metrics\u2014and demonstrate that their adversarial systems outperform state-of-the-art\
  \ GEC systems on the BEA-2019 development set."
---

# Reliability Crisis of Reference-free Metrics for Grammatical Error Correction

## Quick Facts
- arXiv ID: 2509.25961
- Source URL: https://arxiv.org/abs/2509.25961
- Reference count: 36
- Authors demonstrate that adversarial systems can exploit reference-free metrics to achieve unjustifiably high scores, undermining GEC evaluation reliability

## Executive Summary
This paper exposes significant vulnerabilities in reference-free metrics for grammatical error correction (GEC), revealing that adversarial systems can exploit these metrics to achieve unjustifiably high scores. The authors propose attack strategies for four metrics—SOME, Scribendi, IMPARA, and LLM-based metrics—and demonstrate that their adversarial systems outperform state-of-the-art GEC systems on the BEA-2019 development set. For example, their SOME adversarial system achieves a score of 1.013 compared to the previous best of 0.836, while their Scribendi adversarial system reaches 4179 versus 1821. These results undermine the reliability of automatic GEC evaluation, as metrics can be manipulated to rank adversarial systems higher than legitimate ones. The study suggests metric ensembles as a potential short-term solution and highlights the need for more robust evaluation methods that can withstand adversarial attacks.

## Method Summary
The paper attacks four reference-free GEC metrics using distinct strategies: (1) SOME: selects a single generic sentence from training corpus maximizing weighted scores; (2) Scribendi: performs token substitution via BERT MLM to lower perplexity while maintaining surface similarity above threshold; (3) IMPARA: retrieves semantically distant sentences from corpus that bypass cosine similarity filter while possessing high QE scores; (4) LLM: exploits prompt injection to override scoring rubric. The attacks are evaluated on BEA-2019 dev set (first 400 sentences) using both absolute scores and TrueSkill relative rankings, comparing against 10 SOTA systems.

## Key Results
- Adversarial-SOME system achieves score of 1.013 vs SOTA 0.836 on BEA-2019 dev set
- Adversarial-Scribendi system reaches 4179 vs SOTA 1821 on absolute metric
- Adversarial systems outperform SOTA on TrueSkill relative rankings across all four metrics
- Individual attacks fail when evaluated by other metrics, suggesting ensemble protection

## Why This Works (Mechanism)

### Mechanism 1
Reference-free metrics relying on fixed weighted averages can be exploited by maximizing high-weight features while ignoring low-weight constraints. The SOME attack targets the 0.55 grammaticality and 0.43 fluency weights by selecting a single generic sentence (e.g., "The weather is mild.") that maximizes these scores while ignoring the minimal 0.02 meaning preservation weight.

### Mechanism 2
Metrics using hard thresholds for filtering are vulnerable to adversarial inputs engineered to sit just above decision boundaries. Scribendi and IMPARA attacks "game" filters through token substitution that lowers perplexity while maintaining surface similarity above threshold, or retrieving semantically distant sentences that mathematically bypass cosine similarity requirements.

### Mechanism 3
LLM evaluators using prompt-based interfaces are susceptible to instruction injection attacks. The attack exploits the LLM's instruction-following training by embedding malicious directives within the "corrected sentence" field, causing the model to process override commands as higher-priority than evaluation criteria.

## Foundational Learning

- **Concept: Reference-free vs. Reference-based Evaluation**
  - Why needed: The paper exploits the freedom of reference-free metrics. Unlike reference-based metrics, reference-free metrics score based on general qualities (fluency, grammaticality), allowing attackers to propose unrelated but fluent sentences.
  - Quick check: If a metric is reference-free, what prevents a system from outputting a completely unrelated but grammatically perfect sentence?

- **Concept: Perplexity (PPL) and Surface Similarity**
  - Why needed: These signals are hacked in the Scribendi metric. Lower perplexity implies fluency, but fluency doesn't guarantee semantic correctness (e.g., changing "You" to "What" might lower perplexity but ruins meaning).
  - Quick check: Why is a simple Levenshtein distance threshold often insufficient to preserve meaning during an adversarial attack?

- **Concept: Adversarial Machine Learning (Evasion Attacks)**
  - Why needed: The paper frames metric hacking as an evasion attack. Attackers modify input to maximize reward (metric score) without solving the intended task (correcting grammar).
  - Quick check: How does the "Metric Ensemble" defense relate to the concept of increasing attack surface complexity?

## Architecture Onboarding

- **Component map:** Source Sentence (S) + Hypothesis Sentence (H) → Semantic Filter → Quality Scorer → Aggregator
- **Critical path:** The Semantic Filter is the primary point of failure. Experiments show that if the filter (Scribendi's LDR/TSR or IMPARA's Cosine Sim) is bypassed, the Quality Scorer is easily fooled by generic or low-perplexity text.
- **Design tradeoffs:** Robustness vs. Correlation (strict filters may be robust but penalize valid edits; loose filters correlate better but allow adversarial drift); Metric Complexity (LLM metrics are capable but introduce new attack vectors).
- **Failure signatures:** Universal Output (single sentence achieving top scores across inputs); High Score + Low Semantic Sim (high score with similarity score near filter threshold); Instruction Artifacts (presence of instruction-like syntax in output logs).
- **First 3 experiments:** (1) Threshold Stress Test: vary IMPARA/Scribendi thresholds (0.5-0.95) and plot attack success vs SOTA scores; (2) Ensemble Cross-Validation: run 4 adversarial systems against other 3 metrics to confirm metric-specificity; (3) LLM Sanitization: implement regex filter to detect/remove instruction keywords before LLM-E evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the GEC community formally define the boundary between valid grammatical corrections and adversarial or non-corrected sentences to improve metric filtering?
- Basis: Section 4.2 states this boundary is important but "remains ambiguous" because the field hasn't previously considered adversarial inputs.
- Why unresolved: Lack of clear definition hinders creating filters that distinguish legitimate edits from adversarial manipulations preserving superficial similarity.
- What evidence would resolve it: Formalized taxonomy of correction validity or new dataset annotated to distinguish adversarial outputs from valid corrections.

### Open Question 2
- Question: Can metric ensembles be compromised by complex "Pareto-optimal" attacks designed to optimize scores across multiple metrics simultaneously?
- Basis: "Limitations" section notes exploring complex strategies like "Pareto-optimal attacks that cover all metrics" is beyond scope.
- Why unresolved: Authors demonstrated vulnerabilities using simple, isolated attacks, but ensemble defense robustness against coordinated, multi-metric attacks remains untested.
- What evidence would resolve it: Experiments using multi-objective optimization algorithms to generate adversarial sentences that successfully fool metric ensembles.

### Open Question 3
- Question: What architectural or algorithmic designs can effectively increase the cost for attackers attempting to exploit reference-free metrics?
- Basis: Section 4.2 identifies "designing architectures and algorithms that make attacker costs higher" as potential solution for future research.
- Why unresolved: Current metrics rely on standard model structures easily reversed or exploited via prompt injection or simple search strategies.
- What evidence would resolve it: Proposal of metric architecture that provably requires significantly higher computational resources or data access to generate successful adversarial examples.

## Limitations
- Scalability uncertainty: Unclear whether attacks remain effective on full-scale test sets or different data distributions
- LLM attack inconsistency: Effectiveness varies significantly across model sizes (Qwen2.5-32B and Llama-3-70B not vulnerable)
- Limited ensemble defense validation: Only tested against authors' own attacks, not adaptive adversaries targeting ensembles differently

## Confidence
- **High confidence** in SOME and Scribendi vulnerabilities: Well-defined, deterministic attacks directly reproducible from specifications
- **Medium confidence** in IMPARA vulnerability: Clear kNN approach but success depends on uncharacterized corpus composition and embedding quality
- **Low confidence** in LLM vulnerability claims: Inconsistent results across models, attack methodology lacks detail about batch ordering effects

## Next Checks
1. **Threshold Stress Test**: Systematically vary similarity thresholds (0.5 to 0.95) for IMPARA and Scribendi, measuring adversarial attack success rates against SOTA system scores to identify robust operating zones.

2. **Adaptive Attack Validation**: Implement adaptive adversaries that target metric ensembles specifically, crafting inputs that maximize SOME while minimizing IMPARA scores to test ensemble defense claims.

3. **LLM Injection Robustness**: Test prompt injection attack across broader range of LLM configurations (different instruction-following datasets, temperature settings, input sanitization methods) given inconsistent results across model sizes.