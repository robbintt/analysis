---
ver: rpa2
title: 'When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation
  Evaluation'
arxiv_id: '2601.20858'
source_url: https://arxiv.org/abs/2601.20858
tags:
- language
- bleu
- source
- llama
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates cross-direction contamination in machine\
  \ translation, where training on one language pair leads to artificially inflated\
  \ performance on unseen translation directions due to target-side memorization.\
  \ Using Bloomz, a multilingual LLM trained on FLORES-200, and Llama as an uncontaminated\
  \ control, the study confirms Bloomz's FLORES contamination and demonstrates that\
  \ contamination is cross-directional\u2014performance in unseen translation directions\
  \ is artificially boosted due to memorization of target-language text."
---

# When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation

## Quick Facts
- arXiv ID: 2601.20858
- Source URL: https://arxiv.org/abs/2601.20858
- Reference count: 40
- Primary result: Cross-direction contamination in machine translation is driven by target-side memorization, where models recall memorized targets regardless of source language, artificially inflating BLEU scores.

## Executive Summary
This work investigates cross-direction contamination in machine translation, where training on one language pair leads to artificially inflated performance on unseen translation directions due to target-side memorization. Using Bloomz, a multilingual LLM trained on FLORES-200, and Llama as an uncontaminated control, the study confirms Bloomz's FLORES contamination and demonstrates that contamination is cross-directional—performance in unseen translation directions is artificially boosted due to memorization of target-language text. Experiments with back-translated and paraphrased sources show that Bloomz can still recall memorized targets even with altered inputs, indicating source-side triggers. Replacing named entities consistently decreases BLEU scores, suggesting an effective method for probing memorization. Fine-tuning Llama on FLORES eng↔xxx pairs replicates the contamination pattern, reinforcing that target-side memorization drives cross-directional performance gains.

## Method Summary
The study compares bloomz-7b1 (contaminated) vs Llama-3.1-8B-Instruct (control) on FLORES-200 dev set across 15 languages and 225 translation directions. BLEU and COMET-22-DA scores are computed for each pair. Back-translation and paraphrasing experiments test perturbation resistance. Named entity replacement via spaCy identifies retrieval triggers. Fine-tuning Llama on eng↔xxx pairs with Axolotl simulates contamination. Evaluation uses sacrebleu for BLEU and COMET-22-DA for semantic scoring.

## Key Results
- Bloomz shows cross-directional contamination: high BLEU in unseen translation directions when target language matches training data
- Back-translated and paraphrased sources with low BLEU overlap to originals still trigger high recall, indicating threshold-based retrieval
- Replacing named entities consistently decreases BLEU by 5-20 points, confirming their role as retrieval triggers
- Fine-tuning clean Llama on eng↔xxx pairs replicates contamination pattern, with BLEU gains clustering by target language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-direction contamination is driven by target-side memorization, not source-side learning.
- Mechanism: When trained on parallel data with language Y as target (e.g., eng→tam), the model memorizes target text. During inference on unseen directions (e.g., spa→tam), it recalls the memorized target regardless of source language.
- Core assumption: The model treats target-side text as retrievable content rather than learning transferable translation mappings.
- Evidence anchors:
  - [abstract] "machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization"
  - [Section 3.1] Heatmaps show clean columns for certain target languages but non-zero rows when those same languages are sources
  - [Section 4] Fine-tuning Llama shows BLEU gains in xxx→eng "overshadow" eng→xxx gains, "reinforcing target side memorization"

### Mechanism 2
- Claim: Named entities function as retrieval triggers for memorized target sequences.
- Mechanism: Contaminated models use named entities as anchoring cues to retrieve memorized target text. Even when source sentences are altered via back-translation or paraphrasing, retained entities can trigger recall.
- Core assumption: Named entities are semantically stable across translations and thus become reliable retrieval keys during training.
- Evidence anchors:
  - [Section 3.2] Back-translated sources with near-zero BLEU overlap still yield 20-60 BLEU in some cases; manual inspection shows "named entities are still retained"
  - [Section 3.3] Replacing named entities causes "consistent decrease in BLEU"—5-10 points for single entity replacement, 10-20 points for all entities

### Mechanism 3
- Claim: Surface-form perturbations are insufficient to eliminate contamination effects.
- Mechanism: Contamination manifests through partial cue matching—models can recall memorized targets even when source overlap with original training data is low (10-20 BLEU), suggesting threshold-based retrieval rather than exact matching.
- Core assumption: Memorization creates associative bonds that activate on partial similarity, not exact sequence matching.
- Evidence anchors:
  - [Section 3.2] When back-translated sources have 10-20 BLEU overlap with original sources, Bloomz still achieves 45-50 BLEU
  - [Section 3.2] Paraphrased sources with higher surface overlap than back-translated sources score 20-30 BLEU lower

## Foundational Learning

- **Data contamination in benchmark evaluation**: Understanding that models can memorize test data during training, inflating scores without genuine capability gains, is essential for interpreting this paper's claims. *Quick check*: If a model achieves 90 BLEU on a test set it was trained on, does this indicate strong translation ability? (Answer: No—it indicates memorization masquerading as performance.)

- **Multiway parallel corpora**: FLORES-200 contains the same sentences translated into 200+ languages; this structure enables cross-direction contamination because the target text is identical across many source→target pairs. *Quick check*: Why does multiway parallelism create unique contamination risks? (Answer: Memorizing one target benefits all translation directions into that target language.)

- **BLEU vs. COMET as diagnostic metrics**: The paper uses BLEU for surface-form overlap detection (memorization indicator) and COMET for semantic similarity; high BLEU + high COMET suggests contamination, while moderate BLEU + high COMET suggests genuine translation. *Quick check*: What metric pattern would indicate contamination vs. genuine performance? (Answer: Unreasonably high BLEU [e.g., 80+] with high COMET suggests contamination; reasonable BLEU [30-50] with high COMET suggests genuine capability.)

## Architecture Onboarding

- **Component map**: Contamination detection module -> Perturbation pipeline -> Fine-tuning replication -> Evaluation harness

- **Critical path**:
  1. Establish baseline contamination: Run full pairwise translation matrix, compute BLEU and COMET for each pair
  2. Diagnose target-side vs. source-side memorization: Compare column patterns vs. row patterns
  3. Probe trigger mechanisms: Apply perturbations (back-translation, entity replacement) and measure BLEU delta
  4. Replicate causally: Fine-tune clean model on contaminated data pattern to confirm mechanism

- **Design tradeoffs**:
  - Entity replacement precision vs. scalability: Manual correction of spaCy entity labels ensures accuracy but doesn't scale
  - Back-translation quality variance: Using Llama for back-translation creates quality variance, which is actually useful as a natural perturbation gradient
  - Fine-tuning epochs: Paper uses 3 epochs to achieve "severe memorization"—fewer epochs may under-replicate contamination effects

- **Failure signatures**:
  - False negative: Clean model shows high BLEU in some directions—check if those are high-resource language pairs with naturally stronger baselines
  - Inconclusive perturbation results: If entity replacement doesn't decrease BLEU, verify entity identification worked correctly
  - COMET decreasing while BLEU increases (observed in fine-tuned Llama): Indicates extreme memorization of some samples degrades overall semantic quality

- **First 3 experiments**:
  1. Reproduce Bloomz heatmap: Run 15×15 translation matrix, compute BLEU and COMET for each pair; verify asymmetric column/row patterns
  2. Named entity ablation: Select 100 sentences with entities, create one-entity and all-entities replacement variants; measure BLEU delta between original and replaced versions for both Bloomz and Llama
  3. Fine-tuning contamination replication: Fine-tune clean Llama on FLORES eng↔xxx data for 11 languages using Axolotl with specified parameters; evaluate all 121 pairs and compute BLEU/COMET differences

## Open Questions the Paper Calls Out

- **Internal model mechanisms**: What are the internal model mechanisms that activate memorized text during cross-direction contamination? The study focused on empirical behavioral evaluation rather than mechanistic interpretability or internal activation analysis.

- **Model size scaling**: Does the magnitude of cross-direction contamination scale with model size or parameter count? Experiments are "limited to 7-8B parameter models; results may vary for different model sizes."

- **Linguistic trigger features**: Which specific linguistic features in dissimilar source inputs (beyond named entities) trigger the recall of memorized targets? While named entities explain some recall, they do not fully account for cases where structurally divergent, low-similarity inputs successfully trigger memorization.

- **Pre-training vs. fine-tuning contamination**: Does contamination from pre-training manifest differently than contamination from fine-tuning? The fine-tuning experiments "simulate contamination but may not fully replicate contamination that occurs during pre-training."

## Limitations

- Generalizability to other LLM architectures: Results may not transfer to encoder-decoder models or other LLM families, as different architectures may handle memorization and retrieval differently.
- Entity replacement coverage: Method relies on spaCy's named entity recognition, which may miss domain-specific or low-resource language entities, potentially underestimating contamination effects.
- Fine-tuning replication scope: Limited set of 11 languages and 3 epochs; longer training or broader language coverage might yield different contamination patterns.

## Confidence

- **High Confidence**: The core finding that Bloomz is contaminated by FLORES and that this contamination is cross-directional is well-supported by the heatmaps and ablation studies.
- **Medium Confidence**: The named entity retrieval trigger mechanism is plausible and supported by experiments, but could benefit from additional probing.
- **Low Confidence**: The perturbation resistance of contamination is less directly tested; the back-translation results are compelling but could be influenced by the quality variance of the back-translation model.

## Next Checks

1. **Replication on additional architectures**: Run the same contamination detection and ablation experiments on a mix of encoder-decoder (e.g., mBART) and decoder-only (e.g., GPT-style) models to assess if the cross-direction contamination pattern holds across architectures.

2. **Entity perturbation ablation study**: Extend entity replacement experiments to test non-entity tokens (e.g., common nouns, verbs) that are semantically similar to named entities to determine if the effect is specific to named entities or applies more broadly to high-salience tokens.

3. **Multi-way parallel corpus simulation**: Create a controlled multi-way parallel dataset with known contamination and test if the proposed named entity replacement method reliably detects and mitigates contamination across all translation directions, not just those involving the contaminated target.