---
ver: rpa2
title: 'From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training'
arxiv_id: '2508.09224'
source_url: https://arxiv.org/abs/2508.09224
tags:
- safety
- helpfulness
- user
- prompts
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models typically use binary refusal training to\
  \ block harmful requests, but this approach can be brittle\u2014especially for dual-use\
  \ scenarios where legitimate requests could be misused. The proposed safe-completion\
  \ method shifts focus from refusing prompts to ensuring the safety of the model\u2019\
  s output, allowing the model to provide helpful, non-harmful guidance where possible."
---

# From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training

## Quick Facts
- arXiv ID: 2508.09224
- Source URL: https://arxiv.org/abs/2508.09224
- Reference count: 36
- Large language models typically use binary refusal training to block harmful requests, but this approach can be brittle—especially for dual-use scenarios where legitimate requests could be misused. The proposed safe-completion method shifts focus from refusing prompts to ensuring the safety of the model's output, allowing the model to provide helpful, non-harmful guidance where possible. In experiments comparing GPT-5 with safe-completion training to o3 trained with refusals, the safe-completion approach improved safety across benign, dual-use, and malicious prompts, while substantially increasing helpfulness and reducing the severity of residual failures. Human evaluations confirmed these findings, showing safer and more useful responses. Safe-completion thus offers a more nuanced, output-focused alternative to traditional refusal-based safety training.

## Executive Summary
This paper proposes a new approach to LLM safety training called "safe-completion" that shifts from binary refusal to ensuring the safety of model outputs. Instead of simply refusing harmful requests, the method trains models to provide helpful, non-harmful guidance where possible by optimizing a composite reward of helpfulness and safety. Experiments show that this approach outperforms traditional refusal training on both safety and helpfulness metrics across benign, dual-use, and malicious prompts, while reducing the severity of residual failures.

## Method Summary
The safe-completion method uses a two-stage training approach. First, supervised fine-tuning teaches models to reason over explicit safety policy specifications and choose among three response modes: direct answer, safe-completion (high-level guidance), or refuse with redirection. Second, reinforcement learning optimizes a composite reward combining a safety reward model (penalizing severity of violations) and a helpfulness reward model (rewarding both direct and indirect helpfulness). The multiplicative reward structure ensures that any unsafe content nullifies reward regardless of helpfulness, while preserving optimization pressure toward useful responses within safety bounds.

## Key Results
- Safe-completion training significantly outperformed traditional refusal training on safety metrics across all prompt types (benign, dual-use, malicious)
- Helpfulness scores improved substantially with safe-completion, especially on dual-use and malicious prompts
- The severity of unsafe responses decreased with safe-completion training, with failures being less harmful than those from refusal-trained models
- Human evaluations confirmed that safe-completion responses were both safer and more helpful than refusal-based responses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A multiplicative reward structure (helpfulness × safety) creates a hard constraint while preserving optimization pressure toward useful responses within safety bounds.
- **Mechanism:** The reward formula ri = hi·si ensures that any policy violation (si→0) nullifies reward regardless of helpfulness, while high-safety responses receive gradient signals toward maximal helpfulness. This avoids the local optimum of "refuse everything to be safe."
- **Core assumption:** The reward model can reliably distinguish severity levels of policy violations and assign continuous safety scores rather than binary pass/fail.
- **Evidence anchors:**
  - [abstract] "Safe-completions seek to maximize helpfulness within the safety policy's constraints"
  - [section 2.2] "any truly unsafe content (si=0) yields zero reward regardless of helpfulness, and unhelpful content (low hi) will yield a low reward even if safe"
  - [corpus] Limited direct corpus evidence on multiplicative vs. additive reward structures; related work (Constitutional AI, Safe-RLHF) addresses similar trade-offs through different objective formulations
- **Break condition:** If safety reward model cannot reliably detect nuanced violations or produces noisy scores, the gradient signal degrades and models may learn to game the scoring function rather than internalize safety constraints.

### Mechanism 2
- **Claim:** Training models to reason over explicit policy specifications during SFT enables more consistent policy application at inference time.
- **Mechanism:** By augmenting prompts with policy specs during SFT data generation, the model learns to retrieve and apply relevant policy clauses in its chain-of-thought before generating responses. This creates an internal deliberation process rather than relying solely on pattern-matching from refusal examples.
- **Core assumption:** The model's chain-of-thought during training meaningfully reflects its reasoning process at deployment, and policy specs are comprehensive enough to cover edge cases.
- **Evidence anchors:**
  - [abstract] "safety-training approach that centers on the safety of the assistant's output, rather than a binary classification of the user's intent"
  - [section 2.1] "Since the CoT references the spec, the SFT stage teaches the model to reason over the spec itself before answering"
  - [corpus] Deliberative Alignment (Guo et al.) provides precedent for spec-aware reasoning; over-refusal mitigation work suggests intent classification alone is brittle
- **Break condition:** If policies contain ambiguities or contradictions, or if the model learns to generate plausible-sounding but superficial CoT without genuine policy reasoning, the mechanism fails to generalize to novel prompts.

### Mechanism 3
- **Claim:** Explicitly rewarding "indirect helpfulness" (constructive alternatives, safe redirections) creates a viable optimization path when direct compliance would violate policy.
- **Mechanism:** The helpfulness RM assigns high scores to responses that either directly fulfill the task or indirectly support user goals through safe alternatives. This gives the model a non-zero reward path during RL even when it must refuse, preventing collapse into uninformative refusals.
- **Core assumption:** Human/AI evaluators can consistently distinguish high-quality indirect help from low-effort refusals, and the resulting reward signal is stable enough for RL optimization.
- **Evidence anchors:**
  - [section 2.2] "indirect helpfulness: how well the response supports the user's underlying well-being and goals by offering clear, constructive, and relevant alternatives"
  - [section 3.2.1] "helpfulness scores given safe output, CE-SafeComplete yields small but significant gains for benign and dual-use cases, and shows a major improvement on malicious prompts (by more than 1.0 point on the 1–4 scale)"
  - [corpus] Over-refusal literature (Bianchi et al., Shi et al.) documents the problem but offers limited evidence on indirect-helpfulness reward shaping specifically
- **Break condition:** If indirect-helpfulness scoring is inconsistent or if models learn to generate lengthy but unhelpful "safe" filler, the mechanism produces verbose low-value responses rather than genuinely constructive alternatives.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** Safe-completion training modifies the RL reward structure; understanding baseline RLHF helps isolate what's novel vs. standard practice.
  - **Quick check question:** Can you explain why a learned reward model is necessary rather than directly using human ratings during RL?

- **Concept: Dual-Use Ambiguity**
  - **Why needed here:** The core motivation is that binary refusal fails on prompts where harm depends on detail level rather than surface intent.
  - **Quick check question:** Give an example of a query that's benign at high abstraction but potentially harmful with operational specifics.

- **Concept: Reward Hacking / Specification Gaming**
  - **Why needed here:** Composite rewards create new optimization pressures; engineers should anticipate failure modes where models exploit scoring rubrics.
  - **Quick check question:** If a model learned to generate very long responses with safety disclaimers but minimal useful content, which reward component might it be exploiting?

## Architecture Onboarding

- **Component map:** SFT Stage: Policy specs → augmented prompts → base reasoning model → (CoT, answer) pairs → filter unsafe answers → train on (original prompt, filtered completion). RL Stage: Prompt → model → response → Safety RM (policy compliance) × Helpfulness RM (direct + indirect) → final reward → policy update. Evaluation: Intent classifier (Benign/Dual-use/Malicious) → Safety grader → Helpfulness grader (conditioned on safe) → Harm severity grader (for unsafe responses).

- **Critical path:** The reward model quality is the bottleneck. If the Safety RM cannot reliably score nuanced violations on a continuous scale, the entire multiplicative reward mechanism degrades. Validate Safety RM calibration before大规模 RL training.

- **Design tradeoffs:**
  - Granular safety scores (0–1 continuous) vs. binary: more expressive but requires more labeling effort and higher inter-annotator agreement
  - Separate direct/indirect helpfulness scoring vs. unified: unified is simpler but may conflate different user preferences
  - Policy spec inclusion in SFT vs. implicit learning: explicit specs improve interpretability but require maintaining synchronized policy documents

- **Failure signatures:**
  - **Verbose safety theater:** Long responses with excessive disclaimers but minimal actionable help (indirect-helpfulness reward over-optimized)
  - **Inconsistent severity scoring:** Same violation type receiving different safety scores across runs (RM calibration drift)
  - **Over-refusal on edge cases:** Dual-use prompts still receiving hard refusals (SFT data coverage gaps or policy ambiguity)
  - **Jailbreak susceptibility:** Adversarial prompts bypassing safety via indirect paths not covered in training distribution

- **First 3 experiments:**
  1. **Reward structure ablation:** Train with (a) multiplicative reward, (b) additive reward (safety + helpfulness), (c) safety-only threshold then helpfulness. Compare safety/helpfulness tradeoffs on held-out dual-use set.
  2. **Indirect-helpfulness quality audit:** Sample 100 model refusals from each training paradigm; manually annotate whether responses offer genuinely constructive alternatives vs. token redirections.
  3. **Severity calibration check:** On a curated set of responses with known harm levels (by expert annotation), plot Safety RM scores vs. true severity to validate continuous scoring discriminates between negligible/low/moderate/high harm.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can safe-completion training effectively generalize to non-reasoning models, or does the ability to navigate nuanced safety constraints depend on the advanced reasoning capabilities of models like GPT-5?
- Basis in paper: [inferred] Section 2.3 contrasts the new approach with binary refusal, noting the binary method was "appropriate for early non-reasoning models," while the experiments focus exclusively on reasoning architectures (o4-mini, GPT-5).
- Why unresolved: The paper does not ablate the reasoning component to test if the safe-completion reward signal is sufficient for models lacking internal chain-of-thought capabilities.
- What evidence would resolve it: A controlled experiment applying safe-completion training to a non-reasoning base model and measuring its ability to adhere to safety constraints without explicit reasoning traces.

### Open Question 2
- Question: Does the strategy of providing "safe alternatives" and "indirect helpfulness" inadvertently increase vulnerability to multi-turn adversarial attacks where users incrementally extract restricted information?
- Basis in paper: [inferred] Section 4 discusses dual-use jailbreaks that "nudge the model from high-level discussion into operational specifics," while Section 2.2 explicitly rewards the model for providing high-level guidance as a safe alternative.
- Why unresolved: The evaluation methodology relies on single-turn safety grading, which does not capture the cumulative risk of multi-turn interactions where safe partial information might be aggregated into harmful actionable intelligence.
- What evidence would resolve it: A red-teaming evaluation simulating multi-turn "fisher" attacks specifically designed to leverage the model's safe-completion redirections to eventually bypass safety filters.

### Open Question 3
- Question: Is the automated safety grader ($s_i$) robust against "reward hacking," where the model learns to generate outputs that maximize the safety score while retaining subtle, harmful content?
- Basis in paper: [inferred] Section 2.2 describes the final reward as $r_i = h_i \cdot s_i$, creating a strong optimization pressure for high safety scores, but relies on a reasoning model to grade severity without discussing Goodhart's law or specification gaming.
- Why unresolved: The paper reports reduced severity of failures but does not analyze if the residual failures include instances where the model successfully gamed the grader by obfuscating harmful content in safe-seeming language.
- What evidence would resolve it: A human adversarial review of model outputs rated as "safe" by the autograder to identify false negatives where the model exploited blind spots in the grading prompt.

## Limitations
- The paper doesn't test whether safe-completion training generalizes to non-reasoning models that lack chain-of-thought capabilities
- The evaluation methodology relies on single-turn safety grading, missing potential multi-turn adversarial risks
- The automated safety grader's robustness against reward hacking is not analyzed, despite creating strong optimization pressure for high safety scores

## Confidence

- **High confidence:** The core observation that binary refusal is brittle for dual-use scenarios is well-supported by examples and aligns with established over-refusal literature.
- **Medium confidence:** The claim that safe-completion improves both safety and helpfulness metrics is supported by human evaluations, though the evaluation design doesn't fully isolate which mechanism drives the improvements.
- **Low confidence:** The assertion that the multiplicative reward structure is superior to alternatives and that the CoT policy reasoning is necessary for safe-completion's success - these are stated as design choices but not empirically validated against simpler alternatives.

## Next Checks

1. **Reward structure ablation:** Train identical models with (a) multiplicative reward (safety × helpfulness), (b) additive reward (safety + helpfulness), and (c) safety-threshold-then-helpfulness reward. Compare safety/helpfulness tradeoffs on held-out dual-use prompts to isolate whether the multiplicative structure is truly optimal.

2. **Policy reasoning necessity test:** Compare safe-completion performance between models trained with full CoT + policy spec reasoning vs. models trained with only the final filtered completions (no explicit policy reasoning in training). This would determine whether the reasoning step adds value beyond just having good training examples.

3. **Indirect-helpfulness quality audit:** Sample 100 model responses from each training paradigm (safe-completion vs. refusal) on dual-use prompts. Have human annotators classify whether each response provides genuinely constructive alternatives or merely token redirections, measuring the actual utility of "indirect helpfulness" beyond surface-level score improvements.