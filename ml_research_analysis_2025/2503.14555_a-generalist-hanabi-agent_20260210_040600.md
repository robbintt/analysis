---
ver: rpa2
title: A Generalist Hanabi Agent
arxiv_id: '2503.14555'
source_url: https://arxiv.org/abs/2503.14555
tags:
- learning
- agents
- game
- r3d2
- hanabi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces R3D2, a generalist Hanabi agent that overcomes
  the limitation of traditional MARL agents which can only play one specific game
  setting and struggle to cooperate with unfamiliar partners. The key innovation is
  reformulating Hanabi as a text-based game, using natural language for observations
  and actions to create a representation that is agnostic to the number of players.
---

# A Generalist Hanabi Agent

## Quick Facts
- arXiv ID: 2503.14555
- Source URL: https://arxiv.org/abs/2503.14555
- Reference count: 40
- First agent that can play all Hanabi settings concurrently and extend strategies learned from one setting to others

## Executive Summary
This paper introduces R3D2, a generalist Hanabi agent that overcomes the limitation of traditional MARL agents which can only play one specific game setting and struggle to cooperate with unfamiliar partners. The key innovation is reformulating Hanabi as a text-based game, using natural language for observations and actions to create a representation that is agnostic to the number of players. This is combined with a novel neural network architecture that handles dynamic observation- and action-spaces, integrating language models with Deep Recurrent Relevance Q-network (DRRN) and trained using a distributed setup.

## Method Summary
R3D2 reformulates Hanabi as a text-based game using natural language for observations and actions, creating a representation agnostic to the number of players. The method combines language models with Deep Recurrent Relevance Q-network (DRRN) and employs a distributed training setup. This approach allows the agent to handle dynamic observation- and action-spaces while maintaining strong performance across different player counts (2-5 players).

## Key Results
- First agent that can play all Hanabi settings concurrently
- Demonstrates strong zero-shot coordination performance with both unseen R3D2 agents and completely different algorithmic agents
- Achieves competitive self-play scores while maintaining high cross-play performance across different game settings

## Why This Works (Mechanism)
The text-based reformulation of Hanabi allows R3D2 to create a representation that is independent of the number of players. By using natural language for observations and actions, the agent can generalize across different game settings without requiring separate training for each configuration. The integration of language models with DRRN enables the agent to handle the dynamic nature of observation and action spaces in Hanabi, while the distributed training setup ensures efficient learning across all game settings simultaneously.

## Foundational Learning
- Hanabi Game Mechanics: Understanding the cooperative card game where players must work together to play cards in a specific order without seeing their own cards
  - Why needed: Essential for understanding the problem domain and evaluation metrics
  - Quick check: Can explain the win condition and basic gameplay loop

- Text-based Game Representation: Converting game states and actions into natural language descriptions
  - Why needed: Enables creation of player-agnostic representations
  - Quick check: Can demonstrate how a game state translates to text and back

- Deep Recurrent Relevance Q-network (DRRN): A reinforcement learning architecture that combines recurrent networks with Q-learning
  - Why needed: Handles sequential decision-making in partially observable environments
  - Quick check: Understands how recurrence helps maintain game state information

- Zero-shot Coordination: Ability to cooperate with unfamiliar partners without prior training together
  - Why needed: Critical for real-world multi-agent applications where agents must work with unknown partners
  - Quick check: Can define and explain the challenge of generalizing to unseen agents

## Architecture Onboarding

Component Map: Natural Language Encoder -> DRRN Core -> Action Decoder

Critical Path: Text-based observations are encoded through the language model, processed by the DRRN to generate Q-values for possible actions, and the highest-valued action is decoded back into natural language for execution.

Design Tradeoffs: The text-based representation sacrifices some precision for generalization, while the DRRN architecture balances memory requirements with the need to track game state over time. The distributed training setup trades computational resources for faster convergence across multiple game settings.

Failure Signatures: Poor performance with certain partner types may indicate insufficient training diversity, while inconsistent cross-play results could suggest instability in the language model's interpretation of game states.

First Experiments:
1. Test the agent's ability to play a single Hanabi setting (e.g., 2-player) to verify basic functionality
2. Evaluate cross-play performance with a simple heuristic agent to establish baseline coordination
3. Measure the impact of different language model sizes on game performance

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertain generalization beyond the Hanabi domain to other multi-agent cooperative games
- No evaluation under adversarial conditions or with conflicting objectives
- Performance improvements are well-documented within Hanabi but may not translate to other domains

## Confidence

| Claim | Confidence |
|-------|------------|
| R3D2 is a "generalist" agent | Medium (specifically tailored to Hanabi variations) |
| Extends strategies learned from one setting to others | High (within Hanabi scope) |
| Strong zero-shot coordination performance | High (based on reported results) |

## Next Checks
1. Test R3D2 on other cooperative multi-agent games (e.g., Overcooked, StarCraft II) to assess domain generalization
2. Evaluate the agent's performance in Hanabi with restricted communication or adversarial players to understand its robustness
3. Conduct ablation studies to determine the contribution of the text-based representation and the novel neural network architecture to the overall performance