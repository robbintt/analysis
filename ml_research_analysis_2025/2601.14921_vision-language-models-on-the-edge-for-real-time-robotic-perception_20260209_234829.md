---
ver: rpa2
title: Vision-Language Models on the Edge for Real-Time Robotic Perception
arxiv_id: '2601.14921'
source_url: https://arxiv.org/abs/2601.14921
tags:
- edge
- uni00000013
- latency
- robot
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a real-time robotic perception system deploying
  vision-language models (VLMs) on ORAN/MEC edge infrastructure. Using the Unitree
  G1 humanoid robot, the authors implement a WebRTC-based pipeline streaming multimodal
  data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct against cloud deployment.
---

# Vision-Language Models on the Edge for Real-Time Robotic Perception

## Quick Facts
- arXiv ID: 2601.14921
- Source URL: https://arxiv.org/abs/2601.14921
- Reference count: 28
- Edge deployment of VLMs on ORAN/MEC infrastructure achieves 5% latency reduction compared to cloud while maintaining near-cloud accuracy.

## Executive Summary
This paper presents a real-time robotic perception system that deploys vision-language models on ORAN/MEC edge infrastructure. Using the Unitree G1 humanoid robot, the authors implement a WebRTC-based pipeline streaming multimodal data to an edge node. They evaluate LLaMA-3.2-11B-Vision-Instruct against cloud deployment, demonstrating edge deployment achieves 5% latency reduction (1600.03 ms vs. 1685.20 ms) while maintaining near-cloud accuracy. The study also evaluates Qwen2-VL-2B-Instruct, achieving sub-second responsiveness but with 13% lower accuracy, establishing a Pareto frontier for deployment.

## Method Summary
The system uses a Unitree G1 humanoid robot with RealSense camera and microphone streaming data via WebRTC to an ORAN/MEC edge node. The edge node runs a FastAPI server hosting LLaMA-3.2-11B-Vision-Instruct (4-bit NF4 quantized) and Qwen2-VL-2B-Instruct models on an NVIDIA L4 GPU. The pipeline includes WebRTC transport, image preprocessing, vision encoding, and autoregressive text generation. Evaluation uses the Robo2VLM benchmark and a custom robot-collected dataset (200 Q&A pairs across 5 HRI domains). Cloud baseline uses NVIDIA NIM API. Latency is measured from frame capture to response receipt, and accuracy uses exact match for VQA and normalized scores for free-form responses.

## Key Results
- Edge deployment reduces end-to-end latency by 5% compared to cloud (1600.03 ms vs. 1685.20 ms)
- LLaMA-3.2-11B-Vision-Instruct achieves 77-80% accuracy on robot tasks while Qwen2-VL-2B-Instruct achieves sub-second responsiveness but with 13% lower accuracy
- Text generation dominates total inference time, contributing over 85% of end-to-end latency

## Why This Works (Mechanism)

### Mechanism 1: Proximity-Based Latency Reduction
Deploying VLMs on ORAN/MEC edge nodes reduces end-to-end latency compared to cloud deployment by eliminating wide-area network transit. Co-locating compute resources with the radio access network enables local breakout of traffic, reducing backhaul load and propagation delay. The core assumption is that RTT to the cloud is the significant variable while inference compute time remains relatively constant between edge and cloud instances.

### Mechanism 2: The Autoregressive Bottleneck
In current VLM architectures, text generation (autoregressive decoding) is the dominant latency component, not vision encoding or data transport. The model generates tokens sequentially where each token depends on previous ones, limiting parallelization. Vision encoding processes the image in a single forward pass (parallel), while language model generation is serial token-by-token.

### Mechanism 3: Accuracy-Latency Scaling via Model Size
Reducing model parameter count (from 11B to 2B) drastically reduces inference time but degrades reasoning capability. Fewer parameters reduce FLOPs required per token generation, directly speeding up the bottleneck. However, this reduces the model's capacity for complex multimodal grounding, establishing a Pareto frontier for deployment.

## Foundational Learning

- **Concept: ORAN & Multi-access Edge Computing (MEC)**
  - Why needed here: To understand where the compute is happening. Unlike "cloud" (remote datacenter) or "onboard" (robot's CPU), MEC places a capable GPU (NVIDIA L4) at the cellular base station site.
  - Quick check question: Does the VLM run on the robot's internal computer? (Answer: No, it runs on the ORAN edge node).

- **Concept: VLM Inference Pipeline**
  - Why needed here: To diagnose latency. The pipeline splits into Vision Encoding (image → vectors) and Text Generation (vectors → tokens). Understanding this separates data prep from the serial generation bottleneck.
  - Quick check question: If the system takes 1.6 seconds to respond, which component consumes ~1.36 seconds? (Answer: Text generation).

- **Concept: WebRTC (Web Real-Time Communication)**
  - Why needed here: To understand the data transport. WebRTC uses UDP/SRTP to prioritize speed over reliability (dropping frames rather than stalling), which is critical for real-time robot control loops.
  - Quick check question: Why use WebRTC instead of standard HTTP streaming for a robot? (Answer: Lower latency via UDP/SRTP transport).

## Architecture Onboarding

- **Component map:**
  - Robot (UE): Unitree G1 + RealSense Camera + Microphone → WebRTC Sender
  - Transport: WebRTC Data Channels (Control/Text) + Video Tracks (SRTP)
  - Edge Node (MEC): FastAPI Server → VLM Engine (Llama-3.2-11B or Qwen2-VL-2B)
  - Client: React App (Livestream + Chat Interface)

- **Critical path:**
  1. Capture: RealSense RGB Frame (Robot)
  2. Transport: WebRTC Video Track to Edge (Network)
  3. Pre-process: Decode Image (Edge CPU)
  4. Inference: Vision Encode → Autoregressive Text Generation (Edge GPU)
  5. Transport: Text Response to React App (Network)

- **Design tradeoffs:**
  - LLaMA-3.2-11B (Edge): High accuracy (~77-80% on robot tasks), moderate latency (~1.6s). Best for complex reasoning.
  - Qwen2-VL-2B (Edge): Low accuracy (lags in spatial reasoning), fast latency (<1s). Best for simple presence detection.
  - Quantization (4-bit NF4): Essential to fit 11B model on single NVIDIA L4 (24GB VRAM); trades minor accuracy for feasibility.

- **Failure signatures:**
  - High Latency (>2s): Likely GPU contention on the edge node or network congestion breaking the "modest latency" assumption.
  - Low Accuracy on Spatial Tasks: Using Qwen2-VL-2B for "Navigation Context" or "Spatial Relations" (See Fig 4).
  - Stuttering Video: WebRTC bandwidth estimation failure or UDP packet loss.

- **First 3 experiments:**
  1. **Latency Breakdown Profiling:** Isolate and measure Image Decode vs. Vision Encoder vs. Token Generation to verify the 85% bottleneck on your specific hardware.
  2. **Model Swap Stress Test:** Deploy Qwen2-VL-2B vs. Llama-3.2-11B on the exact same queries to empirically map the accuracy drop on your specific robot tasks.
  3. **Network Resilience Test:** Introduce artificial jitter/packet loss on the WebRTC stream to determine the failure threshold for the control loop.

## Open Questions the Paper Calls Out
1. How can adaptive hybrid deployment strategies dynamically route queries between lightweight and large VLMs to optimize the latency–accuracy trade-off in real-time?
2. What model–system co-design techniques can effectively reduce the latency of autoregressive decoding, which accounts for over 85% of end-to-end inference time?
3. How do edge deployment benefits generalize across diverse robotic platforms, environments, and network conditions beyond the single humanoid robot and laboratory setting tested?

## Limitations
- The 5% edge deployment latency advantage is measured against cloud API without specifying geographic distance, network conditions, or inference-only vs end-to-end timing breakdown.
- The 4-bit quantization of LLaMA-3.2-11B-Vision-Instruct is stated as necessary but accuracy impact vs FP16 is not quantified.
- The custom robot-collected dataset (200 Q&A pairs) is not publicly available, making independent verification impossible.
- The study only compares two VLMs and one edge hardware configuration, limiting generalizability.

## Confidence
- **High confidence**: Edge deployment reduces latency vs cloud (supported by ORAN/MEC proximity advantage and the 5% measured reduction)
- **Medium confidence**: Text generation dominates VLM latency (supported by internal profiling showing >85% contribution)
- **Medium confidence**: Qwen2-VL-2B offers sub-second latency at ~13% accuracy cost (supported by cross-model comparison)
- **Low confidence**: The specific Pareto frontier between model size, latency, and accuracy generalizes beyond the two models tested

## Next Checks
1. **Independent accuracy validation**: Run the same edge deployment with LLaMA-3.2-11B-Vision-Instruct on a publicly available VQA benchmark (e.g., VQA-v2, GQA) to verify claimed accuracy levels.
2. **Latency source isolation**: Use NVIDIA Nsight Systems or similar profiling tools to separate network RTT from actual inference compute time on the edge node.
3. **Quantization quality assessment**: Run a controlled experiment comparing FP16 vs 4-bit NF4 quantized LLaMA-3.2-11B on the same validation set to quantify accuracy degradation.