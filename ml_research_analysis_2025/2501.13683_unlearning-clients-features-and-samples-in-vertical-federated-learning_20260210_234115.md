---
ver: rpa2
title: Unlearning Clients, Features and Samples in Vertical Federated Learning
arxiv_id: '2501.13683'
source_url: https://arxiv.org/abs/2501.13683
tags:
- loss
- unlearning
- test
- party
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes two frameworks for unlearning in Vertical
  Federated Learning (VFL): VFU-KD for unlearning passive parties and features, and
  VFU-GA for unlearning samples. VFU-KD leverages knowledge distillation to handle
  model compression and avoids communication between parties, while VFU-GA uses gradient
  ascent for efficient sample unlearning.'
---

# Unlearning Clients, Features and Samples in Vertical Federated Learning

## Quick Facts
- **arXiv ID:** 2501.13683
- **Source URL:** https://arxiv.org/abs/2501.13683
- **Reference count:** 40
- **Primary result:** Proposes VFU-KD and VFU-GA frameworks for unlearning in VFL, achieving comparable or better performance than retraining from scratch in many cases with a modest utility loss of 1-5%.

## Executive Summary
This paper addresses the challenge of unlearning specific entities (clients, features, or samples) in Vertical Federated Learning (VFL). It proposes two frameworks: VFU-KD, which uses knowledge distillation to unlearning passive parties and features without communication, and VFU-GA, which employs gradient ascent for efficient sample unlearning. A membership inference attack is introduced to audit the effectiveness of unlearning. Experiments on six tabular and two image datasets demonstrate that VFU-KD and VFU-GA achieve performance comparable to or better than retraining from scratch and the R2S method, with improvements of 0-2% in many cases and a modest utility loss of 1-5% in others.

## Method Summary
The paper introduces two frameworks for unlearning in VFL. VFU-KD leverages knowledge distillation, where the active party stores historical embeddings from passive parties. Upon an unlearning request, a new student model is trained using these embeddings and guided by the old teacher model's outputs. VFU-GA uses gradient ascent to efficiently unlearn specific samples by modifying the active party's model parameters to maximize loss on the target samples while minimizing loss on the retain set. A membership inference attack is used to audit the success of unlearning.

## Key Results
- VFU-KD and VFU-GA achieve comparable or better performance than retraining from scratch and the R2S method.
- Unlearning is effective for passive parties, features, and samples, with utility loss of 1-5% in some cases.
- The MIA auditing model effectively detects residual influence after unlearning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation enables approximate unlearning of passive parties and features without active-passive communication.
- Mechanism: The active party stores historical embeddings from passive parties. Upon an unlearning request, a new student model is initialized with a reduced input layer and trained using the stored embeddings and the teacher model's outputs.
- Core assumption: The active party has sufficient storage to retain historical embeddings for all training batches.
- Evidence anchors:
  - [abstract] "VFU-KD leverages knowledge distillation to handle model compression and avoids communication between parties... active party to store the previously communicated embeddings."
  - [section] Algorithm 2 and Eq. 5 detail the distillation process using stored $H^t$.
  - [corpus] Related work like *Certifying the Right to Be Forgotten* also explores unlearning in VFL but frames it as a primal-dual optimization problem; this paper chooses distillation specifically for its compression capability.
- Break condition: Storage constraints at the active party prevent retaining historical embeddings; the student model cannot train without this data.

### Mechanism 2
- Claim: Gradient ascent provides a computationally efficient method for sample unlearning in VFL.
- Mechanism: For specific data samples to be unlearned, the active party modifies its model parameters to maximize the loss on the "forget set" while minimizing loss on the "retain set".
- Core assumption: The unlearning rate $\lambda$ and number of epochs are well-calibrated to avoid model instability or local optima.
- Evidence anchors:
  - [abstract] "VFU-GA uses gradient ascent for efficient sample unlearning."
  - [section] Algorithm 4 and Eq. 8 define the gradient ascent update rule for the active party model.
  - [corpus] The corpus neighbor *REMISVFU* proposes a representation misdirection method; this suggests alternative unlearning paths exist, but GA is chosen here for simplicity and speed on samples.
- Break condition: A poor choice of $\lambda$ causes model parameters to diverge or get stuck in poor local optima, degrading utility.

### Mechanism 3
- Claim: A binary Membership Inference Attack (MIA) model can effectively audit the success of unlearning in VFL.
- Mechanism: A classifier is trained to distinguish between the output probabilities of the active model when the unlearned entity was present versus absent. A significant drop in this MIA model's accuracy after the unlearning process provides evidence that the influence has been removed.
- Core assumption: The MIA model used is sufficiently sensitive to detect residual influence.
- Evidence anchors:
  - [abstract] "A membership inference attack is introduced to audit unlearning effectiveness."
  - [section] Section 3.1 and Figure 2 explain the MIA training and its role in auditing. Figure 8 shows the accuracy drop.
  - [corpus] Explicit corpus evidence on the superiority of this specific MIA for auditing is weak or missing in the provided neighbors; the paper positions it as a necessary alternative to backdoor attacks.
- Break condition: The MIA model is too weak to detect subtle residual influence, leading to a false positive audit.

## Foundational Learning

- **Concept:** Vertical Federated Learning (VFL)
  - Why needed here: The entire problem is defined within the VFL paradigm where parties share a sample space but have distinct feature spaces.
  - Quick check question: In a VFL setup with 3 clients, which component holds the labels?

- **Concept:** Knowledge Distillation
  - Why needed here: It is the core technique of VFU-KD, enabling model compression by transferring knowledge from a larger teacher to a smaller student.
  - Quick check question: What are the two components of the loss function in standard knowledge distillation?

- **Concept:** Gradient Ascent
  - Why needed here: It is the core technique of VFU-GA, used to reverse the learning process on specific data points.
  - Quick check question: How does the update rule in gradient ascent differ from gradient descent?

## Architecture Onboarding

- **Component map:**
    - Passive Parties (Clients) -> Active Party (Server) -> Unlearning Engine -> MIA Auditor

- **Critical path:**
    1. **Training:** Passive parties compute embeddings -> Active party concatenates, computes loss, backpropagates gradients. Active party must store embeddings $H^t$ for every batch.
    2. **Unlearning Request:** Identify type (Client, Feature, Sample).
    3. **Execution:**
        *   If Client/Feature: Active party (or passive party for feature) initializes a smaller student model. Trains it using stored embeddings and the teacher model's outputs.
        *   If Sample: Active party computes loss on target and retain sets. Updates model using modified gradient ascent rule.
    4. **Auditing:** Train MIA model on pre- and post-unlearning model outputs. Verify accuracy drop.

- **Design tradeoffs:**
    - **Storage vs. Communication:** VFU-KD avoids costly re-training communication by requiring the active party to store all historical embeddings. High storage cost is traded for zero active-passive communication during unlearning.
    - **Speed vs. Complexity:** VFU-GA is faster for sample unlearning than KD but requires careful tuning of the ascent rate $\lambda$ to avoid instability.
    - **Audit Sensitivity:** The paper uses a simple MIA model for auditing. A more complex model might be more sensitive but is computationally more expensive.

- **Failure signatures:**
    - **VFU-KD:** "Out of memory" error at the active party due to embedding storage. Inability to recover loss after unlearning (student model fails to converge).
    - **VFU-GA:** Exploding loss values or model weights diverging due to excessive gradient ascent rate ($\lambda$).
    - **MIA Audit:** No drop in attack accuracy post-unlearning, indicating the unlearning process failed to remove the influence.

- **First 3 experiments:**
    1. **Baselines vs. VFU-KD:** Retrain a model from scratch without a target client. Compare its performance (AUC, F1) and loss curve to a model unlearned with VFU-KD. Quantify the utility gap and unlearning speed.
    2. **Storage Cost Measurement:** Measure the disk usage at the active party for storing embeddings over a standard training run (e.g., 50 epochs). Correlate this with dataset size and number of clients.
    3. **Unlearning Audit Sensitivity:** Implement the paper's MIA auditor. After running VFU-KD/GA, plot the attack accuracy over time. Then, replace the simple MIA model with a more complex one (e.g., a multilayer perceptron) and check if the audit results change significantly.

## Open Questions the Paper Calls Out

- **Question:** How does the effectiveness of VFU-KD and VFU-GA hold up when audited by state-of-the-art Membership Inference Attack (MIA) models rather than the simple binary classifier used in the paper?
- **Question:** Do the loss spikes induced by the knowledge distillation process in VFU-KD create a temporal vulnerability to membership inference or gradient-based attacks?
- **Question:** What strategies can be developed to reduce the storage overhead for the active party without reintroducing communication costs or compromising unlearning effectiveness?
- **Question:** How do VFU-KD and VFU-GA perform in VFL settings with high data heterogeneity or system heterogeneity, such as the presence of stragglers?

## Limitations

- **Storage Overhead:** VFU-KD requires the active party to store all historical embeddings, which can be a significant memory burden.
- **MIA Sensitivity:** The paper uses a relatively simple MIA model for auditing, which may not be sensitive enough to detect subtle residual influence.
- **Data Heterogeneity:** The approach assumes limited data heterogeneity and that all passive parties are readily available for training, with no stragglers.

## Confidence

- **High:** The core mechanism of knowledge distillation for unlearning passive parties/features (VFU-KD) and gradient ascent for sample unlearning (VFU-GA).
- **Medium:** The effectiveness of the proposed MIA auditing model for detecting residual influence.
- **Low:** The practical feasibility of the storage requirements for VFU-KD in large-scale deployments.

## Next Checks

1. **Storage Cost Validation:** Measure the actual disk storage required to retain embeddings for all training batches on a standard dataset (e.g., Adult). Compare this to available storage on typical hardware.
2. **MIA Sensitivity Test:** Replace the paper's simple MIA model with a more complex architecture (e.g., a 2-layer MLP) and re-run the auditing experiment. Compare the detected drop in accuracy to assess sensitivity.
3. **GA Stability Sweep:** Systematically vary the gradient ascent rate Î» (e.g., {0.1, 0.5, 1.0, 2.0}) in VFU-GA on a sample dataset. Measure the resulting utility and check for stability/divergence across the range.