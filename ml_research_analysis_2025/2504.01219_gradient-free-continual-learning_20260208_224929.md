---
ver: rpa2
title: Gradient-free Continual Learning
arxiv_id: '2504.01219'
source_url: https://arxiv.org/abs/2504.01219
tags:
- learning
- data
- continual
- tasks
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces EvoCL, a gradient-free optimization approach
  for continual learning that mitigates catastrophic forgetting by approximating past
  task losses using an auxiliary adapter network. Unlike traditional gradient-based
  methods, EvoCL does not rely on backpropagation through stored data, enabling fundamentally
  different mechanisms for preserving past knowledge.
---

# Gradient-free Continual Learning

## Quick Facts
- arXiv ID: 2504.01219
- Source URL: https://arxiv.org/abs/2504.01219
- Reference count: 16
- Primary result: Gradient-free evolution strategy outperforms state-of-the-art on MNIST/FashionMNIST but underperforms on CIFAR100

## Executive Summary
This paper introduces EvoCL, a gradient-free optimization approach for continual learning that addresses catastrophic forgetting by approximating past task losses using an auxiliary adapter network. Unlike traditional gradient-based methods, EvoCL employs evolution strategies to update network parameters when gradients cannot be computed due to lack of stored past data. The method shows significant improvements (up to 24.1% on MNIST) on simpler datasets but struggles with more complex data like CIFAR100, suggesting scalability challenges.

## Method Summary
EvoCL uses an evolution strategy to optimize model parameters by minimizing a combined loss function that includes current task loss, approximated past task losses, and adapter training loss. An auxiliary adapter network learns to transform latent space embeddings between consecutive model versions, enabling past task loss estimation without storing raw data. The method stores a small number of latent features per class (N features of size S) to compute loss approximations, making it exemplar-free. For MNIST/FashionMNIST, a full MLP is trained; for CIFAR100, only a subset of parameters is trained using LoRA.

## Key Results
- Achieves up to 24.1% improvement in average accuracy on MNIST split into 5 tasks
- Outperforms state-of-the-art methods on MNIST and FashionMNIST
- Underperforms gradient-based approaches on CIFAR100 by 8.7%
- Maintains performance without requiring stored raw data

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Free Optimization via Evolution Strategies
Evolution strategies evaluate the combined loss (Lt + L̂<t) at perturbed parameter configurations, selecting updates that reduce loss without requiring gradient computation through stored data. This bypasses the fundamental limitation that ∇θLA is unavailable when data for task A is inaccessible.

### Mechanism 2: Latent Space Alignment via Auxiliary Adapter
An auxiliary adapter network learns to map embeddings from frozen model Ft-1 to current model Ft using MSE loss on current task data. This enables computation of L̂<t on transformed past features as a proxy for true past task performance.

### Mechanism 3: Compressed Feature Memory for Exemplar-Free Learning
Storing N latent features per class enables loss approximation without raw data storage. These features are transformed via the adapter and used to compute L̂<t, providing a memory-efficient alternative to exemplar replay.

## Foundational Learning

- **Evolution Strategies (ES)**: Black-box optimization technique that estimates gradient direction using only function evaluations; essential for gradient-free parameter updates.
  - Quick check: Can you explain how evolution strategies estimate gradient direction using only function evaluations?

- **Exemplar-Free Class-Incremental Learning (EFCIL)**: Problem setting where raw data storage is prohibited due to privacy/memory constraints; motivates gradient-free approaches.
  - Quick check: Why does standard SGD cause catastrophic forgetting when past data is unavailable?

- **Latent Space Geometry & Feature Alignment**: Understanding how neural network representations transform across tasks is critical for adapter network design.
  - Quick check: How does the distribution of latent features change as a neural network is updated on new tasks?

## Architecture Onboarding

- **Component map**: Feature extractor F -> Adapter network -> Evolution strategy optimizer -> Linear classification head -> Feature memory buffer
- **Critical path**: Task t data → Ft → compute Lt; stored features from tasks <t → adapter → compute L̂<t; ES optimization minimizes combined loss; memory update stores N features per new class
- **Design tradeoffs**: Adapter capacity vs. overfitting; feature memory size vs. privacy constraints; ES population size vs. computational budget; full model vs. LoRA training
- **Failure signatures**: Performance degradation on complex datasets; high computational costs for evolution strategies; adapter failing to generalize transformations
- **First 3 experiments**:
  1. Sanity check on MNIST 2-task split: Verify EvoCL maintains >90% accuracy on task 1 after training on task 2
  2. Ablation on adapter quality: Train adapter with varying N features per class; plot correlation with final accuracy
  3. ES hyperparameter sweep: Test population sizes [10, 50, 100] on FashionMNIST 5-task split

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the performance degradation on CIFAR100 stem primarily from the constraints of training only a subset of parameters (LoRA) or the inherent complexity of the dataset?
- **Basis in paper**: Section 3 states "Further investigation is required to explain why the method performs poorly - is it because of the frozen part of the feature extractor or more complex dataset?"
- **Why unresolved**: The paper uses different training strategies for different datasets (full MLP for MNIST vs. LoRA for CIFAR100), confounding the root cause.
- **What evidence would resolve it**: Ablation study applying EvoCL to full network vs. subsets on CIFAR100, or applying LoRA constraint to simpler datasets.

### Open Question 2
- **Question**: How can the computational efficiency of the evolution strategy be optimized to handle the increased cost associated with complex datasets?
- **Basis in paper**: Abstract and conclusion identify "higher computational costs, particularly for complex datasets" and list "optimizing computational efficiency" as future work.
- **Why unresolved**: The gradient-free nature requires more evaluations than backpropagation, creating a scalability bottleneck.
- **What evidence would resolve it**: Comparative analysis of wall-clock training time and memory usage, followed by algorithmic improvements that close this gap.

### Open Question 3
- **Question**: Can the auxiliary adapter network's architecture be improved to provide more accurate approximations of past task losses (L̂<t)?
- **Basis in paper**: Conclusion notes effectiveness is contingent upon "adapter network and loss approximation quality" and suggests improving these estimations.
- **Why unresolved**: The current approach uses a standard adapter (e.g., MLP) but doesn't explore if more sophisticated mappings could reduce estimation error.
- **What evidence would resolve it**: Experiments with deeper or attention-based adapter mechanisms demonstrating higher correlation between approximated and true losses.

## Limitations

- Evolution strategy scalability remains unclear, with significant performance degradation on CIFAR100 compared to gradient-based methods
- Adapter network generalization across complex latent space transformations hasn't been validated beyond simple architectures
- Critical hyperparameters (population size, mutation magnitude, feature memory parameters) appear crucial but are not fully specified

## Confidence

- **High Confidence**: Fundamental mechanism of using evolution strategies to avoid gradient computation through stored data is sound and theoretically justified
- **Medium Confidence**: Adapter network approach works well on MNIST/FashionMNIST but performance on CIFAR100 suggests limitations
- **Low Confidence**: Scalability claims to larger models are unsupported given the negative result on CIFAR100

## Next Checks

1. **Adapter robustness test**: Evaluate EvoCL with corrupted or reduced feature memory (vary N from 1 to 100 per class) to determine minimum requirements for effective past loss approximation

2. **Evolution strategy ablation**: Systematically vary population size (10-500) and iterations per task to map the compute-accuracy Pareto frontier on FashionMNIST 5-task split

3. **Latent space analysis**: Compute adapter MSE loss versus final average accuracy across tasks to quantify the relationship between transformation quality and performance preservation