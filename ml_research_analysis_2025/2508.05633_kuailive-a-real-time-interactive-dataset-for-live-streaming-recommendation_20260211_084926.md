---
ver: rpa2
title: 'KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation'
arxiv_id: '2508.05633'
source_url: https://arxiv.org/abs/2508.05633
tags:
- live
- streaming
- recommendation
- user
- kuailive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KuaiLive is the first large-scale real-time interactive dataset
  for live streaming recommendation, containing 23,772 users, 452,621 streamers, and
  11,613,708 live rooms over a 21-day period. It captures rich interaction types (click,
  comment, like, gift) with precise timestamps and includes comprehensive side information
  for users, streamers, and rooms.
---

# KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation

## Quick Facts
- arXiv ID: 2508.05633
- Source URL: https://arxiv.org/abs/2508.05633
- Reference count: 40
- First large-scale real-time interactive dataset for live streaming recommendation with lifecycle information

## Executive Summary
KuaiLive is the first large-scale real-time interactive dataset designed specifically for live streaming recommendation. It contains 23,772 users, 452,621 streamers, and 11,613,708 live rooms over a 21-day period, capturing rich interaction types (click, comment, like, gift) with precise timestamps. The dataset addresses limitations of existing public datasets by including live room lifecycle information and diverse user behaviors, enabling more realistic simulation of dynamic candidate items and better modeling of user and streamer behaviors.

## Method Summary
The dataset provides comprehensive interaction logs with precise timestamps, user and streamer attributes, and live room lifecycle information (start/end timestamps). Researchers can use the ReChorus library to implement various recommendation models including sequential approaches (SASRec, TiSASRec) and traditional collaborative filtering (LightGCN), along with CTR prediction models (Wide&Deep, DeepFM). The evaluation uses leave-one-out for top-K recommendation and temporal splits for CTR prediction, with negative sampling constrained to active candidates at each interaction time.

## Key Results
- Sequential and time-aware recommendation methods (SASRec, TiSASRec) outperform traditional collaborative filtering approaches
- Top 10 streamers account for 1.5% of total interactions, revealing severe long-tail popularity distribution
- User engagement shows high repeat consumption with 27.1% of users interacting with the same streamers multiple times
- CTR prediction faces challenges due to sparse and dynamic nature of live room interactions

## Why This Works (Mechanism)

### Mechanism 1: Time-Constrained Candidate Availability
Live streaming recommendation requires strict adherence to item lifecycles, as candidate pools change dynamically. The dataset includes start_timestamp and end_timestamp for each live room, allowing evaluation protocols to filter candidates and ensuring models are only evaluated on rooms that were actually active at prediction time. This prevents data leakage where a model could predict a room that hadn't started yet or had already ended.

### Mechanism 2: Sequential Dependency and Repeat Consumption
Sequential and time-aware models outperform traditional CF because live streaming exhibits high repeat consumption and periodicity. Users frequently return to the same streamers (27.1% of users), creating strong sequential dependencies in interaction logs. Models like SASRec or TiSASRec capture this temporal order and periodicity, whereas static CF methods treat interactions as unordered sets.

### Mechanism 3: Granularity of Engagement Signals
Differentiating interaction types (click vs. gift) exposes limitations of single-objective optimization. The dataset separates low-effort signals (clicks, 4.9M instances) from high-cost signals (gifts, 72k instances). This skew allows researchers to test if models can predict high-value sparse behaviors rather than just optimizing for clicks.

## Foundational Learning

- **Cold-Start in Ephemeral Items**: Unlike static video libraries, live rooms are new every session. The paper shows massive performance drop when recommending "Rooms" vs. "Streamers" because new rooms have no interaction history. *Quick check*: How does your model handle an item ID it has never seen during training?

- **Time-Aware Negative Sampling**: Standard random negative sampling is flawed here. If you sample a "negative" room that wasn't live when the user was online, it's not a true negativeâ€”it was just unavailable. *Quick check*: Are your negative samples drawn from the global item pool or the pool of items active at that specific timestamp?

- **Multi-Task Learning (MTL)**: User intent is fractured. A user might click for entertainment (Consumption) but gift for support (Contribution). Optimizing for CTR alone might miss high-value gifters. *Quick check*: Is your loss function weighted equally for clicks (dense) and gifts (sparse)?

## Architecture Onboarding

- **Component map**: Input (User Logs, Item Logs, Features) -> Core Processor (Sequential Encoder + Feature MLP) -> Heads (CTR Prediction, Watch Time, Gift Price)

- **Critical path**:
  1. Filter by Lifecycle: Discard training examples where interaction time falls outside room's [start, end] window
  2. Sequence Building: Construct user history sequences ordered by timestamp
  3. Negative Sampling: For each positive interaction, sample negatives only from rooms/streamers active at that moment

- **Design tradeoffs**:
  - Item ID Granularity: Recommending by Streamer_ID yields higher accuracy but risks recommending offline streamers; recommending by Room_ID ensures availability but suffers extreme cold-start
  - Data Sparsity: Filtering for "5-core" cleans noise but destroys long-tail realism (70% of streamers have <5 interactions)

- **Failure signatures**:
  - Random Performance on Rooms: Sequential model performs barely better than random on "Room" task (expected due to transience)
  - High LogLoss on Gifts: Model predicts 0.5 probability for everyone on Gift task (failed to distinguish sparse signal from noise)

- **First 3 experiments**:
  1. Reproduce the ID Gap: Train SASRec on both "Streamer" and "Room" tasks to quantify performance drop from item transience
  2. Temporal Integrity Check: Compare performance of model using random vs. time-constrained negative sampling
  3. Feature Ablation: Run CTR prediction with ID-features only vs. ID + Side Features to measure metadata information gain

## Open Questions the Paper Calls Out

- **End-to-end Generative Recommendation**: How can generative models leverage precise lifecycle timestamps to manage dynamically changing candidate pools in real-time? This requires new architectural approaches as existing paradigms rely on static item corpora.

- **Fairness-aware Recommendation**: What strategies can mitigate extreme long-tail distribution to ensure equitable exposure for the 70% of streamers with fewer than five interactions? Standard accuracy-focused models ignore this ecosystem sustainability problem.

- **Sequential Models for Transient Rooms**: How can sequential recommendation models capture preferences when recommending ephemeral live rooms rather than stable streamers? Current models rely on repeated item IDs, but live rooms are unique sessions.

## Limitations

- 21-day window may not capture sufficient temporal variability for long-term recommendation patterns
- Cold-start challenges are severe, with 70% of streamers having fewer than 5 interactions
- Sparse gift interactions (1.5% of clicks) make commercial intent prediction challenging

## Confidence

- **High Confidence**: Time-constrained candidate availability and sequential dependency claims are well-supported by dataset design and empirical results
- **Medium Confidence**: Multi-behavior optimization benefits are demonstrated but could benefit from more diverse model architectures
- **Low Confidence**: Long-term generalizability beyond 21-day window and cold-start solution effectiveness

## Next Checks

1. Test model performance on extended time windows to assess temporal generalization limits
2. Evaluate cold-start strategies using only side information for new streamers
3. Compare multi-task optimization against single-task models with weighted loss functions