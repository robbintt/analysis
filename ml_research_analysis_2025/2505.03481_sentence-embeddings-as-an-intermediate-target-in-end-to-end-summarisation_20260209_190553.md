---
ver: rpa2
title: Sentence Embeddings as an intermediate target in end-to-end summarisation
arxiv_id: '2505.03481'
source_url: https://arxiv.org/abs/2505.03481
tags:
- sentence
- summarisation
- input
- abstractive
- extractive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel end-to-end summarisation approach
  for generating short unique selling point summaries from user hotel reviews. The
  method combines extractive and abstractive techniques: first selecting salient sentences
  using pre-trained sentence embeddings and angle similarity, then generating the
  final summary with a Transformer model.'
---

# Sentence Embeddings as an intermediate target in end-to-end summarisation

## Quick Facts
- **arXiv ID:** 2505.03481
- **Source URL:** https://arxiv.org/abs/2505.03481
- **Reference count:** 11
- **Primary result:** Novel two-stage end-to-end summarization system combining extractive and abstractive techniques achieves 0.0225 BLEU, 0.1479 ROUGE-L, 0.0602 METEOR, and 0.5115 cosine similarity on USEG dataset.

## Executive Summary
This paper introduces a novel end-to-end summarisation approach for generating short unique selling point summaries from user hotel reviews. The method combines extractive and abstractive techniques: first selecting salient sentences using pre-trained sentence embeddings and angle similarity, then generating the final summary with a Transformer model. The system outperforms existing methods on the USEG dataset, achieving 0.0225 BLEU, 0.1479 ROUGE-L, 0.0602 METEOR, and 0.5115 cosine similarity scores. The approach is particularly effective for large, unstructured inputs with low word overlap between source and target texts, demonstrating the value of sentence embeddings for semantic comparison and content selection.

## Method Summary
The proposed USESUM architecture employs a two-stage approach. First, an extractive model encodes review sentences using Universal Sentence Encoder (USE) and processes them with a bi-directional LSTM. A decoder predicts a target summary embedding vector, and the top 3 sentences with the smallest angular distance to this prediction are selected. Second, an abstractive Transformer model (built on OpenNMT) takes these 3 sentences plus linguistic features (POS, NER, dependency tags) to generate the final summary. The system uses Focal Loss with γ=2 and custom beam search that penalizes hallucinated entities (factor 50) while promoting source entities (factor 0.4).

## Key Results
- Achieved 0.5115 cosine similarity score, outperforming baseline models on semantic matching
- Demonstrated effectiveness for low word overlap corpora (only 1/3 of summaries overlap by words, but 2/3 overlap by embedding similarity)
- Custom beam search modifications reduced entity hallucination while improving factual consistency

## Why This Works (Mechanism)

### Mechanism 1: Semantic Matching for Low-Overlap Corpora
Predicting sentence embeddings as an intermediate target outperforms probability-based selection when source and target texts share low word overlap but high semantic similarity. Instead of selecting sentences based on token overlap (e.g., ROUGE heuristics), the model maps inputs and the target to a shared embedding space (Universal Sentence Encoder). It minimizes the angular distance between the predicted summary vector and the target vector, allowing the system to identify semantically equivalent content that uses different vocabulary. The geometric proximity in the Universal Sentence Encoder space corresponds linearly to semantic "information gain" relevant to the summary.

### Mechanism 2: Iterative Vector Refinement (Extractive Stage)
Iteratively updating a predicted summary vector based on input sentence relevance mitigates information redundancy in large inputs. The extractive model uses a Bi-LSTM to process sentences. At each step, it predicts a summary vector ŷ. It calculates a score based on the reduction in angle to the target if the current sentence embedding xᵢ were added (an MMR-inspired approach). This effectively filters out repetitive reviews that don't add new vector "direction" toward the target. The scoring function accurately captures the marginal utility of a sentence relative to the aggregate document vector ω.

### Mechanism 3: Constrained Beam Search for Factual Grounding
Modifying beam search probabilities to penalize hallucinated entities and promote source entities improves factual consistency. During the abstractive Transformer phase, the beam score is manually adjusted. If the generated token is a Named Entity not found in the source input, its score is penalized (factor of 50). If it matches the source, it is promoted (factor of 0.4). Named Entity Recognition (NER) is accurate for both source and target, and the optimal summary strictly requires entities from the source text (no external knowledge).

## Foundational Learning

- **Concept: Universal Sentence Encoder (USE)**
  - **Why needed here:** The entire extractive logic relies on fixed vector representations. Without understanding that USE creates a semantic space where "great location" and "central positioning" are close, the angle-similarity loss function makes no sense.
  - **Quick check question:** If two sentences have 0% word overlap but mean the same thing, will their cosine similarity in USE space be high or low?

- **Concept: Maximal Marginal Relevance (MMR)**
  - **Why needed here:** The paper adapts MMR from information retrieval to vector spaces. You must understand MMR to grasp why the model calculates "relative gain" (angle(y, ŷᵢ) - angle(y, ŷᵢ₋₁)) rather than just raw similarity.
  - **Quick check question:** Does the model select sentences that are most similar to the target, or sentences that reduce the *remaining* error to the target the most?

- **Concept: Copy Attention / Pointer Networks**
  - **Why needed here:** The abstractive Transformer uses copy attention. This is critical for the low-overlap problem, as it allows the model to copy rare words (like specific hotel amenities) directly from the input reviews into the summary.
  - **Quick check question:** How does the copy mechanism help when the source text contains a unique proper noun (e.g., "Trivago") that isn't in the standard vocabulary?

## Architecture Onboarding

- **Component map:** Large document (merged reviews) -> Universal Sentence Encoder -> Sequence of vectors -> Bi-LSTM (extractive core) -> Decoder predicts target vector ŷ -> Selector calculates angle between ŷ and all input vectors -> Top 3 sentences selected -> Transformer (abstractive core) takes 3 sentences + linguistic features -> Generated summary -> Beam Search with entity penalty/promotion

- **Critical path:** The transition from the continuous vector prediction back to discrete text selection. If the predicted vector ŷ is noisy, the top-3 sentences retrieved will be irrelevant, and the Abstractive Core will generate garbage regardless of its quality.

- **Design tradeoffs:**
  - **RNN vs. Transformer for Extraction:** The authors chose Bi-LSTM over Transformer for the extractive phase due to GPU memory constraints with very long inputs (up to 800 sentences). *Assumption:* This limits long-range dependency modeling compared to a full Transformer but makes the problem tractable.
  - **Fixed vs. Fine-tuned Embeddings:** USE is kept fixed. This reduces training complexity but might limit domain-specific nuance (e.g., specific hotel jargon).

- **Failure signatures:**
  - **"Averaging" Behavior:** If the extractive model converges to a vector average ω, the summary becomes generic (e.g., "Good hotel"). Check cosine similarity vs. distinctiveness.
  - **Entity Hallucination:** If the beam search penalties are off, the model may generate plausible but factually incorrect amenities (e.g., "Free Wi-Fi" when not mentioned).
  - **Selection Mismatch:** If BLEU is low but Cosine Similarity is high, the model likely captured the "vibe" but used completely different vocabulary than the reference.

- **First 3 experiments:**
  1. **Ablation on Intermediate Target:** Compare predicting the embedding vector ŷ vs. predicting a probability distribution over sentences (standard pointer network) to validate the "intermediate target" hypothesis.
  2. **Sentence Count Sensitivity:** Vary the number of extracted sentences (Top-1, Top-3, Top-5) fed into the abstractive model to find the information saturation point.
  3. **Beam Search Calibration:** Grid search the penalty (50) and promotion (0.4) factors to see if the manually estimated values are optimal or if the system is over-constrained.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does replacing the LSTM decoder in the extractive model with a Universal Transformer yield significant performance gains?
- Basis in paper: [explicit] Section 8 states the authors wish to "adapt the universal transformer as a decoder" because the Transformer decoder in BERTSUM outperformed the RNN decoder.
- Why unresolved: The current architecture uses an LSTM decoder, chosen initially to manage GPU memory constraints with very long inputs.
- What evidence would resolve it: A comparative evaluation on the USEG dataset showing ROUGE and semantic similarity scores for a Transformer-based extractive decoder versus the current LSTM implementation.

### Open Question 2
- Question: Can the USESUM architecture be generalized to structured news corpora like CNN/Daily Mail?
- Basis in paper: [explicit] Section 8 lists "adaption and evaluation for the CNN/Daily Mail newswire corpora" as a future extension.
- Why unresolved: The model is currently optimized for unstructured reviews; adapting for datasets with document structure would require architectural changes to generate coherent multi-sentence summaries.
- What evidence would resolve it: Successful application of the model to the CNN/Daily Mail task without the specific constraints of the USEG dataset (e.g., single-sentence limits).

### Open Question 3
- Question: Can the system be adapted for multilingual summarization using language-agnostic embeddings?
- Basis in paper: [explicit] Section 8 suggests adapting the model for "multi-lingual summarisation" using embeddings like Facebook's LASER.
- Why unresolved: The current implementation relies on the English-specific Universal Sentence Encoder; cross-lingual transfer requires validating that the angle-similarity selection logic holds for language-agnostic vector spaces.
- What evidence would resolve it: Performance benchmarks on a non-English review dataset where the encoder is swapped for a multilingual alternative.

### Open Question 4
- Question: How can the cosine similarity metric be refined to avoid rewarding models that generate "safe" averaged predictions?
- Basis in paper: [inferred] Section 6.5 notes that cosine similarity assigns higher scores to models that "excel in averaging predictions" (like BOTTOM-UP), potentially misleading the evaluation of semantic precision.
- Why unresolved: The metric favors general semantic proximity over specific, accurate detail extraction, failing to penalize generic outputs.
- What evidence would resolve it: A modified evaluation metric that penalizes semantic genericness and correlates more strongly with human judgments of semantic specificity.

## Limitations

- **Architectural Precision Gap**: The paper references a "standard Transformer configuration" but does not specify exact hyperparameters (layers, heads, hidden dimensions), making exact replication uncertain.
- **Loss Function Interpretation**: The iterative vector update rule in the extractive stage uses a score based on "relative gain" in angle reduction, but the exact numerical stability and scaling during training is not detailed.
- **Domain Generalization**: The model is evaluated only on hotel review USP summaries; performance on other domains with different writing styles or semantic structures is untested.

## Confidence

- **High Confidence**: The core hypothesis that predicting sentence embeddings as an intermediate target improves semantic matching in low-overlap corpora is well-supported by the quantitative results (0.5115 cosine similarity, outperforming baselines).
- **Medium Confidence**: The effectiveness of the iterative vector refinement mechanism is plausible given the reported metrics, but the lack of ablation studies on the exact scoring function introduces uncertainty.
- **Medium Confidence**: The custom beam search modifications (entity penalties) show measurable impact in Table 5, but the optimal values are not derived systematically.

## Next Checks

1. **Ablation on Intermediate Target**: Train a baseline pointer-network model that selects sentences based on probability scores instead of predicting a target embedding vector. Compare BLEU, ROUGE-L, METEOR, and Cosine Similarity to validate the "intermediate target" hypothesis.

2. **Beam Search Factor Sweep**: Perform a grid search over the entity penalty (e.g., 10, 25, 50, 100) and promotion (e.g., 0.1, 0.4, 0.8, 1.0) factors. Identify if the manually chosen values (50.0 / 0.4) are optimal or if the system is over-constrained.

3. **Long-Document Stress Test**: Evaluate the model on documents longer than those in the USEG dataset (e.g., 1000+ sentences). Measure performance degradation and memory usage to assess the claimed GPU constraints and the effectiveness of the LSTM-based extractive stage.