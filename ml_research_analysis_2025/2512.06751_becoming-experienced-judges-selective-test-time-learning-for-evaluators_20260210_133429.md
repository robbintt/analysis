---
ver: rpa2
title: 'Becoming Experienced Judges: Selective Test-Time Learning for Evaluators'
arxiv_id: '2512.06751'
source_url: https://arxiv.org/abs/2512.06751
tags:
- evaluation
- prompt
- answer
- evaluators
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Evaluators can improve during testing by maintaining and refining
  a meta-prompt that generates sample-specific evaluation instructions. The proposed
  framework, Learning While Evaluating (LWE), enables sequential updates using self-generated
  feedback, while SelectiveLWE focuses updates only on inconsistent cases to reduce
  computation.
---

# Becoming Experienced Judges: Selective Test-Time Learning for Evaluators

## Quick Facts
- **arXiv ID**: 2512.06751
- **Source URL**: https://arxiv.org/abs/2512.06751
- **Reference count**: 40
- **Primary result**: SelectiveLWE improves LLM evaluator accuracy and consistency on pairwise comparison tasks while reducing inference cost by 3.9× through selective meta-prompt updates.

## Executive Summary
This paper introduces a framework for improving LLM evaluators during testing by maintaining and refining a meta-prompt that generates sample-specific evaluation instructions. The approach, Learning While Evaluating (LWE), enables sequential updates using self-generated feedback across test cases. SelectiveLWE builds on this by focusing updates only on inconsistent cases identified through order-swap disagreement, reducing computation while maintaining performance. On two pairwise comparison benchmarks, SelectiveLWE achieves higher accuracy, consistency, and pair accuracy than strong baselines.

## Method Summary
The framework maintains an evolving meta-prompt that produces sample-specific evaluation instructions and refines itself through self-generated feedback. For each test case, the meta-prompt generates a customized evaluation prompt, the model produces a judgment, and self-feedback refines the meta-prompt. SelectiveLWE adds a selection gate that identifies inconsistent cases through order-swap disagreement (where A-vs-B and B-vs-A judgments differ), updating only these cases. Feedback from multiple samples is batched and periodically summarized when the meta-prompt exceeds a length threshold.

## Key Results
- SelectiveLWE achieves higher accuracy, consistency, and pair accuracy than strong baselines on two pairwise comparison benchmarks
- Reduces inference cost by 3.9× compared to full sequential updates
- Demonstrates that evaluators learn most from cases they struggle with (inconsistent cases)
- Shows the importance of batch size selection, with b=4 providing optimal balance

## Why This Works (Mechanism)

### Mechanism 1: Meta-Prompt as Accumulating Evaluation Knowledge
Maintaining an evolving meta-prompt that generates sample-specific evaluation instructions enables evaluators to improve across test cases without training. A meta-prompt M serves as a persistent repository of evaluation insights, generating customized evaluation prompts Pt for each test case and refining itself through self-generated feedback. This assumes the base LLM can produce meaningful self-feedback and domains have transferable evaluation heuristics. Evidence shows this sequential process captures transferable evaluation heuristics across the test set. Break condition: If the base model cannot reliably generate actionable self-feedback, the meta-prompt degrades.

### Mechanism 2: Inconsistency as a Selection Signal
Order-swap inconsistency serves as a test-time accessible proxy for evaluator uncertainty, identifying cases that benefit most from meta-prompt updates. By running two vanilla passes with swapped response order and flagging disagreements, the framework identifies "inconsistent" cases for updates while skipping consistent ones. This assumes position bias correlates with genuine uncertainty. Evidence shows inconsistent cases are harder and benefit more from refined evaluation criteria. Break condition: If consistent-but-wrong cases dominate the error distribution, the selection signal misses valuable learning opportunities.

### Mechanism 3: Batched Self-Refinement with Summarization
Batching feedback across multiple samples (b=4) and periodically summarizing the meta-prompt prevents overfitting to individual cases while managing context length. Feedback from b samples is collected before meta-prompt update, with summarization triggered when exceeding 10,000 characters. This assumes transferable principles emerge from multiple examples while single-example updates cause instability. Evidence shows updating after every sample incurs highest computational cost, while overly large batches degrade performance. Break condition: If summarization loses critical heuristics or batch size is misconfigured, performance degrades.

## Foundational Learning

- **LLM-as-a-Judge Paradigm**: Why needed: The framework assumes the model can serve as an evaluator, following instructions to compare responses and output structured judgments. Quick check: Can you explain why treating an LLM as a judge requires handling position bias differently than using it for generation tasks?

- **Position Bias in Pairwise Comparisons**: Why needed: SelectiveLWE exploits position bias as its selection signal. Understanding this bias is prerequisite to understanding why the mechanism works. Quick check: If a model always prefers the first-presented response regardless of quality, what does this reveal about its internal evaluation criteria?

- **Test-Time Scaling vs. Sequential Learning**: Why needed: Prior test-time scaling operates per-instance, while LWE accumulates experience across samples—this distinction is essential. Quick check: What is the difference between allocating more compute to reason about a single sample versus maintaining state across multiple samples?

## Architecture Onboarding

- **Component map**: Test Cases (x₁...xₜ) → [Selection Gate] → vanilla A vs B → vanilla B vs A → consistent? → skip ↓ (inconsistent only) → [BuildEvalPrompt] ← Meta-Prompt (Mₜ₋₁) → Sample-Specific Prompt (Pₜ) → [Judge] → Judgment (yₜ) → [Feedback] → Self-Feedback (fₜ) → [Buffer] → collect b samples → [RefineMetaPrompt] → Meta-Prompt (Mₜ) → (if |M| > threshold) → [Summarize] → Condensed Mₜ

- **Critical path**: BuildEvalPrompt → Judge → Feedback → (batch accumulation) → RefineMetaPrompt. The selection gate in SelectiveLWE determines which samples enter this pipeline.

- **Design tradeoffs**: Batch size (b): Small batches = more responsive but costly; large batches = stable but slow adaptation. Selection threshold: Only inconsistency currently used; could incorporate confidence scores but would require calibration. Summarization: Risk of losing nuances vs. context explosion.

- **Failure signatures**: Weak base model produces invalid evaluation prompts under meta-prompt updates. High consistency but low accuracy: selection gate filters too aggressively. Meta-prompt degradation over time: summarization or feedback quality issues.

- **First 3 experiments**:
  1. **Ablation on selection signal**: Compare SelectiveLWE vs. random selection vs. uniform updates on same benchmark to isolate the inconsistency signal's contribution.
  2. **Batch size sensitivity**: Run b ∈ {1, 2, 4, 8, 16} on validation subset; plot accuracy vs. inference cost to find Pareto frontier.
  3. **Cross-benchmark transfer**: Initialize meta-prompt on benchmark A, evaluate on benchmark B without updates—tests whether learned heuristics generalize or overfit.

## Open Questions the Paper Calls Out

- **Can evaluators identify and learn from "consistent but wrong" judgments without access to ground-truth labels?**: The authors explicitly state this remains indistinguishable and leaves this for future investigation. The current framework relies on order-swap inconsistency, inherently bypassing cases where the model is confidently incorrect.

- **Is there a minimum capability threshold for base models to effectively utilize test-time meta-prompt evolution?**: The paper notes the method is "less effective for relatively weak models," specifically citing failures with Qwen3-VL-235B-A22B-Instruct. The study primarily validates on strong frontier models, leaving performance scaling laws for smaller models undefined.

- **How can the selective update mechanism be adapted for direct assessment tasks that lack pairwise comparison signals?**: While the study focuses on pairwise comparisons, the authors conclude the LWE framework "can extend to direct assessment." The efficiency depends on the "order-swap" inconsistency signal, which is structurally absent in single-response evaluation tasks.

## Limitations

- The framework's effectiveness hinges on the base LLM's ability to generate high-quality self-feedback, with medium confidence for stronger models but low confidence for weaker models.
- The selection signal (order-swap inconsistency) may miss consistent-but-incorrect judgments, creating a potential blind spot.
- Cross-domain generalization remains untested beyond the two studied benchmarks.

## Confidence

- **Base model capability**: Medium confidence for stronger models (e.g., GPT-4o), Low confidence for weaker models
- **Inconsistency signal**: Medium confidence that inconsistency captures meaningful uncertainty
- **Meta-prompt management**: High confidence that batching and summarization are necessary, Low confidence on optimal configuration
- **Cross-domain generalization**: Low confidence without further testing

## Next Checks

1. **Ablation on selection signal**: Compare SelectiveLWE vs. random selection vs. uniform updates on the same benchmark to isolate the inconsistency signal's contribution.

2. **Batch size sensitivity**: Run b ∈ {1, 2, 4, 8, 16} on a validation subset; plot accuracy vs. inference cost to find the Pareto frontier for your domain.

3. **Cross-benchmark transfer**: Initialize meta-prompt on benchmark A, evaluate on benchmark B without updates—tests whether learned heuristics generalize or overfit.