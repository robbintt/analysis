---
ver: rpa2
title: 'Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check'
arxiv_id: '2509.11629'
source_url: https://arxiv.org/abs/2509.11629
tags:
- safety
- answer
- harmful
- resa-sft
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Reasoned Safety Alignment (ReSA), a method
  that enhances LLM safety against jailbreak attacks by applying an "Answer-Then-Check"
  strategy. The approach first generates a summarized intended answer, then critically
  evaluates its safety before producing the final response.
---

# Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check

## Quick Facts
- arXiv ID: 2509.11629
- Source URL: https://arxiv.org/abs/2509.11629
- Authors: Chentao Cao; Xiaojun Xu; Bo Han; Hang Li
- Reference count: 40
- This paper proposes Reasoned Safety Alignment (ReSA), a method that enhances LLM safety against jailbreak attacks by applying an "Answer-Then-Check" strategy. The approach first generates a summarized intended answer, then critically evaluates its safety before producing the final response. The authors construct an 80K-example ReSA dataset containing prompts and safety analyses in this style. Experimental results show that ReSA-SFT models achieve superior safety performance while maintaining general capabilities and decreasing over-refusal rates. Notably, training on just 500 examples achieves comparable performance to using the full dataset. The method also enables safe completion, allowing helpful responses to sensitive queries like self-harm rather than simple refusal. ReSA-SFT outperforms 13 baselines and achieves the Pareto frontier with strong safety capability while maintaining reasoning performance on benchmarks like MMLU, MATH500, and HumanEval.

## Executive Summary
This paper introduces Reasoned Safety Alignment (ReSA), a novel approach to enhancing LLM safety against jailbreak attacks. The method employs an "Answer-Then-Check" strategy where the model first generates a summarized intended answer, then critically evaluates its safety before producing the final response. The authors construct an 80K-example ReSA dataset containing prompts and safety analyses in this style. Experimental results demonstrate that ReSA-SFT models achieve superior safety performance while maintaining general capabilities and decreasing over-refusal rates. Notably, training on just 500 examples achieves comparable performance to using the full dataset. The method also enables safe completion, allowing helpful responses to sensitive queries like self-harm rather than simple refusal. ReSA-SFT outperforms 13 baselines and achieves the Pareto frontier with strong safety capability while maintaining reasoning performance on benchmarks like MMLU, MATH500, and HumanEval.

## Method Summary
The ReSA method enhances LLM safety through a novel "Answer-Then-Check" approach. First, the model generates a summarized intended answer (1-5 sentences) to the prompt. Second, it performs a critical safety analysis evaluating whether the intended answer is safe, dangerous, or harmless. Third, it produces a final answer or refusal based on this safety assessment. The authors construct an 80K-example ReSA dataset by generating synthetic training data using multiple models: WILDJAILBREAK base data augmented with adversarial queries from PAIR, GPTFuzzer, and PAP; answer generation using Dolphin-2.9.2-Qwen2-72B for harmful and Qwen2.5-72B-Instruct for benign; safety analysis using Llama3.3-70B-Instruct; and classification using Llama-Guard-3-8B. The dataset includes vanilla, adversarial, benign, and harmful queries, plus safe completion examples for self-harm scenarios. SFT training uses TRL with hyperparameters including 2 epochs, AdamW optimizer, cosine learning rate with 5×10⁻⁶ initial value, 10% warmup, max sequence length 8192, and 8×H100 GPUs.

## Key Results
- ReSA-SFT achieves superior safety performance against jailbreak attacks while maintaining general capabilities and reducing over-refusal rates
- Training on just 500 examples achieves comparable safety performance to using the full 80K dataset
- ReSA-SFT outperforms 13 baselines and achieves the Pareto frontier with strong safety capability while maintaining reasoning performance on benchmarks like MMLU, MATH500, and HumanEval
- The method enables safe completion, allowing helpful responses to sensitive queries like self-harm rather than simple refusal

## Why This Works (Mechanism)
The "Answer-Then-Check" strategy works by decoupling the generation of an intended response from the safety evaluation process. By first generating a summarized intended answer without immediate safety constraints, the model can better understand the user's underlying intent. The subsequent safety analysis step then critically evaluates this intended answer against safety policies before deciding on the final response. This separation prevents the model from prematurely filtering out potentially helpful content or being overly cautious. The safety analysis component is trained to identify specific unsafe types (violence, hate, self-harm, etc.) with their definitions, enabling more nuanced decision-making than binary safe/unsafe classification. The reasoning template structure ensures consistency between the intended answer and final decision, reducing internal contradictions.

## Foundational Learning

**LLM Safety Alignment** - Why needed: Prevents models from generating harmful content while maintaining helpfulness. Quick check: Model can handle jailbreak attempts while responding appropriately to benign queries.

**SFT (Supervised Fine-Tuning)** - Why needed: Adapts base models to specific tasks using curated datasets. Quick check: Training converges and produces desired output format.

**Adversarial Query Generation** - Why needed: Creates challenging test cases that probe model safety boundaries. Quick check: Generated queries successfully jailbreak baseline models.

**Safety Classification** - Why needed: Enables automated identification of harmful content categories. Quick check: Llama-Guard-3-8B correctly classifies diverse harmful prompts.

**Dataset Balancing** - Why needed: Prevents model bias toward over-refusal or under-refusal. Quick check: Equal representation of benign and harmful examples in training.

## Architecture Onboarding

**Component Map:** Base Model -> SFT Trainer -> ReSA Dataset -> Reasoning Template -> Safety Analysis Module -> Final Output

**Critical Path:** Prompt → Intended Answer Generation → Safety Analysis → Final Response Decision

**Design Tradeoffs:** The separation of answer generation from safety analysis creates more robust safety boundaries but requires additional computation and training complexity. Using synthetic data generation enables large-scale dataset creation but introduces potential distribution shift concerns.

**Failure Signatures:** Over-refusal on benign queries indicates insufficient adversarial benign examples; successful jailbreaks suggest inadequate safety analysis training; inconsistent safety analyses reveal template or training issues.

**First Experiments:** 1) Verify SFT training converges with 500-sample subset; 2) Test reasoning template with simple benign/harmful pairs; 3) Evaluate safety analysis module on held-out examples.

## Open Questions the Paper Calls Out

**Open Question 1:** Can reinforcement learning techniques make the intended answer summary itself safe and visible to users, rather than requiring it to be hidden? The authors state this as "an important future work" regarding using RL techniques to improve safety rates so "the answer summary can be made safe and visible to the user."

**Open Question 2:** Why does training on only 500 examples achieve safety performance comparable to the full 80K dataset, and what are the minimal sufficient conditions? The paper discovers this phenomenon but does not explain the mechanism, noting that "training on a small subset of just 500 examples can achieve comparable performance... suggesting that safety alignment may require less data than previously assumed."

**Open Question 3:** How robust is ReSA against adaptive adversaries who specifically target the "Answer-Then-Check" reasoning structure? Current evaluations use jailbreak methods developed against standard alignment, but attacks specifically designed to fool the safety analysis component or exploit the reasoning structure remain untested.

**Open Question 4:** Does ReSA maintain effectiveness when scaled to larger models (70B+ parameters) or transferred to different model families? Experiments are limited to Llama3.1-8B and Qwen2.5-7B, and the paper does not verify whether the approach scales or transfers effectively.

## Limitations

- Results depend heavily on quality and representativeness of synthetic training data generation process
- Evaluation focuses primarily on safety metrics with limited analysis of helpfulness or user experience trade-offs
- Safe completion capability for self-harm queries raises questions about handling other sensitive topics
- Paper lacks detailed analysis of reasoning template's impact on different jailbreak attack types

## Confidence

**High Confidence:** The core methodology of separating answer generation from safety analysis is clearly articulated and experimental results demonstrating improved safety metrics over 13 baselines are well-supported.

**Medium Confidence:** Claims about training efficiency (500 examples vs full dataset) and safe completion capabilities are supported by experimental evidence but would benefit from additional validation across different model architectures.

**Low Confidence:** Generalizability to domains beyond tested benchmarks and long-term stability of safety improvements are not sufficiently explored.

## Next Checks

1. **Template Robustness Analysis:** Systematically test the "Answer-Then-Check" template structure against variations in reasoning chain length, different safety policy definitions, and alternative safety analysis frameworks to identify potential vulnerabilities or improvements.

2. **Cross-Domain Generalization:** Evaluate the ReSA-SFT model's safety performance on datasets from different domains (e.g., healthcare, legal, financial) not represented in the training data to assess true generalizability of the safety alignment.

3. **Longitudinal Safety Stability:** Conduct extended testing over multiple weeks/months with continuous adversarial evaluation to measure the stability of safety improvements and identify any gradual degradation or emergence of new failure modes.