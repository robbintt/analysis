---
ver: rpa2
title: 'DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End
  Autonomous Driving'
arxiv_id: '2505.16278'
source_url: https://arxiv.org/abs/2505.16278
tags:
- driving
- vision
- action
- arxiv
- autonomous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DriveMoE addresses the challenges of processing multi-view sensory
  data and handling diverse driving scenarios in end-to-end autonomous driving by
  introducing Mixture-of-Experts (MoE) architectures into both vision and action components.
  The proposed framework dynamically selects relevant camera views based on driving
  context and activates specialized expert modules for different driving behaviors,
  mirroring human driving cognition.
---

# DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving

## Quick Facts
- **arXiv ID:** 2505.16278
- **Source URL:** https://arxiv.org/abs/2505.16278
- **Reference count:** 40
- **Key outcome:** Achieves 22.8% higher Driving Score and 62.1% higher Success Rate on Bench2Drive benchmark compared to baseline.

## Executive Summary
DriveMoE introduces Mixture-of-Experts (MoE) architectures into vision and action components of end-to-end autonomous driving models to address computational efficiency and diverse driving scenario handling. The framework dynamically selects relevant camera views and activates specialized expert modules for different driving behaviors, achieving state-of-the-art performance on the Bench2Drive closed-loop benchmark. The approach mirrors human driving cognition by selectively attending to crucial visual cues and employing specialized skills for different maneuvers.

## Method Summary
DriveMoE extends the Drive-π0 architecture with two MoE components: a Scene-Specialized Vision MoE for dynamic camera selection and a Skill-Specialized Action MoE for trajectory generation. The vision router selects top-k camera views based on driving context, while the action router activates specialized expert modules for different driving behaviors. The model uses a two-stage training approach: Stage 1 trains routers with ground-truth expert labels for stability, then Stage 2 transitions to autonomous routing. The system processes 2 sequential front-view images, 1 dynamic camera view, and vehicle state through PaliGemma VLM backbone with flow-matching transformer decoder.

## Key Results
- **22.8% improvement** in Driving Score on Bench2Drive benchmark
- **62.1% improvement** in Success Rate compared to baseline
- **88.85% vision router accuracy** and **65.40% action router accuracy** in expert selection

## Why This Works (Mechanism)

### Mechanism 1
Dynamically selecting camera views based on driving context reduces visual token redundancy. A lightweight vision router takes front-view embedding and goal waypoint as input, outputs probability distribution over N camera views, and selects only top-k views for processing. Router trained with cross-entropy loss using manually designed annotation filters.

### Mechanism 2
Replacing dense feed-forward networks with skill-specific expert modules prevents mode averaging across diverse driving behaviors. Each decoder layer contains K shared experts and M non-shared experts with sparse activation selecting only top-k experts per input. Expert outputs are combined weighted by router confidence with flow-matching loss ensuring trajectory accuracy.

### Mechanism 3
Two-stage training progression stabilizes MoE learning. Stage 1 (10 epochs) forces MoEs to select ground-truth experts while jointly training routers to prevent early instability. Stage 2 (5 epochs) switches to router-based selection to build robustness to potential router mistakes under realistic inference conditions.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) with Sparse Gating**
  - Why needed here: DriveMoE's core innovation applies MoE to both perception and planning; understanding load balancing and expert collapse is prerequisite.
  - Quick check question: Can you explain why adding more experts can decrease performance (Table 4: 44 experts → lower DS than 6 experts)?

- **Concept: Flow Matching for Continuous Action Generation**
  - Why needed here: Action MoE is built on flow-matching planner (from π0), not discrete token prediction; differs from LLM-based VLAs.
  - Quick check question: How does flow matching differ from diffusion policy for trajectory generation, and why might it preserve multimodality better?

- **Concept: Closed-Loop vs Open-Loop Evaluation in Autonomous Driving**
  - Why needed here: Paper explicitly states open-loop metrics mainly indicate convergence while closed-loop metrics assess true driving performance.
  - Quick check question: Why does low L2 trajectory error not guarantee high Driving Score in closed-loop simulation?

## Architecture Onboarding

- **Component map:**
Multi-Camera Input → Vision Router (front-view + goal) → Top-K Camera Selection → Vision Tower + Projector → PaliGemma VLM Backbone → Action Router (LLM hidden state) → Flow-Matching Transformer Decoder with MoE FFNs → Trajectory (10 waypoints) → PID Controller

- **Critical path:** Vision router accuracy → correct camera selection → VLM receives relevant spatial context → action router receives accurate scene representation → correct expert activation → trajectory quality. Errors compound along this chain.

- **Design tradeoffs:**
  - More experts → finer specialization but higher load-balancing difficulty and routing uncertainty
  - Higher top-k → more robustness to routing errors but less specialization benefit
  - Router noise injection → prevents expert collapse but may reduce routing precision
  - Fixed front-view + 1 dynamic vs. all-dynamic → guarantees velocity estimation capability but limits flexibility

- **Failure signatures:**
  - Expert collapse: Load-balancing loss shows one expert receiving >80% of samples; action router accuracy drops near random
  - Vision router over-reliance on front view: Dynamic view selection collapses to always selecting front-left or front-right
  - Stage 2 performance degradation: Switching from GT routing to autonomous routing causes DS drop >5%
  - Rare maneuver failures: Emergency Brake or Give Way abilities significantly underperform lane following

- **First 3 experiments:**
  1. **Router ablation baseline:** Run Drive-π0 (no MoE) vs. DriveMoE with random routing vs. DriveMoE with GT routing on Bench2Drive validation set to isolate router contribution.
  2. **Expert count sweep:** Train Action MoE with 3, 5, 6, 8, 13 non-shared experts and plot DS vs. expert count to verify 6 is optimal.
  3. **Vision router annotation validation:** Manually inspect 50 test samples where vision router disagrees with GT annotation to identify if errors are router limitations or annotation filter bugs.

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding expert load balancing, action router accuracy, and vision router supervision. Specifically, it notes that effectively achieving load balancing among experts remains a significant challenge as the number of experts grows, with performance degrading when scaling to 44 experts. The action router accuracy of only 65.40% suggests frequent sub-optimal expert selection. Additionally, the reliance on manually designed heuristic filters for vision router supervision limits the model's ability to learn novel attention patterns not defined by human priors.

## Limitations
- **Load balancing challenge:** Expert collapse occurs as the number of experts grows, with performance degrading from 74.22 to 68.22 when increasing from 6 to 44 non-shared experts.
- **Action router uncertainty:** The 65.40% action router accuracy indicates frequent sub-optimal expert selection, potentially relying on shared expert robustness rather than true specialization.
- **Manual supervision dependency:** Vision router labels rely on manually designed heuristic filters based on trajectories, bounding boxes, and maps, limiting learning of novel attention patterns.

## Confidence
- **High Confidence:** The 22.8% DS and 62.1% SR improvements on Bench2Drive are well-supported by closed-loop benchmark results.
- **Medium Confidence:** Vision router effectiveness supported by 88.85% accuracy but limited ablation testing; dynamic camera selection may face break conditions in complex scenarios.
- **Low Confidence:** Action MoE benefits demonstrated through scenario-specific ablations, but 65.40% action router accuracy raises questions about expert selection reliability in novel situations.

## Next Checks
1. **Router Performance Under Distribution Shift:** Test DriveMoE on out-of-distribution scenarios (night driving, adverse weather) to evaluate whether 88.85% vision router and 65.40% action router accuracies hold with unseen conditions.

2. **Expert Specialization Verification:** Conduct t-SNE visualization of expert activation patterns across 44 Bench2Drive scenarios to confirm each expert truly specializes in distinct driving behaviors rather than overlapping significantly.

3. **Stage 2 Robustness Test:** Implement gradual transition from Stage 1 to Stage 2 training (rather than abrupt switch) and measure performance sensitivity to timing of router autonomy to identify optimal transition points for different scenarios.