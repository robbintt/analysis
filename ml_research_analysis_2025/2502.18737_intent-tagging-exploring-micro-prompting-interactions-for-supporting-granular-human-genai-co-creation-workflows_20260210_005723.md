---
ver: rpa2
title: 'Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular
  Human-GenAI Co-Creation Workflows'
arxiv_id: '2502.18737'
source_url: https://arxiv.org/abs/2502.18737
tags:
- slide
- intent
- tags
- users
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Intent tagging is a novel interaction paradigm for granular and
  non-linear human-AI co-creation workflows. The approach uses atomic conceptual units
  (tags) to encapsulate user intent, allowing flexible expression across varying levels
  of ambiguity.
---

# Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAI Co-Creation Workflows

## Quick Facts
- arXiv ID: 2502.18737
- Source URL: https://arxiv.org/abs/2502.18737
- Reference count: 40
- Primary result: Intent tagging significantly improves user control and intention communication in GenAI co-creation workflows compared to chat-based and design gallery-based systems.

## Executive Summary
IntentTagger introduces intent tagging, a micro-prompting interaction technique for granular, non-linear human-AI co-creation workflows. The system enables users to express creative intent through atomic conceptual units (intent tags) that can be freely created and manipulated. A user study (N=12) comparing intent tagging with chat-based and design gallery-based GenAI systems for slide creation showed that participants felt significantly more in control (mean difference = 2.67) and better able to communicate their intentions (mean difference = 1.83) with intent tags. Participants particularly valued the support for non-linear workflows, integrated system suggestions for meta-intent elicitation, and the ability to iteratively refine content.

## Method Summary
The IntentTagger system was implemented using TypeScript, ReactJS, and ReactFlow for the interactive tag canvas. The backend uses OpenAI's GPT-4o API for all generative tasks including tag suggestions, outline creation, slide deck JSON generation, and tag grounding. A Bing Image Search API provides image suggestions. The study employed a within-subjects design with 12 participants who completed three tasks: creating a slide deck using intent tagging, a chat-based system, and a design gallery-based system. Each task involved creating a deck with an outline and 5 slides. User satisfaction, perceived control, and intention communication were measured using 5-point Likert scales.

## Key Results
- Participants felt significantly more in control (mean difference = 2.67) and better able to communicate intentions (mean difference = 1.83) with intent tagging compared to baseline systems
- Users particularly valued support for non-linear workflows, integrated system suggestions, and ability to iteratively refine content
- Participants appreciated the fluid integration of deck-wide and slide-specific edits, though they requested better control over re-generation

## Why This Works (Mechanism)

### Mechanism 1: Granular Intent Decomposition via Intent Tags
- Claim: Breaking down user intent into atomic, manipulatable units (intent tags) improves control and reduces ambiguity in Human-GenAI co-creation.
- Mechanism: Users interact with the system by creating or selecting "Intent Tags" (concept tags like "Topic: Sustainability" or reference tags like external documents). The system compiles these tags into a structured context (e.g., for an outline or slide generation) which is fed to an LLM. This provides the LLM with a structured, explicit, and cumulative representation of user intent, as opposed to a single, potentially ambiguous text prompt. This decomposition allows for fine-grained control and iterative refinement at a low cognitive cost.
- Core assumption: The LLM can interpret and synthesize multiple, independent atomic intent tags into a coherent output more effectively than a single, complex prompt. Users can articulate their intent more easily in discrete, labeled units than in a long-form narrative.
- Evidence anchors:
  - [abstract] "IntentTagger introduces intent tagging, a micro-prompting interaction technique for granular, non-linear human-AI co-creation workflows. The system enables users to express creative intent through atomic conceptual units (intent tags)..."
  - [section 4.1] "Lucy... adds a new concept tag to the 'Narrative' group and enters 'Number of slides' into the tag's upper text field and '10' into the lower... The newly generated outline is now shorter..."
  - [corpus] This aligns with SpecifyUI's approach of a "structured, parameterized, hierarchical intermediate representation" for UI design, suggesting a broader trend in structured intent representation for GenAI control.

### Mechanism 2: Non-Linear Workflow Support via Scoped Manipulation
- Claim: Enabling users to steer generation at multiple scopes (deck, slide, tag level) and seamlessly switch between them supports a more natural and iterative creative process.
- Mechanism: The system provides a "Deck Steering Canvas" for global changes and a "Slide Steering Overlay" for local adjustments. Crucially, changes made at a local level (e.g., to a single slide) can be propagated to the global level (e.g., "Apply to all slides"). This supports a non-linear, iterative design process where users can work from big-picture narrative to fine-grained visual details and back.
- Core assumption: Creative workflows for tasks like slide deck creation are inherently non-linear and benefit from tools that mirror this flexibility. Users desire a bidirectional flow of intent between global and local scopes.
- Evidence anchors:
  - [abstract] "...users particularly valued the system's support for flexible intent expression... and the ability to maintain non-linear workflows while making both deck-wide and slide-specific adjustments."
  - [section 4.2.2] "The Deck Steering Canvas allows users to steer the (re)generation of entire slide decks... Alternatively, the Slide Steering Overlay allows users to steer the (re)generation and explore alternatives of single slides."
  - [section 6.3.2] "Participants appreciated the fluid integration of deck-wide and slide-specific edits... 'I like the ability to go from everything, update everything, zoom in on one, and then experiment on that one. And then again, update all.'" (P02)

### Mechanism 3: Metacognitive Guidance via AI-Suggested Tags
- Claim: Proactively suggesting intent tags helps users clarify their own goals ("meta-intent"), reducing the "blank page" problem and prompting deeper reflection on their design choices.
- Mechanism: Based on the user's existing active tags and the overall context, the system generates suggestions for new tags. This is not just for efficiency but acts as a reflective partner, surfacing options the user may not have considered. This "talk-back" from the system encourages users to refine their mental model of the task, a process linked to "reflection-in-action."
- Core assumption: Users often have an incomplete or unarticulated sense of their own intent. AI-generated suggestions can effectively stimulate this reflection without being overwhelming or distracting.
- Evidence anchors:
  - [abstract] "...integrated suggestions that aided their creative process..."
  - [section 6.3.3] "One of the key benefits highlighted by participants was the system's ability to help them discover and refine their intentions... This process of 'meta-intent' elicitation allowed users to identify important elements that they may have overlooked..."
  - [section 8.2] "When intent tagging, the tags on the canvas talk back to the user... helping them... tackle the cognitively demanding task of clarifying their intent."

## Foundational Learning

- Concept: **Human-Computer Interaction (HCI) design principles (Direct Manipulation, GUI vs. Text)**
  - Why needed here: The core innovation is an interaction technique (intent tagging) that blends text prompting with graphical manipulation. Understanding the trade-offs between free-form text, chat dialogs, and GUI-based wizards is essential to appreciate the design.
  - Quick check question: How does "intent tagging" try to combine the benefits of both open text prompts and structured GUI elements? (Answer: It uses atomic text-based tags that can be freely created but are also treated as manipulatable graphical objects within a structured interface.)

- Concept: **Large Language Model (LLM) Prompt Engineering & Context Management**
  - Why needed here: The system's backend relies on an LLM (GPT-4o). Understanding how to structure a prompt from a set of tags, manage context, and use the LLM to generate structured output (JSON for slides, markdown for outlines) is key to the architecture.
  - Quick check question: How does the system transform a collection of user-selected intent tags into a prompt for the LLM? (Answer: It dynamically constructs a `USER_CONTEXT` string by clustering tags into buckets like Narrative, Visual Style, and Content Sources, then combines this with a static `SYSTEM_PROMPT` for tasks like outline or slide generation.)

- Concept: **Creative Cognition and "Reflection-in-Action"**
  - Why needed here: The paper frames intent tagging within theories of creative design, specifically SchÃ¶n's concept of "reflection-in-action," where a creator iteratively refines their ideas through interaction with their materials. This provides the theoretical basis for why the system is designed to be iterative and non-linear.
  - Quick check question: According to the paper, how does the "intent tagging" interface support the cognitive process of "reflection-in-action"? (Answer: The system's suggestions and the persistent, visible tags act as materials that "talk back" to the user, prompting them to refine and clarify their intentions during the creation process.)

## Architecture Onboarding

- Component map: ReactJS frontend with ReactFlow canvas -> Tag manipulation state -> OpenAI GPT-4o API calls -> Structured output (JSON slides, Markdown outlines) -> React-based slide renderer
- Critical path: User adds/manipulates tags on canvas -> Frontend updates state -> User requests action (e.g., "update outline") -> Backend constructs prompt with relevant tags -> Calls OpenAI API -> Returns result (e.g., markdown outline) -> Frontend renders result
- Design tradeoffs: The system trades the simplicity of a single chat prompt for the control of granular tags, increasing initial complexity. It sacrifices fully manual editing for GenAI-driven generation to focus the study, creating a handoff challenge. The paper notes a key tradeoff in UI customization: giving users control (e.g., creating their own tag groups) adds management overhead and potential confusion.
- Failure signatures: The system can fail if the LLM cannot resolve contradictory tags or ignores key constraints (like slide count), a problem also noted in the chat-based baseline. The system lacks a way to "lock" elements, so re-generation can unexpectedly change aspects a user was satisfied with, which was a major user frustration. The non-linear propagation logic is a complex area prone to unexpected side effects.
- First 3 experiments:
  1.  **Implement Tag Locking:** Add a "pin" feature to individual tags or generated elements (e.g., a specific slide's background color) to prevent them from being altered during subsequent re-generation cycles. This directly addresses the most common user request and tests the feasibility of non-destructive iteration.
  2.  **Test Conflict Resolution:** Systematically present the LLM with conflicting intent tags (e.g., "Number of slides: 5" and "Number of slides: 10") and analyze how the system resolves them in the generated output. This probes a key weakness in the current mechanism.
  3.  **Explore Mixed-Initiative Suggestions:** Modify the system to automatically generate tag suggestions based on the user's interaction patterns or context, rather than requiring an explicit request. This would test the "metacognitive support" mechanism more actively and compare it to the user-driven model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can systems support "locking" or "pinning" specific design elements during non-linear intent tagging workflows to prevent unintended changes in subsequent generation cycles?
- Basis in paper: [explicit] Section 8.1 notes that participants valued flexibility but expressed frustration when satisfied attributes (e.g., fonts) changed unexpectedly, suggesting a need for interfaces that allow users to "gradually lock elements."
- Why unresolved: The current prototype treats all tags as mutable inputs for every generation cycle, lacking a mechanism to anchor specific parameters while exploring others.
- What evidence would resolve it: A study comparing user efficiency and satisfaction in systems with and without "pinning" features, measuring the frequency of regression in design elements.

### Open Question 2
- Question: What mechanisms effectively orchestrate the "hand-off" between intent tag-based steering and direct manual content editing?
- Basis in paper: [explicit] Section 8.3 states that blending steering interactions with manual editing opens questions about identifying situations where steering becomes tedious and how to surface cues for direct editing.
- Why unresolved: The study focused on generation rather than a fully featured editor, leaving the integration of granular micro-prompts with traditional direct manipulation tools unexplored.
- What evidence would resolve it: Usability studies of a hybrid system that logs mode-switching frequency and identifies specific thresholds where users abandon tags for direct manipulation.

### Open Question 3
- Question: How does the intent tagging paradigm generalize to content creation tasks involving temporal or spatial complexity, such as video editing or 3D scene generation?
- Basis in paper: [explicit] Section 7 and the Conclusion suggest intent tagging could apply to domains like video or 3D scenes but acknowledge these are currently only "sketches" requiring future research.
- Why unresolved: The paper evaluated intent tagging solely within the context of slide deck creation; it is unknown if the tag group structure scales to timeline-based or 3D spatial workflows.
- What evidence would resolve it: Implementation of intent tagging in a video editing suite (e.g., using Keyframe Tags) followed by a user study measuring control over temporal dynamics.

## Limitations

- The study's small sample size (N=12) and controlled research context may not represent broader populations or real-world usage patterns
- The system lacks manual editing capabilities, forcing users to rely entirely on GenAI for modifications
- The absence of a locking mechanism caused user frustration when satisfied elements changed unexpectedly during re-generation cycles

## Confidence

**High Confidence:** The user study's quantitative results showing significantly higher control (mean difference = 2.67) and intention communication (mean difference = 1.83) with intent tagging are well-supported by the controlled experiment design and statistical analysis.

**Medium Confidence:** The generalizability of findings to other GenAI co-creation tasks beyond slide creation is plausible but not empirically tested.

**Low Confidence:** The long-term usability and adoption of intent tagging in real-world scenarios remains uncertain, particularly regarding tag management overhead and potential fatigue from managing numerous tags for complex projects.

## Next Checks

1. **Conduct a longitudinal field study** with intent tagging across multiple creative tasks (document writing, code generation, visual design) to assess real-world usability, tag management overhead, and long-term user adoption patterns.

2. **Implement and test a comprehensive locking mechanism** that allows users to preserve specific elements (visual styles, content blocks, layout choices) across regeneration cycles, then measure whether this resolves the primary user frustration around unintended changes.

3. **Systematically evaluate conflict resolution strategies** by creating controlled scenarios with contradictory intent tags and measuring how different approaches (user mediation, LLM prioritization rules, explicit conflict warnings) affect user trust and satisfaction with the system.