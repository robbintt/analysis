---
ver: rpa2
title: 'Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather'
arxiv_id: '2505.12199'
source_url: https://arxiv.org/abs/2505.12199
tags:
- depth
- estimation
- training
- dataset
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ACDepth, a robust monocular depth estimation
  method designed to handle adverse weather conditions. The approach addresses the
  problem of domain shifts and information extraction difficulties under challenging
  conditions such as night and rain.
---

# Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather

## Quick Facts
- arXiv ID: 2505.12199
- Source URL: https://arxiv.org/abs/2505.12199
- Reference count: 23
- Key outcome: ACDepth achieves 2.50% absRel improvement for night scenes and 2.61% absRel for rainy scenes on nuScenes vs. md4all-DD.

## Executive Summary
This paper proposes ACDepth, a robust monocular depth estimation method designed to handle adverse weather conditions such as night and rain. The approach addresses domain shifts and information extraction difficulties by generating high-fidelity synthetic degradation data using a one-step diffusion model with LoRA adapters, and training with a multi-granularity knowledge distillation strategy that incorporates ordinal guidance from Depth Anything V2. Experiments show ACDepth significantly outperforms existing methods on nuScenes and RobotCar datasets, particularly in challenging scenarios involving ground water reflections and nighttime object recognition.

## Method Summary
ACDepth employs a two-stage approach: first, a one-step diffusion model with LoRA adapters generates high-quality synthetic adverse weather data (night, rain) from clear images, constrained by circular consistency loss and adversarial training. Second, a student depth estimation network is trained using multi-granularity knowledge distillation (MKD) from a teacher model, combining standard feature distillation, ordinal guidance distillation from Depth Anything V2, and feature consistency constraints. The method uses ResNet18 backbones and self-supervised training on nuScenes and RobotCar datasets, achieving significant improvements in absRel metrics for adverse weather conditions.

## Key Results
- ACDepth outperforms md4all-DD by 2.50% absRel for night scenes and 2.61% absRel for rainy scenes on nuScenes dataset
- The method shows superior performance in handling complex scenarios like ground water reflections and nighttime object recognition
- ACDepth demonstrates effective domain adaptation through synthetic data generation and multi-granularity knowledge distillation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-fidelity synthetic degradation data improves depth estimation robustness.
- Mechanism: ACDepth uses a one-step diffusion model fine-tuned with LoRA adapters, constrained by circular consistency loss and adversarial training, to generate realistic multi-tuple degradation samples (e.g., rain, night). This creates a high-quality training dataset that simulates adverse weather more realistically than prior GAN-based methods (e.g., md4all's ForkGAN), leading to better domain adaptation for the depth estimation model.
- Core assumption: The generated degradation images are sufficiently realistic and representative of real-world adverse conditions to effectively train the student model without introducing harmful artifacts.
- Evidence anchors:
  - [abstract] "...ACDepth employs a one-step diffusion model with LoRA adapters to generate high-quality multi-tuple degradation datasets..."
  - [section] "Figure 1: (a) Comparison of training data... samples generated by our approach are more realistic, providing a better simulation..."
  - [corpus] ER-LoRA (2509.00665) also explores LoRA for weather-generalized depth estimation, suggesting adapter-based fine-tuning is a valid technique for this domain.
- Break condition: If the diffusion-generated images contain systematic artifacts or lack critical degradation patterns (e.g., ground water reflections are missing), the student model's performance on real-world data will degrade.

### Mechanism 2
- Claim: Multi-granularity knowledge distillation (MKD) ensures comprehensive capability transfer.
- Mechanism: The MKD strategy combines standard distillation loss ($L_d$), ordinal guidance distillation ($L_r$) from a pretrained external model (Depth Anything V2), and a feature consistency constraint ($L_c$). This multi-level approach forces the student network to learn degradation-agnostic scene information from the teacher, rank-based depth priors, and consistent feature representations across clear and degraded inputs.
- Core assumption: The teacher model provides reliable pseudo-labels, and the external pretrained model (Depth Anything V2) offers valid ordinal depth relationships even for unseen adverse-weather inputs.
- Evidence anchors:
  - [abstract] "...multi-granularity knowledge distillation strategy (MKD) that incorporates ordinal guidance distillation from Depth Anything V2."
  - [section] "To achieve full transfer and alignment of capabilities... we propose a multi-granularity knowledge distillation strategy that enhances the knowledge transfer process..."
  - [corpus] "Learning Depth from Past Selves" (2511.15167) uses contrastive learning for robust depth, a different transfer technique. Corpus evidence for MKD specifically is weak; no direct methodological neighbor uses this exact 3-loss combination.
- Break condition: If the teacher model's pseudo-labels are poor for specific degradation types, or if the ordinal guidance from Depth Anything V2 is systematically incorrect for new domains, the student model will inherit these errors.

### Mechanism 3
- Claim: Ordinal Guidance Distillation (OGD) refines depth in uncertain regions.
- Mechanism: OGD identifies regions of high discrepancy between teacher and student depth predictions. It then samples pixel pairs from these uncertain regions and from the rest of the image, enforcing a ranking loss based on depth values provided by the pretrained Depth Anything V2 model. This forces the student model to focus on and correct difficult areas.
- Core assumption: Large teacher-student discrepancies correlate with areas where the student model struggles most due to degradation, and that these areas can be corrected using the ordinal depth relationships from the external pretrained model.
- Evidence anchors:
  - [abstract] "...ordinal guidance distillation mechanism (OGD) that encourages the network to focus on uncertain regions through differential ranking..."
  - [section] "We first identify the regions with significant discrepancies between DT and DS for refinement... These key regions correspond to areas with impaired perception in challenging scenes..."
  - [corpus] No corpus neighbors explicitly use this OGD technique. Evidence is limited to the paper itself.
- Break condition: The ranking loss ($L_r$) may conflict with the distillation loss ($L_d$), potentially causing training instability. This is explicitly noted and mitigated in the paper via windowed sampling.

## Foundational Learning

- Concept: **Diffusion Models for Image Translation**
  - Why needed here: The paper relies on a Stable Diffusion Turbo model to generate its training data. Understanding how latent diffusion works, and how LoRA adapters fine-tune it, is critical to comprehending the data generation pipeline.
  - Quick check question: Can you explain how a LoRA adapter modifies the weights of a pretrained UNet in a diffusion model without retraining the entire base model?

- Concept: **Knowledge Distillation**
  - Why needed here: The entire robust training phase is built upon distilling knowledge from a teacher model (trained on clear data) to a student model (trained on mixed clear/degraded data).
  - Quick check question: What is the primary goal of feature-based distillation versus output-based (pseudo-label) distillation?

- Concept: **Monocular Depth Estimation (Self-Supervised)**
  - Why needed here: The teacher model is trained using a self-supervised approach. Understanding the photometric loss ($L_p$) and smoothness loss ($L_e$) derived from image sequences is necessary to understand how the baseline teacher model works.
  - Quick check question: What two primary assumptions does self-supervised MDE make about the scene and camera that can be violated by adverse weather?

## Architecture Onboarding

- Component map:
  1. Data Generation Pipeline (Offline): A LoRA-tuned Stable Diffusion Turbo model that takes a clear image ($e_i$) and a text prompt ($P_c$) to generate a degraded image ($h^c_i$).
  2. Teacher Network: A standard ResNet18-based MDE network trained on clear data ($e_i$) using self-supervised losses ($L_p$, $L_e$).
  3. Student Network: The target robust MDE network (also ResNet18-based). It takes mixed inputs ($m_i \in \{e_i, h^c_i\}$) and is trained using the MKD losses ($L_d$, $L_r$, $L_c$).
  4. External Prior Model: A frozen Depth Anything V2 model providing ordinal depth supervision for the OGD loss.

- Critical path:
  1. Train the **Data Generation Pipeline** to produce high-quality degraded samples ($h^c_i$).
  2. Train the **Teacher Network** on clear data ($e_i$) until convergence.
  3. Freeze the Teacher and External Prior models. Train the **Student Network** on mixed data ($e_i$, $h^c_i$) using the total loss $L = L_d + \lambda_1 L_r + \lambda_2 L_c$.

- Design tradeoffs:
  - **Teacher vs. Student Model Capacity:** The paper uses the same backbone (ResNet18) for both. A more powerful teacher might provide better pseudo-labels, but the paper aims for a lightweight, deployable student.
  - **Loss Component Weights:** The weights $\lambda_1$ (for $L_r$) and $\lambda_2$ (for $L_c$) must be carefully balanced. The paper provides ablations, suggesting this is a sensitive hyperparameter.
  - **Data Translation Method:** The paper argues for diffusion models over GANs for better quality, but diffusion models may have higher inference costs during dataset creation.

- Failure signatures:
  - **"Washed out" depth maps:** May indicate the student model has failed to learn from the teacher or external prior and is collapsing to a mean prediction. Check loss curves.
  - **Artifacts in generated data:** If the data generation pipeline fails, the student model will learn these artifacts (e.g., unnatural lighting, distortions).
  - **Conflict in losses:** If $L_d$ and $L_r$ are fighting each other, training may become unstable. This is mitigated in the paper, but could reappear with different hyperparameters.

- First 3 experiments:
  1. Reproduce the Data Translation: Take a small set of clear images and train the LoRA adapters as described to generate `day->night` and `day->rain` samples. Visually inspect for quality.
  2. Baseline Teacher Training: Train the teacher model on clear day-only data using the standard self-supervised loss to establish a performance baseline.
  3. Ablate the Student Loss: Train three separate student models: (a) with only $L_d$, (b) with $L_d + L_c$, and (c) with the full MKD ($L_d + L_r + L_c$). Compare absRel metrics on the nuScenes night/rain validation set to verify the contribution of each component.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the data generation pipeline be optimized to handle datasets with highly uneven lighting distributions without requiring a significant increase in training samples?
- **Basis in paper:** [explicit] In Appendix A, the authors note that for the RobotCar dataset, "unique lighting distribution characteristics... make it challenging to train a high-quality translation model with a small number of samples," forcing an increase from 500 to 4,000 images to match the quality of nuScenes translations.
- **Why unresolved:** The current method relies on random sampling or manual selection of "high-quality" images to address lighting variance, lacking a mechanism to inherently handle uneven data distributions efficiently.
- **What evidence would resolve it:** Demonstrating successful image translation on datasets with extreme lighting variance (like RobotCar night) using a fixed, small sample size (e.g., 500 images) through an improved sampling or normalization strategy.

### Open Question 2
- **Question:** Is the 5x5 window-averaging heuristic the optimal solution for resolving gradient conflicts between the distillation loss and the ordinal ranking loss?
- **Basis in paper:** [explicit] Section 3.4 states, "we found that the constraints imposed by the distillation loss sometimes conflicted with the ranking loss. To mitigate this issue... we compute the average depth value of the surrounding 5x5 pixel region."
- **Why unresolved:** The authors introduce window averaging as a patch to prevent conflicts, but do not analyze if this smoothing causes loss of high-frequency geometric detail or if a dynamic weighting strategy would be superior.
- **What evidence would resolve it:** An ablation study comparing the window-averaging approach against gradient surgery methods (e.g., gradient projection or dynamic weight adjustment) to see if precision is lost due to the smoothing heuristic.

### Open Question 3
- **Question:** Can the method generalize to adverse weather conditions involving physical obstructions (e.g., snow or fog) where diffusion models might hallucinate non-existent structures?
- **Basis in paper:** [inferred] The method enforces "circular consistency loss" to guarantee content fidelity between clear and degraded images. While effective for rain/night (re-styling), this constraint might force the generator to reproduce occluded geometry in fog/snow rather than simulating the physical obstruction, a limitation not tested in the current nuScenes/RobotCar benchmarks.
- **Why unresolved:** The paper evaluates rain and night scenarios which primarily alter lighting and add reflections, but does not validate the system on weather types that fundamentally obscure scene geometry.
- **What evidence would resolve it:** Qualitative and quantitative results on foggy or snowy datasets (e.g., Foggy Cityscapes or ACDC) specifically analyzing whether the depth network fails when the generated imagery attempts to "preserve" content that should be occluded.

## Limitations
- The reliance on synthetic degradation data introduces uncertainty about real-world generalization, despite demonstrated improvements on benchmark datasets.
- The paper demonstrates significant improvements but only partially isolates the contributions of individual MKD components through ablation studies.
- The OGD mechanism's specific implementation and effectiveness lacks external validation from corpus evidence.

## Confidence
- **High Confidence:** The effectiveness of the data generation pipeline using LoRA-tuned diffusion models is well-supported by both visual comparisons and quantitative results.
- **Medium Confidence:** The overall framework's superiority (2.50-2.61% absRel improvement) is demonstrated, but individual contributions of MKD components require further validation.
- **Low Confidence:** The OGD mechanism's specific implementation and its effectiveness in refining uncertain regions lacks external validation from corpus evidence.

## Next Checks
1. **Visual Inspection of Generated Data:** Conduct a blind test where human evaluators rate the realism of synthetic degradation images against real adverse-weather images from nuScenes.
2. **Extended Ablation Study:** Systematically remove each component of the MKD strategy (Ld, Lc, Lr) to quantify their individual contributions to the overall performance gain.
3. **Cross-Dataset Generalization:** Evaluate ACDepth on a third, unseen dataset (e.g., Dark Zurich) to assess its robustness beyond the nuScenes and RobotCar domains.