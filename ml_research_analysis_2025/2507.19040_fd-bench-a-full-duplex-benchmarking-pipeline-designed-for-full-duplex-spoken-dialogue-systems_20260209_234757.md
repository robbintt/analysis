---
ver: rpa2
title: 'FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken
  Dialogue Systems'
arxiv_id: '2507.19040'
source_url: https://arxiv.org/abs/2507.19040
tags:
- speech
- user
- interruptions
- noise
- moshi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FD-Bench, a comprehensive benchmarking pipeline
  for evaluating full-duplex spoken dialogue systems (FDSDS). The pipeline addresses
  the lack of standardized metrics for assessing model performance during real-time
  user interruptions.
---

# FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems

## Quick Facts
- **arXiv ID**: 2507.19040
- **Source URL**: https://arxiv.org/abs/2507.19040
- **Reference count**: 0
- **Primary result**: Introduces a comprehensive benchmarking pipeline for evaluating full-duplex spoken dialogue systems with standardized metrics for real-time user interruptions

## Executive Summary
This paper introduces FD-Bench, a comprehensive benchmarking pipeline for evaluating full-duplex spoken dialogue systems (FDSDS). The pipeline addresses the lack of standardized metrics for assessing model performance during real-time user interruptions. It leverages LLMs, TTS, and ASR technologies to simulate natural conversations with interruptions, generate diverse speech corpora, and measure system robustness under challenging conditions. FD-Bench evaluates models across interruption handling, latency management, and response quality using both subjective (GPT-4o scoring) and objective metrics (e.g., WER, success rates, response delays). Applied to three open-source FDSDS (Moshi, Freeze-omni, and VITA-1.5) across 293 conversations and 1,200 interruptions, results show all models struggle with frequent disruptions and noisy environments, highlighting the need for improved real-time conversational capabilities in future FDSDS.

## Method Summary
FD-Bench is a comprehensive benchmarking pipeline designed to evaluate full-duplex spoken dialogue systems (FDSDS) by simulating natural conversations with user interruptions. The pipeline integrates LLM-based scoring (GPT-4o), TTS, and ASR technologies to generate diverse speech corpora and assess system performance across multiple dimensions. It evaluates three key areas: interruption handling (using interrupt success rate and percentage of completed turns), latency management (measuring response delays and average latency), and response quality (through GPT-4o-based scoring and ASR word error rate). The pipeline was applied to three open-source FDSDS models (Moshi, Freeze-omni, and VITA-1.5) across 293 conversations and 1,200 interruptions, testing performance under various conditions including different frequencies of interruptions, noise levels, and conversation scenarios.

## Key Results
- All three FDSDS models (Moshi, Freeze-omni, VITA-1.5) struggled with frequent user interruptions and noisy environments
- FD-Bench successfully demonstrated the pipeline's ability to generate diverse conversational scenarios and measure system performance across multiple dimensions
- The benchmark revealed that current FDSDS models have significant room for improvement in real-time conversational capabilities, particularly in handling interruptions and maintaining response quality under challenging conditions

## Why This Works (Mechanism)
The benchmarking pipeline works by creating a controlled environment that simulates real-world conversational dynamics through automated generation of diverse speech corpora. By leveraging LLM-based scoring, the system can evaluate subjective aspects of conversation quality while ASR and TTS components handle objective performance metrics. The pipeline's ability to systematically vary interruption frequency, noise levels, and conversation scenarios allows for comprehensive testing of FDSDS robustness across multiple dimensions of performance.

## Foundational Learning
- **Full-duplex spoken dialogue systems**: Enable simultaneous speaking and listening during conversations, mimicking natural human dialogue
  - *Why needed*: Traditional dialogue systems operate in half-duplex mode, creating unnatural pauses
  - *Quick check*: System can process incoming speech while generating responses
- **LLM-based scoring**: Uses GPT-4o to evaluate subjective conversation quality metrics
  - *Why needed*: Automated assessment of response appropriateness and conversational flow
  - *Quick check*: Scoring correlates with human judgment across multiple conversation scenarios
- **ASR-WER metrics**: Measures accuracy of speech recognition in noisy, interrupted environments
  - *Why needed*: Critical for evaluating system robustness under real-world conditions
  - *Quick check*: Error rates increase predictably with noise and interruption frequency
- **Interrupt success rate**: Quantifies how effectively systems handle user interruptions
  - *Why needed*: Key metric for full-duplex system evaluation
  - *Quick check*: Rate varies significantly across different FDSDS implementations
- **Response delay measurement**: Tracks latency between user input and system response
  - *Why needed*: Essential for maintaining natural conversational flow
  - *Quick check*: Delays correlate with system processing load and interruption frequency
- **Corpus generation diversity**: Creates varied conversational scenarios for comprehensive testing
  - *Why needed*: Ensures benchmark covers realistic conversation patterns
  - *Quick check*: Generated corpus includes multiple topic domains and interruption styles

## Architecture Onboarding

**Component Map**: LLM scoring -> TTS generation -> ASR processing -> FDSDS evaluation -> Metrics aggregation

**Critical Path**: User interruption simulation → ASR processing → FDSDS response generation → LLM evaluation → Metrics calculation

**Design Tradeoffs**: The pipeline prioritizes comprehensive evaluation over computational efficiency, using multiple models (LLM, TTS, ASR) to simulate realistic conversations, which increases accuracy but also resource requirements

**Failure Signatures**: 
- Low interrupt success rates indicate poor interruption handling
- High WER values suggest inadequate noise robustness
- Increased response delays point to processing bottlenecks
- Poor LLM scores reveal quality degradation in conversational flow

**First Experiments**:
1. Test baseline FDSDS performance with minimal interruptions and clean audio
2. Gradually increase interruption frequency while monitoring success rates
3. Introduce varying noise levels to assess robustness under degraded audio conditions

## Open Questions the Paper Calls Out
The evaluation of full-duplex spoken dialogue systems (FDSDS) faces several key uncertainties. The reliance on LLM-based scoring (GPT-4o) for subjective metrics introduces potential bias and raises questions about the alignment between automated scoring and human judgment in conversational contexts. The benchmarking pipeline's performance metrics may be influenced by the specific choice of ASR, TTS, and LLM models, limiting generalizability across different technical implementations. Additionally, the test conditions (293 conversations, 1,200 interruptions) may not fully capture the diversity of real-world conversational scenarios, particularly regarding user behavior patterns and environmental variability.

## Limitations
- Reliance on GPT-4o for subjective scoring may introduce bias and may not align with human judgment
- Benchmark results may be sensitive to specific ASR, TTS, and LLM model configurations
- Limited test conditions (293 conversations, 1,200 interruptions) may not capture full diversity of real-world scenarios

## Confidence
- Pipeline design and implementation: **High** - The technical architecture is clearly described with reproducible components
- Benchmark results across three FDSDS models: **Medium** - Results are consistent but may be sensitive to specific model configurations
- General conclusions about FDSDS limitations: **Medium** - Findings align with known challenges but sample size may limit broader implications

## Next Checks
1. Conduct human evaluation studies to validate the correlation between GPT-4o scoring and human judgments for interruption handling and response quality metrics
2. Test the benchmarking pipeline with alternative ASR, TTS, and LLM models to assess robustness of results across different technical implementations
3. Expand the corpus to include a wider variety of conversational scenarios, user interruption patterns, and environmental conditions to improve generalizability of findings