---
ver: rpa2
title: 'NorMuon: Making Muon more efficient and scalable'
arxiv_id: '2510.05491'
source_url: https://arxiv.org/abs/2510.05491
tags:
- muon
- normuon
- learning
- training
- orthogonalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces NorMuon, an optimizer that combines Muon\u2019\
  s orthogonalization with neuron-wise adaptive learning rates to improve training\
  \ efficiency for large language models. The authors observe that while Muon\u2019\
  s orthogonalization improves conditioning, it still produces non-uniform per-neuron\
  \ update norms, leading to imbalanced learning."
---

# NorMuon: Making Muon more efficient and scalable

## Quick Facts
- arXiv ID: 2510.05491
- Source URL: https://arxiv.org/abs/2510.05491
- Authors: Zichong Li; Liming Liu; Chen Liang; Weizhu Chen; Tuo Zhao
- Reference count: 11
- Primary result: NorMuon achieves 21.74% better training efficiency than Adam and 11.31% improvement over Muon for large language models

## Executive Summary
NorMuon addresses the limitations of Muon, an orthogonalized optimizer for large language models, by combining its conditioning benefits with neuron-wise adaptive learning rates. While Muon improves optimization geometry through Newton-Schulz orthogonalization, it produces highly non-uniform per-neuron update norms that cause imbalanced learning. NorMuon maintains second-order momentum statistics per neuron and applies row-wise normalization after orthogonalization, achieving both low condition numbers and uniform neuron norms while maintaining comparable memory footprint to Muon.

## Method Summary
NorMuon builds on Muon's orthogonalization approach by adding per-neuron adaptive scaling. After computing momentum gradients, the optimizer applies Newton-Schulz iteration (NS5) for orthogonalization, then normalizes each row by its second-moment statistics (v_t) before applying a global learning rate scaling. The method uses (β1, β2) = (0.95, 0.95) and includes efficient distributed implementation under FSDP2 with round-robin assignment of orthogonalization computations across devices. The row-wise normalization stays shard-local while orthogonalization requires momentum all-gather/scatter.

## Key Results
- 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1B and 5.4B parameter models
- Achieves both flat singular value spectra and uniform per-neuron norms (Figure 1)
- Maintains comparable memory footprint to Muon with only 2.9% per-step overhead versus AdamW
- Validated across multiple model scales including 124M and 350M parameter settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonalization via Newton-Schulz iteration reduces the condition number of update matrices, improving optimization geometry.
- Mechanism: The NS5 operator approximates the orthogonal polar factor of the momentum matrix through iterative fixed-point updates (Equation 5). This pushes singular values toward unity, preventing any single direction from dominating the update spectrum.
- Core assumption: Better-conditioned update matrices lead to more efficient gradient descent by ensuring balanced directional updates.
- Evidence anchors:
  - [abstract] "Muon's orthogonalization improves conditioning"
  - [Figure 1a] Shows Muon and NorMuon achieving uniformly low singular values compared to SGD momentum and AdamW
  - [corpus] "Iterative Orthogonalization Scaling Laws" documents scaling behavior of orthogonalization hyperparameters
- Break condition: If momentum matrix is rank-deficient or nearly so, NS5 iterations may fail to converge to meaningful orthogonal factor.

### Mechanism 2
- Claim: Muon's orthogonalized updates exhibit high variance in per-neuron norms, causing imbalanced learning across neurons.
- Mechanism: While orthogonalization normalizes singular values globally, individual rows (neurons) of the orthogonalized matrix can still have widely varying L2 norms. Neurons with larger norms receive larger effective step sizes.
- Core assumption: Uniform per-neuron update magnitudes are desirable for balanced parameter utilization.
- Evidence anchors:
  - [abstract] "resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate"
  - [Figure 1b] Muon shows per-neuron norms ranging ~0.5-2.5 while AdamW achieves tighter distribution
  - [corpus] "AdaMuon" paper independently observes similar per-coordinate variance issues in orthogonalized updates
- Break condition: If matrix rows are already approximately uniform in norm (e.g., strictly orthogonal m≤n matrices), this mechanism provides diminishing returns.

### Mechanism 3
- Claim: Row-wise normalization using second-order momentum statistics achieves both low condition numbers AND uniform neuron norms.
- Mechanism: After orthogonalization, compute EMA of per-row squared values (v_t), then normalize each row by sqrt(v_t + ε). This adaptively scales each neuron's contribution while preserving the orthogonal structure's conditioning benefits.
- Core assumption: The overhead of maintaining m additional scalars per weight matrix is negligible compared to mn momentum storage.
- Evidence anchors:
  - [abstract] "NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron"
  - [Section 3.1, Algorithm 1 lines 7-9] Formal specification of v_t accumulation and normalization
  - [Figure 1] NorMuon achieves both flat singular value spectrum (1a) and uniform per-neuron norms (1b)
  - [corpus] Limited direct corpus validation; "AdaMuon" explores coordinate-wise normalization but with higher memory cost
- Break condition: If β2 is poorly tuned, second-moment estimates may become stale (too low) or noisy (too high), destabilizing normalization.

## Foundational Learning

- **Condition number of matrices**:
  - Why needed here: Core to understanding why Muon helps and what it doesn't solve. High condition number means some update directions dominate others by orders of magnitude.
  - Quick check question: Given a matrix with singular values [0.01, 0.1, 1.0, 10.0], what is its condition number and why does this matter for optimization?

- **Newton-Schulz iteration for orthogonalization**:
  - Why needed here: NS5 is the computational heart of Muon. Understanding its fixed-point nature explains both its efficiency and limitations.
  - Quick check question: Why does Newton-Schulz require the input to be Frobenius-normalized first, and what happens if singular values are not in the convergence basin?

- **FSDP2 and ZeRO-3 sharding strategies**:
  - Why needed here: The distributed implementation depends on understanding how parameters, gradients, and optimizer states are partitioned across devices.
  - Quick check question: In FSDP2 with row-wise sharding, which device(s) hold the complete information needed to compute per-neuron normalization?

## Architecture Onboarding

- **Component map**:
  - Momentum buffer (M_t) -> Orthogonalization (NS5) -> Second-order momentum (v_t) -> Row normalizer -> Learning rate scaler -> Weight update

- **Critical path**:
  1. Gradient compute → momentum update (local)
  2. All-gather momentum for assigned parameters (communication)
  3. NS5 orthogonalization (computation on assigned device)
  4. Scatter orthogonalized updates (communication)
  5. Row-wise normalization using local v_t (local)
  6. Weight update (local)

- **Design tradeoffs**:
  - Memory: NorMuon uses m(n+1) vs Adam's 2mn (≈50% reduction) vs Muon's mn (negligible overhead)
  - Communication: 33-50% increase over baseline FSDP due to momentum gather/scatter in BF16
  - Computation: 2.9% per-step overhead vs AdamW; orthogonalization distribution critical (without it: 8.1% overhead)
  - Normalization position: Post-orthogonalization (NorMuon) outperforms pre-orthogonalization variant

- **Failure signatures**:
  - NaN/Inf in v_t: ε too small or β2 poorly tuned for sparse gradients
  - Load imbalance: Orthogonalization assigned to few devices; check round-robin assignment
  - Memory OOM with Muon+Adam variant: Full second-order momentum doubles memory
  - Training instability at scale: Verify β1=0.95 (not 0.9) for NorMuon; check Depth-μp learning rate scaling

- **First 3 experiments**:
  1. Reproduce Figure 1 on a small model: Plot singular value distribution and per-neuron norms for SGD, AdamW, Muon, NorMuon. Validate that NorMuon achieves both metrics.
  2. Ablation on normalization position: Compare NorMuon (post-ortho) vs "NorMuon (Front)" (pre-ortho) on 124M model. Expect post-ortho to win.
  3. Distributed scaling test: Train 1.1B model with orthogonalization distribution enabled vs disabled. Measure per-step latency; expect 2.7× slower without distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does NorMuon maintain its efficiency advantages when scaling to models significantly larger than 5.4B parameters or training on datasets exceeding 50B tokens?
- Basis in paper: [inferred] Experiments are limited to 1.1B and 5.4B models trained on 50B tokens (SlimPajama) and smaller NanoGPT settings.
- Why unresolved: The interaction between orthogonalization and adaptive scaling may exhibit different dynamics at frontier scales (e.g., 70B+ parameters).
- What evidence would resolve it: Pretraining benchmarks on 70B+ parameter models or 1T+ token datasets showing consistent convergence speedups over Adam and Muon.

### Open Question 2
- Question: To what extent can the 33–50% communication overhead be hidden by overlapping orthogonalization computation with gradient synchronization?
- Basis in paper: [explicit] Section 3.3 notes the increased communication volume but states it "can be overlapped... to minimize latency impact," though this is not benchmarked.
- Why unresolved: The paper reports a 3% latency increase but does not isolate the gains from specific overlapping strategies.
- What evidence would resolve it: Profiling data comparing standard NorMuon against an implementation with explicit communication-computation overlapping.

### Open Question 3
- Question: How does NorMuon impact downstream task performance (e.g., reasoning, knowledge retrieval) compared to Adam or Muon?
- Basis in paper: [inferred] The paper focuses primarily on validation loss and training efficiency, reporting minimal zero-shot results.
- Why unresolved: Lower validation loss does not always guarantee superior transfer learning capabilities or specific functional skills.
- What evidence would resolve it: A suite of standard downstream evaluations (e.g., MMLU, HellaSwag) for models trained with NorMuon versus baselines.

## Limitations
- Exact Newton-Schulz iteration coefficients are not specified in the paper, only described as "carefully chosen"
- Limited evaluation at frontier scales (70B+ parameters or 1T+ token datasets)
- Communication overhead increase of 33-50% not fully benchmarked with overlapping strategies

## Confidence

- **High confidence**: The orthogonality mechanism (NS5) and its conditioning benefits are well-established from the original Muon paper and validated through singular value distributions. The efficiency gains over AdamW are clearly demonstrated across multiple model scales.
- **Medium confidence**: The complementary nature of orthogonalization and adaptive learning rates is theoretically sound but could benefit from deeper analysis of how these mechanisms interact at different gradient magnitudes and model depths.
- **Medium confidence**: The distributed implementation claims are supported by experiments but rely on specific FSDP2 configurations that may not generalize to all distributed training setups.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary β2 (second-moment decay) and ε (normalization constant) to identify optimal ranges and potential failure modes, particularly for sparse gradients common in LLM training.

2. **Gradient sparsity impact study**: Measure how row-wise normalization behaves under different gradient sparsity patterns, comparing performance between dense and sparse gradient regimes.

3. **Cross-framework validation**: Implement NorMuon in a different deep learning framework (e.g., PyTorch vs JAX) to verify the distributed implementation details and ensure the claimed memory and communication patterns hold across implementations.