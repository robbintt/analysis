---
ver: rpa2
title: From model-based learning to model-free behaviour with Meta-Interpretive Learning
arxiv_id: '2507.16434'
source_url: https://arxiv.org/abs/2507.16434
tags:
- solver
- environment
- controller
- state
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of creating autonomous agents
  that can operate in novel environments by combining model-based planning capabilities
  with model-free exploration abilities. The core method involves using Meta-Interpretive
  Learning (MIL) to learn a model-based solver for planning problems, which is then
  used to generate examples for training a model-free controller in the form of a
  Nondeterministic Finite State Controller (FSC).
---

# From model-based learning to model-free behaviour with Meta-Interpretive Learning
## Quick Facts
- arXiv ID: 2507.16434
- Source URL: https://arxiv.org/abs/2507.16434
- Reference count: 13
- Creates autonomous agents that combine model-based planning with model-free exploration using Meta-Interpretive Learning

## Executive Summary
This paper presents a novel approach to creating autonomous agents that can operate in novel environments by combining the planning capabilities of model-based methods with the exploration abilities of model-free approaches. The core innovation lies in using Meta-Interpretive Learning (MIL) to learn a model-based solver for planning problems, which then generates examples to train a model-free controller in the form of a Nondeterministic Finite State Controller (FSC). The method bridges the gap between explicit reasoning and implicit behavior learning, enabling agents to handle ambiguous situations through nondeterministic choices and backtracking.

The approach is demonstrated on grid navigation tasks in both maze-like and open environments with obstacles. The learned FSCs successfully solve the same problems as the model-based solver, achieving 100% success rate on mazes and 92-100% on open environments. This work provides a formal framework for learning equivalent model-based and model-free agents, extends FSCs to handle ambiguity through nondeterminism, and offers concrete implementations for both learning and execution phases.

## Method Summary
The method combines model-based and model-free learning through a two-stage process. First, Meta-Interpretive Learning is used to learn a model-based solver for planning problems, which can find optimal paths in grid navigation tasks. This solver is then used to generate examples of state-action pairs that lead to successful outcomes. In the second stage, these examples are used to train a model-free controller in the form of a Nondeterministic Finite State Controller (FSC). The FSC is executed by specialized interpreters that manage exploration and backtracking when multiple actions are possible. This approach allows the agent to learn behavior that is equivalent to the model-based solver but can operate without explicit planning at runtime, making it more suitable for real-time applications in novel environments.

## Key Results
- Learned FSCs solve identical problems as model-based solvers with 100% success rate on maze environments
- Model-free controllers achieve 92-100% success rate on open environments with obstacles
- The approach successfully bridges model-based planning and model-free exploration in novel grid navigation tasks

## Why This Works (Mechanism)
The approach works by leveraging the strengths of both model-based and model-free learning paradigms. Meta-Interpretive Learning provides a way to learn declarative models of the environment that can be used for planning, while the FSC captures the learned behavior in a form that can be executed efficiently without explicit planning. The nondeterministic nature of the FSC allows it to handle ambiguous situations through exploration and backtracking, which is crucial for operating in novel environments where the agent cannot rely on precomputed plans.

## Foundational Learning
- Meta-Interpretive Learning (MIL): A form of inductive logic programming that learns logical theories from examples and background knowledge. Needed to learn declarative models from planning examples. Quick check: Can MIL learn correct theories from incomplete examples?
- Nondeterministic Finite State Controllers (FSCs): State machines that can take multiple possible actions from a given state. Needed to capture model-free behavior that can handle ambiguity. Quick check: Does the FSC correctly represent all valid paths from the learned model?
- Model-based vs Model-free learning: Model-based methods use explicit models for planning, while model-free methods learn direct state-action mappings. Needed to understand the complementary strengths being combined. Quick check: Are the learned FSC's behaviors truly equivalent to the model-based solver's outputs?
- Specialized interpreters for FSC execution: Systems that manage exploration and backtracking in nondeterministic FSCs. Needed to execute the learned controllers in practice. Quick check: Does the interpreter correctly handle backtracking when dead ends are reached?

## Architecture Onboarding
Component map: Environment -> MIL Learner -> Model-based Solver -> Example Generator -> FSC Learner -> Nondeterministic FSC -> Specialized Interpreter -> Agent Behavior

Critical path: The MIL learner must successfully learn a correct model-based solver, which must then generate valid examples that the FSC learner can use to create an equivalent controller. Any failure in this chain breaks the equivalence between model-based and model-free approaches.

Design tradeoffs: The approach trades computational efficiency during execution (model-free) for potentially higher training complexity (learning the FSC). The nondeterministic nature of the FSC adds expressiveness but requires specialized execution mechanisms.

Failure signatures: If the MIL learner fails to learn a correct model, the generated examples will be invalid, leading to an FSC that cannot solve the target problems. If the FSC learner cannot capture the full behavior of the model-based solver, the resulting controller will have lower success rates.

First experiments: 
1. Verify the MIL learner can correctly learn a model-based solver for a simple grid navigation problem
2. Test that the model-based solver can find optimal paths in both maze and open environments
3. Confirm that the FSC learner produces a controller with behavior equivalent to the model-based solver on training examples

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the work raises several important questions about generalization, scalability, and the limits of learning equivalent model-based and model-free agents that are addressed in the limitations section.

## Limitations
- The learned controllers may not generalize well beyond the specific grid sizes and obstacle configurations used in training
- Nondeterministic FSCs may experience exponential blowup in exploration time when scaling to larger state spaces
- The performance gap between model-based and model-free approaches (92% vs 100% success) suggests potential robustness issues with learned controllers

## Confidence
High: The formal framework for learning equivalent model-based and model-free agents is technically sound
Medium: The claim that MIL can successfully learn such equivalent agents based on limited experimental domains
Low: The scalability of the approach to domains with continuous state spaces or partial observability

## Next Checks
1. Test the learned controllers on unseen grid sizes and configurations not present in training data to assess generalization capabilities
2. Measure computational overhead and scaling behavior when applying the approach to domains with larger state spaces (e.g., 20x20 grids or multi-agent scenarios)
3. Compare performance against standard deep reinforcement learning baselines on identical tasks to establish relative strengths and weaknesses of the MIL-based approach