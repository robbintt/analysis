---
ver: rpa2
title: 'Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive
  Vision-Language Models'
arxiv_id: '2602.02043'
source_url: https://arxiv.org/abs/2602.02043
tags:
- benchmark
- pipeline
- color
- compositional
- swap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Auto-Comp is a fully automated pipeline for generating scalable,
  photorealistic benchmarks to evaluate compositional reasoning in Vision-Language
  Models (VLMs). It uses controlled parallel generation of Minimal (template-based,
  white background) and Contextual (natural language, realistic scenes) captions to
  systematically disentangle core binding ability from visio-linguistic complexity.
---

# Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models
## Quick Facts
- arXiv ID: 2602.02043
- Source URL: https://arxiv.org/abs/2602.02043
- Authors: Cristian Sbrolli; Matteo Matteucci; Toshihiko Yamasaki
- Reference count: 40
- Primary result: Auto-Comp is a fully automated pipeline for generating scalable, photorealistic benchmarks to evaluate compositional reasoning in Vision-Language Models (VLMs).

## Executive Summary
Auto-Comp is a fully automated pipeline for generating scalable, photorealistic benchmarks to evaluate compositional reasoning in Vision-Language Models (VLMs). It uses controlled parallel generation of Minimal (template-based, white background) and Contextual (natural language, realistic scenes) captions to systematically disentangle core binding ability from visio-linguistic complexity. Evaluation of 20 VLMs on novel benchmarks for color and spatial binding reveals universal compositional failures: performance collapses on complex tasks (N=3) and models are highly susceptible to low-entropy distractors (e.g., repeated objects/colors) beyond simple bag-of-words errors. SigLIP models outperform CLIP, and richer visual context aids spatial reasoning but hinders attribute binding due to visual clutter. The pipeline enables flexible, scalable benchmark creation for user-defined compositional skills.

## Method Summary
Auto-Comp is a fully automated pipeline that generates controlled, photorealistic benchmarks to evaluate compositional reasoning in VLMs. It uses parallel generation of Minimal captions (template-based, white backgrounds) and Contextual captions (natural language, realistic scenes) to systematically disentangle core binding ability from visio-linguistic complexity. The pipeline allows flexible creation of benchmarks for user-defined compositional skills, enabling scalable and reproducible evaluation of model performance on tasks involving attribute binding (e.g., color) and spatial reasoning.

## Key Results
- Performance collapses on complex tasks (N=3) and models are highly susceptible to low-entropy distractors (e.g., repeated objects/colors) beyond simple bag-of-words errors.
- SigLIP models outperform CLIP, and richer visual context aids spatial reasoning but hinders attribute binding due to visual clutter.
- The pipeline enables flexible, scalable benchmark creation for user-defined compositional skills.

## Why This Works (Mechanism)
The pipeline's effectiveness stems from its controlled parallel generation of Minimal and Contextual captions, which systematically isolates core binding ability from visio-linguistic complexity. By disentangling these factors, Auto-Comp reveals universal compositional failures in VLMs, such as performance collapse on complex tasks and susceptibility to low-entropy distractors. The photorealistic nature of the generated benchmarks ensures realistic evaluation conditions, while the scalability of the pipeline allows for extensive and reproducible testing across diverse compositional skills.

## Foundational Learning
1. **Minimal vs. Contextual Captions**: Minimal captions are template-based with white backgrounds, while Contextual captions use natural language and realistic scenes. *Why needed*: To isolate core binding ability from visio-linguistic complexity. *Quick check*: Ensure the pipeline generates both types of captions consistently for the same visual concepts.
2. **Attribute Binding**: Refers to the ability to associate attributes (e.g., color) with objects. *Why needed*: A key aspect of compositional reasoning in VLMs. *Quick check*: Verify that benchmarks test attribute binding across diverse objects and attributes.
3. **Spatial Reasoning**: Involves understanding spatial relationships (e.g., "left of," "behind"). *Why needed*: Essential for evaluating VLMs' ability to reason about object arrangements. *Quick check*: Confirm that benchmarks include varied spatial configurations and relationships.

## Architecture Onboarding
**Component Map**: Visual Concept Generation -> Minimal Caption Generation -> Contextual Caption Generation -> Benchmark Assembly
**Critical Path**: Visual Concept Generation -> Minimal Caption Generation -> Benchmark Assembly
**Design Tradeoffs**: Minimal captions simplify binding ability testing but lack linguistic complexity; Contextual captions add realism but introduce visual clutter.
**Failure Signatures**: Performance collapse on N=3 tasks, susceptibility to low-entropy distractors, and differential impact of visual context on attribute vs. spatial binding.
**First Experiments**:
1. Generate benchmarks for color and spatial binding using Minimal and Contextual captions.
2. Evaluate 20 VLMs (CLIP and SigLIP variants) on the benchmarks.
3. Analyze performance differences between Minimal and Contextual tasks to identify compositional failure patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focuses primarily on color and spatial binding tasks, leaving the generalizability of findings to other compositional skills (e.g., logical operations, counting) uncertain.
- The controlled generation of minimal and contextual captions assumes that the generated natural language captions adequately represent realistic linguistic complexity, which may not fully capture the diversity of human language patterns.
- The benchmarks are evaluated on a fixed set of 20 VLMs, predominantly CLIP and SigLIP variants, potentially limiting conclusions about broader model families.

## Confidence
- High confidence: The systematic approach to disentangling core binding ability from visio-linguistic complexity through parallel minimal and contextual caption generation is methodologically sound and well-validated through the controlled experiments.
- Medium confidence: Claims about universal compositional failures and model susceptibility to low-entropy distractors are supported by the experimental results but may be influenced by the specific task design and the relatively narrow focus on color and spatial reasoning.
- Medium confidence: The finding that SigLIP outperforms CLIP and that visual context differentially affects attribute versus spatial binding is robust but may be task-dependent and require further validation across diverse compositional skills.

## Next Checks
1. Evaluate Auto-Comp benchmarks on a broader range of VLMs, including non-contrastive models (e.g., Flamingo, BLIP) and those trained on diverse datasets, to assess generalizability of compositional failure patterns.
2. Extend the compositional skill coverage beyond color and spatial binding to include logical reasoning (e.g., negation, conjunction) and counting tasks, ensuring the pipeline's flexibility and robustness across varied compositional challenges.
3. Conduct human evaluation studies to validate whether the generated minimal and contextual captions accurately represent linguistic complexity and whether the failure modes identified align with human expectations for compositional reasoning.