---
ver: rpa2
title: 'Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient and
  Feasible Multitasking with Time Constraints Between Actions'
arxiv_id: '2503.02238'
source_url: https://arxiv.org/abs/2503.02238
tags:
- step
- action
- time
- actions
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RECIPE 2PLAN evaluates the multitasking abilities of LLMs under
  time constraints between actions. Unlike prior benchmarks focusing solely on task
  feasibility, this work introduces strict temporal dependencies between steps, requiring
  agents to balance efficiency and feasibility during parallel task execution.
---

# Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient and Feasible Multitasking with Time Constraints Between Actions

## Quick Facts
- **arXiv ID**: 2503.02238
- **Source URL**: https://arxiv.org/abs/2503.02238
- **Reference count**: 40
- **Key outcome**: RECIPE2PLAN benchmark shows LLMs struggle with time-constrained multitasking, with GPT-4o achieving only 21.5% success when temporal dependencies must be respected.

## Executive Summary
This paper introduces RECIPE2PLAN, a benchmark that evaluates large language models' ability to perform efficient multitasking under strict time constraints between actions. Unlike previous benchmarks that focus solely on task feasibility, this work introduces temporal dependencies requiring agents to balance efficiency and feasibility during parallel task execution. The benchmark is built from real-world cooking recipes annotated with action durations, dependencies, and resource constraints. Experiments reveal that while LLMs can generate feasible plans without time constraints, their performance drops sharply when such constraints are imposed, with time constraint violations being the primary source of failure.

## Method Summary
RECIPE2PLAN is built from cooking recipes where each recipe is annotated with action durations, dependencies, and resource constraints. The benchmark evaluates multitasking by requiring models to execute multiple recipes simultaneously while respecting temporal dependencies between actions. A heuristic baseline algorithm is introduced as a reference point for optimal scheduling. The evaluation uses two metrics: multitasking score (balancing efficiency and feasibility) and feasibility score. Models are tested both with and without time constraints, and with oracle constraints to isolate different failure modes. The LLM-Modulo framework is employed for iterative plan refinement using critics to improve feasibility.

## Key Results
- GPT-4o achieves 21.5% success rate under time constraints compared to 90.0% without constraints
- Time constraint violations are the primary source of failure, occurring even with oracle constraints and executable action hints
- Open-source models like Qwen2.5-Coder-32B achieve 100% feasibility without time constraints but drop to 15.8% with constraints
- The heuristic baseline provides a practical reference but is suboptimal, suggesting room for improved scheduling algorithms

## Why This Works (Mechanism)
The benchmark works by exposing a fundamental limitation in current LLMs' global planning capabilities. When time constraints between actions are introduced, models must balance parallel execution efficiency with sequential dependency requirements. The temporal dependencies create a complex search space where actions can be executed at arbitrary time stamps and split into arbitrary intervals. This complexity exceeds current models' ability to perform strategic planning that simultaneously considers efficiency and feasibility, leading to time constraint violations as the dominant failure mode.

## Foundational Learning
- **Temporal Reasoning**: Understanding and reasoning about time dependencies between actions is critical for efficient multitasking. Quick check: Can the model correctly identify which actions can be executed in parallel versus must be sequential?
- **Resource Constraint Management**: Simultaneous execution of multiple recipes requires tracking shared resources (oven, stove, utensils). Quick check: Does the model avoid resource conflicts when planning parallel execution?
- **Global Planning vs Local Optimization**: The benchmark distinguishes between generating locally feasible action sequences versus globally optimal plans that balance efficiency and feasibility. Quick check: Can the model optimize an entire plan rather than individual action choices?
- **Time Constraint Enforcement**: The ability to enforce strict temporal boundaries between dependent actions is essential for practical multitasking. Quick check: Does the model maintain minimum time gaps between prerequisite actions?
- **Efficiency-Feasibility Tradeoff**: Successful multitasking requires balancing execution speed with constraint satisfaction. Quick check: Can the model identify when parallel execution violates constraints and adjust accordingly?
- **Iterative Refinement**: The LLM-Modulo framework demonstrates that iterative critic feedback can improve plan feasibility. Quick check: Does repeated refinement with critics lead to improved constraint satisfaction?

## Architecture Onboarding

**Component Map**: Recipe Annotation -> Benchmark Generation -> Model Input Generation -> LLM Execution -> Plan Evaluation -> Metrics Calculation

**Critical Path**: Recipe Annotation → Benchmark Generation → LLM Execution → Plan Evaluation

**Design Tradeoffs**: The benchmark trades domain specificity (cooking) for realistic temporal dependencies and resource constraints. This provides concrete evaluation but may limit generalizability to other multitasking domains.

**Failure Signatures**: Primary failure occurs as time constraint violations between dependent actions, even when individual actions are feasible and resources are available. Models often violate minimum time gaps between prerequisite actions.

**First 3 Experiments**:
1. Test model performance on single-recipe execution to establish baseline planning ability without multitasking complexity
2. Evaluate parallel execution without time constraints to isolate resource management capabilities from temporal reasoning
3. Test with oracle constraints but without executable action hints to determine whether constraint awareness or action execution knowledge is the limiting factor

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can LLMs' global planning abilities be enhanced to balance efficiency and feasibility under time constraints between actions?
- **Basis in paper**: [explicit] The analysis identifies global planning as the "primary source of task failure and inefficient multitasking," stating this "paves the way for future work to focus on enhancing temporal reasoning and strategic planning."
- **Why unresolved**: Current models like GPT-4o achieve only 21.5% success; even with oracle constraints and executable action hints, success rates remain low, indicating fundamental architectural limitations.
- **What evidence would resolve it**: Development of methods (architectural changes, training approaches, or prompting strategies) that significantly improve multitasking scores on RECIPE2PLAN while maintaining or improving efficiency.

### Open Question 2
- **Question**: Can the LLM-Modulo framework be extended to enable models to optimize feasible plans for higher efficiency without breaking feasibility?
- **Basis in paper**: [explicit] Appendix F shows that when prompted to optimize feasible plans, "GPT-4o and o1-mini can only maintain the feasibility of half of the plans and the relative efficiency of these also decreases."
- **Why unresolved**: While iterative critics help models find feasible plans, models cannot reliably improve efficiency while preserving feasibility—suggesting a fundamental tension in current approaches.
- **What evidence would resolve it**: Demonstrating a framework where models can iteratively refine feasible plans toward higher efficiency with high success rate preservation.

### Open Question 3
- **Question**: What scheduling algorithms can optimally or near-optimally handle multitasking with time constraints between actions?
- **Basis in paper**: [explicit] The limitations section states "the search space is complex because the model can choose to execute actions at arbitrary time stamps and split actions into arbitrary time intervals, making it beyond the scope of classical scheduling algorithms...For future work, we plan to explore scheduling algorithms that can better handle the complexities of multitasking with time constraints."
- **Why unresolved**: The heuristic baseline is suboptimal but provides a practical reference; no known algorithms efficiently handle the full complexity of this problem space.
- **What evidence would resolve it**: Development and validation of algorithms providing provably optimal or bounded-approximation solutions on RECIPE2PLAN instances.

### Open Question 4
- **Question**: Do findings from text-based RECIPE2PLAN transfer to embodied agents operating in physical environments with real-world execution uncertainty?
- **Basis in paper**: [inferred] The limitations acknowledge the text-based environment "does not fully capture the complexities of real-world cooking and experimentation" and that "the agent is assumed to perform every action without delay or failure."
- **Why unresolved**: Real-world execution introduces sensorimotor challenges, object detection, execution timing variability, and recovery from partial failures not modeled in the current benchmark.
- **What evidence would resolve it**: Comparative studies showing correlation between text-based benchmark performance and success rates in embodied implementations (simulated or physical).

## Limitations
- Domain specificity to cooking may limit generalizability to other multitasking scenarios with different temporal dependency structures
- Time constraint annotations are subjective and may not capture all real-world variability in action durations
- Evaluation assumes deterministic action durations, not reflecting practical scenarios where execution times vary
- Comparison between open-source and closed-source models is constrained by API access limitations

## Confidence

**High Confidence**:
- Core finding that LLMs struggle with time-constrained multitasking, supported by consistent performance drops across multiple model families
- Time constraint violations as the primary failure mode, occurring consistently even with oracle constraints and executable action hints

**Medium Confidence**:
- Interpretation of time constraint violations as fundamental planning limitations versus annotation granularity issues
- Generalizability of cooking domain findings to broader multitasking scenarios without cross-domain validation

## Next Checks
1. **Cross-Domain Validation**: Test the benchmark framework on non-culinary multitasking scenarios (e.g., office workflows, home maintenance) to assess domain generalizability of the observed LLM limitations.

2. **Temporal Granularity Analysis**: Conduct ablation studies varying the granularity of time constraint annotations to determine how annotation precision impacts model performance and identify potential thresholds for effective planning.

3. **Human Performance Baseline**: Establish human expert performance on the same benchmark tasks to contextualize LLM performance and identify whether observed failures represent fundamental planning limitations or domain-specific challenges.