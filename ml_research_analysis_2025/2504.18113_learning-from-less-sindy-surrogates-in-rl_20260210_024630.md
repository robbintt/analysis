---
ver: rpa2
title: 'Learning from Less: SINDy Surrogates in RL'
arxiv_id: '2504.18113'
source_url: https://arxiv.org/abs/2504.18113
tags:
- surrogate
- environment
- sindy
- environments
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SINDy-based surrogate environments for reinforcement
  learning, addressing the challenge of data-intensive training by leveraging the
  Sparse Identification of Nonlinear Dynamics algorithm to create interpretable, computationally
  efficient models. The method uses pre-trained RL agents to collect minimal state
  transition data (75 for Mountain Car, 1000 for Lunar Lander), then applies SINDy
  to derive sparse governing equations that replace physics engines in OpenAI Gym
  environments.
---

# Learning from Less: SINDy Surrogates in RL

## Quick Facts
- arXiv ID: 2504.18113
- Source URL: https://arxiv.org/abs/2504.18113
- Reference count: 1
- Key outcome: SINDy-based surrogate environments achieve >0.997 state correlation with 95% less computational cost, requiring 20-35% fewer training steps while maintaining policy performance

## Executive Summary
This paper introduces SINDy-based surrogate environments for reinforcement learning, addressing the challenge of data-intensive training by leveraging the Sparse Identification of Nonlinear Dynamics algorithm to create interpretable, computationally efficient models. The method uses pre-trained RL agents to collect minimal state transition data (75 for Mountain Car, 1000 for Lunar Lander), then applies SINDy to derive sparse governing equations that replace physics engines in OpenAI Gym environments. Experimental results show state-wise correlations exceeding 0.997 with mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06 for Lunar Lander position, while reducing computational costs by 20-35%. RL agents trained in these surrogate environments require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs. 1,000,000 for Lunar Lander) while achieving comparable performance to those trained in original environments, demonstrating the approach's effectiveness for sample-efficient, interpretable surrogate environment development.

## Method Summary
The approach combines pre-trained RL agents with epsilon-greedy exploration to collect minimal state transition data, then uses SINDy's sparse regression to identify parsimonious governing equations. A function library containing polynomial and trigonometric terms is constructed, and Sequential Thresholded Least Squares fitting with regularization is applied to select essential dynamics terms. The resulting sparse model replaces physics engines in OpenAI Gym environments, creating surrogate environments where agents can be trained more efficiently. The method requires only 75 transitions for Mountain Car and 1000 for Lunar Lander, achieving high-fidelity dynamics representation with significantly reduced computational overhead.

## Key Results
- State-wise correlations exceeding 0.997 achieved with mean squared errors as low as 3.11e-06 for Mountain Car velocity
- Computational resource reduction of approximately 95% during model training
- RL agents trained in surrogate environments require 20-35% fewer total steps while achieving comparable performance

## Why This Works (Mechanism)

### Mechanism 1: Sparse Regression for Dynamics Discovery
SINDy's sparse regression identifies parsimonious governing equations from minimal state transitions, enabling sample-efficient surrogate environment creation. The algorithm applies Sequential Thresholded Least Squares (STLSQ) to a library of candidate functions, iteratively thresholding small coefficients to zero to produce an explicit transition function: $s_{t+1} = f_{SINDy}(s_t, a_t)$. This replaces computationally expensive physics engines with simple equation evaluations. The approach assumes true environment dynamics are governed by equations sparse in some known library of candidate functions, implying low-complexity, smooth dynamics.

### Mechanism 2: Sample-Efficient Exploration via Pre-trained Policies
Pre-trained agents with epsilon-greedy exploration enable highly efficient data collection, ensuring small datasets cover relevant state-action regions. A pre-trained agent provides optimal trajectory baselines, while epsilon-greedy policy (ε = 0.2) injects 20% random actions, forcing exploration of regions the optimal policy wouldn't visit. This yields diverse state-action pairs from both high-performing and exploratory paths within 75-1000 transitions, providing sufficient coverage for the surrogate model to generalize to new policies.

### Mechanism 3: Surrogate Environment for Accelerated Policy Learning
Agents trained in SINDy-based surrogates converge to comparable policies with fewer total steps than in original physics-based environments. The surrogate replaces expensive physics calculations with fast equation evaluation in the `step` function, yielding both wall-clock speedup and potentially smoother gradient landscapes for policy optimization. Results show 20-35% fewer training steps with comparable final performance, as the learned surrogate dynamics preserve core learning signals sufficiently for policy optimization despite being simplified approximations.

## Foundational Learning

- **Concept: SINDy Algorithm (Sparse Identification of Nonlinear Dynamics)**
  - Why needed here: Core method for discovering governing equations from data. Understanding STLSQ thresholding and library function selection is essential for adapting the approach to new environments.
  - Quick check question: Given a state trajectory, can you explain how increasing the sparsity threshold affects model complexity versus accuracy?

- **Concept: Model-Based Reinforcement Learning**
  - Why needed here: Provides context for why surrogate environments matter. Understanding the model-free vs. model-based tradeoff clarifies where SINDy surrogates fit in the RL taxonomy.
  - Quick check question: What is the primary risk when training a policy on a learned model rather than the true environment?

- **Concept: OpenAI Gym Environment Interface**
  - Why needed here: The surrogate must implement the standard Gym API (`reset`, `step`, observation/action spaces) to be compatible with existing RL algorithms.
  - Quick check question: Which method in a Gym environment must be modified to replace physics simulation with a learned dynamics model?

## Architecture Onboarding

- **Component map:** Data Collection Module -> SINDy Training Pipeline -> Surrogate Environment Wrapper -> RL Training Loop
- **Critical path:** Library function selection (trigonometric for oscillatory dynamics, polynomial for smoother systems) -> Hyperparameter grid search (threshold, regularization) -> Cross-validation on held-out transitions -> Policy training and transfer evaluation
- **Design tradeoffs:** Sparsity vs. accuracy (lower threshold → more terms → better fit but risk of overfitting and reduced interpretability), Data quantity vs. coverage (more transitions improve fit but reduce sample-efficiency gains), Library complexity vs. generalization (richer libraries capture more dynamics but may extrapolate poorly)
- **Failure signatures:** MSE plateaus despite added library terms (dynamics may not be sparse in chosen basis), Policy succeeds in surrogate but fails in original (sim-to-real gap from model inaccuracies), SINDy model fits training data but correlation drops on validation (overfitting, increase threshold)
- **First 3 experiments:** Baseline replication (reproduce Mountain Car results with 75 transitions, validating correlation >0.99 and policy equivalence), Ablation on library functions (test polynomials-only vs. polynomials + trigonometric on both environments to confirm library choice impact), Transfer stress test (train policy in surrogate, evaluate in original environment across multiple random seeds to quantify sim-to-real gap variance)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SINDy-based surrogate environments scale effectively to higher-dimensional state spaces and more complex dynamics found in continuous control benchmarks beyond Mountain Car and Lunar Lander?
- Basis in paper: The paper states "The scalability to higher-dimensional state spaces and more complex dynamics remains to be fully explored."
- Why unresolved: The experiments were limited to two relatively low-dimensional OpenAI Gym environments (Mountain Car and Lunar Lander).
- What evidence would resolve it: Successful training of RL agents in surrogate environments with high-dimensional states (e.g., MuJoCo Humanoid) showing fidelity and efficiency comparable to the reported results.

### Open Question 2
- Question: How well do SINDy surrogate models generalize when agents encounter significantly different initial conditions or out-of-distribution states?
- Basis in paper: The authors list "generalization capability to significantly different initial conditions" as a specific limitation requiring future exploration.
- Why unresolved: Data collection utilized single episodes (75 or 1000 steps) from specific pre-trained policies, potentially biasing the learned dynamics toward a narrow region of the state space.
- What evidence would resolve it: Robustness tests showing agents trained in the surrogate environment can succeed from initial states statistically distant from the training distribution.

### Open Question 3
- Question: Is this approach applicable to physical systems where data collection involves sensor noise and real-world safety constraints?
- Basis in paper: The conclusion notes that "testing on physical systems would validate real-world applicability."
- Why unresolved: All reported results derive from simulated physics engines (OpenAI Gym) rather than hardware, leaving the impact of real-world noise and physical constraints unknown.
- What evidence would resolve it: Successful identification of dynamics from physical robot data and subsequent Sim-to-Real transfer of policies trained in the surrogate.

## Limitations
- The approach's effectiveness depends critically on environment dynamics being sparse in the chosen library, which may not hold for highly complex or discontinuous systems
- The sim-to-real gap risk increases when surrogate models approximate but don't perfectly capture environment dynamics
- While promising for simple control tasks, scalability to high-dimensional continuous control or multi-agent settings remains unexplored

## Confidence

- **High Confidence:** SINDy's ability to learn parsimonious dynamics from small datasets (75-1000 transitions) is well-established through the presented empirical results (MSE ≤ 3.11e-06, correlations >0.997)
- **Medium Confidence:** The claim that surrogate training requires fewer steps (20-35% reduction) while maintaining performance is supported by the Mountain Car and Lunar Lander experiments, though transfer robustness across multiple seeds needs validation
- **Low Confidence:** Generalization claims to arbitrary OpenAI Gym environments are not empirically tested beyond the two specific cases presented

## Next Checks
1. **Transfer Robustness Test:** Evaluate surrogate-trained policies on original environments across 10 random seeds to quantify performance variance and sim-to-real gap consistency
2. **Library Function Ablation:** Systematically test polynomial-only versus trigonometric-enhanced libraries on both environments to confirm the claimed importance of library choice for accuracy
3. **Complexity Scaling Study:** Apply the approach to a slightly more complex environment (e.g., Acrobot or HalfCheetah) to assess performance degradation and data requirements as dimensionality increases