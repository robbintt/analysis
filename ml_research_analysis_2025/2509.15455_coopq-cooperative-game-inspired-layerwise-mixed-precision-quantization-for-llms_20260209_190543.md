---
ver: rpa2
title: 'CoopQ: Cooperative Game Inspired Layerwise Mixed Precision Quantization for
  LLMs'
arxiv_id: '2509.15455'
source_url: https://arxiv.org/abs/2509.15455
tags:
- quantization
- layer
- coopq
- shapley
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoopQ, a novel method for mixed-precision
  quantization of large language models (LLMs) that addresses the challenge of maintaining
  model performance when average precision drops below four bits. Unlike existing
  methods that rely on isolated layer-specific metrics, CoopQ frames quantization
  as a cooperative game among layers and uses Shapley value analysis to capture inter-layer
  interactions.
---

# CoopQ: Cooperative Game Inspired Layerwise Mixed Precision Quantization for LLMs

## Quick Facts
- arXiv ID: 2509.15455
- Source URL: https://arxiv.org/abs/2509.15455
- Reference count: 9
- Primary result: CoopQ reduces perplexity by 20-80% relative to best baselines at 2-4 bit precision

## Executive Summary
CoopQ introduces a cooperative game-theoretic approach to mixed-precision quantization for large language models, addressing the challenge of maintaining performance when average precision drops below 4 bits. The method models quantization as a cooperative game among layers, using Shapley value analysis to capture inter-layer interactions that isolated metrics miss. Through progressive quantization estimation and mixed-integer linear programming optimization, CoopQ consistently outperforms existing methods across three PTQ backends (Quanto, HQQ, GPTQ) and multiple model families.

## Method Summary
CoopQ formulates layerwise quantization as a cooperative game where each layer is a player contributing to model utility measured by negative log-likelihood. The method uses Shapley-based Progressive Quantization Estimation (SPQE) to measure marginal contributions by progressively quantizing layers from 4-bit to 2-bit while monitoring NLL changes. These contributions form a sensitivity matrix that, when combined with a covariance matrix representing interactions, is solved as a binary quadratic optimization problem via Mixed-Integer Linear Programming (MILP) to determine optimal 2/4-bit assignments under memory constraints.

## Key Results
- CoopQ reduces perplexity by 20-80% relative to best baselines across 4-bit to 2-bit precision ranges
- Performance margin grows as bit-width tightens, with particularly strong results at 2-bit precision
- Method demonstrates consistent superiority across three PTQ backends (Quanto, HQQ, GPTQ) and multiple model families (Llama-3, Gemma-2, Qwen-3)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling layers as cooperative game players captures inter-layer dependencies that isolated metrics miss
- **Mechanism:** Shapley values measure marginal contribution by averaging NLL changes over random layer permutations
- **Core assumption:** NLL payoff function accurately reflects model utility
- **Evidence anchors:** Abstract and Methods sections; weak corpus support
- **Break condition:** Non-representative validation dataset causes biased Shapley estimates

### Mechanism 2
- **Claim:** Progressive quantization (4-bit → 2-bit) enables stable Shapley estimation where pruning fails
- **Mechanism:** Incremental precision reduction keeps model functional, avoiding catastrophic failure
- **Core assumption:** Degradation trajectory is smooth enough for low-variance estimation
- **Evidence anchors:** Abstract and Ablation Study section
- **Break condition:** Calibration hardware cannot handle 4-bit inference efficiently

### Mechanism 3
- **Claim:** Global MILP optimization outperforms greedy sensitivity-based allocation
- **Mechanism:** Covariance matrix captures interactions; MILP solves binary quadratic problem globally
- **Core assumption:** Finite sampling approximates true pairwise interactions
- **Evidence anchors:** Methods and Results sections
- **Break condition:** Noisy interaction matrix causes solver overfitting

## Foundational Learning

- **Concept: Shapley Values (Cooperative Game Theory)**
  - **Why needed:** Mathematical core for capturing interactions rather than just sensitivity
  - **Quick check:** If layer A has high sensitivity but always follows layer B (which corrects errors), would its Shapley value be high or low?

- **Concept: Mixed-Precision Quantization (PTQ)**
  - **Why needed:** Target application requiring trade-off between memory bandwidth and model fidelity
  - **Quick check:** Why does uniform quantization often fail below 4-bit for LLMs?

- **Concept: Quadratic Programming & Linearization**
  - **Why needed:** Translates game-theoretic results into solvable optimization problem
  - **Quick check:** What does q^T K q represent compared to a^T q in the optimization objective?

## Architecture Onboarding

- **Component map:** SPQE Sampler → Stats Engine → MILP Optimizer → PTQ Backend
- **Critical path:** SPQE sampling loop (18 hours for Llama-3.1-8B) is computational bottleneck
- **Design tradeoffs:**
  - Sample Count (M): Higher M reduces variance but linearly increases calibration time
  - Diagonal Shrinkage (α): High α ignores interactions; low α trusts noisy interaction data
- **Failure signatures:**
  - High Variance: Use progressive quantization; verify smooth NLL changes
  - Solver Timeout: Increase α or simplify constraints if interaction matrix ill-conditioned
  - Catastrophic Drop: Sensitivity estimation failed due to bad calibration data
- **First 3 experiments:**
  1. Run SPQE with M=10 vs M=100 on small model; plot Shapley value variance
  2. Run CoopQ with α=1.0 vs α=0.5 at 2.5 avg bits; compare perplexity
  3. Apply same CoopQ mask using different backends (GPTQ vs Quanto); verify allocation robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can further performance gains be achieved by expanding search space beyond binary 2/4-bit assignments?
- **Basis:** Authors state exploring granular precision assignments could yield further gains
- **Why unresolved:** Expanding variables increases MILP complexity significantly
- **What evidence would resolve it:** Experiments incorporating 3-bit or 8-bit options, measuring optimization time vs perplexity reduction

### Open Question 2
- **Question:** Can interaction-aware framework be adapted for structured compression techniques like layer/head pruning?
- **Basis:** Discussion notes principles could extend to pruning where interdependencies are critical
- **Why unresolved:** Current SPQE models gradual precision changes; unclear if stable for complete layer removal
- **What evidence would resolve it:** Applying Shapley-based estimation to pruning task and comparing against magnitude-based pruning

### Open Question 3
- **Question:** Can advanced sampling techniques like stratified Monte Carlo reduce Shapley estimation overhead?
- **Basis:** Authors suggest efficiency could improve with stratified sampling
- **Why unresolved:** Standard Monte Carlo requires substantial computation; unverified if structured sampling achieves convergence with fewer permutations
- **What evidence would resolve it:** Ablation studies comparing convergence rate and performance of stratified vs random permutation sampling

### Open Question 4
- **Question:** Does discrepancy between estimation backend (Quanto) and deployment backend (e.g., GPTQ) introduce noise?
- **Basis:** Calibration-free Quanto used for all SPQE estimates even when targeting GPTQ/HQQ backends
- **Why unresolved:** Different backends have different error propagation characteristics
- **What evidence would resolve it:** Comparing bit-allocation maps from running SPQE with Quanto vs GPTQ

## Limitations

- SPQE sampling process is computationally intensive (18 hours for Llama-3.1-8B), limiting practical applicability
- Reliance on NLL payoff function assumes validation dataset represents deployment distribution
- Interaction matrix estimation from finite samples creates potential overfitting to calibration noise
- Scalability to extremely large models (>70B parameters) and performance under distribution shift remain unexplored

## Confidence

- **High confidence:** Cooperative game framing using Shapley values is mathematically sound; progressive quantization effectively avoids pruning failures; consistent perplexity improvements are reproducible
- **Medium confidence:** Choice of α=0.5 and convergence with M=100 samples are empirically validated but may not generalize to all model families
- **Low confidence:** Method's performance on extremely large models, under limited calibration data, and computational overhead for resource-constrained deployment remain unexplored

## Next Checks

1. **Distribution shift test:** Evaluate performance when calibration data (C4) differs significantly from evaluation data (WikiText-2), quantifying Shapley estimate sensitivity

2. **Sample efficiency analysis:** Systematically vary M (10, 25, 50, 100, 200) and measure trade-off between Shapley estimation variance and resulting perplexity

3. **Cross-backend consistency check:** Apply exact same bit allocation mask to all three PTQ backends on same model and measure variance in final perplexity