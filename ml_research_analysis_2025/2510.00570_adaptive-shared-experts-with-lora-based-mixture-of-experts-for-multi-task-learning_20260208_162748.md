---
ver: rpa2
title: Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning
arxiv_id: '2510.00570'
source_url: https://arxiv.org/abs/2510.00570
tags:
- experts
- shared
- expert
- lora
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient multi-task learning
  (MTL) with Mixture-of-Experts (MoE) architectures, where existing methods suffer
  from redundant adaptation and imbalanced knowledge sharing during the transition
  from single-task to multi-task learning. To solve this, the authors propose Adaptive
  Shared Experts (ASE), where shared experts are dynamically assigned router-computed
  gating weights that are jointly normalized with sparse experts, enabling balanced
  contributions and reducing gradient conflicts.
---

# Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning

## Quick Facts
- **arXiv ID**: 2510.00570
- **Source URL**: https://arxiv.org/abs/2510.00570
- **Reference count**: 0
- **Primary result**: ASE achieves 74.0 mIoU on semantic segmentation with 7.58% average relative performance gain over baseline

## Executive Summary
This paper addresses the challenge of efficient multi-task learning (MTL) with Mixture-of-Experts (MoE) architectures, where existing methods suffer from redundant adaptation and imbalanced knowledge sharing during the transition from single-task to multi-task learning. To solve this, the authors propose Adaptive Shared Experts (ASE), where shared experts are dynamically assigned router-computed gating weights that are jointly normalized with sparse experts, enabling balanced contributions and reducing gradient conflicts. Additionally, they incorporate fine-grained experts by increasing the number of LoRA experts while reducing their rank to enhance specialization under a comparable parameter budget. Evaluated on PASCAL-Context with a unified training setup, ASE consistently improves performance across diverse configurations, with the best setting (32/6/2/2) achieving 74.0 mIoU on semantic segmentation and a 7.58% average relative performance gain over the baseline, demonstrating the effectiveness of adaptive gating and fine-grained designs in MTL.

## Method Summary
The authors propose Adaptive Shared Experts (ASE) for efficient MTL with LoRA-based Mixture-of-Experts. The method combines two key innovations: adaptive gating for shared experts and fine-grained expert specialization. Shared experts receive router-computed gating weights that are jointly normalized with sparse expert weights through post-softmax normalization, allowing shared experts to dominate early training for knowledge transfer and decay adaptively as task-specific experts specialize. For fine-grained experts, the architecture increases expert count while proportionally reducing LoRA rank (e.g., 16/3/1/4 → 32/6/2/2 → 64/12/4/1), maintaining comparable parameter budgets while improving specialization. The framework uses ViT-base backbone with LoRA modules in FFN layers, task-specific routers for sparse and shared experts, and Mod-Squad regularization loss during training.

## Key Results
- ASE consistently improves performance across diverse configurations on PASCAL-Context benchmark
- Best configuration (32/6/2/2) achieves 74.0 mIoU on semantic segmentation
- ASE demonstrates 7.58% average relative performance gain over baseline
- Fine-grained expert scaling shows nearly linear improvements from 16 to 32 to 64 experts
- Adaptive shared experts outperform naive shared experts with fixed weights

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jointly normalizing shared and sparse expert gating weights stabilizes the STL-to-MTL transition and reduces gradient conflicts compared to fixed-weight shared experts.
- **Mechanism:** The router computes separate logits for sparse experts ($z$) and shared experts ($z_s$), then applies post-softmax normalization across both (Eq. 8-9) so all active weights sum to 1. This allows shared experts to dominate early training (capturing common knowledge) and decay adaptively as task-specific experts specialize, rather than exerting constant influence.
- **Core assumption:** The optimal contribution of shared knowledge changes over training: high initially for transfer, lower later to avoid interference.
- **Evidence anchors:**
  - [abstract] "shared experts are assigned router-computed gating weights jointly normalized with sparse experts... facilitates STL-to-MTL transition"
  - [Section 2.2] "This post-softmax normalization ensures that the contributions of all activated experts sum to one, thereby balancing sparse and shared experts"
  - [corpus] Neighbor paper "Dynamic Expert Specialization" (arXiv:2509.16882) supports adaptive expert contribution patterns but does not test joint normalization directly.
- **Break condition:** If shared expert logits collapse early (near-zero $z_s$), the mechanism fails; monitor $g^s_i$ magnitudes across epochs.

### Mechanism 2
- **Claim:** Increasing expert count while proportionally reducing LoRA rank (fine-grained experts) improves specialization under fixed parameter budgets.
- **Mechanism:** Configuration $(N/k/S/r)$ scales $N$ up and $r$ down (e.g., 16/3/1/4 → 32/6/2/2 → 64/12/4/1), keeping total LoRA parameters ~constant. More experts provide finer task-expert assignments; lower rank per expert prevents parameter explosion.
- **Core assumption:** Specialization benefits from expert diversity outweigh expressivity loss from lower per-expert rank.
- **Evidence anchors:**
  - [abstract] "incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank"
  - [Section 3.1] "finer expert granularity yields nearly linear improvements" (Fig. 3b)
  - [corpus] Weak direct evidence; "Each Rank Could be an Expert" (arXiv:2501.15103) explores rank-expert relationships but not this specific scaling.
- **Break condition:** If rank drops too low (r=1 with few experts), per-expert capacity may insufficiently model task features; validate on held-out tasks.

### Mechanism 3
- **Claim:** LoRA-based experts reduce memory and FLOP overhead while preserving MoE flexibility for MTL.
- **Mechanism:** Each expert $E_i$ is a LoRA module ($\Delta W = BA$, Eq. 4) added to frozen base weights $W_0$. Only low-rank matrices $A$, $B$ are trained; backbone remains frozen. This enables many experts without proportional memory growth.
- **Core assumption:** Low-rank updates are sufficient for task-specific adaptations in MTL vision tasks.
- **Evidence anchors:**
  - [Section 2.1] "substantially reduces trainable parameters and FLOPs while preserving the flexibility of MoE"
  - [Table 1] ASE achieves +7.58% $\Delta_m$ with only ~4% parameter increase over plain ViT
  - [corpus] LoRAMoE (cited as [11]) validates LoRA-MoE for LLMs; this paper extends to vision MTL.
- **Break condition:** If tasks require high-rank feature modifications (e.g., significantly different domains), LoRA may underfit; compare to full fine-tuning baseline.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) routing**
  - **Why needed here:** Understanding how top-k selection and softmax gating distribute inputs to experts is prerequisite to grasping ASE's joint normalization.
  - **Quick check question:** Given logits $[2.0, 1.0, 0.5]$ for 3 experts with top-2 selection, what are the final gating weights after softmax?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** All experts in this architecture are LoRA modules; understanding $\Delta W = BA$ factorization is essential for parameter budget calculations.
  - **Quick check question:** For $d_{in}=768$, $d_{out}=3072$, rank $r=4$, how many trainable parameters does one LoRA expert require versus full $W$?

- **Concept: Gradient conflicts in MTL**
  - **Why needed here:** The paper explicitly positions ASE as reducing gradient conflicts from shared experts; understanding task interference is necessary context.
  - **Quick check question:** If task A's gradient updates $W$ in direction $g_A$ and task B's in direction $g_B$ with $\cos(g_A, g_B) < 0$, what happens to shared parameters?

## Architecture Onboarding

- **Component map:** Input x → Task embedding (e_t) → ViT blocks with LoRA-MoE FFN layers → Task-specific router R^(ℓ)_t → logits z (sparse) + z_s (shared) → joint softmax → weighted sum → Task-specific head H_t → Output ŷ_t

- **Critical path:**
  1. Task embedding concatenation (distinguishes tasks for shared backbone)
  2. Per-layer routing with separate sparse and shared logits (Eq. 6-7)
  3. Joint normalization (Eq. 8-9)—**implementation critical; do not use standard softmax**
  4. Expert outputs weighted by normalized gates (Eq. 10)

- **Design tradeoffs:**
  - Higher $N$ (expert count): Better specialization, higher router overhead
  - Lower $r$ (LoRA rank): Parameter efficiency, potential under-capacity
  - More shared experts $S$: Stronger common knowledge transfer, risk of gradient conflict if not adaptively gated

- **Failure signatures:**
  - Shared expert weights ($g^s_i$) near 0 at epoch 1: router not learning to use shared experts
  - Large performance gap between fine-grained and coarse configurations on same task: rank too low
  - Training instability after epoch ~10: check if shared weights decay properly (Fig. 4 pattern)

- **First 3 experiments:**
  1. **Sanity check:** Implement naive shared expert (fixed weight=1, Eq. 5) vs. ASE on 2-task subset; expect ASE to close the performance gap per Fig. 2.
  2. **Ablation on $k$:** Vary top-k $\in \{3,4,5\}$ with $S=1$; verify ASE consistently outperforms baseline per Fig. 3a.
  3. **Fine-grained scaling:** Compare configurations $(16/3/1/4)$ vs. $(32/6/2/2)$; expect ~linear improvement in $\Delta_m$ per Fig. 3b.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the performance gain from fine-grained expert configurations plateau due to the increasing parameter overhead of the router network?
- **Basis in paper:** [inferred] The authors note in Section 3.1 that for the high-granularity configuration (64/12/4/1), "router cost still grows with N, making [it] more computationally demanding," leading them to relegate it to a supplementary comparison.
- **Why unresolved:** It is unclear if the near-linear improvement observed from 16 to 32 to 64 experts continues indefinitely or if the router's parameter growth eventually offsets the efficiency benefits of low-rank adaptation.
- **What evidence would resolve it:** A scaling analysis comparing performance gains versus total parameter count (including routers) for configurations with significantly higher expert counts (e.g., $N > 64$).

### Open Question 2
- **Question:** How does the ASE framework perform on high-resolution inputs or with larger backbone architectures (e.g., ViT-Large) given the current memory constraints?
- **Basis in paper:** [inferred]** The Implementation Details (Section 3.1) state that "all experiments are conducted with a reduced input resolution of $224 \times 224$" specifically to reduce memory usage, leaving standard higher resolutions unexplored.
- **Why unresolved:** The effectiveness of the "adaptive" shared expert mechanism in balancing gradient conflicts might change with the increased feature complexity and computational load of larger models or inputs.
- **What evidence would resolve it:** Benchmark results on the PASCAL-Context dataset using standard higher resolutions (e.g., $512^2$) or larger pre-trained backbones without downsizing.

### Open Question 3
- **Question:** Do the benefits of joint normalization for shared experts transfer to non-vision domains such as Natural Language Processing (NLP)?
- **Basis in paper:** [inferred] The Introduction explicitly identifies NLP and speech as domains driven by large-scale networks where MoE is relevant, but the Evaluation (Section 3) is restricted exclusively to the PASCAL-Context vision benchmark.
- **Why unresolved:** The "STL-to-MTL transition" dynamics and gradient conflicts in attention-based language models may exhibit different behaviors than the Vision Transformer (ViT) used in the study.
- **What evidence would resolve it:** Application of the ASE method to multi-task NLP benchmarks (e.g., GLUE variants) to verify if the adaptive gating mechanism reduces gradient conflicts in linguistic tasks.

## Limitations
- **Limited comparison to full fine-tuning baselines:** ASE lacks direct comparison to non-MoE MTL approaches under identical training conditions
- **Dataset specificity concerns:** Optimal balance between shared and sparse expert contributions may not generalize beyond PASCAL-Context
- **Training stability uncertainty:** Long-term stability of ASE's adaptive gating mechanism beyond 40 epochs is not evaluated

## Confidence
- **High Confidence:** The LoRA-MoE implementation and parameter budget calculations are technically sound and well-supported by the literature
- **Medium Confidence:** The adaptive gating mechanism's effectiveness is demonstrated within the paper's experimental scope but lacks broader validation across diverse datasets and task combinations
- **Medium Confidence:** The fine-grained expert scaling relationship shows consistent patterns in Fig. 3b but requires additional validation to confirm linear improvements hold across different rank ranges

## Next Checks
1. Implement a direct comparison between ASE and naive shared experts (fixed weight=1) on a 2-task subset, monitoring shared expert gating weights across training epochs to verify the adaptive decay pattern
2. Test ASE's performance degradation when shared expert logits are artificially constrained to near-zero values at initialization, validating the mechanism's dependence on proper router learning
3. Evaluate ASE on a held-out multi-task dataset (e.g., NYUD-v2 with depth, segmentation, and normal estimation) to assess generalization beyond PASCAL-Context