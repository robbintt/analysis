---
ver: rpa2
title: 'Point What You Mean: Visually Grounded Instruction Policy'
arxiv_id: '2512.18933'
source_url: https://arxiv.org/abs/2512.18933
tags:
- object
- point-vla
- visual
- spatial
- grounded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of vision-language-action
  (VLA) models in referring to objects using only text instructions, particularly
  in cluttered or out-of-distribution scenes. To resolve this, the authors introduce
  Point-VLA, a plug-and-play policy that augments language instructions with explicit
  visual cues such as bounding boxes for precise, pixel-level grounding.
---

# Point What You Mean: Visually Grounded Instruction Policy

## Quick Facts
- **arXiv ID:** 2512.18933
- **Source URL:** https://arxiv.org/abs/2512.18933
- **Reference count:** 40
- **Primary result:** Point-VLA achieves 92.5% average success rate vs 32.4% baseline by augmenting text instructions with explicit visual cues

## Executive Summary
This paper addresses the limitations of vision-language-action (VLA) models in referring to objects using only text instructions, particularly in cluttered or out-of-distribution scenes. To resolve this, the authors introduce Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues such as bounding boxes for precise, pixel-level grounding. They also develop an automatic data annotation pipeline using multimodal large language models to efficiently scale visually grounded datasets with minimal human effort. Evaluated on diverse real-world referring tasks, Point-VLA consistently outperforms text-only VLA models, achieving an average success rate of 92.5% compared to 32.4% for the baseline, with particularly large gains in cluttered (94.3% vs. 43.3%) and unseen-object scenarios (92.5% vs. 57.5%). The model maintains strong text-only performance through co-training and generalizes across different VLA backbones and robot embodiments, demonstrating its plug-and-play capability. These results show that explicit visual grounding effectively resolves referential ambiguity, enabling more robust and generalizable embodied control.

## Method Summary
Point-VLA extends standard VLA models by incorporating a first-frame overhead image with a bounding box overlay as an additional input. The policy conditions on both the current observations and this visually grounded first frame to resolve referential ambiguity. The method uses co-training with a 1:1 mixture of text-only and visually grounded samples, along with two key augmentations: random translation of the grounded image to encourage spatial invariance, and localized CutMix within the bounding box to improve appearance robustness. An automatic annotation pipeline using multimodal LLMs generates bounding box labels with minimal human effort. The approach is evaluated on a π0.5 VLA backbone, fine-tuned for 20k steps per task.

## Key Results
- Point-VLA achieves 92.5% average success rate compared to 32.4% for text-only baseline across 6 real-world tasks
- Largest gains in cluttered scenes (94.3% vs 43.3%) and out-of-distribution object picking (92.5% vs 57.5%)
- Model maintains strong text-only performance through co-training, matching or exceeding baseline on text-only spatial tasks
- Demonstrates plug-and-play capability by generalizing across different VLA backbones and robot embodiments

## Why This Works (Mechanism)

### Mechanism 1: Explicit Pixel-Level Binding
Overlaying bounding boxes on images resolves the referential ambiguity inherent in text-only instructions, particularly for cluttered or linguistically indescribable targets. The model shifts from inferring a target from natural language to conditioning on a direct visual prompt (a highlighted region), bypassing the "information bottleneck" of language by providing a pixel-grounded residual connection to the target object.

### Mechanism 2: Spatial-Invariant Grounding via Translation Augmentation
Randomly translating the grounded image forces the policy to learn relative spatial relationships rather than overfitting to absolute screen coordinates. By shifting the scene and the bounding box together, the model learns that the "marked region" is the target, regardless of its absolute position, decoupling the grounding logic from fixed positional encodings.

### Mechanism 3: Appearance Robustness via CutMix
Perturbing the visual appearance inside the bounding box prevents the model from overfitting to specific object textures and encourages reliance on the bounding box itself as the primary locator. By replacing parts of the object inside the box with random patches, the model cannot rely on "memorized texture" to identify the target and must treat the content within the box as the target definition.

## Foundational Learning

- **Concept: Vision-Language-Action (VLA) Formulation**
  - **Why needed here:** To understand that standard VLAs map $(I_t, l_t) \to a_t$. Point-VLA modifies this by adding a third conditional input $(\tilde{I}_{g,0})$.
  - **Quick check question:** How does the Point-VLA formulation differ from the standard VLA policy function $\pi_\theta(l_t, I_t)$?

- **Concept: Visual Prompting / Grounding**
  - **Why needed here:** The core innovation is treating a bounding box not as a label, but as a *visual prompt* (input) to the model.
  - **Quick check question:** Why does the paper argue that "textual coordinates" (appending numbers to text) are inferior to a visual overlay?

- **Concept: Co-training / Multitask Learning**
  - **Why needed here:** The model must not lose its ability to follow text-only instructions.
  - **Quick check question:** Why is training on a 1:1 mixture of text-only and visually grounded samples critical for the model's backward compatibility?

## Architecture Onboarding

- **Component map:** Overhead Camera + Wrist Cameras (Standard VLA) + First-Frame Overhead w/ Bounding Box (New) -> π0.5 VLA Backbone (Transformer/Flow-matching) -> Action Chunk (Robot trajectory)

- **Critical path:** The "First-Frame Grounding" ($\tilde{I}_{g,0}$). The system relies on a static assumption: the bounding box drawn on the first frame must remain valid for the duration of the episode.

- **Design tradeoffs:**
  - **Overlay vs. Mask:** The paper selects bounding box overlays because they preserve global context. Object-only masking (cropping everything else) removes environmental cues necessary for spatial reasoning.
  - **Automatic vs. Manual Annotation:** Automatic pipeline uses MLLMs to reduce human labor but introduces a 8% error rate in bounding box accuracy, requiring robustness to noisy labels.

- **Failure signatures:**
  - **Coordinate Overfitting:** If translation augmentation is insufficient, the robot will reach for a fixed $(x,y)$ coordinate regardless of where the object actually is.
  - **Static Camera Drift:** If the overhead camera is bumped or moved between the "first frame" annotation and execution, the grounding is misaligned.

- **First 3 experiments:**
  1. Ablation on Input Modality: Compare "Text-Only" vs. "Text + Box Overlay" vs. "Text + Coordinates" to verify that spatial binding (and not just extra tokens) drives the performance.
  2. Augmentation Validation: Train two models, one with and one without random translation/CutMix. Evaluate on a task where the object tray is physically shifted between trials to test robustness.
  3. Compatibility Test: Run the trained Point-VLA model on standard text-only tasks (no box provided) to ensure performance hasn't degraded compared to the baseline VLA.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the visual grounding pipeline be adapted to maintain accuracy in dynamic environments where the camera or target object moves significantly during task execution?
- **Basis in paper:** The Conclusion states: "A remaining limitation is that the grounding relies on a static overhead view; when the camera moves, the first-frame annotation may become partially inaccurate. Extending the pipeline with temporal tracking and multi-view consistency is a promising direction for future work."
- **Why unresolved:** The current architecture anchors the visual prompt to the first frame ($\tilde{I}_{g,0}$). If the camera moves (e.g., on a mobile robot) or the object is displaced, the static bounding box coordinates become invalid relative to the new observation.
- **What evidence would resolve it:** Evaluation of success rates on a mobile manipulator platform or in tasks where the target object is intentionally displaced mid-trajectory.

### Open Question 2
- **Question:** How can the automatic annotation pipeline be improved to robustly handle severe occlusion?
- **Basis in paper:** The Appendix ("Annotation Quality Analysis") reports a 92% accuracy for the MLLM annotator but notes that "Most errors arise from severe occlusion, ambiguous container views, or extremely similar object appearances."
- **Why unresolved:** The current reliance on MLLMs for automatic labeling fails when visual evidence of the target is blocked, potentially introducing noise into the training data for cluttered scenarios.
- **What evidence would resolve it:** A comparative analysis of annotation accuracy on a dataset specifically curated with high-occlusion scenes, potentially integrating 3D or multi-view reasoning into the annotation step.

### Open Question 3
- **Question:** Can Point-VLA generalize zero-shot to alternative visual prompt geometries (e.g., circles, clicks, or scribbles) without specific fine-tuning?
- **Basis in paper:** The paper claims the "interface is agnostic to the specific marker shape and can also support other lightweight visual cues such as circles or clicks," but all reported experiments and ablations exclusively validate bounding boxes.
- **Why unresolved:** While the architecture is theoretically agnostic, the data augmentation (CutMix, translation) is designed around bounding boxes, and it is unverified if the model has implicitly overfitted to the rectangular structure of the training prompts.
- **What evidence would resolve it:** An ablation study reporting success rates when using click-points or circle overlays as the visual prompt during inference.

## Limitations

- The static assumption of the bounding box remaining valid throughout an episode is a significant limitation - any camera movement or object displacement between the first frame and execution would break the grounding
- The automatic annotation pipeline, while reducing human labor, introduces potential noise in bounding box coordinates (8% error rate reported in Appendix A)
- Evaluation focuses on success rates without analyzing failure cases in detail, making it difficult to determine whether failures stem from the grounding approach itself or from other factors in the VLA pipeline

## Confidence

- **High confidence:** The core mechanism of using visual grounding (bounding boxes) to resolve referential ambiguity is well-supported by the dramatic performance improvements (92.5% vs 32.4% average success rate) and ablation studies
- **Medium confidence:** The generalizability claims across different VLA backbones and robot embodiments, as the paper demonstrates this but with limited diversity in tested configurations
- **Low confidence:** The robustness claims against annotation errors, as the paper mentions the 8% error rate but doesn't provide systematic evaluation of how this noise affects performance

## Next Checks

1. Test Point-VLA performance when the overhead camera is slightly repositioned between first-frame annotation and execution to quantify sensitivity to coordinate drift
2. Conduct systematic analysis of failure cases to determine whether errors stem from the grounding approach, VLA backbone limitations, or task complexity
3. Evaluate Point-VLA on tasks where the target object moves during the episode to test the static bounding box assumption