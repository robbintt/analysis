---
ver: rpa2
title: Optimizing open-domain question answering with graph-based retrieval augmented
  generation
arxiv_id: '2503.02922'
source_url: https://arxiv.org/abs/2503.02922
tags:
- trex
- graphrag
- retrieval
- arxiv
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Optimizing open-domain question answering with graph-based retrieval augmented generation

## Quick Facts
- arXiv ID: 2503.02922
- Source URL: https://arxiv.org/abs/2503.02922
- Reference count: 40
- Primary result: None specified

## Executive Summary
TREX introduces a cost-effective graph-based RAG system that balances OLTP and OLAP query performance through hierarchical summarization and reciprocal rank fusion. The system uses GMM clustering and LLM summarization to build a tree structure that enables multi-granularity retrieval, then fuses vector similarity and keyword search results to optimize ranking. TREX achieves comparable accuracy to state-of-the-art approaches while reducing indexing costs by approximately 10x.

## Method Summary
TREX processes documents by chunking text, embedding with text-ada-embeddings-002, reducing dimensions with UMAP, clustering with GMM, and recursively summarizing clusters with LLM to build a hierarchical tree. The system retrieves via fused cosine similarity + keyword search using Reciprocal Rank Fusion (RRF, k=60), then generates answers with GPT-4o. It's evaluated on HotPotQA, MSMarco, earnings call transcripts, and podcast transcripts using both accuracy metrics for fact-based queries and LLM-as-judge metrics for multi-document synthesis tasks.

## Key Results
- TREX indexing costs: HotPotQA $36.51 vs GraphRAG $389.12 (10x reduction)
- Comparable accuracy to GraphRAG and RAPTOR on OLTP queries
- Better cost-performance balance than GraphRAG for mixed query workloads
- Outperforms baseline retrieval methods on multi-hop and multi-document queries

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Summarization Tree Enables Multi-Granularity Retrieval
TREX's tree structure captures semantic relationships at multiple abstraction levels, allowing retrieval to match query scope. GMM clustering groups semantically similar chunks, then LLM-generated summaries create parent nodes. This recurses upward, producing a tree where leaf nodes are raw chunks and root nodes are corpus-wide summaries. Queries retrieve across all levels simultaneously.

### Mechanism 2: Reciprocal Rank Fusion Combines Complementary Retrieval Signals
Fusing vector similarity and keyword search via RRF improves recall without sacrificing precision. TREX runs parallel retrieval—cosine similarity on embeddings and keyword matching on raw text—then combines rankings using RRF: RRF(d) = Σ(1/(k + r(d))). Higher-ranked documents from either system contribute more, but neither dominates.

### Mechanism 3: Truncated Tree Reduces Indexing Cost While Preserving OLTP Performance
Limiting tree depth reduces LLM summarization calls with minimal accuracy loss on fact-based queries. Full RAPTOR builds trees to corpus-level root. TREX truncates earlier, storing fewer summary nodes. For OLTP queries targeting specific facts, deep abstractions are unnecessary—leaf-adjacent summaries suffice.

## Foundational Learning

- Concept: OLTP vs OLAP Query Classification
  - Why needed here: System design depends on query type. OLTP (fact-based, key-value) suits vector search; OLAP (thematic, multi-document) benefits from graph structures and summarization.
  - Quick check question: Given a query "What was Microsoft's Q4 2024 revenue?", classify it and identify which RAG method would likely perform best.

- Concept: Gaussian Mixture Models for Clustering
  - Why needed here: TREX uses GMM to group text chunks before summarization. Understanding probabilistic clustering helps diagnose when clusters fail to capture semantic boundaries.
  - Quick check question: Why might GMM clustering produce poor results on highly skewed or non-Gaussian text embeddings?

- Concept: Reciprocal Rank Fusion (RRF)
  - Why needed here: TREX and Hybrid Search both use RRF to merge rankings. Understanding fusion helps debug retrieval quality and tune the constant k.
  - Quick check question: If k=60 in RRF, how much influence does a rank-1 result have compared to rank-10?

## Architecture Onboarding

- Component map: Document chunking -> UMAP dimensionality reduction -> GMM clustering -> LLM summarization -> Vector + keyword indexing -> Query embedding -> Parallel vector/keyword search -> RRF fusion -> Top-k node selection -> LLM generation
- Critical path: The clustering and summarization step is the bottleneck. Poor clusters produce incoherent summaries, degrading downstream retrieval. Validate cluster quality before proceeding to summarization.
- Design tradeoffs: Tree depth vs cost (deeper trees enable better OLAP performance but increase indexing cost exponentially); chunk size vs granularity (smaller chunks improve precision but increase node count and clustering complexity); RRF constant k (lower k emphasizes top ranks; higher k flattens differences).
- Failure signatures: Low precision despite high recall (retrieved context contains relevant entities but excessive noise—tune RRF or reduce top-k); hallucinated summaries (LLM generates claims not grounded in child nodes—add grounding validation); poor OLAP performance (queries requiring synthesis return fragmented answers—increase tree depth or switch to GraphRAG).
- First 3 experiments: 1) Baseline comparison: Run TREX, RAPTOR, and Hybrid Search on a held-out OLTP dataset (e.g., HotPotQA dev split). Measure accuracy, precision/recall, and per-query cost. 2) Ablation on tree depth: Build TREX trees with depths 1, 2, 3 and evaluate on both OLTP and OLAP queries. 3) Cluster quality audit: Sample 10 GMM clusters from your corpus. For each, manually assess whether chunks share a coherent theme.

## Open Questions the Paper Calls Out

- What is the precise distribution of OLTP versus OLAP-style queries in real-world enterprise workloads, and how does it vary by industry?
- How can evaluation frameworks be expanded to assess the correctness and faithfulness of specific claims within open-ended OLAP answers?
- How does TREX performance compare to other graph-based RAG agents like PEARL, ReadAgent, or GraphReader?
- Do standard precision metrics underreport retrieval effectiveness in RAG systems because ground truth labels underrepresent relevant context?

## Limitations
- Critical hyperparameters (UMAP settings, chunking parameters, prompt templates) are not specified, making faithful reproduction challenging
- GMM clustering assumes semantic boundaries align with Gaussian distributions, which may fail on skewed text embeddings
- Truncated tree design assumes OLTP queries never require cross-document synthesis, but boundary isn't clearly defined

## Confidence
- High confidence in the core retrieval fusion mechanism (RRF combining vector and keyword search)
- Medium confidence in the hierarchical summarization tree approach (depends heavily on sensitive clustering and summarization parameters)
- Low confidence in the cost-performance tradeoff claims without access to exact hyperparameter settings

## Next Checks
1. Cluster quality audit: Sample 10 GMM clusters from your corpus and manually assess coherence. If >30% show mixed topics, investigate UMAP parameters or alternative clustering methods before proceeding.

2. Tree depth ablation: Build TREX trees with depths 1, 2, 3 and evaluate on both OLTP and OLAP queries. Identify the depth where OLAP performance degrades significantly, establishing the practical tradeoff between cost and capability.

3. RRF parameter sensitivity: Vary the RRF constant k (try 30, 60, 90) and measure impact on precision/recall for both query types. This identifies whether the default k=60 is optimal for your specific corpus characteristics.