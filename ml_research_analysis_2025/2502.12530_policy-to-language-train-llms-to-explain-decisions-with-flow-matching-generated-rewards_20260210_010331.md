---
ver: rpa2
title: 'Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching Generated
  Rewards'
arxiv_id: '2502.12530'
source_url: https://arxiv.org/abs/2502.12530
tags:
- learning
- explanation
- arxiv
- flow
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a model-agnostic explanation generator that
  trains an LLM to explain agent decisions using rewards generated by a rectified
  flow matching model. The method leverages a GUIDANCE LLM to provide positive samples
  and uses a specially designed rectified flow network with cross-attention to incorporate
  linguistic cues, enabling effective generalization to negative samples.
---

# Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching Generated Rewards

## Quick Facts
- arXiv ID: 2502.12530
- Source URL: https://arxiv.org/abs/2502.12530
- Authors: Xinyi Yang; Liang Zeng; Heng Dong; Chao Yu; Xiaoran Wu; Huazhong Yang; Yu Wang; Milind Tambe; Tonghan Wang
- Reference count: 40
- One-line primary result: Flow-matching generated rewards improve LLM explanation accuracy by 4%-20% over SFT and RLHF baselines

## Executive Summary
This paper introduces a model-agnostic method to train LLMs to generate explanations for agent decisions using rewards generated by a rectified flow-matching model. The approach leverages a Guidance LLM to identify positive samples (explanations that correctly predict decisions) and trains a rectified flow network to generate per-sentence rewards. The flow model is integrated into the Guidance LLM via cross-attention to leverage linguistic cues. Experiments on SMAC (RL) and MMLU/MathQA (LLM) tasks demonstrate significant improvements in explanation accuracy over baseline methods.

## Method Summary
The method trains an Explanation LLM to generate natural language explanations for agent decisions without seeing the actual decision. First, a Guidance LLM classifies explanations as positive or negative based on whether it can correctly predict the decision from the explanation alone. A rectified flow model is then trained on positive samples to generate decision-probability distributions. During training, the Explanation LLM is updated via PPO using sentence-level rewards derived from probability changes in the flow model's output. The process alternates between training the flow model and the Explanation LLM for two rounds, with the flow model integrated into the Guidance LLM via cross-attention to leverage linguistic cues.

## Key Results
- Outperforms SFT and RLHF baselines by 4%-20% in accuracy on SMAC, MMLU, and MathQA tasks
- Ablation studies confirm the importance of the rectified flow model for generating reliable rewards
- Cross-attention integration significantly improves generalization to negative samples
- Robust performance across different base models (Llama-3.1-8B)

## Why This Works (Mechanism)

### Mechanism 1: Flow-Matching for Denoised Reward Generation
A rectified flow model trained on positive samples can generate reliable decision-probability distributions, filtering noise from direct Guidance LLM feedback. The system identifies positive samples where the Guidance LLM correctly infers the actual decision from an explanation. A rectified flow model learns to transport Gaussian noise to target distributions via a learned ODE vector field, encouraging straight-line trajectories for fast inference. During deployment, the flow model produces distribution for each partial explanation, enabling per-sentence reward calculation as probability changes.

### Mechanism 2: Cross-Attention Linguistic Cue Integration
Embedding the rectified flow model within the Guidance LLM's last layer via cross-attention enables the flow model to leverage contextual linguistic information, improving generalization to unseen negative samples. Flow inputs are projected to "flow tokens" that attend to the Guidance LLM's final-layer hidden states. This architecture allows the flow model to selectively extract decision-relevant linguistic cues from context-explanation pairs.

### Mechanism 3: Iterative Reward-Explanation Co-Training
Alternating training between the rectified flow model and Explanation LLM enables progressive refinement—better rewards produce better explanations, which provide better training data for the flow model. The system converges as explanations become more effective at eliciting correct decision predictions. Notably, the Explanation LLM never sees the actual decision, forcing it to reason from context alone.

## Foundational Learning

- **Rectified Flow / Flow Matching**
  - Why needed here: The core reward-generation mechanism assumes familiarity with continuous normalizing flows and ODE-based generative modeling
  - Quick check question: Given initial noise z₀ ~ N(0, I) and target z₁, what is the rectified flow objective, and why does it require fewer ODE steps than diffusion models?

- **PPO (Proximal Policy Optimization)**
  - Why needed here: The Explanation LLM is trained via PPO with flow-generated rewards
  - Quick check question: In PPO, what does the clipping parameter ε control, and how does it prevent excessive policy updates?

- **Cross-Attention in Transformers**
  - Why needed here: The flow model's ability to extract linguistic cues depends on cross-attention between flow tokens and Guidance LLM hidden states
  - Quick check question: In cross-attention, how do queries from one sequence attend to keys/values from another, and what does this enable that self-attention cannot?

## Architecture Onboarding

- **Component map**: Explanation LLM (context) -> Guidance LLM (positive/negative classification) -> Rectified Flow Model (reward generation) -> Explanation LLM (PPO update)

- **Critical path**:
  1. Prompt engineering for Explanation LLM must elicit reasoning without revealing decisions
  2. Positive sample identification requires Guidance LLM to assign highest probability to correct decision
  3. Flow model training requires |A|-dimensional output matching decision space
  4. PPO reward normalization matters; per-sentence rewards are small differences

- **Design tradeoffs**:
  - Per-sentence vs. per-token rewards: Per-sentence is computationally cheaper but may miss fine-grained contributions
  - Withholding decision from Explanation LLM prevents shortcut learning but increases task difficulty
  - Two training rounds chosen empirically; more rounds risk overfitting to flow model quirks

- **Failure signatures**:
  - Low positive-sample rate: If Guidance LLM rarely assigns highest probability to correct decision
  - Reward collapse: If rewards concentrate on trivial linguistic patterns
  - Cross-attention ablation failure: If "Ours w/o Attn" matches full model performance

- **First 3 experiments**:
  1. **Positive-sample rate audit**: Compute what fraction of initial explanations are classified as positive by the Guidance LLM across each dataset
  2. **Flow model validation on held-out positives**: Train φ on 80% of positive samples, evaluate whether it reproduces correct decision distributions on held-out 20%
  3. **Ablation sequence**: Run (a) full model, (b) Ours w/o Flow, (c) Ours w/o Attn on a single dataset to verify performance gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the flow-matching reward mechanism be extended to general LLM alignment tasks (e.g., helpfulness) beyond specific explanation generation?
- Basis in paper: The authors state in the conclusion: "we envision extending this method to a general LLM training approach... ultimately reducing the reliance on human feedback."
- Why unresolved: The current study only validates the method on decision explanation tasks rather than broader alignment objectives
- What evidence would resolve it: Successful application to standard RLHF benchmarks where rewards approximate human preferences without labeled data

### Open Question 2
- Question: How does the method scale to environments with continuous action spaces or decision sets of very high cardinality?
- Basis in paper: The method constructs the flow target in ℝ|A| where |A| is the number of discrete decisions
- Why unresolved: The architecture relies on a fixed output dimension for the flow model matching the number of decisions
- What evidence would resolve it: Adapting the output representation to handle continuous control tasks or ranking tasks with thousands of candidates

### Open Question 3
- Question: Does optimizing for decision "reconstructibility" guarantee that explanations are faithful to the agent's actual internal reasoning?
- Basis in paper: The method optimizes based on whether a third party can "reconstruct agent decisions" rather than verifying if the explanation matches the agent's ground-truth causal logic
- Why unresolved: The model may learn to generate plausible rationalizations that correlate with the decision without accurately reflecting the true reasoning process
- What evidence would resolve it: Evaluations comparing generated explanations against ground-truth reasoning traces or measuring intervention success rates

## Limitations
- Limited validation of how representative positive samples are of true causal explanations versus surface-level patterns
- Cross-attention mechanism's effectiveness depends on Guidance LLM hidden states containing meaningful decision-relevant information
- Two-round training procedure is empirically chosen without theoretical justification for convergence

## Confidence

- **High confidence**: The core mechanism of using rectified flow to generate rewards for explanation training is technically sound and well-supported by the mathematical formulation
- **Medium confidence**: The empirical results showing 4%-20% accuracy improvements over baselines are convincing within the experimental setup
- **Low confidence**: The claim that the flow model can effectively generalize from positive to negative samples relies heavily on the assumption that linguistic patterns are transferable

## Next Checks

1. **Positive-sample audit**: Measure the initial positive-sample rate across datasets to ensure sufficient training data for the flow model
2. **Flow model generalization test**: Evaluate the flow model's accuracy on held-out negative samples to confirm it learns meaningful linguistic patterns
3. **Ablation validation**: Run the full ablation sequence on a single dataset to verify the reported performance gaps and investigate any discrepancies