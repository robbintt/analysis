---
ver: rpa2
title: 'Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs'
arxiv_id: '2506.19185'
source_url: https://arxiv.org/abs/2506.19185
tags:
- spiritual
- mental
- response
- user
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GITes, a dataset that integrates spiritual
  teachings from the Bhagavad Gita into mental health dialogues by enriching the existing
  ExTES dataset with 10,729 spiritually guided responses. The authors benchmark GITes
  against 12 state-of-the-art LLMs and propose a novel Spiritual Insight metric along
  with an LLM-as-Jury evaluation framework.
---

# Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs

## Quick Facts
- arXiv ID: 2506.19185
- Source URL: https://arxiv.org/abs/2506.19185
- Reference count: 40
- One-line primary result: GITes dataset integrates Bhagavad Gita teachings with mental health dialogues; fine-tuned models show significant improvements in both NLP and spiritual metrics

## Executive Summary
This paper introduces GITes, a dataset that integrates spiritual teachings from the Bhagavad Gita into mental health dialogues by enriching the existing ExTES dataset with 10,729 spiritually guided responses. The authors benchmark GITes against 12 state-of-the-art LLMs and propose a novel Spiritual Insight metric along with an LLM-as-Jury evaluation framework. Fine-tuned models, especially Phi3-Mini 3.2B Instruct, show significant improvements in NLP and spiritual metrics compared to zero-shot baselines—achieving 122.71% in ROUGE, 126.53% in METEOR, 8.15% in BERT score, 15.92% in Spiritual Insight, 18.61% in Sufficiency, and 13.22% in Relevance. The study demonstrates that integrating spiritual guidance enhances AI-driven mental health support, though further validation in real-world settings is needed.

## Method Summary
The study builds GITes by collecting Bhagavad Gita shlokas with purports, categorizing them by emotion situations, and using GPT-4o to generate spiritual responses mapped to user emotional states. The dataset (15,578 samples) is used to fine-tune 12 LLMs using LoRA (rank=16) with strategy-conditioned prompts. Evaluation combines traditional NLP metrics (ROUGE, METEOR, BERTScore, BLEU) with LLM-as-Jury assessment using three judge models (LLaMA 3.1 8B, Mistral 7B, DeepSeek R1 Distill LLaMA 8B) to rate Spiritual Insight, Sufficiency, and Relevance on 1-5 scales.

## Key Results
- Fine-tuned models significantly outperform zero-shot baselines across all NLP metrics: 122.71% ROUGE, 126.53% METEOR, 8.15% BERTScore
- Spiritual metrics show consistent improvement: 15.92% in Spiritual Insight, 18.61% in Sufficiency, 13.22% in Relevance
- Phi3-Mini 3.2B Instruct emerges as top performer with ROUGE 0.681, METEOR 0.485, BERTScore 0.838, Spiritual Insight 4.035, Sufficiency 4.114, Relevance 4.173
- Error analysis reveals models sometimes provide spiritual comfort without actionable guidance for practical problems

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on spiritually-enriched dialogue data causes models to generate contextually appropriate spiritual guidance rather than generic empathetic responses. The fine-tuning process updates model weights to predict tokens that align spiritual teachings with user emotional states. The preference flag P conditions response generation toward spiritual vs. non-spiritual output based on AI strategy.

### Mechanism 2
Emotion-to-shloka mapping enables relevant spiritual guidance by linking user emotional states to canonical teachings. GPT-4o analyzes user query and conversation context, predicts emotion from predefined list, then matches against shloka "situations" extracted from purports. This creates a retrieval-like grounding where responses draw from established spiritual wisdom.

### Mechanism 3
LLM-as-Jury evaluation with chain-of-thought prompting enables scalable assessment of spiritual depth that n-gram metrics cannot capture. Three judge models independently rate responses on Spiritual Insight, Sufficiency, and Relevance using detailed prompts. Aggregated ratings reduce individual model bias and provide nuanced evaluation of spiritual appropriateness.

## Foundational Learning

- **Concept:** Low-Rank Adaptation (LoRA)
  - **Why needed here:** Fine-tuning 7B+ parameter models requires memory-efficient training; LoRA enables adaptation without updating full weight matrices.
  - **Quick check question:** Can you explain why LoRA rank=16 was chosen and how it affects the tradeoff between adaptation capacity and memory usage?

- **Concept:** Strategy-conditioned response generation
  - **Why needed here:** The framework distinguishes 9 spiritual strategies from 6 non-spiritual strategies—understanding when to apply each is critical for appropriate deployment.
  - **Quick check question:** Given a user query about job interview preparation, which strategy category applies and should the response be spiritual or non-spiritual?

- **Concept:** Embedding similarity limitations for semantic evaluation
  - **Why needed here:** The paper explicitly rejects cosine similarity for spiritual metrics due to "embedding saturation, geometric biases and lack of contextual nuance."
  - **Quick check question:** Why would two responses with high BERTScore potentially differ significantly in Spiritual Insight ratings?

## Architecture Onboarding

- **Component map:** Shloka collection → Situation extraction → Emotion prediction & shloka mapping → Spiritual response generation → LoRA fine-tuning → LLM-as-Jury evaluation
- **Critical path:** Understanding the dataset creation pipeline is essential—without grasping how shlokas map to emotions via purport situations, you cannot debug or extend the system. The Phase 3 mapping logic is the core integration point.
- **Design tradeoffs:** Spiritual fidelity vs. actionability; cultural specificity vs. generalizability; automated evaluation vs. clinical validation.
- **Failure signatures:** Spiritual responses for practical queries; generic spiritual statements when specific emotional nuance needed; relevance degradation for some models after fine-tuning.
- **First 3 experiments:**
  1. Run zero-shot inference with base models on 100 test queries to establish spiritual response baseline before fine-tuning
  2. Fine-tune with vs. without AI strategy labels to measure impact of explicit strategy supervision
  3. Compare LLM-as-Jury ratings against human expert ratings on held-out set to quantify evaluation reliability

## Open Questions the Paper Calls Out
- Does integrating GITes improve therapeutic outcomes in real-world patient populations compared to standard AI-therapy baselines?
- Can the Spiritual-LLM framework be effectively generalized to other spiritual traditions while maintaining theological accuracy?
- How can the system be improved to balance deep spiritual insight with actionable, practical guidance?

## Limitations
- The emotion-to-shloka mapping relies on GPT-4o's judgment rather than extensive human validation
- Evaluation framework depends entirely on LLM-as-Jury assessment without clinical validation on real patients
- Single-text grounding (Bhagavad Gita) creates cultural specificity that may not generalize beyond Hindu contexts
- Error analysis reveals models sometimes provide spiritual comfort without actionable guidance

## Confidence
- **High Confidence:** Technical execution of LoRA fine-tuning and LLM-as-Jury evaluation framework; NLP metric improvements are objectively measurable
- **Medium Confidence:** Claim that spiritual integration enhances mental health support rests on LLM-as-Jury evaluations without clinical validation
- **Low Confidence:** Fundamental assumption that systematically mapping Bhagavad Gita shlokas to emotional states produces therapeutically beneficial outcomes lacks clinical validation

## Next Checks
1. Conduct a randomized controlled trial comparing responses from fine-tuned spiritual models against standard empathetic models with actual mental health service users
2. Replicate the entire framework using spiritual texts from different traditions (Buddhist sutras, Christian scripture, Islamic teachings) to assess generalizability
3. Have licensed mental health professionals rate 500 generated responses across all models on therapeutic appropriateness, safety, and practical utility, comparing against LLM-as-Jury scores