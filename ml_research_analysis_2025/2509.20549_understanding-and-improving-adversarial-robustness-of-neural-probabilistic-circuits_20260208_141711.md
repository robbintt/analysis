---
ver: rpa2
title: Understanding and Improving Adversarial Robustness of Neural Probabilistic
  Circuits
arxiv_id: '2509.20549'
source_url: https://arxiv.org/abs/2509.20549
tags:
- rnpc
- attribute
- adversarial
- robustness
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes and improves the adversarial robustness of
  Neural Probabilistic Circuits (NPCs), a class of interpretable concept bottleneck
  models. NPCs combine an attribute recognition model with a probabilistic circuit
  to produce compositional and interpretable predictions.
---

# Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits

## Quick Facts
- arXiv ID: 2509.20549
- Source URL: https://arxiv.org/abs/2509.20549
- Reference count: 40
- Key outcome: This paper analyzes and improves the adversarial robustness of Neural Probabilistic Circuits (NPCs), a class of interpretable concept bottleneck models. NPCs combine an attribute recognition model with a probabilistic circuit to produce compositional and interpretable predictions. The authors theoretically demonstrate that NPC's robustness solely depends on the attribute recognition model and is independent of the probabilistic circuit's robustness. To improve robustness, they propose Robust Neural Probabilistic Circuits (RNPC), which introduces a novel class-wise integration inference approach. Theoretical analysis shows RNPC achieves provably improved adversarial robustness compared to NPC under certain conditions. Empirical results on four image classification datasets show RNPC outperforms existing concept bottleneck models in robustness against three types of adversarial attacks while maintaining high accuracy on benign inputs. Specifically, RNPC achieves up to 45% higher adversarial accuracy than the best baseline when the number of attacked attributes does not exceed the dataset radius.

## Executive Summary
This paper addresses the adversarial robustness of Neural Probabilistic Circuits (NPCs), interpretable models that combine attribute recognition with probabilistic reasoning. The authors theoretically prove that NPC robustness depends solely on the attribute recognition model, not the probabilistic circuit. To improve this, they propose Robust Neural Probabilistic Circuits (RNPC) with a novel class-wise integration inference approach that achieves provably better robustness. Experiments on four datasets show RNPC outperforms baselines in both benign and adversarial accuracy, with up to 45% higher adversarial accuracy when attack limits are respected.

## Method Summary
The paper proposes RNPC, which improves upon Neural Probabilistic Circuits (NPCs) by using a class-wise integration inference approach to defend against white-box, norm-bounded, untargeted adversarial attacks targeting the attribute recognition model. The method involves training independent two-layer MLPs per attribute on annotated datasets, learning probabilistic circuit structure via LearnSPN, and implementing class-wise integration inference. The inference aggregates probability mass over class neighborhoods rather than exact attribute instantiations. The approach is evaluated on four custom datasets with varying attribute counts and classes, using ∞-norm PGD, 2-norm PGD, and CW attacks.

## Key Results
- RNPC achieves up to 45% higher adversarial accuracy than the best baseline when the number of attacked attributes does not exceed the dataset radius
- RNPC maintains high benign accuracy while improving robustness, showing minimal trade-off between accuracy and robustness
- The robustness improvement depends critically on the dataset radius r, which must be properly computed as ⌊(dmin-1)/2⌋ where dmin is the minimum inter-class distance

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Robustness Propagation
In a Neural Probabilistic Circuit (NPC), the adversarial robustness of the final prediction is strictly bottlenecked by the attribute recognition model and is independent of the probabilistic circuit's robustness. Under the complete information assumption, the prediction perturbation (Total Variation distance) of the overall model is upper-bounded by the sum of the perturbation errors of the individual attribute predictors. The reasoning module merely combines these probabilities; it does not amplify or dampen the adversarial noise inherently. If attributes are highly correlated given X (violating Assumption 3.2), the error propagation might not decompose linearly, potentially allowing the circuit to influence robustness.

### Mechanism 2: Class-wise Neighborhood Integration (RNPC)
Replacing the standard node-wise integration with class-wise neighborhood integration provably improves robustness against bounded perturbations. Standard NPC integrates probabilities over exact attribute instantiations (nodes). RNPC integrates probability mass over a "neighborhood" N(ỹ, r)—a ball of radius r around the high-probability attributes for a class. As long as an adversarial attack shifts the predicted attribute probability mass within this neighborhood, the class-level prediction remains stable because the integral over the region is unchanged. If the attacker perturbs more than r attributes simultaneously, the probability mass shifts out of the neighborhood N(ỹ, r), causing a prediction flip.

### Mechanism 3: Robustness-Accuracy Trade-off
There is a quantifiable "price of robustness" where the optimal robust model diverges from the ground-truth data distribution. By enforcing prediction consistency over a neighborhood N(y, r), the model effectively averages the ground-truth conditional probabilities within that neighborhood. If the data distribution is not uniform or smooth within N(y, r), this averaging introduces an error, representing the cost paid in benign accuracy to gain robustness. If distinct classes exist very close to each other in the attribute space (small dmin), the radius r must be small, limiting the robustness gains; conversely, forcing a large r on a dataset with small dmin collapses accuracy.

## Foundational Learning

- **Concept: Probabilistic Circuits (PCs)**
  - Why needed here: The paper leverages PCs as the reasoning engine. You must understand how PCs allow tractable computation of marginal and conditional probabilities (P(Y|A)) which would be intractable in standard deep nets.
  - Quick check question: Can you explain how a Sum-Product Network computes the partition function efficiently compared to a standard Bayesian Network?

- **Concept: Concept Bottleneck Models (CBMs)**
  - Why needed here: NPCs/RNPCs are a specific instance of CBMs. Understanding the standard CBM architecture (Predictor on top of Concepts) highlights why the "Black Box" attribute recognizer is the security liability.
  - Quick check question: In a standard CBM, does a linear predictor on concepts amplify or ignore errors in the concept predictions? (Hint: Linear layers typically propagate errors directly).

- **Concept: Total Variation (TV) Distance**
  - Why needed here: TV distance is the primary metric used to quantify "prediction perturbation" and robustness throughout the paper's theoretical proofs.
  - Quick check question: If P and Q are two probability distributions, what does a TV distance of 0.5 signify about the overlap between the distributions?

## Architecture Onboarding

- **Component map:** Input X -> Attribute Recognition Model (θ) -> Probabilistic Circuit (w) -> Class-wise Integration Inference (RNPC)
- **Critical path:** The efficiency of RNPC depends on calculating P(A₁:ₖ ∈ N(ỹ, r)|X). If the attribute space |A| is large, iterating over the neighborhood nodes can be expensive, though Proposition 4.4 claims it is more efficient than NPC's node-wise integration.
- **Design tradeoffs:**
  - Radius (r) Selection: Ideally set to r* = ⌊(dmin-1)/2⌋. Increasing r improves robustness but risks smoothing over distinct classes (accuracy drop). Setting r=0 reverts to standard NPC.
  - Attribute Granularity: More attributes increase computational complexity O(|S| · |V|) but may increase the radius r if inter-class distance grows faster than noise.
- **Failure signatures:**
  - Attack Propagation: If the Attribute Recognition Model learns spurious correlations (e.g., "Red" always implies "Circle"), attacking "Red" inadvertently flips "Circle". This effectively increases the number of attacked attributes, potentially exceeding the radius r.
  - Zero Radius: On datasets with high class overlap (small dmin), r might be 0. RNPC then provides no robustness benefit over NPC.
- **First 3 experiments:**
  1. Sanity Check (Node vs. Class-wise): Implement both Eq (1) (NPC) and Eq (2) (RNPC) on MNIST-Add3. Verify that NPC accuracy drops to ~0% when attributes are flipped, while RNPC maintains high accuracy as long as flipped attributes ≤ r.
  2. Radius Sensitivity: On MNIST-Add5 (r*=2), vary the hyperparameter r from 0 to 5. Plot the "cliff" where benign accuracy drops (expected at r=5) and observe the peak robustness at r=r*.
  3. Spurious Correlation Stress Test: Train the Attribute Recognition model on GTSRB-Sub without adversarial training. Attack one attribute and measure the accuracy drop of unattacked attributes to quantify the "Attack Propagation" effect described in Section 5.3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can RNPC be effectively adapted to large-scale datasets like ImageNet that lack explicit concept annotations by leveraging multi-modal models to transfer concepts?
- **Basis in paper**: [explicit] Appendix B states: "we can extend RNPC to annotation-free settings by using multi-modal models [46] to transfer concepts from other datasets or from natural language descriptions of concepts."
- **Why unresolved**: The current implementation assumes the availability of ground-truth concept annotations, which restricts its application to curated datasets.
- **What evidence would resolve it**: Successful deployment of RNPC on standard benchmarks (e.g., CIFAR-10) using generated concepts, showing robustness comparable to annotated settings.

### Open Question 2
- **Question**: How does modeling the full joint distribution of attributes, rather than assuming conditional independence, affect the robustness guarantees and computational tractability of RNPC?
- **Basis in paper**: [explicit] Appendix B suggests: "we consider the possibility of relaxing [Assumption 3.2] in future work. When the complete information assumption does not hold, one could instead model the full joint distribution."
- **Why unresolved**: The theoretical bounds in the paper rely on Assumption 3.2 (conditional independence), which simplifies inference but may not hold for complex data.
- **What evidence would resolve it**: Derivation of new robustness bounds that account for attribute correlations and empirical validation on datasets specifically designed to violate conditional independence.

### Open Question 3
- **Question**: Can the intrinsic radius of a dataset be systematically augmented to improve RNPC robustness without requiring extensive manual attribute engineering?
- **Basis in paper**: [explicit] Appendix B notes that "For real-world datasets, the radius can be very small... augmenting this radius can enhance RNPC's performance" and suggests repeating attributes or introducing new ones.
- **Why unresolved**: The paper suggests naive approaches like attribute repetition, but a systematic method to increase the dataset radius (and thus robustness) for arbitrary data is not established.
- **What evidence would resolve it**: An algorithm that automatically generates or selects attributes to maximize the inter-class distance/radius, resulting in higher adversarial accuracy.

## Limitations
- The theoretical framework relies heavily on Assumption 3.2 (Complete Information), which states attributes are conditionally independent given X. This assumption may not hold in real-world datasets with correlated attributes.
- The robustness benefits depend critically on the dataset radius r, which requires careful calculation of inter-class distances that may not be straightforward for complex attribute spaces.
- The paper acknowledges that attack propagation from spurious correlations can degrade performance, particularly on datasets like GTSRB-Sub where attacking one attribute affects others.

## Confidence
- **High Confidence**: The decoupling of robustness from the probabilistic circuit (Mechanism 1) - the proof is straightforward and the independence claim follows directly from the TV distance bound.
- **Medium Confidence**: The class-wise integration improvement (Mechanism 2) - while the theoretical bound is sound under Differential Privacy conditions, the practical benefits depend on proper radius selection and the assumption that attacks don't exceed r attributes.
- **Medium Confidence**: The robustness-accuracy trade-off analysis (Mechanism 3) - the theoretical bound is valid, but empirical validation shows the trade-off is negligible on tested datasets, suggesting the bound may be conservative.

## Next Checks
1. **Assumption Validation**: Measure attribute correlations in the four test datasets to quantify violations of Assumption 3.2 and measure the resulting degradation in RNPC's theoretical robustness bounds.
2. **Radius Sensitivity Analysis**: Systematically vary r beyond the optimal ⌊(dmin-1)/2⌋ on MNIST-Add5 to map the full accuracy-robustness trade-off curve and identify the exact point where robustness benefits plateau.
3. **Attack Propagation Quantification**: Design a controlled experiment where the attribute recognition model is deliberately trained with known spurious correlations, then measure how attacking one attribute degrades others to quantify the attack propagation effect described in Section 5.3.