---
ver: rpa2
title: Bayesian Optimization of a Lightweight and Accurate Neural Network for Aerodynamic
  Performance Prediction
arxiv_id: '2503.19479'
source_url: https://arxiv.org/abs/2503.19479
tags:
- optimization
- number
- mape
- performance
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Bayesian optimization framework for tuning
  neural network hyperparameters in aerodynamic performance prediction. The approach
  uses hierarchical and categorical kernels to efficiently explore mixed-variable
  hyperparameter spaces, including continuous, integer, ordinal, and categorical parameters.
---

# Bayesian Optimization of a Lightweight and Accurate Neural Network for Aerodynamic Performance Prediction

## Quick Facts
- arXiv ID: 2503.19479
- Source URL: https://arxiv.org/abs/2503.19479
- Reference count: 40
- Primary result: Bayesian optimization with hierarchical/categorical kernels reduces MAPE from 0.1433% to 0.0163% for drag coefficient prediction

## Executive Summary
This paper presents a Bayesian optimization framework that tunes neural network hyperparameters for aerodynamic performance prediction, achieving superior accuracy with fewer parameters than existing models. The approach integrates hierarchical and categorical kernels to efficiently explore mixed-variable hyperparameter spaces, while the Levenberg-Marquardt algorithm provides fast convergence for trainable parameters. Results show MAPE reduced from 0.1433% to 0.0163% for drag coefficient prediction, and 0.82% for aircraft self-noise estimation compared to 3.40% for previous methods. The optimized networks achieve better accuracy while using fewer parameters, demonstrating enhanced scalability for large-scale multidisciplinary design optimization problems.

## Method Summary
The framework uses Bayesian Optimization (BO) with SMT 2.0 to tune neural network hyperparameters including number of layers (2-3), neurons per layer (10-80), and activation functions (ReLU, tanh, sigmoid). The Levenberg-Marquardt algorithm optimizes trainable parameters through second-order convergence using Jacobian-based updates. Mixed-categorical kernels handle continuous, integer, ordinal, and categorical variables, with hierarchical structure where the number of layers controls neuron availability in subsequent layers. The method uses 80/10/10 train/validation/test splits with z-score normalization and early stopping based on validation MSE.

## Key Results
- MAPE reduced from 0.1433% to 0.0163% for drag coefficient prediction on airfoil dataset
- Aircraft self-noise MAPE improved from 3.40% to 0.82% compared to previous best methods
- Optimized 3-layer network (6,521 parameters) outperforms larger models like NeuralFoil's 5,313-parameter "small" model (0.124% MAPE)
- Parameter Efficiency metric demonstrates superior accuracy per parameter compared to baseline architectures

## Why This Works (Mechanism)

### Mechanism 1: Structured Bayesian Optimization for Mixed-Variable Spaces
The integration of hierarchical and categorical kernels enables efficient exploration of hyperparameter spaces with continuous, integer, ordinal, and categorical variables. SMT 2.0 defines variables with specific types and roles, allowing the Gaussian Process surrogate to model the response surface accurately using appropriate distance metrics. This structured kernel ensures meaningful suggestions, like only evaluating neurons in a third layer if "number of layers" is set to 3.

### Mechanism 2: Levenberg-Marquardt Algorithm for Second-Order Convergence
The LM algorithm provides faster convergence and higher final accuracy compared to first-order methods by using an approximation of the Hessian matrix derived from the Jacobian of errors. This second-order information allows the optimizer to take more direct steps toward minimizing the Mean Squared Error loss landscape, efficiently navigating the parameter space of moderately-sized networks.

### Mechanism 3: Parameter Efficiency via Targeted Hyperparameter Search
A compact neural network architecture discovered through targeted Bayesian optimization achieves superior accuracy and resource efficiency compared to larger unoptimized models. The framework optimizes key hyperparameters to find a "sweet spot" in model capacity—sufficient to learn aerodynamic relationships but small enough to avoid overfitting and maintain computational efficiency for large-scale MDO applications.

## Foundational Learning

### Concept: Gaussian Process (GP) Kernels
**Why needed here:** The entire BO framework relies on a GP as a surrogate model to predict model performance based on hyperparameters. Understanding that the kernel function defines how the GP measures similarity between data points is crucial to grasping why special kernels for categorical and ordinal variables are a key contribution.
**Quick check question:** How does using a standard continuous kernel for a categorical variable (like "activation function") potentially mislead the optimization process?

### Concept: The Levenberg-Marquardt Algorithm
**Why needed here:** This is the chosen optimizer for the neural network weights. It is critical to understand that it is a second-order method that blends gradient descent and Gauss-Newton, and that it uses the Jacobian matrix. This explains both its speed and its limitation to moderate-sized networks.
**Quick check question:** Why does the LM algorithm include a damping factor, and how does its value steer the update step toward either gradient descent or Gauss-Newton behavior?

### Concept: Hyperparameter Types in Search Spaces
**Why needed here:** The paper's core methodological contribution is handling different variable types and roles. Distinguishing between an *integer* (e.g., count), an *ordinal* variable (e.g., a setting with a clear order but not a continuous scale), and a *categorical* variable (e.g., a choice with no order) is essential for understanding the paper's use of SMT 2.0.
**Quick check question:** In the context of this paper, why is the number of neurons in a layer treated as an ordinal variable with a fixed interval instead of a continuous integer?

## Architecture Onboarding

### Component map:
1. Data Source: CFD simulation results for airfoils (14 design vars + 2 flow conditions -> CL, CD)
2. Neural Network (MLP): Shallow Multi-Layer Perceptron with tunable hyperparameters (layers, neurons, activation)
3. Bayesian Optimization (BO) Loop: Meta-optimizer using Design of Experiments to propose hyperparameter configurations
4. GP Surrogate Model: Models relationship between hyperparameters and validation MAPE using mixed-categorical kernels
5. Acquisition Function: Decides next hyperparameter configuration, balancing exploration and exploitation
6. SMT 2.0: Software framework implementing GP, kernels, and acquisition function

### Critical path:
1. Define hyperparameter search space (Table 1), specifying variable types (Ordinal, Categorical) and roles (Meta, Decreed)
2. Initialize BO with small Design of Experiments (DoE)
3. Inner Loop: For each hyperparameter configuration, build NN, train with LM algorithm, evaluate performance (MAPE) on validation set
4. Outer Loop: Feed validation MAPE back to BO process, update GP surrogate model, use acquisition function to select next configuration
5. Repeat until convergence or budget exhaustion

### Design tradeoffs:
- Network Size vs. Optimizability: Smaller networks are easier to optimize and more computationally efficient, though larger networks could theoretically perform better
- LM vs. First-Order Optimizers: LM chosen for speed and accuracy on moderate-sized networks but becomes computationally expensive for very deep architectures
- Exploration vs. Exploitation: BO acquisition function manages this tradeoff; poor choices lead to local optima or wasted resources

### Failure signatures:
- Overfitting: CV MSE stops decreasing while training MSE continues to drop
- Non-convergence of LM: Damping factor management issues or ill-conditioned problems
- Poor GP Surrogate: Kernels don't match data structure, leading to poor hyperparameter suggestions
- Overly Constrained Search Space: Optimal solution lies outside searchable region

### First 3 experiments:
1. **Reproduce Baseline:** Implement 2-layer MLP with 20 neurons per layer and tanh activation. Train with standard optimizer (Adam) and LM. Compare convergence speed and final MAPE to verify LM advantage.
2. **Ablation on Variable Types:** Run BO treating all hyperparameters as continuous vs. properly defining ordinal/categorical types. Compare iterations needed to reach target MAPE.
3. **Parameter Efficiency Sweep:** Train networks with increasing size (1-5 layers, varying neurons). Plot MAPE vs. parameters. Mark BO-optimized model performance to verify superior efficiency claims.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can fused activation functions via weighted averages enhance model flexibility and predictive accuracy?
**Basis in paper:** Conclusion states "it is possible to fuse different activation functions to form a generic one by weighted average... This will be one of the directions of our ongoing research."
**Why unresolved:** Current study optimizes single fixed activation function per network but doesn't explore hybrid or weighted combinations.
**What evidence would resolve it:** Comparative study evaluating networks utilizing proposed fused activation layers against current single-function baselines.

### Open Question 2
**Question:** How does computational efficiency degrade as hyperparameter search space dimensionality increases significantly?
**Basis in paper:** Paper notes "GP models do not handle high-dimensional design spaces well" and training time escalates rapidly with input dimensions, despite KPLS mitigation.
**Why unresolved:** Current study explores only 5 hyperparameters; unclear if BO formulation remains efficient for complex architectures requiring significantly more hyperparameters.
**What evidence would resolve it:** Scalability benchmarks showing optimization wall-clock time and convergence behavior as tunable parameters increase to 20+ variables.

### Open Question 3
**Question:** Does reliance on Levenberg-Marquardt algorithm limit framework applicability to larger deep-learning-scale networks?
**Basis in paper:** Text acknowledges LM is "particularly effective for training moderate-sized neural networks," implying computational/memory constraints when scaling to very deep architectures.
**Why unresolved:** Paper targets "lightweight" networks but claims applicability to "large-scale" MDO problems, creating ambiguity regarding upper bound of trainable network size.
**What evidence would resolve it:** Performance analysis comparing LM against first-order optimizers (like Adam) on networks with significantly increased depth and parameter counts.

## Limitations
- Airfoil dataset source is not specified, making exact reproduction difficult
- BO budget (iterations) and acquisition function choice are unspecified
- Claim that LM outperforms first-order methods assumes moderate-sized networks are optimal, may not generalize to more complex problems
- Limited exploration of scalability to very deep architectures

## Confidence
- **High Confidence:** MAPE improvements on self-noise dataset (3.40% → 0.82%) are verifiable since dataset is publicly available
- **Medium Confidence:** Claims about hierarchical BO efficiency improvements, as methodology is clearly specified but requires proprietary airfoil dataset
- **Low Confidence:** Generalization to other aerodynamic problems and assertion that LM is universally superior to first-order methods for moderate-sized networks

## Next Checks
1. **Reproduce self-noise results:** Implement BO framework with LM optimizer on publicly available Kaggle dataset to verify 0.82% MAPE claim
2. **Ablation study on variable types:** Compare BO performance when treating all hyperparameters as continuous versus properly defining ordinal/categorical types to quantify efficiency gains
3. **Architectural scalability test:** Evaluate whether BO-optimized architecture maintains accuracy when scaled to more complex aerodynamic problems with higher dimensionality