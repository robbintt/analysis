---
ver: rpa2
title: 'FACTS: Table Summarization via Offline Template Generation with Agentic Workflows'
arxiv_id: '2510.13920'
source_url: https://arxiv.org/abs/2510.13920
tags:
- table
- template
- facts
- query
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FACTS introduces an agentic workflow that generates reusable offline\
  \ templates\u2014consisting of schema-aware SQL queries and Jinja2 templates\u2014\
  for query-focused table summarization. By validating each component through an LLM\
  \ Council ensemble and iterating based on execution feedback, FACTS ensures accurate,\
  \ fast, and privacy-compliant outputs without exposing raw table data."
---

# FACTS: Table Summarization via Offline Template Generation with Agentic Workflows

## Quick Facts
- arXiv ID: 2510.13920
- Source URL: https://arxiv.org/abs/2510.13920
- Reference count: 40
- Primary result: FACTS consistently outperforms baselines in BLEU, ROUGE-L, and METEOR scores for query-focused table summarization

## Executive Summary
FACTS introduces an agentic workflow that generates reusable offline templates—consisting of schema-aware SQL queries and Jinja2 templates—for query-focused table summarization. By validating each component through an LLM Council ensemble and iterating based on execution feedback, FACTS ensures accurate, fast, and privacy-compliant outputs without exposing raw table data. Evaluated on FeTaQA, QTSumm, and QFMTS, FACTS consistently outperforms baselines in BLEU, ROUGE-L, and METEOR scores. Experiments show that once an offline template is generated, it can be reused across any table sharing the same schema, delivering dramatic speedups for large-scale summarization while maintaining 100% SQL execution success and high factual alignment.

## Method Summary
FACTS employs a three-stage agentic workflow for table summarization. First, schema-guided specification generates guided questions and filtering rules from the query and table schema. Second, SQL queries are synthesized from these specifications, executed locally in DuckDB, and iteratively refined based on execution errors and LLM Council feedback. Third, Jinja2 templates are generated to render SQL outputs into natural language, with alignment validated by the Council. The LLM Council—composed of GPT-4o-mini, Claude-4 Sonnet, and DeepSeek v3—validates each artifact through majority voting and provides aggregated feedback for iterative refinement. Templates are schema-bound and reusable across tables sharing the same schema, enabling privacy-preserving and efficient summarization.

## Key Results
- FACTS achieves superior BLEU, ROUGE-L, and METEOR scores compared to baselines across FeTaQA, QTSumm, and QFMTS benchmarks
- FACTS demonstrates 100% SQL execution success rate with templates that can be reused across same-schema tables
- Template reuse provides dramatic speedups for large-scale summarization while maintaining high factual alignment

## Why This Works (Mechanism)

### Mechanism 1
Factual alignment improves when natural language summaries are rendered from SQL-retrieved values through Jinja2 templates rather than free-form LLM generation. An offline template couples schema-aware SQL queries with a Jinja2 template. SQL execution produces concrete, structured values; the Jinja2 template injects these values into fixed natural language slots. This constrains the output space and grounds every stated fact in an executed query result, reducing hallucination pathways. The core assumption is that the synthesized SQL correctly captures user query semantics and executes without error; the Jinja2 template accurately references the column names returned by SQL. Evidence includes the abstract stating "accurate outputs with executable SQL queries" and SPaGe (arXiv:2507.22829) supporting execution-based grounding. Break condition: If SQL queries fail, return empty results, or the template references non-existent columns, the summary degrades or raises an error.

### Mechanism 2
Template reuse across same-schema tables amortizes generation cost and preserves data privacy. Templates are generated once using only schema metadata (column names, types), not raw table values. The resulting SQL+Jinja2 artifact is applied to any table sharing that schema. SQL executes locally; only schemas are exposed to LLMs, never cell values. The core assumption is that target tables share identical schemas and query semantics; exposing schema alone does not leak sensitive information. Evidence includes the abstract stating "without exposing raw table data" and Figure 3 showing near-constant runtime for FACTS across 1 vs. 100 tables under the same schema. Break condition: If schemas drift (renamed/dropped columns) or query semantics change, the template becomes invalid and must be regenerated.

### Mechanism 3
A multi-model LLM Council with majority voting and aggregated feedback improves correctness of intermediate artifacts over single-model validation. Multiple heterogeneous LLMs independently judge each candidate artifact (guided questions, SQL, template, summary) with structured YES/NO decisions and brief feedback. Majority vote determines acceptance; aggregated feedback guides iterative revision. This reduces reliance on any single model's blind spots. The core assumption is that diverse models provide complementary error detection; their feedback is actionable and consistent enough to drive convergence. Evidence includes the abstract stating "validating each component through an LLM Council ensemble" and Table 2 showing FACTS outperforms FACTS (GPT-Only). Break condition: If council members share systematic blind spots or produce contradictory, non-actionable feedback, iterations may not converge within the patience budget.

## Foundational Learning

- **Concept: Jinja2 templating basics**
  - Why needed here: Jinja2 templates define how SQL results are verbalized into natural language. Understanding variable substitution (`{{ row["column"] }}`), loops (`{% for row in values %}`), and conditionals (`{% if values %}`) is essential for reading, writing, and debugging templates.
  - Quick check question: Given a SQL result `[{name: "Alice", balance: 100}, {name: "Bob", balance: 200}]`, write a Jinja2 snippet that outputs "Alice has 100; Bob has 200."

- **Concept: SQL query synthesis from natural language**
  - Why needed here: The FACTS agent must translate user queries and schemas into valid DuckDB SQL. Understanding SELECT, JOIN, WHERE, GROUP BY, and aggregation is critical for anticipating what the agent should produce and diagnosing failures.
  - Quick check question: Given tables `Accounts(id, name)` and `Transactions(account_id, amount)`, write a SQL query to find the total transaction amount per account name.

- **Concept: Agentic validation loops**
  - Why needed here: FACTS relies on iterative refinement: generate → execute → validate → revise. Understanding how to parse execution errors and Council feedback into actionable revisions is central to the workflow.
  - Quick check question: If a generated SQL query fails with "column not found: balnce", what specific revision should the agent make?

## Architecture Onboarding

- **Component map:**
  1. Schema-Guided Specification and Filtering (Stage 1): Generates guided questions and filtering rules from query + schema
  2. SQL Queries Generation (Stage 2): Synthesizes SQL from specifications, executes locally in DuckDB, iteratively refines based on errors and Council feedback
  3. Jinja2 Template Generation and Alignment (Stage 3): Produces templates that render SQL outputs into natural language; validates field alignment
  4. LLM Council: Multi-model ensemble (GPT-4o-mini, Claude, DeepSeek) that votes on artifact acceptance and provides aggregated feedback
  5. DuckDB Executor: In-memory SQL execution engine; returns results or error traces to the agent

- **Critical path:**
  1. Input: user query + table schema → Stage 1 produces guided questions/filtering rules → Council validates each
  2. Validated specifications + schema → Stage 2 generates SQL → DuckDB executes → Council validates (with execution logs)
  3. Validated SQL + schema → Stage 3 generates Jinja2 template → Council checks SQL-template alignment
  4. Output: offline template (SQL queries + Jinja2 template), reusable for any same-schema table

- **Design tradeoffs:**
  - Privacy vs. expressiveness: Only schema is sent to LLMs; reasoning that depends on data distributions (e.g., "most frequent category") is not accessible at template generation time
  - Reusability vs. flexibility: Templates are schema-bound; any schema evolution requires regeneration
  - Council diversity vs. cost/latency: Heterogeneous councils improve robustness but multiply LLM calls and latency; the GPT-Only variant shows reduced but still competitive performance at lower cost

- **Failure signatures:**
  - SQL syntax errors or missing columns → DuckDB returns error trace → Council feedback triggers revision
  - Empty SQL results → Council may reject as insufficient for summarization → agent refines query or filters
  - Template–SQL misalignment (template references column not in SQL output) → Council rejects → both may be co-refined
  - Council deadlock (no clear majority) → typically handled by patience limit; may require human intervention or relaxed acceptance criteria

- **First 3 experiments:**
  1. Reproduce main result: Run FACTS vs. DirectSumm on a 100-example subset of FeTaQA. Compare BLEU/ROUGE-L/METEOR and log SQL pass rates and iteration counts
  2. Ablate the Council: Replace the multi-model council with GPT-Only (all GPT-4o-mini). Measure quality drop, iteration count, and latency to quantify the diversity benefit
  3. Test reusability: Generate an offline template on one QTSumm table, then apply it to 50 tables with the same schema. Measure total runtime and per-example summary consistency (e.g., BLEU variance) compared to regenerating from scratch for each table

## Open Questions the Paper Calls Out

- **Question:** What is the optimal trade-off between LLM Council diversity and computational cost, and can smaller, specialized models replace the heterogeneous ensemble (GPT-4o-mini, Claude, DeepSeek) without degrading validation quality?
  - Basis in paper: The authors state the full framework achieves the best performance, "attributed in part to the diversity of reasoning behaviors introduced by heterogeneous Council members," but note that the "GPT-Only" variant remains competitive
  - Why unresolved: The paper establishes that diversity helps but does not explore the minimal requirements for that diversity or the cost-to-benefit ratio of using multiple proprietary models versus a single fine-tuned validator
  - What evidence would resolve it: An ablation study varying the number and size of models in the Council, measuring validation success rates against latency and token costs

- **Question:** How robust is the offline template generation mechanism when faced with schema evolution or semantic drift in table structures over time?
  - Basis in paper: The methodology explicitly relies on the assumption that templates are "bound to both the table schema and the user query semantics" and can be reused for any table "sharing the same schema"
  - Why unresolved: Real-world databases often undergo schema changes (e.g., column renaming, type changes). The paper does not address how FACTS handles templates when the underlying schema shifts slightly, which could break the SQL/Jinja2 bindings
  - What evidence would resolve it: Experiments testing template performance on synthetic datasets where the schema is perturbed (e.g., column name synonyms, added/removed columns) after the template is generated

- **Question:** Does the use of static Jinja2 templates for natural language generation limit the fluency or stylistic variability of summaries compared to free-form LLM generation?
  - Basis in paper: Section 3.1 notes that the Jinja2 template "verbalizes SQL outputs into natural language," and Section 4.1 relies on n-gram overlap metrics (BLEU, ROUGE-L, METEOR)
  - Why unresolved: N-gram metrics often correlate poorly with human judgment regarding fluency and coherence. Templates may produce "robotic" or repetitive text, a limitation not captured by the reported automatic metrics
  - What evidence would resolve it: A human evaluation study rating FACTS-generated summaries against baselines specifically on naturalness, coherence, and stylistic variety

## Limitations
- The full prompt templates for each pipeline stage are not yet released, limiting exact replication of Council validation logic and iterative refinement behavior
- Council aggregation and feedback consolidation details are underspecified - it's unclear whether majority voting is weighted, how feedback is synthesized, or how "patience" is applied in practice
- Generalization to unseen schemas or queries requiring data-dependent reasoning (e.g., mode, percentile) that cannot be resolved from schema alone

## Confidence
- **High confidence:** Mechanism 1 (factual grounding via execution-based templates), Mechanism 2 (template reuse for privacy and speed), and the core FACT architecture (three-stage pipeline with DuckDB/SQL execution)
- **Medium confidence:** Mechanism 3 (LLM Council benefits) - while ablation shows improvement, the specific aggregation logic and heterogeneity value are not fully specified
- **Low confidence:** Generalization to unseen schemas or queries requiring data-dependent reasoning that cannot be resolved from schema alone

## Next Checks
1. **Council diversity ablation:** Compare FACTS performance when replacing the multi-model Council with a single model (GPT-Only) across all benchmarks, measuring quality drop, iteration count, and latency
2. **Schema drift resilience:** Generate a template on one table, then apply it to tables with slight schema variations (renamed columns, added/removed fields). Measure SQL execution success rate and summary accuracy
3. **Privacy verification:** Log all inputs sent to LLMs during template generation. Confirm that only schema metadata (not raw table values) is exposed, and that no sensitive data can be reconstructed from the schema alone