---
ver: rpa2
title: 'FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models'
arxiv_id: '2505.00725'
source_url: https://arxiv.org/abs/2505.00725
tags:
- answer
- bert
- financial
- answers
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FinBERT-QA introduces a financial QA system that leverages pre-trained
  BERT models to address data scarcity and language specificity in the financial domain.
  The system uses BM25 for candidate retrieval followed by BERT-based re-ranking.
---

# FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models

## Quick Facts
- arXiv ID: 2505.00725
- Source URL: https://arxiv.org/abs/2505.00725
- Authors: Bithiah Yuan
- Reference count: 0
- Improves FiQA Task 2 state-of-the-art by 16% MRR, 17% NDCG, and 21% Precision@1

## Executive Summary
FinBERT-QA addresses the challenge of financial non-factoid answer selection by combining retrieval with BERT-based re-ranking. The system tackles data scarcity and domain-specific language through a Transfer and Adapt fine-tuning approach, initializing from a BERT model pre-trained on MS MARCO before adapting to the FiQA financial QA dataset. By leveraging BM25 for candidate retrieval followed by BERT re-ranking, FinBERT-QA demonstrates significant improvements over previous deep learning and information retrieval baselines.

## Method Summary
FinBERT-QA employs a two-stage pipeline: BM25 retrieval followed by BERT-based re-ranking. The system initializes a BERT-base uncased model with weights from a checkpoint pre-trained on MS MARCO Passage Ranking, then fine-tunes on the FiQA Task 2 dataset using pointwise cross-entropy loss. For each question, BM25 retrieves the top 50 answer candidates from a corpus of 57,600 passages, which are then re-ranked by the fine-tuned BERT model to identify the most relevant answer. The approach is trained with batch size 16, learning rate 3e-6, and stops after 3 epochs.

## Key Results
- Improves FiQA Task 2 state-of-the-art by 16% on MRR@10
- Achieves 17% improvement on NDCG@10
- Demonstrates 21% improvement on Precision@1
- Shows pointwise learning is more efficient than pairwise learning
- Transfer and Adapt fine-tuning outperforms domain-specific pre-training

## Why This Works (Mechanism)
The system leverages transfer learning from a large-scale general QA dataset (MS MARCO) to overcome the data scarcity in the financial domain. By initializing with a BERT model already fine-tuned on MS MARCO, the system retains general question-answer matching capabilities while adapting to financial domain specificity through fine-tuning on FiQA data. The BM25 retrieval step provides a diverse set of candidates that the BERT re-ranker can then prioritize based on semantic relevance.

## Foundational Learning
- **BERT Architecture**: Transformer-based language model for contextual embeddings - needed for capturing semantic relationships between questions and answers
- **Transfer Learning**: Using pre-trained models as starting points for new tasks - needed to overcome data scarcity in financial QA
- **Pointwise vs Pairwise Learning**: Different approaches to learning ranking functions - needed to determine most efficient fine-tuning strategy
- **BM25 Retrieval**: Traditional information retrieval method using term frequency and inverse document frequency - needed to efficiently narrow down candidate answers before BERT re-ranking
- **Cross-Entropy Loss**: Classification loss function for binary relevance - needed to train the re-ranker on relevant vs non-relevant answers

## Architecture Onboarding

**Component Map**: Question -> BM25 Retrieval -> Top-50 Candidates -> BERT Re-ranker -> Final Answer

**Critical Path**: The pipeline's performance is bottlenecked by the 19% recall rate in BM25 retrieval, meaning 81% of test questions have no relevant answers in the candidate pool by design.

**Design Tradeoffs**: The system prioritizes efficiency by using a two-stage pipeline (retrieval + re-ranking) rather than end-to-end deep learning, accepting the recall limitation for computational feasibility.

**Failure Signatures**: Low Precision@1 scores indicate either poor recall in retrieval or ineffective fine-tuning; validation loss that decreases while accuracy fluctuates suggests catastrophic forgetting from aggressive fine-tuning.

**First Experiments**:
1. Verify BM25 recall@50 on test set to confirm 19% coverage claim
2. Re-train BERT-base on MS MARCO subset if exact checkpoint unavailable
3. Test negative sampling ratios (1:1 vs 1:3) to measure impact on Precision@1

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements are constrained by only 19% recall in BM25 retrieval, capping the upper bound of achievable results
- Exact pre-trained checkpoint specifications are unclear, creating ambiguity in exact replication
- Financial domain specificity is addressed only through fine-tuning without explicit vocabulary integration
- Negative sampling strategy and class balance during fine-tuning are not disclosed

## Confidence
- **High confidence**: The retrieval-then-re-ranking pipeline is sound and metric improvements are reproducible with identical data
- **Medium confidence**: Superiority of pointwise learning and Transfer-and-Adapt approach is supported but may not generalize
- **Low confidence**: Exact model initialization checkpoint and negative sampling details are underspecified

## Next Checks
1. Recompute Recall@50 on test set to confirm 19% coverage and quantify retrieval ceiling effect
2. Reproduce full pipeline using Hugging Face `bert-base-uncased-msmarco` checkpoint or retrain on MS MARCO subset
3. Conduct ablation study varying negative sampling ratio (1:1 vs 1:3) to measure sensitivity of Precision@1