---
ver: rpa2
title: 'GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models'
arxiv_id: '2509.18122'
source_url: https://arxiv.org/abs/2509.18122
tags:
- problem
- then
- page
- each
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GAUSS (General Assessment of Underlying
  Structured Skills in Mathematics), a benchmark that evaluates large language models''
  (LLMs) mathematical abilities across twelve core skill dimensions, grouped into
  three domains: knowledge and understanding, problem solving and communication, and
  meta-skills and creativity. By categorizing problems according to cognitive skills
  and designing tasks that isolate specific abilities, GAUSS constructs comprehensive,
  fine-grained, and interpretable profiles of models'' mathematical abilities.'
---

# GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models

## Quick Facts
- arXiv ID: 2509.18122
- Source URL: https://arxiv.org/abs/2509.18122
- Reference count: 40
- Key outcome: GAUSS evaluates LLMs across 12 mathematical skills, revealing GPT-5-thinking's strengths in intuition and weaknesses in computational skills compared to o4-mini-high

## Executive Summary
GAUSS introduces a benchmark that evaluates large language models' mathematical abilities across twelve core skill dimensions grouped into knowledge and understanding, problem solving and communication, and meta-skills and creativity. By categorizing problems according to cognitive skills and designing tasks that isolate specific abilities, GAUSS constructs comprehensive, fine-grained, and interpretable profiles of models' mathematical abilities. The benchmark reveals that GPT-5-thinking excels in recalling mathematical taxonomies, evaluating arguments, posing novel problems, writing proofs, reading advanced materials, and demonstrating strong intuition through plausibility checks, while showing limitations in theorem understanding, computational and analytical skills, problem-solving strategies, and generalization.

## Method Summary
GAUSS constructs skill profiles by evaluating LLMs on curated advanced mathematics problems across 12 distinct cognitive skills, each with specific sub-skills. The benchmark uses a Balanced Score metric that averages performance across all sub-skills equally to prevent high-volume, low-difficulty tasks from masking critical weaknesses. Problems are drawn from advanced undergraduate, graduate, and competition-level sources to reduce data contamination risk. Manual expert grading using specific rubrics evaluates responses, with plans to develop automated grading methods calibrated against human standards. The framework aims to reveal latent capabilities and fragilities masked by aggregate benchmarks.

## Key Results
- GPT-5-thinking shows strong performance in recalling mathematical taxonomies, evaluating arguments, and demonstrating intuition through plausibility checks
- The model exhibits limitations in theorem understanding, computational skills, and generalization despite strong performance in other areas
- GAUSS successfully reveals skill-specific profiles that aggregate benchmarks mask, with GPT-5-thinking achieving 50% Balanced Score in Problem-Solving Framework despite 0% in Application of Strategies
- Performance differences between GPT-5-thinking and o4-mini-high are marginal, suggesting current limitations in advancing mathematical reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Skill Isolation and Decomposition
GAUSS reveals latent capabilities and fragilities that aggregate benchmarks mask by decomposing mathematical reasoning into 12 distinct cognitive skills. The benchmark enforces a many-to-one mapping from granular cognitive skills to specific problem instances, isolating failure modes rather than assigning single correctness scores to complex tasks. This assumes mathematical proficiency is a composition of independent cognitive modules rather than a monolithic reasoning ability.

### Mechanism 2: Balanced Scoring for Diagnostic Profiling
The Balanced Score uses a uniform average of sub-skill scores rather than a weighted average, preventing models from hiding critical weaknesses behind high-volume, low-difficulty tasks. This enforces a "no weak links" evaluation standard where competence in advanced mathematics requires baseline proficiency across all foundational skills, not just strength in one area.

### Mechanism 3: Contamination-Resistant Difficulty Scaling
Targeting advanced undergraduate, graduate, and competition-level problems reduces data contamination and saturation risks found in standard datasets. By sourcing problems from lesser-known competition problems and curated textbook materials, the benchmark increases the probability that models encounter genuine reasoning challenges rather than retrieving memorized solutions.

## Foundational Learning

- **Concept: Cognitive Skill Taxonomy in Mathematics** - Why needed: To understand GAUSS, one must accept that "math ability" is a vector of skills (e.g., Intuition vs. Aesthetic Judgment vs. Computational Fluency). Quick check: Can you distinguish between a student who memorizes a proof (Memory) versus one who can adapt that proof to a new domain (Generalization)?

- **Concept: Benchmark Saturation and Contamination** - Why needed: The paper critiques existing benchmarks (GSM8K, MATH). Understanding that high accuracy on a public test set often implies memorization is crucial for valuing GAUSS's design choices. Quick check: Why does a model achieving 99% on a test set provide less information about its intelligence than a model achieving 60% on a novel test set?

- **Concept: Rubric-based Evaluation vs. Exact Match** - Why needed: GAUSS evaluates open-ended tasks like "posing novel problems" or "aesthetic judgment" which cannot be graded by exact string matching. Quick check: How would you design a rubric to grade "mathematical elegance" (Aesthetic Judgment) on a scale of 0-3?

## Architecture Onboarding

- **Component map:** Problem Store -> Evaluation Pipeline (Response Generator, Grader) -> Scoring Engine -> Profile Visualizer
- **Critical path:** 1. Finalize 12-skill taxonomy 2. Select problems uniquely testing specific skills 3. Create 0-3 scale rubrics for subjective skills 4. Run model -> Collect Response -> Grade -> Aggregate Scores
- **Design tradeoffs:** Granularity vs. Noise (Balanced Score sensitive to sub-skills with very few problems), Manual vs. Auto-grading (manual provides rigor but doesn't scale)
- **Failure signatures:** "Savant" Profile (high Weighted Score but very low Balanced Score), Prompt Brittleness (drastic score changes when instructions are rephrased)
- **First 3 experiments:** 1. Calibration Run (weak model to ensure Balanced Score identifies lack of advanced reasoning) 2. Skill Isolation Ablation (modify problem prompts to test specific strategy application) 3. Inter-Annotator Agreement (validate rubrics for subjective skills)

## Open Questions the Paper Calls Out

1. What is the correlation between automated grading methods (LLM-assisted rubric scoring) and human expert evaluation for subjective mathematical tasks like proof writing, problem posing, and aesthetic judgment? [explicit: Page 29 mentions ongoing work to calibrate auto-graders against human-graded gold standards]

2. How do mathematical skill profiles differ systematically across model families (e.g., open-source vs. proprietary, different training regimes)? [explicit: Page 28-29 states authors plan to test this framework by generating skill profiles for various models including open-source and proprietary LLMs]

3. How can contamination from training data be reliably detected and quantified for skill-based benchmarks? [explicit: Page 3 identifies "potential data contamination" as a critical limitation and states GAUSS draws from sources unlikely to appear in training data, but provides no validation methodology]

## Limitations

- Small number of problems per sub-skill (often 1-2) raises concerns about statistical reliability and potential overfitting
- Manual grading process for subjective skills introduces potential rater bias and limits scalability
- Results from evaluating GPT-5-thinking (internal/prototype model) may not translate to publicly available models

## Confidence

**Skill Isolation Effectiveness** - Medium Confidence: The decomposition framework is theoretically sound but limited problem counts and potential skill correlation may reduce practical distinctiveness.

**Contamination Resistance** - Medium Confidence: Plausible but difficult to verify; assumes knowledge of model training corpora that may not be complete or accurate.

**Diagnostic Value** - High Confidence: The benchmark's ability to reveal specific strengths and weaknesses is well-supported by the structured evaluation framework, though absolute reliability of individual skill measurements remains uncertain.

## Next Checks

1. Conduct statistical power analysis varying the number of problems per sub-skill to determine minimum sample size needed for stable skill measurements and Balanced Score reliability.

2. Evaluate multiple publicly available models (e.g., GPT-4, Claude 3.5, Gemini 1.5) on the same GAUSS problems to verify that skill profiles are consistent and not model-specific artifacts.

3. Perform blinded grading of the same responses by multiple expert annotators to establish reliability and reproducibility of subjective skill assessments, particularly for aesthetic judgment and creativity.