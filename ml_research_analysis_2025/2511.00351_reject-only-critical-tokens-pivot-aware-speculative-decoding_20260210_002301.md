---
ver: rpa2
title: 'Reject Only Critical Tokens: Pivot-Aware Speculative Decoding'
arxiv_id: '2511.00351'
source_url: https://arxiv.org/abs/2511.00351
tags:
- target
- decoding
- tokens
- utility
- draft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses slow text generation in large language models\
  \ (LLMs) by reformulating speculative decoding (SD) to prioritize task-specific\
  \ utility over exact distribution matching. Instead of rejecting draft tokens based\
  \ on strict probability ratios, the proposed Pivot-Aware Speculative Decoding (PAD)\
  \ rejects only tokens that would decrease the expected utility of the final output\u2014\
  these are called pivot tokens."
---

# Reject Only Critical Tokens: Pivot-Aware Speculative Decoding

## Quick Facts
- **arXiv ID:** 2511.00351
- **Source URL:** https://arxiv.org/abs/2511.00351
- **Reference count:** 37
- **Primary result:** Pivot-Aware Speculative Decoding achieves up to 2.5× speedup while maintaining accuracy on GSM8K, AIME24, and MBPP.

## Executive Summary
The paper introduces Pivot-Aware Speculative Decoding (PAD), a novel approach to accelerate text generation in large language models (LLMs) by rejecting only those draft tokens that would decrease the expected utility of the final output. Unlike traditional speculative decoding that relies on strict probability ratios, PAD uses a lightweight classifier to identify and reject "pivot tokens" based on target-side features such as hidden states, token probabilities, and entropy. The method achieves up to 2.5× speedup compared to target-only generation while preserving accuracy on reasoning, math, and code generation tasks. By prioritizing task-specific utility over exact distribution matching, PAD offers a more flexible and efficient decoding strategy that can be extended to various applications.

## Method Summary
PAD reformulates speculative decoding by focusing on utility preservation rather than exact distribution matching. Instead of rejecting draft tokens based on probability ratios, PAD trains a lightweight classifier to identify "pivot tokens"—those whose rejection would increase the expected utility of the final output. The classifier uses target-side features including hidden states, token probabilities, and entropy to make decisions. During decoding, only pivot tokens are rejected, allowing non-critical tokens to pass through quickly. This approach maintains task accuracy while significantly accelerating generation, as demonstrated on GSM8K, AIME24, and MBPP benchmarks.

## Key Results
- PAD achieves up to 2.5× speedup compared to target-only generation
- On GSM8K, accuracy remains at 93% with a speedup of 2.46×
- Outperforms standard speculative decoding's 1.57× speedup on the same benchmark

## Why This Works (Mechanism)
Traditional speculative decoding rejects draft tokens based on probability ratios, which can be overly conservative and reject tokens that don't significantly impact final output quality. PAD instead identifies tokens that are truly critical to task performance—the pivot tokens—and only rejects those. By training a classifier to detect these critical tokens using target-side features, PAD can maintain utility while allowing non-critical tokens to pass through quickly. This selective rejection strategy balances speed and accuracy more effectively than uniform rejection approaches.

## Foundational Learning
- **Speculative Decoding:** Why needed? Accelerates LLM inference by using a smaller draft model to propose tokens that are verified by a larger target model. Quick check: Compare generation speed with and without speculative decoding.
- **Utility Preservation:** Why needed? Ensures that speed improvements don't come at the cost of task performance degradation. Quick check: Measure accuracy retention across different speedup levels.
- **Pivot Token Detection:** Why needed? Identifies which draft tokens are critical to final output quality versus those that can be accepted without verification. Quick check: Analyze false positive/negative rates of pivot detection.
- **Target-side Feature Extraction:** Why needed? Provides the information needed to assess token criticality without requiring full target model evaluation. Quick check: Validate feature importance through ablation studies.
- **Lightweight Classification:** Why needed? Enables real-time decision making without introducing significant computational overhead. Quick check: Measure classifier latency relative to generation time.

## Architecture Onboarding

**Component Map:**
Input Text -> Draft Model -> Pivot Classifier -> Target Model Verification -> Output Text

**Critical Path:**
The critical path is Draft Model → Pivot Classifier → (optional) Target Model Verification. Tokens identified as non-pivot by the classifier bypass target verification, while pivot tokens undergo full target model checking.

**Design Tradeoffs:**
- **Speed vs. Accuracy:** More aggressive pivot detection increases speed but risks accepting low-quality tokens
- **Classifier Complexity:** More complex classifiers may improve accuracy but reduce the speed benefits
- **Feature Selection:** Richer feature sets improve detection but increase computational overhead

**Failure Signatures:**
- Accuracy degradation when pivot classifier has high false negative rate
- Reduced speedup when pivot classifier has high false positive rate
- Performance collapse when draft model quality is significantly mismatched from target

**First Experiments:**
1. Validate speedup and accuracy on a held-out benchmark not seen during pivot classifier training
2. Perform ablation study on feature importance (hidden states, probabilities, entropy)
3. Test robustness to draft model quality degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization claims lack validation on diverse task types beyond GSM8K, AIME24, and MBPP
- Feature set for pivot detection not justified with comprehensive ablation studies
- Performance scaling with draft model quality mismatch not thoroughly explored

## Confidence

**High confidence:**
- Speedup measurements and accuracy retention on evaluated benchmarks
- Methodology of using lightweight classifier for pivot detection is clearly described

**Medium confidence:**
- Utility preservation via pivot-aware rejection is well-supported on tested tasks
- Generalization claims lack broader validation

**Low confidence:**
- Claims about extensibility and universality across arbitrary tasks without additional training

## Next Checks
1. Test PAD on at least three additional task types (e.g., summarization, translation, code generation) not seen during pivot classifier training to assess cross-task generalization
2. Perform an ablation study isolating the impact of each feature (hidden states, probabilities, entropy) on pivot detection accuracy and final task performance
3. Evaluate the effect of draft model quality mismatch on PAD's speedup and utility preservation to quantify robustness to model heterogeneity