---
ver: rpa2
title: 'PARK: Personalized academic retrieval with knowledge-graphs'
arxiv_id: '2507.13910'
source_url: https://arxiv.org/abs/2507.13910
tags:
- user
- retrieval
- graph
- search
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PARK, a personalized academic retrieval system
  that leverages knowledge graph embeddings to enhance user modeling in academic search.
  The proposed two-stage approach first trains a neural language model for retrieval
  and then converts the academic citation graph into a knowledge graph, embedding
  it into the same semantic space using translational embedding techniques.
---

# PARK: Personalized academic retrieval with knowledge-graphs

## Quick Facts
- arXiv ID: 2507.13910
- Source URL: https://arxiv.org/abs/2507.13910
- Authors: Pranav Kasela; Gabriella Pasi; Raffaele Perego
- Reference count: 40
- Key outcome: PARK significantly outperforms traditional graph-based and personalized models in three out of four academic domains, achieving up to 10% improvement in MAP@100 over the second-best model

## Executive Summary
This paper introduces PARK, a personalized academic retrieval system that leverages knowledge graph embeddings to enhance user modeling in academic search. The proposed two-stage approach first trains a neural language model for retrieval and then converts the academic citation graph into a knowledge graph, embedding it into the same semantic space using translational embedding techniques. This allows user models to capture both explicit relationships and hidden structures in citation graphs and paper content. Experiments across four academic domains (Computer Science, Physics, Political Science, and Psychology) demonstrate significant performance improvements over traditional approaches.

## Method Summary
PARK uses a two-stage pipeline: (1) BM25 first-stage retrieval, (2) MiniLM neural re-ranker trained with triplet margin loss and in-batch negatives. The academic citation graph is converted into a property graph with 4 node types (Users, Documents, Venues, Affiliations) and 5 relations (wrote, cited, in_venue, affiliated, co_author). TransE/TransH KG embeddings are trained with document embeddings fixed from MiniLM, and final scores use a convex combination of BM25, MiniLM, and user similarity scores. The approach is evaluated on semi-synthetic queries from four academic domains using standard IR metrics.

## Key Results
- PARK significantly outperforms traditional graph-based and personalized models in three out of four domains
- Achieves up to 10% improvement in MAP@100 over the second-best model
- Ablation study reveals affiliation nodes contribute most significantly to performance gains (2.7-2.9% NDCG@10 increase)
- Institution context proves important for academic user modeling

## Why This Works (Mechanism)

### Mechanism 1
If document embeddings are fixed during KG training, the resulting entity embeddings (users, affiliations) are forced into the pre-existing semantic space of the content, allowing for direct vector-based similarity scoring between users and queries. PARK utilizes a "two-stage" alignment where a neural language model (MiniLM) defines a semantic space for documents, then a translational KG embedding algorithm (TransE or TransH) is run with document nodes frozen and initialized with MiniLM vectors. The optimization process (minimizing distance for valid triples like `<User, Wrote, Doc>`) moves the User vectors toward the Doc vectors, effectively mapping graph structure onto semantic space.

### Mechanism 2
Incorporating institutional affiliation nodes into the graph improves retrieval effectiveness by capturing implicit collaborative or topical clusters that author-paper links alone miss. The graph construction adds nodes for affiliations (universities/labs), and TransE/H minimizes the distance between authors and their affiliations. This groups users by institution, meaning a user querying for a topic popular in their lab (but not explicitly in their history) may surface papers from lab-mates due to the shared affiliation vector proximity.

### Mechanism 3
A convex combination of lexical (BM25), semantic (Neural), and structural (KG) scores provides a more robust ranking signal than any single representation method. PARK calculates three independent scores: BM25 handles exact keyword matches, the Neural model handles semantic synonymy, and the User Similarity score handles personalized relevance. These are weighted and summed, as personal relevance is partially orthogonal to semantic relevance.

## Foundational Learning

- **Concept: Translational Embeddings (TransE/TransH)**
  - Why needed here: PARK relies on these to map graph edges into vector operations ($h + r \approx t$). Without understanding this, the "User Embedding" generation is a black box.
  - Quick check question: If User A wrote Document B, how does TransE adjust the vector for User A relative to Document B?

- **Concept: Bi-Encoder (Siamese) Architectures**
  - Why needed here: The retrieval model uses a bi-encoder to independently embed queries and documents. This is distinct from cross-encoders and dictates how the "Dense Similarity Score" is computed.
  - Quick check question: Why does a bi-encoder allow for pre-indexing document embeddings, whereas a cross-encoder does not?

- **Concept: Closed World Assumption (CWA)**
  - Why needed here: The paper explicitly notes the use of CWA in KG construction. This implies that missing edges (citations) are treated as false, which risks false negatives if the citation graph is incomplete.
  - Quick check question: In PARK's graph, if Paper A does not cite Paper B, does the model assume they are unrelated?

## Architecture Onboarding

- **Component map:** BM25 (First-Stage Retriever) -> MiniLM (Neural Re-ranker) -> KG Constructor -> TransE/H Embedder -> Fusion Layer (Weighted Sum)
- **Critical path:** The synchronization between the Neural Re-ranker and the KG Embedder is the highest risk. You must ensure the document IDs and embeddings passed to the Graph Constructor exactly match the dimensionality and semantic space of the Neural Re-ranker (384 dimensions for MiniLM).
- **Design tradeoffs:**
  - **Fixed vs. Trainable Doc Nodes:** The paper fixes document nodes to maintain semantic integrity and speed up training. *Tradeoff:* The document vectors cannot adapt to graph structure (e.g., a paper might be semantically distant from its cluster because its text is unique, even if highly cited).
  - **TransE vs. TransH:** TransE is faster but struggles with 1-to-many relations (one author writing many papers). TransH is used to mitigate this via hyperplanes.
- **Failure signatures:**
  - **Cold Start:** A new user with no authored papers in the dataset has no node in the KG. PARK cannot generate a user embedding.
  - **Domain Mismatch:** The paper notes PARK underperforms "Popularity" baselines in Computer Science. If deploying in CS, verify if pure citation count (Popularity) is the stronger signal for your specific use case.
- **First 3 experiments:**
  1. **Baseline Fusion:** Run the pipeline with $\lambda_{KG} = 0$ (just BM25 + Neural). Establishes the upper bound of non-personalized retrieval.
  2. **Ablation on Node Types:** Run PARK using only User-Paper edges, then add Affiliations. Verify the ~2.7% NDCG lift claimed in the paper.
  3. **Dimension Alignment Check:** Visualize (t-SNE/PCA) the User embeddings and Document embeddings. If they form separate clusters rather than overlapping, the "translational" alignment failed.

## Open Questions the Paper Calls Out

### Open Question 1
Can introducing soft constraints in the loss function to allow dynamic adjustment of document nodes improve retrieval performance compared to the current fixed-node approach? The conclusion states, "Future research could explore the introduction of soft constraints in the loss function, allowing document nodes to adjust dynamically while still converging toward optimal document embeddings." This remains unresolved because the current architecture freezes document embeddings derived from the neural language model to preserve semantics, which restricts the flexibility of the knowledge graph embedding space. Evidence would require a comparative study where document nodes are updated via a soft-constraint loss function, showing statistically significant improvements in MAP or NDCG over the fixed-node PARK model.

### Open Question 2
Can a hybrid model integrating popularity-based features with knowledge graph embeddings effectively close the performance gap in the Computer Science domain? The conclusion suggests, "exploring hybrid models that integrate popularity-based features with knowledge graph embeddings to improve retrieval effectiveness." This remains unresolved because PARK underperforms compared to the simple Popularity (POP) baseline in Computer Science, suggesting that citation counts (popularity) are a stronger signal than structural relationships in this specific domain. Evidence would require a new hybrid model combining PARK with citation count features that outperforms both the standalone PARK-H and POP baselines specifically on the Computer Science benchmark.

### Open Question 3
Does the minimal contribution of venue nodes in the current ablation study indicate that venues are poor predictors of preference, or is it a limitation of the current node representation granularity? The ablation study notes that adding venue nodes yielded "minimal performance gains," but the authors interpret this as "venue context may not heavily influence individual user preferences" rather than a failure of the specific modeling technique. This remains unresolved because it's unclear if the poor performance is intrinsic to academic user behavior (users don't care about venues) or if modeling a venue as a single undifferentiated node fails to capture nuanced signals (e.g., top-tier vs. workshop venues). Evidence would require an experiment using hierarchical or weighted venue representations to see if refined venue data improves the user model.

## Limitations

- Experimental evaluation uses semi-synthetic queries generated from paper titles, which may not fully capture real user search behavior patterns
- The two-stage training approach assumes semantic and structural spaces align meaningfully, but this alignment is not explicitly validated beyond retrieval performance metrics
- PARK underperforms compared to simple Popularity baselines in Computer Science, suggesting domain-specific limitations

## Confidence

- **High Confidence**: The core mechanism of fixing document embeddings during KG training (Mechanism 1) is clearly specified and technically sound
- **Medium Confidence**: The claim that affiliation nodes improve performance (Mechanism 2) is supported by ablation results but lacks qualitative validation
- **Medium Confidence**: The fusion approach combining three scores (Mechanism 3) shows empirical gains but the optimal weighting remains dataset-dependent

## Next Checks

1. **Embedding Space Alignment**: Visualize t-SNE plots of user and document embeddings to verify they occupy the same semantic space after KG training
2. **Cold Start Evaluation**: Test PARK's performance on new users with minimal publication history to quantify cold-start limitations
3. **Query Drift Sensitivity**: Evaluate retrieval performance when users search for topics outside their historical research profile to assess robustness to intent drift