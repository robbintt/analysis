---
ver: rpa2
title: Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin
  Signal in Pan-cancer Mortality Classification
arxiv_id: '2504.10343'
source_url: https://arxiv.org/abs/2504.10343
tags:
- cancer
- shap
- dann
- domain
- survival
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of tissue-of-origin bias in pan-cancer
  survival analysis, where gene expression patterns strongly reflect cancer type rather
  than patient survival. To overcome this, a Domain-Adversarial Neural Network (DANN)
  was trained on TCGA RNA-seq data to learn domain-invariant representations focused
  on mortality rather than tissue identity.
---

# Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin Signal in Pan-cancer Mortality Classification

## Quick Facts
- arXiv ID: 2504.10343
- Source URL: https://arxiv.org/abs/2504.10343
- Reference count: 30
- The study trains a Domain-Adversarial Neural Network on TCGA RNA-seq data to learn domain-invariant representations focused on mortality rather than tissue identity, then applies layer-aware SHAP to identify biologically meaningful mortality-associated subpopulations.

## Executive Summary
This study addresses the challenge of tissue-of-origin bias in pan-cancer survival analysis, where gene expression patterns strongly reflect cancer type rather than patient survival. To overcome this, a Domain-Adversarial Neural Network (DANN) was trained on TCGA RNA-seq data to learn domain-invariant representations focused on mortality rather than tissue identity. Two interpretability strategies were compared: standard SHAP on raw inputs and a layer-aware SHAP applied to hidden activations. While vanilla SHAP remained confounded by tissue signals, the layer-aware approach revealed clearer separation by survival status and identified biologically meaningful subpopulations.

## Method Summary
The method uses a DANN architecture with three components: a feature extractor that maps 39,979 gene expression values to a 1000-dimensional latent space, a label predictor for binary mortality classification, and a domain classifier for cancer type identification. A Gradient Reversal Layer (GRL) with λ=0.01 forces the feature extractor to learn tissue-invariant representations. The model was trained for 499 epochs using 5-fold stratified cross-validation on TCGA data, with AdamW optimization and dual loss functions (BCE for labels, CrossEntropy for domains). Layer-aware SHAP values were computed on the feature extractor's dropout1 layer activations using an XGBoost explainer, then projected via UMAP for visualization.

## Key Results
- DANN achieved ~71% validation accuracy for mortality classification while domain classifier accuracy steadily declined, indicating successful tissue signal suppression
- Layer-aware SHAP manifold showed improved separation by survival status compared to raw activations and vanilla SHAP, which remained tissue-confounded
- Identified gene clusters included established mortality markers (SPP1, APOE) and novel associations like PRSS3 with pan-cancer survival

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Gradient Reversal for Domain Confusion
The model suppresses tissue-of-origin signals by maximizing the error of a domain classifier via gradient reversal, forcing the feature extractor to learn tissue-invariant representations. A Gradient Reversal Layer (GRL) is inserted between the feature extractor and the domain classifier. During backpropagation, the GRL negates the gradient from the domain classifier ($-\lambda \cdot \partial L_d / \partial \theta_f$). This creates a minimax game where the feature extractor learns features that minimize the mortality loss (Label Predictor) while simultaneously maximizing the domain classification loss (making tissue features indistinguishable).

### Mechanism 2: Layer-Aware Attribution for Disentangled Interpretability
Interpreting hidden layer activations (layer-aware SHAP) reveals survival signals that are invisible to standard input-level explainability (Vanilla SHAP). Vanilla SHAP attributes importance to input genes, which are inherently dominated by tissue-of-origin expression patterns (the confounder). The DANN's feature extractor transforms these inputs into a latent space where tissue signals are suppressed. By computing SHAP values on the activations of this hidden layer, the method attributes importance to the learned concepts rather than the raw, biased genes.

### Mechanism 3: Signal-to-Noise Enhancement via Manifold Transformation
Visualizing SHAP values as a manifold enhances the separation of survival classes by weighting dimensions by functional importance rather than raw magnitude. Raw activation manifolds are often dominated by high-magnitude neurons ("activation strength"), which can mask subtle survival signals. By computing SHAP values for these activations, dimensions are weighted by their contribution to the classification output. Projecting these importance weights into 2D using UMAP creates a "functional manifold" where clusters represent biological subpopulations rather than just high-activity groups.

## Foundational Learning

- **Concept: Adversarial Training (Minimax Game)**
  - Why needed: The core of the DANN is not just minimizing error, but simultaneously maximizing error for a specific task (domain classification). Without understanding this tension, the training dynamics are unintelligible.
  - Quick check: If the domain classifier loss decreases to near zero during training, has the adversarial training succeeded or failed? (Answer: Failed; the feature extractor is not confusing the domains).

- **Concept: Shapley Values (SHAP)**
  - Why needed: The paper relies on SHAP not just for "explainability" but as a dimensionality reduction technique. Understanding that SHAP quantifies contribution to the output is necessary to see why it cleans up the "activation magnitude" noise.
  - Quick check: Why would a gene with high expression (high activation) have a low SHAP value? (Answer: Because changing that gene's value does not change the model's prediction output).

- **Concept: Manifold Learning (UMAP)**
  - Why needed: The paper evaluates success by visualizing clusters. You must understand that UMAP projects high-dimensional data into 2D based on local neighborhood preservation to interpret the "survival separation" plots.
  - Quick check: If a UMAP projection shows clear clusters by cancer type but random noise for survival status, what does that imply about the feature space? (Answer: The features encode tissue identity strongly and survival status weakly or not at all).

## Architecture Onboarding

- **Component map:** Input (39,979 gene expressions) -> Feature Extractor (1000 neurons) -> Label Predictor (sigmoid output) and Domain Classifier (33 softmax outputs with GRL)
- **Critical path:** The gradient update for the Feature Extractor ($G_f$). It receives: 1) Standard gradients from Label Predictor (to improve survival prediction) and 2) Reversed gradients from Domain Classifier (to degrade tissue prediction). The balance of these two forces determines model quality.
- **Design tradeoffs:**
  - High λ (Adversarial Strength): Stronger tissue suppression, but risks destroying survival signal
  - Low λ: Better survival accuracy, but latent space remains tissue-biased
  - Layer Choice for Explanation: The paper selects feature_extractor.dropout1 over label_predictor layers for final SHAP analysis
- **Failure signatures:**
  - Oscillating Losses: Indicates label and domain tasks are contradictory to the point of instability
  - Domain Accuracy Stays High: Adversarial branch is ineffective; results will be tissue-biased
  - Label Accuracy ≈ Random (50%): Adversarial pressure was too strong and washed out the signal
- **First 3 experiments:**
  1. Lambda (λ) Sweep: Train 3 models with λ ∈ {0.001, 0.01, 0.1}. Plot Label Accuracy vs. Domain Accuracy to find the "Pareto frontier"
  2. Layer Probing: Train simple logistic regressions on frozen activations of $G_f$ to predict Tissue Type. If accuracy is high, the DANN has failed to achieve invariance
  3. Explainability Contrast: Generate UMAPs for (A) Raw Input Genes, (B) Raw $G_f$ Activations, and (C) SHAP-weighted $G_f$ Activations. Verify visually that (C) shows the best separation of Alive/Deceased and worst separation of Tissue Type

## Open Questions the Paper Calls Out

### Open Question 1
How can the DANN architecture be optimized to close the performance gap between training and validation in mortality classification without reintroducing tissue-specific bias? The authors state that the "performance gap between training and validation... underscores the need to improve domain generalization without suppressing label-relevant features" (Page 4).

### Open Question 2
Can synthetic balancing strategies effectively mitigate the bias in SHAP values caused by imbalanced survival and tissue classes? The authors note that vanilla SHAP was confounded by tissue signals, exacerbated by data imbalance, and that "synthetic balancing strategies—an approach not pursued here" could address this (Page 9).

### Open Question 3
Do the mortality-associated genes identified via layer-aware SHAP represent causal pan-cancer biomarkers or merely statistical artifacts of the specific DANN initialization? While the paper identifies biologically meaningful subpopulations, it relies on internal clustering metrics and literature corroboration rather than independent, external validation.

## Limitations
- The study relies exclusively on TCGA data, limiting generalizability to external cohorts
- The adversarial training mechanism depends critically on the assumption that survival signals are separable from tissue-specific patterns
- Quantitative validation of whether identified clusters represent biologically distinct survival mechanisms remains absent

## Confidence
- DANN Architecture: High - loss curves demonstrate expected training dynamics
- Interpretability Improvements: Medium - strong visual separation but limited biological validation
- Generalizability: Low - single-dataset evaluation

## Next Checks
1. Apply the trained model to an independent pan-cancer cohort (e.g., ICGC) to test domain invariance across institutions
2. Conduct controlled experiments ablating tissue-specific gene sets to quantify adversarial training's impact on survival prediction accuracy
3. Validate identified subpopulations through survival analysis and pathway enrichment across cancer types