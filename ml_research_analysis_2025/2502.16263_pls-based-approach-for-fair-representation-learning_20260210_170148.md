---
ver: rpa2
title: PLS-based approach for fair representation learning
arxiv_id: '2502.16263'
source_url: https://arxiv.org/abs/2502.16263
tags:
- fair
- representation
- learning
- dataset
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Fair Partial Least Squares (FPLS) to address
  fairness in representation learning by incorporating fairness constraints into the
  PLS dimensionality reduction framework. The method balances predictive performance
  with fairness by minimizing the dependence between learned representations and sensitive
  attributes.
---

# PLS-based approach for fair representation learning

## Quick Facts
- arXiv ID: 2502.16263
- Source URL: https://arxiv.org/abs/2502.16263
- Reference count: 40
- The paper proposes Fair Partial Least Squares (FPLS) to address fairness in representation learning by incorporating fairness constraints into the PLS dimensionality reduction framework.

## Executive Summary
The paper introduces Fair Partial Least Squares (FPLS), a supervised dimensionality reduction technique that balances predictive performance with fairness by minimizing the dependence between learned representations and sensitive attributes. The method extends the standard PLS framework by adding a fairness regularization term that penalizes covariance with sensitive attributes. Through kernel methods, it can handle non-linear relationships, and experiments demonstrate superior fairness-accuracy trade-offs compared to Fair PCA across multiple datasets.

## Method Summary
FPLS modifies the standard PLS objective by incorporating a fairness penalty that minimizes covariance between projections and sensitive attributes. The optimization uses gradient descent rather than closed-form solutions due to the added fairness constraint. The method iteratively finds weight vectors that maximize covariance with the target while minimizing covariance with sensitive attributes, followed by deflation to find subsequent components. It extends to non-linear cases via kernel methods using HSIC as the independence measure, and can incorporate equality of odds fairness constraints.

## Key Results
- FPLS achieves better fairness-accuracy trade-offs compared to Fair PCA
- Disparate Impact (DI) approaches 1 while maintaining accuracy above 0.8300 for Logistic Regression on Adult Income dataset
- The method successfully handles both linear and non-linear bias patterns through kernel extensions
- Single hyperparameter η controls the fairness-utility trade-off, with η=1 often providing optimal balance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating a fairness penalty into the Partial Least Squares (PLS) objective function allows for a tunable trade-off between predictive utility and independence from sensitive attributes.
- **Mechanism:** The standard PLS objective (maximizing covariance with target $Y$) is modified to include a regularization term $\eta$ that penalizes the squared covariance between the projection and the sensitive attribute $S$. The optimization becomes $\text{Cov}^2(Xw, Y) - \eta \text{Cov}^2(Xw, S)$.
- **Core assumption:** The paper assumes that minimizing the squared covariance is a sufficient proxy for achieving statistical independence (Demographic Parity) for the linear case.
- **Evidence anchors:**
  - [Section 3.2]: Defines the Fair PLS optimization problem (Eq. 4) explicitly as a trade-off controlled by $\eta$.
  - [Section 3.2]: States, "Fair PLS is formulated as... restricting to vectors such that the projections... and the sensitive si are statistically independent."
  - [Corpus]: Corpus evidence for this specific fairness regularization is missing; neighbors focus on standard PLS applications (food, medicine) rather than fairness constraints.
- **Break condition:** If the relationship between the input features and the sensitive attribute is non-linear and the linear FPLS is used, the covariance penalty may fail to capture the dependence, leading to unfair representations.

### Mechanism 2
- **Claim:** The method extends to non-linear relationships by kernelizing the data and measuring independence using the Hilbert Schmidt Independence Criterion (HSIC).
- **Mechanism:** Data is mapped to a Reproducing Kernel Hilbert Space (RKHS). Instead of covariance, the HSIC is used as the fairness constraint. This allows the algorithm to capture complex, non-linear dependencies between the representation and the sensitive attribute.
- **Core assumption:** Assumption: The chosen kernel (e.g., RBF) accurately captures the relevant semantic structures of the data such that independence in the RKHS implies independence in the original space.
- **Evidence anchors:**
  - [Section 4.1]: Formulates Kernel Fair PLS (Eq. 7) using kernel matrices $K_X$, $K_S$ and the HSIC trace operator.
  - [Abstract]: Mentions the method "can be extended to non linear cases via kernel methods."
  - [Corpus]: Corpus neighbors discuss "Generative Flexible Latent Structure" and spectral analysis, providing background on latent structures but not specific evidence for HSIC-fairness.
- **Break condition:** If the kernel Gram matrix becomes singular or ill-conditioned due to high dimensionality and low sample size ($n < d$), the optimization may become unstable.

### Mechanism 3
- **Claim:** Iterative deflation ensures that subsequent components remain orthogonal and informative, preventing the "fairness-utility" collapse seen in single-step methods.
- **Mechanism:** Similar to standard PLS, after a component is found, the data matrices $X$ and $Y$ are "deflated" (subtracting the rank-1 approximation). This ensures the next weight vector $w$ explains residual variance not captured by previous components, maintaining predictive power while enforcing fairness constraints iteratively.
- **Core assumption:** The paper assumes that the gradient descent algorithm will converge to a solution satisfying the constraints at each step, even though a closed-form solution does not exist.
- **Evidence anchors:**
  - [Section 3.2]: Describes the update step using gradient descent and the subsequent computation of residual matrices.
  - [Section 3.2]: "Why cannot Fair PLS be formulated in closed form?" explains the necessity of the iterative approach due to the complexity added by the fairness penalty.
- **Break condition:** If the gradient step size $\epsilon$ is too large, the algorithm may oscillate and fail to converge to a weight vector that effectively balances the objective.

## Foundational Learning

- **Concept: Partial Least Squares (PLS) vs. PCA**
  - **Why needed here:** PLS is a supervised dimensionality reduction technique. Unlike PCA (unsupervised), PLS looks for directions that maximize covariance with the *target* $Y$. Understanding this distinction is critical to why FPLS outperforms Fair PCA in predictive tasks.
  - **Quick check question:** Does PLS seek directions of maximum variance in $X$ (like PCA) or maximum covariance between $X$ and $Y$?
- **Concept: Demographic Parity (DP) vs. Equality of Odds (EO)**
  - **Why needed here:** The default mechanism targets DP (independence $X \perp S$). The paper proposes an extension for EO (conditional independence $X \perp S | Y$). Distinguishing these is necessary for selecting the right objective for the specific ethical context.
  - **Quick check question:** Does the standard Fair PLS formulation enforce independence of the prediction from $S$ (DP), or independence conditional on the ground truth $Y$ (EO)?
- **Concept: Gradient Descent in Constrained Optimization**
  - **Why needed here:** The paper notes that Fair PLS cannot be solved in closed form (like standard PLS). Engineers must understand that the weights are derived iteratively, requiring tuning of a learning rate $\epsilon$ and monitoring for convergence.
  - **Quick check question:** Why does adding the fairness penalty $\eta$ prevent the use of standard SVD eigen-decomposition used in vanilla PLS?

## Architecture Onboarding

- **Component map:** Input: Centered Data Matrix $X$, Target Vector $Y$, Sensitive Attribute Vector $S$. Hyperparameters: $\eta$ (Fairness/Utility trade-off), $k$ (Number of components), $\epsilon$ (Learning rate). Core Loop: Initialize $X_1=X$. For $h=1$ to $k$: Optimize weight $w_h$ via Gradient Descent on $f_{FPLS}(w) = \text{Cov}^2(X_h w, Y) - \eta \text{Cov}^2(X_h w, S)$. Calculate scores $t_h = X_h w_h$. Deflate $X$: $X_{h+1} = X_h - t_h p_h^T$. Output: Weight Matrix $W$ and Score Matrix $T$ (the fair representation).
- **Critical path:** The selection of $\eta$. This single parameter controls the behavior of the system. $\eta \approx 0$: Behaves like standard PLS (high accuracy, low fairness). $\eta \gg 1$: Enforces strict independence (high fairness, potentially random guessing accuracy). [Section 5] suggests $\eta \in [0, 2]$ is often the effective range, with $\eta=1$ frequently providing the best trade-off.
- **Design tradeoffs:** Accuracy vs. Fairness: [Section 5, Figure 1] shows that as Disparate Impact (DI) approaches 1 (perfect fairness), Accuracy typically drops. Reconstruction vs. Prediction: The method prioritizes predicting $Y$ over reconstructing $X$ (unlike Fair PCA). [Section 8.5] claims this results in higher accuracy for the same level of fairness compared to Fair PCA.
- **Failure signatures:** Vanishing Gradients: If $\eta$ is too large relative to the signal in $Y$, the gradient might push $w$ to a null space where it predicts nothing. Corpus Weakness: The lack of robust theoretical bounds in the provided corpus suggests the method's stability on noisy, real-world data should be stress-tested.
- **First 3 experiments:**
  1. Baseline Sanity Check: Run FPLS with $\eta=0$ on a dataset (e.g., Adult Income). Verify the output matches standard PLS (maximum covariance with $Y$).
  2. Trade-off Curve Analysis: Sweep $\eta$ from 0 to 10. Plot Accuracy vs. Disparate Impact (DI). Verify the "elbow" of the curve aligns with the paper's claim that $\eta=1$ is often optimal.
  3. Ablation Study (Linear vs. Kernel): Implement the Kernel FPLS (Section 4.1) on a synthetic dataset with a known non-linear bias. Compare the linear FPLS failure mode against the Kernel FPLS success in removing bias.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a computationally efficient closed-form solution be derived for Fair PLS, particularly for regularization strengths where standard eigen-decomposition conditions fail?
- **Basis in paper:** [explicit] Section 3.2 explicitly includes the sub-heading "Why cannot Fair PLS be formulated in closed form?" and notes that adding the fairness penalty makes the computation "less tractable," forcing a reliance on gradient descent.
- **Why unresolved:** The authors state that a closed-form solution via eigenvectors only exists if $\eta \le \sigma_{min,Y}/\sigma_{max,S}$, leaving the general case solved only iteratively.
- **What evidence would resolve it:** A theoretical derivation providing a direct algebraic solution for the weight vectors $W$ for any $\eta$, or a proof that no such solution exists outside the specified eigenvalue constraints.

### Open Question 2
- **Question:** Can Fair PLS be effectively integrated as a differentiable layer within the end-to-end training pipeline of deep neural networks, such as Large Language Models (LLMs)?
- **Basis in paper:** [inferred] Section 4.3 proposes applying Fair PLS to the "CLS-embedding matrix" of LLMs to intervene in latent representations, but frames it as an algebraic decomposition (SVD-style) on static outputs rather than a dynamic training component.
- **Why unresolved:** The paper demonstrates the method as a post-hoc analysis tool for embeddings, but does not explore how the orthogonal constraints would interact with the non-convex optimization landscapes of deep learning during training.
- **What evidence would resolve it:** Experiments showing the successful backpropagation of gradients through the FPLS projection step in a neural network, maintaining fairness without degrading convergence speed or model utility.

### Open Question 3
- **Question:** Is there a theoretically grounded criterion for optimally selecting the regularization parameter $\eta$ a priori to balance utility and fairness?
- **Basis in paper:** [inferred] The results in Section 5 and Appendix 8.2 show that the trade-off is highly sensitive to $\eta$ (e.g., $\eta=1$ is often optimal, while $\eta=10$ destroys utility), yet the selection relies on empirical cross-validation rather than a theoretical framework.
- **Why unresolved:** The paper empirically determines that $\eta$ should not be "much higher than the value 1," but provides no formula to predict the optimal value based on dataset statistics (e.g., covariance structure of $X$, $Y$, and $S$).
- **What evidence would resolve it:** A theoretical guideline or bound that maps the statistical properties of the input data to an optimal $\eta$ value that maximizes accuracy for a target Disparate Impact (DI).

## Limitations
- The linear FPLS may fail to capture complex non-linear dependencies between features and sensitive attributes
- Gradient descent convergence depends on careful hyperparameter tuning (learning rate, η)
- The method assumes binary sensitive attributes and centered data
- No theoretical guarantees on convergence or optimality are provided

## Confidence
- Mathematical Foundation (High): The extension of PLS with fairness regularization has solid theoretical grounding
- Empirical Validation (Medium): Experiments demonstrate effectiveness but are limited to specific datasets
- Practical Applicability (Medium): Shows promise but requires careful hyperparameter tuning and may not generalize to all bias patterns

## Next Checks
1. **Stability Analysis**: Test FPLS on high-noise versions of existing datasets to evaluate sensitivity to gradient descent hyperparameters and data quality
2. **Non-linear Dependency Testing**: Create synthetic datasets with known non-linear biases to validate the Kernel FPLS extension against the linear version
3. **Generalization Assessment**: Apply FPLS to datasets from domains outside the original experiments (e.g., healthcare, hiring) to test robustness to different feature distributions and bias patterns