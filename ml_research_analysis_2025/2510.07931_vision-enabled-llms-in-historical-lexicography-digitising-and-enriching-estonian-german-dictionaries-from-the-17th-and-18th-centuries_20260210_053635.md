---
ver: rpa2
title: 'Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German
  Dictionaries from the 17th and 18th Centuries'
arxiv_id: '2510.07931'
source_url: https://arxiv.org/abs/2510.07931
tags:
- estonian
- language
- dictionary
- dictionaries
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors applied large language models to historical Estonian-German
  dictionaries from the 17th and 18th centuries. They investigated using LLMs to enrich
  old dictionaries with modern word forms and meanings, and to perform text recognition
  and structured parsing of Gothic script (Fraktur) printed sources.
---

# Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries

## Quick Facts
- **arXiv ID**: 2510.07931
- **Source URL**: https://arxiv.org/abs/2510.07931
- **Reference count**: 0
- **Primary result**: Claude 3.7 Sonnet accurately provided modern forms and meanings for 81% of 342 headword entries from J. Gutslaff's 1648 dictionary

## Executive Summary
This paper demonstrates how vision-enabled large language models can process and enrich historical Estonian-German dictionaries from the 17th and 18th centuries. The authors successfully used LLMs for two tasks: recognizing Gothic script (Fraktur) printed text and enriching historical headwords with modern word forms and meanings. Their approach achieved 81% accuracy in providing modern equivalents and 41% error-free structured parsing of dictionary entries, showing that LLMs can serve as a bridge between historical and contemporary language forms while handling complex document structures.

## Method Summary
The authors applied vision-enabled LLMs (primarily Claude 3.7 Sonnet) to scanned images of historical dictionaries, using zero-shot prompting with explicit output schemas. For the Helle 1732 dictionary, they used whole-page processing with JSON schema output. For the more complex Hupel 1780 dictionary, they implemented overlapping tiling with 4-8 image segments per page, using one LLM for text recognition and a second for merging structured XML outputs. The enrichment task involved mapping historical headwords to modern Estonian forms and German equivalents using GPT-4o followed by Claude 3.7 Sonnet review.

## Key Results
- Claude 3.7 Sonnet provided accurate modern equivalents and meanings for 81% of 342 headword entries from Gutslaff's 1648 dictionary
- Zero-shot method successfully structured 41% of headword entries from Helle's 1732 dictionary into error-free JSON output
- 8-segment tiling improved textual accuracy by 40% but increased computational cost by 184% compared to whole-page processing
- Claude 3.7 Sonnet achieved 17% CER for German text vs 42% CER for Estonian text recognition

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Scaffolding for Low-Resource Historical Languages
Vision-enabled LLMs leverage co-occurring German text as semantic anchors to guide interpretation of historical Estonian word forms through pattern completion and alignment. The bilingual dictionary format provides parallel text where the LLM's stronger German language representations help interpret low-resource historical Estonian. This works because the model's training data contains sufficient German to create reliable semantic bridges, and historical Estonian shares enough morphological patterns with modern Estonian for transfer.

### Mechanism 2: Direct Vision-to-Structure Mapping via In-Context Schema Specification
LLMs can perform OCR and structured data extraction in a single pass when provided with explicit output schemas (JSON/XML). The vision encoder processes image patches into token representations, while the language model attends to both visual tokens and the prompt-specified schema, generating output that satisfies both transcription and formatting constraints simultaneously. This eliminates traditional OCR pipeline stages by handling transcription and structure generation concurrently.

### Mechanism 3: Accuracy-Cost Tradeoff via Overlapping Tiling
Decomposing dense document pages into smaller overlapping image segments increases transcription accuracy at disproportionately higher computational cost. Smaller segments reduce per-query token burden and attention complexity, allowing the model to focus visual attention on finer detail. Overlap prevents boundary artifacts where entries span segment edges. A second LLM merges fragmented outputs, with the tradeoff being 40% accuracy gain for 184% cost increase with 8-segment approach.

## Foundational Learning

- **Character Error Rate (CER)**: Standard OCR evaluation metric measuring character-level differences between recognition and ground truth. Why needed: Paper reports 17% CER for German vs 42% for Estonian, but also discusses why CER is problematic for structured multi-field output. Quick check: Given "körts" and "kõrts" (1 character difference out of 5), CER = 1/5 = 20%.

- **Zero-shot vs Few-shot Prompting**: Contrasts approaches where zero-shot provides no examples while few-shot includes ground truth samples. Why needed: Paper contrasts zero-shot approach (Helle dictionary) with few-shot approach (Hupel dictionary with ground truth sample). Quick check: Providing 3 example input-output pairs before the actual task is few-shot prompting.

- **Temperature Parameter in LLM APIs**: Controls output randomness; low temperature (0.0-0.3) produces deterministic outputs. Why needed: Paper explicitly discusses tuning temperature for reproducibility via API vs chat interface variability. Quick check: For OCR tasks requiring exact transcription fidelity, set temperature low (0.1) not high (0.8).

## Architecture Onboarding

- **Component map**: Scanned PNG images (base64-encoded) -> PIL-based page splitting -> overlapping tile generation (4-8 segments) -> OCR LLM (Claude 3.7 Sonnet/Gemini 2.5 Pro) -> Merge LLM (Gemini 2.5 Pro) -> SequenceMatcher evaluation -> CSV/JSON/XML output

- **Critical path**: Image quality → tiling strategy → prompt engineering (schema + accuracy instructions) → model selection (reasoning vs non-reasoning variants) → merge logic → human review

- **Design tradeoffs**: Whole page vs segmented processing: +40% accuracy but +184% cost for 8-segment approach; JSON vs XML output: JSON easier to generate; TEI Lex-0 requires manually compiled ground truth sample; Reasoning vs non-reasoning models: Reasoning improves structural adherence (+20%) but minimal content gain (+3%), may decrease accuracy in some models (Claude 4 Opus: -15% content accuracy)

- **Failure signatures**: Page propagation errors (single character misread corrupts all subsequent entries); Modernization tendency (model substitutes historical forms with contemporary equivalents); Structural misalignment (CER spikes >100% when correctly recognized words placed in wrong fields); Horizontal reading of columns (API vs chat may read two-column layouts left-to-right)

- **First 3 experiments**: 1) Baseline calibration: Submit single well-scanned page via both chat interface and API with identical prompts; compare CER and structural accuracy to isolate temperature/settings effects. 2) Schema complexity test: Define minimal JSON schema (5 fields) vs granular XML schema (15+ fields) on same page; measure structural similarity scores. 3) Tiling threshold analysis: Compare whole-page, 2-column, and 4-segment processing on 10 pages; plot accuracy vs cost curve to identify optimal segment count for your document density

## Open Questions the Paper Calls Out

- **Alternative evaluation metrics**: What metrics beyond Character Error Rate better capture accuracy for structured, multi-field OCR outputs? The paper identifies CER's limitations for structured output but doesn't propose alternatives that separately account for structural accuracy and textual accuracy across fields.

- **Monolingual historical Estonian analysis**: To what extent can LLMs analyze historical Estonian without relying on German contextual scaffolding? All experiments used bilingual sources where German could guide interpretation; no monolingual historical Estonian test was conducted to isolate the model's independent capability.

- **Optimal tiling strategy**: What is the optimal balance between image segmentation granularity, accuracy gains, and computational cost? The paper presents a tradeoff curve but offers no principled method for determining optimal segmentation for new documents across different document types and model configurations.

- **Context preservation**: How can page-to-page context be maintained to prevent error propagation within individual pages? The paper identifies this as a cause of CER spikes but doesn't implement or test context-preservation mechanisms like sliding context windows or incremental correction prompts.

## Limitations
- Results depend heavily on specific historical dictionary format and Fraktur typography, limiting generalizability to other script types
- 81% enrichment accuracy represents a relatively small sample (342 entries) from a single dictionary
- 41% error-free structured parsing rate leaves substantial room for improvement
- 184% cost increase for 35.5% accuracy gain with 8-segment tiling raises economic viability questions

## Confidence
- **High Confidence**: Vision-enabled LLM's ability to process Fraktur script and extract structured data with clear output schemas (41% error-free JSON extraction, 17% CER for German)
- **Medium Confidence**: Cross-lingual scaffolding mechanism using German as semantic anchors for historical Estonian (81% enrichment accuracy, but exact contribution unclear)
- **Low Confidence**: Optimal tiling strategy and cost-accuracy tradeoff (single Hupel dictionary case study insufficient for generalization)

## Next Checks
1. Apply the 9-field JSON schema to a different historical dictionary (different language pair or century) to assess whether structured extraction accuracy remains above 30% error-free rate
2. Repeat the enrichment task using GPT-4o and Gemini 2.5 Pro vision models to determine if 81% accuracy is model-specific or represents broader LLM capability
3. Systematically vary the number of image segments (1, 2, 4, 8) across 20+ pages of the Hupel dictionary to quantify the precise accuracy-cost relationship and identify optimal segment count for practical deployment