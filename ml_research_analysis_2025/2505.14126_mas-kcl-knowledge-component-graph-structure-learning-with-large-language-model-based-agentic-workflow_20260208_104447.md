---
ver: rpa2
title: 'MAS-KCL: Knowledge component graph structure learning with large language
  model-based agentic workflow'
arxiv_id: '2505.14126'
source_url: https://arxiv.org/abs/2505.14126
tags:
- learning
- graph
- mas-kcl
- datasets
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of knowledge component (KC) graph
  structure learning in education, aiming to identify causal dependencies among KCs
  to support targeted instructional interventions. The authors propose MAS-KCL, a
  multi-agent system driven by large language models (LLMs) that leverages a bidirectional
  feedback mechanism to adaptively optimize the KC graph structure.
---

# MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow

## Quick Facts
- **arXiv ID:** 2505.14126
- **Source URL:** https://arxiv.org/abs/2505.14126
- **Reference count:** 40
- **Key outcome:** MAS-KCL achieves up to 7.71% improvement in loss reduction compared to baseline methods on 5 synthetic and 4 real-world educational datasets.

## Executive Summary
This paper presents MAS-KCL, a multi-agent system that leverages large language models to learn knowledge component (KC) graph structures in educational settings. The system uses a bidirectional feedback mechanism where LLM-based agents assess edge importance and dynamically adjust generation probabilities during optimization. By integrating differential evolution with LLM-guided parameter control, MAS-KCL identifies causal dependencies among KCs to support targeted instructional interventions. Experimental results demonstrate superior performance compared to baseline methods, with strong generalization across different datasets and LLM versions.

## Method Summary
MAS-KCL employs a multi-agent system where LLM-based agents guide a differential evolution algorithm to optimize KC graph structures. The population is partitioned into superior, exploratory, and elimination sub-populations based on a dynamic Ambient Pressure parameter controlled by the Game Agent. Positive and Negative Feedback Agents promote high-quality edges and suppress low-quality edges through PF and NF multipliers. The system iterates until reaching maximum function evaluations, with LLM agents invoked after each iteration to adjust parameters based on loss trajectory analysis.

## Key Results
- MAS-KCL achieves up to 7.71% improvement in loss reduction compared to baseline methods
- GPT models yield the best results, though LLaMA shows lower variance across runs
- Ablation studies show Negative Feedback Agent removal causes the largest performance degradation (~3% loss increase)

## Why This Works (Mechanism)

### Mechanism 1: LLM-Guided Adaptive Parameter Control via Game Agent
The Game Agent dynamically adjusts the Ambient Pressure (AP) parameter based on observed loss changes, improving the exploration-exploitation balance during KC graph optimization. After each iteration, the LLM-based Game Agent evaluates loss magnitude changes and returns a structured JSON decision on whether to adjust AP and by how much. Higher AP preserves more elite individuals; lower AP encourages broader exploration.

### Mechanism 2: Bidirectional Feedback for Edge Quality Assessment
Separate Positive and Negative Feedback Agents promote high-quality edges and suppress low-quality edges, accelerating KC graph structure learning. The Positive Feedback Agent identifies edges associated with increased loss and promotes their removal, while the Negative Feedback Agent identifies edges associated with decreased loss and promotes their addition.

### Mechanism 3: Multi-Sub-Population Partitioning with Differential Evolution
Partitioning the population into superior, exploratory, and elimination sub-populations maintains diversity while preserving elite solutions. After DE generates offspring, individuals are sorted by fitness and divided: top AP×N individuals form the superior sub-population, remaining (1−AP)×N form the exploratory sub-population, and the lowest-fitness individuals are eliminated.

## Foundational Learning

### Concept: Knowledge Components (KCs) and KC Graphs
- **Why needed here:** KCs are the fundamental units the entire system optimizes. Understanding that KC graphs represent prerequisite dependencies is essential for interpreting the algorithm's output.
- **Quick check question:** If a learner fails questions about "multi-digit multiplication," what upstream KCs might the KC graph identify as root causes?

### Concept: Differential Evolution (DE) and Population-Based Optimization
- **Why needed here:** MAS-KCL uses DE as its base optimizer. Understanding mutation, crossover, and selection operators helps explain how candidate KC graphs are generated and refined.
- **Quick check question:** In DE, how does the offspring population differ from simply keeping the best individual each iteration?

### Concept: Large Language Model Agents and Prompt Engineering
- **Why needed here:** The three agents are instantiated via LLM calls with structured prompts. Understanding how to design prompts that return parseable JSON is critical for reproducing this system.
- **Quick check question:** What information must be included in the Game Agent's prompt for it to decide on AP adjustment?

## Architecture Onboarding

### Component Map
INPUT: KC data (learner interactions, problem-KC mappings)
↓
INITIALIZATION: Random population of KC graph candidates
↓
ITERATION LOOP (until FE ≥ maxFE):
1. DE operator generates offspring population
2. Sort by fitness (loss = 1 - F)
3. Partition: Superior / Exploratory / Elimination
4. Bidirectional feedback on exploratory sub-pop
5. Merge superior + modified exploratory
6. Eliminate lowest-fitness individuals
7. Invoke LLM agents: Game Agent → update AP, Positive/Negative Feedback Agents → update PF/NF
↓
OUTPUT: Best KC graph (adjacency matrix) + loss value

### Critical Path
1. **Fitness function implementation** – Must correctly compute how well a candidate KC graph explains learner performance data
2. **LLM agent prompt design** – Prompts must extract structured decisions (JSON) with reasoning
3. **Bidirectional feedback application** – Edge modifications via PF/NF must be applied correctly to the exploratory sub-population only

### Design Tradeoffs
| Tradeoff | Option A | Option B | Paper Choice |
|----------|----------|----------|--------------|
| LLM selection | GPT-4.0 (better for large-scale) | GPT-3.5 (better for small-scale, faster) | Use both based on dataset size |
| Exploration vs. exploitation | High AP (preserve elites) | Low AP (encourage exploration) | Dynamic, controlled by Game Agent |
| Feedback aggressiveness | High PF/NF (fast convergence) | Low PF/NF (more stable) | PF=0.6, NF=0.4 (initial values) |

### Failure Signatures
- **LLM API failures:** If agent calls timeout or return invalid JSON, the algorithm falls back to previous parameter values
- **Population collapse:** If loss variance across individuals drops below threshold, diversity is lost
- **Divergent loss:** If loss increases over multiple iterations, PF/NF may be inverted or fitness function is incorrectly implemented

### First 3 Experiments
1. **Reproduce baseline comparison on LPR-RWD1:** Run MAS-KCL vs. EESHHO on the smallest real-world dataset (D=1225) with GPT-3.5
2. **Ablate single agents:** Remove each agent (Game, PFA, NFA) one at a time and measure loss increase
3. **LLM sensitivity test:** Run the same dataset with GPT-4.0, GPT-3.5, and LLaMA-70B

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved based on the experimental design and results.

## Limitations
- All experimental datasets are restricted to mathematics, limiting generalization to other domains
- The reduction in loss metric does not directly correlate with improved instructional interventions or learning outcomes
- The monetary and computational cost of LLM API calls is not analyzed relative to performance gains

## Confidence
- **High Confidence:** The overall multi-agent architecture and bidirectional feedback mechanism are clearly described
- **Medium Confidence:** The experimental results and ablation studies are well-documented
- **Low Confidence:** The fitness function implementation and exact LLM prompt engineering details

## Next Checks
1. **Reproduce the smallest real-world dataset (LPR-RWD1)** with MAS-KCL vs. EESHHO baseline to verify the reported loss of ~0.22-0.27
2. **Conduct ablation studies** by removing each LLM agent (Game, Positive Feedback, Negative Feedback) to confirm the finding that Negative Feedback Agent removal causes the largest loss increase (~3%)
3. **Test LLM sensitivity** by running identical experiments with GPT-4.0, GPT-3.5, and LLaMA-70B to validate the reported performance differences and variance patterns