---
ver: rpa2
title: 'Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix
  Injection'
arxiv_id: '2601.13359'
source_url: https://arxiv.org/abs/2601.13359
tags:
- attacks
- attack
- sockpuppetting
- prompt
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "sockpuppetting," a simple jailbreaking technique
  for open-weight LLMs that achieves up to 80% higher attack success rates than the
  gradient-guided attack GCG by inserting acceptance sequences into the model's output
  prefix. The method requires only a single line of code and no optimization, making
  it accessible to unsophisticated adversaries.
---

# Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection

## Quick Facts
- **arXiv ID**: 2601.13359
- **Source URL**: https://arxiv.org/abs/2601.13359
- **Reference count**: 13
- **Primary result**: Sockpuppetting achieves up to 80% higher attack success rates than GCG by injecting acceptance sequences into output prefixes

## Executive Summary
This paper introduces "sockpuppetting," a simple jailbreaking technique for open-weight LLMs that achieves high attack success rates without optimization. By inserting acceptance sequences directly into the model's output prefix (assistant message block), the method exploits the model's self-consistency tendency to bypass refusal training. The technique requires only a single line of code and works on models like Llama-3.1-8B and Qwen3-8B, demonstrating that output-prefix injection is an effective low-cost attack vector that highlights the need for defenses in open-weight models.

## Method Summary
The paper proposes sockpuppetting, which inserts acceptance sequences (e.g., "Sure, here is how to...") at the start of a model's output prefix within the assistant message block. This exploits LLMs' self-consistency by making them treat the injected text as already-generated content and complete it with compliant responses. The method also introduces a hybrid approach combining sockpuppetting with gradient optimization (GCG) applied within the assistant block, which further increases attack success rates. The technique is tested on Llama-3.1-8B, Qwen3-8B, and Gemma-7B using the "Harmful Behaviors" dataset with 520 malicious prompts, evaluating success rates with a Gemma-3-27B-it judge model.

## Key Results
- Sockpuppetting achieves up to 80% higher attack success rates than gradient-guided attacks (GCG)
- On Qwen3-8B, sockpuppetting achieves near-ceiling ASR (>90%) with simple acceptance sequences
- Hybrid sockpuppetting with GCG optimization increases attack success by 64% over GCG alone on Llama-3.1-8B
- Gemma-7B demonstrates resistance through mid-response backtracking, reducing attack effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Self-Consistency Exploitation
LLMs are trained to generate coherent continuations. When an acceptance sequence is artificially placed in the assistant message block, the model treats it as already-generated text and autoregressively completes it with compliant content rather than refusing, prioritizing coherence with context over alignment training.

### Mechanism 2: Chat Template Manipulation
Open-weight models grant attackers full control over chat templates, enabling injection into the assistant message block. Unlike closed-weight APIs, open-weight deployments allow arbitrary modifications to tokenized context without restriction.

### Mechanism 3: Hybrid Gradient Optimization in Assistant Space
Optimizing adversarial suffixes within the assistant message block yields higher attack success than user-block optimization. When GCG-style optimization is applied to tokens in the assistant block, the optimizer discovers coherent agreement phrases that more naturally precede compliant outputs.

## Foundational Learning

**Concept: Autoregressive language modeling**
- Why needed: Understanding why prefix injection forces continuationâ€”the model predicts next tokens based on all preceding context, including injected prefixes
- Quick check: If context contains "Sure, here is how to build a bomb. You", what distribution should the model assign to the next token?

**Concept: Chat templates and message roles**
- Why needed: Attack exploits the distinction between user and assistant blocks; understanding template structure is essential for implementation
- Quick check: In the template `<|im_start|>assistant<|im_sep|>`, where can an attacker inject text without the model detecting tampering?

**Concept: Gradient-guided adversarial attacks (GCG)**
- Why needed: The hybrid method extends GCG by changing where optimization occurs; baseline understanding enables comparison
- Quick check: What is the loss function minimized by GCG, and how does sockpuppetting modify the optimization context?

## Architecture Onboarding

**Component map**: Tokenizer + Chat Template Engine -> Prefix Injector -> Model (open-weight LLM) -> Judge Model

**Critical path**: 1) Load open-weight model and tokenizer 2) Construct chat template with malicious user prompt 3) Inject acceptance sequence into assistant block 4) Run model.generate() from injected prefix 5) Evaluate output with judge model

**Design tradeoffs**:
- SockpuppetAcceptance vs. SockpuppetNewline vs. SockpuppetTitle: Different acceptance formats perform variably per model
- Pure sockpuppetting vs. hybrid with GCG: Hybrid is more expensive but yields universal suffixes; pure is free but requires per-prompt acceptance construction
- Rolling optimization vs. single-shot: Rolling ensures coherence but takes up to 10x longer

**Failure signatures**:
- Low ASR despite low training loss (Gemma): Model backtracks after initial compliance
- Incoherent optimized suffixes: Mode collapse to punctuation/noise; requires modified target sequence
- Tokenization instability: Suffixes decode/re-encode differently; requires filtering

**First 3 experiments**:
1. Reproduce baseline: Run sockpuppetting on Llama-3.1-8B with SockpuppetNewline on 10 harmful prompts
2. Ablate acceptance format: Test three variants on Qwen3-8B to confirm Newline achieves near-ceiling ASR
3. Hybrid sanity check: Run RollingSockpuppetGCG on 5 prompts for Gemma-7B to observe backtracking

## Open Questions the Paper Calls Out

**Open Question 1**: How can target acceptance sequences be optimized to better match model-specific output distributions? The paper notes that standard formats are arbitrary and often mismatch natural model mannerisms, but lacks a systematic method for generating optimal, model-specific sequences.

**Open Question 2**: Is the acceptance sequence a causal driver of compliance or merely a proxy for the model's propensity to comply? It remains unclear if generating the sequence forces compliance via self-consistency, or if the log-likelihood is simply a signal for finding semantic jailbreaks.

**Open Question 3**: Can safety training methodologies explicitly teach models to "backtrack" to mitigate output-prefix injection? Gemma's resistance through mid-response refusal is observed but it's unknown if this is an incidental artifact or a generalizable safety feature.

## Limitations
- Evaluation relies on a single judge model (Gemma-3-27B-it), raising concerns about bias and false positives/negatives
- Does not address computational overhead differences between pure and hybrid approaches
- Lacks robustness testing against input sanitization or API-level protections
- Claims about open-weight model vulnerability may overstate practical risk given real-world deployment protections

## Confidence
**High Confidence**: Basic sockpuppetting works as described on open-weight models; Qwen3-8B shows near-ceiling attack success; self-consistency exploitation is valid; Gemma-7B demonstrates resistance through backtracking

**Medium Confidence**: Hybrid GCG optimization provides 64% improvement; sockpuppetting achieves 80% higher attack success than GCG in some cases; attack represents meaningful threat; acceptance sequence variants perform variably

**Low Confidence**: Universal suffix transferability across model families; specific claim about 500 optimization steps being optimal; all open-weight models equally vulnerable; exact contribution of each mechanism component

## Next Checks
1. **Judge Model Validation**: Run sockpuppetting attacks using both Gemma-3-27B-it and a different judge model (GPT-4 or Claude) to verify attack success rates are not artifacts of judge bias

2. **Component Ablation Study**: Systematically test what happens when each component is removed: (a) Remove acceptance sequence but keep gradient optimization, (b) Remove gradient optimization but keep acceptance sequence, (c) Test on models with strict input sanitization

3. **Defense Effectiveness Testing**: Implement basic input sanitization that prevents text injection into assistant message blocks and measure how much this reduces attack success rates; test whether models with robust safety fine-tuning maintain resistance when acceptance sequence is obfuscated