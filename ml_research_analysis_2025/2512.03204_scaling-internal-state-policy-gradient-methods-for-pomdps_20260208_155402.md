---
ver: rpa2
title: Scaling Internal-State Policy-Gradient Methods for POMDPs
arxiv_id: '2512.03204'
source_url: https://arxiv.org/abs/2512.03204
tags:
- state
- gradient
- which
- agent
- gamp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces three new policy-gradient algorithms for
  learning in large partially observable Markov decision processes (POMDPs) using
  finite-state controllers (FSCs) to store information about the observation history.
  The first algorithm, GAMP, computes the gradient using a known model of the environment
  via matrix series approximations, enabling solution of POMDPs an order of magnitude
  larger than previous model-based methods.
---

# Scaling Internal-State Policy-Gradient Methods for POMDPs

## Quick Facts
- arXiv ID: 2512.03204
- Source URL: https://arxiv.org/abs/2512.03204
- Reference count: 2
- Key outcome: Introduces three new policy-gradient algorithms for large POMDPs using finite-state controllers, enabling solution of problems an order of magnitude larger than previous methods

## Executive Summary
This paper addresses the challenge of solving large partially observable Markov decision processes (POMDPs) by introducing three policy-gradient algorithms that use finite-state controllers (FSCs) to compress the history of observations into a finite number of internal states. The methods include GAMP (model-based with matrix series approximations), IState-GPOMDP (model-free with stochastic sampling), and Exp-GPOMDP (model-free with variance reduction via Rao-Blackwellization). The algorithms are tested on problems ranging from a 21,632-state multi-agent task to problems requiring multi-step memory, demonstrating that FSCs provide a practical middle ground between memoryless and full belief-state approaches.

## Method Summary
The paper introduces three policy-gradient algorithms for POMDPs using finite-state controllers. GAMP computes gradients using a known model via matrix series approximations with Richardson iteration, reducing computational complexity from O(|S|³|G|³) to O(c|S||G|N) for sparse POMDPs. IState-GPOMDP estimates gradients through simulation by sampling world and I-state trajectories with eligibility traces. Exp-GPOMDP reduces variance by computing true expectations over I-state trajectories while sampling world trajectories, using belief distributions over I-states updated deterministically. All methods use softmax distributions for action and I-state transitions, initialized to zero, and optimize parameters via Polak-Ribière conjugate-gradient ascent with line search. Critical to success is using sparse FSCs with out-degree k ≪ |G| (typically k=2-3) to avoid zero-gradient regions.

## Key Results
- GAMP solves POMDPs an order of magnitude larger than previous model-based methods by using matrix series approximations
- Exp-GPOMDP achieves better performance in fewer steps than IState-GPOMDP due to reduced gradient variance through Rao-Blackwellization
- On a multi-agent factory problem with 21,632 states, the methods successfully learn policies while maintaining tractable computation
- The methods handle problems requiring multi-step memory, such as the Heaven-Hell problem where the agent must remember a signpost reading to reach the correct goal

## Why This Works (Mechanism)

### Mechanism 1: Finite-State Controllers as Compressible Belief-State Approximators
FSCs approximate optimal POMDP policies using a fixed number of internal states, providing a tractable middle ground between memoryless policies and full belief-state methods. The FSC maintains a finite set of internal states (I-states) with stochastic transitions governed by parameters φ, creating a Markov chain over world/I-state pairs. As the number of I-states increases, the representable policy space expands toward the optimal policy. This works when a good policy exists that can be represented with limited I-states and the POMDP's mixing time is finite.

### Mechanism 2: Richardson Iteration for Scalable Gradient Computation (GAMP)
Matrix series expansion enables gradient computation in O(c|S||G|N) rather than O(|S|³|G|³), making model-based POMDPs with thousands of states tractable. Instead of directly computing the inverse matrix, GAMP approximates via power series using Richardson iteration, exploiting that the transition matrix converges to a stationary distribution exponentially. This works when the transition matrix is sparse and convergence is achieved within practical iterations.

### Mechanism 3: Rao-Blackwellization for Variance Reduction (Exp-GPOMDP)
Computing exact expectations over I-state trajectories while sampling world trajectories reduces gradient variance compared to full Monte Carlo sampling, enabling faster convergence. Exp-GPOMDP replaces sampled I-state transitions with a belief distribution over I-states updated deterministically, integrating out I-state trajectory randomness while retaining world trajectory sampling. This works when the I-state transition model is known and the O(|G|²) per-step cost is acceptable.

## Foundational Learning

- **Concept: Policy Gradient Methods**
  - Why needed here: All three algorithms adjust policy parameters via gradient ascent on long-term average reward, avoiding divergence issues of value-based methods under function approximation.
  - Quick check question: Why does Theorem 2 guarantee convergence to a local optimum of η even with function approximation, when value-based methods might diverge?

- **Concept: POMDP Belief States and Computational Burden**
  - Why needed here: FSCs are motivated as approximations to belief-state policies. Understanding what information the belief state contains vs. what an FSC might lose clarifies the tradeoff.
  - Quick check question: What information does the belief state contain that an FSC with |G| internal states might lose, and how does increasing |G| reduce this gap?

- **Concept: Mixing Time and Credit Assignment**
  - Why needed here: The discount factor β and mixing time τ determine how many steps of history influence current rewards, directly affecting gradient variance and the 1/[T(1-β)] scaling in Theorem 2.
  - Quick check question: If a POMDP has mixing time τ = 100, what range of β would balance gradient bias (from β < 1) against variance (from 1/(1-β) being too large)?

## Architecture Onboarding

- **Component map**:
  ```
  [World POMDP: S, U, Y, Q(u), ν(y|i), r(i)]
         ↓ observations y_t
  [FSC Controller: G internal states]
    ├── ω(h|φ, g, y): I-state transition network (parameters φ)
    ├── μ(u|θ, g, y): Action selection network (parameters θ)
    └── [Exp-GPOMDP only: Belief α_t(g) over I-states]
         ↓ actions u_t
  [Gradient Estimator]
    ├── GAMP: Model-based, Richardson iteration
    ├── IState-GPOMDP: Samples trajectories, eligibility traces z_φ, z_θ
    └── Exp-GPOMDP: Rao-Blackwellized, computes α_t and ∇α_t exactly
         ↓
  [Optimizer: Polak-Ribière conjugate gradient + line search]
  ```

- **Critical path**:
  1. Initialize FSC structure: Choose |G|, connectivity k, parameterization (softmax). Sparse FSCs with k ≪ |G| prevent zero-gradient regions.
  2. Gradient estimation loop: GAMP uses power method + Richardson; IState/Exp run T simulation steps accumulating traces.
  3. Parameter update: Apply conjugate gradient step with line search.

- **Design tradeoffs**:
  - |G| vs. representational power: More I-states → better policies, but O(|G|²) for Exp-GPOMDP
  - Dense vs. sparse FSC (connectivity k): Dense risks zero-gradient initialization; sparse (k=2-3) works better empirically
  - Model-based vs. model-free: GAMP is zero-variance but requires known P; model-free scales with simulation steps, not |S|

- **Failure signatures**:
  1. Zero gradient at initialization: Near-uniform ω and μ → ∇φη ≈ 0. Fix: Use sparse FSC with k=2-3.
  2. Slow convergence in IState-GPOMDP: High variance. Fix: Increase T, tune β, or switch to Exp-GPOMDP.
  3. Policy doesn't use memory: FSC converges to memoryless behavior. Fix: Verify problem requires memory; check k setting.

- **First 3 experiments**:
  1. Validate on Heaven-Hell with sparse FSC: |G|=20, k=3, T=10⁷. Verify agent learns to visit signpost and use multi-step memory. Compare IState vs Exp convergence.
  2. Stress test GAMP scaling: Run on multi-agent factory (21,632 states). Measure wall-clock time vs. |G|. Replicate Figure 2 error decay analysis.
  3. Ablate FSC connectivity: On Pentagon navigation, vary k ∈ {1, 2, 3, |G|}. Track final η and convergence rate. Confirm k=2-3 sweet spot.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can advanced numerical iteration techniques such as Krylov subspace methods (for computing x) and the Lanczos method (for computing π) significantly improve GAMP's computational efficiency and convergence rates? The paper notes these methods "are worthy of further investigation" as alternatives to Richardson iteration and the power method, but only implements basic iterative methods.

- **Open Question 2**: What theoretical conditions guarantee avoidance of zero-gradient regions in FSC parameter space, and can principled initialization strategies replace the ad-hoc sparse FSC construction? Section 4.2 identifies zero-gradient regions as a failure mode and uses sparse FSCs with out-degree k ≪ |G| as a "trick" to overcome it, but no principled method for choosing k or initializing φ is provided.

- **Open Question 3**: Can additional variance reduction techniques (e.g., baseline subtraction, control variates) be combined with Rao-Blackwellization in Exp-GPOMDP to further improve sample efficiency? The conclusion states future work includes "incorporating further variance reduction methods," but no other variance reduction techniques are explored.

## Limitations
- GAMP requires full knowledge of transition probabilities, limiting applicability to model-free scenarios
- Results depend critically on sparse FSC connectivity (k=2-3), with no principled method for choosing this parameter
- The methods assume the agent can eventually disambiguate world states through observations, which may not hold for POMDPs with unidentifiable states

## Confidence
- **High confidence**: GAMP's matrix series approximation correctly reduces computational complexity from O(|S|³|G|³) to O(c|S||G|N) for sparse POMDPs. The variance reduction mechanism in Exp-GPOMDP through Rao-Blackwellization is mathematically sound.
- **Medium confidence**: Empirical results demonstrating superior scaling on the 21,632-state multi-agent problem are convincing, though exact transition matrices are unspecified. The k=2-3 connectivity recommendation is supported by limited ablation studies.
- **Low confidence**: Claims about memory requirements for specific problems (e.g., "Pentagon requires >3 steps of memory") lack rigorous justification. The relationship between mixing time, discount factor, and gradient variance is not fully characterized.

## Next Checks
1. Scale GAMP to larger POMDPs: Implement GAMP on a synthetic POMDP with 100,000+ states and varying sparsity levels to validate the claimed O(c|S||G|N) scaling empirically. Measure wall-clock time and gradient accuracy as functions of c and N.

2. Ablate FSC connectivity systematically: On the Pentagon problem, run experiments varying k from 1 to |G| with multiple random seeds. Track convergence rate, final reward, and gradient norm statistics to establish formal guidelines for k selection.

3. Test on POMDPs with unidentifiable states: Implement a POMDP where some world states are observationally equivalent (e.g., a maze with symmetric rooms). Compare FSC performance against belief-state methods to quantify the information loss from finite-memory policies.