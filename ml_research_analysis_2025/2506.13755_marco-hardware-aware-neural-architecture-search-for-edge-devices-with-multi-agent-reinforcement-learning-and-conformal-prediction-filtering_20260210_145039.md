---
ver: rpa2
title: 'MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent
  Reinforcement Learning and Conformal Prediction Filtering'
arxiv_id: '2506.13755'
source_url: https://arxiv.org/abs/2506.13755
tags:
- marco
- latency
- search
- accuracy
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MARCO, a hardware-aware neural architecture
  search (NAS) framework designed for resource-constrained edge devices. The core
  innovation lies in combining multi-agent reinforcement learning (MARL) with conformal
  prediction filtering to efficiently explore the design space while respecting strict
  hardware constraints.
---

# MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering

## Quick Facts
- arXiv ID: 2506.13755
- Source URL: https://arxiv.org/abs/2506.13755
- Authors: Arya Fayyazi; Mehdi Kamal; Massoud Pedram
- Reference count: 40
- Primary result: Hardware-aware NAS framework combining MARL with conformal prediction filtering, achieving 3-4× search time reduction while maintaining near-baseline accuracy within 0.3% on edge devices.

## Executive Summary
MARCO introduces a novel hardware-aware neural architecture search framework designed for resource-constrained edge devices. The core innovation lies in combining multi-agent reinforcement learning with conformal prediction filtering to efficiently explore the design space while respecting strict hardware constraints. By decomposing the search task into separate hardware configuration and quantization agents, MARCO enables fine-grained mixed-precision design without combinatorial explosion. The framework achieves significant search time reductions (3-4×) compared to baselines while maintaining competitive accuracy, validated through both simulation and on-device testing on MAX78000 evaluation boards.

## Method Summary
MARCO employs a two-agent reinforcement learning approach where a Hardware Configuration Agent selects macro-architecture parameters (layer count, kernel size, channel width) and a Quantization Agent independently assigns per-layer bit-widths. The framework uses a centralized-critic, decentralized-execution paradigm where a centralized critic observes the full state during training while agents execute based on local observations. A calibrated conformal prediction surrogate model statistically prunes unpromising architectures before expensive evaluation, reducing search time by 3-4×. The search process involves partial training (5 epochs on 10% data) to estimate accuracy, followed by hardware simulator queries for latency and memory usage. The reward function balances accuracy against hardware resource constraints.

## Key Results
- 3-4× reduction in search time compared to once-for-all baselines
- Near-baseline accuracy within 0.3% maintained despite aggressive pruning
- Simulator estimates deviate from measured values by less than 5% on MAX78000 boards
- Reduced inference latency achieved through optimized architecture and quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the NAS action space into separate agents reduces combinatorial explosion in mixed-precision search.
- Mechanism: The Hardware Configuration Agent (HCA) selects macro-architecture parameters while the Quantization Agent (QA) independently assigns per-layer bit-widths. A centralized critic observes the full state during training, but each agent executes based on local observations, enabling fine-grained mixed-precision design without a flat, exponentially large action space.
- Core assumption: Architecture and quantization decisions are sufficiently independent that decoupled agents can learn cooperative policies without explicit coordination at every step.
- Evidence anchors:
  - [abstract] "MARCO decomposes the NAS task into a hardware configuration agent (HCA) and a Quantization Agent (QA)... enabling fine-grained mixed-precision design without combinatorial explosion."
  - [section III-B] Defines separate action spaces where HCA selects macro-level parameters and QA selects bit-widths, with local observations for each agent.
  - [corpus] Weak direct evidence—related NAS papers (MCUNet, MnasNet) use single-agent approaches; MARCO's multi-agent decomposition appears novel in this context.
- Break condition: If architecture and quantization choices exhibit strong conditional dependencies (e.g., certain layer types require specific bit-widths), the decoupled agents may converge to suboptimal joint policies.

### Mechanism 2
- Claim: Conformal prediction filtering with a surrogate model prunes unpromising architectures while providing statistical coverage guarantees.
- Mechanism: A surrogate model g(·) trained on a calibration set predicts reward for candidate architectures. CP computes residual quantiles to establish an upper confidence bound; candidates whose UCB falls below threshold τ receive reward -1 and skip evaluation. This guarantees that with probability ≥1-δ, viable candidates survive pruning.
- Core assumption: Calibration and test candidates are exchangeable (distribution drift from policy updates is negligible or mitigated by periodic recalibration).
- Evidence anchors:
  - [abstract] "A key contribution is the integration of a calibrated conformal prediction surrogate model that statistically prunes unpromising architectures before expensive evaluation, reducing search time by 3-4×."
  - [section III-C] Equation (10) provides the formal coverage guarantee; Algorithm 1 details calibration; Table VII shows 25-30% pruning with negligible accuracy loss.
  - [corpus] ESM paper mentions surrogate models for HW-NAS but does not employ CP-based statistical guarantees.
- Break condition: If policy drift during training violates exchangeability, the coverage guarantee degrades. The paper notes periodic recalibration mitigates this but does not quantify robustness.

### Mechanism 3
- Claim: Partial training (5 epochs on 10% data) provides sufficiently correlated accuracy estimates for effective reward ranking.
- Mechanism: Instead of full training, candidates undergo 5-epoch fine-tuning on a 10% calibration subset, yielding provisional accuracy A(a) for reward computation. This reduces per-candidate evaluation cost by ~95% while maintaining ordinal ranking quality.
- Core assumption: Relative accuracy ordering from partial training correlates strongly with final fully-trained accuracy.
- Evidence anchors:
  - [section III-B] "This inexpensive procedure provides a stable estimate of top-1 accuracy A(at), while it costs less than 5% of full training time."
  - [Table V] 5 epochs yields 87.2% accuracy vs. 87.5% for 10 epochs, with 20% search time reduction.
  - [corpus] No direct corroboration found; common NAS practice but validation is dataset-specific.
- Break condition: For tasks where early training dynamics diverge from final convergence (e.g., complex optimization landscapes), partial accuracy may misrank candidates.

## Foundational Learning

- Concept: **Centralized Training with Decentralized Execution (CTDE)**
  - Why needed here: Understanding how the centralized critic enables coordinated policy learning while agents execute independently is essential for debugging agent coordination failures.
  - Quick check question: Can you explain why the centralized critic sees the full state during training but each agent only receives local observations during execution?

- Concept: **Conformal Prediction Coverage Guarantees**
  - Why needed here: The CP filter's statistical guarantee (Eq. 10) underpins the pruning logic; misunderstanding this leads to incorrect threshold tuning.
  - Quick check question: Given a miscoverage rate δ=0.1 and calibration residuals, how would you compute the uncertainty offset α₁₋δ?

- Concept: **Proximal Policy Optimization (PPO) Clipping**
  - Why needed here: Both agents use PPO for stable policy updates; the clipping mechanism prevents destructive policy divergence during multi-agent training.
  - Quick check question: What does the clipping parameter ε=0.2 constrain in the PPO loss function?

## Architecture Onboarding

- Component map:
  - HCA (Hardware Configuration Agent) -> QA (Quantization Agent) -> Candidate Architecture -> CP Filter -> Partial Training -> Hardware Simulator -> Reward Computation -> PPO Update

- Critical path: Episode loop → HCA/QA sample actions → merge to candidate aₜ → CP filter check → if pass: partial training + simulator query → reward computation → PPO update; if fail: r=-1, skip evaluation.

- Design tradeoffs:
  - **τ threshold**: Lower τ increases pruning (faster search) but risks discarding viable candidates (Table IV shows τ=5.0 yields 86.8% accuracy vs. 87.4% at τ=6.0)
  - **Partial training epochs**: 5 epochs faster but noisier; 10 epochs improves accuracy estimates by ~0.3% but adds 20% search time (Table V)
  - **δ miscoverage**: Lower δ (stricter coverage) reduces pruning aggression; δ=0.05 yields 20% discard vs. 40% at δ=0.2 (Table VIII)

- Failure signatures:
  - **CP filter too aggressive**: Search converges quickly but final accuracy >0.5% below baseline (check τ calibration)
  - **Agent coordination failure**: HCA selects wide channels while QA assigns 8-bit, exceeding memory budget repeatedly (inspect reward penalty frequency)
  - **Simulator-reality gap**: Deployed latency deviates >5% from simulator (paper reports <5% on MAX78000; verify oracle fidelity)

- First 3 experiments:
  1. **CP calibration validation**: Run 100 random architectures, compute calibration residuals, verify α₁₋δ achieves target coverage on held-out set before enabling filtering.
  2. **Agent ablation**: Run MARL without CP (as in Table VII) on CIFAR-10 subset to establish baseline search time and accuracy; compare to full MARCO.
  3. **Threshold sweep**: On a validation dataset, sweep τ ∈ {5.0, 5.5, 6.0} with fixed δ=0.1 to characterize accuracy vs. search time tradeoff before full deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the distribution drift caused by iterative RL policy updates quantitatively impact the coverage guarantees of the Conformal Prediction filter between recalibration intervals?
- **Basis in paper:** [Explicit] The authors acknowledge in Section III.C.1.a that "iterative policy updates could theoretically violate this [exchangeability], we mitigate drift by periodically recalibrating," but they do not analyze the failure modes or sensitivity of the miscoverage rate (δ) relative to the recalibration frequency.
- **Why unresolved:** While a mitigation strategy (periodic refitting) is proposed, the paper lacks an ablation study showing how quickly the theoretical guarantees degrade if recalibration is delayed or if the policy shifts rapidly.
- **What evidence would resolve it:** A sensitivity analysis plotting the empirical miscoverage rate against the number of search steps since the last surrogate model recalibration.

### Open Question 2
- **Question:** Does MARCO's efficiency persist when targeting edge accelerators with complex memory hierarchies (e.g., hierarchical caches or off-chip DRAM) versus the flat SRAM of the tested MAX78000?
- **Basis in paper:** [Explicit] The abstract and conclusion claim the framework is "portable across various edge accelerators" because it interfaces only with "abstract latency and memory queries," yet the experimental validation is restricted to a single microcontroller architecture with a specific memory profile.
- **Why unresolved:** The simulator rewards and penalties are tuned for a sub-megabyte SRAM environment; it is uncertain if the simple "abstract queries" sufficiently capture the non-linear latency spikes of cache misses or DRAM bursts on other hardware.
- **What evidence would resolve it:** Benchmarking MARCO on a diverse set of hardware targets, such as a Raspberry Pi (using DRAM) or a dedicated FPGA accelerator, to verify that search time reductions remain consistent.

### Open Question 3
- **Question:** Can the Quantization Agent (QA) efficiently navigate a continuous or fine-grained mixed-precision space (e.g., 2-bit to 8-bit) without suffering from the combinatorial explosion the framework aims to avoid?
- **Basis in paper:** [Inferred] The search space (Table I) restricts bit-widths to discrete 4-bit and 8-bit choices. The paper claims to solve the "explosion of dimensions" associated with mixed precision, but only demonstrates this on a very coarse granularity.
- **Why unresolved:** Expanding the action space to include every integer bit-width (or sub-8-bit floating point) would significantly increase the QA's action space, potentially reducing the hit-rate of the CP filter or slowing policy convergence.
- **What evidence would resolve it:** Experiments where the QA's action space is expanded to include 2-bit, 3-bit, and floating-point options, measuring the resulting increase in search time and convergence stability.

## Limitations
- The framework's portability claims require validation across diverse edge hardware platforms beyond the MAX78000 demonstration
- The partial training protocol's effectiveness may vary significantly across different datasets and task complexities
- The multi-agent decomposition introduces coordination complexity that is not fully characterized

## Confidence

- **High confidence:** Hardware-aware reward formulation, CP coverage guarantees (given exchangeability), simulator accuracy validation (<5% deviation)
- **Medium confidence:** Multi-agent decomposition benefits, partial training effectiveness, search time reduction claims
- **Low confidence:** Cross-platform portability, CP robustness to policy drift, generalization to non-image tasks

## Next Checks

1. **CP robustness test:** Run MARCO with periodic policy updates and monitor coverage guarantee degradation over training episodes; quantify needed recalibration frequency.
2. **Cross-hardware validation:** Port MARCO to a different edge platform (e.g., ARM Cortex-M with CMSIS-NN) and measure performance degradation from simulator-reality gap.
3. **Task generalization study:** Apply MARCO to a non-image task (e.g., keyword spotting) and evaluate whether partial training (5 epochs) maintains sufficient accuracy correlation for effective ranking.