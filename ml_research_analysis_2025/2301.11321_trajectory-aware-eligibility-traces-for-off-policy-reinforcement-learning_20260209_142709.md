---
ver: rpa2
title: Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning
arxiv_id: '2301.11321'
source_url: https://arxiv.org/abs/2301.11321
tags:
- retrace
- learning
- traces
- trajectory-aware
- off-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unifying theoretical framework for trajectory-aware
  eligibility traces in off-policy reinforcement learning. The authors propose a general
  multistep operator M that encompasses both per-decision and trajectory-aware methods,
  enabling analysis of algorithms that consider entire sequences of past experiences
  when assigning credit.
---

# Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning

## Quick Facts
- arXiv ID: 2301.11321
- Source URL: https://arxiv.org/abs/2301.11321
- Reference count: 40
- Primary result: Introduces Recency-Bounded Importance Sampling (RBIS), a new trajectory-aware algorithm that outperforms Retrace, Truncated IS, and Recursive Retrace in off-policy control tasks.

## Executive Summary
This paper introduces a unifying theoretical framework for trajectory-aware eligibility traces in off-policy reinforcement learning. The authors propose a general multistep operator M that encompasses both per-decision and trajectory-aware methods, enabling analysis of algorithms that consider entire sequences of past experiences when assigning credit. They prove convergence conditions for policy evaluation and control, removing assumptions from previous work and establishing the first theoretical guarantees for several existing trajectory-aware methods including Recursive Retrace. The paper also introduces Recency-Bounded Importance Sampling (RBIS), a new trajectory-aware algorithm that performs robustly across λ-values in off-policy control tasks, outperforming Retrace, Truncated IS, and Recursive Retrace in gridworld experiments with significantly higher area-under-the-curve scores.

## Method Summary
The paper develops a generalized multistep operator M that updates Q-values using traces β_t that depend on the entire trajectory history rather than just the immediate previous state. The key theoretical contribution is proving that M is a contraction mapping when β_t satisfies the recursive inequality β_t ≤ β_{t-1}ρ_t (Condition 5.1), where ρ_t is the importance sampling ratio. The authors use this framework to analyze existing trajectory-aware methods and introduce RBIS, which computes traces as β_t = min(λ^t, β_{t-1}ρ_t). RBIS preserves longer traces than Retrace by allowing accumulated traces to grow as long as they don't exceed a decay threshold λ^t, while still satisfying the contraction condition. Experiments compare RBIS against Retrace, Truncated IS, and Recursive Retrace on bifurcating gridworlds across different λ values.

## Key Results
- Proves that the generalized operator M contracts to the true value function Q^π when Condition 5.1 holds (β_t ≤ β_{t-1}ρ_t), establishing convergence guarantees for policy evaluation.
- Demonstrates that Truncated IS can diverge in off-policy settings when Condition 5.1 is violated, with explicit counterexamples provided.
- Introduces RBIS algorithm that outperforms Retrace, Truncated IS, and Recursive Retrace in gridworld experiments, with AUC scores consistently 20-50% higher in mid-to-high λ ranges.
- Shows RBIS maintains robust performance across the full λ-spectrum while other methods show significant degradation at high λ values.

## Why This Works (Mechanism)

### Mechanism 1: Contraction via Effective Per-Decision Bounds
The generalized operator M updates values using traces that depend on entire history sequences. The paper proves that if the ratio of the current trace to the previous trace (the "effective" per-decision factor) never exceeds the Importance Sampling ratio ρ_t, the resulting update matrix Z has non-negative elements and row sums ≤ γ. This prevents the unstable "bootstrap subtraction" effect and ensures convergence via the Banach fixed-point theorem.

### Mechanism 2: Credit Preservation via Recency-Bounded Thresholds
RBIS defines β_t = min(λ^t, β_{t-1}ρ_t). Unlike Retrace, which cuts traces based solely on the current IS ratio (min(1, ρ_t)), RBIS allows the accumulated trace to grow unboundedly large (matching the IS product) as long as it does not exceed a decay threshold λ^t. This "reverses" the opportunity cost of cutting traces prematurely, ensuring that early critical actions retain high eligibility for credit assignment.

### Mechanism 3: Divergence from Non-Markovian Bootstrap Subtraction
When β_t > β_{t-1}ρ_t, the algebraic expansion of the M operator reveals a weighted sum where some bootstrap weights become negative. In Truncated IS, clipping the IS product to a fixed cap can cause the trace to jump from a low value to a high value relative to the previous step, violating the contraction requirement and destabilizing learning.

## Foundational Learning

- **Concept: Importance Sampling (IS) Ratios (ρ_t)**
  - Why needed here: The entire framework relies on correcting off-policy bias using ρ_t = π(a|s)/μ(a|s). You must understand that ρ_t is the "exchange rate" between behavior and target policies.
  - Quick check question: If the behavior policy selects an action with probability 0.2 and the target policy selects it with 0.8, what is ρ_t? (Answer: 4.0)

- **Concept: Eligibility Traces (λ-returns)**
  - Why needed here: The paper generalizes eligibility traces from per-decision (Markov) to trajectory-aware. You need to know that traces determine how much a reward at time t updates a state visited at time t-k.
  - Quick check question: In standard TD(λ), what happens to the trace for a state if λ=0? (Answer: It updates only the immediately preceding state/1-step return).

- **Concept: Contraction Mapping**
  - Why needed here: The paper's theoretical contribution is proving the M operator is a contraction. You need to grasp that a contraction mapping guarantees a unique fixed point (convergence) regardless of initialization.
  - Quick check question: Does an operator with norm (contraction factor) > 1 guarantee convergence? (Answer: No, it likely diverges).

## Architecture Onboarding

- **Component map:** M Operator Core -> Trace Manager (β_t) -> IS Calculator (ρ_t) -> Buffer/History
- **Critical path:** Implementing the recursive trace update for RBIS (Algorithm 2). You must maintain the dynamic array Y (storing β_{t-1}) and update it as: Y(k) = min(lambda^(t-k), Y(k) * rho_t). This ensures Condition 5.1 holds.
- **Design tradeoffs:** RBIS vs. Retrace: RBIS preserves long-term credit (better for sparse rewards/long horizons) but requires checking a decay threshold. Retrace is computationally simpler (per-decision) but cuts traces aggressively, potentially slowing learning in "tightrope" scenarios.
- **Failure signatures:**
  - Truncated IS Divergence: Value estimates exploding or oscillating in off-policy settings with high IS ratios. (Check: Is β_t jumping above β_{t-1}·ρ_t?)
  - Trace Collapse (Retrace): Very slow learning in long-horizon tasks where ρ_t is occasionally small, causing the trace to decay to zero prematurely.
- **First 3 experiments:**
  1. Replicate Bifurcated Gridworld: Implement RBIS (Algorithm 2) and Retrace. Plot Area Under the Curve (AUC) against λ-values (0 to 1) to verify that RBIS maintains higher AUC in the mid-to-high λ range.
  2. Divergence Test (Counterexample 5.7): Implement Truncated IS in the specific 1-state, 2-action MDP described in the paper (Section C.1) with γ=0.94, p=0.6 to confirm value divergence.
  3. Sensitivity Analysis: Stress test RBIS with varying divergence gaps between target and behavior policies to observe if the λ^t threshold effectively bounds variance without collapsing the trace.

## Open Questions the Paper Calls Out

- **Open Question 1:** Is Condition 5.1 (β_t ≤ β_{t-1}ρ_t) necessary for convergence of trajectory-aware methods, or merely sufficient?
  - Basis: The authors state it remains open whether Condition 5.1 is necessary or merely sufficient.
  - Why unresolved: The proofs use Condition 5.1 to guarantee the matrix Z has nonnegative elements, but M could theoretically remain a contraction even when Z has negative elements.

- **Open Question 2:** Under what precise conditions does Truncated IS converge in off-policy settings?
  - Basis: The authors state the precise conditions under which Truncated IS converges remains an open problem.
  - Why unresolved: Truncated IS violates Condition 5.1, so existing theorems cannot guarantee convergence. The counterexample shows divergence is possible, but the full boundary between convergence and divergence is unmapped.

- **Open Question 3:** Can the convergence guarantees for trajectory-aware operators be extended to stochastic algorithms with function approximation?
  - Basis: The conclusion states this work focused on convergence in expectation and extending to function approximation is a natural next step.
  - Why unresolved: The current analysis addresses the expected update operator M, but practical TD learning involves noise from stochastic samples and requires appropriately annealed step sizes.

## Limitations

- Theoretical framework assumes tabular settings rather than function approximation, limiting practical applicability to modern deep RL.
- Divergence counterexamples rely on specific MDP structures and initializations that may not generalize to more complex environments.
- Empirical evaluation limited to gridworld domains without testing on continuous control tasks or more challenging benchmarks.

## Confidence

- Theoretical convergence guarantees (Section 5.1): High - The contraction mapping proofs are mathematically rigorous and the counterexamples for violated conditions are explicit.
- RBIS performance claims: Medium - Results show consistent improvement in gridworlds, but the narrow task set and lack of comparison to modern deep RL methods limit generalizability.
- Practical implementation recommendations: Medium - While RBIS is presented as robust across λ-values, the paper doesn't provide guidance on hyperparameter tuning or computational overhead compared to simpler methods.

## Next Checks

1. **Function Approximation Test:** Implement RBIS with neural network function approximation to verify that the theoretical contraction properties break down in practical settings and identify new stability requirements.

2. **Continuous Control Benchmark:** Evaluate RBIS on continuous control tasks (e.g., MuJoCo locomotion) to test whether the trajectory-aware benefits translate beyond tabular gridworlds.

3. **Variance Analysis:** Measure and compare the variance of return estimates across all compared methods, particularly examining whether RBIS's trace preservation strategy introduces harmful variance that offsets its learning speed advantages.