---
ver: rpa2
title: 'SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation'
arxiv_id: '2505.20622'
source_url: https://arxiv.org/abs/2505.20622
tags:
- translation
- seqpo-simt
- latency
- simt
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sequential Policy Optimization for Simultaneous
  Machine Translation (SeqPO-SiMT), a novel RLHF framework designed for multi-step
  SiMT tasks. Unlike single-step RLHF methods like PPO or DPO, SeqPO-SiMT simulates
  the streaming SiMT process by segmenting full sentences into chunks and optimizing
  with a fused reward that balances translation quality and latency.
---

# SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation

## Quick Facts
- arXiv ID: 2505.20622
- Source URL: https://arxiv.org/abs/2505.20622
- Reference count: 39
- One-line primary result: SeqPO-SiMT improves COMET scores by up to 1.13 points while reducing AL by 6.17 on NEWSTEST2021, with 7B LLM performance rivaling offline models.

## Executive Summary
This paper introduces Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a novel RLHF framework designed for multi-step SiMT tasks. Unlike single-step RLHF methods like PPO or DPO, SeqPO-SiMT simulates the streaming SiMT process by segmenting full sentences into chunks and optimizing with a fused reward that balances translation quality and latency. Experiments on six datasets (Zh→En and En→Zh) show SeqPO-SiMT consistently achieves higher COMET scores with lower AL than SFT baselines, improving by up to 1.13 COMET points and reducing AL by 6.17 in NEWSTEST2021. Notably, its 7B LLM performance rivals high-performing offline models like Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct, despite operating with far less context.

## Method Summary
SeqPO-SiMT treats simultaneous machine translation as a sequential decision-making problem, where the model generates translations chunk-by-chunk based on streaming source input and translation history. The method uses Group Relative Policy Optimization (GRPO) with a fused reward combining normalized COMET (quality) and truncated Average Lagging (latency) scores. The model is first pre-trained on a large SFT dataset of partial translation pairs, then fine-tuned using the sequential policy optimization framework. The key innovation is the multi-step chunking approach that allows the model to learn optimal waiting and translation decisions dynamically, rather than applying single-step RLHF to full sequences.

## Key Results
- Achieves up to 1.13 COMET points improvement over SFT baseline on NEWSTEST2021
- Reduces Average Lagging by 6.17 points compared to baseline on the same dataset
- 7B LLM performance rivals high-performing offline models like Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct
- Consistent improvements across six datasets for both Zh→En and En→Zh translation directions
- Demonstrates effective quality-latency trade-off optimization through sequential decision modeling

## Why This Works (Mechanism)

### Mechanism 1: Sequential Decision Modeling via Policy Gradient
The system segments source text into chunks and treats SiMT as an MDP where at each step $t$, the policy model generates a translation chunk based on current source chunk and translation history. The final reward $r_T$ is assigned only at step $T$, forcing the model to learn long-term credit assignment for earlier decisions through policy gradients and GRPO.

### Mechanism 2: Fused Reward with Truncation and Normalization
A reward function combining quality (COMET) and latency (AL) enables joint optimization, but requires specific normalization and truncation to prevent the model from "hacking" the latency metric. The latency score is truncated by chunk size to avoid artificial AL reduction through excessive token generation.

### Mechanism 3: Group Relative Policy Optimization (GRPO)
GRPO uses group sampling instead of a critic model, sampling $B$ trajectories and using the group average reward as baseline. This reduces variance in gradient estimates efficiently without the memory overhead of training a separate value network, which is particularly important for variable-length SiMT sequences.

## Foundational Learning

- **Concept: Reinforcement Learning (Policy Gradients)**
  - Why needed here: SeqPO-SiMT is fundamentally an RL algorithm. Understanding how a policy $\pi_\theta$ is updated via gradients derived from rewards is essential to grasp how the model "learns" to wait or translate.
  - Quick check question: How does the model adjust its probability of generating a specific token if that token leads to a negative reward (high latency)?

- **Concept: Simultaneous Machine Translation (SiMT) Metrics**
  - Why needed here: The paper optimizes for specific metrics like Average Lagging (AL). You must understand that AL measures how far the translation lags behind the source to interpret the results (e.g., "reducing AL by 6.17").
  - Quick check question: Does a lower Average Lagging score indicate a faster (lower latency) translation or a slower one?

- **Concept: Credit Assignment**
  - Why needed here: The reward is given only at the end of the sentence ($r_T$). The learner must understand how this final score is attributed back to early decisions (like deciding to wait for the verb at step 1).
  - Quick check question: In a 5-step translation process, how does receiving a score only at step 5 help the model improve its decision at step 2?

## Architecture Onboarding

- **Component map:** Source chunker -> Policy Model (LLM) -> Reward Module (COMET+AL) -> GRPO Optimizer -> Model weights

- **Critical path:**
  1. **Chunking:** Source sentence $x$ is split into $T$ chunks.
  2. **Rollout:** For each chunk $t$, the LLM generates a translation $\hat{y}_t$ based on history. This is repeated $B$ times to create a group.
  3. **Scoring:** The full generated sequence is scored against the reference.
  4. **Advantage Calculation:** Rewards are normalized within the group.
  5. **Update:** The LLM weights are updated to favor high-reward trajectories relative to the group average.

- **Design tradeoffs:**
  - **Chunk size ($m$):** Small chunks allow lower latency but increase the difficulty of context understanding; large chunks improve quality but increase lag.
  - **Group Size ($B$):** Larger $B$ stabilizes training but increases inference cost during the training phase.

- **Failure signatures:**
  - **"Latency Collapse" (Negative AL):** If the truncation mechanism is disabled, the model outputs gibberish tokens to minimize latency artificially.
  - **Stagnant Quality:** If $\lambda$ is too low, the model prioritizes speed and outputs empty strings or minimal translations.

- **First 3 experiments:**
  1. **Reward Ablation:** Reproduce the "w/o truncation" experiment to observe the "negative AL" failure mode and verify the reward hacking hypothesis.
  2. **Lambda Sweep:** Vary $\lambda$ (the quality/latency trade-off weight) to plot the Pareto frontier and confirm if the default $\lambda=2$ is optimal for your specific dataset.
  3. **Baseline Comparison:** Compare against the "wait-k" baseline on a held-out set to visualize how SeqPO dynamically adjusts the "wait" decision compared to the rigid "wait-k" rule.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SeqPO-SiMT maintain training stability and computational efficiency when scaling to extremely large language models (e.g., 671B parameters)?
- Basis in paper: The authors state in the Limitations section: "We cannot ensure that this method can scale to extremely large LLMs, like Deepseek-V3-671B. Future works can scale to larger language models."
- Why unresolved: The current experiments were restricted to 7B and 8B models; the memory overhead of the multi-step sampling process and GRPO optimization may scale non-linearly with model size.
- What evidence would resolve it: Successful application and convergence of SeqPO-SiMT on models with parameters exceeding 70B, demonstrating manageable GPU memory usage and training time.

### Open Question 2
- Question: How can the fused reward mechanism be adapted to effectively balance quality and latency across a multilingual simultaneous translation setup?
- Basis in paper: The paper notes: "The current method is still bilingual... balancing quality and latency across multiple languages presents significant challenges. Future research could expand to multilingual SiMT."
- Why unresolved: The current reward normalization (standard deviation) and latency truncation are calculated per-sample or per-batch, which may be inconsistent when language pairs have vastly different syntactic structures or latency distributions.
- What evidence would resolve it: Experiments on a many-to-many translation benchmark showing consistent COMET and AL improvements across all language pairs within a single unified model.

### Open Question 3
- Question: Does SeqPO-SiMT generalize to language pairs with high syntactic divergence (e.g., Subject-Object-Verb languages) where latency-quality trade-offs are more severe?
- Basis in paper: The paper focuses exclusively on English-Chinese (Zh $\leftrightarrow$ En) tasks. While these have structural differences, the authors did not test on language pairs with significantly different word orders (like English-Japanese) where the "wait-for-context" pressure is fundamentally higher.
- Why unresolved: The optimal policy for En-Zh may rely on specific heuristics learned from that data distribution; languages requiring end-of-sentence verbs might force the model into higher latency regimes not explored in the current data.
- What evidence would resolve it: Evaluation of SeqPO-SiMT on SVO-to-SOV translation tasks (e.g., En $\rightarrow$ Ja or De) to verify if the reward fusion still outperforms SFT baselines in high-latency scenarios.

## Limitations
- The exact source chunk size $m$ is not specified in the provided text, which is a critical hyperparameter for the SiMT process
- The specific prompt template used to generate the SFT training data from LLMs is described conceptually but not detailed
- Reported results are primarily benchmark-based with COMET scores, lacking human evaluation to validate perceived translation quality

## Confidence

- **High Confidence:** The sequential decision-making framework and the GRPO optimization approach are technically sound and well-explained. The mechanism of using group relative baselines is a well-established method in RL.
- **Medium Confidence:** The fused reward function with normalization and truncation appears theoretically sound, but the specific implementation details (exact truncation formula, normalization parameters) are not fully specified. The claim that SeqPO outperforms SFT baselines is supported by the data, but the exact magnitude may depend on implementation details.
- **Low Confidence:** The claim that SeqPO "rivals" offline models like Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct is based on COMET scores alone. Without human evaluation or additional metrics, it's difficult to assess whether the streaming translation quality is truly comparable to offline models that process full sentences.

## Next Checks

1. **Reproduce Reward Ablation:** Implement and test the "w/o truncation" variant to observe whether the model exhibits the described "negative AL" failure mode, confirming that the truncation mechanism prevents reward hacking as claimed in Figure 6.

2. **Lambda Sensitivity Analysis:** Conduct a systematic sweep of the quality/latency trade-off weight $\lambda$ to generate a Pareto frontier. This will verify whether the default value of $\lambda=2$ is indeed optimal or if the model's performance is sensitive to this hyperparameter across different domains.

3. **Human Evaluation Study:** Conduct a human evaluation study comparing SeqPO-generated translations against both the SFT baseline and the offline reference models (Qwen-2.5-7B-Instruct, LLaMA-3-8B-Instruct). This will validate whether the COMET score improvements translate to perceptible quality improvements in actual simultaneous interpretation scenarios.