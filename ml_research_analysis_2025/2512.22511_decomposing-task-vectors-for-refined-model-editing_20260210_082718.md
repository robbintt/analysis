---
ver: rpa2
title: Decomposing Task Vectors for Refined Model Editing
arxiv_id: '2512.22511'
source_url: https://arxiv.org/abs/2512.22511
tags:
- task
- vectors
- shared
- unique
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a principled decomposition method for task\
  \ vectors\u2014parameter differences between fine-tuned and pre-trained models\u2014\
  into shared and unique components. The approach uses column space projections to\
  \ identify invariant subspaces across task vectors, enabling isolation of task-specific\
  \ behaviors from common knowledge."
---

# Decomposing Task Vectors for Refined Model Editing

## Quick Facts
- arXiv ID: 2512.22511
- Source URL: https://arxiv.org/abs/2512.22511
- Reference count: 40
- Key outcome: Task vector decomposition enables controlled model editing by separating shared and unique knowledge components

## Executive Summary
This paper introduces a principled approach to decomposing task vectors—parameter differences between fine-tuned and pre-trained models—into shared and unique components. The method uses column space projections to identify invariant subspaces across task vectors, enabling isolation of task-specific behaviors from common knowledge. This decomposition addresses interference issues during task vector arithmetic, which can lead to unpredictable outcomes when overlapping concepts interact. The approach is validated across three domains: image classification, diffusion models, and language models, demonstrating practical utility for multi-task merging, style mixing, and behavior modification.

## Method Summary
The method decomposes task vectors into shared and unique components through orthogonal projection onto the column space of the task vector matrix. The column space represents the common subspace across all task vectors, while the orthogonal complement captures unique task-specific information. By isolating these components, the approach enables controlled arithmetic operations: shared components can be merged across tasks to enhance transfer learning, while unique components can be combined without interference. The decomposition is achieved through singular value decomposition (SVD) and projection operations, providing a mathematically rigorous framework for understanding and controlling task vector interactions.

## Key Results
- Multi-task merging in image classification improved by 5% through shared component usage
- Clean style mixing in diffusion models without generation degradation by combining only unique components
- 47% toxicity reduction in language models while preserving general knowledge task performance by negating the isolated toxic component

## Why This Works (Mechanism)
The decomposition works by exploiting the linear structure of task vectors in parameter space. When multiple tasks are learned, their parameter updates often share common directions representing general knowledge or base capabilities, while also having task-specific directions. By projecting onto the column space of the task vector matrix, the method identifies these shared directions through their presence across multiple task vectors. The orthogonal complement then captures what makes each task unique. This separation enables controlled composition: shared components can be safely combined to enhance common capabilities, while unique components can be mixed without the interference that occurs when conceptually overlapping but structurally different parameter updates are added together naively.

## Foundational Learning

Singular Value Decomposition (SVD)
- Why needed: Provides optimal low-rank approximation and basis for column space extraction
- Quick check: Verify that UΣV^T reconstructs the original matrix and that Σ contains ordered singular values

Column Space Projection
- Why needed: Identifies shared subspace across task vectors through linear algebra
- Quick check: Confirm that projected vectors lie within the span of the original matrix columns

Orthogonal Complement
- Why needed: Isolates unique task-specific components by removing shared subspace
- Quick check: Verify that shared and unique components are orthogonal (dot product ≈ 0)

Task Vector Arithmetic
- Why needed: Foundation for understanding interference when combining model edits
- Quick check: Test that adding task vectors produces predictable behavior when concepts are independent

Linear Subspace Methods
- Why needed: Provides mathematical framework for component isolation and decomposition
- Quick check: Confirm that decomposition satisfies subspace properties (closure under addition and scalar multiplication)

## Architecture Onboarding

Component map: Pre-trained Model -> Fine-tuned Models -> Task Vectors -> SVD Decomposition -> Shared/Unique Components -> Controlled Arithmetic

Critical path: The decomposition occurs through SVD of the task vector matrix, followed by projection onto column space (shared) and its orthogonal complement (unique). The critical insight is that this linear algebra operation cleanly separates common from task-specific knowledge.

Design tradeoffs: The method trades computational overhead of SVD decomposition for controlled composition capabilities. While exact decomposition requires full SVD, approximate methods could reduce cost at the expense of component purity.

Failure signatures: Poor decomposition quality manifests as residual interference during component arithmetic, visible as degraded performance when combining supposedly independent components. This typically indicates insufficient task diversity or highly overlapping concept spaces.

First experiments:
1. Verify decomposition on synthetic task vectors with known shared/unique structure
2. Test component orthogonality through dot product analysis
3. Validate controlled arithmetic by combining components from independent tasks

## Open Questions the Paper Calls Out

None

## Limitations

The orthogonality assumption between shared and unique components may not hold for tasks with complex interdependencies or overlapping concept representations. The column space projection technique requires sufficient diversity in the task vector set to accurately identify the shared subspace, potentially degrading in scenarios with limited task variety or highly specialized tasks.

## Confidence

High confidence in the mathematical framework and decomposition methodology
Medium confidence in cross-domain generalization due to varying task characteristics
Medium confidence in the quantitative improvements reported, though specific performance gains may vary with task selection and implementation details

## Next Checks

1. Test decomposition stability across varying task vector sets with different degrees of conceptual overlap to assess robustness to the orthogonality assumption
2. Evaluate performance on specialized domains with limited task diversity to determine decomposition quality in constrained scenarios
3. Conduct ablation studies isolating the contribution of shared vs unique components in downstream tasks to validate the practical utility of the decomposition