---
ver: rpa2
title: 'Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on Advanced
  Mathematical Problem-Solving'
arxiv_id: '2501.17084'
source_url: https://arxiv.org/abs/2501.17084
tags:
- https
- accessed
- january
- code
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks 10 large language models on 945 competition-level
  mathematics problems, evaluating their ability to generate and execute Python code
  as a reasoning step. Using a mistral-large-2411 evaluator, results show a 34.5%
  performance gap between the best commercial model (gpt-4o-mini, 83.7% accuracy)
  and the weakest open-source model (open-codestral-mamba:v0.1, 49.2%).
---

# Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on Advanced Mathematical Problem-Solving
## Quick Facts
- arXiv ID: 2501.17084
- Source URL: https://arxiv.org/abs/2501.17084
- Reference count: 0
- Performance gap: 34.5% between best commercial and weakest open-source models

## Executive Summary
This study benchmarks 10 large language models on 945 competition-level mathematics problems, evaluating their ability to generate and execute Python code as a reasoning step. The evaluation reveals a substantial 34.5% performance gap between the best commercial model (gpt-4o-mini, 83.7% accuracy) and the weakest open-source model (open-codestral-mamba:v0.1, 49.2%). Accuracy consistently declined with problem difficulty, and models struggled most with Number Theory. Token-by-token regeneration on llama3.1:8b yielded a marginal +0.8% accuracy gain but reduced execution time by 36.7%. Less than 1% of generated code was unsafe, while 3.17% of problems remained unsolved after 10 attempts, highlighting room for improvement in hybrid reasoning approaches.

## Method Summary
The study evaluates 10 large language models on 945 competition-level mathematics problems using a mistral-large-2411 evaluator. Models are assessed on their ability to generate and execute Python code as a reasoning step. The evaluation includes token-by-token regeneration experiments on llama3.1:8b to measure accuracy gains and execution time changes. Safety analysis is performed on generated code, and unsolved problems are tracked across multiple attempts.

## Key Results
- 34.5% performance gap between best commercial model (gpt-4o-mini, 83.7% accuracy) and weakest open-source model (open-codestral-mamba:v0.1, 49.2%)
- Accuracy declined with problem difficulty; Number Theory was the most challenging domain
- Token-by-token regeneration yielded +0.8% accuracy gain but reduced execution time by 36.7%

## Why This Works (Mechanism)
The study demonstrates that hybrid reasoning approaches combining code generation and execution can effectively solve advanced mathematical problems. The mechanism relies on iterative refinement through token-by-token regeneration, which allows models to correct errors in their generated code. The performance gap between commercial and open-source models suggests that larger, more capable models can better handle the complexity of mathematical reasoning tasks.

## Foundational Learning
- **Mathematical problem-solving with code generation**: Needed to evaluate models' ability to translate abstract problems into executable solutions; quick check: verify code correctness and problem-solving accuracy.
- **Token-by-token regeneration**: Required for iterative refinement of generated code; quick check: measure accuracy gains and execution time changes.
- **Safety analysis of generated code**: Important to ensure generated solutions are safe and reliable; quick check: assess percentage of unsafe code and identify patterns.

## Architecture Onboarding
**Component Map**: Problem -> Code Generation -> Code Execution -> Evaluation -> Safety Check
**Critical Path**: Problem statement → Code generation → Code execution → Solution verification → Safety validation
**Design Tradeoffs**: Accuracy vs. execution time in token-by-token regeneration; model capability vs. computational cost
**Failure Signatures**: Unsolved problems after 10 attempts (3.17%); domain-specific weaknesses (Number Theory); marginal accuracy gains from regeneration
**3 First Experiments**: 1) Cross-validate evaluator results using alternative evaluators; 2) Conduct ablation studies on regeneration attempt limits; 3) Analyze unsolved problems through qualitative review

## Open Questions the Paper Calls Out
None

## Limitations
- Single evaluator introduces potential bias
- Marginal accuracy gain from regeneration may not justify computational overhead
- Low unsafe code rate could reflect limitations in safety evaluation framework

## Confidence
- Commercial vs. open-source performance gap (High): Transparent methodology and consistent results
- Domain-specific weaknesses (High): Supported by large sample size
- Token-by-token regeneration benefits (Medium): Marginal accuracy gain contrasts with substantial time reduction
- Safety evaluation (Low): Lacks detail on evaluation criteria and potential blind spots

## Next Checks
1. Cross-validate evaluator results using at least two independent evaluators to confirm the 34.5% performance gap is not evaluator-dependent.
2. Conduct ablation studies on token-by-token regeneration by varying attempt limits (e.g., 5, 10, 20) to determine if accuracy gains plateau or if execution time savings remain consistent.
3. Analyze unsolved problems (3.17%) through qualitative review to distinguish between problems requiring novel approaches versus those with identifiable solution patterns that models failed to recognize.