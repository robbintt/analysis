---
ver: rpa2
title: Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model
arxiv_id: '2512.00630'
source_url: https://arxiv.org/abs/2512.00630
tags:
- classification
- financial
- text
- qwen3-8b
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of financial text classification
  for quantitative trading systems, focusing on sentiment and news topic classification.
  The core method involves finetuning the Qwen3-8B model using Noisy Embedding Instruction
  Finetuning (NEFTune) combined with rank-stabilized Low-Rank Adaptation (rLoRA) and
  FlashAttention for efficiency.
---

# Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model

## Quick Facts
- arXiv ID: 2512.00630
- Source URL: https://arxiv.org/abs/2512.00630
- Authors: Zhiming Lian
- Reference count: 0
- Primary result: Qwen3-8B achieves 0.8415 sentiment accuracy and 0.9315 topic accuracy using rLoRA+NEFTune

## Executive Summary
This paper addresses financial text classification for quantitative trading systems by finetuning Qwen3-8B with Noisy Embedding Instruction Finetuning (NEFTune) and rank-stabilized Low-Rank Adaptation (rLoRA). The approach combines Qwen3-8B's grouped-query attention, rotary position embeddings, and scalable context window with parameter-efficient tuning techniques. Experiments demonstrate that this configuration outperforms several strong baselines including RoBERTa, BERT, Baichuan2-7B, LLaMA-7B, and LLaMA2-7B on both financial sentiment and topic classification tasks, while requiring only three epochs compared to traditional methods' ten-plus epochs.

## Method Summary
The method employs Qwen3-8B finetuned using NEFTune for robustness and rLoRA for parameter efficiency. NEFTune injects uniform noise (α=0.3) into embedding layers during supervised training to improve generalization, while rLoRA stabilizes low-rank adaptation by scaling updates by √rank to prevent performance degradation at higher ranks. The model uses grouped-query attention with 32 query heads and 8 key-value heads for inference efficiency, along with FlashAttention for memory optimization. Training uses batch_size=3, gradient_accumulation=4, learning_rate=5e-5, max_tokens=360, and 3 epochs with Adam optimizer and cross-entropy loss.

## Key Results
- Sentiment classification accuracy: 0.8415 (vs 0.7928 for RoBERTa baseline)
- Topic classification accuracy: 0.9315 on 20-class Twitter financial news dataset
- Convergence achieved in 3 epochs versus >10 for traditional finetuning methods
- Parameter-efficient approach maintains strong performance while reducing computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Rank-Stabilized Low-Rank Adaptation (rLoRA)
Scaling LoRA updates by √r enables stable convergence at higher ranks without performance degradation. Standard LoRA adds trainable low-rank matrices A×B to frozen weights, but variance explodes when rank increases. rLoRA divides update magnitude by √r, allowing safe use of rank=8 instead of defaulting to lower ranks. Core assumption: financial classification benefits from higher-rank adaptations capturing more task-specific variance. Break condition: if gradient norms explode or loss diverges in first 100 steps despite rLoRA scaling.

### Mechanism 2: Noisy Embedding Instruction Finetuning (NEFTune)
Injecting uniform noise into input embeddings during training acts as regularization, improving generalization on held-out financial texts. Noise sampled from U(-α, α) forces the model to learn robust representations rather than overfitting to exact embedding coordinates. With α=0.3, this simulates lexical diversity in financial text. Break condition: if validation accuracy degrades relative to non-NEFTune baseline.

### Mechanism 3: Grouped-Query Attention for Inference Efficiency
Reducing KV-heads to 8 (vs 32 query heads) cuts memory bandwidth by ~4× with negligible accuracy loss, enabling real-time trading latency. Standard multi-head attention uses separate K,V projections per head, while GQA shares KV-heads across multiple query heads, reducing KV-cache size. Break condition: if topic classification accuracy drops >2% vs full attention baseline.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: rLoRA builds directly on LoRA's W' = W + BA decomposition. Without understanding rank constraints and merge behavior, debugging convergence issues is impossible.
  - Quick check question: If LoRA rank=8 and hidden_dim=512, what are the dimensions of matrices A and B?

- Concept: Instruction Tuning Templates
  - Why needed here: The paper uses explicit `\no_think` tags and two-turn user/assistant formatting. Misformatted templates cause silent failures where the model generates reasoning chains instead of labels.
  - Quick check question: What token does Qwen3 expect to signal end of thinking mode?

- Concept: Embedding Noise Regularization
  - Why needed here: NEFTune's α=0.3 is a hyperparameter requiring tuning. Understanding why noise helps (smoothed decision boundaries) vs hurts (signal corruption) informs debugging.
  - Quick check question: Should noise be applied to position embeddings in addition to token embeddings?

## Architecture Onboarding

- Component map: Tokenize with chat template → embed tokens → add NEFTune noise → 36 transformer blocks with rLoRA-adapted projections → extract last-token logits → map to classes → compute cross-entropy loss

- Critical path: 1. Tokenize with chat template → embed tokens → add NEFTune noise 2. Forward through 36 transformer blocks with rLoRA-adapted projections 3. Extract last-token logits → map to {Positive, Negative, Neutral} or 20 topic classes 4. Compute cross-entropy loss → backprop through rLoRA params only

- Design tradeoffs:
  - Rank=8 vs rank=16: Higher rank captures more task variance but risks instability; rLoRA mitigates but doesn't eliminate
  - α=0.3 vs α=0.1: Higher noise improves robustness on diverse texts but may corrupt short inputs
  - 3 epochs vs 5 epochs: Paper claims convergence by epoch 3; longer training risks overfitting on 2,879 sentiment samples

- Failure signatures:
  - Loss plateau at epoch 1 with no improvement: Check learning rate (5e-5) or verify rLoRA is actually training (print grad norms)
  - Model outputs reasoning chains instead of labels: `\no_think` tag missing or template malformed
  - OOM at 360 token length: FlashAttention not enabled or batch_size too large

- First 3 experiments:
  1. Ablate NEFTune: Train with α=0.0 vs α=0.3 on sentiment task; expect ~1-2% accuracy gap if mechanism works
  2. Rank sweep: Test r∈{4, 8, 16} with rLoRA scaling; verify r=8 is optimal, not just convenient
  3. Baseline architecture comparison: Run RoBERTa-large on identical data to reproduce paper's 0.7928 vs 0.8415 gap

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset provenance unclear: Exact sources for financial sentiment and Twitter financial news datasets not specified, preventing exact reproduction
- No ablation studies: NEFTune effectiveness lacks sensitivity analysis or comparison with other regularization techniques
- Efficiency claims unverified: No actual inference latency measurements provided to support real-time capability claims

## Confidence

- High confidence: Technical architecture clearly specified with concrete hyperparameters (rank=8, α=0.3, batch_size=3, lr=5e-5, 3 epochs)
- Medium confidence: Comparative results presented with specific accuracy numbers, but baseline training configurations not detailed
- Low confidence: Convergence claim (3 epochs vs >10) lacks supporting evidence such as learning curves or validation curves

## Next Checks

1. **Ablation study of NEFTune effectiveness**: Train Qwen3-8B with rLoRA only (α=0.0) versus NEFTune (α=0.3) on sentiment classification task using identical hyperparameters. Measure both final accuracy and training stability metrics (loss variance, gradient norms).

2. **Rank sensitivity analysis for rLoRA**: Systematically test rLoRA with ranks {4, 8, 16, 32} while maintaining √r scaling factor. Plot accuracy versus rank to identify optimal tradeoff between task performance and parameter efficiency.

3. **KV-head sharing impact measurement**: Conduct controlled experiments comparing Qwen3-8B with full attention (32 KV-heads) versus GQA (8 KV-heads) on 20-class topic classification task. Measure both accuracy degradation and inference memory usage.