---
ver: rpa2
title: 'FSC-Net: Fast-Slow Consolidation Networks for Continual Learning'
arxiv_id: '2511.11707'
source_url: https://arxiv.org/abs/2511.11707
tags:
- consolidation
- learning
- replay
- task
- seeds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FSC-Net addresses catastrophic forgetting in continual learning
  by introducing a dual-network architecture inspired by biological memory consolidation.
  The method employs a fast network (NN1) for rapid task adaptation and a slow network
  (NN2) that consolidates knowledge through replay and distillation.
---

# FSC-Net: Fast-Slow Consolidation Networks for Continual Learning

## Quick Facts
- **arXiv ID:** 2511.11707
- **Source URL:** https://arxiv.org/abs/2511.11707
- **Reference count:** 27
- **Primary result:** Dual-network architecture achieves 91.71%±0.62% retention on Split-MNIST through pure replay consolidation without distillation

## Executive Summary
FSC-Net addresses catastrophic forgetting in continual learning by separating rapid task adaptation from gradual knowledge consolidation. The method employs a fast network (NN1) for immediate task learning and a slow network (NN2) that consolidates knowledge through replay and distillation. Through systematic hyperparameter analysis, FSC-Net demonstrates that pure replay without distillation during consolidation achieves superior performance by avoiding recency bias. The approach shows strong empirical gains: +4.27pp over fast network alone on Split-MNIST and +8.20pp on Split-CIFAR-10.

## Method Summary
FSC-Net implements a dual-network architecture inspired by biological memory systems. During task training, NN1 adapts rapidly to new tasks while NN2 receives distillation from NN1's softened outputs every 10 batches. After each task, NN2 undergoes consolidation using replay buffer samples with pure cross-entropy (λ=0), avoiding distillation-induced recency bias. The slow network processes concatenated raw input and NN1's 64-dim summary embedding through an 848→256→128→10 MLP. Key hyperparameters include learning rates (1e-3 for NN1, 5e-4 for NN2), replay probability (0.3), and buffer size (200 samples/task).

## Key Results
- Split-MNIST (30 seeds): 91.71%±0.62% retention accuracy (+4.27pp over fast network alone, p<1e-10)
- Split-CIFAR-10 (5 seeds): 33.31%±0.38% retention (+8.20pp over fast network alone, p<1e-3)
- Architecture ablation: Simple MLP (126K params) outperforms similarity-gated variants (145K params) by 1.2pp

## Why This Works (Mechanism)

### Mechanism 1: Temporal Decoupling of Fast and Slow Learning
Separating rapid adaptation from gradual consolidation reduces interference between new task learning and long-term knowledge retention. NN1 uses aggressive learning rate (10⁻³) for immediate task adaptation; NN2 uses conservative learning rate (5×10⁻⁴) and updates less frequently (every 10 batches during task training). This creates distinct timescales where fast learning does not directly destabilize consolidated representations. Core assumption: gradient updates for new tasks are sufficiently decorrelated from consolidation updates. Evidence: dual-network architecture design and performance gains. Break condition: highly similar tasks requiring shared low-level features.

### Mechanism 2: Pure Replay Avoids Recency Bias from Distillation
During offline consolidation, pure replay on ground-truth labels (λ=0) outperforms knowledge distillation from NN1 because distillation introduces recency bias toward recently learned tasks. When NN1 has recently learned task k, its output distribution places disproportionate probability mass on active classes from that task. The KL divergence gradient in distillation pulls NN2 toward these recent logits, reducing margins on earlier tasks. Pure cross-entropy on replay labels avoids this bias. Core assumption: replay buffer contains representative samples with correct labels. Evidence: λ=0 achieves 91.71%±0.62% vs λ=0.5 at 90.20%±1.67% on MNIST (+1.51%, p=0.021). Break condition: severely imbalanced or noisy replay buffer.

### Mechanism 3: Consolidation Effectiveness Driven by Methodology, Not Architecture Complexity
Within MLP-based architectures, consolidation gains come from the dual-timescale training protocol rather than specialized architectural components. Simple MLP (126K params) outperforms similarity-gated variants (145K params) by 1.2pp, suggesting architectural embellishments add noise without improving knowledge transfer. Core assumption: finding generalizes within tested MLP family. Evidence: Simple MLP achieves 91.5% NN2 retention vs Similarity-gated: 91.2%. Break condition: more complex data modalities where architectural capacity becomes bottleneck.

## Foundational Learning
- **Knowledge Distillation:** Essential for understanding how NN2 learns from NN1's softened outputs during task training. Quick check: Explain why T>1 produces softer probability distributions and how this affects gradient magnitude during distillation.
- **Experience Replay Buffers:** Critical for maintaining 200 samples/task from prior tasks. Quick check: What happens to retention if replay buffer contains class-imbalanced samples from earlier tasks?
- **Catastrophic Forgetting:** The entire framework addresses this fundamental problem. Quick check: Why does fine-tuning achieve only 21.3% on Split-MNIST when individual binary tasks should be easy to solve?

## Architecture Onboarding

**Component map:**
- NN1 (Fast): 784→128→64→128→64→10 MLP with LayerNorm+ReLU; outputs class logits (y_NN1) and 64-dim summary embedding (s)
- NN2 (Slow): Receives concatenated [x; s] (848-dim), processes through 848→256→128→10 with Dropout(0.2) and LayerNorm
- Replay Buffer: Stores 200 samples per task; sampled with probability p_replay=0.3 during task training
- Loss Computation: L2 = (1−λ)L_CE + λL_KD where λ=0.3 during task training, λ=0 during consolidation

**Critical path:**
1. Initialize both networks; freeze NN1 only during consolidation phase
2. For each task: train NN1 on mixed current + replay data for 5 epochs
3. Every 10 batches, update NN2 with distillation loss (λ=0.3)
4. After task completion: run 2 epochs of consolidation on replay buffer with pure replay (λ=0)
5. Evaluate using NN2 for final retention metrics

**Design tradeoffs:**
- Memory: Dual networks require ~2× parameters; replay buffer adds 200 samples/task
- Distillation timing: λ=0.3 during training helps NN2 track NN1; λ=0 during consolidation avoids recency bias—setting λ constant across phases degrades performance by ~1.5pp
- Buffer size: 200 samples/task is sweet spot; 50 samples drops to 80.5%, 500 samples yields diminishing returns (92.7%)

**Failure signatures:**
- NN2 retention approaches NN2 retention (no consolidation gain): Check that NN2 learning rate is lower than NN1, consolidation epochs are non-zero, and replay buffer is being sampled
- High variance across seeds (>2% std): Likely using λ>0 during consolidation; verify consolidation phase uses λ=0
- CIFAR-10 retention below 50%: Expected with MLP backbone; architecture capacity is insufficient for visual complexity

**First 3 experiments:**
1. Reproduce Split-MNIST baseline: Train NN1 alone (no NN2) for 5 tasks; should achieve ~87-89% retention. This validates your data pipeline and establishes forgetting baseline.
2. Ablate consolidation λ: Compare λ∈{0.0, 0.3, 0.5} during consolidation on seed 42; should see peak at λ=0.0 (~90-91%). This validates recency bias finding before committing to full 30-seed runs.
3. Verify NN2 dependence on NN1 embedding: Run with detached vs non-detached summary embedding s; paper detaches to prevent gradients flowing back to NN1. Performance should be similar; if not, check gradient flow.

## Open Questions the Paper Calls Out

### Open Question 1
Does the consolidation protocol transfer effectively to convolutional and transformer architectures, and can it achieve practical performance on complex visual benchmarks like CIFAR-10? The authors state extension to CNNs/Transformers is future work and acknowledge MLP achieves only 33.31% on Split-CIFAR-10, "below the 50% chance baseline." Run FSC-Net with CNN (e.g., ResNet-9/18) or vision transformer backbones on Split-CIFAR-10; compare retention gains and absolute performance against MLP baseline.

### Open Question 2
What is the theoretical mechanism by which knowledge distillation introduces recency bias during offline consolidation? The authors provide heuristic gradient analysis but note "A fuller theoretical treatment would require modelling how replay sampling interacts with the KL term, which we leave to future work." Derive theoretical bounds on task-specific gradient interference during consolidation; or empirically measure per-task logit shifts during consolidation with vs. without distillation across varied replay distributions.

### Open Question 3
Would formal cross-seed hyperparameter selection yield different optimal configurations than single-seed (seed 42) tuning used? The methodological limitation states "formal cross-validation across seeds during hyperparameter selection would strengthen the methodology and remains future work." All 126 configurations were evaluated only on seed 42. Perform k-fold cross-validation across seeds during hyperparameter search; compare resulting optimal λ, learning rates, and buffer sizes to current defaults; assess whether retention improves.

### Open Question 4
How does FSC-Net perform in class-incremental and domain-incremental settings beyond task-incremental scenarios evaluated? The limitations section notes "We evaluate in task-incremental scenarios where task identity is unknown at test time. Class-incremental and domain-incremental settings warrant further investigation." Different continual learning scenarios impose different constraints; the dual-network consolidation mechanism may behave differently under these regimes. Evaluate FSC-Net on standard class-incremental benchmarks (e.g., CIFAR-100 sequential) and domain-incremental benchmarks (e.g., Permuted-MNIST variations) using the same hyperparameters; report retention and forgetting metrics.

## Limitations
- Limited to task-incremental settings; class-incremental and domain-incremental scenarios remain unexplored
- MLP architecture achieves only 33.31% on Split-CIFAR-10, below chance baseline, indicating insufficient capacity for complex visual data
- Hyperparameter tuning performed on single seed (42) rather than cross-validated across seeds

## Confidence
- Mechanism 1 (Temporal Decoupling): Medium - supported by design and performance but lacks direct interference measurements
- Mechanism 2 (Recency Bias): High - clear performance gains from λ=0 consolidation; gradient analysis provided
- Mechanism 3 (Methodology vs Architecture): Medium - clear within tested MLP variants, but claim extends beyond experimental scope

## Next Checks
1. Measure gradient similarity between NN1 and NN2 updates during consolidation to directly validate temporal decoupling reduces interference
2. Ablate replay buffer size systematically (50, 100, 200, 500) to confirm the 200-sample sweet spot and test distribution coverage
3. Test consolidation with balanced replay sampling versus uniform sampling to validate the assumption that buffer representativeness matters more than sampling strategy