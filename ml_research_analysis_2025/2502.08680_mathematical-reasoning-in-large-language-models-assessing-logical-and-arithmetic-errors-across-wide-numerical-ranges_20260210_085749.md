---
ver: rpa2
title: 'Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic
  Errors across Wide Numerical Ranges'
arxiv_id: '2502.08680'
source_url: https://arxiv.org/abs/2502.08680
tags:
- errors
- logical
- error
- numerical
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the evaluation gap in Large Language Models'
  (LLMs) mathematical reasoning capabilities, particularly their performance across
  diverse numerical scales and the distinction between logical and computational errors.
  The authors introduce GSM-Ranges, a dataset generator that systematically perturbs
  numerical values in GSM8K math problems across six distinct scales, and a novel
  grading methodology that distinguishes between logical and non-logical errors using
  GPT-4o to translate responses into Python code for automated evaluation.
---

# Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges

## Quick Facts
- arXiv ID: 2502.08680
- Source URL: https://arxiv.org/abs/2502.08680
- Reference count: 40
- Key finding: Logical error rates increase by up to 14 percentage points as numerical values fall outside training distribution

## Executive Summary
This paper addresses the evaluation gap in Large Language Models' (LLMs) mathematical reasoning capabilities, particularly their performance across diverse numerical scales and the distinction between logical and computational errors. The authors introduce GSM-Ranges, a dataset generator that systematically perturbs numerical values in GSM8K math problems across six distinct scales, and a novel grading methodology that distinguishes between logical and computational errors using GPT-4o to translate responses into Python code for automated evaluation. Their experiments with nine models reveal that logical error rates increase by up to 14 percentage points as numerical complexity rises, demonstrating LLMs' sensitivity to out-of-distribution numerical values. Additionally, while models perform well on standalone arithmetic tasks, their accuracy significantly deteriorates when computations are embedded within word problems. The automated grading methodology achieves 98.5% accuracy in classifying errors, and recall rate analysis shows that correct logic exists within model distributions despite larger numerical values. These findings provide a more comprehensive evaluation framework for mathematical reasoning in LLMs and highlight the need for improved numerical generalization.

## Method Summary
The study introduces GSM-Ranges, a dataset generator that perturbs numerical values in GSM8K math problems across six scales (from same-digit numbers to 1M-10M range). For each of 100 filtered GSM8K questions, 50 variations are generated per level, creating 5,000 problems per level. Models are evaluated using zero-shot inference with chain-of-thought prompting. A novel grading methodology uses GPT-4o to translate LLM responses into executable Python code, allowing automated distinction between logical errors (flawed reasoning) and non-logical errors (arithmetic/copy mistakes). The approach extracts only numeric tokens from questions to avoid biasing the grader, and if the corrected code output matches ground truth, the original error is classified as non-logical.

## Key Results
- Logical error rates increase by up to 14 percentage points as numerical values move further from training distribution
- Standalone arithmetic accuracy exceeds word problem accuracy, indicating context embedding drives computational failures
- Pass@k analysis shows correct logical reasoning exists in model distributions, recoverable with sampling (94-99% recall at pass@48)
- The automated grading methodology achieves 98.5% accuracy in distinguishing logical from non-logical errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logical error rates increase when numerical values fall outside training distribution, even when reasoning structure is unchanged.
- Mechanism: LLMs appear to anchor reasoning quality to familiar numerical ranges; large out-of-distribution values trigger degraded multi-step planning, including missing steps, operator errors, and contextual value mismatches.
- Core assumption: Training corpora for math reasoning are numerically skewed toward small values (<1,000), as shown in GSM8K/SVAMP/MATH distributions.
- Evidence anchors:
  - [abstract] "up to 14 percentage point increases in logical error rates as numerical complexity rises"
  - [Section 5.1.1] "Gemma 2 2B model shows a 14% absolute increase in logical error rate" between Level 1 and Level 6 perturbations.
  - [corpus] Neighbor paper "Exposing Numeracy Gaps" confirms LLMs struggle with numerical reasoning; FMR=0.314 suggests moderate validation.
- Break condition: If models were explicitly trained on broad numerical ranges (millions+), this degradation should attenuate or disappear.

### Mechanism 2
- Claim: Translating natural language reasoning into executable Python code isolates non-logical errors from logical errors with high reliability.
- Mechanism: By extracting the reasoning structure into code, arithmetic execution errors and number-copy errors can be corrected at runtime. If the corrected output matches ground truth, the original response contained only non-logical errors; otherwise, the reasoning itself is flawed.
- Core assumption: GPT-4o can faithfully transcribe reasoning logic into executable Python without altering the underlying logical structure.
- Evidence anchors:
  - [abstract] "The grading methodology achieves 98.5% accuracy in error classification"
  - [Section 4.4] "We found that our grading methodology correctly classified 197 of the responses correctly, achieving a high accuracy of 98.5%"
  - [corpus] No direct corpus validation for this specific code-translation mechanism; neighboring papers do not address code-based error isolation.
- Break condition: If the code generator "fixes" logical errors during translation (which the paper attempts to mitigate via prompt constraints), classification accuracy would degrade.

### Mechanism 3
- Claim: Correct logical reasoning exists in model distribution but is not reliably surfaced in single-pass greedy decoding under numerical perturbation.
- Mechanism: Sampling multiple responses at non-zero temperature increases probability of recovering correct logic; the gap between perturbation levels narrows substantially at pass@48.
- Core assumption: Logical reasoning is a distributional property, not a deterministic output; temperature-based sampling explores this distribution.
- Evidence anchors:
  - [Section 6.1] "At the highest sample size of 48, the gap between level 1 and 6 is no more than 2 for any model"
  - [Section 6.1] Recall experiments show 94-99% recovery of correct logic across models at pass@48.
  - [corpus] Neighbor paper "Can Large Reasoning Models Improve Accuracy...Using Flawed Thinking?" discusses CoT brittleness but not multi-pass recovery specifically.
- Break condition: If reasoning were fundamentally absent (not just low-probability), increased sampling would not close the gap.

## Foundational Learning

- Concept: **Distribution Shift in Tokenized Numbers**
  - Why needed here: Understanding that numerical tokens (e.g., "3,124,213") are out-of-distribution when training data skews small; this explains the logical error increase without changes to problem structure.
  - Quick check question: If 95% of your training numbers are <1,000, what happens when inference sees "7,832,129"?

- Concept: **Logical vs. Non-Logical Error Taxonomy**
  - Why needed here: The paper's core contribution requires distinguishing whether a wrong answer stems from flawed reasoning (logical) or execution/copy errors (non-logical); conflating these obscures model diagnostics.
  - Quick check question: If a model mis-copies "1,337,042" as "13,337,042" but applies correct operations, is that a logical error?

- Concept: **Pass@k Evaluation for Distributional Reasoning**
  - Why needed here: Single-pass accuracy understates latent capability; pass@k reveals whether correct reasoning exists in the distribution but requires sampling to surface.
  - Quick check question: If pass@1 = 60% and pass@48 = 95%, what does this imply about the model's learned reasoning distribution?

## Architecture Onboarding

- Component map:
  - GSM-Ranges Generator -> LLM Inference Engine -> GPT-4o Code Translator -> Error Classifier -> Results Analyzer

- Critical path:
  1. Generate perturbed questions via GSM-Ranges (6 levels × 50 variations × 100 base questions = 30,000 total).
  2. Collect LLM responses with greedy decoding.
  3. For incorrect answers, invoke GPT-4o to translate response → Python code → execute → compare to ground truth.
  4. Classify: correct / logical error / non-logical error.

- Design tradeoffs:
  - Providing only extracted numbers (not full question) to GPT-4o reduces bias but may miss context-dependent copy errors.
  - Scaling only one side of multiplication prevents answer explosion but limits perturbation coverage for certain problem structures.
  - Manual validation on 200 samples yields high confidence (98.5%) but doesn't guarantee scalability to all problem types.

- Failure signatures:
  - **Level gap persists at pass@48**: Indicates reasoning is truly absent, not just low-probability.
  - **Code generator produces correct output from flawed logic**: Suggests prompt constraints are insufficient; GPT-4o is "fixing" errors during translation.
  - **Non-logical error rate doesn't decrease with standalone arithmetic testing**: Suggests embedding context is not the primary driver of arithmetic failure.

- First 3 experiments:
  1. **Baseline calibration**: Run 100 original GSM8K questions through the grading pipeline to establish baseline logical/non-logical error rates for each model.
  2. **Numerical sensitivity sweep**: Generate Level 1–6 perturbations for a single base question (50 variations each); plot logical error rate vs. perturbation level to confirm the 14pp degradation pattern.
  3. **Multi-pass recovery test**: For 20 Level 6 questions where pass@1 fails, run pass@48 sampling (temperature=0.8) and verify whether correct logic surfaces; if gap >5%, investigate sampling hyperparameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the plateau in logical error rates at extremely high numerical perturbation levels stem directly from the cumulative frequency distribution of numerical values in the model's pre-training data?
- Basis in paper: [explicit] The authors state on Page 6 that "Further investigation is needed to validate this hypothesis" regarding why the increase in logical errors becomes more gradual at higher perturbation levels.
- Why unresolved: The specific composition of pre-training data for proprietary models is unknown, making it impossible to definitively correlate the error plateau with training data exposure without access to the data statistics.
- What evidence would resolve it: An analysis correlating the frequency of large numbers in a model's training corpus with its error rate curve across the perturbation levels defined in GSM-Ranges.

### Open Question 2
- Question: Does the degradation in logical reasoning due to out-of-distribution numerical values persist when models are evaluated on significantly more complex mathematical benchmarks compared to grade-school problems?
- Basis in paper: [explicit] The authors note in Limitations that "exploring the impact of varying numerical ranges on performance in more complex mathematical tasks would further enrich the findings."
- Why unresolved: The current study is restricted to the GSM8K dataset (grade school math), and it is unknown if the numerical sensitivity trends hold for higher-level reasoning tasks involving advanced algebra or calculus.
- What evidence would resolve it: Applying the GSM-Ranges perturbation methodology to complex datasets like MATH and measuring the resulting logical error shifts.

### Open Question 3
- Question: Can an automated grading methodology be developed to distinguish between specific sub-types of logical errors (e.g., missing steps vs. operator misuse) rather than grouping them into a single category?
- Basis in paper: [explicit] The authors state in Limitations that "a more granular grading methodology could offer deeper insights into model performance and refinement."
- Why unresolved: The current methodology maps responses to Python code to separate logical from non-logical errors but does not classify the specific semantic nature of the logical failure.
- What evidence would resolve it: A new evaluation framework that successfully categorizes responses into multiple logical error classes (e.g., missing steps, operator errors) with high inter-rater reliability compared to human graders.

## Limitations
- The core methodology relies on GPT-4o faithfully translating reasoning logic without "fixing" logical errors, which wasn't directly tested across all problem types
- Selective scaling for multiplicative operations prevents answer explosion but may introduce systematic biases in how numerical complexity affects reasoning performance
- The focus on GSM8K-style problems with small original numbers may limit generalizability to problems with inherently larger values or different mathematical structures

## Confidence

- **High Confidence**: The finding that logical error rates increase with numerical complexity (up to 14 percentage points) is well-supported by systematic perturbation experiments across six models and multiple numerical scales.
- **Medium Confidence**: The code-based error classification methodology (98.5% accuracy) is empirically validated but depends on the assumption that GPT-4o doesn't "fix" logical errors during translation, which wasn't directly tested for all problem types.
- **Medium Confidence**: The pass@k recovery experiments showing correct logic exists in model distributions are compelling but limited to sampling experiments that may not reflect real-world single-pass usage scenarios.

## Next Checks

1. **Code Generation Fidelity Test**: Manually validate 50 randomly selected GPT-4o code translations across all six numerical levels to confirm that logical errors are not being inadvertently corrected during the translation process.

2. **Cross-Dataset Generalization**: Apply the GSM-Ranges perturbation methodology to a different mathematical reasoning dataset (e.g., MATH or SVAMP) to verify whether the numerical sensitivity pattern holds across problem types.

3. **Temporal Stability Analysis**: Run the same 100 GSM8K questions through the grading pipeline after a 3-month interval to assess whether the logical/non-logical error classification remains consistent as model behaviors evolve.