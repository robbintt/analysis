---
ver: rpa2
title: 'No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal
  in Language Models'
arxiv_id: '2509.08075'
source_url: https://arxiv.org/abs/2509.08075
tags:
- refusal
- 'false'
- refusals
- task
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates false refusals in large language models
  (LLMs) when prompted with sociodemographic personas, revealing that certain personas
  (especially Black, White, transgender, and Muslim) trigger higher refusal rates,
  particularly in sensitive tasks like offensiveness classification. The authors propose
  a Monte Carlo sampling method to efficiently quantify the impact of multiple factors
  (persona, task, model, prompt phrasing) on false refusal behavior.
---

# No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models

## Quick Facts
- arXiv ID: 2509.08075
- Source URL: https://arxiv.org/abs/2509.08075
- Reference count: 40
- Primary result: Certain sociodemographic personas (especially Black, White, transgender, and Muslim) trigger higher false refusal rates in LLMs, with model choice being the strongest predictor of this behavior.

## Executive Summary
This paper investigates false refusals in large language models when prompted with sociodemographic personas across different tasks, models, and prompt phrasings. Using Monte Carlo sampling and Wasserstein distance analysis, the authors find that model choice dominates as the primary factor influencing false refusal rates, followed by task type and persona effects. Notably, newer Llama and Qwen models show reduced false refusals compared to older versions, while Gemma2 models exhibit increased refusals. The study reveals that open-ended prompts elicit more refusals than constrained ones, and that the impact of persona is overestimated relative to other factors.

## Method Summary
The authors employ Monte Carlo sampling to efficiently test combinations of 15 sociodemographic personas, 16 open-weight models, 3 tasks (NLI, politeness, offensiveness classification), and 9 prompt variations across constrained and unconstrained response formats. They use string-matching algorithms to detect refusals and apply Wasserstein distance and logistic regression to quantify the relative importance of each experimental factor. The evaluation covers approximately 30,000-50,000 test samples per task, using instruction-tuned model variants with greedy decoding (temperature=0).

## Key Results
- Model choice is the strongest predictor of false refusal rates, followed by task type and persona effects
- Offensiveness classification tasks show the highest false refusal rates (14.68% average), while NLI tasks show the lowest (1.37%)
- Open-ended prompts trigger significantly more refusals than forced-response formats
- Newer Llama and Qwen models demonstrate reduced false refusals compared to older versions, while Gemma2 models show increased refusal rates
- Persona effects are overestimated; task and model choice have larger impacts on false refusal behavior

## Why This Works (Mechanism)

### Mechanism 1: Safety Alignment Overgeneralization to Demographic Markers
- Claim: Models with certain alignment strategies disproportionately refuse requests when personas contain specific sociodemographic markers.
- Mechanism: Safety training appears to create associations between protected demographic categories (race, gender identity, religion) and refusal behaviors. When models detect these markers in persona prompts, safety mechanisms trigger even for benign tasks, treating the combination of demographic context + task as potentially harmful.
- Core assumption: The refusal patterns reflect learned associations from alignment data rather than explicit rules.
- Evidence anchors:
  - [abstract] "Certain personas (especially Black, White, transgender, and Muslim) trigger higher refusal rates, particularly in sensitive tasks"
  - [section 4.4] "Black (2.62), White (2.18), transgender woman (1.37), transgender man (1.31), Muslim (1.31) ... eliciting significantly higher refusal rates"
  - [corpus] Related work on semantic confusion in refusals suggests safety mechanisms may over-generalize contextual cues
- Break condition: If alignment strategies are revised to decouple demographic markers from safety triggers, the persona-based refusal disparity should decrease

### Mechanism 2: Task-Sensitivity Amplifies Refusal Behavior
- Claim: The semantic content of the task moderates whether personas trigger refusals.
- Mechanism: Tasks involving subjective judgments about content (offensiveness, politeness) activate safety-related reasoning pathways. When combined with demographic personas, models appear to apply heightened caution—possibly because the intersection of "judging content" + "demographic context" resembles patterns flagged during safety training.
- Core assumption: Task type primes certain safety-related activations in the model.
- Evidence anchors:
  - [abstract] "model choice and task significantly influence false refusals, especially in sensitive content tasks"
  - [section 4.2] "Offensiveness task has the highest rate of false refusals, with an average of 14.68% across models, followed by politeness (5.64%) and NLI (1.37%)"
  - [corpus] Weak direct corpus support for task-specific safety activation; this inference comes primarily from the paper's findings
- Break condition: If models are explicitly instructed that demographic context is irrelevant to classification accuracy, task-based amplification should diminish

### Mechanism 3: Prompt Constraint Reduces Refusal Degrees of Freedom
- Claim: Constrained response formats reduce refusal rates by limiting the model's ability to hedge or refuse.
- Mechanism: Open-ended prompts ("unforced") allow models to generate explanatory text, which provides space for safety justifications and refusals. Constrained prompts ("forced" with explicit "you must pick one of the options") narrow the action space, making refusals structurally harder and less likely.
- Core assumption: The effect is primarily about output format constraints rather than deeper changes in the model's internal safety assessment.
- Evidence anchors:
  - [section 4.5] "Models tend to refuse more when not forced to answer (unforced-response), i.e., when prompts are less restrictive and allow broader interpretation"
  - [section 4.5] Shows Llama2-13B with 74.29% refusal for unforced vs 94.55% for forced on offensiveness task
  - [corpus] Related work on prompt sensitivity supports the general finding that output format affects behavior
- Break condition: If constrained prompts are modified to include explicit refusal options, refusal rates may increase even under constraint

## Foundational Learning

- Concept: **False Refusal (Over-refusal)**
  - Why needed here: The paper's central phenomenon—models refusing safe requests—is distinct from appropriate safety behavior and requires separate evaluation frameworks
  - Quick check question: Can you distinguish between a legitimate safety refusal (e.g., "how to build a bomb") and a false refusal (e.g., refusing to classify text offensiveness when prompted as a "Black person")?

- Concept: **Global Sensitivity Analysis via Wasserstein Distance**
  - Why needed here: The paper uses this statistical method to quantify which experimental factors (model, task, persona, prompt) most influence refusal behavior
  - Quick check question: If varying the model changes refusal rates by 40% while varying the prompt changes them by 5%, which factor has higher sensitivity?

- Concept: **Monte Carlo Sampling for Combinatorial Evaluation**
  - Why needed here: Exhaustively testing all combinations of 15 personas × 16 models × 3 tasks × 9 prompts is infeasible; sampling provides statistically valid estimates
  - Quick check question: Given 1000 test samples and 10 persona-prompt combinations, how would you ensure balanced representation across experimental factors?

## Architecture Onboarding

- Component map:
  - Persona set (15 sociodemographic categories) -> Monte Carlo sampler -> 16 instruction-tuned models -> String-matching refusal detector -> Wasserstein sensitivity analysis -> Logistic regression quantification

- Critical path:
  1. Define persona set (15 sociodemographic categories)
  2. Select tasks spanning sensitivity spectrum (NLI → politeness → offensiveness)
  3. Design prompt paraphrases varying constraint levels (unforced → semi-forced → forced)
  4. Sample balanced test sets using Monte Carlo approach
  5. Run inference across all models
  6. Detect refusals via string matching
  7. Quantify factor contributions via sensitivity analysis

- Design tradeoffs:
  - **Automated vs human refusal detection**: String-matching is scalable but may miss subtle refusals; paper acknowledges potential undercounting
  - **Model coverage vs depth**: Testing 16 models across 3 tasks provides breadth but limited to open-weight models (no GPT, Claude)
  - **Persona granularity**: 15 personas cover major categories but miss intersections (e.g., "Black Muslim woman")

- Failure signatures:
  - High refusal variance across prompt paraphrases suggests unstable safety alignment
  - Newer model versions with higher refusal rates (Gemma2 vs Gemma) indicate regression in alignment quality
  - Task-specific refusal spikes (offensiveness >> NLI) reveal over-sensitive content filters

- First 3 experiments:
  1. **Baseline measurement**: Run persona-free prompts across all 16 models on offensiveness task to establish refusal baseline
  2. **Persona ablation**: Test single demographic factor (e.g., race only: Black/White/Asian) while holding task and model constant to isolate effect size
  3. **Prompt constraint sweep**: For highest-refusing model (e.g., Llama2-13B), test all 9 prompt variants to quantify how much constraint reduces refusals

## Open Questions the Paper Calls Out

- **Question**: Do larger or proprietary language models exhibit false refusal patterns similar to the open-weight models tested?
  - **Basis in paper**: [explicit] The authors note in the Limitations section that they focused on small to medium open-weight models, explicitly suggesting that "future research could build on our work by investigating larger models as well as proprietary models."
  - **Why unresolved**: The study restricted its evaluation to 16 specific open-weight models (Llama, Gemma, Qwen) to control the environment, leaving the behavior of commercial/closed APIs unknown.
  - **What evidence would resolve it**: Applying the proposed Monte Carlo sampling methodology to proprietary models (e.g., GPT-4, Claude) and larger parameter versions of the tested families.

- **Question**: To what extent does prompting language influence false refusal rates compared to persona and task factors?
  - **Basis in paper**: [explicit] The authors list "prompting language" as a factor that "may also influence false refusal behavior in models [but] remains unexplored."
  - **Why unresolved**: The experiments were conducted exclusively in English for feasibility, so the impact of multilingual contexts on safety alignment over-refusal is currently unquantified.
  - **What evidence would resolve it**: Replicating the experimental setup using multilingual datasets (e.g., XNLI) to compare refusal rates across different languages for the same personas and tasks.

- **Question**: What specific alignment strategies cause newer Gemma models to increase false refusals while Llama and Qwen models improve?
  - **Basis in paper**: [inferred] The results show a divergence where Llama and Qwen reduce refusals in newer versions, whereas "the latest Gemma models show a significant increase," suggesting a difference in underlying alignment philosophies.
  - **Why unresolved**: The paper quantifies the *behavior* (the change in refusal rates) but does not analyze the internal model weights, training data, or specific safety alignment techniques responsible for this regression.
  - **What evidence would resolve it**: A comparative analysis of the safety tuning data and reinforcement learning parameters used in Gemma2 versus Llama3.1/Qwen2.5.

## Limitations

- **String-matching bias**: Automated detection via keyword matching may undercount nuanced refusals, potentially underestimating true false refusal rates.
- **Model scope constraint**: Exclusively uses open-weight models, excluding proprietary systems like GPT-4 or Claude, limiting generalizability.
- **Persona intersectionality**: Analyzes 15 personas individually, missing compounding effects when multiple sociodemographic markers combine.

## Confidence

- **High confidence**: Model choice is the dominant factor influencing false refusals, supported by both Wasserstein sensitivity analysis and logistic regression.
- **Medium confidence**: Persona effects are statistically significant but smaller than model and task effects, with consistent ranking patterns.
- **Low confidence**: Mechanism explanations connecting safety alignment to demographic overgeneralization remain speculative, with inferred rather than directly measured causal pathways.

## Next Checks

1. **Manual refusal annotation validation**: Select 100 random refused responses and have human annotators classify whether each constitutes a true false refusal. Compare against string-matching results to estimate detection accuracy and potential undercounting.

2. **Intersectional persona testing**: Create compound personas combining multiple sociodemographic markers (e.g., "Black Muslim woman") and measure whether refusal rates exceed the sum of individual persona effects, testing for compounding bias.

3. **Cross-task generation evaluation**: Extend the experimental framework to include open-ended generation tasks (story writing, explanation generation) to determine whether false refusal patterns persist outside classification contexts.