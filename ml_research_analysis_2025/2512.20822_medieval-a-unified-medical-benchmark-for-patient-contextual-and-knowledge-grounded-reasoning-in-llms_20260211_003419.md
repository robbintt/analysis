---
ver: rpa2
title: 'MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded
  Reasoning in LLMs'
arxiv_id: '2512.20822'
source_url: https://arxiv.org/abs/2512.20822
tags:
- medical
- patient
- medieval
- knowledge
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MediEval, a benchmark designed to evaluate\
  \ whether large language models can reason reliably about medical knowledge while\
  \ grounding conclusions in real patient records. It constructs a unified knowledge\
  \ base from MIMIC-IV EHRs and biomedical ontologies (UMLS, SNOMED CT, RxNorm) to\
  \ generate diverse statements\u2014true/false and supported/unsupported\u2014that\
  \ span four reasoning quadrants."
---

# MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs

## Quick Facts
- **arXiv ID:** 2512.20822
- **Source URL:** https://arxiv.org/abs/2512.20822
- **Reference count:** 26
- **Primary result:** Even strong LLMs hallucinate evidence (HSR 36-48%) and invert truth (TIR 21-29%) on clinical reasoning; CoRFu DPO fine-tuning reduces these errors by up to 50%.

## Executive Summary
MediEval is a benchmark designed to test whether large language models can reason reliably about medical knowledge while grounding conclusions in real patient records. It constructs a unified knowledge base from MIMIC-IV EHRs and biomedical ontologies (UMLS, SNOMED CT, RxNorm) to generate diverse statements—true/false and supported/unsupported—that span four reasoning quadrants. The benchmark reveals that even strong models frequently hallucinate evidence or invert truth, with accuracy limited to 60-74% and macro F1 from 50-71%. To mitigate these safety-critical errors, the authors propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty that more harshly penalizes unsafe confusions. CoRFu achieves up to +16.4 macro F1 points over base and eliminates truth inversion errors, demonstrating both higher accuracy and improved safety in clinical reasoning.

## Method Summary
MediEval constructs a 4-way NLI classification task from MIMIC-IV EHRs and biomedical ontologies. Patient context (340-2,827 tokens) is extracted from discharge summaries, and statements are generated via ontology-guided substitutions and recombinations to span four quadrants: (Q1) true and supported, (Q2) true but unsupported, (Q3) false but plausible, (Q4) false and implausible. The dataset contains 37,144 samples, balanced across quadrants. Base models are first fine-tuned with SFT (LoRA, 3 epochs), then refined with CoRFu—DPO with an asymmetric penalty λ that penalizes unsafe confusions (e.g., Q2→Q1, Q3→Q1) more than other errors. The method is evaluated on Llama-3.1-8B-Instruct and Qwen3-8B, reporting accuracy, macro-F1, per-quadrant F1, and safety metrics HSR and TIR.

## Key Results
- Base models show high hallucination (HSR 36-48%) and truth inversion (TIR 21-29%) rates.
- CoRFu achieves up to +16.4 macro F1 improvement over base and eliminates truth inversion errors.
- Safety-critical error reduction is driven by asymmetric penalty weighting unsafe confusions more heavily.
- Performance is measured on Llama-3.1-8B-Instruct and Qwen3-8B, with hardware requirements of NVIDIA L40S 46GB.

## Why This Works (Mechanism)
The 4-way NLI formulation explicitly separates whether a statement is true from whether it is supported by patient context, exposing two distinct failure modes: hallucinating evidence (confusing Q2→Q1) and accepting false but plausible statements (confusing Q3→Q1). The asymmetric CoRFu penalty directly penalizes these unsafe confusions more than benign errors, forcing the model to learn to reject unsupported truths and false-but-plausible statements. This targeted fine-tuning aligns the model's decision boundaries with clinical safety requirements.

## Foundational Learning
- **UMLS ontology normalization:** Required to map diverse clinical codes (ICD, NDC) to unified CUIs for cross-referencing. Quick check: Verify CUI mapping accuracy on a held-out set of codes.
- **Ontology-guided statement generation:** Needed to systematically generate Q2/Q4 statements and recombine them for Q3. Quick check: Ensure generated statements preserve semantic plausibility via human review.
- **4-way NLI formulation:** Distinguishes truth from support, exposing two separate failure modes. Quick check: Evaluate confusion matrices to confirm Q2→Q1 and Q3→Q1 errors are reduced by CoRFu.
- **Asymmetric penalty in DPO:** Specifically penalizes unsafe confusions (Q2→Q1, Q3→Q1) more than other errors. Quick check: Compare ablation results with symmetric penalty to isolate effect.
- **Safety metrics HSR/TIR:** Quantify hallucination and truth inversion rates, directly measuring clinical safety. Quick check: Monitor HSR/TIR during CoRFu training to confirm reduction.

## Architecture Onboarding
- **Component map:** MIMIC-IV EHRs + UMLS/SNOMED CT/RxNorm ontologies → Ontology graph (G) → Patient context extraction → Statement generation (4 quadrants) → 4-way NLI classification head → CoRFu fine-tuning (asymmetric DPO) → Safety metrics (HSR, TIR).
- **Critical path:** Ontology graph construction → Patient context extraction → Quadrant statement generation → Fine-tuning (SFT + CoRFu) → Evaluation.
- **Design tradeoffs:** The 4-way NLI formulation increases complexity over binary NLI but better exposes safety-critical failure modes; the asymmetric penalty improves safety but requires careful λ tuning.
- **Failure signatures:** High HSR (>30%) indicates hallucination of evidence; high TIR (>20%) indicates truth inversion; both signal unsafe clinical reasoning.
- **Exactly 3 first experiments:** 1) Replicate SFT baseline to confirm high HSR/TIR rates; 2) Apply CoRFu with default λ=0.5 and measure HSR/TIR reduction; 3) Conduct ablation with λ=0 (symmetric penalty) to isolate asymmetric penalty effect.

## Open Questions the Paper Calls Out
- **Adaptation to generative QA:** The framework could extend to clinical question answering, but it is unclear if the 4-way NLI safety gains transfer to open-ended generation. Evidence: performance metrics from applying CoRFu to a generative medical QA dataset.
- **RAG/temporal reasoning integration:** Dynamic retrieval or temporal attention might better ground reasoning in patient timelines, but their impact on HSR/TIR is unknown. Evidence: comparative evaluation of RAG-enabled or temporal LLMs on MediEval.
- **Cross-institutional generalization:** The pipeline is claimed extensible to other institutions, but it is unclear if UMLS normalization holds for heterogeneous EHR standards. Evidence: replication on external dataset (e.g., eICU) with code-mapping failure analysis.

## Limitations
- Asymmetric penalty lacks ablated comparison against symmetric alternatives, so its necessity is uncertain.
- Fixed context window may truncate relevant information for complex cases, limiting clinical applicability.
- All evaluations use the same dataset for construction, so generalization to truly unseen clinical scenarios is untested.

## Confidence
- **High confidence:** Base models exhibit high rates of safety-critical errors (HSR 36-48%, TIR 21-29%), and CoRFu reduces these errors as measured by confusion matrices.
- **Medium confidence:** Asymmetric penalty likely contributes to CoRFu's gains, but direct ablations are missing; generalization to broader clinical settings is assumed but not demonstrated.
- **Low confidence:** Claims about clinical deployment safety improvements are unsupported, as the study only measures in-distribution performance.

## Next Checks
1. Conduct ablation study comparing CoRFu with symmetric vs. asymmetric penalties (λ=0, 0.5, 1.0) and with standard DPO (λ=0) to isolate the effect of the asymmetric penalty on HSR/TIR reductions.
2. Evaluate CoRFu on an external, held-out clinical dataset (e.g., eICU or a different hospital's EHRs) to measure out-of-distribution generalization and robustness to domain shift.
3. Implement a randomized clinical reasoning task with clinician-generated statements to test whether the benchmark's artifacts (ontology-guided substitutions, specific MIMIC-IV biases) limit external validity.