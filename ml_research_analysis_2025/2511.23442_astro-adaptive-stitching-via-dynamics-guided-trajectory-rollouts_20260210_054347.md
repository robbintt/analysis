---
ver: rpa2
title: 'ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts'
arxiv_id: '2511.23442'
source_url: https://arxiv.org/abs/2511.23442
tags:
- stitching
- astro
- trajectory
- offline
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ASTRO is a data augmentation framework for offline RL that generates
  distributionally novel and dynamics-consistent trajectories through adaptive stitching.
  It addresses the limitations of existing methods by using a Temporal Distance Representation
  (TDR) for distinct, reachable target selection and a dynamics-guided stitch planner
  with rollout deviation feedback for feasible completion.
---

# ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts

## Quick Facts
- **arXiv ID:** 2511.23442
- **Source URL:** https://arxiv.org/abs/2511.23442
- **Reference count:** 40
- **Primary result:** ASTRO improves average task performance by 32.7% (+9.68) across multiple offline RL algorithms on the challenging OGBench suite.

## Executive Summary
ASTRO is a data augmentation framework for offline reinforcement learning that generates distributionally novel and dynamics-consistent trajectories through adaptive stitching. It addresses limitations of existing methods by using a Temporal Distance Representation (TDR) for distinct, reachable target selection and a dynamics-guided stitch planner with rollout deviation feedback for feasible completion. The framework achieves significant performance gains on challenging benchmarks like OGBench and standard datasets like D4RL.

## Method Summary
ASTRO generates novel trajectories by stitching together existing trajectories from offline datasets. It employs a Temporal Distance Representation (TDR) to select temporally reachable and distinct stitch targets, avoiding the geometric-only selection of previous methods. A diffusion-based stitch planner generates connecting action sequences while using rollout deviation feedback from a learned dynamics model to ensure physical feasibility. The approach decouples high-level planning from low-level dynamics execution, reducing action-state misalignment.

## Key Results
- Achieves 32.7% improvement (+9.68) in average task performance on OGBench suite
- Improves performance across multiple offline RL algorithms (CQL, IQL, TD3+BC)
- Reduces Dynamics Violation Rate from 21.1% to 7.7% compared to state-of-the-art methods
- Demonstrates consistent improvements on standard D4RL benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Temporal Reachability Encoding
Replaces Euclidean distance with a learned Temporal Distance Representation (TDR) that enables selection of stitch targets that are spatially distant but temporally reachable within a fixed horizon. The TDR encoder maps states into a latent space where Euclidean distance approximates minimum steps to reach a target state, filtering out unreachable targets.

### Mechanism 2: Deviation-Guided Denoising
Uses Rollout Deviation Feedback during diffusion sampling to improve dynamics consistency. The planner computes the gap between target states and states reached by simulating the action sequence via a learned dynamics model, conditioning the next denoising step to correct physically impossible states.

### Mechanism 3: Decoupled Action-Space Planning
Decouples high-level stitch planning from low-level dynamics execution using a diffusion planner plus forward dynamics model. This reduces action-state misalignment found in inverse-dynamics methods by planning actions directly and using forward models to check validity.

## Foundational Learning

- **Concept: Goal-Conditioned Value Functions**
  - **Why needed here:** The TDR module is trained as a goal-conditioned value function using expectile regression to approximate temporal distance
  - **Quick check question:** Can you explain why expectile regression (as opposed to MSE) is used to learn the minimum temporal distance in TDR?

- **Concept: Diffusion Models for Control (DiT/DDPM)**
  - **Why needed here:** Both the Stitch Planner and Dynamics Model use diffusion transformers, requiring understanding of the reverse denoising process
  - **Quick check question:** How does conditioning the denoising network on the "masked stitching sequence" differ from standard classifier-free guidance?

- **Concept: Offline RL Distributional Shift**
  - **Why needed here:** The paper frames trajectory stitching as a solution to "Confined Target Selection" and "Dynamics Violation" - specific failure modes of distributional shift
  - **Quick check question:** Why does generating novel trajectories (distributionally novel) help offline RL, considering the risk of querying out-of-distribution states during Q-learning?

## Architecture Onboarding

- **Component map:**
  TDR Encoder -> Dynamics Model -> Stitch Planner

- **Critical path:**
  1. Pre-train TDR: Optimize TDR encoder using Expectile TD loss on offline triples
  2. Pre-train Dynamics: Train dynamics model to predict state trajectories given action sequences
  3. Train Planner: Train stitch planner using joint objective with frozen dynamics model for deviation feedback

- **Design tradeoffs:**
  - Filter Threshold (Δ_thresh): Lower values ensure higher geometric consistency but increase selection latency exponentially
  - Masking Ratio (M/l): Ratio of 1 (balancing mask length and sub-trajectory length) provided best flexibility-constraint balance

- **Failure signatures:**
  - "Infeasible Stitch": TDR selects target requiring passing through wall; check if TDR distance matches heuristic path length
  - "Dynamics Drift": Generated trajectory diverges from target; monitor Rollout Deviation δ during inference
  - "Planner Collapse": Diffusion planner fails to generate valid actions; check action space diversity in offline dataset

- **First 3 experiments:**
  1. TDR Validation: Visualize latent space of TDR vs. Euclidean space on 2D Maze to verify topology respect
  2. Dynamics Model Fidelity: Train dynamics model alone and plot Predicted State vs. Ground Truth State MSE for long horizons
  3. Ablation on Feedback: Run Stitch Planner with Rollout Deviation Feedback disabled vs. enabled; compare Dynamics Violation Rate

## Open Questions the Paper Calls Out
None

## Limitations
- TDR's ability to generalize to out-of-distribution state pairs is untested
- Approach's scalability to high-dimensional, continuous state spaces (e.g., real-world robotics) remains unclear
- Missing ablation study on dynamics model's accuracy over long horizons, critical for feedback loop reliability

## Confidence
- **High Confidence:** The decoupled planning architecture is novel and addresses well-documented failure modes in inverse-dynamics stitching methods
- **Medium Confidence:** The specific contribution of TDR encoder to overall performance gain is plausible but requires further ablation
- **Low Confidence:** Long-term stability of deviation feedback loop is uncertain due to potential dynamics model error accumulation

## Next Checks
1. **Ablation on TDR:** Train ASTRO with random goal selector replacing TDR; compare Dynamics Violation Rate and overall performance
2. **Dynamics Model Fidelity Test:** Train dynamics model alone and plot Predicted State vs. Ground Truth State MSE for horizons of 5, 10, and 15 steps
3. **OGBench Stress Test:** Run ASTRO on smaller, interpretable subset of OGBench (2D maze); visualize stitched trajectories and TDR latent space to verify topology respect