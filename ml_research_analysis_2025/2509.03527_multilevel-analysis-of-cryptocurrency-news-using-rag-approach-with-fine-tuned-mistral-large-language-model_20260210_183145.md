---
ver: rpa2
title: Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned
  Mistral Large Language Model
arxiv_id: '2509.03527'
source_url: https://arxiv.org/abs/2509.03527
tags:
- entity
- sentiment
- crypto
- trend
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multilevel multitask framework for cryptocurrency
  news analysis using a fine-tuned Mistral 7B large language model with retrieval-augmented
  generation (RAG). The approach generates graph and text summaries with sentiment
  scores, creates JSON representations, and applies hierarchical stacking to consolidate
  these summaries into comprehensive reports.
---

# Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model

## Quick Facts
- arXiv ID: 2509.03527
- Source URL: https://arxiv.org/abs/2509.03527
- Reference count: 40
- Key outcome: Proposes multilevel multitask framework for cryptocurrency news analysis using fine-tuned Mistral 7B with RAG, generating graph/text summaries with sentiment scores and hierarchical stacking

## Executive Summary
This paper introduces a multilevel multitask framework for analyzing cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG). The approach generates structured knowledge graphs and narrative text summaries with sentiment scores, creates JSON representations, and applies hierarchical stacking to consolidate these outputs into comprehensive reports. The framework claims to reduce LLM hallucinations through knowledge graph representations while enabling risk assessment through detection of consensus and contradictory trends in cryptocurrency news.

## Method Summary
The method uses Mistral-7B-Instruct-v0.1 fine-tuned with 4-bit quantization using PEFT/LoRA approach. The model is trained on synthetically generated instruction sets using GPT-4.1 prompts for tasks including knowledge graph generation, text summarization with sentiment detection, JSON representation creation, and hierarchical stacking. The framework operates across three levels: Level 1 generates individual article summaries (graph + text), Level 2 consolidates 5 summaries into meta-summaries, and Level 3 merges graph and text meta-summaries into final reports with explicit trend categories (upward, downward, contradictory).

## Key Results
- Fine-tuned Mistral 7B generates knowledge graphs that constrain LLM outputs to verifiable entity-relation triples
- Hierarchical stacking across 3 levels identifies consensus trends and contradictory signals in cryptocurrency news
- PEFT/LoRA with 4-bit quantization enables effective domain adaptation while preserving base model capabilities
- Framework produces JSON reports with sentiment scores ranging from -10 to +10 for risk assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graph representations reduce LLM hallucinations by constraining output to structured entity-relation triples that can be validated against source text.
- Mechanism: Instead of free-form text generation, the model produces explicit (Entity 1 | Relationship | Entity 2) tuples with sentiment scores. This structured format forces discrete claims rather than fluent but unsupported assertions, enabling systematic cross-checking of extracted relationships.
- Core assumption: Hallucinations arise primarily from unconstrained generative decoding; structured output schemas provide sufficient constraint to reduce fabrication.
- Evidence anchors:
  - [abstract] "The representation of cryptocurrency news as knowledge graph can essentially eliminate problems with large language model hallucinations."
  - [section 2, p.4] "Graph representations... provide the structure, verification, and grounding that effectively mitigate LLM hallucinations. They anchor outputs in factual triples, enable automated claim for checking and refinement."
  - [corpus] Related work (Agrawal et al., arXiv:2311.07914) surveys knowledge graph approaches for hallucination reduction, supporting this mechanism direction.
- Break condition: If hallucinations in this domain stem primarily from missing training knowledge rather than generation constraints, graph structuring alone will not resolve factual errors.

### Mechanism 2
- Claim: Hierarchical stacking of summaries across multiple levels enables detection of consensus trends and contradictory signals that single-pass summarization would miss.
- Mechanism: Level 1 generates individual article summaries (graph + text). Level 2 consolidates 5 summaries into meta-summaries. Level 3 merges graph-meta and text-meta summaries into final reports with explicit "upward_trend," "downward_trend," and "contradictory_trend" categories.
- Core assumption: Information quality improves through iterative aggregation; contradictions become salient only when comparing multiple independent sources.
- Evidence anchors:
  - [abstract] "Higher levels perform hierarchical stacking that consolidates sets of graph-based and text-based summaries as well as summaries of summaries into comprehensive reports."
  - [section 3, p.5] "Stacking approach makes it possible to combine LLM models output on the next analytical level... As a result, one can get diversified optimized results."
  - [section 5.2.5-5.2.7, p.12-14] Examples show stacking outputs explicitly identifying contradictory trends (e.g., Bitcoin showing both "strong upward momentum" and "significant downward price movement").
  - [corpus] Weak direct corpus support; stacking mechanism appears novel to this paper's application domain.
- Break condition: If summary compression at each level loses critical nuance, stacking may amplify noise rather than signal; contradictory detection would then produce false positives.

### Mechanism 3
- Claim: PEFT/LoRA with 4-bit quantization enables effective domain adaptation while preserving base model capabilities and reducing computational requirements.
- Mechanism: Freezes pretrained Mistral-7B weights; injects trainable low-rank decomposition matrices into transformer layers. 4-bit quantization reduces memory footprint, allowing fine-tuning on consumer-grade GPUs (e.g., Google Colab).
- Core assumption: Domain-specific knowledge for cryptocurrency news analysis can be captured in low-rank parameter updates without full-model retraining.
- Evidence anchors:
  - [abstract] "The model is fine-tuned with 4-bit quantization using the PEFT/LoRA approach."
  - [section 4, p.5-6] "PEFT/LoRA approach makes it possible to fine-tune LLMs with sizes near 7B parameters, using Google Colab... greatly reducing the number of trainable parameters for downstream tasks."
  - [section 4, p.6] Training configuration specified: batch_size=2, gradient_accumulation=4, learning_rate=3e-5, num_epochs=7.
  - [corpus] Related work (Hu et al., LoRA paper; Dettmers et al., QLoRA) provides foundational evidence that low-rank adaptation achieves comparable performance to full fine-tuning.
- Break condition: If the multilevel multitask instruction set (graph generation, summarization, JSON formatting, stacking) requires knowledge distributed across many model components, low-rank updates may be insufficient for convergence.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The framework uses RAG to ground LLM outputs in external cryptocurrency news sources rather than relying solely on pretrained knowledge.
  - Quick check question: Can you explain how RAG differs from standard LLM inference in terms of input composition and knowledge sourcing?

- **Concept: Knowledge Graphs and Entity-Relation Extraction**
  - Why needed here: Core output format; must understand what constitutes valid (entity, relation, entity) triples and how sentiment scores attach to entities.
  - Quick check question: Given the sentence "Coinbase partnered with JPMorgan to enable crypto purchases," what entities and relations would you extract?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Understanding how LoRA enables parameter-efficient fine-tuning by decomposing weight updates into low-rank matrices rather than updating full weight tensors.
  - Quick check question: If a weight matrix is 4096Ã—4096 and LoRA uses rank 8, how many trainable parameters does the adaptation add per layer?

## Architecture Onboarding

- **Component map:**
  Input News Articles -> [Level 1: Fine-tuned Mistral-7B + PEFT/LoRA adapter] -> Graph Summary (entity-relation triples + sentiment scores) -> Stacked Graph Summary (trend aggregation) -> Final JSON Report with upward/downward/contradictory trends
  Input News Articles -> [Level 1: Fine-tuned Mistral-7B + PEFT/LoRA adapter] -> Text Summary (narrative summary + crypto descriptions) -> Stacked Text Summary (trend aggregation) -> Final JSON Report with upward/downward/contradictory trends

- **Critical path:**
  1. Load base model (mistralai/Mistral-7B-Instruct-v0.1) with 4-bit quantization using bitsandbytes, then load published PEFT adapter (bpavlsh/Mistral-crypto-news) using PeftModel.from_pretrained()
  2. Format input using specified prompt template: `<s>[INST] <<SYS>> You are an expert in analyzing cryptocurrency news. <</SYS>> {query} {text} [/INST]`
  3. Test with provided queries: (a) "Generate a knowledge graph from cryptocurrency news:" (b) "Generate summaries of cryptocurrency news and detect sentiment signals:" (c) "Create a JSON representation of the summary of cryptocurrency news:"; use max_new_tokens=1500

- **Design tradeoffs:**
  - Graph vs. text summaries: Graph provides verifiable structure but loses narrative nuance; text provides context but harder to validate. Paper uses both for complementary views.
  - Stacking batch size (5 summaries): Larger batches capture more diversity but increase prompt length and potential information loss through compression.
  - 4-bit quantization: Reduces memory but may degrade output quality compared to full-precision inference.

- **Failure signatures:**
  - Graph extraction produces incomplete or malformed triples (missing entities, invalid relations)
  - Sentiment scores inconsistent between graph and text summaries for same article
  - Stacking level fails to identify contradictions present in source summaries
  - JSON output not parseable (syntax errors, missing fields)
  - Model generates hallucinated entities not present in source news

- **First 3 experiments:**
  1. **Baseline validation:** Run fine-tuned model on 10 sample news articles; manually verify graph triples against source text for accuracy and completeness.
  2. **Hallucination comparison:** Generate summaries with graph-format vs. free-text prompts on same inputs; count factual assertions that cannot be verified in source text for each condition.
  3. **Stacking effectiveness:** Process 25 articles through full pipeline; evaluate whether Level 3 reports correctly identify at least 80% of manually-labeled trend contradictions present across the corpus.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the sentiment signal scores generated by the fine-tuned Mistral 7B model significantly improve the accuracy of downstream cryptocurrency price predictive models?
- Basis in paper: [Explicit] The conclusion states, "The sentiments signal scores... can be used in predictive models as features," yet the paper focuses only on the generation phase.
- Why unresolved: The study analyzes the extraction and summarization of news but does not integrate these features into actual price forecasting models to test predictive power.
- What evidence would resolve it: A comparative study of price forecasting models with and without the inclusion of the generated sentiment scores.

### Open Question 2
- Question: Can the representation of news as knowledge graphs quantitatively demonstrate the "essential elimination" of hallucinations compared to standard text-based RAG methods?
- Basis in paper: [Explicit] The abstract claims that representing news as knowledge graphs "can essentially eliminate problems with large language model hallucinations," but offers only qualitative examples.
- Why unresolved: The paper presents qualitative outputs but lacks a quantitative evaluation metric (e.g., factuality scores) comparing graph-based vs. text-based outputs.
- What evidence would resolve it: Benchmarking results using a hallucination detection metric (e.g., FACTSCORE) comparing the proposed method against a baseline RAG.

### Open Question 3
- Question: How does the precision of the training dataset and specific PEFT/LoRA hyperparameters impact the reduction of inaccuracies in generated summaries?
- Basis in paper: [Explicit] The conclusion notes that "some inaccuracy can appear" and suggests one "needs a more precisely created training dataset and find optimized parameters."
- Why unresolved: The author identifies inaccuracy as a limitation but does not conduct an ablation study on data quality or parameter settings.
- What evidence would resolve it: An ablation study showing error rates across different dataset precisions and fine-tuning configurations.

## Limitations

- **No quantitative evaluation metrics**: The framework provides qualitative examples but lacks automated metrics for accuracy, hallucination rates, or stacking effectiveness.
- **GPT-4.1 dependency**: Training instructions generated by GPT-4.1 introduce potential bias and limit reproducibility.
- **4-bit quantization trade-offs**: While computationally efficient, may impact output quality and sentiment signal precision.

## Confidence

**High Confidence** in technical implementation of knowledge graph generation and PEFT/LoRA fine-tuning methodology, following established best practices with clear specification of quantization and training parameters.

**Medium Confidence** in hierarchical stacking effectiveness for contradiction detection, supported by qualitative examples but lacking systematic validation across diverse news corpora and contradictory signal types.

**Low Confidence** in the claim that the framework "essentially eliminates" LLM hallucinations, as this conflates structured output constraints with complete hallucination prevention without empirical measurement.

## Next Checks

1. **Hallucination Rate Measurement**: Conduct controlled study comparing factual accuracy between fine-tuned model outputs and baseline Mistral-7B on identical cryptocurrency news samples, using human annotators to identify hallucinated entities, relations, and sentiment scores.

2. **Stacking Effectiveness Quantification**: Test the multilevel framework on curated dataset containing known contradictory trends across multiple articles, measuring precision and recall of contradiction detection against ground truth annotations.

3. **Computational Efficiency Analysis**: Benchmark inference latency and memory usage across different quantization levels (8-bit, 4-bit, 2-bit) while measuring degradation in summary quality using automated metrics like ROUGE and human evaluation of sentiment accuracy.