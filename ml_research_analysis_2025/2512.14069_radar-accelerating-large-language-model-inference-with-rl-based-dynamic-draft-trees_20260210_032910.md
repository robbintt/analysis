---
ver: rpa2
title: 'RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft
  Trees'
arxiv_id: '2512.14069'
source_url: https://arxiv.org/abs/2512.14069
tags:
- draft
- radar
- calls
- arxiv
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating large language
  model (LLM) inference, which is expensive and slow due to the autoregressive generation
  process requiring access to entire model parameters for each generated token. The
  authors propose RADAR, a novel speculative sampling method that uses reinforcement
  learning (RL) to dynamically generate draft trees, allowing for more effective generation
  and utilization of candidate tokens.
---

# RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees

## Quick Facts
- arXiv ID: 2512.14069
- Source URL: https://arxiv.org/abs/2512.14069
- Authors: Junjie Ma; Jinlong Li
- Reference count: 0
- Key outcome: Achieves 3.17x–4.82x speedup over auto-regressive decoding while reducing draft model calls by 18.7% compared to EAGLE-3

## Executive Summary
This paper addresses the challenge of accelerating large language model (LLM) inference, which is expensive and slow due to the autoregressive generation process requiring access to entire model parameters for each generated token. The authors propose RADAR, a novel speculative sampling method that uses reinforcement learning (RL) to dynamically generate draft trees, allowing for more effective generation and utilization of candidate tokens. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline RL to train a prediction model that makes real-time decisions on the number of calls to the draft model, reducing redundant computations and further accelerating inference.

## Method Summary
RADAR proposes a reinforcement learning-based approach to dynamic draft tree generation for LLM inference acceleration. The method formulates draft tree generation as an MDP where the agent decides whether to continue or stop calling the draft model at each generation step. An LSTM-based prediction model takes top-k confidence scores from the draft model and outputs continue/stop actions. The model is trained offline using REINFORCE with rewards based on acceptance length and computational cost. The training dataset is constructed by running EAGLE-3 on the ShareGPT dataset and enumerating draft model calls to collect confidence scores and acceptance length distributions.

## Key Results
- Achieves 3.17x–4.82x speedup over auto-regressive decoding baseline
- Reduces average draft model calls by 18.7% compared to EAGLE-3
- Maintains high average acceptance length (only about 1.2% lower than EAGLE-3)
- Validated across three LLMs (LLaMA-Instruct 3.1 8B, Vicuna 13B, DeepSeek-R1-Distill-LLaMA 8B) and four tasks (MT-bench, GSM8K, Alpaca, MBPP)

## Why This Works (Mechanism)
The RL-based dynamic draft tree approach works by learning when to stop draft model calls rather than using a fixed number of calls. This reduces redundant computations while maintaining high acceptance rates. The LSTM prediction model effectively learns from confidence score distributions to make optimal continue/stop decisions.

## Foundational Learning
- **Reinforcement Learning with REINFORCE**: Needed to train the prediction model to make optimal continue/stop decisions based on rewards. Quick check: Verify REINFORCE gradients are computed correctly and the policy improves over training.
- **Speculative Sampling**: Required to understand the baseline approach where draft tokens are generated ahead of time and potentially accepted. Quick check: Confirm draft model outputs can be correctly matched to verification model outputs.
- **Markov Decision Process**: Needed to formalize the draft tree generation as a sequential decision problem. Quick check: Verify state transitions and reward calculations follow MDP formulation.

## Architecture Onboarding

Component map: Input prompts -> Draft model -> Confidence scores -> LSTM prediction model -> Continue/stop decision -> Verification model -> Output tokens

Critical path: The critical path is Input prompts → Draft model → Confidence scores → LSTM prediction model → Continue/stop decision → Verification model. The LSTM prediction model must make real-time decisions to minimize latency.

Design tradeoffs: The main tradeoff is between acceptance length and computational efficiency. More draft calls increase acceptance probability but reduce speedup. The RL approach dynamically balances this tradeoff.

Failure signatures: 
- If prediction model always chooses "continue", speedup will be worse than EAGLE-3
- If prediction model always chooses "stop", acceptance length will drop significantly
- If confidence scores are not properly calibrated, the prediction model will make poor decisions

First experiments:
1. Verify baseline EAGLE-3 implementation works correctly on test prompts
2. Test the LSTM prediction model's continue/stop decisions on validation prompts
3. Profile draft model calls to confirm the 18.7% reduction compared to EAGLE-3

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Unspecified penalty constant α in reward function, which affects the trade-off between acceptance length and computational efficiency
- Missing LSTM architecture details (hidden size, number of layers) that could impact model performance
- Hardware-specific latency parameters not provided, making it difficult to precisely reproduce speedup calculations
- Training duration for prediction model not specified, potentially affecting convergence

## Confidence
- High confidence: RL-based dynamic draft tree approach achieves 3.17x-4.82x speedup while maintaining high acceptance length
- Medium confidence: Offline RL training method effectively reduces average draft model calls by 18.7% compared to EAGLE-3
- Low confidence: Hardware-independent performance metrics without exact latency values used in speedup calculation

## Next Checks
1. Implement the full RADAR pipeline with reasonable default values for unspecified hyperparameters and verify similar speedup and acceptance length improvements on benchmark tasks
2. Profile actual draft model calls during inference to confirm the 18.7% reduction compared to EAGLE-3 baseline under same conditions
3. Conduct ablation studies varying the penalty constant α to determine its sensitivity and impact on speed-acceptance length trade-off