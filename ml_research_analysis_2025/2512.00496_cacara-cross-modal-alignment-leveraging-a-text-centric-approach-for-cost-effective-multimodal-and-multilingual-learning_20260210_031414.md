---
ver: rpa2
title: 'CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective
  Multimodal and Multilingual Learning'
arxiv_id: '2512.00496'
source_url: https://arxiv.org/abs/2512.00496
tags:
- audio
- training
- multimodal
- cacara
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CACARA, a multimodal and multilingual model\
  \ that leverages emergent alignment learning to efficiently integrate new modalities\
  \ and languages. Unlike traditional approaches that require costly retraining across\
  \ all modalities and languages, CACARA achieves multilingual capabilities by fine-tuning\
  \ the new modality only on English-aligned data, preserving the text encoder\u2019\
  s inherent cross-lingual features."
---

# CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning

## Quick Facts
- arXiv ID: 2512.00496
- Source URL: https://arxiv.org/abs/2512.00496
- Reference count: 8
- Introduces CACARA model achieving multilingual audio-text retrieval through English-only training

## Executive Summary
This paper introduces CACARA, a multimodal and multilingual model that leverages emergent alignment learning to efficiently integrate new modalities and languages. Unlike traditional approaches that require costly retraining across all modalities and languages, CACARA achieves multilingual capabilities by fine-tuning the new modality only on English-aligned data, preserving the text encoder's inherent cross-lingual features. The model demonstrates strong performance in audio-text retrieval, improving R@1 by up to 14.24 percentage points over state-of-the-art models, and extends to over 100 languages without explicit multilingual training. CACARA significantly reduces computational costs, achieving 73% lower energy consumption and 79% faster training compared to fully tri-modal baselines, while maintaining high performance across tasks.

## Method Summary
CACARA extends an existing image-text multimodal model by adding an audio modality through emergent alignment. The method freezes pretrained image and text encoders (ViT and XLM-RoBERTa from OpenCLIP) and trains only the audio encoder (BEATs) using contrastive learning on English audio-text pairs. The frozen text encoder's multilingual capabilities enable zero-shot cross-lingual retrieval. The model uses InfoNCE loss with SpecAugment and Random Truncation augmentation, training for 2-3 epochs on datasets including AudioCaps, ClothoV2, WavCaps, Auto-ACD, and AudioSetCaps.

## Key Results
- Achieves up to 14.24 percentage points improvement in R@1 for audio-text retrieval compared to state-of-the-art models
- Extends to over 100 languages without explicit multilingual training of the audio encoder
- Reduces computational costs by 73% in energy consumption and 79% in training time compared to fully tri-modal baselines

## Why This Works (Mechanism)

### Mechanism 1: Emergent Cross-Modal Alignment via Pre-Aligned Anchor
Training a new modality encoder against a frozen text anchor implicitly aligns it to all modalities already sharing that text space. The pretrained OpenCLIP image-text model creates a shared embedding where images and text co-locate. When audio is contrastively trained only against text, it maps into this existing space. Audio representations that activate similar text embeddings as images become automatically image-aligned without ever seeing image data.

### Mechanism 2: Multilingual Transfer from Monolingual Audio-Text Training
A multilingual text encoder (XLM-RoBERTa) can confer multilingual capability to a non-multilingual audio encoder through frozen-anchor contrastive training on English-only pairs. XLM-RoBERTa encodes semantically similar text across languages into proximate embeddings. When audio is trained to minimize distance to English text embeddings, the audio encoder learns features corresponding to semantic content. At inference, non-English text queries activate similar embeddings (due to XLM-RoBERTa's cross-lingual structure), matching audio that shares that semantic content regardless of training language.

### Mechanism 3: Efficiency via Selective Encoder Training (Modified LiT)
Freezing pretrained encoders and training only the new modality encoder preserves prior knowledge while reducing computational cost substantially. Gradients flow only through the audio encoder and its projection head. The frozen image and text encoders serve as fixed target distributions. This eliminates backpropagation through large transformers, reducing FLOPs, memory, and iterations needed for convergence.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: The core training objective that pulls positive audio-text pairs together and pushes negative pairs apart in embedding space.
  - Quick check question: Can you explain why increasing the batch size typically improves contrastive learning quality?

- **Concept: Frozen Encoder Transfer / Adapter Training**
  - Why needed here: Understanding why and when to freeze parts of a model during extension is essential to applying CACARA's strategy correctly.
  - Quick check question: What are two failure modes when adding a new modality to a frozen multimodal backbone?

- **Concept: Cross-Lingual Representation Learning**
  - Why needed here: The multilingual transfer relies on understanding how models like XLM-RoBERTa encode semantic similarity across languages.
  - Quick check question: Why might a multilingual text encoder fail to transfer capability to a new modality for low-resource languages?

## Architecture Onboarding

- **Component map:**
  - Audio Encoder: BEATs (trainable) -> Projection Head (trainable) -> Shared Embedding Space
  - Text Encoder: XLM-RoBERTa-base (frozen, serves as alignment anchor) -> Projection Head (frozen) -> Shared Embedding Space
  - Image Encoder: ViT (frozen) -> Projection Head (frozen) -> Shared Embedding Space
  - Loss: InfoNCE contrastive loss between audio and text embeddings

- **Critical path:**
  1. Initialize image and text encoders from OpenCLIP checkpoint (already image-text aligned)
  2. Initialize audio encoder from BEATs pretrained weights
  3. Add trainable linear projection for audio to match text embedding dimension
  4. Freeze image and text encoders completely
  5. Train audio encoder + projection on English audio-text pairs with InfoNCE loss
  6. Apply SpecAugment + Random Truncation augmentation to audio

- **Design tradeoffs:**
  - BEATs encoder vs. alternatives: BEATs performed best in ablation (Table 2, 3) but may not generalize to all audio domains
  - Dataset combination: Human-annotated (AudioCaps, ClothoV2) + machine-annotated (WavCaps, AudioSetCaps, Auto-ACD) balances quality and scale; filtering via CLIP similarity (f0.1, f0.2) further refines quality
  - Training resources: 3 epochs + batch size 64 achieves strong results; expanded resources (10 epochs, batch 110) improves classification but not retrieval uniformly

- **Failure signatures:**
  - Low R@1 with high R@10: Weak fine-grained alignment—check projection head capacity and learning rate
  - Multilingual retrieval near zero for specific languages: Those languages likely underrepresented in XLM-RoBERTa pretraining; no fix without text encoder modification
  - Audio-image retrieval fails despite good audio-text: Indicates emergent alignment did not occur—verify text-image encoder was truly pretrained jointly and remains frozen

- **First 3 experiments:**
  1. **Reproduce encoder ablation:** Train CACARA with AudioMAE, HTS-AT, and BEATs on a small subset (5k samples) to confirm BEATs superiority on your target domain before full training.
  2. **Validate emergent alignment:** After training audio against text only, evaluate audio-to-image retrieval on VGG-Sound or similar held-out dataset. Near-random performance indicates implementation error or insufficient training.
  3. **Probe multilingual transfer:** Test audio-to-text retrieval in 2-3 non-English languages (Spanish, Japanese, Swahili) using translated captions. Swahili should underperform significantly (per paper results), confirming language-resource dependence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the emergent alignment paradigm successfully extend to modalities beyond audio (e.g., video, thermal, depth, wearable sensor data) while maintaining cross-modal alignment quality?
- Basis in paper: The authors state: "the present study's scope was limited to the incorporation of the audio modality. Extending the framework to incorporate additional modalities... would provide a more comprehensive assessment of the model's scalability."
- Why unresolved: The text-centric alignment approach was validated only for audio; different modalities may have distinct representational properties that affect emergent alignment efficacy.
- What evidence would resolve it: Experiments integrating video, thermal, or depth encoders using the same emergent alignment protocol, evaluated on multimodal retrieval and classification benchmarks.

### Open Question 2
- Question: How does the emergent multilingual transfer mechanism perform across truly low-resource languages beyond the 12 evaluated, particularly those with minimal representation in XLM-RoBERTa's pretraining data?
- Basis in paper: The authors note: "A more comprehensive evaluation should include languages exhibiting greater typological diversity and, crucially, languages with varying levels of available digital resources." Swahili showed dramatically lower R@1 (1.11%) versus English (33.98%).
- Why unresolved: Performance variations may correlate with XLM-RoBERTa's pretraining language coverage; emergent alignment may inherit and amplify existing multilingual representation gaps.
- What evidence would resolve it: Systematic evaluation across languages stratified by resource level (high/medium/low/extremely-low) with correlation analysis between XLM-RoBERTa pretraining data volume and CACARA multilingual performance.

### Open Question 3
- Question: Does the emergent alignment approach scale effectively to larger encoder architectures while preserving the demonstrated computational efficiency advantages?
- Basis in paper: The authors acknowledge: "our experimental design was constrained to base-level encoder models. A more rigorous evaluation... would necessitate an investigation across a spectrum of encoder sizes."
- Why unresolved: Scaling relationships between model capacity, emergent alignment quality, and computational savings remain uncharacterized; larger encoders may require different alignment strategies.
- What evidence would resolve it: Comparative experiments using base, large, and xxl encoder variants measuring R@1, training time, and energy consumption to establish scaling laws for emergent alignment.

## Limitations
- Direct empirical validation of emergent cross-modal alignment is absent; the hypothesis relies on geometric intuition without ablation studies isolating this effect
- Multilingual performance degrades substantially for lower-resource languages, suggesting transfer is limited by text encoder's pretraining coverage rather than being truly universal
- Key architectural details remain underspecified (exact OpenCLIP model variants, projection head dimensions, batch sampling strategies), creating nontrivial barriers to faithful reproduction

## Confidence

- **High Confidence**: The efficiency claims (73% energy reduction, 79% training time reduction) are well-supported by the ablation study comparing fully tri-modal training vs. frozen-encoder training. The computational savings are clearly demonstrated.
- **Medium Confidence**: The multilingual retrieval improvements (up to 14.24 percentage points R@1 over SoTA) are demonstrated but rely on the untested assumption of emergent alignment. The performance gains could be partially attributed to improved audio encoder quality rather than the cross-modal alignment mechanism itself.
- **Low Confidence**: The claim that the model supports "over 100 languages" is overstated given the substantial performance drop for underrepresented languages. The approach appears to work well for high-resource languages but offers minimal benefit for truly low-resource language pairs.

## Next Checks

1. **Ablation on Emergent Alignment**: Train CACARA with frozen text encoder, but then fine-tune the audio encoder on a small set of image-audio pairs (without text). Compare whether this additional training improves audio-image retrieval beyond what was achieved through text-only training. This would directly test whether emergent alignment actually occurred.

2. **Language Resource Sensitivity Analysis**: Systematically evaluate multilingual retrieval performance across languages grouped by resource levels (high, medium, low based on XLM-RoBERTa pretraining data). Quantify the relationship between text encoder language coverage and audio-to-text retrieval quality to establish the true limits of multilingual transfer.

3. **Domain Transfer Robustness**: Test CACARA on non-speech audio domains (animal sounds, environmental sounds, musical instruments) where the correspondence between audio and text semantics may be weaker or more abstract. Evaluate whether emergent alignment still occurs or whether the approach is primarily effective for speech-like audio content.