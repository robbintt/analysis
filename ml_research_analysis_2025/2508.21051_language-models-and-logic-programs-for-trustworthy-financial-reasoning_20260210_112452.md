---
ver: rpa2
title: Language Models and Logic Programs for Trustworthy Financial Reasoning
arxiv_id: '2508.21051'
source_url: https://arxiv.org/abs/2508.21051
tags:
- reasoning
- which
- few-shot
- direct
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neuro-symbolic architecture that integrates
  large language models with a Prolog-based symbolic solver to improve accuracy and
  auditability in tax reasoning tasks. By combining direct calculation, zero-shot
  parsing, and few-shot parsing using gold statutes and exemplars, the system reduces
  expected error costs to under 20% of the average tax filing cost.
---

# Language Models and Logic Programs for Trustworthy Financial Reasoning

## Quick Facts
- **arXiv ID:** 2508.21051
- **Source URL:** https://arxiv.org/abs/2508.21051
- **Reference count:** 16
- **Primary result:** Combines LLMs with Prolog solver to reduce tax reasoning error costs to under 20% of filing cost

## Executive Summary
This paper introduces a neuro-symbolic architecture that integrates large language models with a Prolog-based symbolic solver to improve accuracy and auditability in tax reasoning tasks. By combining direct calculation, zero-shot parsing, and few-shot parsing using gold statutes and exemplars, the system reduces expected error costs to under 20% of the average tax filing cost. The approach achieves high performance, particularly when using gold symbolic representations and intelligently retrieved precedents, demonstrating the feasibility and effectiveness of augmenting language models with symbolic reasoning for complex statutory reasoning tasks.

## Method Summary
The system reframes statutory reasoning as semantic parsing by translating natural language statutes and cases into executable Prolog programs. Three approaches are evaluated: direct calculation (LLM outputs answer directly), zero-shot parsing (LLM translates to Prolog without exemplars), and few-shot parsing (LLM uses gold statutes and retrieved precedents as examples). The symbolic solver executes the Prolog code with a 10-second timeout, and self-consistency requires agreement between two independently sampled solutions. Performance is measured by exact match accuracy and break-even price, which combines error penalties with the $270 cost of human filing.

## Key Results
- Combining language models with symbolic solvers reduces expected error costs below human filing costs ($<54 vs $270)
- Few-shot parsing with gold statutes and retrieved exemplars dramatically improves accuracy over direct calculation
- Self-consistency significantly reduces false positives by requiring agreement between independent reasoning paths
- Chat models (GPT-4.1, DeepSeek-V3) outperform reasoning models at few-shot parsing when gold exemplars are available

## Why This Works (Mechanism)

### Mechanism 1: Compositional Reasoning Offload to Symbolic Solver
Offloading arithmetic and rule-chaining from the LLM to a Prolog execution engine reduces compositional reasoning errors and provides natural refusal when parsing fails. The LLM translates statutory text and case facts into Prolog Horn clauses. SWI-Prolog then performs backward-chaining search to prove/evaluate the tax obligation query. If the program fails to execute within 10 seconds or produces malformed output, the system abstains rather than guessing. Core assumption: The symbolic solver faithfully implements the logic the LLM intended to express; parsing errors are the dominant failure mode, not solver bugs.

### Mechanism 2: Few-Shot Parsing with Gold Statutes and Retrieved Exemplars
Providing models with gold Prolog representations of statutes and intelligently retrieved precedent cases reduces the parsing task to pattern-matching and event extraction. A retrieval model (o4-mini) ranks cases by structural similarity to the target case. The top-5 retrieved cases and their gold Prolog parses are provided as in-context examples. The LLM conditions on these to parse the new case's facts into Prolog that references the same predicate vocabulary defined in the gold statutes. Core assumption: Similar case text implies similar logical structure; the retrieval model's similarity judgments align with structural analogy needed for correct parsing.

### Mechanism 3: Self-Consistency as Selectivity Filter
Requiring agreement between two independently sampled solutions (same or different methods) reduces false positives at the cost of increased abstention. Two chains-of-thought are sampled with the same prompt. Only if both produce the same final tax obligation is the answer accepted. For parsing-based approaches, this effectively requires two independently generated Prolog programs to yield identical outputs. Core assumption: Correct answers cluster; incorrect answers are more dispersed. This holds better for parsing (execution-constrained) than for direct calculation.

## Foundational Learning

- **Prolog and Horn Clause Logic**: The symbolic solver uses Prolog's backward-chaining unification. Understanding how rules (Horn clauses) and facts populate a knowledge base is essential for debugging parsing failures. Quick check: Given a Prolog rule `liable(X) :- income(X, Y), Y > 10000.`, what query would test if `taxpayer(john)` with `income(john, 12000)` is liable?

- **Semantic Parsing / Autoformalization**: The core task reframes statutory reasoning from "answer the question" to "translate text to executable code." You must understand how natural language maps to formal representations. Quick check: What ambiguities arise when translating "a person who earns more than $10,000 must file" into first-order logic?

- **Self-Consistency and Test-Time Scaling**: The system uses multiple samples and method combinations to improve reliability. Understanding variance vs. bias tradeoffs in sampling is critical. Quick check: If a model produces answers [100, 100, 150, 100] across four samples, what does majority voting yield and what information is discarded?

## Architecture Onboarding

- **Component map:** LLM Parser -> Retrieval System -> SWI-Prolog Engine -> Self-Consistency Layer
- **Critical path:** 1) Ingest case facts and question 2) Retrieve top-5 similar exemplar cases and their gold parses (if few-shot mode) 3) LLM generates Prolog program conditioned on statutes and exemplars 4) Execute in SWI-Prolog; if failure/timeout → abstain 5) If self-consistency enabled, repeat steps 3-4 and compare; accept only on agreement
- **Design tradeoffs:** Upfront annotation vs. inference cost (gold statutes require manual translation but enable cheaper chat models to outperform reasoning models at few-shot parsing); Abstention vs. accuracy (higher abstention reduces error costs but shifts burden to human preparers); Reasoning vs. chat models (reasoning models excel at zero-shot parsing and direct solving; chat models excel at few-shot parsing)
- **Failure signatures:** Timeout (Prolog execution exceeds 10 seconds → complex or cyclic rules); Format mismatch (Parsed Prolog doesn't conform to expected predicate vocabulary → check exemplar alignment); Self-consistency divergence (Two samples disagree → flag for human review); Consistent wrong execution (Program runs but produces wrong answer → parsing captured wrong semantics)
- **First 3 experiments:** 1) Baseline direct calculation: Run target model on SARA cases with plain-text statutes, measuring exact match accuracy and break-even price; 2) Zero-shot parsing ablation: Have model parse cases into Prolog without gold statutes or exemplars, compare execution success rate and accuracy to baseline; 3) Few-shot parsing with retrieval: Implement retrieval-based exemplar selection and measure improvement in parsing accuracy, ablate retrieval quality by using random vs. retrieved exemplars

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies on synthetic SARA dataset with fixed statutory structure, which may not capture real-world tax code evolution and ambiguity
- The retrieval mechanism's effectiveness is asserted but not directly validated through ablation studies comparing different similarity measures
- Gold Prolog representations for statutes represent significant upfront annotation burden that may limit scalability

## Confidence

**Medium:** The core architectural insight (offloading compositional reasoning to Prolog) is well-grounded in literature, but empirical validation relies on synthetic dataset that may not capture real-world complexity.

**High:** The claim that combining language models with symbolic solvers can reduce error costs below human filing costs is supported by reported break-even prices ($<54 vs $270 filing cost).

**Low:** The generalizability of the approach to more complex statutory domains beyond SARA corpus remains uncertain, with no address of how the system would handle statutory amendments or evolving legal precedent.

## Next Checks

1. **Retrieval Ablation Study**: Systematically vary the number and quality of retrieved exemplars (random vs. top-5 vs. top-1) to quantify the marginal contribution of the retrieval mechanism to parsing accuracy.

2. **Temporal Generalization Test**: Apply the system to SARA cases with statutes modified to simulate legislative amendments. Measure degradation in accuracy and parsing success rates to assess robustness to statutory evolution.

3. **Real-World Case Validation**: Test the system on a small set of actual tax cases from federal courts or IRS database (with appropriate redaction). Compare performance against human tax preparers on cases involving genuine statutory ambiguity or competing interpretations.