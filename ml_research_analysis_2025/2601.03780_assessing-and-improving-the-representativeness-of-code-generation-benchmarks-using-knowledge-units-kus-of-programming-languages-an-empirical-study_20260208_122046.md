---
ver: rpa2
title: Assessing and Improving the Representativeness of Code Generation Benchmarks
  Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study
arxiv_id: '2601.03780'
source_url: https://arxiv.org/abs/2601.03780
tags:
- benchmarks
- code
- projects
- real-world
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the representativeness of code generation
  benchmarks by analyzing Knowledge Units (KUs) of Python programming concepts. We
  find that popular benchmarks like HumanEval and MBPP cover only 50% of 20 identified
  Python KUs, while real-world projects use all KUs more evenly.
---

# Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study

## Quick Facts
- arXiv ID: 2601.03780
- Source URL: https://arxiv.org/abs/2601.03780
- Reference count: 40
- This study investigates the representativeness of code generation benchmarks by analyzing Knowledge Units (KUs) of Python programming concepts.

## Executive Summary
This study investigates the representativeness of code generation benchmarks by analyzing Knowledge Units (KUs) of Python programming concepts. We find that popular benchmarks like HumanEval and MBPP cover only 50% of 20 identified Python KUs, while real-world projects use all KUs more evenly. To address this gap, we develop an LLM-based framework that generates 440 KU-specific tasks for underrepresented concepts like Object-Oriented Programming, Exception Handling, and Concurrency. Augmenting benchmarks with these tasks improves KU coverage and distributional alignment by over 60%. Evaluations show that all seven studied LLMs experience statistically significant performance drops (12.54–44.82%) on the augmented benchmarks, indicating that current benchmarks overestimate LLM capabilities due to limited KU coverage.

## Method Summary
The study analyzes KU coverage in HumanEval (164 tasks) and MBPP (974 tasks) by using an LLM-based KU detector to identify which of 20 Python KUs appear in each task. It compares these distributions against 30 real-world Python projects using Jensen-Shannon Distance to identify gaps. The researchers then generate 440 new tasks targeting underrepresented KUs using contextual prompting with real code snippets, validating each task by executing the solution and checking KU presence. The augmented benchmarks are evaluated across seven LLMs using pass@1, pass@3, and pass@5 metrics.

## Key Results
- Popular benchmarks like HumanEval and MBPP cover only 50% of 20 identified Python KUs
- Augmenting benchmarks with KU-specific tasks improves KU coverage and distributional alignment by over 60%
- All seven studied LLMs experience statistically significant performance drops (12.54–44.82%) on augmented benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Distributional Alignment Reduces Evaluation Bias
If code generation benchmarks are skewed toward basic syntax and exclude complex structures, they overestimate model performance. By measuring KU distribution against real-world projects and augmenting benchmarks to match this distribution, the evaluation forces models to demonstrate capabilities in under-represented areas, exposing blind spots.

### Mechanism 2: Context-Grounded Synthesis Preserves KU Integrity
Using real-world code snippets as context allows LLMs to generate synthetic benchmark tasks that reliably exercise specific target KUs. The framework selects source files with high incidence of a target KU and uses these files as "Codebase Context" in the prompt, instructing the generator to create a task requiring that specific KU to solve.

### Mechanism 3: KU Vectorization Enables Fine-Grained Capability Mapping
Modeling code as a 20-dimensional vector of KUs allows for precise identification of model strengths and weaknesses that aggregate metrics obscure. Instead of binary pass/fail, tasks are decomposed into KU vectors, revealing that different LLMs show distinct strengths and weaknesses across KUs.

## Foundational Learning

**Knowledge Units (KUs)**: The core unit of analysis used to measure "representativeness," moving evaluation from functional correctness to conceptual coverage. Quick check: Can you distinguish between the "Concurrency" KU (using threads/async) and the "Context Managers" KU (using `with` statements), or can they overlap in a single task?

**Distributional Fairness (Jensen-Shannon Distance)**: The paper argues that a benchmark isn't just about what concepts are present, but how often they appear relative to real usage. This metric quantifies the "skew" of a benchmark. Quick check: If a benchmark has 100 tasks, and 90 are simple arithmetic (Operators KU), would the JSDistance to a real-world project likely be high (dissimilar) or low (similar)?

**Synthetic Benchmark Augmentation**: The proposed solution isn't replacing benchmarks but augmenting them. Understanding how to safely add synthetic data to ground truth is critical for system design. Quick check: Why is "validation" (running the generated code and checking for the KU) a necessary step before accepting a synthetically generated task?

## Architecture Onboarding

**Component map**: KU Detector -> Distribution Comparator -> Task Synthesizer -> Validator

**Critical path**: The Validation Step. If the Task Synthesizer hallucinates a solution that doesn't actually use the target KU, the augmentation fails to improve representativeness.

**Design tradeoffs**: The authors chose LLMs for KU detection over static analysis for cost-effectiveness and context awareness, trading off deterministic guarantees for flexibility. Generating tasks synthetically allows for rapid scaling but may lack the "messiness" of real-world repository dependencies.

**Failure signatures**: A model scores 90% on HumanEval but drops to 50% on Augmented-HumanEval, indicating the original benchmark was "inflated" by over-represented easy KUs. The synthesizer produces code that passes the prompt check but fails execution or the KU detector check.

**First 3 experiments**:
1. Run the KU Detector on a small set of canonical solutions and manually verify if the detected KUs match the code's functionality.
2. Calculate the JSDistance of your current benchmark against the "Real-World" distribution to see which KUs are missing.
3. Generate 5 tasks for a single missing KU and evaluate your model's pass@1 on just those 5 tasks vs. its average HumanEval score.

## Open Questions the Paper Calls Out

**Open Question 1**: Do the observed gaps in KU coverage and model performance drops generalize to programming languages other than Python? The study focuses only on Python and invites future work to extend KU-based analysis to other programming languages.

**Open Question 2**: How can the task generation framework be extended to capture complex structural dependencies, such as class-level or module-level interactions? The current framework focuses on language constructs and APIs, ignoring higher-level structural complexity.

**Open Question 3**: Does incorporating capability-level difficulty weighting into the task selection process provide a more nuanced evaluation of LLMs? The current method selects source files based on KU instance counts, potentially missing diverse difficulty levels within a single KU.

## Limitations
- The study's focus on Python limits generalizability to other programming languages
- The synthetic task generation approach may not capture the complexity of real-world code dependencies
- The KU detector's accuracy on benchmark data may differ from its validation on Python files

## Confidence

**High Confidence**: The observation that HumanEval and MBPP show significant KU coverage gaps compared to real-world projects is well-supported by the distributional analysis.

**Medium Confidence**: The finding that LLM performance drops on augmented benchmarks is robust, but specific performance degradation percentages depend on the quality of synthetic tasks.

**Low Confidence**: The claim that no single model dominates across all KUs is based on aggregate analysis without deeper investigation into whether specific models excel at specific KU types.

## Next Checks

1. **Cross-Detector Validation**: Implement the KU detector using a different LLM or static analysis approach and compare results on a subset of benchmarks to verify the 93% accuracy claim holds across detection methods.

2. **Task Validation Audit**: Manually review 50 randomly selected generated tasks to verify that solutions actually require the target KU to solve, and that test cases correctly validate KU usage.

3. **Temporal Distribution Analysis**: Recompute KU distributions using projects from different time periods (e.g., 2018 vs 2023) to assess whether identified coverage gaps are stable over time or reflect temporary language trends.