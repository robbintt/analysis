---
ver: rpa2
title: Applying Large Language Models to Characterize Public Narratives
arxiv_id: '2511.13505'
source_url: https://arxiv.org/abs/2511.13505
tags:
- story
- narrative
- codes
- values
- public
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a computational framework that leverages large
  language models (LLMs) to automate qualitative annotation of Public Narratives (PNs),
  a leadership and civic mobilization tool. A codebook co-developed with subject-matter
  experts enables structured annotation of 14 narrative codes across 22 narratives.
---

# Applying Large Language Models to Characterize Public Narratives

## Quick Facts
- arXiv ID: 2511.13505
- Source URL: https://arxiv.org/abs/2511.13505
- Reference count: 40
- Introduces LLM-based framework for automated qualitative annotation of Public Narratives with 0.80 average F1 score

## Executive Summary
This work presents a computational framework that leverages large language models (LLMs) to automate qualitative annotation of Public Narratives (PNs), a leadership and civic mobilization tool. The framework uses a codebook co-developed with subject-matter experts to enable structured annotation of 14 narrative codes across 22 narratives. Testing demonstrates that o3-mini with Chain-of-Thought prompting and prompt chaining achieves 0.80 average F1 score, performing near human experts.

The study shows that LLMs excel at structural and categorical codes but struggle with more subjective elements like dream, nightmare, and urgency. The framework is validated through analysis of 22 stories and two political speeches, demonstrating scalability and applicability to broader civic narrative contexts. This establishes LLMs as a viable tool for large-scale narrative analysis and provides a foundation for future computational civic storytelling research.

## Method Summary
The framework co-develops a codebook with subject-matter experts to define 14 narrative codes for annotating Public Narratives. LLMs are prompted using Chain-of-Thought reasoning and prompt chaining to systematically annotate each code across narratives. The o3-mini model is evaluated against expert annotations from 8 stories, achieving 0.80 average F1 score. The approach is then applied to analyze 22 stories and two political speeches, demonstrating both precision and scalability for civic narrative analysis.

## Key Results
- o3-mini with Chain-of-Thought prompting achieves 0.80 average F1 score, performing near human expert level
- Framework successfully scales to analyze 22 narratives and two political speeches
- LLMs show strong performance on structural and categorical codes but struggle with subjective elements like dream, nightmare, and urgency

## Why This Works (Mechanism)
The framework succeeds because it combines expert-designed codebooks with structured prompting strategies that guide LLMs through systematic narrative analysis. Chain-of-Thought prompting enables step-by-step reasoning that mirrors human annotation processes, while prompt chaining allows complex multi-code annotation tasks to be broken down into manageable components. The o3-mini model's architecture appears well-suited for handling the categorical and structural elements of Public Narratives, though its limitations with subjective interpretation remain evident.

## Foundational Learning
- **Public Narrative structure**: Understanding the three-part structure (story of self, us, now) is essential for meaningful annotation and code design.
- **Chain-of-Thought prompting**: Enables LLMs to perform multi-step reasoning similar to human experts, improving annotation quality for complex tasks.
- **Prompt chaining**: Breaks down complex annotation tasks into sequential steps, allowing systematic processing of multiple narrative codes.
- **Codebook development**: Expert collaboration ensures codes capture meaningful narrative elements while maintaining annotation consistency.
- **F1 score evaluation**: Provides balanced measure of precision and recall for assessing annotation accuracy against human standards.

## Architecture Onboarding

**Component Map**: Expert Codebook -> Chain-of-Thought Prompting -> Prompt Chaining -> o3-mini Model -> F1 Score Evaluation

**Critical Path**: Codebook development → Prompt strategy design → Model annotation → Expert evaluation

**Design Tradeoffs**: Uses smaller o3-mini model for efficiency versus larger models that might offer better performance but higher computational cost. Prioritizes systematic annotation over exploratory analysis.

**Failure Signatures**: Poor performance on subjective codes (dream, nightmare, urgency) indicates limitations in handling interpretive elements. Potential overfitting to specific narrative styles or domains.

**3 First Experiments**:
1. Test framework on narratives from different domains (healthcare, education, environmental) to assess domain transfer
2. Compare performance of different LLM architectures (GPT-4, Claude) using identical prompting strategies
3. Evaluate impact of different prompt engineering approaches (few-shot vs zero-shot) on annotation accuracy

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Results may not generalize beyond the specific 22 narratives and two political speeches used in the study
- Small expert annotation set (8 stories) may not capture full complexity of human annotation practices
- Codebook may not comprehensively capture all aspects of Public Narratives, particularly subjective elements
- Specific prompting strategies and o3-mini model choice may limit generalizability to other LLM architectures

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| LLM framework's structural/categorical annotation accuracy | High |
| Framework scalability and broader applicability | Medium |
| LLM struggle with subjective elements | Medium |

## Next Checks
1. Test the framework on a larger and more diverse corpus of narratives from various domains, cultural contexts, and time periods to assess generalizability and robustness.
2. Conduct a comparative study with multiple LLM architectures (e.g., GPT-4, Claude, Llama) and prompting techniques (e.g., few-shot learning, zero-shot learning) to identify optimal combinations.
3. Perform a longitudinal study using the LLM framework to analyze time series narratives from the same domain, tracking trends and shifts in public narratives over time.