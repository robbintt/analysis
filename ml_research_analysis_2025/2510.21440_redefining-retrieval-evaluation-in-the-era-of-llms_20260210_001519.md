---
ver: rpa2
title: Redefining Retrieval Evaluation in the Era of LLMs
arxiv_id: '2510.21440'
source_url: https://arxiv.org/abs/2510.21440
tags:
- answer
- relevance
- relevant
- metrics
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the misalignment between traditional IR evaluation
  metrics and the needs of RAG systems. It introduces a utility-based annotation schema
  that captures both the positive contribution of relevant passages and the negative
  impact of distracting ones, and proposes UDCG, a metric using an LLM-oriented positional
  discount to directly optimize correlation with end-to-end answer accuracy.
---

# Redefining Retrieval Evaluation in the Era of LLMs

## Quick Facts
- arXiv ID: 2510.21440
- Source URL: https://arxiv.org/abs/2510.21440
- Reference count: 23
- Primary result: UDCG improves correlation with end-to-end accuracy by up to 36% compared to traditional metrics

## Executive Summary
This paper addresses the misalignment between traditional IR evaluation metrics and the needs of RAG systems by introducing a utility-based annotation schema that captures both the positive contribution of relevant passages and the negative impact of distracting ones. The authors propose UDCG, a metric using an LLM-oriented positional discount to directly optimize correlation with end-to-end answer accuracy. Experiments on five datasets and six LLMs demonstrate that UDCG significantly outperforms traditional metrics in predicting RAG system accuracy, with improvements of up to 36% in Spearman correlation.

## Method Summary
The method introduces utility-based annotation where passages are scored based on their contribution to LLM answer quality rather than binary relevance. Utility is computed as `u(q,p) = R(q,p) × (1 - P_LLM(NO-RESPONSE|q,p))`, where relevant passages contribute positively if the LLM can extract answers, and irrelevant passages contribute negatively proportional to their distracting effect. UDCG aggregates these utilities using a learned positional discount function, though experiments show that ignoring position yields nearly identical results. The framework requires access to LLM logits to compute abstention probabilities, limiting its applicability to black-box APIs.

## Key Results
- UDCG achieves up to 36% higher Spearman correlation with end-to-end accuracy compared to traditional metrics like nDCG
- Utility-based scoring improves accuracy by approximately 2% over binary relevance alone
- Hard distractors (DE>0.8) reduce accuracy by 5-9 points compared to weak distractors (DE<0.2)
- Training-free UDCG performs nearly identically to position-aware UDCG_θ, suggesting positional bias is marginal in real RAG scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Utility-based annotation captures LLM-specific document value better than binary relevance.
- Mechanism: Replace human relevance judgments with continuous utility scores computed as `u(q,p) = R(q,p) × (1 - P_LLM(NO-RESPONSE|q,p))`, where relevant passages contribute positively if the LLM can extract the answer, and irrelevant passages contribute negatively proportional to their distracting effect.
- Core assumption: LLM abstention probability when shown a passage in isolation is a valid proxy for that passage's utility in a multi-document context.
- Evidence anchors: [abstract] utility-based schema quantifies positive contribution and negative impact; [section 4] Eq. 2 formalizes utility; Table 2 shows utility-based selection improves accuracy by ~2% over binary relevance.

### Mechanism 2
- Claim: Distracting passages cause active harm proportional to their semantic relatedness to the query.
- Mechanism: Semantically related but non-answer-containing passages trigger hallucination or incorrect synthesis. The distracting effect `DE_q(p) = 1 - P_LLM(NO-RESPONSE|q,p)` quantifies this: high-decoy passages cause the LLM to answer incorrectly rather than abstain.
- Core assumption: The distracting effect of individual passages aggregates linearly (or predictably) when multiple distracting passages appear together.
- Evidence anchors: [section 3, Table 1] Hard distractors reduce accuracy by 5-9 points vs weak distractors; [section 7.3, Table 4] removing distracting effect features drops correlation by 3.4-8%.

### Mechanism 3
- Claim: LLM positional bias patterns differ from human monotonic discounting, but the effect is marginal in real RAG settings.
- Mechanism: LLMs exhibit U-shaped attention ("lost-in-the-middle"), not monotonic decay. UDCG_θ learns position-specific weights for relevance and distraction. However, the training-free UDCG performs nearly identically, suggesting positional effects are small in practice.
- Core assumption: Positional bias observed in controlled experiments generalizes to typical RAG retrieval distributions.
- Evidence anchors: [section 3, Figure 1] LLM accuracy shows U-shape; traditional metrics decline monotonically; [section 7.2] UDCG and UDCG_θ achieve similar correlation.

## Foundational Learning

- Concept: Discounted Cumulative Gain (DCG) and nDCG
  - Why needed here: UDCG directly extends DCG's weighted-sum formulation; understanding the base metric is prerequisite.
  - Quick check question: Given relevance grades [3, 0, 2, 1] at positions 1-4, compute DCG@4 using log_2(i+1) discount.

- Concept: LLM-as-a-Judge evaluation
  - Why needed here: Paper uses Claude 3.7 Sonnet for relevance annotation and Gemini 2.0 Flash for answer grading; this is the evaluation infrastructure.
  - Quick check question: What are two failure modes of LLM-as-judge when grading semantic equivalence?

- Concept: Abstention-based evaluation
  - Why needed here: Utility scoring relies on P_LLM(NO-RESPONSE|q,p); understanding abstention calibration is critical.
  - Quick check question: Why might an LLM abstention rate of 30% be preferable to 5% in high-stakes domains?

## Architecture Onboarding

- Component map:
  Retrieval Stage -> Annotation Stage -> Scoring Stage -> Evaluation Stage
  BGE-large-en-v1.5 -> Claude 3.7 Sonnet -> Utility computation -> Gemini 2.0 Flash

- Critical path:
  1. Retrieve top-25 passages per query using BGE-large-en-v1.5
  2. For each passage: get relevance label from Claude 3.7 Sonnet, query target LLM with passage-only prompt, extract P(NO-RESPONSE) from logits
  3. Compute utility scores; select top-k by utility for ranking evaluation
  4. Generate LLM answers using sampled 5-passage contexts, grade with Gemini 2.0 Flash, compute Spearman correlation

- Design tradeoffs:
  - UDCG_θ vs UDCG: Trainable version captures positional bias but requires dataset-specific training and fixed k; training-free version generalizes better
  - Isolated-passage vs full-context utility: Isolated evaluation is O(k·w²) vs full-context O(g·k²·w²), but may miss interaction effects
  - Binary vs graded relevance: Binary is simpler; graded could capture partial relevance but increases annotation cost

- Failure signatures:
  - Low correlation with accuracy: Check if distracting passages dominate (γ too low) or relevance dominates (γ too high)
  - Metric saturates at 1.0: Verify relevance annotations aren't overly generous; check for annotation leakage
  - Abstention probability always near 0 or 1: Check LLM prompt formatting; verify logit access is working

- First 3 experiments:
  1. Reproduce Table 1 on your domain: Compare accuracy with hard vs weak distractors to validate distracting effect transfer
  2. Calibrate γ via grid search: Sweep γ ∈ {0, 0.1, ..., 1.0} on held-out queries; select value maximizing Spearman correlation with accuracy
  3. Ablate utility components: Compare binary relevance only, relevance + utility scoring, and full UDCG with distraction; quantify each component's contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does UDCG maintain its predictive power for RAG tasks beyond single-hop question answering, such as multi-hop reasoning, fact verification, or open-ended generation tasks?
- Basis in paper: [explicit] "The applicability of our metrics to other tasks, such as multi-hop question answering or fact verification, remains unexplored."
- Why unresolved: Experiments were restricted to QA datasets with verifiable answers, leaving other RAG use cases untested.
- What evidence would resolve it: Correlation experiments on multi-hop QA (e.g., HotpotQA) and fact verification benchmarks (e.g., FEVER) comparing UDCG to traditional metrics.

### Open Question 2
- Question: Can the utility-based annotation schema be adapted for black-box LLM APIs that do not expose output probabilities or logits?
- Basis in paper: [explicit] "Our utility-based annotation schema requires access to the LLM's output probabilities to compute abstention likelihood. This requirement limits our approach to settings where logit access is available, excluding black-box commercial APIs."
- Why unresolved: The utility score depends on computing p_LLM(NO-RESPONSE|q,p), which requires access to token probabilities unavailable from many commercial APIs.
- What evidence would resolve it: Development and validation of proxy methods (e.g., consistency-based abstention detection or semantic classifiers) that approximate abstention likelihood without logit access.

### Open Question 3
- Question: How effectively does UDCG transfer to multilingual or cross-lingual RAG settings?
- Basis in paper: [explicit] "The effectiveness of our utility-based scoring and the UDCG metric in multilingual or cross-lingual RAG settings remains to be investigated, as language-specific characteristics may influence both passage utility and distracting effects."
- Why unresolved: All five evaluation datasets are English-only, and distracting effects may manifest differently across languages.
- What evidence would resolve it: Experiments on multilingual QA benchmarks (e.g., MKQA, XOR-TyDi) showing whether utility scores and γ hyperparameters generalize across languages.

## Limitations

- Utility score calibration uncertainty: The assumption that LLM abstention probability in isolation proxies for utility in multi-passage contexts is only partially validated
- Generalization across domains: Limited to five QA datasets without testing cross-domain transfer of utility scoring or γ calibration
- Positional bias claims: Based on correlation comparison between UDCG and UDCG_θ, but lacks direct positional effect validation

## Confidence

**High Confidence**: Mechanism 1 (Utility-based annotation captures LLM-specific value) - Well-formalized with equations and quantitative improvements (~2% accuracy gain)

**Medium Confidence**: Mechanism 2 (Distracting passages cause active harm) - Supported by accuracy drops but linear aggregation assumption isn't validated; Mechanism 3 (Positional bias is marginal) - Based on correlation comparison but lacks direct positional effect validation

**Low Confidence**: Generalization across domains - Focused on five QA datasets without cross-domain testing; Abstention detection reliability - Critical for utility computation but method details are unspecified

## Next Checks

1. **Domain Transfer Test**: Apply UDCG to a different domain (e.g., legal documents, medical records) and validate whether the γ=1/3 parameter and utility scoring mechanism maintain correlation improvements

2. **Ablation of Positional Effects**: Systematically vary context window size (k=10, 25, 50, 100) and measure the performance gap between UDCG and UDCG_θ to validate whether positional bias truly becomes negligible

3. **Alternative Abstention Detection Methods**: Implement and compare at least two different methods for detecting LLM abstention (e.g., explicit "I don't know" patterns vs. empty response vs. confidence threshold) to validate robustness to the unspecified detection method