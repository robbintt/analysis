---
ver: rpa2
title: Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks
arxiv_id: '2509.13338'
source_url: https://arxiv.org/abs/2509.13338
tags:
- uni00000013
- uni00000014
- uni00000003
- uni00000011
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes an evidence-retrieval mechanism for uncertainty-aware
  decision-making that replaces global cutoff thresholds with instance-adaptive, evidence-conditioned
  criteria. For each test instance, the method retrieves similar exemplars from an
  embedding space, combines their predictive distributions using Dempster-Shafer theory,
  and uses the fused belief as a per-instance threshold.
---

# Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks

## Quick Facts
- arXiv ID: 2509.13338
- Source URL: https://arxiv.org/abs/2509.13338
- Reference count: 40
- One-line primary result: Instance-adaptive evidence-conditioned criteria outperform global thresholds for uncertainty-aware decision-making

## Executive Summary
This paper introduces a proximity-based evidence retrieval mechanism for uncertainty-aware neural networks that replaces static decision thresholds with instance-specific criteria derived from similar exemplars. The approach retrieves relevant evidence from an embedding space, fuses their predictive distributions using Dempster-Shafer theory, and uses the resulting belief mass as a per-instance threshold for classification decisions. Experiments demonstrate that this evidence-conditioned approach achieves higher or comparable uncertainty-aware performance while producing fewer confidently incorrect outcomes and maintaining sustainable review loads compared to fixed entropy thresholds.

## Method Summary
The method operates by retrieving similar exemplars from an embedding space for each test instance, then combining their predictive distributions through Dempster-Shafer evidence fusion. This produces a belief mass for each class that serves as an instance-adaptive threshold for decision-making. Instead of applying a global confidence cutoff, the system uses the fused belief from retrieved evidence to determine whether to accept a prediction or defer to human review. The approach is evaluated on CIFAR-10/100 datasets using BiT and ViT backbone architectures, demonstrating improvements across accuracy, calibration, and uncertainty-aware metrics while requiring only a small number of evidence instances to achieve these gains.

## Key Results
- Instance-adaptive evidence-conditioned criteria achieve higher or comparable uncertainty-aware performance versus fixed prediction entropy thresholds
- The approach produces fewer confidently incorrect outcomes while maintaining sustainable review loads
- Performance improvements are realized with only a few evidence instances, demonstrating efficiency in the retrieval mechanism

## Why This Works (Mechanism)
The method works by grounding decisions in retrieved historical examples rather than relying on global thresholds. By retrieving exemplars similar to each test instance and fusing their predictive distributions, the system creates context-specific decision boundaries that reflect the uncertainty inherent in similar cases. The Dempster-Shafer theory of evidence provides a principled framework for combining multiple sources of uncertainty, allowing the system to quantify belief and plausibility for each class. This evidence-conditioned approach naturally adapts to the local uncertainty landscape of the data, providing more nuanced decision boundaries than static thresholds.

## Foundational Learning

**Embedding-based similarity search**: Understanding how to represent instances in a shared embedding space where semantic similarity corresponds to proximity. Needed to retrieve meaningful exemplars; quick check: verify retrieval quality through nearest-neighbor analysis.

**Dempster-Shafer theory of evidence**: Grasping belief function theory for combining uncertain evidence from multiple sources. Needed to fuse predictions from retrieved exemplars; quick check: validate fusion results on synthetic evidence combinations.

**Uncertainty quantification in neural networks**: Understanding how predictive distributions encode uncertainty and how to interpret entropy/conformity scores. Needed to evaluate confidence in predictions; quick check: analyze calibration curves on validation data.

**Instance-adaptive decision thresholds**: Recognizing when per-instance criteria outperform global thresholds for decision-making under uncertainty. Needed to justify the evidence-conditioned approach; quick check: compare performance against fixed-threshold baselines.

## Architecture Onboarding

**Component map**: Input data → Embedding network → Similarity search → Evidence retrieval → Dempster-Shafer fusion → Belief mass computation → Decision threshold → Classification/rejection

**Critical path**: The retrieval and fusion components form the critical path, as the quality of retrieved exemplars and the effectiveness of evidence combination directly determine decision quality and uncertainty quantification.

**Design tradeoffs**: The method trades computational overhead of similarity search and evidence fusion against improved uncertainty calibration and reduced false positives. More retrieved exemplars improve reliability but increase latency and computational cost.

**Failure signatures**: Poor retrieval quality (semantically dissimilar exemplars), over-reliance on a small number of highly confident but incorrect exemplars, and failure to handle domain shifts where retrieved exemplars are not representative.

**First experiments**: 1) Validate retrieval quality by analyzing nearest neighbors for sample instances, 2) Test Dempster-Shafer fusion on synthetic evidence with known ground truth, 3) Compare calibration curves against fixed-threshold baselines on validation data.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on embedding space proximity may not capture true semantic similarity, especially under domain shifts or adversarial perturbations
- Dempster-Shafer fusion assumes independence of evidence, which may not hold for correlated features in natural image datasets
- Performance evaluation is limited to CIFAR-10/100 datasets with specific backbone architectures, limiting generalizability
- The claimed sustainability of review loads lacks quantitative thresholds or operational constraints
- Transparency benefits assume human reviewers can meaningfully interpret retrieved exemplars, which is not empirically validated

## Confidence
- Uncertainty-aware performance improvements: Medium (controlled experimental conditions)
- Operational claims about review load and transparency: Low (no real-world deployment evidence)
- Robustness to domain shifts and adversarial attacks: Low (not tested)

## Next Checks
1. Test the evidence-retrieval mechanism on out-of-distribution datasets and under adversarial attacks to quantify robustness degradation
2. Conduct human studies to verify that reviewers can meaningfully use retrieved exemplars for decision-making and that review loads are indeed sustainable
3. Evaluate the method across diverse architectures (CNNs, RNNs, transformers) and domains (medical imaging, NLP) to assess generalizability beyond CIFAR experiments