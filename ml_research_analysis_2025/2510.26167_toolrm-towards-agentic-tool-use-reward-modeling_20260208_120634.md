---
ver: rpa2
title: 'ToolRM: Towards Agentic Tool-Use Reward Modeling'
arxiv_id: '2510.26167'
source_url: https://arxiv.org/abs/2510.26167
tags:
- response
- tool
- type
- user
- call
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToolRM, a family of lightweight reward models
  for tool-use tasks. The authors develop a novel pipeline that constructs high-quality
  pairwise preference data using rule-based scoring and multidimensional sampling,
  yielding a diverse and challenging dataset of 30,000 preference pairs.
---

# ToolRM: Towards Agentic Tool-Use Reward Modeling

## Quick Facts
- arXiv ID: 2510.26167
- Source URL: https://arxiv.org/abs/2510.26167
- Reference count: 40
- Primary result: 17.94% higher pairwise reward judgment accuracy on tool-use tasks

## Executive Summary
This paper introduces ToolRM, a family of lightweight reward models for tool-use tasks. The authors develop a novel pipeline that constructs high-quality pairwise preference data using rule-based scoring and multidimensional sampling, yielding a diverse and challenging dataset of 30,000 preference pairs. They also introduce TRBench BFCL, a benchmark for evaluating reward models on tool calling tasks. Trained on this data, models from the Qwen3 series achieve up to 17.94% higher accuracy in pairwise reward judgments compared to existing LLMs and reward models. Beyond training, generative ToolRM generalizes to broader critique tasks, enables efficient inference-time scaling, and supports effective downstream RL training, reducing output token usage by over 66%.

## Method Summary
ToolRM constructs high-quality pairwise preference data through a rule-based scoring pipeline applied to seven standardized tool-calling datasets. The process involves schema validation, trajectory segmentation, and response sampling from five different LLMs. Rule-based scoring evaluates tool call correctness by comparing tool names and argument key-value pairs, filtering out invalid or duplicate calls. The balanced multi-dimensional sampling (BMDS) algorithm ensures diverse coverage across preference intensity, task complexity, and data sources. Two reward model variants are trained: ToolRM-Gen using GRPO for generative critique capabilities, and ToolRM-Disc using Bradley-Terry loss for discriminative pairwise classification. Both models are evaluated on the TRBench BFCL benchmark.

## Key Results
- Up to 17.94% higher pairwise reward judgment accuracy compared to existing LLMs and reward models
- 66%+ reduction in output token usage when used in downstream RL training
- Successful generalization to Best-of-N sampling and self-correction tasks
- Superior performance on multi-turn and multi-tool scenarios

## Why This Works (Mechanism)

### Mechanism 1: Rule-Based Preference Scoring Enables Verifiable Ground Truth
Conditional on having valid tool-call trajectories and schemas, rule-based scoring can produce reliable preference labels without human annotation. Parse ground-truth and candidate tool calls into structured JSON; apply disqualifiers (mismatched call count, duplicates); compute argument-level similarity matches; aggregate into scalar scores. Core assumption: Tool-call correctness can be decomposed into discrete, comparable elements (name matching, argument key-value alignment) and that these correlate with task success.

### Mechanism 2: Balanced Multi-Dimensional Sampling Prevents Distribution Collapse
If preference data covers diverse sources, preference intensities, and task complexities in balanced proportions, reward models generalize better to OOD tool-use scenarios. Discretize preference intensity into bins; group samples by (source, bin_index); sort groups by complexity; greedily allocate quotas to balance representation across dimensions. Core assumption: Generalization requires exposure to both easy/hard comparisons and strong/weak preference signals; omitting any dimension creates exploitable shortcuts.

### Mechanism 3: Pairwise GRPO Training Induces Verifiable Critique Reasoning
Training generative reward models with binary rewards on pairwise comparisons encourages reasoning that generalizes to critique and self-correction tasks. Given query q and ground-truth answer a, generate G rollout trajectories; compute relative advantages via group normalization; apply clipped PPO objective with binary reward on extracted choice correctness. Core assumption: The extraction and equivalence check reliably maps model outputs to discrete choices; reasoning traces learned during training transfer to pointwise critique.

## Foundational Learning

- **Reward Modeling as Classification**: Why needed here: ToolRM frames reward judgment as pairwise classification, not regression; requires understanding preference ordering rather than absolute scoring. Quick check question: Given responses A (score 0.8) and B (score 0.6), should a pairwise RM learn P(A>B) or |0.8-0.6|?

- **Advantage Normalization in RL**: Why needed here: GRPO replaces critic networks with group-relative advantages; requires understanding why normalization stabilizes training. Quick check question: Why divide by std({r1...rG}) instead of using raw rewards directly?

- **Position Bias in Preference Data**: Why needed here: 50% random swapping prevents models from learning "first response is usually better"; requires understanding evaluation robustness. Quick check question: If a model achieves 70% accuracy on swapped-order evaluation but 90% on original-order, what does this indicate?

## Architecture Onboarding

- **Component map**: Raw Trajectories → Schema Validation → Segmentation → Response Sampling (5 LLMs) → Rule-Based Scoring (eq. 1-4) → Difficulty Filtering → D_cand → BMDS (Algorithm 1) → D_pair-sampled (30K pairs) → GRPO Training (GenRM) / BT Loss (DiscRM) → ToolRM-Gen / ToolRM-Disc → TRBench_BFCL Evaluation

- **Critical path**: 1) Rule-based scoring correctness (disqualifiers + match calculation) 2) BMDS balancing across 3 dimensions (source, intensity, complexity) 3) GRPO rollout generation and binary reward extraction

- **Design tradeoffs**: GenRM vs DiscRM: GenRM provides interpretable critiques (self-correction) but DiscRM yields higher pairwise accuracy (Table 6: 77.61% vs 66.85% on Qwen3-4B-Instruct). Data scale vs complexity: 30K optimal; larger datasets reduce average complexity (Figure 5b). Think-mode vs No-think-mode: Reasoning models benefit more from RL; instruction-tuned models better for BT objective.

- **Failure signatures**: Extraction failures: Model outputs invalid `<choice>` tags → binary reward always 0. Position bias: Accuracy varies with response order → insufficient swapping during training. Overfitting to score distribution: Pointwise training mimics scores without learning reasoning (preliminary experiments, section 3.2).

- **First 3 experiments**: 1) Validate rule-based scoring: Apply scorer to held-out trajectories; correlate with human judgments on 100 samples; expect >0.8 agreement on clear-cut cases. 2) Ablate BMDS dimensions: Train models with each dimension removed; measure TRBench_BFCL accuracy; expect largest drop from removing task complexity. 3) Compare training objectives: Train identical base model (Qwen3-4B) with GRPO vs BT loss on same 30K data; evaluate on BoN sampling and self-correction separately; expect GenRM superior on self-correction, DiscRM superior on BoN.

## Open Questions the Paper Calls Out

- **How can ToolRM be extended to provide holistic trajectory-level feedback for multi-turn agentic tasks?** The authors state in the Limitations section: "Extending ToolRM to evaluate full trajectories is important but technically challenging; because it would require different objectives and substantial additional work, we leave it to future research." Current ToolRM operates as a step-wise critic (process reward model), not evaluating overall task outcomes.

- **Can reward models effectively guide multi-agent coordination and planning in open-ended agentic scenarios?** The conclusion states: "Future work could extend this approach to more open-ended agentic tasks, including RM-guided multi-agent coordination and planning." ToolRM was only evaluated on single-agent tool-use tasks.

- **How can training data be improved to help models distinguish primary from secondary errors in tool-call preferences?** Section 4.5 Error Analysis identifies that the model "fails to distinguish primary errors from secondary ones" when the chosen response contains minor errors, and notes this "can be mitigated through targeted optimization using higher-quality, non-perfect preference pairs."

## Limitations
- Rule-based scoring mechanism's reliability depends heavily on tool schema quality and may not generalize to tasks with ambiguous or underspecified tool descriptions
- The balanced multi-dimensional sampling strategy's effectiveness is difficult to verify without access to the complete candidate pool
- The GRPO training approach's transfer to broader critique tasks, while demonstrated, may not hold for domains outside tool-use scenarios

## Confidence

- **High confidence** in the pairwise accuracy improvements (up to 17.94%) and downstream RL benefits (66%+ token reduction), as these are directly measurable on established benchmarks
- **Medium confidence** in the generalization to critique tasks, given the novel application of GRPO to this domain
- **Medium confidence** in the BMDS sampling strategy's contribution, as the specific implementation details and candidate pool characteristics are not fully disclosed

## Next Checks

1. **Schema Dependency Validation**: Test rule-based scoring on tool descriptions with varying levels of specification completeness to quantify performance degradation from schema ambiguities

2. **Sampling Strategy Ablation**: Systematically remove each BMDS dimension (source, intensity, complexity) during training to measure individual contributions to final accuracy

3. **Cross-Domain Transfer Test**: Evaluate the trained reward models on non-tool-use critique tasks (e.g., code review, mathematical proof checking) to assess generalization beyond the training domain