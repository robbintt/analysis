---
ver: rpa2
title: Censoring-Aware Tree-Based Reinforcement Learning for Estimating Dynamic Treatment
  Regimes with Censored Outcomes
arxiv_id: '2503.06690'
source_url: https://arxiv.org/abs/2503.06690
tags:
- treatment
- stage
- survival
- censoring
- ca-trl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of estimating dynamic treatment
  regimes (DTRs) from observational data with censored survival outcomes, where the
  exact time of an event is unknown but is known to occur after a certain observed
  time point. The proposed Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL)
  framework extends traditional tree-based reinforcement learning methods by incorporating
  augmented inverse probability weighting (AIPW) and censoring-aware modifications.
---

# Censoring-Aware Tree-Based Reinforcement Learning for Estimating Dynamic Treatment Regimes with Censored Outcomes

## Quick Facts
- arXiv ID: 2503.06690
- Source URL: https://arxiv.org/abs/2503.06690
- Reference count: 40
- Primary result: CA-TRL achieves higher RMST and decision-making accuracy than ASCL in settings with censored survival outcomes

## Executive Summary
This paper addresses the challenge of estimating dynamic treatment regimes (DTRs) from observational data with censored survival outcomes, where the exact time of an event is unknown but is known to occur after a certain observed time point. The proposed Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL) framework extends traditional tree-based reinforcement learning methods by incorporating augmented inverse probability weighting (AIPW) and censoring-aware modifications. This allows CA-TRL to deliver robust and interpretable treatment strategies even in the presence of incomplete data. Through extensive simulations and real-world applications using the SANAD epilepsy dataset, CA-TRL demonstrated superior performance compared to the recently proposed ASCL method, achieving higher restricted mean survival time (RMST) and decision-making accuracy. The method represents a significant advancement in personalized and data-driven treatment strategies for diverse healthcare settings.

## Method Summary
CA-TRL is a two-stage tree-based reinforcement learning method designed to estimate optimal DTRs from observational data with right-censored survival outcomes. The method uses backward induction, starting with the final stage where the observed survival time serves as the pseudo-outcome. At each stage, CA-TRL estimates propensity scores, censoring survival probabilities, and conditional mean outcomes using Random Survival Forests. These estimates are combined in a modified AIPW estimator that accounts for censoring, producing counterfactual mean outcomes. Decision trees are grown using counterfactual purity measures, and pseudo-outcomes for the previous stage are calculated based on the estimated optimal treatment rules. The process repeats backward through all stages, resulting in interpretable treatment rules that can be directly applied in clinical settings.

## Key Results
- CA-TRL achieved 89.13% average correct decision rate (ACDR) compared to ASCL's 52.09% under exponential censoring with binary treatments
- On the SANAD epilepsy dataset, CA-TRL achieved a first-stage correct decision rate (CDR1) of 93.17% versus ASCL's 70.80%
- CA-TRL demonstrated double robustness, maintaining performance when either propensity or outcome models were misspecified but not both
- The method successfully handles up to 62% censoring rates in simulated settings while maintaining superior performance over baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CA-TRL provides unbiased estimation of counterfactual outcomes even when survival times are right-censored.
- **Mechanism:** The modified AIPW estimator incorporates censoring survival probability Ŝ_C(T|H_K) to weight observed outcomes, dividing by this probability to account for subjects whose outcomes are unobserved. The estimator combines an inverse-probability-weighted observed outcome with a model-based conditional mean, ensuring consistency if either component is correctly specified.
- **Core assumption:** Censoring is conditionally independent of the event time given observed history (non-informative censoring given covariates).
- **Evidence anchors:**
  - [abstract]: "By enhancing traditional tree-based reinforcement learning methods with augmented inverse probability weighting (AIPW) and censoring-aware modifications, CA-TRL delivers robust and interpretable treatment strategies."
  - [Methods section]: "This estimator exhibits double robustness, meaning it remains consistent if either the propensity model or the conditional mean model is correctly specified. The censoring survival probability in the first term adjusts for the incomplete nature of the observed outcomes T, ensuring unbiased estimation."
  - [corpus]: Related paper "Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley James Boosting" addresses similar censoring challenges using alternative accelerated failure time approaches.

### Mechanism 2
- **Claim:** Recursive pseudo-outcome propagation enables optimal sequential decision-making by incorporating future expected outcomes into current-stage decisions.
- **Mechanism:** Starting from the final stage K (where R̃_K = T), the algorithm computes pseudo-outcomes R̃_k for earlier stages that combine: (1) observed stage outcomes plus expected future value under optimal treatment, minus expected value under actual treatment (for uncensored individuals), or (2) expected future value under optimal treatment (for censored individuals). This backward induction aligns with Bellman's optimality principle.
- **Core assumption:** Sequential randomization—no unmeasured confounding at each stage conditional on observed history.
- **Evidence anchors:**
  - [Methods section]: "The pseudo-outcome R̃_k at stage k is defined as: R̃_k = δ_{k+1}(R̃_{k+1} + μ̂_{k+1,g^{opt}_{k+1}}(H_{k+1}) - μ̂_{k+1,A_{k+1}}(H_{k+1})) + (1-δ_{k+1})μ̂_{k+1,g^{opt}_{k+1}}(H_{k+1})"
  - [Results section, Table 2]: CA-TRL achieved ACDR (average correct decision rate across stages) of 89.13% vs. ASCL's 52.09% under exponential censoring with binary treatments, demonstrating effective multi-stage optimization.
  - [corpus]: "A Graphical Approach to State Variable Selection in Off-policy Learning" addresses related challenges in sequential off-policy learning but uses different identification strategies.

### Mechanism 3
- **Claim:** Tree-based partitioning creates interpretable, clinically actionable treatment rules by recursively splitting the covariate space to maximize counterfactual outcome differences.
- **Mechanism:** At each node, CA-TRL evaluates candidate splits by computing the improvement in counterfactual purity—the weighted sum of estimated counterfactual mean outcomes in child nodes minus the parent node. Unlike standard CART which uses observed outcomes, CA-TRL optimizes for counterfactual means under optimal treatment assignment within each partition.
- **Core assumption:** Treatment effect heterogeneity can be captured through recursive binary partitions of the covariate space.
- **Evidence anchors:**
  - [Methods section]: "At each stage of the tree-growing process, CA-TRL evaluates possible splits by calculating the counterfactual mean outcomes for the resulting child nodes. The purity improvement from a split is determined by comparing the counterfactual outcomes of the child nodes to the parent node."
  - [Results section]: "CA-TRL achieved a CDR1 of 93.17 ± 5.90... significantly higher than those obtained by ASCL, which recorded 70.80 ± 9.88" demonstrating superior first-stage decision accuracy.

## Foundational Learning

- **Concept: Right Censoring in Survival Data**
  - **Why needed here:** CA-TRL's core contribution is handling situations where exact event times are unknown—we only know the event occurs after some observed time. Without understanding censoring mechanisms, you cannot interpret why standard approaches fail or why AIPW adjustments are necessary.
  - **Quick check question:** A patient in a study was alive at their last follow-up 18 months ago but has not returned. Is their survival time (a) 18 months, (b) greater than 18 months, or (c) unknown? If you answered (a), you need to review censoring fundamentals.

- **Concept: Potential Outcomes and Counterfactual Reasoning**
  - **Why needed here:** DTRs require estimating what would have happened under treatments not actually received. The paper's entire estimation framework relies on the counterfactual framework and its identifying assumptions (positivity, consistency, no unmeasured confounding).
  - **Quick check question:** Under the consistency assumption, if patient X received treatment A and survived 24 months, what do we know about their counterfactual survival time under treatment A?

- **Concept: Inverse Probability Weighting and Double Robustness**
  - **Why needed here:** CA-TRL's estimator combines IPW with outcome regression to achieve double robustness. Understanding how IPW creates a pseudo-population where treatment is unconfounded, and why combining it with outcome models provides protection against misspecification, is essential for debugging and extending the method.
  - **Quick check question:** If the propensity model is wrong but the outcome model is correct, will a standard IPW estimator be consistent? Will an AIPW estimator be consistent?

## Architecture Onboarding

- **Component map:**
  Input Data (H_k, A_k, R_k, δ_k) -> Propensity Score Estimator (multinomial logistic regression) -> π̂_{a_k}(H_k)
  -> Censoring Survival Estimator (Random Survival Forests) -> Ŝ_C(R_k|H_k)
  -> Conditional Mean Estimator (Random Survival Forests) -> μ̂_{k,a_k}(H_k)
  -> CA-TRL Estimator (modified AIPW) -> μ̂^{CAIPW}_{k,a_k}(H_k)
  -> Tree Builder (counterfactual purity criterion) -> g^{opt}_k(H_k)
  -> Pseudo-Outcome Calculator -> R̃_{k-1}
  [Repeat backward for k = K-1,...,1]
  -> Output: Complete DTR {g^{opt}_1, ..., g^{opt}_K} as interpretable trees

- **Critical path:**
  1. Initialize pseudo-outcome at final stage: R̃_K = T (observed total survival time)
  2. For each stage k from K down to 1:
     - Fit propensity model π̂_{a_k}(H_k) using multinomial logistic regression
     - Fit censoring survival model Ŝ_C(R_k|H_k) using Random Survival Forests
     - Fit conditional mean model μ̂_{k,a_k}(H_k) using Random Survival Forests
     - Compute CA-TRL estimator for each treatment option
     - Build decision tree using counterfactual purity splits
     - Extract optimal rule g^{opt}_k
     - Compute pseudo-outcome R̃_{k-1} for previous stage
  3. Output stage-specific treatment rules as decision trees

- **Design tradeoffs:**
  - **Tree depth vs. interpretability:** Deeper trees capture more complex treatment interactions but reduce clinical interpretability—the paper uses stopping rules (minimum node size n_0, purity threshold λ, max depth) to balance this
  - **Double robustness vs. estimation complexity:** AIPW requires fitting three nuisance models (propensity, censoring survival, conditional mean) per stage; computational cost scales with number of stages and treatment options
  - **Random Survival Forests vs. parametric alternatives:** RSF provides flexibility for censoring survival estimation but lacks closed-form inference; parametric Cox models would be faster but impose proportional hazards assumption
  - **Greedy vs. lookahead splitting:** Current implementation uses greedy splits for computational efficiency; paper acknowledges this may miss globally optimal partitions

- **Failure signatures:**
  - **Positivity violations:** Estimated propensity scores near zero cause extreme IPW weights, inflating variance; manifests as unstable or divergent estimates in sparse covariate regions
  - **High censoring rates (>70%):** Sparse event information makes survival probability estimates unreliable near distribution tails; manifests as erratic pseudo-outcomes in later stages
  - **Propensity and outcome model both misspecified:** Double robustness provides no protection; manifests as biased estimates that don't improve with sample size
  - **Informative censoring:** If censoring depends on unmeasured factors correlated with outcomes, the survival probability adjustment will be biased and estimates will not converge to true counterfactual means
  - **Overfitting in nuisance models:** Random Survival Forests may overfit with limited events; manifests as poor cross-validation performance despite high training metrics

- **First 3 experiments:**
  1. **Semi-synthetic validation with controlled censoring:** Replicate the paper's MIMIC-IV experiment. Generate synthetic treatment assignments and outcomes using the paper's specified functions. Vary censoring rates (30%, 50%, 70%) and mechanisms (exponential, conditional, uniform censoring mechanisms at ~62% rate). Compare CA-TRL vs. random assignment vs. ASCL on τ-RMST, CDR1, and ACDR. **Success criterion:** CA-TRL achieves ≥10% higher ACDR than ASCL across all censoring scenarios.
  
  2. **Double robustness stress test:** Systematically misspecify either the propensity model (omit key covariates) or the conditional mean model (use wrong functional form), then misspecify both. Measure bias in estimated value function V(g^{opt}). **Success criterion:** Bias remains bounded when only one model is misspecified; bias is substantially larger when both are misspecified.
  
  3. **SANAD dataset reproduction with ablation:** Apply CA-TRL to the epilepsy dataset using the paper's preprocessing. Run ablations: (a) without censoring adjustment (set Ŝ_C = 1), (b) without AIPW (use outcome regression only), (c) full CA-TRL. Compare RMST across ablations. **Success criterion:** Full CA-TRL achieves RMST ≥1700 days; censoring-aware version outperforms non-censoring-aware by statistically significant margin (p < 0.05).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the CA-TRL framework be adapted to handle continuous treatment variables, such as precise drug dosages, rather than discrete treatment options?
- **Basis in paper:** [explicit] The authors state the framework "currently supports discrete treatment options," limiting use in scenarios like "drug dosage optimization [which] involve continuous treatment variables."
- **Why unresolved:** The current tree-based structure and splitting criteria are designed for categorical treatment assignments.
- **What evidence would resolve it:** A modified CA-TRL implementation successfully optimizing dosages on a dataset with continuous treatment variables, outperforming discretized baselines.

### Open Question 2
- **Question:** Would incorporating a "lookahead" mechanism improve the global optimality of the decision rules compared to the current greedy splitting approach?
- **Basis in paper:** [explicit] The authors note the method "employs a greedy approach... which may not always lead to the global optimum," and suggest incorporating a lookahead mechanism to evaluate future splits.
- **Why unresolved:** Greedy splits maximize immediate purity improvement without considering the holistic impact on downstream nodes.
- **What evidence would resolve it:** Simulations demonstrating that a lookahead-enhanced CA-TRL achieves higher Restricted Mean Survival Time (RMST) than the greedy version.

### Open Question 3
- **Question:** Can the CA-TRL method maintain superior performance when applied to diverse clinical domains with different censoring patterns, such as cardiovascular diseases or diabetes management?
- **Basis in paper:** [explicit] The authors state that "additional validation across diverse clinical conditions is necessary to generalize its applicability" beyond the tested epilepsy and semi-synthetic CAD datasets.
- **Why unresolved:** The method has primarily been validated on the SANAD epilepsy dataset and semi-synthetic data; its robustness across varying clinical complexities is unproven.
- **What evidence would resolve it:** Successful application and consistent outperformance of baselines on real-world datasets from distinct medical domains like diabetes or cardiovascular disease.

## Limitations

- The method requires correct specification of nuisance models (propensity, censoring survival, conditional mean) and assumes non-informative censoring for theoretical guarantees
- Greedy tree-splitting may miss globally optimal partitions, particularly in high-dimensional covariate spaces
- Computational scalability to longer treatment sequences (>2 stages) or many treatment options remains unclear
- Performance in highly sparse event settings (censoring >70%) may degrade significantly

## Confidence

- **High confidence:** The mechanism of censoring-aware AIPW estimation and its double-robustness properties
- **Medium confidence:** The practical performance gains over ASCL given limited real-world validation (one dataset)
- **Low confidence:** Generalizability to different clinical domains or treatment paradigms without further testing

## Next Checks

1. Implement sensitivity analysis varying the degree of informative censoring to assess robustness of CA-TRL estimates
2. Test CA-TRL on additional real-world datasets with different censoring patterns and treatment structures to evaluate generalizability
3. Compare computational runtime and memory usage of CA-TRL versus parametric alternatives (e.g., Cox-based DTRs) for multi-stage problems