---
ver: rpa2
title: 'BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation
  and Binary Codebook'
arxiv_id: '2506.12040'
source_url: https://arxiv.org/abs/2506.12040
tags:
- binary
- btc-llm
- quantization
- codebook
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BTC-LLM introduces a sub-1-bit quantization framework for LLMs
  using learnable weight transformations and binary codebook compression. The method
  employs invertible scaling and rotation matrices to align binarized weights with
  full-precision distributions, suppressing activation outliers, and uses a binary
  codebook to compress recurring weight patterns into compact indices.
---

# BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation and Binary Codebook

## Quick Facts
- **arXiv ID:** 2506.12040
- **Source URL:** https://arxiv.org/abs/2506.12040
- **Reference count:** 40
- **Primary result:** Achieves 6.06 perplexity at 1.11 bits and 6.60 perplexity at 0.8 bits with up to 7.9× memory reduction compared to FP16.

## Executive Summary
BTC-LLM introduces a sub-1-bit quantization framework for LLMs that combines learnable weight transformations with binary codebook compression. The method uses invertible scaling and rotation matrices to align binarized weights with full-precision distributions, suppressing activation outliers that typically degrade binary quantization performance. It then identifies recurring binary vector patterns and compresses them into compact codebook indices, achieving compression below 1 bit per weight while maintaining competitive perplexity and zero-shot accuracy. Experiments demonstrate superior performance compared to state-of-the-art sub-1-bit quantization methods across LLaMA and Qwen model families.

## Method Summary
BTC-LLM employs a post-training quantization approach that first optimizes learnable scaling (Λ) and rotation (R) matrices to minimize the L2 reconstruction loss between full-precision and binarized outputs, effectively suppressing activation outliers. The transformed weights are then binarized using ARB quantization and reshaped into vectors of length v. A binary-optimized K-Means algorithm clusters these binary vectors using Hamming distance (via XOR/POPCNT) and updates centroids via sign-based majority voting, creating a compact codebook. During inference, the transformation is fused into the weight matrix, eliminating computational overhead, while the codebook enables efficient sub-1-bit reconstruction through standard lookup operations.

## Key Results
- Achieves 6.06 perplexity at 1.11 bits and 6.60 perplexity at 0.8 bits on WikiText2
- Outperforms state-of-the-art sub-1-bit methods by up to 5.0% in zero-shot accuracy across 7 tasks
- Provides up to 7.9× memory reduction compared to FP16 while maintaining model quality
- Eliminates sparse mask overhead through binary codebook compression

## Why This Works (Mechanism)

### Mechanism 1: Outlier Suppression via Learnable Incoherence Processing
The paper identifies activation outliers as the primary source of error in binarized LLMs. By optimizing learnable scaling and rotation matrices to minimize reconstruction loss, the transformation redistributes activation and weight channels, spreading outliers across dimensions rather than allowing any single activation to dominate. This prevents extreme values from corrupting the binary approximation.

### Mechanism 2: Sub-1-Bit Compression via Binary Vector Clustering
Instead of pruning with sparse masks (which requires storing indices), BTC-LLM clusters binary vectors using binary-optimized K-Means. Distance is calculated via Hamming distance (XOR/POPCNT) and centroids are updated via majority vote. This captures local structural patterns in binary weights while avoiding hardware-inefficient sparse mask overhead.

### Mechanism 3: Hardware-Efficient Inference Fusion
The learnable transformation T=ΛR is mathematically merged into the weight matrix before binarization and codebook compression. This ensures no additional computational overhead during inference—the model performs only standard lookup operations from the binary codebook, avoiding the overhead of managing N:M sparse matrices used in baseline methods.

## Foundational Learning

- **Incoherence Processing (Randomized/Orthogonal Rotations):** Needed to understand why learnable transformations generalize fixed Hadamard rotations used in methods like QuIP or QuaRot. Quick check: Why does applying a rotation matrix before quantization help reduce error from outlier channels?

- **Vector Quantization (VQ) vs Scalar Quantization:** Needed to understand how binary codebook clustering differs from treating each weight independently. Quick check: How does the compression ratio formula 16·v/⌈log₂c⌉ change if you double the vector length v?

- **Straight-Through Estimator (STE):** Needed to understand how gradients flow back to transformation parameters despite binarization being non-differentiable. Quick check: How can you calculate the gradient of a function that outputs discrete values (±1)?

## Architecture Onboarding

- **Component map:** Input FP16 Weight Matrix W → Learnable Transform (optimizes Λ, R) → Binarizer (ARB) → Binary Codebook (clusters → indices) → Compressed Indexes + Codebook + Scales

- **Critical path:** The optimization loop for learnable transformation is most sensitive. If rotation R is not initialized or optimized correctly using Cayley SGD to preserve orthogonality, subsequent binarization error becomes irrecoverable.

- **Design tradeoffs:** Larger vector length v allows better compression ratios but may increase reconstruction error if weights aren't locally redundant. Larger codebook size c improves accuracy but increases storage and lookup latency. The method trades sparse mask overhead for codebook lookup efficiency.

- **Failure signatures:** Exploding perplexity (>100) at 0.7-0.8 bits indicates vector length is too small or codebook capacity is insufficient. High codebook reconstruction error (>10% vector mismatch) suggests issues with Hamming distance implementation or centroid updates. No memory reduction indicates transformations weren't properly fused.

- **First 3 experiments:**
  1. Ablation on Rotation: Compare fixed Hadamard rotation vs Learnable Transformation on LLaMA-2-7B (Table 4b) to validate the core learnable contribution.
  2. Vector Length Sensitivity: Run BTC-LLM on WikiText2 with vector lengths [4,8,10,12,14,16,18,20] (Table 4a) to find the accuracy-efficiency knee point.
  3. Activation Outlier Visualization: Reproduce Figure 2—plot activation distributions before and after transformation to verify outlier suppression is occurring.

## Open Questions the Paper Calls Out

- **KV Cache Compression:** The current approach doesn't address KV cache compression for long-context inference. Future work should integrate binary compression with efficient KV cache management to demonstrate maintained accuracy and reduced memory usage on long-context benchmarks.

- **Weight-Activation Bit Pairing:** The paper hasn't fully explored theoretical foundations for optimal pairing of weight and activation bit-widths (e.g., W0.8A8). A theoretical study deriving optimal activation precision for given sub-1-bit weight configurations, supported by empirical validation, would be valuable.

- **Reasoning Task Sensitivity:** Reasoning tasks show higher sensitivity to aggressive bit-width reduction than knowledge-retrieval tasks. An analysis of layer-wise error propagation in reasoning versus retrieval tasks, followed by a strategy to improve reasoning scores at low bit-widths, remains unexplored.

## Limitations

- The method doesn't address KV cache compression for long-context inference, limiting its applicability to memory-intensive inference scenarios.
- The theoretical foundations for optimal pairing of weight and activation bit-widths remain unexplored, leaving empirical tuning as the primary approach.
- Reasoning tasks show higher sensitivity to aggressive bit-width reduction, suggesting the method may be less suitable for reasoning-heavy applications without additional tuning.

## Confidence

**High Confidence:**
- Learnable transformation mechanism for outlier suppression is well-supported by abstract and section 3.1 description with clear visualization evidence.
- Binary codebook compression approach using Hamming distance and sign-based centroid updates is technically sound and matches standard VQ practices.
- Memory reduction claims (7.9× at 0.7 bits) are consistent with described compression methodology.

**Medium Confidence:**
- Zero-shot accuracy improvements (+5.0% over baselines) depend on specific baseline implementations and may vary with different evaluation protocols.
- "No computational overhead" claim assumes optimal hardware support for codebook lookups and may not hold across all inference platforms.
- Specific hyperparameter choices (vector lengths, codebook sizes, optimization parameters) are not fully specified and may require empirical tuning.

**Low Confidence:**
- Latency and throughput performance claims lack direct experimental validation in provided corpus.
- Generalizability to model architectures beyond LLaMA and Qwen families remains untested.
- Method's behavior on tasks with different activation distributions is not evaluated.

## Next Checks

1. **Transformation Convergence Validation:** Implement learnable transformation with Cayley SGD and verify activation outliers are suppressed. Track maximum absolute activation values before and after transformation (should drop from ~15 to ~0.4 as shown in Figure 2).

2. **Codebook Size Sensitivity Analysis:** Systematically vary codebook size K and vector length v across ranges to identify optimal tradeoff curve between compression ratio and perplexity. Document the "knee point" where additional compression degrades accuracy.

3. **Hardware Profiling:** Measure actual inference latency and memory bandwidth usage on representative hardware (CPU with POPCNT support and GPU) to validate claimed computational efficiency gains over sparse mask approaches.