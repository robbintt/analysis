---
ver: rpa2
title: Evaluating Large Language Models on Multiword Expressions in Multilingual and
  Code-Switched Contexts
arxiv_id: '2504.20051'
source_url: https://arxiv.org/abs/2504.20051
tags:
- mwes
- language
- experiments
- score
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how large language models (LLMs) handle multiword
  expressions (MWEs) in multilingual and code-switched contexts. MWEs, which have
  non-compositional meanings and syntactic irregularities, pose challenges for NLP
  systems.
---

# Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts

## Quick Facts
- **arXiv ID**: 2504.20051
- **Source URL**: https://arxiv.org/abs/2504.20051
- **Reference count**: 25
- **Key outcome**: LLMs perform reasonably well on MWE detection in English but struggle with lower-resource languages (Portuguese, Galician) and semantic understanding tasks, with code-switched data improving detection but not semantics.

## Executive Summary
This study evaluates how large language models (LLMs) handle multiword expressions (MWEs) in multilingual and code-switched contexts. MWEs, which have non-compositional meanings and syntactic irregularities, pose challenges for NLP systems. The study assesses whether LLMs can detect and interpret MWEs, especially when used idiomatically versus literally. Experiments were conducted using datasets in English, Portuguese, Galician, and synthetic code-switched data mixing Spanish with these languages. Models evaluated include GPT-4, GPT-3.5, and Llama 3 variants. The results show that while LLMs perform reasonably well in English, they struggle with lower-resource languages like Portuguese and Galician. Additionally, LLMs fail to outperform smaller fine-tuned models in tasks involving nuanced language interpretation, particularly in semantic tasks and synthetic MWE scenarios. Code-switched data improved detection performance but did not enhance semantic understanding. The findings indicate that LLMs, despite their strengths, continue to face significant challenges in processing MWEs, highlighting the need for improved approaches to handle complex linguistic phenomena.

## Method Summary
The study evaluates LLMs on three tasks using the SemEval 2022 Task 2 dataset (English, Portuguese, Galician) and synthetic code-switched variants. Models tested include GPT-3.5, GPT-4, and Llama-3 (8B/70B) in zero-shot and few-shot settings. The detection task classifies MWEs as idiomatic or literal, while the semantic task determines meaning similarity between sentences. Synthetic code-switched data was generated using GPT-4-turbo, and novel synthetic MWEs were created with definitions provided in prompts. Performance was measured using F1 scores and compared against a fine-tuned xlm-roBERTa-base baseline. The study reveals that while LLMs handle English MWEs reasonably well, they struggle with lower-resource languages and semantic understanding, particularly in code-switched contexts.

## Key Results
- LLMs perform significantly worse on Portuguese and Galician compared to English (0.50-0.57 vs 0.70+ F1)
- Code-switching improves MWE detection but harms semantic understanding performance
- LLMs fail to outperform smaller fine-tuned models on semantic MWE tasks
- Synthetic MWE interpretation results are near-random, suggesting memorization over compositional understanding

## Why This Works (Mechanism)
LLMs process MWEs through pattern recognition from pre-training data, but struggle with non-compositional meanings and syntactic irregularities. Code-switching provides additional linguistic cues that help detection through familiar patterns, but these same cues can interfere with semantic interpretation by introducing ambiguity. The performance degradation in lower-resource languages reflects pre-training data imbalances, while synthetic MWE failures suggest models rely on memorization rather than true semantic understanding of novel expressions.

## Foundational Learning
- **Multiword Expressions (MWEs)**: Phrases with non-compositional meanings requiring special handling beyond standard tokenization. *Why needed*: Core task target; understanding how models handle them reveals linguistic generalization capabilities.
- **Code-switching**: Alternating between languages within discourse, creating syntactic and semantic complexity. *Why needed*: Tests model robustness across linguistic boundaries and reveals cross-lingual transfer patterns.
- **Zero-shot vs. Few-shot Learning**: Prompting strategies that vary in example provision to evaluate model generalization. *Why needed*: Determines whether performance gains come from true understanding or pattern matching from examples.
- **F1 Score**: Harmonic mean of precision and recall, critical for imbalanced classification tasks. *Why needed*: Standard metric for comparing model performance across tasks with different baseline difficulties.
- **Semantic Similarity vs. Detection**: Distinct tasks requiring different cognitive mechanisms - recognition versus meaning representation. *Why needed*: Reveals whether models can distinguish form from function in linguistic phenomena.
- **Synthetic Data Generation**: Using LLMs to create task-specific data when natural examples are scarce. *Why needed*: Enables controlled experiments but introduces potential generation artifacts that must be understood.

## Architecture Onboarding

**Component map:**
SemEval 2022 Task 2 dataset (EN/PT/GL) → code-switching synthesizer (GPT-4-turbo, temp=0.65) → MWE extraction module → Task adapters (Detection, Semantic, Synthetic MWE) → Model families (GPT-3.5/GPT-4, Llama-3 8B/70B, xlm-roBERTa-base) → Output evaluation

**Critical path:**
1. Load SemEval test data (labels unavailable, reducing memorization risk)
2. Generate CS variants via GPT-4-turbo with language-pair-specific prompts
3. Run detection task in zero-shot → few-shot (5 examples: 3 EN, 2 PT)
4. Run semantic task on dev set (521 EN, 454 PT examples with binary labels)
5. Run synthetic MWE task with definitions in prompt
6. Compare all LLM results against fine-tuned xlm-roBERTa baseline

**Design tradeoffs:**
- **Zero-shot vs. few-shot**: Few-shot helps semantic task but confuses Llama 8B (high variance, invalid outputs); zero-shot more stable for smaller models
- **Prompt diversity**: Three prompt types reveal sensitivity—Llama 8B shows ±0.397 variance across prompts in few-shot detection
- **Synthetic CS data**: Reduces memorization but may not reflect natural code-switching patterns; Spanish inclusion boosts detection F1 (familiar pre-training language) but harms semantic performance
- **Model scale**: 8B competitive for EN detection, but 70B/GPT-4 needed for semantic understanding and CS handling

**Failure signatures:**
- **Llama 8B few-shot collapse**: Produces invalid outputs (multi-word instead of single-word responses) when combining few-shot examples with input text—indicates context window confusion
- **Synthetic MWE near-random performance**: GPT-3 achieves 0.5444 F1 vs. 0.5025 random baseline on semantic task—definition-in-prompt insufficient for novel vocabulary
- **Language-specific degradation**: PT/GL F1 scores 0.10–0.20 lower than EN across all models—pre-training data imbalance propagates to downstream tasks
- **CS helps detection, hurts semantics**: GPT-4 drops from 0.7628 → 0.5455 F1 on EN semantic task with CS—detection and semantic understanding require different generalization mechanisms

**First 3 experiments:**
1. **Establish baseline parity**: Run xlm-roBERTa fine-tuning on your target language pair to confirm baseline F1 matches paper (EN: 0.707, PT: 0.680, GL: 0.507 for detection)
2. **Prompt sensitivity audit**: Test your target LLM with all three prompt types on 100 EN examples—if variance exceeds ±0.05, prompt engineering is your bottleneck before model selection
3. **Synthetic MWE probe**: Replace 50 MWEs with synthetic equivalents + definitions; if F1 < 0.55, your model relies on memorization rather than compositional understanding (assumption: your use case requires handling novel expressions)

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** What specific methods or architectures can improve LLMs' ability to capture the semantics of nuanced language, such as MWEs, beyond current fine-tuning approaches?
- **Basis in paper:** [explicit] The conclusion states that "New approaches are required to enable models to effectively address challenging linguistic phenomena, including MWE and CS."
- **Why unresolved:** The study demonstrates that current LLMs fail to match smaller fine-tuned models on semantic tasks, but does not propose a solution to this architectural limitation.
- **What evidence would resolve it:** A study introducing a novel training objective or architecture that allows LLMs to surpass xlm-roBERTa baselines on semantic MWE tasks.

**Open Question 2**
- **Question:** How does LLM performance on MWEs differ when using naturally occurring code-switched data versus the synthetic data generated for this study?
- **Basis in paper:** [explicit] The authors explicitly list this as a limitation: "In some experiments, we use synthetic CS data instead of naturally occurring CS data."
- **Why unresolved:** The synthetic data was generated by GPT-4, which may lack the natural syntactic irregularities or patterns found in human code-switching.
- **What evidence would resolve it:** A comparative evaluation using a dataset of naturally occurring code-switched examples (e.g., from multilingual social media) rather than synthetic mixtures.

**Open Question 3**
- **Question:** Why does code-switched text improve performance on metalinguistic detection tasks while simultaneously hindering the models' ability to represent semantic meaning?
- **Basis in paper:** [inferred] The results show that while code-switching improved detection F1 scores, it caused performance to drop in semantic similarity tasks compared to monolingual inputs.
- **Why unresolved:** The paper notes this divergence but does not investigate the underlying mechanisms causing the split between recognizing an MWE and understanding its meaning in mixed-language contexts.
- **What evidence would resolve it:** Probing studies or ablation experiments that analyze the internal token representations of MWEs in code-switched contexts versus monolingual ones.

## Limitations
- Synthetic code-switched data may not accurately reflect natural code-switching patterns, limiting generalizability
- Heavy reliance on single SemEval dataset with unavailable test labels raises evaluation concerns
- Few-shot learning shows high variance, particularly for smaller models, suggesting prompt sensitivity over capability differences
- Semantic task failures (near-random performance on synthetic MWEs) may indicate fundamental LLM limitations in processing nuanced semantic relationships

## Confidence

**High Confidence**: The core finding that LLMs perform significantly worse on Portuguese and Galician compared to English (0.50-0.57 vs 0.70+ F1) is robust and well-supported by consistent results across multiple models and tasks. The observation that code-switching helps detection but hurts semantic understanding is also clearly demonstrated.

**Medium Confidence**: The claim that LLMs fail to outperform smaller fine-tuned models needs qualification - while true for the semantic task, the detection task shows competitive performance from larger LLMs. The synthetic MWE task results showing near-random performance suggest fundamental limitations, but the artificial nature of the task makes real-world implications uncertain.

**Low Confidence**: The generalizability of code-switched data results is questionable due to the synthetic generation process. The specific mechanisms causing semantic task failures are not fully explored, and it's unclear whether these represent fundamental LLM limitations or issues with prompt design.

## Next Checks

1. **Replication with Natural Code-Switched Data**: Test the same models and tasks on naturally occurring code-switched datasets to validate whether the synthetic data generation approach accurately captures real-world patterns and whether the detection/semantic performance trends hold.

2. **Cross-Lingual Semantic Understanding Probe**: Design a controlled experiment comparing LLM performance on semantic similarity tasks across multiple language pairs (EN-PT, EN-ES, PT-ES) to determine whether the semantic task failures stem from language-specific issues or fundamental MWE interpretation challenges.

3. **Fine-tuning Impact Assessment**: Fine-tune xlm-roBERTa or similar models on the SemEval dataset and evaluate on synthetic MWE tasks to determine whether the LLMs' poor performance on novel MWEs reflects a fundamental limitation or simply the absence of task-specific training.