---
ver: rpa2
title: 'Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length
  Penalty'
arxiv_id: '2506.10446'
source_url: https://arxiv.org/abs/2506.10446
tags:
- reasoning
- length
- arxiv
- reward
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve the efficiency of large
  language models' reasoning by adapting the length of their responses based on question
  difficulty. The core idea is to use a reinforcement learning approach with a novel
  length penalty that encourages concise answers for simple questions while allowing
  longer reasoning for complex ones.
---

# Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty
## Quick Facts
- arXiv ID: 2506.10446
- Source URL: https://arxiv.org/abs/2506.10446
- Reference count: 7
- The paper proposes a method to improve LLM reasoning efficiency by adapting response length based on question difficulty.

## Executive Summary
This paper introduces a reinforcement learning method to improve the efficiency of large language models' reasoning by dynamically adjusting the length of responses based on question difficulty. The core innovation is a powered length penalty that scales differently for simple versus hard problems, encouraging concise answers for easy questions while allowing longer reasoning for complex ones. The approach was evaluated on three datasets using 1.5B and 7B parameter models, demonstrating significant token reduction on simpler tasks with minimal accuracy loss and improved performance on challenging problems.

## Method Summary
The method employs reinforcement learning with a modified reward function that incorporates a powered length penalty. This penalty adapts based on estimated question difficulty, computed via a value network. The reward function encourages concise answers for simple problems while permitting longer, more detailed reasoning for complex ones. The approach was tested on GSM8K (simple math), MATH500 (moderate difficulty), and AIME2024 (challenging competition math), comparing 1.5B and 7B parameter models against baselines using standard length penalties.

## Key Results
- On GSM8K, the 1.5B model reduced tokens by 40% and increased accuracy by 10%
- The 7B model achieved up to 90% token reduction on GSM8K with minimal accuracy loss
- On AIME2024, the approach improved accuracy while maintaining efficiency gains

## Why This Works (Mechanism)
The powered length penalty mechanism works by creating a non-linear relationship between response length and reward that varies with problem difficulty. For simple questions, the penalty grows rapidly with length, strongly discouraging verbosity. For complex questions, the penalty grows more slowly, allowing extended reasoning when needed. This adaptive approach optimizes the trade-off between efficiency and accuracy by recognizing that different problems require different amounts of computational reasoning effort.

## Foundational Learning
- **Reinforcement Learning**: The framework for training models to maximize cumulative reward through trial and error. Needed because it allows optimization of both accuracy and efficiency simultaneously. Quick check: Verify the reward signal properly balances accuracy and length.
- **Length Penalty**: A mechanism to discourage overly verbose outputs in sequence generation. Needed to control computational cost and improve efficiency. Quick check: Confirm the penalty doesn't overly constrain valid reasoning.
- **Value Network**: A neural network that estimates the expected reward for different actions or states. Needed to estimate problem difficulty for adaptive length control. Quick check: Validate the value network accurately distinguishes problem difficulty levels.

## Architecture On