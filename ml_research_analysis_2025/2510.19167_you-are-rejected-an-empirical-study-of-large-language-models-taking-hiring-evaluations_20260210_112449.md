---
ver: rpa2
title: '"You Are Rejected!": An Empirical Study of Large Language Models Taking Hiring
  Evaluations'
arxiv_id: '2510.19167'
source_url: https://arxiv.org/abs/2510.19167
tags:
- llms
- good
- questions
- candidate
- hiring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  pass standardized hiring evaluations for software engineering roles. The authors
  test 12 state-of-the-art LLMs on a professional assessment questionnaire, comparing
  their responses against human reference answers.
---

# "You Are Rejected!": An Empirical Study of Large Language Models Taking Hiring Evaluations

## Quick Facts
- arXiv ID: 2510.19167
- Source URL: https://arxiv.org/abs/2510.19167
- Authors: Dingjie Fu; Dianxing Shi
- Reference count: 40
- All 12 tested LLMs fail to pass standardized hiring evaluations, with RMSE > 2.0 and weak-to-moderate correlation with human preferences

## Executive Summary
This paper investigates whether large language models can pass standardized hiring evaluations for software engineering roles. The authors test 12 state-of-the-art LLMs on a professional assessment questionnaire, comparing their responses against human reference answers. Results show significant discrepancies: all models exhibit RMSE values exceeding 2.0 (on a 1-9 scale), with DeepSeek-R1 performing best at RMSE=1.5870 and Pearson correlation coefficient γ=0.7809. Most models show only weak-to-moderate correlation with human preferences (γ values between 0.3-0.5), and display a strong tendency to select positive responses regardless of question type. When simulating HR roles, the models fail to generate reliable hiring recommendations, with only two models assigning "Strongly Recommend" ratings uniformly across all candidates.

## Method Summary
The study employs a two-stage experimental design: first, 12 LLMs answer a 216-question personality assessment as SDE candidates via sequential prompting (one question per API call), then 9 of these models simulate HR evaluators providing hiring recommendations. Responses are compared against company-provided reference answers using RMSE and Pearson correlation metrics. The evaluation distinguishes between positive questions (expecting high scores) and negative questions (expecting low scores), revealing systematic biases in how models handle question valence.

## Key Results
- All 12 models show RMSE > 2.0 (1-9 scale), indicating substantial deviation from human reference answers
- DeepSeek-R1 achieves best performance with RMSE=1.5870 and γ=0.7809, while most models show only weak-to-moderate correlation (γ=0.3-0.5)
- Models display strong tendency to select positive responses regardless of question type, averaging >7 on negative questions where reference expects <5
- HR simulation fails to discriminate between candidates, with GPT-4o and GPT-5-chat assigning "Strongly Recommend" to all candidates

## Why This Works (Mechanism)

### Mechanism 1: Response Valence Misalignment
- Models trained on web-scale corpora internalize patterns favoring agreeable, positive language, causing systematic failure to disagree with negative questions where strategic disagreement is required.

### Mechanism 2: Reasoning Capability Correlates with Behavioral Discernment
- Reasoning models (DeepSeek-R1, Gemini-2.5-pro) outperform standard models because chain-of-thought processing enables better comprehension of question intent and appropriate response calibration.

### Mechanism 3: Homogenized Training Produces Convergent Personality Profiles
- Training on overlapping web data causes models to internalize similar representations of "good employee" traits, leading to high cross-model correlation (>0.7) and limiting the value of ensemble approaches.

## Foundational Learning

- **Concept: Likert Scale Response Bias**
  - Why needed here: Understanding why models cluster around positive values (6-9) requires recognizing that personality questionnaires use 1-9 scales where "neutral" is 5, and that systematic deviation from this midpoint signals response style rather than content.
  - Quick check question: If a model averaged 7.5 across all questions regardless of type, would this indicate high trait presence or response style bias?

- **Concept: Question Valence (Positive vs. Negative Items)**
  - Why needed here: The paper's key finding is that models fail to distinguish between questions where agreement signals fit (positive items) vs. misfit (negative items). Understanding this design pattern is essential for interpreting why high scores aren't always desirable.
  - Quick check question: For "I like to argue with people" (reference=2), should a well-calibrated model score high or low?

- **Concept: Pearson Correlation vs. RMSE as Complementary Metrics**
  - Why needed here: The paper uses both metrics to capture different failure modes—RMSE measures absolute deviation from reference answers, while γ measures whether response patterns trend in the same direction.
  - Quick check question: If Model A has RMSE=2.0, γ=0.8 and Model B has RMSE=1.8, γ=0.3, which better captures human preference patterns?

## Architecture Onboarding

- **Component map:**
  ```
  Hiring Evaluation Pipeline:
  ├── Candidate Mode (12 LLMs tested)
  │   ├── Prompt: Role-play as SDE candidate with strategic guidelines
  │   ├── Input: Sequential questions (1-9 Likert scale)
  │   └── Output: 216 responses per model
  ├── Reference Ground Truth
  │   ├── Source: Company-provided "ideal answer" key
  │   └── Structure: Integer scores 1-9 for each question
  ├── Evaluation Layer
  │   ├── RMSE: Global deviation measure
  │   ├── Pearson γ: Pattern similarity
  │   └── P/N Analysis: Separate positive/negative question performance
  └── HR Simulation Mode (9 LLMs tested)
      ├── Prompt: Role-play as HR evaluator
      ├── Input: Completed questionnaires
      └── Output: Strongly Recommend / Recommend w/ Reservations / Not Recommended
  ```

- **Critical path:**
  1. Question categorization (positive/negative/self-descriptive) is prerequisite for meaningful error analysis—misclassify this and RMSE becomes uninterpretable
  2. Reference answer extraction from actual company evaluation materials (not researcher intuition) determines ground truth validity
  3. Sequential prompting (one question at a time) prevents cross-question contamination but may miss holistic response patterns

- **Design tradeoffs:**
  - Single evaluation vs. multiple temperature runs: Paper acknowledges this limitation—single runs can't distinguish systematic bias from random variance
  - One questionnaire vs. diverse instruments: Findings may not generalize to other hiring assessments (different constructs, scoring algorithms)
  - Candidate-perspective vs. HR-perspective testing: Paper tests both, but findings diverge—models fail as candidates (RMSE) AND as evaluators (discrimination failure)

- **Failure signatures:**
  - Uniform positive scoring: Models averaging >7 on negative questions (where reference expects <5)
  - Collapsing discrimination: HR mode assigning identical ratings to all candidates (GPT-4o, GPT-5-chat)
  - Outlier correlation: γ < 0.3 indicates fundamentally misaligned response logic (Qwen3-32B exception)
  - Scale reversal invariance: If reversing 1↔9 scale doesn't change response patterns, model isn't reading scale semantics

- **First 3 experiments:**
  1. Valence-aware prompting: Add explicit instruction identifying question type (positive/negative) before each item; measure RMSE reduction vs. baseline
  2. Temperature sweep: Run each model at 3+ temperature settings (0.0, 0.7, 1.0) to separate systematic bias from stochastic variance; report variance bounds
  3. Cross-questionnaire validation: Test top-performing model (DeepSeek-R1) on a second hiring instrument to assess whether performance is questionnaire-specific or generalizable

## Open Questions the Paper Calls Out

- **Generalizability of Findings:** Do LLMs exhibit similar discrepancies and acquiescence biases across a broader range of professional hiring questionnaires beyond software engineering?
- **Temperature Parameter Impact:** To what extent does the temperature parameter influence the stability and consistency of LLM-generated responses in standardized evaluations?
- **Reasoning Capabilities Isolation:** Does the integration of specific reasoning capabilities (e.g., Chain-of-Thought) directly improve an LLM's ability to discern the valence of negative survey items?

## Limitations

- Limited to a single questionnaire, preventing comprehensive exploration of generalizability
- Did not test multiple inference runs to measure temperature parameter impact on response stability
- Unclear API versions/release dates for tested models, which may affect reproducibility

## Confidence

- **Method reproducibility:** Medium - sequential prompting and evaluation metrics are clear, but temperature parameters and exact model versions are unspecified
- **Findings generalizability:** Low - single questionnaire limits external validity
- **Technical conclusions:** High - clear statistical evidence (RMSE, correlation) supports findings

## Next Checks

1. Validate model behavior by running DeepSeek-R1 and Qwen2.5-32B at multiple temperature settings to measure response variance
2. Implement valence-aware prompting on Qwen2.5-32B to test if RMSE improves with explicit question type instructions
3. Extract and verify question categorization (positive/negative) from Appendix A against model response patterns