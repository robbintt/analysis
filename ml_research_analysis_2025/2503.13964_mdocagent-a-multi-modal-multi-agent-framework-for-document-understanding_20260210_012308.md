---
ver: rpa2
title: 'MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding'
arxiv_id: '2503.13964'
source_url: https://arxiv.org/abs/2503.13964
tags:
- agent
- information
- text
- answer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MDocAgent introduces a multi-modal multi-agent framework for document\
  \ question answering that integrates both text and visual information through specialized\
  \ agents and dual RAG pipelines. The system employs five agents\u2014general, critical,\
  \ text, image, and summarizing\u2014to collaboratively process and synthesize information\
  \ from retrieved textual and visual contexts."
---

# MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding

## Quick Facts
- arXiv ID: 2503.13964
- Source URL: https://arxiv.org/abs/2503.13964
- Authors: Siwei Han; Peng Xia; Ruiyi Zhang; Tong Sun; Yun Li; Hongtu Zhu; Huaxiu Yao
- Reference count: 40
- Key outcome: MDocAgent achieves 12.1% average improvement over M3DocRAG across five benchmarks

## Executive Summary
MDocAgent introduces a multi-modal multi-agent framework for document question answering that integrates both text and visual information through specialized agents and dual RAG pipelines. The system employs five agents—general, critical, text, image, and summarizing—to collaboratively process and synthesize information from retrieved textual and visual contexts. Experimental results show MDocAgent achieves an average improvement of 12.1% compared to the state-of-the-art M3DocRAG method across five benchmarks, with particular gains in handling long documents and complex multi-modal reasoning tasks. Ablation studies confirm the effectiveness of each agent, demonstrating that the critical agent's information extraction and the specialized text/image agents' focused analysis are crucial for the framework's success.

## Method Summary
MDocAgent uses a dual RAG pipeline with ColBERTv2 for text retrieval and ColPali for visual retrieval, followed by a five-agent sequential pipeline. The general agent provides preliminary answers, the critical agent identifies key information, specialized text and image agents perform focused analysis, and the summarizing agent synthesizes the final answer. The framework processes documents through OCR and PDF parsing, preserving both text segments and page images. Agents are implemented using a combination of Llama-3.1-8B for text-specific tasks and Qwen2-VL-7B for vision-language tasks.

## Key Results
- MDocAgent achieves 12.1% average improvement over M3DocRAG across five benchmarks
- Ablation studies show critical agent and specialized agents are essential, with performance dropping 12.5% without image agent and 15.1% without text agent
- Top-4 retrieval (k=4) improves performance from 0.407 to 0.465 average score compared to top-1 retrieval

## Why This Works (Mechanism)

### Mechanism 1
Separating modality-specific processing into specialized agents improves retrieval-to-answer fidelity over monolithic LVLM approaches. Text and image agents process retrieved context exclusively within their modalities, guided by critical information, enabling deeper analysis before synthesis. The text agent receives $T_q$ (retrieved text segments) and $T_c$ (critical textual information); the image agent receives $I_q$ and $I_c$ analogously. This division reduces cross-modal interference during evidence extraction.

### Mechanism 2
The critical agent functions as an information router that focuses downstream agent attention, reducing irrelevant processing. The critical agent $A_C$ receives the question, retrieved contexts, and the general agent's preliminary answer, then outputs $T_c$ (critical text) and $I_c$ (critical visual description). These outputs explicitly guide specialized agents to "pinpoint the most relevant evidence" rather than processing all retrieved content uniformly.

### Mechanism 3
Dual RAG pipelines with late fusion enable retrieval of complementary evidence that single-modal RAG misses. ColBERTv2 retrieves top-k text segments $T_q$; ColPali retrieves top-k image pages $I_q$ using visual embeddings. These operate independently, and the framework does not fuse retrieval scores—fusion occurs at the agent synthesis stage. This allows retrieval of evidence unique to each modality.

## Foundational Learning

- Concept: **Late-interaction retrieval (ColBERT-style)**
  - Why needed here: ColBERTv2 uses token-level late interaction rather than single-vector embeddings, improving fine-grained text matching for document segments.
  - Quick check question: Given a query and document, can you explain how MaxSim operation aggregates token-level similarities into a relevance score?

- Concept: **Vision-language embeddings for document retrieval (ColPali)**
  - Why needed here: ColPali generates dense visual embeddings for page images without OCR, enabling retrieval based on visual layout and figures.
  - Quick check question: How does ColPali handle queries that reference text within images—does it rely on implicit OCR within the vision encoder?

- Concept: **Multi-agent orchestration patterns**
  - Why needed here: MDocAgent's sequential pipeline (general→critical→specialized→summarizing) differs from parallel or debate-style multi-agent frameworks.
  - Quick check question: What are the failure modes of sequential agent pipelines compared to parallel voting ensembles?

## Architecture Onboarding

- Component map:
  Document Pre-processor → Text RAG (ColBERTv2) → General Agent (Qwen2-VL-7B) → Critical Agent (Qwen2-VL-7B) → Text Agent (Llama-3.1-8B) & Image Agent (Qwen2-VL-7B) → Summarizing Agent (Qwen2-VL-7B) → Final Answer

- Critical path:
  1. Retrieval quality directly bounds system performance—poor $T_q$ or $I_q$ cannot be recovered by agents
  2. Critical agent output quality determines specialized agent focus—errors here propagate
  3. Summarizing agent must resolve conflicts; if $a_T$ and $a_I$ disagree, synthesis logic is the bottleneck

- Design tradeoffs:
  - Sequential vs. parallel agents: Sequential enables information flow (critical→specialized) but increases latency
  - Separate text model (Llama-3.1-8B) vs. unified Qwen2-VL: Text agent uses Llama for "specialized knowledge"—assumes text-only LLM outperforms LVLM on pure text tasks, but this adds model management complexity
  - Top-k=1 vs. Top-k=4: Table 1 shows Top-4 improves MDocAgent (0.407→0.465 avg) but may introduce noise for weaker synthesis

- Failure signatures:
  - Retrieved pages correct but answer wrong: Check critical agent extraction and summarizing agent conflict resolution
  - Text and image agents disagree: Examine $I_c$ quality—visual descriptions may be ambiguous
  - Performance degrades with longer documents: Verify retrieval coverage; ColPali may miss relevant pages in 50+ page documents

- First 3 experiments:
  1. **Retrieval ablation**: Manually inject ground-truth relevant pages into $T_q$ and $I_q$ to isolate agent performance from retrieval quality. Compare against baseline retrieval
  2. **Agent swap**: Replace Llama-3.1-8B text agent with Qwen2-VL to test if specialized text-only model is necessary or if unified LVLM suffices
  3. **Critical agent perturbation**: Provide randomized or empty $T_c, I_c$ to specialized agents to quantify critical agent contribution magnitude

## Open Questions the Paper Calls Out

### Open Question 1
How can dynamic or iterative inter-agent communication protocols (such as feedback loops or debates) improve the system's ability to resolve conflicting information compared to the current sequential pipeline? The paper states, "Future work will explore more advanced inter-agent communication..." (Page 8). The current architecture utilizes a strictly sequential flow where agents cannot iteratively query or correct one another, potentially limiting complex conflict resolution.

### Open Question 2
Does the integration of external knowledge sources into the MDocAgent framework improve performance on open-domain questions that require reasoning beyond the immediate document context? The authors explicitly list "...and the integration of external knowledge sources" as a direction for future work (Page 8). The current framework is constrained to the information retrieved strictly from the input document via RAG, preventing it from answering questions that require common-sense reasoning or up-to-date facts not present in the source text.

### Open Question 3
What is the trade-off between the accuracy gains of the multi-agent system and its computational efficiency (latency/cost) compared to single-model baselines? The paper focuses exclusively on accuracy improvements while requiring the execution of five distinct agents and dual RAG pipelines for every query (Page 3). While the method improves accuracy, the computational overhead of loading and inferencing with multiple models serially may hinder real-time application feasibility.

## Limitations
- Framework effectiveness heavily depends on retrieval quality and critical agent's ability to identify relevant evidence, with limited evidence on error propagation
- Claims about specialization advantages and critical agent routing mechanism lack external validation beyond ablation studies
- Use of separate text and vision models introduces architectural complexity that may not generalize across domains

## Confidence
- **High confidence**: Dual RAG pipeline design, sequential agent orchestration, and overall performance improvements over M3DocRAG baseline
- **Medium confidence**: Claims about specialization advantages and critical agent routing mechanism (supported by ablation but limited external validation)
- **Low confidence**: Assumptions about cross-modal retrieval coverage and conflict resolution in the summarizing agent

## Next Checks
1. **Retrieval quality isolation test**: Manually inject ground-truth relevant pages into retrieval results to measure pure agent performance versus retrieval-bounded performance
2. **Text agent necessity validation**: Replace Llama-3.1-8B text agent with Qwen2-VL to determine if specialized text-only processing is essential
3. **Critical agent perturbation analysis**: Systematically remove or randomize critical agent outputs to quantify its impact on specialized agent performance and overall framework accuracy