---
ver: rpa2
title: 'FIGROTD: A Friendly-to-Handle Dataset for Image Guided Retrieval with Optional
  Text'
arxiv_id: '2511.22247'
source_url: https://arxiv.org/abs/2511.22247
tags:
- image
- retrieval
- text
- vagfem
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses Image-Guided Retrieval with Optional Text (IGROT),
  a task that unifies visual retrieval and composed retrieval by supporting both image-only
  and image-plus-text queries. The main challenges are the lack of an accessible benchmark
  and methods that perform well across both subtasks.
---

# FIGROTD: A Friendly-to-Handle Dataset for Image Guided Retrieval with Optional Text

## Quick Facts
- arXiv ID: 2511.22247
- Source URL: https://arxiv.org/abs/2511.22247
- Reference count: 30
- Primary result: Introduces FIGROTD dataset (16,474 training, 1,262 test triplets) and VaGFeM masking to achieve 34.8 mAP@10 on CIRCO and 75.7 mAP@200 on Sketchy

## Executive Summary
FIGROTD addresses Image-Guided Retrieval with Optional Text (IGROT) by unifying visual retrieval and composed retrieval tasks. The paper introduces a lightweight, high-quality dataset with 16,474 training triplets and proposes the Variance Guided Feature Mask (VaGFeM) to enhance discriminative dimensions through variance-based masking. A dual-loss training objective combining InfoNCE and Triplet loss improves compositional reasoning. VaGFeM achieves competitive results on nine benchmarks despite using fewer training examples than previous methods.

## Method Summary
FIGROTD uses BLIP ViT-B encoders for images and text, followed by a 2-layer transformer fusion module that produces weighted combinations of visual and textual features. The VaGFeM mechanism computes batch-wise variance for each embedding dimension, masks the top-20% highest-variance dimensions, and applies them via residual connection. Training employs a dual-loss objective: InfoNCE contrastive loss with temperature τ=0.01 plus Triplet loss (margin α=0.3, weight λ=0.2) that treats the reference image as a hard negative. The dataset is constructed by filtering LAION pairs at CLIP similarity ≥ 0.9 and generating captions for CSTBIR using LLaVA.

## Key Results
- Achieves 34.8 mAP@10 on CIRCO benchmark for composed image retrieval
- Reaches 75.7 mAP@200 on Sketchy benchmark for sketch-based image retrieval
- Demonstrates competitive performance across nine benchmarks while training on only 16,474 triplets versus millions used by previous methods

## Why This Works (Mechanism)

### Mechanism 1: Variance-guided masking
The VaGFeM mechanism selectively enhances discriminative dimensions by computing batch-wise variance for each embedding dimension. The top-k highest-variance dimensions receive a binary mask and are scaled by sigmoid activation, then combined via residual connection. This approach assumes high variance correlates with informativeness across the batch, reducing redundancy without learned attention weights.

### Mechanism 2: Dual-loss design with reference-as-negative
The triplet loss with reference image as negative explicitly penalizes queries closer to the unmodified reference than to the modified target. This forces the model to encode textual modifications meaningfully and prevents over-reliance on visual similarity alone. Combined with InfoNCE, this improves compositional reasoning for CIR tasks.

### Mechanism 3: Quality-over-quantity data strategy
FIGROTD uses CLIP ViT-L filtering (similarity ≥ 0.9) to retain only well-aligned image-text pairs, plus LLaVA-generated captions. This stringent filtering reduces annotation noise and focuses learning on high-quality examples, allowing competitive performance with only 16K training triplets versus millions in other approaches.

## Foundational Learning

- **Contrastive learning objectives (InfoNCE)**: Essential for understanding how positive pairs are pulled together and negatives pushed apart. Quick check: In a batch of B samples, how many negative pairs does InfoNCE implicitly contrast against for each positive?
- **Feature-level fusion in multimodal retrieval**: Required to grasp how VaGFeM operates on concatenated image-text embeddings. Quick check: Why might simple concatenation of V and T fail to capture compositional semantics without additional processing?
- **Variance as a proxy for feature informativeness**: The core VaGFeM hypothesis relies on understanding when variance correlates with discriminative power versus noise. Quick check: If all samples in a batch are nearly identical, what happens to the variance mask M?

## Architecture Onboarding

- **Component map**: Image/text → BLIP encoders → transformer fusion → variance mask → L2-normalized query embedding → cosine similarity against target gallery → dual loss backprop
- **Critical path**: BLIP ViT-B encoders process images and text, transformer fusion produces H_V and H_T, weighted fusion creates U, variance mask M enhances top-20% dimensions, L2 normalization applied, cosine similarity computed against gallery, dual loss (InfoNCE + Triplet) drives training
- **Design tradeoffs**: Top-k=20% masks balance sparsity versus information retention; triplet loss weight λ=0.2 balances CIR gains against SBIR degradation; 2-epoch training assumes pretrained BLIP provides strong initialization
- **Failure signatures**: Collapsed embeddings from too aggressive masking or uninformative variance; SBIR underperformance with triplet loss due to semantic incorrectness of reference-as-negative; no improvement beyond 10K triplets indicating dataset saturation
- **First 3 experiments**: (1) Ablate k (top-% dimensions): sweep k ∈ {10%, 20%, 30%, 50%} on FIGROTD validation; (2) Triplet loss impact by task: train with/without triplet loss, evaluate separately on CIR vs SBIR; (3) Data efficiency curve: train on 1K, 2K, 5K, 10K, 16K triplets to verify peak at 10K

## Open Questions the Paper Calls Out

- **Synthetic bias analysis**: How does the lack of fully supervised annotation contribute to synthetic biases in FIGROTD, and how can these biases be quantitatively analyzed? The authors hypothesize synthetic biases from lack of supervision but leave detailed analysis for future work.
- **Unified loss design**: Can a unified training objective be developed that improves compositional reasoning for CIR without degrading SBIR performance? The current dual-loss presents a zero-sum trade-off where triplet loss gains for CIR cause SBIR degradation.
- **Category imbalance**: What specific data balancing or sampling strategies are required to solve the category imbalance problem in FIGROTD? The dataset shows uneven distribution across classes due to aggregation from varied sources.

## Limitations

- Efficiency claim lacks direct external validation against large-scale noisy datasets
- No systematic analysis of domain coverage or diversity in filtered training set
- Category imbalance remains unsolved, potentially affecting generalization
- Out-of-distribution generalization not evaluated

## Confidence

- Variance-guided masking effectiveness: High (controlled ablation, consistent gains)
- Triplet loss benefit for CIR: High (clear performance lift, acknowledged SBIR trade-off)
- Small-dataset efficiency claim: Medium (strong internal curves, weak external validation)
- Generalization to unseen domains: Low (no out-of-distribution tests reported)

## Next Checks

1. Run a diversity audit of the filtered training set (e.g., CLIP-based category coverage) to confirm that quality filtering does not induce severe domain collapse
2. Conduct an out-of-distribution evaluation on CIR benchmarks not used in training (e.g., unseen object categories) to test true generalization
3. Replicate the data-efficiency curve on an external dataset (e.g., using Laion-400M at varying subset sizes) to validate that gains persist outside the curated FIGROTD distribution