---
ver: rpa2
title: 'Anchor-MoE: A Mean-Anchored Mixture of Experts For Probabilistic Regression'
arxiv_id: '2508.16802'
source_url: https://arxiv.org/abs/2508.16802
tags:
- anchor
- mean
- experts
- bounded
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Anchor-MoE, a novel approach to probabilistic
  regression that combines a strong anchor predictor with a mixture-of-experts architecture
  to provide both accurate point estimates and well-calibrated uncertainty quantification.
  The method uses a tuned gradient-boosting model as an anchor mean, which is then
  projected into a latent space where a learnable metric-window kernel and soft router
  dispatch samples to specialized mixture-density-network experts.
---

# Anchor-MoE: A Mean-Anchored Mixture of Experts For Probabilistic Regression

## Quick Facts
- **arXiv ID**: 2508.16802
- **Source URL**: https://arxiv.org/abs/2508.16802
- **Authors**: Baozhuo Su; Zhengxian Qu
- **Reference count**: 23
- **Primary result**: Consistently matches or surpasses NGBoost baseline in both RMSE and NLL across standard UCI regression datasets, achieving new state-of-the-art results for probabilistic regression.

## Executive Summary
Anchor-MoE introduces a novel probabilistic regression framework that combines a strong anchor predictor with a mixture-of-experts architecture. The method uses a tuned gradient-boosting model as an anchor mean, which is then projected into a latent space where a learnable metric-window kernel and soft router dispatch samples to specialized mixture-density-network experts. These experts produce heteroscedastic corrections and predictive variances, and the overall predictive distribution is formed by aggregating expert outputs with learned mixture weights. Training minimizes negative log-likelihood, and a held-out calibration split fits a post-hoc linear map to improve point accuracy without leakage.

## Method Summary
The Anchor-MoE architecture consists of a pre-trained GBDT anchor predictor that provides a baseline mean estimate, followed by a latent projection layer and a mixture-of-experts layer. The router uses a metric-window kernel combined with soft top-k masking to dispatch samples to K experts. Each expert is a small MLP that outputs parameters for a Gaussian mixture (weights, means, and scales). The final prediction is the anchor mean plus a weighted sum of expert outputs. After training, a linear calibration map is fit on a held-out set to correct systematic bias in the mean prediction. The method is trained to minimize negative log-likelihood of the predicted mixture density.

## Key Results
- Anchor-MoE consistently matches or surpasses NGBoost baseline in both RMSE and NLL across standard UCI regression datasets
- Under Hölder smoothness assumptions, Anchor-MoE achieves the minimax-optimal L2 risk rate of O(N^(-2α/(2α+d)))
- The CRPS generalization gap scales as Õ(√((log(Mh)+P+K)/N)), logarithmic in model width
- Under bounded-overlap routing, the dependence on the number of experts K can be replaced by the constant k

## Why This Works (Mechanism)

### Mechanism 1: Residual Correction via Anchoring
If a strong point predictor (anchor) is provided, the Mixture-of-Experts (MoE) layers can focus capacity on learning heteroscedastic residuals rather than the global mean, improving convergence stability. The architecture uses a pre-trained GBDT to provide a baseline mean μ_anc(x). The experts predict a delta Δ_j,c(x) added to this anchor. This limits the search space for the neural experts to the error surface of the tree model. Core assumption: The anchor model captures the bulk of the conditional mean signal.

### Mechanism 2: Bounded-Overlap Routing for Generalization
If routing is constrained to a bounded overlap (top-k), the generalization error scales with the constant number of active experts (k) rather than the total expert count (K), allowing for massive over-parameterization without overfitting. The router combines a metric-window kernel (locality) with a soft top-k mask. Theoretical analysis shows this ensures the effective capacity scales with k, bounding the Rademacher complexity. Core assumption: The partition of unity (PoU) has bounded overlap and the regression function is Hölder smooth.

### Mechanism 3: Post-Hoc Linear Calibration
Fitting a linear affine map on a disjoint calibration set can correct systematic bias in the mean prediction without leaking test information or altering the learned variance. After training, an affine transform (a, b) is fit via least squares on a held-out split to map predicted means to targets. This separates the goal of accurate point estimation (RMSE) from density estimation (NLL). Core assumption: The calibration split is drawn from the same distribution as the test set.

## Foundational Learning

- **Concept: Mixture Density Networks (MDN)**
  - **Why needed here**: The "Experts" in this architecture are MDNs; they do not output a single value but parameters (mean, variance, weights) of a Gaussian mixture.
  - **Quick check question**: Can you explain why predicting the variance σ² alongside the mean is necessary for optimizing Negative Log-Likelihood (NLL)?

- **Concept: Hölder Smoothness**
  - **Why needed here**: The theoretical minimax-optimal rates rely on the assumption that the regression function is Hölder smooth of order α. This defines how "nice" the underlying data manifold is.
  - **Quick check question**: If the data has discontinuities (is not smooth), how might the claimed O(N^(-2α/(2α+d))) rate be affected?

- **Concept: Rademacher Complexity**
  - **Why needed here**: Used in the paper to bound the generalization gap. It measures the richness of the function class relative to the sample size.
  - **Quick check question**: Why does limiting the "active" experts via top-k routing reduce the Rademacher complexity of the model class?

## Architecture Onboarding

- **Component map**: Input -> LayerNorm + Linear projection -> Metric-window kernel + Router -> Top-k mask -> Expert weights -> K MLPs (3-component MDN) -> Weighted sum of expert outputs -> Anchor + Delta aggregation -> Linear calibration

- **Critical path**: The interaction between the Metric-Window (locality) and the Router (content) is the core novelty. Ensure the gradient flows through the top-k masking operation (paper uses a smoothing epsilon ε).

- **Design tradeoffs**:
  - K (Expert Count) vs. N (Data): Theory suggests K ∝ N^(d/(2α+d)). Increasing K without increasing N yields diminishing returns.
  - Anchor vs. No-Anchor: Without the anchor, the model may converge to "hedging" (large variance) to minimize NLL, harming point accuracy.

- **Failure signatures**:
  - Variance Inflation: If NLL drops but RMSE stalls, experts may be inflating σ to cover uncertainty rather than learning the mean. (Check ablation "No-Anchor").
  - Router Collapse: If all inputs map to the same expert, the "bounded overlap" assumption fails. Monitor the load balance.

- **First 3 experiments**:
  1. Ablation Study: Run No-Anchor vs. Anchor+Delta on a toy dataset to visualize the difference in learned variance (see Fig 4 in paper).
  2. Scaling Law Check: Vary K (e.g., 4, 8, 16) on a fixed dataset size and plot test NLL. Verify if performance plateaus as predicted by the O(K/N) estimation term.
  3. Calibration Validation: Fit the post-hoc linear map on different random splits to ensure the calibration parameters (a,b) are stable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Mixture of Experts (MoE) be trained to avoid large-variance hedging without relying on an anchor mean?
- Basis in paper: The conclusion states, "Avoiding large-variance hedging when training MoE without an anchor is a key open issue and would further decouple the two stages."
- Why unresolved: The current architecture relies on the anchor to stabilize variance; removing it exposes the model to hedging behaviors that degrade uncertainty estimates.
- What evidence would resolve it: A regularization technique or objective function modification that prevents variance inflation in an anchor-free setting while maintaining NLL performance.

### Open Question 2
- Question: Can "calibration-by-design" replace the post-hoc linear map on held-out data?
- Basis in paper: The conclusion suggests, "Replacing the held-out mean calibration with 'calibration-by-design' could simplify the pipeline and reduce data fragmentation."
- Why unresolved: The current method requires splitting data for calibration; integrating this into the training phase is a proposed simplification not yet implemented.
- What evidence would resolve it: A model architecture or loss function that achieves comparable RMSE on test data without utilizing a disjoint calibration split.

### Open Question 3
- Question: Do adaptive top-k gating or data-dependent temperature schedules improve expert specialization?
- Basis in paper: The authors propose that "Exploring capacity-controlled learned routers... adaptive k for top-k gating... may improve specialization without hurting generalization."
- Why unresolved: The current implementation uses fixed k and static parameters; dynamic routing strategies remain unexplored.
- What evidence would resolve it: Empirical results demonstrating that dynamic routing strategies reduce NLL or improve RMSE compared to the static configuration used in the paper.

## Limitations
- Theoretical generalization bounds are asymptotic and rely on strong assumptions (Hölder smoothness, bounded overlap routing) with unknown constants
- Router implementation details (metric-window kernel, bounded overlap mechanism) are not fully specified in text
- Direct comparison to vanilla MoE (without anchor) on same datasets would strengthen case for residual correction mechanism

## Confidence
- **High Confidence**: Core empirical results (Anchor-MoE outperforms NGBoost on UCI datasets in both RMSE and NLL) are well-supported by presented tables
- **Medium Confidence**: Theoretical minimax-optimal rate and generalization gap bounds are valid under stated assumptions, but practical tightness and dependence on unknown constants are unclear
- **Medium Confidence**: Specific implementation details of the router and necessity of the anchor (vs. strong baseline) have some uncertainty

## Next Checks
1. **Router Load Balance Analysis**: For a fixed dataset, train Anchor-MoE with different k values (e.g., k=1,2,4) and plot the average number of experts active per sample and the entropy of the routing distribution. This will empirically verify if the "bounded overlap" assumption holds and if increasing k leads to expected improvements.

2. **Anchor vs. No-Anchor Direct Comparison**: Implement a "Vanilla MoE" baseline that predicts the mean directly (without an anchor) and compare its RMSE and NLL to Anchor-MoE on a small dataset (e.g., Boston). This will isolate the benefit of the residual learning framework.

3. **Calibration Set Size Sensitivity**: Train Anchor-MoE and fit the post-hoc linear map on calibration sets of varying sizes (e.g., 1%, 5%, 10% of training data). Plot resulting RMSE to determine minimum effective calibration set size and test claim that it does not leak test information.