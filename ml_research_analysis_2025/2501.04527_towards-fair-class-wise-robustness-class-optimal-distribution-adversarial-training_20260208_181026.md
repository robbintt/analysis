---
ver: rpa2
title: 'Towards Fair Class-wise Robustness: Class Optimal Distribution Adversarial
  Training'
arxiv_id: '2501.04527'
source_url: https://arxiv.org/abs/2501.04527
tags:
- robust
- accuracy
- codat
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of robust fairness in adversarial
  training, where models exhibit significant disparities in robustness across different
  classes. The authors propose Class Optimal Distribution Adversarial Training (CODAT),
  a novel min-max framework that employs distributionally robust optimization to explore
  the class-wise weight space and identify the optimal weight assignments.
---

# Towards Fair Class-wise Robustness: Class Optimal Distribution Adversarial Training

## Quick Facts
- arXiv ID: 2501.04527
- Source URL: https://arxiv.org/abs/2501.04527
- Reference count: 40
- Primary result: CODAT improves worst-class robust accuracy by at least 1% under PGD-100 attack on CIFAR-10 while maintaining average accuracy

## Executive Summary
This paper addresses robust fairness in adversarial training, where models exhibit significant disparities in robustness across different classes. The authors propose Class Optimal Distribution Adversarial Training (CODAT), a novel min-max framework that employs distributionally robust optimization to explore the class-wise weight space and identify optimal weight assignments. CODAT derives a closed-form solution to the inner maximization problem, enabling joint optimization of weights and model parameters. Experiments on CIFAR-10, CIFAR-100, SVHN, and STL-10 datasets demonstrate that CODAT outperforms state-of-the-art methods in improving robust fairness.

## Method Summary
CODAT extends standard adversarial training by adding a distribution-level maximization to the min-max optimization framework. The method formulates robust training as a distributionally robust optimization problem with a χ²-divergence constraint. Using Lagrangian duality, CODAT derives a closed-form optimal solution for class weights, converting the inner maximization into a deterministic objective: E[H] + √(η·Var[H]). This enables joint gradient-based optimization of weights and model parameters. The method uses PGD-10 for adversarial example generation and SGD with piece-wise constant learning rate decay. The key hyperparameter η controls the exploration of the weight space, with optimal values depending on model architecture.

## Key Results
- On CIFAR-10, CODAT improves worst-class robust accuracy by at least 1% under PGD-100 attack compared to standard adversarial training
- CODAT achieves a Fairness Elasticity Coefficient of 2.05, indicating superior performance in enhancing robust fairness
- The method demonstrates consistent improvements across multiple datasets (CIFAR-10, CIFAR-100, SVHN, STL-10) and architectures (ResNet-18, WideResNet-34-10)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distributionally Robust Optimization (DRO) systematically explores the class-wise weight space with theoretical guarantees, avoiding the limitations of heuristic weight assignment.
- **Mechanism:** CODAT formulates a min-max problem where the inner maximization searches over all valid class distributions within a χ²-divergence ball. The worst-case distribution corresponds to optimal class weights—classes with higher risk receive higher weights automatically via the closed-form solution.
- **Core assumption:** The optimal class weighting lies within the ambiguity set defined by D(P||P₀) ≤ η, and the empirical distribution P₀ is sufficiently representative.
- **Evidence anchors:**
  - [abstract]: "employs distributionally robust optimization to fully explore the class-wise weight space, thus enabling the identification of the optimal weight with theoretical guarantees"
  - [Section 4.1, Eq. 4-6]: The min-max formulation with ambiguity set constraint
  - [corpus]: Related work (CFOL, FAAL) uses different divergence metrics (CVaR, KL) but lacks the closed-form derivation; this paper explicitly contrasts against heuristic approaches
- **Break condition:** If η is set too large, the ambiguity set becomes too broad, making the model overly conservative and potentially causing training failure (Section 5.5).

### Mechanism 2
- **Claim:** The derived closed-form optimal solution converts the inner maximization into a deterministic objective, enabling joint gradient-based optimization of weights and model parameters.
- **Mechanism:** Using Lagrangian duality, the paper derives p*(ξ) = p₀(ξ) + p₀(ξ)√(η/Var[H(θ,ξ)])·(H(θ,ξ) - E[H(θ,ξ)]). Substituting this yields a deterministic equivalent: E[H] + √(η·Var[H]). This is differentiable and can be optimized alongside θ.
- **Core assumption:** The variance Var_{P₀}[H(θ,ξ)] is finite (Assumption 1), which holds since H is bounded and P₀ is discrete.
- **Evidence anchors:**
  - [Section 4.2, Theorem 1 and Eq. 8]: Explicit closed-form solution for p*(ξ)
  - [Section 4.2, Eq. 11]: Deterministic equivalent objective function
  - [corpus]: No direct corpus comparison found for this specific closed-form derivation approach
- **Break condition:** If Var[H] → 0 (all classes have identical risk), the weight update term becomes undefined; practically, this is unlikely during training.

### Mechanism 3
- **Claim:** The χ²-divergence constraint naturally prevents weight degeneration into a Dirac distribution without requiring explicit regularization.
- **Mechanism:** The χ²-divergence between a Dirac distribution and uniform P₀ equals K-1 (Eq. 14). For K=10 classes, this is 9.0, but typical η values are <1.0. Thus, Dirac distributions lie outside the feasible ambiguity set, ensuring non-degenerate weight distributions.
- **Core assumption:** The hyperparameter η is chosen appropriately small (Section 5.5 shows η ∈ {0.1, 2.0} with optimal around 0.3 for ResNet-18).
- **Evidence anchors:**
  - [Section 4.3, Eq. 14]: Mathematical proof that χ²(Dirac||Uniform) = K-1
  - [Section 4.3]: "our CODAT framework does not require the incorporation of a regularization term to prevent degeneration"
  - [corpus]: Related methods (FAAL, CFOL) use KL-divergence or CVaR which may require explicit regularization
- **Break condition:** If η ≥ K-1, the ambiguity set could include Dirac distributions, leading to unstable optimization focused on a single class.

## Foundational Learning

- **Concept: Distributionally Robust Optimization (DRO)**
  - Why needed here: CODAT's core theoretical framework; understanding ambiguity sets and worst-case distribution is essential to grasp why the method theoretically guarantees optimal weights.
  - Quick check question: Given an empirical distribution P₀ and divergence constraint η, what does the worst-case distribution maximize?

- **Concept: Adversarial Training as Min-Max Optimization**
  - Why needed here: CODAT extends standard AT by adding a distribution-level maximization; understanding the standard AT formulation (Eq. 1) is prerequisite.
  - Quick check question: In standard AT, what does the inner maximization optimize, and what does the outer minimization optimize?

- **Concept: χ²-divergence**
  - Why needed here: The choice of divergence metric determines the closed-form solution and whether regularization is needed; χ²-divergence has specific properties exploited in Section 4.3.
  - Quick check question: Why does χ²-divergence between a Dirac and uniform distribution scale with K, and how does this property prevent degeneration?

## Architecture Onboarding

- **Component map:**
  - Input layer -> Adversarial perturbation module -> Class-wise loss aggregator -> Distribution optimizer -> Model parameter update

- **Critical path:** The class-wise loss computation → variance estimation → deterministic equivalent → joint gradient update. Errors in class-wise aggregation will propagate through the variance term and corrupt weight updates.

- **Design tradeoffs:**
  - η selection: Lower η = more conservative weight exploration, higher η = broader search but risk of over-conservatism (Section 5.5 shows inverted-U relationship for worst-class accuracy)
  - Batch size: Must be large enough to provide stable variance estimates per class; small batches may cause noisy weight updates
  - Model capacity: Larger models (WideResNet-34-10 vs ResNet-18) allow more effective capacity reallocation, amplifying CODAT's benefits (Table 5)

- **Failure signatures:**
  - Training divergence or NaN loss: η likely too large, causing numerical instability in variance term
  - No improvement in worst-class accuracy: η too small, insufficient exploration; or batch size too small for stable variance
  - Degraded average accuracy without worst-class gain: Weight distribution collapsed toward underperforming classes too aggressively

- **First 3 experiments:**
  1. **Baseline sanity check:** Run standard AT and CODAT on CIFAR-10 with ResNet-18, η=0.3; verify worst-class robust accuracy improves ≥1% under PGD-100 (Table 1 benchmark).
  2. **η sensitivity sweep:** Train with η ∈ {0.1, 0.3, 0.5, 1.0, 2.0} and plot both average and worst-class robust accuracy; confirm inverted-U pattern for worst-class and monotonic decrease for average (Figure 7).
  3. **Weight distribution inspection:** After training, extract final p*(ξ) values per class; verify they correlate with class-wise robust risk and that no class receives near-zero weight (non-degenerate distribution).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying theoretical causes of the bimodal distribution phenomenon in class-wise robust accuracy observed during adversarial training, and can specific strategies be developed to mitigate this pattern?
- Basis in paper: [explicit] The conclusion explicitly states the intent to "investigate the underlying causes of the bimodal distribution phenomenon observed in the class-wise robust accuracy... and to develop strategies to further enhance robust fairness."
- Why unresolved: While Figure 6 shows that CODAT narrows the gap between the two peaks of the distribution, the bimodal shape persists. The paper currently lacks a theoretical explanation for why classes cluster into two distinct performance groups rather than exhibiting a uniform spread of robustness.
- What evidence would resolve it: A theoretical analysis linking semantic relationships or data geometry to the observed clustering, or a modified objective function that results in a unimodal distribution of class-wise robust accuracy.

### Open Question 2
- Question: Can the ambiguity set radius (η) be theoretically determined or adapted dynamically during training to remove the need for manual, dataset-specific tuning?
- Basis in paper: [inferred] Section 5.5 highlights that the model is highly sensitive to η, noting that larger values cause training failure while smaller values limit exploration. However, the paper relies on grid search to select different optimal η values for ResNet-18 versus WideResNet-34-10.
- Why unresolved: The framework currently treats η as a static hyperparameter, but the optimal value appears to depend on the model capacity and dataset, suggesting a lack of theoretical guidance for setting this constraint a priori.
- What evidence would resolve it: An adaptive algorithm that adjusts η based on the training variance Var_{P₀}(R_{ξ}^{rob}) or convergence metrics, achieving optimal worst-class accuracy without requiring a manual grid search.

### Open Question 3
- Question: Does the assumption of a uniform empirical distribution (P₀) limit CODAT's effectiveness on long-tailed or inherently imbalanced datasets where class priors are skewed?
- Basis in paper: [inferred] Section 3.1 and Algorithm 1 define the empirical distribution P₀ as uniform, and all experiments are conducted on balanced benchmark datasets (CIFAR-10/100, SVHN, STL-10), leaving the interaction between CODAT and class imbalance unexplored.
- Why unresolved: While the closed-form solution theoretically supports non-uniform P₀, the method's mechanism of assigning higher weights to high-risk classes might conflict with the small sample sizes of tail classes in long-tailed distributions, potentially leading to overfitting or instability.
- What evidence would resolve it: Experimental results on standard long-tailed datasets (e.g., ImageNet-LT or CIFAR-100-LT) comparing the performance of uniform P₀ versus P₀ set to the true class frequencies.

## Limitations

- The method's performance on real-world imbalanced datasets remains unexplored, as CIFAR-10/100 are relatively balanced
- The χ²-divergence constraint requires careful tuning of η, which appears architecture-dependent (0.3 for ResNet-18 vs 0.8 for WideResNet-34-10)
- The variance-based weight updates may introduce instability with small batch sizes, though this is not extensively analyzed

## Confidence

- **High Confidence:** The theoretical framework (DRO formulation, closed-form solution) is mathematically sound and internally consistent. The CIFAR-10 results showing consistent worst-class improvement across architectures are reproducible.
- **Medium Confidence:** The Fairness Elasticity Coefficient as a metric is novel but not yet widely adopted; its correlation with practical fairness improvements needs broader validation. The claim that χ²-divergence naturally prevents degeneration is mathematically proven but depends on appropriate η selection.
- **Low Confidence:** Generalization to highly imbalanced datasets is speculative without empirical validation. The sensitivity to batch size and its impact on variance estimation is acknowledged but not systematically studied.

## Next Checks

1. **Imbalanced Dataset Test:** Apply CODAT to a deliberately imbalanced version of CIFAR-10 (e.g., reduce certain class samples by 50-90%) and measure whether worst-class robust accuracy still improves proportionally.

2. **Batch Size Sensitivity:** Systematically vary batch size (32, 64, 128, 256) and measure the stability of class-wise variance estimates and corresponding worst-class accuracy gains.

3. **Cross-Dataset Transfer:** Train CODAT on CIFAR-10 and evaluate on a different dataset (e.g., STL-10 or TinyImageNet) to test whether learned class weights generalize beyond the training distribution.