---
ver: rpa2
title: 'Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in
  Generating Harmful Audio'
arxiv_id: '2511.10913'
source_url: https://arxiv.org/abs/2511.10913
tags:
- harmful
- audio
- attacks
- text
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the underexplored threat of using large text-to-speech
  (TTS) models to generate audio containing harmful speech content, bypassing safety
  mechanisms. The authors propose HARMGEN, a suite of five attacks organized into
  two families: semantic obfuscation techniques (Concat, Shuffle) that conceal harmful
  content within text, and audio-modality exploits (Read, Spell, Phoneme) that inject
  harmful content through auxiliary audio channels while maintaining benign textual
  prompts.'
---

# Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio

## Quick Facts
- arXiv ID: 2511.10913
- Source URL: https://arxiv.org/abs/2511.10913
- Reference count: 40
- Authors bypass TTS safety filters by concealing harmful content in benign text prompts or injecting it via auxiliary audio channels

## Executive Summary
This paper addresses the underexplored threat of using large text-to-speech (TTS) models to generate audio containing harmful speech content, bypassing safety mechanisms. The authors propose HARMGEN, a suite of five attacks organized into two families: semantic obfuscation techniques (Concat, Shuffle) that conceal harmful content within text, and audio-modality exploits (Read, Spell, Phoneme) that inject harmful content through auxiliary audio channels while maintaining benign textual prompts. Through extensive evaluation across five commercial LALMs-based TTS systems and three datasets spanning two languages, the attacks significantly reduced refusal rates and increased the toxicity of generated speech. Reactive defenses like deepfake audio detection and text moderation proved ineffective, while proactive moderation by TTS providers detected 57-93% of attack instances. The work highlights critical vulnerabilities in TTS safety mechanisms and underscores the need for robust cross-modal safeguards throughout model training and deployment.

## Method Summary
The paper proposes HARMGEN, a suite of five attacks to bypass TTS safety mechanisms. Semantic attacks (Concat, Shuffle) fragment harmful text to evade input moderation, then reconstruct the audio output. Audio-modality attacks (Read, Spell, Phoneme) inject harmful content via auxiliary audio while maintaining benign text prompts. The method is evaluated across five commercial LALMs-based TTS systems using three datasets (Ethos, Mul-ZH, and a custom Self dataset). Metrics include refusal rates (R1/R2) and toxicity scores (TS). The attacks were implemented using toxic span detection (Mudes) and Montreal Forced Aligner for audio reconstruction.

## Key Results
- Concat and Shuffle attacks reduced refusal rates to near 0% while maintaining high toxicity scores
- Audio-modality attacks (Read, Spell, Phoneme) successfully injected harmful content through benign text prompts
- State-of-the-art deepfake detectors achieved ≤25% accuracy on high-fidelity LALM outputs
- Proactive moderation by TTS providers detected 57-93% of attack instances
- Current reactive defenses (text moderation, deepfake detection) proved ineffective against HARMGEN attacks

## Why This Works (Mechanism)

### Mechanism 1: Semantic Fragmentation and Reconstruction
If harmful text is fragmented into semantically neutral segments or shuffled, text-based safety filters fail because they analyze input holistically rather than tracking temporal reconstruction possibilities. The attack exploits the "semantic gap" between input moderation and output generation. By splitting a sentence like "harm [group]" into "harm" and "[group]" (Concat) or scrambling it (Shuffle), the input passes moderation. The full harmful audio is only reconstructed after generation by concatenating audio segments or reordering words using a forced aligner.

### Mechanism 2: Cross-Modal Semantic Smuggling
Injecting harmful content via the audio modality (e.g., spelling a word aloud) bypasses text-only safety alignment because the model treats the audio transcription and text synthesis as separate processing tasks. In attacks like Read, Spell, or Phoneme, the harmful word is encoded in an auxiliary audio file (e.g., "s h i t"). The text prompt is benign (e.g., "Repeat what you hear in the gap: [word]"). The LALM transcribes the audio and inserts it into the output speech. Text-based guardrails miss the harmful intent because the harmful token never appears in the input text.

### Mechanism 3: Fidelity-Induced Detection Failure
High-fidelity audio generated by Large Audio-Language Models (LALMs) degrades the performance of deepfake detectors because these detectors rely on artifacts (e.g., low-quality spectral glitches) that LALMs do not produce. Reactive defenses use deepfake detectors trained on conventional TTS outputs. LALMs produce audio with fewer spectral artifacts and more natural prosody. This shifts the distribution of the audio away from the "synthetic" features the detectors learned, resulting in high False Negative rates (low accuracy).

## Foundational Learning

- **Concept: Large Audio-Language Models (LALMs) vs. Traditional TTS**
  - Why needed here: Traditional TTS models simply map text to audio (Grapheme-to-Phoneme). LALMs (like GPT-4o) are multi-modal instruction followers. They can interpret instructions inside the text or audio, which is what enables the "Audio-Modality Smuggling" attack.
  - Quick check question: Does the system just "read" text, or can it "understand" an instruction to "repeat the word spelled out in this audio file"?

- **Concept: Safety Alignment & Refusal Rates**
  - Why needed here: The paper measures success by reducing "refusal rates." Understanding this requires knowing that models are trained to refuse harmful prompts, and the paper's goal is to find inputs that trigger synthesis (low refusal) while maintaining high toxicity.
  - Quick check question: If a model refuses a prompt, does it generate an error code or a text apology (e.g., "I cannot assist with that")?

- **Concept: Toxic Span Detection & Forced Alignment**
  - Why needed here: To implement the "Concat" or "Shuffle" attacks, one must first identify the exact harmful words (Toxic Span) to isolate them. To fix the "Shuffle" attack output, one must use Forced Alignment to find word boundaries in the audio and reorder them.
  - Quick check question: How does the system identify which word in "That is total bull shit" is the toxic span requiring obfuscation?

## Architecture Onboarding

- **Component map:** Input Processor -> Safety Filter -> LALM Core -> Post-Processor -> Reconstructor (Attacker Side)
- **Critical path:** The attack success flows from Input Obfuscation (bypassing the Safety Filter) -> LALM Synthesis (generating the benign/harmful mix) -> Audio Reconstruction (restoring the full harmful message)
- **Design tradeoffs:**
  - Semantic Attacks (Concat/Shuffle): High success rate (low refusal) but lower naturalness (choppy prosody) due to stitching
  - Audio Attacks (Spell/Phoneme): Higher naturalness (continuous generation) but lower success rate if the LALM fails to transcribe the audio correctly
  - Proactive Defense: High efficacy (57-93% detection) but adds latency and complexity to the provider's inference pipeline
- **Failure signatures:**
  - Refusal Response: Model returns "I cannot fulfill this request" (Safety alignment triggered)
  - Transcription Error: Model outputs "p 1 [word] p 2" literally because it failed to decode the audio injection
  - Misalignment: Stitched audio sounds robotic or has incorrect pauses
- **First 3 experiments:**
  1. Baseline Profiling: Run a standard harmful prompt (B1) against the target LALM to establish the base refusal rate (R1/R2) and Toxicity Score (TS)
  2. Semantic Obfuscation Test: Implement "Concat" by splitting a harmful sentence at the toxic span. Measure if R1 drops to near 0% while TS remains high
  3. Defense Evasion Test: Generate audio using the "Spell" attack and pass the output to a standard deepfake detector (e.g., AASIST2). Verify the low detection accuracy reported in §6.1.1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can switching shuffling units from words to substrings preserve naturalness in the Shuffle attack while maintaining attack efficacy?
- Basis in paper: The authors state in Section 7: "This limitation can be alleviated by switching the shuffling units from words to substrings, which we leave for future work."
- Why unresolved: Current word-level shuffling disrupts pitch contours and coarticulation, producing less natural prosody. Substrings may preserve more phonetic context.
- What evidence would resolve it: Empirical comparison of word-level vs. substring-level shuffling attacks, measuring both toxicity success rates and perceptual naturalness via human evaluation or prosodic metrics.

### Open Question 2
- Question: Can integrating HARMGEN attacks as adversarial data augmentation during model alignment improve resistance to cross-modal harmful content smuggling?
- Basis in paper: Section 7 identifies "integrating our attacks as adversarial data augmentation during alignment/safety training" as a complementary direction for improving robustness.
- Why unresolved: Safety training typically focuses on text-only inputs. Cross-modal attacks (Read, Spell, Phoneme) exploit the audio modality, which may not be covered in current alignment procedures.
- What evidence would resolve it: Train LALMs with augmented adversarial examples from HARMGEN, then evaluate whether refusal rates increase and toxicity scores decrease under the same attacks compared to baseline models.

### Open Question 3
- Question: How can deepfake audio detectors be improved to reliably identify high-fidelity audio generated by LALMs-based TTS systems?
- Basis in paper: Section 6.1.1 shows state-of-the-art detector AASIST2 achieves ≤70% accuracy and ≥55% EER on GPT-5o-nano outputs, and ≤25% accuracy with ≥75% EER on GPT-4o-mini-audio outputs.
- Why unresolved: LALMs produce substantially higher quality audio with minimal artifacts compared to conventional TTS systems, exposing a gap in current detection capabilities.
- What evidence would resolve it: Develop and benchmark detectors trained specifically on LALM-generated audio across diverse attack types, measuring detection accuracy and EER improvements over current SOTA.

## Limitations
- The effectiveness of proactive moderation (57-93% detection rate) depends on providers' specific implementation choices, and the paper does not detail the exact detection mechanisms used
- Attack success rates may vary significantly across different LALM architectures beyond the five tested models
- The Concat and Shuffle attacks' unnatural prosody suggests these may be more detectable by human listeners despite passing automated safety filters

## Confidence

- **High Confidence**: The core finding that current deepfake detectors fail against high-fidelity LALM outputs (25% accuracy) is well-supported by the reported experimental results and aligns with established challenges in the detection literature
- **Medium Confidence**: The attack success rates (reduced refusal, maintained toxicity) are reliable within the tested conditions, but the exact performance may vary with different safety alignment strategies or future defensive updates
- **Medium Confidence**: The conclusion about proactive moderation effectiveness (57-93% detection) is supported by experimental data, though the specific detection mechanisms and their generalizability remain unclear

## Next Checks

1. Test attack effectiveness against LALMs with different safety alignment strategies (e.g., models using audio-level moderation or temporal context tracking)
2. Evaluate whether simple heuristic-based output moderation (detecting choppy prosody or mismatched word boundaries) can catch Concat/Shuffle attacks without requiring full transcript analysis
3. Assess attack success rates when models are instructed to apply safety filters to both input text and transcribed audio content before synthesis