---
ver: rpa2
title: 'Unfolding the Headline: Iterative Self-Questioning for News Retrieval and
  Timeline Summarization'
arxiv_id: '2501.00888'
source_url: https://arxiv.org/abs/2501.00888
tags:
- news
- timeline
- chronos
- events
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CHRONOS, a framework for open-domain news timeline
  summarization using large language models (LLMs) with iterative self-questioning.
  The method constructs timelines by iteratively retrieving news documents based on
  questions about the target news, rewriting complex questions for better retrieval,
  and merging generated timelines.
---

# Unfolding the Headline: Iterative Self-Questioning for News Retrieval and Timeline Summarization

## Quick Facts
- **arXiv ID:** 2501.00888
- **Source URL:** https://arxiv.org/abs/2501.00888
- **Reference count:** 11
- **Primary result:** Introduces CHRONOS, an LLM-based framework for open-domain news timeline summarization using iterative self-questioning, achieving state-of-the-art performance on benchmarks.

## Executive Summary
This paper introduces CHRONOS, a novel framework for open-domain news timeline summarization that leverages large language models with iterative self-questioning. The system constructs timelines by iteratively retrieving news documents based on questions about the target news, rewriting complex questions for better retrieval, and merging generated timelines. The approach introduces a new dataset, Open-TLS, containing 50 timelines on recent news topics authored by professional journalists. Experiments show CHRONOS achieves comparable performance to state-of-the-art closed-domain methods on established benchmarks while demonstrating significant improvements in efficiency and scalability. The framework uses a Chrono-Informativeness metric to select informative few-shot examples for prompting and employs a divide-and-conquer strategy for timeline generation and merging.

## Method Summary
CHRONOS employs an iterative self-questioning mechanism where an LLM generates questions about the target news, which are then rewritten into focused search queries. These queries retrieve relevant documents from either web search (open-domain) or a provided corpus (closed-domain). The system uses a Chrono-Informativeness metric to select few-shot examples that help guide question generation. Retrieved documents are processed to extract milestone events, which are merged across multiple rounds to produce a final timeline. The framework supports both open-domain (using Bing Web Search API) and closed-domain (using Elasticsearch) settings, with automatic chunking of documents for efficient processing.

## Key Results
- CHRONOS achieves state-of-the-art performance on established timeline summarization benchmarks (T17 and Crisis)
- The framework demonstrates significant efficiency gains in open-domain settings compared to closed-domain approaches
- Ablation studies confirm the effectiveness of the Chrono-Informativeness metric for exemplar selection and the question rewriting module

## Why This Works (Mechanism)

### Mechanism 1: Iterative Self-Questioning for Graph Traversal
The system treats events as nodes and uses iterative questioning to traverse an implicit event graph. Instead of a single search, the LLM generates questions that the current context cannot answer, effectively discovering causally or temporally linked events that direct keyword search might miss. This approach builds a timeline by expanding from initial events through successive retrieval rounds.

### Mechanism 2: Chrono-Informativeness for Few-Shot Selection
The system calculates a CI score (Date F1) for potential example questions offline, using this metric to select the most effective questions as demonstrations in the prompt. This anchors the LLM's questioning logic in historical success, significantly outperforming random example selection. The assumption is that effective questioning strategies are transferable between semantically similar news topics.

### Mechanism 3: Question Rewriting for Retrieval Precision
LLMs naturally generate complex, multi-hop questions that perform poorly in standard search engines. The Rewriter module decomposes these into 2-3 shorter keyword-based queries that better match search engine behaviors. This decomposition improves retrieval precision by converting natural language questions into focused search queries that commercial search engines can handle effectively.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: CHRONOS is fundamentally a RAG architecture but uses iterative retrieval loops instead of single retrieval
  - Quick check question: How does CHRONOS differ from a standard RAG pipeline that retrieves once and then generates?

- **Concept: In-Context Learning (Few-Shot Prompting)**
  - Why needed here: The Questioner relies on dynamically retrieved examples to learn how to ask questions without weight updates
  - Quick check question: Why does the quality of few-shot examples (Chrono-Informativeness) matter more than their mere existence?

- **Concept: Timeline Summarization (TLS) Evaluation**
  - Why needed here: Understanding "Date F1" vs "ROUGE" is critical as the paper optimizes for dates in its example selection
  - Quick check question: Why might a timeline have high ROUGE scores but low Date F1?

## Architecture Onboarding

- **Component map:** Context Survey -> Questioner -> Rewriter -> Retriever -> Generator -> Merger
- **Critical path:** The Questioner is the bottleneck; if it generates irrelevant questions or the Rewriter cannot simplify them, the retrieval loop fails
- **Design tradeoffs:** Rounds vs Noise - increasing rounds initially improves performance but eventually causes degradation due to noise and merging complexity; Open vs Closed Domain - open-domain introduces more noise but higher scalability
- **Failure signatures:** Topic Drift (later rounds generate peripheral topics), Redundancy (duplicate events on same date), Empty Retrieval (queries return zero results)
- **First 3 experiments:**
  1. Baseline Comparison: Run DIRECT vs REWRITE vs CHRONOS on Open-TLS to validate iterative loop necessity
  2. Ablation on Exemplars: Compare Zero-Shot vs Random Exemplar vs CI-Selected Exemplar to quantify Chrono-Informativeness value
  3. Iterative Scaling: Plot performance (Date F1) against retrieved documents (20, 30, 40) to find saturation point

## Open Questions the Paper Calls Out

- **Open Question 1:** How can retrieval strategies be adapted for events linked only by chronological coincidence rather than strong causal relationships? The current iterative self-questioning mechanism relies on establishing causal edges between event nodes, but purely chronological sequences lack the semantic hooks needed for document retrieval.

- **Open Question 2:** Can a dynamic stopping criterion be developed to detect the optimal number of self-questioning rounds during inference? While performance improves then declines with more rounds, the paper provides no automated method for identifying this peak, which appears to vary by model and topic.

- **Open Question 3:** How can the framework be stabilized to ensure consistency despite the volatility of Search Engine Results Pages (SERPs)? The authors identify this volatility as a limitation that leads to variations in summary quality and reliability across different retrieval attempts.

## Limitations

- The system's performance on truly novel topics (with no similar examples in the example pool) remains untested
- The merge strategy for combining sub-timelines lacks detailed specification of how conflicts are resolved
- The question rewriting module's effectiveness depends on balancing query simplification without losing critical specificity

## Confidence

- **Iterative self-questioning framework effectiveness:** High confidence - demonstrated through controlled ablations and comparison to strong baselines
- **Chrono-Informativeness metric for exemplar selection:** High confidence - validated through direct ablation showing random exemplars cause performance degradation
- **Open-domain scalability advantage:** Medium confidence - while efficiency gains are shown, web search noise and API dependency raise concerns about consistent real-world performance
- **Question rewriting contribution:** High confidence - ablation shows clear performance drop without rewriting
- **Novelty of Open-TLS dataset:** High confidence - introduced as first of its kind with professional journalist authorship

## Next Checks

1. **Reproducibility stress test:** Run CHRONOS on 5 new topics outside the Open-TLS domain (e.g., emerging technology, niche sports) to test generalizability of Chrono-Informativeness exemplar selection when semantic similarity is low.

2. **Noise vs. information tradeoff:** Systematically vary the number of retrieved documents per round (N=10, 20, 30, 40, 50) on the T17 benchmark to quantify exactly where noise overwhelms information gain, and whether different topics exhibit different saturation points.

3. **Rewriter precision analysis:** Manually examine the rewriter's output for 50 complex questions across different domains to measure (a) semantic preservation rate and (b) retrieval precision improvement, determining if the rewriter introduces systematic biases or information loss.