---
ver: rpa2
title: 'TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands
  Vision Task Types'
arxiv_id: '2502.09925'
source_url: https://arxiv.org/abs/2502.09925
tags:
- task
- types
- image
- type
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TaskGalaxy introduces a large-scale multimodal instruction fine-tuning
  dataset containing 19,227 hierarchical task types and 413,648 high-quality samples.
  It uses GPT-4o and CLIP to automate task expansion, image matching, and question-answer
  generation, with open-source models filtering for quality.
---

# TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types

## Quick Facts
- arXiv ID: 2502.09925
- Source URL: https://arxiv.org/abs/2502.09925
- Authors: Jiankang Chen; Tianke Zhang; Changyi Liu; Haojie Ding; Yaya Shi; Feng Cheng; Huihui Xiao; Bin Wen; Fan Yang; Tingting Gao; Di Zhang
- Reference count: 40
- Primary result: TaskGalaxy improves multimodal model performance by 3.83–4.5 points across 16 benchmarks

## Executive Summary
TaskGalaxy introduces a large-scale multimodal instruction tuning dataset containing 19,227 hierarchical task types and 413,648 high-quality samples. It uses GPT-4o and CLIP to automate task expansion, image matching, and question-answer generation, with open-source models filtering for quality. Fine-tuning LLaVA-v1.5 and InternVL-Chat-v1.0 models with TaskGalaxy improved performance by an average of 3.83–4.5 points across 16 benchmarks, including gains of 68 points on MME, demonstrating that task diversity is crucial for enhancing multimodal model generalization.

## Method Summary
TaskGalaxy generates diverse multimodal instruction data through a hierarchical pipeline. Starting with ~100 seed task types, GPT-4o expands them across three levels to create 19,227 task types. CLIP matches these tasks to images from a pool of ~800k sources, then GPT-4o refines matches and generates question-answer pairs. A three-model ensemble (GLM4v, InternVL2-26B, InternVL-Chat-v1.5) filters samples requiring ≥2/3 approval. The resulting 413,648 samples are used to fine-tune LLaVA-v1.5 and InternVL-Chat-v1.0 models, freezing the visual encoder while training the LLM and projector with a learning rate of 2e-5 for one epoch.

## Key Results
- Average performance gain of 3.83–4.5 points across 16 benchmarks
- 68-point improvement on MME benchmark for LLaVA-v1.5-13B
- Performance consistently improves as task types increase from 2k to 19,227
- Demonstrates task diversity's crucial role in multimodal model generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing task type diversity during supervised fine-tuning improves multimodal model generalization across diverse benchmarks.
- Mechanism: Hierarchical task expansion exposes the model to a broader instruction distribution, reducing overfitting to narrow task patterns. The ablation study shows consistent improvements on benchmarks like LLaVA-in-the-wild, ChartQA, AI2D, Q-Bench, and MMMU as task types increase.
- Core assumption: Generated Q&A quality is sufficiently high that benefits derive from task diversity rather than noise.
- Evidence anchors: Performance gains across 16 benchmarks; ablation study showing improvement with increasing task types.
- Break condition: If Q&A quality degrades significantly at task-type margins, noise could counteract diversity benefits.

### Mechanism 2
- Claim: Automated hierarchical task generation from minimal human seeds enables scalable coverage without proportional manual labeling effort.
- Mechanism: GPT-4o expands task types iteratively across three levels using carefully designed prompts that request non-overlapping categories, generating 115, 2,796, and 14,370 task types across levels 1-3 respectively.
- Core assumption: GPT-4o's task taxonomy expansion produces meaningful, non-redundant categories mapping to real visual reasoning needs.
- Evidence anchors: Generation of 19,227 hierarchical task types from ~100 seeds; systematic expansion across three levels.
- Break condition: If prompt design fails to prevent semantic overlap, effective task diversity could be lower than the raw count suggests.

### Mechanism 3
- Claim: Multi-stage filtering combining CLIP retrieval, GPT-4o refinement, and multi-model scoring ensures image-task-question alignment and reduces hallucination-induced noise.
- Mechanism: CLIP provides top-k candidate task types per image via embedding similarity; GPT-4o refines these candidates contextually; three open-source models score each sample, retaining only those with ≥2/3 approval.
- Core assumption: The ensemble of three open-source models reliably identifies misalignment and hallucination better than any single model.
- Evidence anchors: Concrete examples of filtered samples showing CLIP mismatches and hallucination correction; majority vote requirement.
- Break condition: If open-source judge models share systematic blind spots, the ensemble may fail to catch certain hallucination patterns.

## Foundational Learning

- Concept: CLIP vision-language alignment
  - Why needed here: Understanding how CLIP computes image-text similarity (cosine similarity of embeddings) is essential for grasping the initial matching stage in TaskGalaxy's pipeline.
  - Quick check question: Given an image embedding I(x) and text embedding T(t), how does CLIP determine their match score?

- Concept: Supervised fine-tuning (SFT) for multimodal models
  - Why needed here: TaskGalaxy operates in the SFT phase after pre-training; understanding the two-stage training paradigm clarifies where and why this dataset is applied.
  - Quick check question: What model components are typically frozen vs. fine-tuned during the SFT phase for models like LLaVA-v1.5?

- Concept: Hierarchical task taxonomy
  - Why needed here: TaskGalaxy organizes 19,227 tasks across three levels (e.g., OCR → webpage OCR → link text extraction); understanding this structure is key to interpreting the dataset's coverage claims.
  - Quick check question: If a model underperforms on "logo-text association," which parent categories should you examine for related failure modes?

## Architecture Onboarding

- Component map: Task Expansion Module -> CLIP Matching -> GPT-4o Refinement -> Q&A Generation -> Multi-Model Screening
- Critical path: The CLIP → GPT-4o handoff is the bottleneck. CLIP's top-10 retrieval must include at least one valid task; otherwise, GPT-4o cannot recover correct matches. Monitor the "None" rate from GPT-4o's filtering prompt.
- Design tradeoffs:
  - Manual seeds vs. fully automated: More seeds improve coverage but increase upfront cost; current design uses ~100 seeds as a practical balance.
  - Top-k selection (k=10): Higher k improves recall but increases GPT-4o API cost; k=10 is an untested hyperparameter.
  - Three-model voting vs. single judge: Ensemble reduces false positives but triples inference cost during screening.
- Failure signatures:
  - CLIP mismatches: Appendix A.4 shows CLIP associating "smart home" with bathroom images; GPT-4o typically corrects these, but verify rejection logs.
  - Hallucination leakage: If judge models miss hallucinations (e.g., Appendix Figure A-10 where InternVL scored a math error as valid), quality degrades. Spot-check samples where vote splits 2-1.
  - Task-type imbalance: 50.1% of task types have only 1-10 samples; rare tasks may be undertrained.
- First 3 experiments:
  1. Ablate task count: Replicate Figure 6 (left) by sampling 2k, 5k, 10k, 15k, and 19,227 task types with fixed 100k samples. Plot benchmark performance to verify scaling trend holds for your target model.
  2. Ablate filtering stages: Disable the multi-model screening stage and measure quality degradation on a held-out validation set. This quantifies the screening ROI.
  3. Cross-model transfer: Fine-tune a different architecture (e.g., Qwen-VL or a newer InternVL version) with TaskGalaxy and compare gains against the reported LLaVA/InternVL-Chat-v1.0 baselines to assess generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does increasing the number of hierarchical task types beyond 19,227 yield diminishing returns or performance saturation?
- **Basis in paper:** [inferred] The ablation study demonstrates consistent performance improvements as task types scale from 2k to 19k, but does not identify a ceiling where the model capacity is exhausted.
- **Why unresolved:** The authors did not test task expansion beyond the generated 19,227 types; the trend line suggests potential for further gains, but it is unknown if the model's instruction-following capability would plateau.
- **What evidence would resolve it:** Extending the pipeline to generate 40,000+ task types and observing if the average benchmark score continues to rise or flattens.

### Open Question 2
- **Question:** Can the TaskGalaxy pipeline maintain high-quality data generation when replacing the proprietary GPT-4o with high-performing open-source models?
- **Basis in paper:** [inferred] The methodology relies on GPT-4o for task expansion, filtering, and question-answering, creating a dependency on a closed-source system that may limit reproducibility.
- **Why unresolved:** While the evaluation uses open-source models as judges, the generation phase is exclusively GPT-4o; the paper does not ablate the generator to see if open models can replicate the nuance required for diverse task generation.
- **What evidence would resolve it:** A comparative study generating datasets with open-source MLLMs (e.g., InternVL-2 or LLaVA-OneVision) and comparing the downstream fine-tuning performance against the GPT-4o generated dataset.

### Open Question 3
- **Question:** Does the use of specific open-source models as "referee" filters introduce a systematic bias against task types that lie outside those models' specific competency domains?
- **Basis in paper:** [inferred] The screening stage employs a majority vote of three specific models (GLM-4v, InternVL variants) to reject samples.
- **Why unresolved:** If the referee models are weak in specific areas (e.g., complex math), they will reject valid but difficult samples generated by GPT-4o, potentially creating a "regression to the mean" where only easy tasks survive.
- **What evidence would resolve it:** A human evaluation of the samples rejected by the referee models to calculate the false-negative rate (valid samples incorrectly filtered out).

## Limitations

- Dependency on GPT-4o for task expansion and Q&A generation creates a closed-source bottleneck that limits reproducibility and scalability
- 50.1% of task types have only 1-10 samples, raising concerns about whether rare tasks meaningfully contribute to generalization rather than introducing noise
- No direct human-verified quality assessment provided; "high-quality" samples rely entirely on automated scoring by the three-model ensemble

## Confidence

- **High Confidence**: The average 3.83–4.5 point gains across 16 benchmarks are well-supported by the ablation study showing consistent improvement with increasing task diversity.
- **Medium Confidence**: The hierarchical task expansion mechanism is plausible given GPT-4o's demonstrated capability, but the semantic uniqueness of all 19,227 categories remains unverified without human auditing.
- **Low Confidence**: The filtering pipeline's effectiveness against hallucinations is asserted through example cases rather than systematic error rate measurements across the full dataset.

## Next Checks

1. **Quality Validation**: Sample 100 random Q&A pairs from the final dataset and have human annotators rate alignment quality. Compare against the reported 2/3 model approval threshold to assess if automated scoring correlates with human judgment.

2. **Task-Type Impact**: Replicate the ablation study from Figure 6 but stratify by task-type frequency bands (1-10, 11-50, 51-100, >100 samples). This would reveal whether rare task types contribute positively or merely add noise.

3. **Cross-Model Generalization**: Fine-tune a different multimodal architecture (e.g., Qwen-VL or a newer InternVL version) using TaskGalaxy and measure performance changes. This would test whether gains stem from the dataset's diversity or specific interactions with LLaVA/InternVL architectures.