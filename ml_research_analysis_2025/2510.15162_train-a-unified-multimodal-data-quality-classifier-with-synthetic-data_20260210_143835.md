---
ver: rpa2
title: Train a Unified Multimodal Data Quality Classifier with Synthetic Data
arxiv_id: '2510.15162'
source_url: https://arxiv.org/abs/2510.15162
tags:
- data
- quality
- caption
- document
- interleaved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of filtering high-quality multimodal
  data for training Multimodal Large Language Models (MLLMs), particularly for image-text
  interleaved document data which has been underexplored. The authors propose UniFilter,
  a unified multimodal data quality classifier that uses an MLLM architecture to assess
  both image-text caption and interleaved document data quality.
---

# Train a Unified Multimodal Data Quality Classifier with Synthetic Data

## Quick Facts
- arXiv ID: 2510.15162
- Source URL: https://arxiv.org/abs/2510.15162
- Reference count: 40
- The paper introduces UniFilter, a unified MLLM-based classifier for filtering high-quality multimodal data (both captions and interleaved documents), trained entirely on synthetic data.

## Executive Summary
This paper addresses the challenge of curating high-quality multimodal data for training Multimodal Large Language Models (MLLMs), particularly for interleaved document data which has been underexplored. The authors propose UniFilter, a unified multimodal data quality classifier that uses an MLLM architecture to assess both image-text caption and interleaved document data quality. To overcome the difficulty of obtaining labeled training data, they introduce a semi-synthetic data generation method that leverages raw images and generates corresponding text across four quality levels using proprietary MLLMs. Comprehensive experiments show that MLLMs pre-trained on data filtered by UniFilter achieve significant performance improvements over baseline methods, with +3.1 average improvement on VQA tasks and +1.5 improvement on MMMU benchmark after fine-tuning.

## Method Summary
UniFilter is trained on a semi-synthetic corpus consisting of 80k sample-score pairs (40k captions + 40k interleaved documents) plus 4k MSCOCO/Flickr positives. Images are sampled from DataComp and OBELICS via CLIP clustering, and text is generated at four quality levels (0–3) using a proprietary MLLM. The classifier architecture combines a frozen SigLIP-SO-400M vision encoder (384px) with a Qwen-2.5-0.5B/1.5B LLM backbone. A 2D adaptive average pooling projector compresses visual tokens to 144 tokens per image, which are interleaved with text tokens. A classification head is appended to the LLM, trained with MSE loss to predict scalar quality scores. The same model weights process both caption and interleaved document data.

## Key Results
- UniFilter achieves 87.1% accuracy on the validation set for caption quality classification
- For interleaved document data, UniFilter achieves 84.7% accuracy
- MLLMs pre-trained on data filtered by UniFilter show +3.1 average improvement on VQA tasks and +1.5 improvement on MMMU benchmark after fine-tuning
- UniFilter outperforms CLIP-based and other MLLM-based methods on 5 zero-shot VQA datasets for caption data filtering

## Why This Works (Mechanism)

### Mechanism 1
Synthetic data generation across defined quality levels enables robust multimodal quality classification without human labels. UniFilter creates sample-score pairs by using proprietary MLLMs to generate text at four distinct quality levels (0–3) grounded in real images sampled from DataComp and OBELICS. This 4-level taxonomy (easy negative, medium negative, hard negative, positive) provides clear boundaries for training, allowing the classifier to learn discriminative features across a quality spectrum rather than binary good/bad labels. Core assumption: Synthetic text generated by the proprietary MLLM accurately reflects specified quality requirements and generalizes to real-world noisy multimodal data distributions.

### Mechanism 2
MLLM-based architectures unify quality assessment for both caption pairs and interleaved documents by processing combined visual-textual context. Unlike CLIP-based methods that compute similarity for single image-text pairs, UniFilter uses an MLLM backbone to jointly encode interleaved images and text. A classification head appended to the LLM outputs a quality score, capturing cross-modal coherence and document-level structure. The same model weights process both caption and interleaved data. Core assumption: The MLLM can compress interleaved visual and textual tokens into a representation sufficient for quality scoring while maintaining high throughput.

### Mechanism 3
High-quality filtered pre-training data improves MLLM zero-shot, few-shot, and post-SFT performance across downstream benchmarks. UniFilter curates subsets with higher semantic alignment and lower noise. Pre-training on this data yields better parametric knowledge and in-context learning. Gains persist through SFT, indicating strong pre-training representations facilitate downstream task learning. Core assumption: Filtering to a higher-quality subset (e.g., top 15–30%) preserves sufficient data diversity and does not introduce bias harming generalization.

## Foundational Learning

**Interleaved multimodal documents**: Sequences of images and text paragraphs within a single document. Why needed: UniFilter explicitly targets this data type, which is common in web corpora but poorly handled by pairwise filters like CLIPScore. Quick check: Given a web page with three images and five paragraphs in alternating order, how does UniFilter process it differently than a CLIPScore-based filter?

**Semi-synthetic data generation**: Real images + synthetic text. Why needed: The classifier is trained entirely on this pipeline; understanding trade-offs is critical for reproduction or extension. Quick check: Why does UniFilter sample real images from DataComp/OBELICS instead of using fully synthetic image generation?

**MSE loss for ordinal quality classification**: Using MSE rather than categorical cross-entropy, reflecting ordinal relationships between quality levels 0–3. Why needed: UniFilter uses MSE for scores 0–3 instead of treating them as four discrete classes. Quick check: What is the implication of using MSE for scores 0–3 instead of treating them as four discrete classes?

## Architecture Onboarding

**Component map**: Vision Encoder (SigLIP-SO-400M) -> Visual Projector (2D AvgPool to 144 tokens) -> LLM Backbone (Qwen-2.5-0.5B/1.5B) -> Classification Head (linear layer outputting scalar logit)

**Critical path**: 1) Encode images via SigLIP; tokenize text via LLM embeddings. 2) Interleave image and text tokens (for documents) or concatenate (for captions). 3) Compress visual tokens via AvgPool to 144 tokens/image. 4) Process sequence through LLM backbone. 5) Pass final representation to classification head for quality score prediction.

**Design tradeoffs**: 1.5B LLM improves accuracy but slows inference; 0.5B offers best trade-off (~130 samples/s). AvgPool compression maintains throughput; non-compressive MLPs increase token count. Filtering fraction (15% vs 30%) balances quality vs diversity; 30% generally yields better downstream performance.

**Failure signatures**: Low F1 on hard negatives → subtle misalignment detection is weak. Drops on specific datasets (e.g., Oxford-Pet) → insufficient fine-grained category coverage. Throughput <100 samples/s → projector or resolution misconfiguration.

**First 3 experiments**: 1) Reproduce synthetic data generation using a proprietary MLLM and held-out images; manually inspect label quality. 2) Train UniFilter variants (0.5B vs 1.5B) on synthetic data; compare validation accuracy, F1, and inference speed. 3) Filter DataComp-medium and OBELICS with trained UniFilter; pre-train a small MLLM and evaluate zero-shot/few-shot VQA vs DFN and no-filter baselines.

## Open Questions the Paper Calls Out

**Open Question 1**: Can a fully synthetic multimodal generation pipeline (generating both images and text) effectively train a data quality classifier once image generation models overcome their current style biases? Basis: The authors explicitly rejected fully synthetic data because current SOTA models are "stuck into specific image styles" like cartoons. Unresolved because current generative models lack the visual diversity and realism of web-scraped images. Evidence needed: Training a version of UniFilter using future diverse image generators and comparing performance against the current semi-synthetic baseline.

**Open Question 2**: Does the UniFilter's unified training objective compromise performance on text-heavy or OCR-specific tasks compared to specialized text-quality filters? Basis: UniFilter lags behind the MLMFilter-CTQ metric on TextVQA, and the authors suggest CTQ might be inherently better for "text-rendering related pre-training data." Unresolved because it's unclear if the unified architecture dilutes the signal for specific modalities or if the synthetic training data simply lacked sufficient text-heavy examples. Evidence needed: An ablation study comparing a text-specialized UniFilter variant against the unified model on a broader suite of OCR and document understanding benchmarks.

**Open Question 3**: How does the optimal retention ratio (e.g., 15% vs. 30%) shift when pre-training compute budgets are scaled up significantly beyond the 10B tokens tested? Basis: The Introduction mentions limited compute resources prevent full-epoch training on massive datasets, and Appendix H concludes 30% retention offers the best trade-off between quality and diversity, implying this balance is sensitive to scale. Unresolved because the experiments were limited to a specific compute window (10B tokens). Evidence needed: Pre-training MLLMs with varying retention rates (10%, 30%, 50%) at larger token scales (e.g., 100B+ tokens) to map the interaction between data quantity, quality, and model performance.

**Open Question 4**: Can a general-purpose synthetic training pipeline effectively capture the nuances required for fine-grained visual distinction tasks? Basis: Appendix A highlights a significant performance gap on the Oxford-Pet dataset, which the authors hypothesize is due to a lack of fine-grained category coverage in public datasets compared to proprietary datasets used by baselines like DFN. Unresolved because the paper does not determine if the failure is due to the underlying data sources or if the synthetic generation method fails to produce the subtle descriptive contrasts needed for fine-grained classification. Evidence needed: Enriching the synthetic training data with specific fine-grained examples and re-evaluating performance on specialized benchmarks like Oxford-Pet or Stanford Cars.

## Limitations

**Synthetic Data Dependency**: UniFilter's performance is fundamentally bounded by the quality of its semi-synthetic training corpus. The method assumes that a proprietary MLLM's generated text at each quality level accurately represents real-world noise patterns, but lacks external validation of synthetic-label fidelity.

**Architecture Generalization Gap**: The paper does not explicitly validate that the same model weights generalize equally well across caption pairs and interleaved documents, nor does it test on document types outside OBELICS.

**Filtering Threshold Sensitivity**: The reported downstream gains depend on aggressive filtering (top 15-30% quality). Over-filtering could bias the MLLM toward common concepts and harm generalization to out-of-distribution tasks.

## Confidence

**High Confidence**: 
- Synthetic Data for Quality Levels: The 4-level taxonomy and semi-synthetic generation approach are clearly defined and validated through downstream performance gains.
- Downstream Performance Gains: The reported +3.1 VQA and +1.5 MMMU improvements are substantial and consistent across multiple benchmarks.

**Medium Confidence**:
- MLLM Architecture for Interleaved Data: The claim that an MLLM unifies quality assessment for both modalities is plausible given the architecture, but lacks direct validation on real interleaved documents outside the OBELICS domain.

**Low Confidence**:
- Synthetic Data Fidelity: The paper assumes synthetic labels generalize to real data but provides no external validation. The proprietary nature of the MLLM and lack of human studies make this claim difficult to verify.

## Next Checks

1. **Synthetic Data Fidelity Audit**: Manually annotate a small set of real noisy multimodal samples (caption pairs and interleaved documents) and compare UniFilter's scores against human judgments. This will reveal whether the 4-level synthetic taxonomy aligns with human-perceived quality and identify domain gaps.

2. **Cross-Domain Interleaved Document Test**: Evaluate UniFilter on a new interleaved document dataset from a different domain (e.g., scientific papers, product manuals) not seen in OBELICS. Measure classification accuracy and downstream MLLM performance to test generalization beyond the training distribution.

3. **Diversity Impact Analysis**: Train MLLMs on UniFilter-filtered data at varying filtering thresholds (e.g., 10%, 30%, 50%) and evaluate on a rare-concept benchmark (e.g., A-OKVQA or a custom rare-object detection task). This will quantify the tradeoff between quality and diversity, and identify the optimal filtering fraction for balanced performance.