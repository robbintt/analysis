---
ver: rpa2
title: 'Read as You See: Guiding Unimodal LLMs for Low-Resource Explainable Harmful
  Meme Detection'
arxiv_id: '2506.08477'
source_url: https://arxiv.org/abs/2506.08477
tags:
- harmful
- meme
- content
- u-cot
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces U-CoT+, a low-resource harmful meme detection
  framework that converts multimodal memes into unimodal text-based inputs using a
  high-fidelity meme-to-text pipeline. It leverages lightweight unimodal LLMs for
  zero-shot inference, guided by interpretable human-crafted guidelines to enable
  explainable and context-adaptive reasoning.
---

# Read as You See: Guiding Unimodal LLMs for Low-Resource Explainable Harmful Meme Detection

## Quick Facts
- **arXiv ID:** 2506.08477
- **Source URL:** https://arxiv.org/abs/2506.08477
- **Reference count:** 40
- **Primary result:** Achieves highly competitive zero-shot harmful meme detection performance using lightweight unimodal LLMs, guided by human-crafted guidelines for explainability and resource efficiency.

## Executive Summary
This paper introduces U-CoT+, a low-resource harmful meme detection framework that converts multimodal memes into unimodal text-based inputs using a high-fidelity meme-to-text pipeline. It leverages lightweight unimodal LLMs for zero-shot inference, guided by interpretable human-crafted guidelines to enable explainable and context-adaptive reasoning. Extensive experiments on seven benchmark datasets show U-CoT+ achieves performance highly competitive with, and in some cases surpassing, resource-intensive baselines, demonstrating strong resource efficiency, flexibility, scalability, and explainability.

## Method Summary
U-CoT+ converts multimodal memes to unimodal text descriptions through a High-Fidelity Meme2Text pipeline. This pipeline uses a lightweight LMM (e.g., LLaVA1.6-7B) to answer atomic visual questions (identifying humans, race, gender, etc.) and generate a general description. A unimodal LLM then synthesizes these outputs into a coherent description. For classification, another unimodal LLM performs zero-shot Chain-of-Thought reasoning over the description, guided by human-crafted, context-specific guidelines. The framework supports ensemble predictions from different LMM/LLM combinations to improve robustness.

## Key Results
- U-CoT+ achieves accuracy and macro-F1 scores competitive with, and sometimes exceeding, resource-intensive baselines across seven harmful meme detection benchmarks.
- The framework demonstrates strong resource efficiency by operating in a zero-shot setting with lightweight 7B models, requiring no task-specific training.
- Guideline-enhanced reasoning significantly improves detection accuracy and provides interpretable rationales for predictions.
- Ensemble strategies can further improve performance, particularly on datasets like MAMI and PrideMM.

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Visual Recognition and Textual Reasoning
The framework effectively separates the error-prone visual recognition task from the subsequent reasoning task by converting memes to text descriptions first. This reduces complexity and allows lightweight models to focus on their strengths. The core assumption is that critical information for harmfulness assessment can be captured through targeted visual questions without exhaustive image description. Evidence shows this decoupling improves performance compared to direct multimodal reasoning. The break condition is when visual cues critical to harmfulness are not captured by the VQA questions.

### Mechanism 2: Guideline-Enhanced Zero-Shot CoT Reasoning
Human-crafted guidelines significantly improve lightweight LLMs' zero-shot harmful meme detection accuracy and interpretability. The guidelines provide explicit criteria for harmfulness (e.g., identifying protected groups, implicit harmful content patterns) that the LLM applies systematically during step-by-step reasoning. The core assumption is that human-crafted guidelines can effectively transfer commonsense knowledge about harmfulness criteria to LLMs. Evidence includes improved performance over unguided reasoning and qualitative examples of correct interpretations. The break condition is when guidelines are poorly crafted or do not match the specific sociocultural context.

### Mechanism 3: Ensemble and Robustness via Multi-Source Predictions
Aggregating predictions from different LMM sources and reasoning schemes mitigates uncertainty and balances over-censorship tendencies. Different LMMs have varying strengths in visual recognition; ensembling helps balance their biases. The ensemble strategy also considers guideline confidence levels. The core assumption is that errors from different LMMs and schemes are somewhat independent, and appropriate aggregation rules can improve overall reliability. Evidence shows ensemble results improving on specific datasets. The break condition is when ensemble aggregation rules are poorly designed or all base models make correlated errors.

## Foundational Learning

- **Concept: Visual Question Answering (VQA)**
  - **Why needed here:** The core of the Meme2Text pipeline uses VQA to extract specific visual information from meme images. Understanding VQA is essential for grasping how the system converts images to text.
  - **Quick check question:** Can you explain why a VQA model might struggle to identify sensitive attributes (e.g., race) in images due to safety alignment?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** U-CoT+ relies on zero-shot CoT prompting to elicit step-by-step reasoning from LLMs. The "guided" aspect adds external rules to this process.
  - **Quick check question:** How does adding explicit guidelines to a CoT prompt differ from standard few-shot CoT?

- **Concept: Zero-Shot Inference with LLMs**
  - **Why needed here:** The entire framework operates in a zero-shot settingâ€”no task-specific training. This requires understanding how to prompt LLMs effectively without examples.
  - **Quick check question:** What are the main challenges of zero-shot inference for a subjective, context-dependent task like harmful meme detection?

## Architecture Onboarding

- **Component map:** Meme Image -> LMM (VQA) -> LLM (Integration) -> LLM (Guided CoT) -> Prediction
- **Critical path:** Meme Image -> LMM (VQA) -> LLM (Integration) -> LLM (Guided CoT) -> Prediction. The VQA question design and guideline crafting are the most critical and manual components.
- **Design tradeoffs:**
  - **7B LMM vs. Larger LMM:** 7B models are resource-efficient but may miss visual details or hallucinate. The paper shows 13B LMM offers limited visual advantage over 7B for this task.
  - **Guideline specificity vs. generality:** Highly specific guidelines improve accuracy on a target dataset but reduce transferability. The paper crafts guidelines per dataset.
  - **Ensemble vs. Single Best:** Ensembling adds complexity but can improve robustness, especially when single models have complementary strengths.
- **Failure signatures:**
  - **Incorrect/missing visual details:** LMM fails to recognize a critical element (e.g., Anne Frank in Figure 4a), leading to misclassification.
  - **Excessive censorship:** LLM applies guidelines too rigidly, over-interpreting satire as harmful.
  - **Target mismatch:** LLM identifies wrong target group, applying incorrect guidelines (e.g., Figure 4e targets Biden but LLM sees LGBTQ+ community).
  - **Blind spot:** Guidelines do not cover a specific harmfulness type present in the meme (e.g., vegan group in PrideMM context).
  - **Annotation errors:** Discrepancy between LLM's reasonable interpretation and dataset label (Figure 16).
- **First 3 experiments:**
  1. **Baseline comparison:** Evaluate U-CoT+ (with a single LMM+LLM pair and default guidelines) on one benchmark dataset (e.g., FHM) against GPT-4o-mini zero-shot and a simple M-CoT baseline to establish the performance gain.
  2. **Ablation: Meme2Text pipeline:** Compare the performance of U-CoT (no guidelines) using meme descriptions from the proposed VQA-based pipeline vs. direct GPT-4o-mini descriptions to validate the pipeline's effectiveness despite using lighter models.
  3. **Ablation: Guideline importance:** Compare U-CoT+ (with guidelines) vs. U-CoT (without guidelines) and vs. a few-shot CoT baseline on the same dataset to quantify the contribution of the guidelines versus in-context learning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the manual guideline-crafting process be automated using LLMs while maintaining the alignment and performance achieved by human-crafted rules?
- Basis in paper: Section F lists "future efforts toward automatic guideline generation using LLMs" as a key limitation to address.
- Why unresolved: The current method relies on humans to observe dataset patterns and summarize them into coherent logic; it is unclear if LLMs can autonomously distill context-specific nuances into effective, interpretable guidelines without hallucinating rules.
- What evidence would resolve it: Developing an automated pipeline that generates guidelines from data and benchmarking its zero-shot detection accuracy against the manual U-CoT+ baseline.

### Open Question 2
- Question: How does the U-CoT+ framework generalize to culturally diverse, non-English meme datasets (e.g., Mandarin or Hindi)?
- Basis in paper: Section F states the authors plan to "extend our investigation to culturally diverse meme datasets (e.g., Mandarin, Hindi) as they become more widely available."
- Why unresolved: The paper relies on English benchmarks where lightweight LLMs possess strong linguistic reasoning; cross-cultural memes often rely on specific local visual metaphors or vernacular that these models may not grasp.
- What evidence would resolve it: Evaluating U-CoT+ on emerging multilingual meme benchmarks to assess if the "reading" capability transfers across cultural contexts.

### Open Question 3
- Question: To what extent do hallucinations in the lightweight LMM-based Meme2Text stage negatively impact the final harmfulness prediction?
- Basis in paper: Section 6 identifies "Incorrect or missing visual details" as a major error type, and Section F notes "Future work will manually assess the robustness of our Meme2Text pipeline to hallucination."
- Why unresolved: The study lacks ground-truth visual descriptions to quantify how often the 7B LMMs hallucinate critical details, and whether these hallucinations systematically bias the final classification.
- What evidence would resolve it: Creating a dataset with ground-truth visual descriptions to isolate and measure the performance degradation caused by errors in the Meme2Text stage alone.

## Limitations
- The framework's performance critically depends on the quality of the High-Fidelity Meme2Text pipeline, which uses 7B LMMs that may struggle with complex or ambiguous visual cues.
- Human-crafted guidelines introduce potential bias and may not generalize well across diverse sociocultural contexts beyond the seven benchmark datasets tested.
- The reliance on OCR for overlaid text is mentioned but not specified, introducing potential variability in reproduction.

## Confidence
- **High Confidence:** The core methodology of converting multimodal memes to unimodal text for zero-shot inference with lightweight models is well-defined and reproducible. The empirical results showing competitive performance against resource-intensive baselines are supported by extensive experiments across seven datasets.
- **Medium Confidence:** The effectiveness of the guideline-enhanced CoT reasoning is supported by qualitative examples and ablation studies, but the generalizability of the guidelines across diverse meme types and cultural contexts requires further validation. The ensemble strategy shows promise but lacks comprehensive analysis of when and why it helps.
- **Low Confidence:** The robustness of the system to adversarial examples or memes with subtle, context-dependent harmful content that requires nuanced cultural understanding is not thoroughly explored. The paper's discussion of "excessive censorship" as a failure mode suggests potential brittleness in guideline application.

## Next Checks
1. **Cross-Dataset Guideline Transferability:** Apply the FHM guidelines to a subset of the PrideMM dataset (or vice versa) to quantify performance degradation and identify guideline-specific vs. dataset-specific knowledge. This will test the generalizability claim.
2. **Error Analysis on Visual Recognition:** Systematically analyze the VQA outputs from the 7B LMMs on a set of memes where the final classification was incorrect. Categorize errors by type (e.g., identity recognition failure, attribute misclassification, hallucination) to understand the pipeline's failure modes.
3. **Ablation Study on Guideline Granularity:** Create a simplified version of the guidelines (e.g., removing the "Patterns" and "Exception" principles) and compare performance to the full guidelines on one dataset. This will quantify the contribution of each guideline component and inform future guideline design.