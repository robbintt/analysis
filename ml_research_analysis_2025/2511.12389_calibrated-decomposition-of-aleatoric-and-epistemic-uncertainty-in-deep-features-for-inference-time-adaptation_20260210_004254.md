---
ver: rpa2
title: Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features
  for Inference-Time Adaptation
arxiv_id: '2511.12389'
source_url: https://arxiv.org/abs/2511.12389
tags:
- uncertainty
- epistemic
- aleatoric
- feature
- components
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Uncertainty-Guided Inference-Time Model Selection addresses the
  challenge of conflating aleatoric and epistemic uncertainty in deep visual inference
  systems. The framework disentangles these uncertainty modes using global feature
  density deviation for aleatoric uncertainty and three local geometric statistics
  (support deficiency, spectral collapse, cross-layer inconsistency) for epistemic
  uncertainty, all computed directly in semantic feature space without sampling or
  ensembling.
---

# Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation

## Quick Facts
- arXiv ID: 2511.12389
- Source URL: https://arxiv.org/abs/2511.12389
- Reference count: 40
- Primary result: Disentangles aleatoric and epistemic uncertainty in feature space to guide adaptive model selection, achieving ~60% computational savings with negligible accuracy loss.

## Executive Summary
This work introduces a framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty in deep feature space, enabling efficient inference-time model selection with calibrated prediction intervals. The method computes uncertainty directly from frozen encoder features using global density deviation for aleatoric uncertainty and three local geometric statistics for epistemic uncertainty, without requiring sampling or ensembling. Experiments across pedestrian tracking benchmarks demonstrate that the decomposed uncertainties remain orthogonal while tracking distinct error modes, enabling compute-adaptive model selection that saves ~60% computational cost with minimal accuracy degradation.

## Method Summary
The framework estimates aleatoric uncertainty using regularized Mahalanobis distance from a global feature density model, while epistemic uncertainty is formed from three complementary components: local support deficiency (kNN distance), manifold spectral collapse (effective rank of local covariance), and cross-layer feature inconsistency (cosine similarity divergence). These uncertainties are combined through a quadrature sum to form a unified nonconformity score, which feeds a distribution-free conformal calibration procedure to produce prediction intervals. A model selection policy (threshold-based or RL) uses the gap between aleatoric and epistemic uncertainties to decide whether to escalate to larger models, reducing computational load while maintaining accuracy.

## Key Results
- Uncertainty components remain orthogonal with correlation |r| < 0.3 while tracking distinct error modes
- Achieves approximately 60% computational savings on MOT17 with negligible accuracy loss
- Improves margins by 13.6 percentage points over total-uncertainty baselines
- Maintains 90% coverage with 30% narrower prediction intervals compared to confidence-based calibration

## Why This Works (Mechanism)

### Mechanism 1: Global Feature Density Deviation for Aleatoric Uncertainty
Aleatoric uncertainty is estimated by measuring deviation from a regularized global feature density using Mahalanobis distance. A frozen encoder maps inputs to feature space, and the method calculates the Mahalanobis distance of test features from a global mean and covariance estimated from calibration data. This detects when observation degradation (blur, occlusion) perturbs features away from the dense global manifold. The approach assumes noisy inputs appear as outliers in the global distribution rather than forming distinct clusters.

### Mechanism 2: Local Geometric Statistics for Epistemic Uncertainty
Epistemic uncertainty is estimated by combining local support deficiency, geometric collapse, and cross-layer inconsistency without ensembling. The method aggregates three scores: kNN distance aggregation to measure support, spectral entropy of local covariance matrix to detect geometric collapse (low effective rank), and cosine similarity divergence across encoder layers. This captures when unfamiliar or OOD inputs result in sparse neighborhoods and degenerate local geometry in feature space, distinct from the global density deviation caused by noise.

### Mechanism 3: Disentangled Uncertainties for Adaptive Model Selection
Disentangled uncertainties enable efficient conformal calibration and compute-adaptive model selection. A unified nonconformity score constructed from both uncertainty types feeds a distribution-free conformal procedure to tighten prediction intervals. Simultaneously, a policy uses the gap between uncertainties to decide whether to escalate to larger models, based on the assumption that epistemic uncertainty is reducible while aleatoric uncertainty is not, making compute increases only worthwhile when epistemic uncertainty is high relative to aleatoric.

## Foundational Learning

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The framework relies on the premise that these two uncertainty types require different handling (one is irreducible noise, the other is fixable ignorance)
  - Quick check question: Does a blurry image indicate high aleatoric or high epistemic uncertainty? (Answer: High Aleatoric)

- **Concept: Conformal Prediction**
  - Why needed here: Converts raw uncertainty heuristics into statistically rigorous prediction intervals with guaranteed coverage probability
  - Quick check question: Does conformal prediction assume a specific data distribution (e.g., Gaussian) to provide coverage guarantees? (Answer: No, it is distribution-free)

- **Concept: Manifold Geometry in Deep Learning**
  - Why needed here: Understanding data lying on lower-dimensional manifolds in feature space is necessary to grasp why "spectral collapse" or "effective rank" indicates model failure
  - Quick check question: If local covariance matrix of feature neighbors has low effective rank, does this suggest features are well-distributed or collapsing? (Answer: Collapsing/degenerate)

## Architecture Onboarding

- **Component map:** Frozen Encoder -> Aleatoric Estimator -> Epistemic Estimator -> Calibration Layer -> Selection Policy
- **Critical path:** Inference latency is dominated by kNN search and local covariance computation for epistemic score (adds <1ms, contingent on efficient approximate nearest neighbor libraries)
- **Design tradeoffs:**
  - Accuracy vs. Orthogonality: Component weights must be tuned to minimize correlation with aleatoric score; fixed weights may fail on new datasets without tuning
  - Calibration vs. Sharpness: Local adaptive scaling tightens intervals but requires sufficient calibration data per leaf to avoid coverage failure
- **Failure signatures:**
  - High Correlation: If corr(σ_alea, σ_epis) rises significantly above 0.1, decomposition has failed due to dataset-specific feature distributions
  - Conservative Intervals: Excessively wide prediction intervals despite low uncertainty scores suggest skewed conformal quantiles from outliers
- **First 3 experiments:**
  1. Orthogonality Validation: Scatter plot σ_alea vs. σ_epis on validation split; target: |r| < 0.1
  2. Coverage Test: Run conformal calibration on held-out test set to verify if prediction interval coverage matches target 1-α
  3. Compute-Accuracy Curve: Execute adaptive model selection on MOT17 and plot FLOPs/latency vs. Tracking Accuracy against fixed-model baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can the decomposed uncertainty framework maintain orthogonality and calibration when the feature encoder itself is updated or fine-tuned online, rather than remaining frozen? The method operates on a frozen encoder with cached calibration features, but does not address whether uncertainty estimates remain valid if encoder representations shift during deployment. Evidence would require experiments with gradual fine-tuning or domain adaptation measuring degradation in orthogonality and coverage.

### Open Question 2
How does the uncertainty decomposition generalize to non-pedestrian vision domains such as medical imaging, satellite imagery, or industrial inspection, where failure modes and noise characteristics differ substantially? All evaluation is conducted on pedestrian tracking benchmarks, leaving cross-domain generalization untested. Evidence would require evaluation on benchmarks from at least two distinct domains, reporting orthogonality coefficients, calibration metrics, and compute-accuracy tradeoffs.

### Open Question 3
Can temporally aware conformal methods improve upon the current frame-wise calibration, particularly for video tasks where uncertainty exhibits temporal autocorrelation? The conclusion states future work will extend these ideas to temporally aware conformal methods. The current conformal procedure treats each detection independently, potentially underutilizing sustained uncertainty regimes in video data. Evidence would require comparison of frame-wise versus sequence-aware calibration, measuring interval width, coverage stability, and policy switching behavior.

### Open Question 4
What are the failure modes of the RL-based model selection policy when faced with adversarially crafted inputs designed to manipulate uncertainty estimates? The framework uses uncertainty to guide resource allocation, but no analysis examines whether adversaries can trigger unnecessary escalation or induce harmful de-escalation by perturbing features. Evidence would require adversarial robustness tests measuring policy behavior under feature-space perturbations optimized to inflate or suppress specific uncertainty components.

## Limitations
- Effectiveness contingent on assumption that aleatoric and epistemic uncertainty manifest as distinct geometric patterns in feature space; overlapping regions may cause decomposition failure
- RL policy performance tied to specific model pool; extrapolation to architectures outside YOLOv8/DINO/RT-DETR is untested
- Calibration cache requires representative training data; domain shifts could invalidate global statistics and kNN references

## Confidence

- **High Confidence**: Orthogonality of uncertainty components (corr < 0.3) and 90% conformal coverage are directly measurable from experimental results and align with theoretical expectations
- **Medium Confidence**: Computational savings (~60% on MOT17) are reported but depend on specific model selection policy and dataset characteristics; generalization to other video benchmarks or non-visual domains is plausible but unverified
- **Low Confidence**: Exact mechanism for optimizing epistemic component weights is not detailed; method's robustness to adversarial perturbations is not discussed

## Next Checks

1. **Orthogonality Validation**: Compute Pearson correlation between σ_alea and σ_epis on held-out validation set; verify |r| < 0.1 to confirm decomposition is functioning as intended

2. **Coverage Calibration**: Run conformal prediction on test set and calculate empirical coverage rate; confirm it meets or exceeds target 1-α (e.g., 90%) to validate calibration layer

3. **Cross-Dataset Generalization**: Apply trained model selection policy from MOT17 to new dataset (e.g., DanceTrack); measure change in compute savings and accuracy to assess robustness to domain shifts