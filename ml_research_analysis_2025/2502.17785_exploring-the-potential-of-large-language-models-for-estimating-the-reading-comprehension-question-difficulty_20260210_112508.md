---
ver: rpa2
title: Exploring the Potential of Large Language Models for Estimating the Reading
  Comprehension Question Difficulty
arxiv_id: '2502.17785'
source_url: https://arxiv.org/abs/2502.17785
tags:
- difficulty
- reading
- comprehension
- question
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the use of Large Language Models (LLMs),
  specifically GPT-4o and o1, for estimating the difficulty of reading comprehension
  questions. Traditional methods like Item Response Theory (IRT) require extensive
  human annotation and large-scale testing, limiting their scalability.
---

# Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty

## Quick Facts
- **arXiv ID**: 2502.17785
- **Source URL**: https://arxiv.org/abs/2502.17785
- **Reference count**: 31
- **Primary result**: LLMs (GPT-4o and o1) achieved 98.84-99.14% accuracy on reading comprehension questions compared to 87.57% for human students

## Executive Summary
This study investigates whether Large Language Models can effectively estimate reading comprehension question difficulty, potentially offering a scalable alternative to traditional psychometric methods. The research compares LLM performance (GPT-4o and o1) against human students using the SARA dataset, finding that LLMs achieved near-perfect accuracy rates of 98.84-99.14% versus 87.57% for human students. The study also examines how LLM-generated difficulty estimates align with traditional Item Response Theory (IRT) parameters, demonstrating that LLMs can produce difficulty estimates consistent with established psychometric approaches while offering greater scalability and efficiency.

## Method Summary
The study employs a two-pronged approach to evaluate LLM capabilities for reading comprehension difficulty assessment. First, it directly compares LLM performance on the SARA dataset against human student performance across various reading comprehension subtests. Second, it uses IRT-based analysis to examine how LLM-generated responses correlate with traditional difficulty parameters, allowing for comparison between AI-driven and psychometric difficulty estimation methods. The research specifically uses GPT-4o and o1 models, analyzing their accuracy rates and comparing their difficulty estimates with those derived from IRT analysis of human student responses.

## Key Results
- LLMs achieved 98.84% (GPT-4o) and 99.14% (o1) accuracy versus 87.57% for human students on reading comprehension questions
- Both LLMs demonstrated superior performance across all reading comprehension subtests compared to human students
- LLM-generated difficulty estimates aligned with traditional IRT analysis, though showing different sensitivities to extreme item characteristics

## Why This Works (Mechanism)
The success of LLMs in estimating reading comprehension difficulty appears to stem from their ability to process and understand complex textual relationships at scale, combined with their capacity to recognize patterns in question structures and content that correlate with difficulty levels. Unlike human students who may be influenced by fatigue, test anxiety, or varying levels of prior knowledge, LLMs provide consistent performance across large question sets. Their pattern recognition capabilities allow them to identify subtle linguistic and structural features that make questions more or less challenging, potentially capturing difficulty factors that traditional IRT methods might miss or require extensive human annotation to identify.

## Foundational Learning
- **Item Response Theory (IRT)**: A psychometric framework for analyzing how test-takers of different abilities respond to test items; needed to establish baseline difficulty estimates and validate LLM performance against established standards; quick check: examine item characteristic curves and parameter estimates
- **Reading comprehension assessment frameworks**: Understanding the components of reading comprehension (literal, inferential, evaluative) that questions target; needed to interpret subtest performance differences; quick check: review question classification schema used in SARA dataset
- **Large Language Model capabilities**: Understanding how LLMs process and reason about textual information; needed to contextualize their performance advantages; quick check: compare token-level processing patterns with human reading strategies
- **Psychometric validation methods**: Techniques for comparing AI-generated estimates with human-based measurements; needed to assess the reliability of LLM difficulty estimates; quick check: examine correlation coefficients and agreement metrics
- **Educational assessment scalability challenges**: Understanding limitations of traditional testing methods; needed to appreciate the value proposition of LLM-based approaches; quick check: analyze time and cost requirements for traditional vs. AI-based difficulty estimation

## Architecture Onboarding

**Component Map**: SARA Dataset -> LLM Models (GPT-4o, o1) -> Response Analysis -> IRT Parameter Estimation -> Difficulty Comparison

**Critical Path**: Data preprocessing and question formatting → LLM inference and response generation → Performance evaluation against human benchmarks → IRT analysis of response patterns → Difficulty parameter extraction and comparison

**Design Tradeoffs**: The study prioritizes accuracy and alignment with established psychometric methods over computational efficiency, accepting the resource requirements of running large LLMs to achieve superior performance. This tradeoff favors methodological rigor and scalability potential over immediate practical deployment considerations.

**Failure Signatures**: If LLMs were to fail at estimating difficulty accurately, we would expect to see: significant deviations between LLM and IRT difficulty parameters, poor correlation between LLM performance and established difficulty metrics, inconsistent performance across different reading comprehension subtests, or failure to generalize beyond the specific dataset used.

**First 3 Experiments**:
1. Compare LLM difficulty estimates across multiple independent reading comprehension datasets to test generalizability
2. Implement cross-validation with expert human rater assessments to validate LLM estimates
3. Test LLM performance on deliberately constructed "edge case" questions designed to challenge traditional difficulty estimation methods

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Reliance on a single dataset (SARA) limits generalizability across different educational contexts and grade levels
- Exceptional LLM performance raises questions about whether models replicate human cognitive processes or merely leverage pattern recognition
- Lack of contextual information about student demographics and testing conditions makes direct comparisons potentially misleading

## Confidence
- **High confidence**: Technical implementation of IRT analysis and basic comparison between LLM and human performance metrics
- **Medium confidence**: Alignment between LLM-generated difficulty estimates and traditional IRT parameters
- **Low confidence**: Generalizability of findings to real-world educational settings and diverse student populations

## Next Checks
1. Replicate the analysis using multiple, diverse reading comprehension datasets from different educational systems and grade levels to test generalizability
2. Conduct controlled experiments comparing LLM difficulty estimates with expert human rater assessments across the same question sets
3. Implement the LLM-based difficulty estimation in a small-scale adaptive testing environment with actual students to evaluate practical utility and identify potential biases or limitations not apparent in offline analysis