---
ver: rpa2
title: 'HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong'
arxiv_id: '2507.11502'
source_url: https://arxiv.org/abs/2507.11502
tags:
- hong
- alignment
- kong
- hkgai-v1
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HKGAI-V1 is a 685-billion-parameter sovereign large language model
  tailored for Hong Kong's multilingual and culturally specific context. Built on
  DeepSeek architecture, it employs full-parameter fine-tuning, retrieval-augmented
  generation, and advanced alignment techniques including reinforcement learning from
  human feedback and language feedback modeling.
---

# HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong

## Quick Facts
- **arXiv ID:** 2507.11502
- **Source URL:** https://arxiv.org/abs/2507.11502
- **Reference count:** 40
- **Primary result:** 685B-parameter sovereign LLM achieving 81.4% on HKMMLU, 88.95% on Beaver-zh-hk, and 80.1% on SafeLawBench

## Executive Summary
HKGAI-V1 is a 685-billion-parameter large language model designed to serve as a sovereign AI system tailored for Hong Kong's unique multilingual (Cantonese, Mandarin, English) and socio-legal context under the "one country, two systems" framework. Built on DeepSeek architecture, it employs full-parameter fine-tuning, retrieval-augmented generation, and advanced alignment techniques including reinforcement learning from human feedback and language feedback modeling. The model demonstrates state-of-the-art performance on culturally specific benchmarks while maintaining strong general capabilities, establishing a new standard for regional AI alignment that balances local values with broader functionality.

## Method Summary
HKGAI-V1 is built on DeepSeek (67B MoE variant implied) and employs full-parameter fine-tuning with RLHF using Bradley-Terry reward modeling and KL penalty. Learning from Language Feedback (LLF) is integrated for feedback modeling and self-evolving refinement. A proprietary HKValue-Aligner uses correction-based learning from a Q-A-C dataset to synthesize value-aligned preferences. The model incorporates modular RAG with intent classification, query enhancement, retrieval, moderation, and answer generation components. Training follows a weak-to-strong generalization approach, combining these techniques to achieve both capability and value alignment specific to Hong Kong's cultural and legal context.

## Key Results
- Achieved 81.4% accuracy on HKMMLU, demonstrating strong performance on Hong Kong-specific multilingual understanding
- Scored 88.95% on Beaver-zh-hk harmless evaluation, showing effective alignment with local cultural values
- Attained 80.1% on SafeLawBench, indicating robust performance on legal and safety-sensitive tasks

## Why This Works (Mechanism)
The model's effectiveness stems from its multi-layered alignment approach that combines traditional RLHF with innovative techniques like Learning from Language Feedback and the HKValue-Aligner. This architecture allows for iterative refinement of both capabilities and values, while the modular RAG system provides dynamic access to proprietary knowledge sources. The weak-to-strong generalization strategy enables the model to maintain broad competence while developing specialized alignment with Hong Kong's unique cultural and legal requirements.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Essential for aligning model outputs with human preferences and values; quick check: monitor reward model convergence and KL divergence during training
- **Learning from Language Feedback (LLF)**: Enables iterative refinement through critique-based feedback loops; quick check: measure improvement in response quality after LLF iterations
- **Bradley-Terry Reward Modeling**: Provides pairwise comparison framework for preference learning; quick check: validate reward model consistency across diverse prompt types
- **Weak-to-Strong Generalization**: Allows effective fine-tuning from smaller aligned models to larger ones; quick check: compare performance of weak-to-strong vs direct fine-tuning approaches
- **Modular RAG Architecture**: Enables specialized handling of retrieval, moderation, and generation tasks; quick check: measure RAG module performance in isolation

## Architecture Onboarding
**Component Map:** Base Model -> SFT -> RLHF (BT reward + KL penalty) -> LLF -> HKValue-Aligner -> Modular RAG -> Evaluation

**Critical Path:** The sequence Base Model -> SFT -> RLHF -> LLF -> HKValue-Aligner forms the core alignment pipeline, with RAG integration providing dynamic knowledge access throughout.

**Design Tradeoffs:** The model prioritizes deep regional value alignment over general instruction robustness, accepting increased vulnerability to adversarial attacks in exchange for stronger local value compliance. The proprietary nature of HK-specific datasets and knowledge sources limits reproducibility but ensures authentic cultural alignment.

**Failure Signatures:** Reward hacking during RLHF manifests as capability degradation; cross-lingual inconsistency appears as value alignment variance across Cantonese, Mandarin, and English; RAG failures show as knowledge gaps in culturally specific domains.

**First Experiments:**
1. Train BT reward model on synthetic HK preference pairs and validate pairwise preference accuracy
2. Implement single-step LLF refinement and measure quality improvement on HK-sensitive prompts
3. Test HKValue-Aligner on Q-A-C dataset and evaluate synthesized preference quality

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the trade-off between deep regional value alignment and vulnerability to general instruction attacks be resolved? The authors identify this as a "classic tension between compliance and robustness" that remains a central challenge in AI alignment requiring further research.

- **Open Question 2:** How can implicit value encodings inherent in pre-trained base models be effectively quantified and mitigated during regional fine-tuning? The paper suggests current techniques present their own challenges in reconciling diverse data without compromising local values.

- **Open Question 3:** How can precise cross-lingual value alignment be achieved across Cantonese, Mandarin, and English to ensure a consistent representation of Hong Kong values? The authors note that ensuring cross-lingual consistency is difficult and demands further research in sociolinguistics and culturally sensitive AI development.

## Limitations
- Lack of specific training details including exact base model variant, hyperparameters, and compute resources
- Proprietary nature of HK-specific datasets (Q-A-C, preference pairs) and RAG knowledge sources limits reproducibility
- No detailed error analysis or failure case studies for performance on HK-sensitive queries

## Confidence
- **High confidence**: General methodological approach (DeepSeek base, RLHF with BT reward modeling, LLF, RAG integration)
- **Medium confidence**: Benchmark results (81.4% HKMMLU, 80.0% SafeLawBench, 88.95% Beaver-zh-hk)
- **Low confidence**: Claims about "state-of-the-art" performance and specific capability thresholds

## Next Checks
1. Replicate the training pipeline using open-source DeepSeek weights and publicly available Chinese/Cantonese datasets, then measure performance on HKMMLU and SafeLawBench
2. Conduct ablation studies to isolate the contribution of each component (RLHF, LLF, HKValue-Aligner, RAG)
3. Evaluate cross-lingual consistency by testing the model on identical prompts in Cantonese, Mandarin, and English and measuring value alignment variance