---
ver: rpa2
title: 'Keywords are not always the key: A metadata field analysis for natural language
  search on open data portals'
arxiv_id: '2509.14457'
source_url: https://arxiv.org/abs/2509.14457
tags:
- metadata
- dataset
- data
- search
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how metadata field design affects conversational
  dataset search. It conducts an ablation study using 255 datasets from the London
  Datastore and 765 synthetic natural language queries, evaluating retrieval performance
  across different metadata configurations.
---

# Keywords are not always the key: A metadata field analysis for natural language search on open data portals

## Quick Facts
- arXiv ID: 2509.14457
- Source URL: https://arxiv.org/abs/2509.14457
- Reference count: 40
- Primary result: Dataset descriptions significantly outperform keywords and topic fields for conversational dataset search

## Executive Summary
This study investigates how metadata field design affects conversational dataset search through an ablation study using 255 datasets from the London Datastore and 765 synthetic natural language queries. The research evaluates retrieval performance across different metadata configurations, revealing that LLM-generated descriptions achieve MRR of 0.925 compared to 0.820 for publisher-provided descriptions and all other metadata types. Topic fields proved largely ineffective while keyword-based configurations provided only moderate performance. The findings suggest open data portals should prioritize rich narrative metadata and leverage LLMs to generate semantically dense descriptions that better align with user intent.

## Method Summary
The research conducted an ablation study evaluating different metadata field combinations for conversational dataset search. Using 255 London Datastore datasets and 765 synthetic natural language queries (categorized as Requesting, Describing, and Implying), the study tested various metadata configurations including original publisher descriptions, LLM-generated descriptions, keywords, and topic fields. Retrieval performance was measured using Mean Reciprocal Rank (MRR) with vector embeddings from BAAI/bge-base-en model stored in FAISS. The LLM-generated descriptions were created using column headers and sample rows as input to normalize heterogeneous metadata into consistent narrative descriptions.

## Key Results
- Dataset descriptions are the most critical metadata field, with LLM-generated descriptions achieving MRR of 0.925 versus 0.820 for publisher descriptions
- Topic fields proved largely ineffective, scoring near zero MRR in ablation tests
- Keyword-based configurations provided only moderate performance compared to narrative descriptions
- LLM-generated descriptions maintained high performance even for "Implying" queries, while keyword configurations dropped significantly

## Why This Works (Mechanism)

### Mechanism 1
Dense narrative descriptions bridge the semantic gap between ambiguous user intent and structured data better than sparse keywords. Vector embedding models map natural language queries and narrative descriptions into a shared latent space, allowing semantic context (e.g., "seasonal trends" or "urban heat islands") to align vectorially with conceptual meaning even when specific terms differ. The retrieval model must capture semantic similarity rather than just lexical overlap.

### Mechanism 2
LLMs function as semantic normalizers, converting heterogeneous raw data structures into consistent, queryable narratives. Publisher-provided metadata is often inconsistent or sparse, so the LLM ingests structural signals (column headers, sample rows) and implicit context to generate standardized descriptions that map dataset schema to natural language concepts users are likely to use in queries.

### Mechanism 3
Ablation of specific metadata fields reveals that "Topic" and "Keyword" fields act as weak signals for conversational intent compared to full descriptions. In the vector space, high-dimensional dense text provides richer similarity search signals than low-dimensional categorical metadata. Topic fields likely function as coarse filters that fail to capture the nuance required for complex query types.

## Foundational Learning

- **Dense Retrieval & Vector Embeddings**: The evaluation relies on FAISS and bge-base-en embeddings rather than standard database indexing. Understanding that text converts to vectors and similarity is measured by distance is required to interpret why descriptions beat keywords. *Quick check: Does the system match the word "pollution" in the query to the word "pollution" in the dataset, or does it match the vector of "air quality issues" to the vector of "pollution data"?*

- **Ablation Studies**: The paper's core methodology is an ablation study. You must understand that this involves systematically removing or isolating variables (metadata fields) to determine their individual contribution to the outcome (MRR). *Quick check: If full_llm performs worse than desc_llm, what does that imply about the contribution of the additional keyword fields?*

- **Prompting Strategies (Walker et al.)**: The evaluation dataset is segmented into "Requesting," "Describing," and "Implying" queries. The mechanism of action depends on the system's ability to handle the increasing abstraction of these query types. *Quick check: Which query type places the highest burden on the system's ability to infer user intent rather than match explicit terms?*

## Architecture Onboarding

- **Component map**: Dataset Source (London Datastore) -> Metadata Enrichment (LLM Generator + NLP Extractors) -> Encoder (BAAI/bge-base-en) -> Vector Store (FAISS) -> User Query -> Encoder -> Vector Search -> Ranked Dataset List

- **Critical path**: The Metadata Enrichment pipeline. The system's performance relies on the LLM's ability to generate the llm_description field. If this generation fails or produces low-quality text, the retrieval MRR drops from 0.925 (LLM) to 0.820 (Publisher).

- **Design tradeoffs**:
  - *Cost vs. Quality*: Generating LLM descriptions incurs API/compute costs but yields higher MRR
  - *Latency vs. Context*: Including more metadata slightly lowers performance compared to Description-only in some configs, suggesting noise injection vs. signal density
  - *Staleness*: LLM-generated metadata is static; if the dataset changes, the description must be regenerated

- **Failure signatures**:
  - Low MRR on "Implying" queries indicates metadata lacks semantic narrative
  - Hallucination occurs when LLM descriptions reference columns or trends not present in actual data
  - Retrieval of irrelevant datasets shows high semantic similarity score but wrong temporal/spatial scope

- **First 3 experiments**:
  1. Baseline Validation: Replicate desc_original vs. desc_llm comparison on a small sample to validate the MRR lift locally
  2. Prompt Optimization: Test different prompts for the LLM to see which better aligns with Describing vs. Implying queries
  3. Error Analysis on "Implying" Queries: Identify where desc_llm fails to determine if failure is due to lack of data context or embedding model limitations

## Open Questions the Paper Calls Out
The paper acknowledges potential hallucination risks from LLM-generated descriptions and notes that this is an important area for future research. It also suggests that the generalizability of findings to other data portals and domains beyond the London Datastore requires further investigation.

## Limitations
- The study's dataset scope (255 London Datastore datasets) may not generalize to domains with different metadata conventions or data structures
- Vector retrieval quality depends entirely on the BAAI/bge-base-en model's ability to capture dataset semantics, which may degrade for niche topics or non-English metadata
- LLM-generated descriptions introduce potential hallucination risks where descriptions reference non-existent columns or trends

## Confidence

- **High Confidence**: Dataset descriptions significantly outperform keywords (MRR 0.925 vs 0.820); ablation results showing topic fields are ineffective
- **Medium Confidence**: LLM descriptions consistently improve across all query types; the semantic gap explanation for why descriptions work better
- **Low Confidence**: Generalizability to other data portals beyond London Datastore; long-term stability of LLM-generated metadata without updates

## Next Checks

1. **Cross-portal validation**: Replicate the ablation study on at least two additional open data portals with different metadata standards to test generalizability of the description-first approach

2. **Hallucination audit**: Manually verify 50 LLM-generated descriptions against source data to quantify factual accuracy and identify patterns in hallucinated content

3. **Temporal robustness test**: Measure retrieval performance degradation over 6-12 months on dynamic datasets to assess whether static LLM descriptions maintain relevance as underlying data evolves