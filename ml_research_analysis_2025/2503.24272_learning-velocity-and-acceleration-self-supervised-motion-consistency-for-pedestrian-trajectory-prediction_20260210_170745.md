---
ver: rpa2
title: 'Learning Velocity and Acceleration: Self-Supervised Motion Consistency for
  Pedestrian Trajectory Prediction'
arxiv_id: '2503.24272'
source_url: https://arxiv.org/abs/2503.24272
tags:
- velocity
- prediction
- acceleration
- trajectory
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses pedestrian trajectory prediction by proposing
  a self-supervised framework that explicitly models position, velocity, and acceleration.
  The key idea is to leverage velocity and acceleration information through feature
  injection and a self-supervised motion consistency mechanism, where the model learns
  from pseudo labels generated from predicted positions.
---

# Learning Velocity and Acceleration: Self-Supervised Motion Consistency for Pedestrian Trajectory Prediction

## Quick Facts
- arXiv ID: 2503.24272
- Source URL: https://arxiv.org/abs/2503.24272
- Reference count: 39
- Primary result: Up to 16% improvement in ADE and 15% improvement in FDE over state-of-the-art methods

## Executive Summary
This paper proposes a self-supervised framework for pedestrian trajectory prediction that explicitly models position, velocity, and acceleration. The key innovation is a three-stream transformer network with hierarchical feature fusion and a self-supervised motion consistency mechanism that learns from pseudo labels generated from predicted positions. The method achieves state-of-the-art performance on ETH-UCY and Stanford Drone datasets, demonstrating particular effectiveness in handling abnormal behaviors and directional changes in pedestrian motion. The framework shows flexibility across different prediction horizons and outperforms existing approaches in both average and final displacement errors.

## Method Summary
The method uses a three-stream transformer architecture where position, velocity, and acceleration sequences are processed in parallel. Velocity and acceleration features are injected into the position stream through hierarchical cross-attention, allowing the model to condition position predictions on higher-order kinematics. A self-supervised motion consistency mechanism generates pseudo velocity and acceleration from predicted positions, enforcing consistency between explicitly predicted dynamics and derived ones. The model outputs K multimodal trajectories and uses physics-grounded heuristics (directional consistency and acceleration similarity) to select the best prediction. Training combines four loss terms: position prediction with tolerance intervals, velocity/acceleration supervision, and two consistency losses.

## Key Results
- Achieves up to 16% improvement in ADE compared to existing approaches
- Achieves up to 15% improvement in FDE over state-of-the-art methods
- Shows 0.16 ADE and 0.32 FDE on ETH-UCY dataset (best-of-K=20)
- Demonstrates superior performance on abnormal behaviors and directional changes
- Maintains flexibility across different prediction horizons

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Feature Injection
The three-stream network improves position prediction by conditioning it on velocity and acceleration features via cross-attention. This forces the position encoder to account for the agent's current momentum and rate of change, capturing predictive signal that pure position histories lack, particularly for non-linear movement.

### Mechanism 2: Self-Supervised Consistency Loop
The model predicts position, velocity, and acceleration, then derives pseudo velocity and acceleration from predicted positions using finite differences. It enforces consistency between explicitly predicted dynamics and derived dynamics, creating a training signal for under-represented behaviors without needing ground truth labels for every edge case.

### Mechanism 3: Physics-Grounded Heuristic Selection
Instead of relying solely on L2 distance, the model scores predicted velocity/acceleration pairs based on alignment with historical trends. The pair with highest score acts as anchor to guide position prediction, effectively ranking multimodal outputs based on physical plausibility of motion patterns.

## Foundational Learning

- **Kinematic State Derivation:** Understanding how velocity (V_t = P_t - P_{t-1}) and acceleration (A_t = V_t - V_{t-1}) are derived from raw positions is fundamental to the data pipeline. *Quick check:* If time-step is 0.4s and pedestrian moves 1.2m, what is the input velocity feature?

- **Self-Supervised Pseudo-Labeling:** The core innovation is "learning from data-generated pseudo labels." You must understand how the network generates its own supervision signals from predicted positions to close the training loop. *Quick check:* Does the pseudo label come from a pre-trained teacher or from finite difference of the student's own prediction?

- **Multimodal Trajectory Prediction:** The system outputs K potential trajectories. The motion consistency mechanism is essentially a ranking/selection loss applied to these K options. *Quick check:* Why would MSE loss fail if applied blindly to all K trajectories against single ground truth?

## Architecture Onboarding

- **Component map:** Position, Velocity, Acceleration inputs → 3 parallel Transformer Encoders → Hierarchical Cross-Attention (Accel→Velocity, Velocity→Position) → Social Decoders → K trajectory outputs + Motion Consistency Module

- **Critical path:** The feature injection flow. Acceleration features alter velocity query, modified velocity features alter position query. Removing cross-attention causes significant performance drop (ADE 0.27→0.16).

- **Design tradeoffs:** Explicit dynamics modeling adds computational overhead (3 encoders vs 1) but improves handling of rapid direction changes. Uses 4 loss terms requiring careful weight tuning, which is critical and potentially unstable.

- **Failure signatures:** Weak Lcons1 causes physically impossible trajectories (predicted velocity implies turn while acceleration implies straight). Misconfigured variance term in Lpos causes K predictions to collapse to single average path.

- **First 3 experiments:**
  1. Sanity Check: Feed stationary trajectory, verify velocity/acceleration inputs near zero and model predicts no movement.
  2. Ablation Study: Run model with w/o Lcons2 to isolate value of self-supervised pseudo-labeling.
  3. Visualization: Show Directional Consistency score for sudden 90-degree turn to verify heuristic selects correct trajectory.

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but the limitations section highlights several unresolved issues around the self-supervised consistency mechanism's sufficiency, robustness to noisy inputs, and quantitative analysis of abnormal behavior handling.

## Limitations
- Three-stream architecture increases computational overhead significantly compared to single-stream approaches
- Effectiveness depends heavily on proper hyperparameter tuning (weights λ, Wα, Wβ) which isn't extensively explored
- Reliance on historical consistency heuristics may fail in highly dynamic environments with unpredictable external factors
- Limited quantitative analysis of failure cases and performance in scenarios with sudden external interventions

## Confidence

**High Confidence:** Core claim that explicit velocity/acceleration modeling improves trajectory prediction is well-supported by experimental results (Table 3 ablation showing 0.27→0.16 ADE improvement).

**Medium Confidence:** Self-supervised motion consistency approach shows promise but lacks ablation studies isolating contribution of each consistency loss component. Claims up to 16% ADE improvement but doesn't compare against recent state-of-the-art methods.

**Low Confidence:** Directional consistency heuristic's robustness across diverse pedestrian behaviors is not thoroughly validated. Shows qualitative examples but lacks quantitative analysis of failure cases.

## Next Checks

1. **Ablation on Consistency Loss Components:** Run model with only Lcons1 vs only Lcons2 to quantify individual contributions to reported performance gains.

2. **Cross-Dataset Generalization Test:** Evaluate ETH-UCY trained model on SDD without fine-tuning to assess whether motion consistency mechanism generalizes beyond training distribution's pedestrian behavior patterns.

3. **Failure Mode Analysis:** Create controlled test scenarios with sudden direction changes, collisions, or stops to systematically measure how often directional consistency heuristic selects incorrect trajectories compared to random selection.