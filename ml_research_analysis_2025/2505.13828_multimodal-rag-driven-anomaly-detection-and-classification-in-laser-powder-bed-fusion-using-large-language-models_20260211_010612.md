---
ver: rpa2
title: Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder
  Bed Fusion using Large Language Models
arxiv_id: '2505.13828'
source_url: https://arxiv.org/abs/2505.13828
tags:
- anomaly
- detection
- anomalies
- manufacturing
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel multimodal Retrieval-Augmented Generation
  (RAG) framework for automated anomaly detection and classification in Laser Powder
  Bed Fusion (L-PBF) additive manufacturing processes. The framework leverages literature-based
  information, including images and descriptive text, rather than training datasets,
  enabling zero-shot anomaly identification and explanation generation.
---

# Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models

## Quick Facts
- arXiv ID: 2505.13828
- Source URL: https://arxiv.org/abs/2505.13828
- Reference count: 0
- This study introduces a novel multimodal RAG framework for automated anomaly detection and classification in Laser Powder Bed Fusion (L-PBF) additive manufacturing processes.

## Executive Summary
This study presents a zero-shot multimodal RAG framework for anomaly detection and classification in L-PBF additive manufacturing. The system leverages scientific literature as a knowledge base rather than training datasets, enabling automated anomaly identification and explanation generation. By integrating text and image retrieval from PDFs with multimodal generation models, the framework achieves 12% accuracy improvement over baseline methods while avoiding hallucination risks. The approach demonstrates adaptability across diverse L-PBF datasets without requiring additional training, addressing key challenges in AM quality control including literature overload and lack of annotated training data.

## Method Summary
The framework implements a two-phase pipeline combining retrieval and generation components. First, ColPali performs image retrieval and text-embedding-ada-002 handles text retrieval from PDF documents in the knowledge base. Second, the retrieved context is passed to multimodal large language models (Qwen2-VL-2B or GPT-4o-mini) which generate anomaly detection results. For each of 13 anomaly types, the system runs three detection attempts and aggregates results into one-hot encoded classifications. The approach uses ORNL's L-PBF dataset with 54 test images across four printer/material combinations, comparing MLLM performance while measuring the impact of retrieval integration on classification accuracy.

## Key Results
- GPT-4o-mini outperforms Qwen2-VL-2B and proportional random baseline in manufacturing anomalies classification
- Retrieval integration improves average accuracy by 12% by reducing hallucination risks and providing additional information
- Framework demonstrates adaptability across diverse L-PBF datasets from Oak Ridge National Laboratory without requiring additional training
- System achieves zero-shot anomaly identification and explanation generation using literature-based information

## Why This Works (Mechanism)
The framework succeeds by leveraging domain-specific literature as a dynamic knowledge source rather than static training data. The retrieval component ensures relevant context is provided to the generation model, reducing hallucination risks while maintaining adaptability to new anomaly types. The multimodal approach handles both visual and textual information from scientific papers, creating a comprehensive understanding of L-PBF defects. By using a zero-shot paradigm, the system can immediately incorporate emerging research findings without retraining, making it scalable for evolving AM technologies.

## Foundational Learning
- **Multimodal RAG fundamentals**: Understanding how retrieval-augmented generation works with both text and image inputs is crucial for grasping the framework's architecture. Quick check: Can you explain how the retrieval component feeds into the generation model?
- **L-PBF defect taxonomy**: Knowledge of common L-PBF anomalies (keyhole, lack of fusion, porosity, etc.) is necessary to interpret results and evaluate classification performance. Quick check: List three common L-PBF defects and their visual characteristics.
- **Zero-shot learning principles**: Understanding how models can perform tasks without task-specific training data explains the framework's adaptability. Quick check: How does the system handle anomaly types not present in the training corpus?
- **Document embedding and retrieval**: Understanding text and image embedding techniques and similarity matching is essential for the retrieval phase. Quick check: What embedding models are used for text and image retrieval respectively?

## Architecture Onboarding

**Component Map:** PDF documents -> ColPali (image retrieval) + text-embedding-ada-002 (text retrieval) -> MLLM (Qwen2-VL-2B or GPT-4o-mini) -> Anomaly classification

**Critical Path:** Retrieval phase (document processing, embedding, similarity search) → Context generation → Multimodal generation (3 runs per anomaly) → Aggregation → Classification

**Design Tradeoffs:** Uses literature-based knowledge instead of training datasets, enabling zero-shot capability but depending heavily on corpus quality; employs dual retrieval (image+text) for comprehensive context but increases computational overhead; implements 3-run aggregation to improve reliability but increases inference time.

**Failure Signatures:** Qwen2-VL-2B over-predicting "present" for all anomalies; poor retrieval quality leading to irrelevant context; hallucination in generation without adequate retrieval support.

**3 First Experiments:**
1. Test retrieval quality by manually inspecting top-5 results for each anomaly query before generation
2. Run ablation study removing retrieval component to quantify its contribution to accuracy
3. Compare single-run vs. three-run aggregation performance for classification stability

## Open Questions the Paper Calls Out
None

## Limitations
- Performance relies on a single proprietary dataset from ORNL, limiting generalizability across the broader additive manufacturing field
- The 12% accuracy improvement from retrieval integration requires independent validation on publicly available datasets
- Claims about zero-shot capability and continuous updating potential remain largely theoretical without empirical validation
- The framework depends heavily on the quality and comprehensiveness of the literature corpus

## Confidence
- **High confidence**: The RAG framework architecture and implementation details are clearly specified and reproducible
- **Medium confidence**: Comparative performance results between MLLMs are reliable for the specific ORNL dataset but may not generalize
- **Low confidence**: Claims about zero-shot capability and continuous updating potential remain largely theoretical without empirical validation

## Next Checks
1. Test the framework on publicly available L-PBF datasets with different acquisition conditions to assess cross-dataset generalization
2. Conduct ablation studies removing the retrieval component to quantify its contribution across different MLLM configurations
3. Implement uncertainty quantification for individual predictions and evaluate performance on out-of-distribution anomaly types not present in the training corpus