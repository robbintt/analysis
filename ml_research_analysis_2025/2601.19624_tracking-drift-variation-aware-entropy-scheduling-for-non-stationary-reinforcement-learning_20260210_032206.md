---
ver: rpa2
title: 'Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement
  Learning'
arxiv_id: '2601.19624'
source_url: https://arxiv.org/abs/2601.19624
tags:
- entropy
- drift
- learning
- reinforcement
- non-stationary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of environment drift in reinforcement
  learning, where static entropy coefficients lead to over-exploration during stable
  periods and under-exploration after drift, slowing recovery. We propose Adaptive
  Entropy Scheduling (AES), a principled method that adjusts the entropy coefficient/temperature
  online using observable drift signals, reducing the problem to a one-dimensional
  per-round trade-off between tracking speed and stability.
---

# Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2601.19624
- **Source URL**: https://arxiv.org/abs/2601.19624
- **Reference count**: 40
- **Primary result**: AES reduces performance degradation and accelerates recovery after abrupt changes while maintaining comparable performance in steady environments.

## Executive Summary
This work addresses the challenge of environment drift in reinforcement learning, where static entropy coefficients lead to over-exploration during stable periods and under-exploration after drift, slowing recovery. The authors propose Adaptive Entropy Scheduling (AES), a principled method that adjusts the entropy coefficient online using observable drift signals, reducing the problem to a one-dimensional per-round trade-off between tracking speed and stability. AES is integrated as a plug-in module into four maximum-entropy RL algorithms (SAC, PPO, SQL, MEow) without modifying their core structures. Across 4 algorithm variants, 12 tasks, and 4 drift modes, AES significantly reduces performance degradation caused by drift and accelerates recovery after abrupt changes.

## Method Summary
AES schedules the entropy coefficient λₜ online based on observable drift proxies derived from TD errors. The scheduler computes λₜ = clip[λₘᵢₙ, λₘₐₓ](√(C₁/C₂) × √(Ãₜ/t)), where Ãₜ is the cumulative proxy representing detected drift. The default proxy uses the 0.9-quantile of absolute TD errors with EMA smoothing. This approach converts the entropy scheduling problem into an online convex optimization framework, balancing tracking cost (large λ for rapid adaptation) against stability cost (small λ for consistent performance). The method plugs into existing maximum-entropy RL algorithms by replacing fixed entropy weights with the scheduled λₜ in the policy or actor loss.

## Key Results
- Significant improvement in normalized AUC across all tested drift modes compared to static entropy baselines
- Reduced performance drop-area ratio by up to 50% in abrupt drift scenarios
- Faster recovery time after drift events, with normalized recovery times improving by 30-40%
- Maintained comparable performance to static entropy in steady-state environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy scheduling under non-stationarity reduces to a one-dimensional per-round trade-off between tracking cost and stability cost.
- Mechanism: Dynamic mirror descent analysis yields a regret decomposition where each round contributes φₜ(λ) = C₁αₜ/λ + C₂λ. The first term (tracking cost) dominates when drift αₜ is large, favoring larger λ to redistribute probability mass faster. The second term (stability cost) penalizes over-smoothing when λ is too large. The oracle optimal schedule is λ*ₜ ∝ √αₜ.
- Core assumption: The optimal comparator sequence (drifting optimal policy) has bounded path variation αₜ = ||uₜ - uₜ₋₁||₁.
- Evidence anchors:
  - [Section 3.1]: "each round contributes φₜ(λ) := C₁αₜ/λ + C₂λ... The first term is a tracking cost (large drift αₜ favors larger λₜ), while the second is a stability cost."
  - [Section B.2.2]: The coupling ηₜ = cλₜ converts the drift term Σ αₜ/ηₜ into Σ αₜ/λₜ, producing the explicit trade-off.
- Break condition: If the true drift αₜ is zero for all t (stationary environment), the oracle λ*ₜ = 0, and any positive λ is suboptimal—the mechanism addresses non-stationarity specifically.

### Mechanism 2
- Claim: A fully online schedule can be driven by an observable drift proxy that upper-bounds the unobservable comparator drift.
- Mechanism: Since αₜ (true policy drift) cannot be observed, AES substitutes a proxy sequence ãₜ ≥ αₜ (e.g., high quantiles of TD errors). The schedule λₜ = √(C₁/C₂) × √(Ãₜ/t) uses cumulative proxy Ãₜ to set entropy strength, automatically increasing after sustained drift and decreasing during stable phases. Clipping to [λₘᵢₙ, λₘₐₓ] adds only bounded compensation terms.
- Core assumption: The proxy ãₜ correlates monotonically with true drift αₜ; specifically, αₜ ≤ ãₜ (overestimation is acceptable).
- Evidence anchors:
  - [Abstract]: "AES is driven by measurable online drift signals... using smoothed, clipped quantiles of td errors."
  - [Section 3.4]: "AES uses the time-varying regularized losses ℓₜ(x) := fₜ(x) + λₜΨ(x)... AES acts as a plug-in exploration controller that schedules the entropy weight online."
- Break condition: If the proxy ãₜ fails to upper-bound αₜ (e.g., silent drift that doesn't affect TD errors), the schedule will under-explore and recovery will be delayed.

### Mechanism 3
- Claim: MDP variation (reward/transition changes) induces Q*-drift, which induces π*-drift via softmax Lipschitzness, creating a theoretically grounded path from observable signals to optimal entropy scaling.
- Mechanism: The soft Bellman operator Tₜ is γ-contractive. Perturbation analysis gives ||Q*ₜ - Q*ₜ₋₁||_∞ ≤ (∆rₜ + γVₘₐₓ∆Pₜ)/(1-γ). Softmax has operator norm 1/µ in the ∞→1 pairing, so ||π*ₜ - π*ₜ₋₁||₁ ≤ ||Q*ₜ - Q*ₜ₋₁||_∞/µ. This chain connects MDP variation budget B_MDP to the OCO path variation that drives the λ-schedule.
- Core assumption: Bounded rewards and a fixed baseline temperature µ across all states.
- Evidence anchors:
  - [Section 3.3]: "MDP variation controls Q* drift... Q* drift controls π* drift... Combining (25)-(26) shows that the comparator drift in the OCO view is controlled by B^MDP_T."
- Break condition: If function approximation introduces large Q-substitution bias (||Q^πₜ - Q*ₜ||_∞ large), the theoretical rate becomes dominated by bias terms rather than the non-stationarity term.

## Foundational Learning

- Concept: **Mirror Descent with Bregman Divergence**
  - Why needed here: The entire theoretical framework builds on dynamic mirror descent analysis. Understanding how negative entropy (mirror map Ψ(x) = Σxᵢ log xᵢ) generates the KL divergence (Bregman distance) and how step sizes interact with strong convexity is essential for following Sections 3.1-3.2.
  - Quick check question: Given the mirror map Ψ(x) = Σxᵢ log xᵢ on the simplex, what is D_Ψ(x, y) and what norm is Ψ strongly convex with respect to?

- Concept: **Maximum Entropy Reinforcement Learning (Soft Actor-Critic family)**
  - Why needed here: AES is instantiated as a plug-in for SAC, PPO, SQL, and MEow. You need to understand how entropy regularization appears in the objective (J(π) includes -µH(π)), how the temperature α or coefficient c_ent controls exploration, and where these terms appear in actor/critic losses.
  - Quick check question: In SAC, where does the temperature α appear in the actor objective, and what happens to policy stochasticity as α → 0 vs. α → ∞?

- Concept: **Dynamic Regret in Online Convex Optimization**
  - Why needed here: Standard regret compares to a fixed optimal point; dynamic regret compares to a drifting optimal sequence u₁:T. The core contribution is bounding Reg^dyn_T = Σ(fₜ(xₜ) - fₜ(uₜ)) in terms of path variation Σ||uₜ - uₜ₋₁||. Understanding this distinction is critical for Section 3 and Appendix B.
  - Quick check question: If a comparator sequence has zero path variation (uₜ = u* for all t), how does dynamic regret relate to static regret?

## Architecture Onboarding

- Component map:
  - Drift Proxy Computation -> Entropy Scheduler -> Injection Point -> Base Algorithm
  - Quantile statistic extraction → λₜ calculation → Loss term replacement → Training update

- Critical path:
  1. Compute TD errors δ from critic update (already available in standard training)
  2. Extract quantile statistic ãₜ from |δ| batch
  3. Apply EMA smoothing to reduce noise
  4. Update cumulative proxy Ãₜ ← Ãₜ₋₁ + ãₜ
  5. Compute λₜ = √(Ãₜ/t) × scaling_factor, clip to bounds
  6. Replace fixed entropy coefficient with λₜ in actor/policy loss

- Design tradeoffs:
  - Quantile level q: Lower q (e.g., 0.5) under-reacts to drift; higher q (e.g., 0.95) increases variance. Default q=0.9 balances sensitivity vs. noise.
  - Smoothing strength (EMA β): Weak smoothing (β≈0.8) causes jittery λₜ; strong smoothing (β≈0.99) delays response to abrupt changes. Default β=0.95.
  - Clipping range [λₘᵢₙ, λₘₐₓ]: Wide ranges risk instability; narrow ranges limit adaptation. Default [0.05, 1.0] for most carriers.
  - Scaling factor √(C₁/C₂): Paper treats this as a tunable hyperparameter; ablation shows robustness to moderate variation.

- Failure signatures:
  - **Silent drift with zero TD-error response**: If environment changes don't affect critic TD errors (e.g., reward reshaping that preserves Q-rankings), AES won't increase λ, leading to delayed recovery. Mitigation: Consider multi-proxy fusion (critic parameter drift, policy entropy variance).
  - **Oscillating λₜ during stable phases**: Caused by insufficient smoothing or too-aggressive quantile. Increase EMA β or reduce q.
  - **Stuck at λₘₐₓ**: Cumulative proxy Ãₜ grows unbounded. May indicate persistent non-stationarity or proxy overestimation. Consider decaying Ãₜ or using windowed sum instead of full history.
  - **No improvement over baseline**: Check that λₜ is actually being used in the correct loss term (actor entropy bonus, not critic). Verify injection point matches Table 1.

- First 3 experiments:
  1. **Proxy validation on toy environment**: Implement AES with default settings on the 2D multi-goal environment under "Abrupt" drift. Plot λₜ alongside TD-error quantiles and ground-truth change points. Verify that λₜ spikes within ~100 steps of each change and decays during stable phases.
  2. **Ablation across quantile levels**: Run SAC+AES on HalfCheetah under "Mixed" drift with q ∈ {0.5, 0.7, 0.8, 0.9, 0.95}. Report normalized AUC and recovery time. Confirm q∈[0.7, 0.9] outperforms extremes (matching Table 5).
  3. **Cross-carrier sanity check**: Integrate AES into PPO on Walker2d under "Periodic" drift. Verify that replacing -c_ent·H(π) with -λₜ·H(π) in the policy loss produces similar improvements (lower drop-area ratio) as SAC+AES. Confirm the mechanism is carrier-agnostic as claimed.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the quantile-based TD error proxy satisfy the theoretical condition αt ≤ bαt (that the proxy upper-bounds true comparator drift), and what are the failure modes when this monotonicity breaks down?
- **Open Question 2**: How do the interface error terms (Q-substitution bias and occupancy mismatch) scale with non-stationarity intensity, and can they be controlled without additional assumptions on function approximation quality?
- **Open Question 3**: Can the AES scheduling principle be extended to continuous action spaces where the simplex-based analysis (relying on negative entropy on ∆_K) does not directly apply?
- **Open Question 4**: How should AES hyperparameters (quantile level q, smoothing coefficient, clipping bounds) be adapted when the drift mode (abrupt vs. gradual vs. periodic) is unknown a priori?

## Limitations

- AES depends on observable proxies (TD errors) that may not capture all types of drift, particularly silent changes that don't immediately affect value estimates
- Theoretical guarantees assume bounded MDP variation and stationary entropy temperature, but practical implementations use function approximation where bias can dominate
- The method introduces hyperparameters (quantile level, smoothing factor, clipping bounds) that require tuning for each environment

## Confidence

- **High Confidence**: The core mechanism of trade-off between tracking and stability costs is theoretically sound and validated through ablation studies. The empirical improvements across multiple algorithm variants and drift modes are well-documented.
- **Medium Confidence**: The drift proxy using TD-error quantiles works well in tested scenarios but may fail for certain drift types. The theoretical connection between MDP variation and optimal entropy scaling is rigorous but relies on idealized assumptions about function approximation.
- **Low Confidence**: Generalization to completely different non-stationary settings (e.g., sparse rewards, partial observability) hasn't been tested. The method's behavior under continuous, gradual drift versus discrete changes may vary significantly.

## Next Checks

1. Test AES on environments with reward-only drift (no state transition changes) to verify TD-error sensitivity covers this case.
2. Implement windowed cumulative proxy instead of full history to prevent unbounded growth and compare recovery performance.
3. Evaluate cross-task transfer by training AES hyperparameters on one set of environments and testing on held-out environments with different characteristics.