---
ver: rpa2
title: 'AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager'
arxiv_id: '2508.11416'
source_url: https://arxiv.org/abs/2508.11416
tags:
- inventory
- demand
- llms
- management
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates decision-making biases of large language models
  (LLMs) as inventory managers using AIM-Bench, a benchmark for inventory replenishment
  under uncertainty. The research examines 5 LLMs across 5 supply chain environments
  with stochastic demand, lead times, and partner behavior.
---

# AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager

## Quick Facts
- arXiv ID: 2508.11416
- Source URL: https://arxiv.org/abs/2508.11416
- Reference count: 13
- Evaluates LLM biases in inventory management across 5 supply chain environments with stochastic demand

## Executive Summary
This study introduces AIM-Bench, a benchmark for evaluating decision-making biases of large language models (LLMs) when deployed as inventory managers. The research examines 5 LLMs across 5 supply chain environments with stochastic demand, lead times, and partner behavior. Key findings reveal that most LLMs exhibit human-like biases such as anchoring on mean demand and the bullwhip effect, though they show less susceptibility to demand chasing bias. Cognitive reflection and information sharing strategies effectively mitigate these biases, while distance to optimal order quantity proves more informative than outcome metrics for model differentiation.

## Method Summary
The research employs a Partially Observable Markov Game (POMG) framework where LLM agents take actions via natural language in simulation environments. Five environments are tested: Newsvendor Problem (single-period), Multi-period Replenishment (stochastic lead times), Beer Game (multi-agent), Two-level Warehouse Network, and Supply Chain Network. Agents receive textual state descriptions with memory modules containing past observations and actions. Performance is measured through outcome metrics (stockout rate, turnover rate) and process metrics (distance to optimal order quantity, bullwhip effect ratio, anchoring factor). Mitigation strategies include cognitive reflection prompts and information sharing between agents.

## Key Results
- Most LLMs exhibit human-like biases including anchoring on mean demand (α typically 0.5-0.8) and bullwhip effect (BWE ratios often >5)
- Cognitive reflection prompts reduce anchoring factor by 60-70% (e.g., Qwen-2.5: α from 0.7 to 0.255)
- Information sharing reduces bullwhip effect by 60-70% but may induce action-chasing conformity in some models
- Distance to optimal order quantity better differentiates model performance than outcome metrics alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit mean anchoring bias in inventory decisions due to statistical pattern matching rather than logical reasoning.
- Mechanism: The model anchors on available reference points (mean demand μ) and makes insufficient adjustment toward optimal quantity q*. The adjustment process follows q = μ + α'(q* - μ) where α' < 1 indicates incomplete adjustment. High-frequency associations in training data between anchor points and answers dominate over computed optimization.
- Core assumption: Anchoring stems from parametric knowledge retrieval patterns rather than explicit reasoning; this may not generalize to models with stronger Chain-of-Thought training.
- Evidence anchors:
  - [abstract] "most LLMs exhibit human-like biases such as anchoring on mean demand"
  - [section] "One explanation for the anchoring of the observed demand is that the model does not use logical reasoning to arrive at an answer, but rather statistical pattern matching over large amounts of text."
  - [corpus] Related work on LLM decision-making biases under uncertainty (Jia et al., 2024) supports context-dependent behavioral patterns.
- Break condition: If models are explicitly prompted with step-by-step optimal calculation (cognitive reflection), anchoring reduces significantly (e.g., Qwen-2.5: α dropped from 0.7 to 0.255).

### Mechanism 2
- Claim: Bullwhip Effect in LLM agents arises from demand overestimation and variance amplification upstream.
- Mechanism: In multi-echelon supply chains, each agent perceives downstream demand with partial observability, then over-orders to buffer uncertainty. This compounds across echelons: β = σ_upstream / σ_downstream > 1. Without shared state information, agents cannot coordinate ordering behavior.
- Core assumption: The BWE observed is primarily behavioral (irrational overestimation) rather than structurally optimal given information constraints; the paper acknowledges both sources exist.
- Evidence anchors:
  - [abstract] "most LLMs exhibit...the bullwhip effect, though they show less susceptibility to demand chasing bias"
  - [section] "All the evaluated LLMs show the BWE due to demand overestimation. Results in Fig 5 indicate that the bullwhip effect is widespread. For example, Gemini-2.5-flash-lite...demonstrates the highest BWE of 19.22 and 28.61."
  - [corpus] Limited direct corpus evidence on LLM-specific BWE; this appears to be a novel contribution.
- Break condition: Information sharing between supply chain roles significantly reduces BWE (e.g., Qwen-2.5: BWE reduced from 13.78 to 4.45).

### Mechanism 3
- Claim: Distance to optimal order quantity is more discriminating than outcome metrics for differentiating model performance.
- Mechanism: Outcome metrics (stockout rate, turnover rate) can converge across models with different decision processes. Process metrics (d(a, a*) = √Σ(am - a*m)²) capture the trajectory quality, revealing whether similar outcomes arise from luck or consistent near-optimal decisions.
- Core assumption: The ex-post optimal quantity a*m computed via dynamic programming correctly represents ground-truth optimal behavior; this depends on accurate demand/lead-time realizations.
- Evidence anchors:
  - [abstract] "distance to optimal order quantity is more informative than outcome metrics for model differentiation"
  - [section] "GPT-4.1 model and the Qwen-2.5-72B model exhibit similar out-of-stock rates (0.250 and 0.256, respectively), but their distance metrics differ significantly: 467 for the GPT-4.1 model and 608 for the Qwen-2.5-72B model."
  - [corpus] Weak corpus evidence; this metric approach appears novel to this benchmark.
- Break condition: When models have vastly different outcome metrics, distance metrics remain consistent in ranking (e.g., GPT-4.1 vs GPT-4o: 0.21 lower stockout rate, 758 lower distance).

## Foundational Learning

- Concept: **Newsvendor Problem (NVP)**
  - Why needed here: Single-period inventory optimization under stochastic demand; foundational for understanding pull-to-center bias and critical ratio calculations.
  - Quick check question: Given unit cost c=$3, revenue r, and demand distribution, can you derive the optimal order quantity using the critical ratio cu/(cu+co)?

- Concept: **Partially Observable Markov Game (POMG)**
  - Why needed here: Formal framework used to model multi-agent supply chain environments where each agent has limited observation of global state.
  - Quick check question: In a 4-echelon Beer Game, what information does the wholesaler NOT observe that the retailer observes?

- Concept: **Anchoring and Insufficient Adjustment**
  - Why needed here: Behavioral economics heuristic explaining why decision-makers anchor on salient reference points and under-adjust toward optimal.
  - Quick check question: If mean demand is 100 and optimal order is 150, what order quantity would an agent with adjustment factor α'=0.4 produce?

## Architecture Onboarding

- Component map: Environment generates textual state description -> LLM agent receives observation + memory context -> Agent outputs action as natural language -> Environment parses action, updates state, computes cost -> Loop for T periods -> Compute outcome and process metrics

- Critical path:
  1. Environment generates textual state description → 2. LLM agent receives observation + memory context → 3. Agent outputs action as natural language → 4. Environment parses action, updates state, computes cost → 5. Loop for T periods → 6. Compute outcome and process metrics

- Design tradeoffs:
  - **Prompt framing** (positive/negative): Paper found no reliable risk reversal from framing; context-specific testing required before assuming behavioral interventions transfer.
  - **Information sharing**: Reduces BWE but may induce action-chasing conformity (observed in GPT-4o); balance transparency vs. independent exploration.
  - **Cognitive reflection prompts**: Reduces anchoring but increases inference cost and latency.

- Failure signatures:
  - **High anchoring factor (α > 0.5)**: Indicates model not reasoning quantitatively; try cognitive reflection prompt.
  - **BWE > 5**: Demand variance amplification upstream; check if information sharing is enabled.
  - **High turnover rate + low stockout rate** (e.g., Qwen-2.5: TR=10.2, SR=0.2): Over-ordering bias; model over-reacts to demand uncertainty.
  - **Low turnover + high stockout** (e.g., DeepSeek: TR=1.69, SR=0.54): Under-ordering; model under-estimates demand uncertainty.

- First 3 experiments:
  1. Run NVP with both positive and negative framing on your target LLM; compute anchoring factor α to establish baseline bias profile.
  2. Test cognitive reflection prompt variant on NVP; measure α reduction to quantify mitigation effectiveness.
  3. Deploy agent in Beer Game with and without information sharing; compare BWE ratio β to determine coordination improvement potential.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning (RL)-based approaches outperform prompt-dependent methods, such as cognitive reflection, in mitigating decision-making biases of LLM agents in inventory management?
- Basis in paper: [explicit] The authors state in the Limitations section that this work "only explores prompt-dependent methods to mitigate bias" and explicitly "recommend that future work investigate RL-based approaches to improve decision support."
- Why unresolved: The current study focused exclusively on prompt engineering strategies like cognitive reflection prompts and information sharing; RL integration was suggested but not tested.
- What evidence would resolve it: A comparative study benchmarking RL-finetuned LLM agents against the current prompt-based baselines on the AIM-Bench metrics (e.g., distance to optimal order quantity).

### Open Question 2
- Question: Do the decision-making behaviors and biases identified in AIM-Bench's simulation environments generalize to complex, real-world supply chain operations?
- Basis in paper: [explicit] The authors note that "the use of simulation environments simplifies the scenarios in which LLM is typically applied and may limit the generalisability of our findings."
- Why unresolved: The benchmark relies on defined mathematical environments (e.g., Beer Game, Newsvendor) which may not capture the full noise, complexity, and unstructured data of actual logistics operations.
- What evidence would resolve it: Deployment of the evaluated LLM agents in live or high-fidelity digital twin environments to compare the observed bias levels (e.g., bullwhip effect magnitude) against the simulation results.

### Open Question 3
- Question: To what extent do specific training data compositions and alignment strategies influence the manifestation of context-dependent risk preferences (or lack thereof) in LLMs?
- Basis in paper: [explicit] The authors note that "different levels of risk reversal observed in different contexts may stem from differences in training data and alignment strategies. However, the lack of publicly available information... prevents us from drawing definitive conclusions."
- Why unresolved: The "black box" nature of the proprietary models (GPT, Gemini) and unknown training details of open models make it difficult to pinpoint the root cause of why certain models fail to exhibit expected behavioral economics theories like risk reversal.
- What evidence would resolve it: A comparative analysis of models with known, controlled training corpora and alignment techniques to correlate specific training factors with the presence or absence of framing effects and risk reversal.

### Open Question 4
- Question: How can information sharing strategies be refined to mitigate the bullwhip effect without inducing detrimental "action chasing" behaviors in specific model architectures?
- Basis in paper: [inferred] While the paper highlights information sharing as a success, it also notes that "GPT-4o shows a tendency to action chasing and conform under the information-sharing setting, which restricts the agent’s ability to explore different strategies."
- Why unresolved: The study identifies a trade-off where the solution (information sharing) introduces a new behavioral issue (action chasing) in certain models, but does not offer a solution for this specific negative byproduct.
- What evidence would resolve it: Developing and testing mechanism designs or prompt filters that utilize shared information for demand forecasting without allowing the agent to mimic partner actions directly, followed by re-evaluation on the AIM-Bench.

## Limitations

- The benchmark relies on simulation environments that may not fully capture the complexity and noise of real-world supply chain operations
- The study only explores prompt-dependent methods for bias mitigation, suggesting RL-based approaches as future work
- Lack of publicly available information about model training data and alignment strategies prevents definitive conclusions about the source of context-dependent risk preferences

## Confidence

- **High** confidence in mitigation effectiveness claims (cognitive reflection, information sharing) based on reported numerical results
- **Medium** confidence in behavioral bias findings due to novel corpus evidence rather than extensive prior work
- **Medium** confidence in distance metric claims - while the GPT-4.1 vs Qwen-2.5 comparison shows clear separation, corpus evidence is weak and this metric approach appears novel

## Next Checks

1. **Prompt Template Validation**: Implement and test the exact cognitive reflection prompt format across multiple environments to verify the reported α reductions (e.g., Qwen-2.5: 0.7 → 0.255) are reproducible with consistent prompt engineering.

2. **Information Sharing Sensitivity**: Systematically vary the scope of shared information in multi-agent environments (partial vs. complete state visibility) to quantify the trade-off between BWE reduction and action-chasing conformity observed in GPT-4o.

3. **Distributional Robustness**: Test the benchmark across different demand distribution families (normal, lognormal, bimodal) to assess whether observed biases are consistent or distribution-dependent, particularly for the anchoring mechanism which may be sensitive to reference point salience.