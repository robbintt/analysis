---
ver: rpa2
title: 'CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex
  Reasoning over Structured Documents'
arxiv_id: '2505.17503'
source_url: https://arxiv.org/abs/2505.17503
tags:
- reasoning
- answer
- question
- documents
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CReSt, a benchmark designed to evaluate the
  capabilities of large language models in practical Retrieval-Augmented Generation
  (RAG) scenarios involving complex reasoning over structured documents. CReSt addresses
  the gap in existing benchmarks by assessing not only answer correctness but also
  answer refusal, citation accuracy, and understanding of structured formats such
  as HTML.
---

# CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents

## Quick Facts
- **arXiv ID:** 2505.17503
- **Source URL:** https://arxiv.org/abs/2505.17503
- **Reference count:** 40
- **Primary result:** Introduces CReSt, a benchmark for evaluating RAG models on complex reasoning over structured documents with metrics for correctness, refusal, citation, and document understanding.

## Executive Summary
This paper introduces CReSt, a benchmark designed to evaluate the capabilities of large language models in practical Retrieval-Augmented Generation (RAG) scenarios involving complex reasoning over structured documents. CReSt addresses the gap in existing benchmarks by assessing not only answer correctness but also answer refusal, citation accuracy, and understanding of structured formats such as HTML. The benchmark comprises 2,245 human-verified examples in English and Korean, constructed from real-world documents parsed into both plain-text and HTML formats. The evaluation framework uses an LLM-as-a-judge to classify answers into correct, partially correct, or wrong categories, and also measures refusal accuracy and citation performance. Experimental results show that even advanced models struggle with consistent performance across these dimensions, highlighting the benchmark's ability to identify weaknesses in complex reasoning, refusal handling, and document understanding. Models with stronger reasoning capabilities, such as OpenAI's o4-mini, outperform others, but overall performance remains limited, emphasizing the need for further research in these areas.

## Method Summary
CReSt is a benchmark for evaluating large language models in Retrieval-Augmented Generation (RAG) scenarios involving complex reasoning over structured documents. The benchmark consists of 2,245 human-verified examples in both English and Korean, created from real-world documents parsed into plain-text and HTML formats. CReSt assesses four key capabilities: answer correctness (using an LLM-as-a-judge to classify responses as correct, partially correct, or wrong), answer refusal (measuring whether the model appropriately declines unanswerable questions), citation accuracy (checking if answers are supported by relevant source text), and understanding of structured documents (evaluating the ability to extract and reason over structured formats like HTML). The benchmark is designed to be adaptable for future document formats and languages. Experimental results show that even advanced models struggle with consistent performance across these dimensions, highlighting the benchmark's ability to identify weaknesses in complex reasoning, refusal handling, and document understanding.

## Key Results
- CReSt is the first benchmark to comprehensively evaluate RAG models on complex reasoning, answer refusal, citation accuracy, and structured document understanding.
- Even advanced models struggle with consistent performance across all four dimensions, with refusal and citation metrics being particularly challenging.
- Models with stronger reasoning capabilities, such as OpenAI's o4-mini, outperform others, but overall performance remains limited, highlighting the need for further research in these areas.

## Why This Works (Mechanism)
CReSt works by providing a comprehensive evaluation framework that goes beyond simple answer correctness to assess multiple critical aspects of RAG model performance. The benchmark uses real-world structured documents and human-verified examples to create realistic and challenging scenarios. By employing an LLM-as-a-judge to classify answers and evaluate citations, CReSt ensures consistent and scalable assessment across multiple languages and document formats. The inclusion of structured document understanding, particularly HTML parsing, tests the model's ability to extract and reason over complex data layouts, which is essential for practical RAG applications. This multi-faceted approach allows CReSt to identify specific weaknesses in model capabilities, such as refusal handling and citation accuracy, that are often overlooked in traditional benchmarks.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation):** Combining retrieval of relevant documents with generative models to produce answers; needed for understanding the core task evaluated by CReSt.
- **LLM-as-a-judge:** Using a large language model to evaluate the quality of generated responses; needed for scalable and consistent answer assessment in CReSt.
- **HTML parsing and structured document understanding:** Extracting and reasoning over information embedded in structured formats; needed for evaluating model performance on real-world documents.
- **Citation accuracy:** Verifying that generated answers are supported by the retrieved source text; needed for assessing the reliability of RAG outputs.
- **Answer refusal:** Evaluating whether models appropriately decline to answer unanswerable questions; needed for safe and responsible deployment of RAG systems.
- **Cross-lingual evaluation:** Testing model performance across multiple languages (English and Korean in CReSt); needed for assessing generalization and robustness.

## Architecture Onboarding
- **Component map:** Input Document (HTML/plain-text) -> Parser (structured extraction) -> Retriever (relevant passage selection) -> Generator (answer synthesis) -> Evaluator (LLM-as-a-judge for correctness, refusal, citation).
- **Critical path:** Document parsing and structured understanding -> Complex reasoning over retrieved content -> Answer generation with proper citation -> Evaluation for correctness and refusal.
- **Design tradeoffs:** Real-world documents vs. synthetic data (tradeoff between realism and control), HTML parsing complexity vs. model capability (tradeoff between challenge and feasibility), multi-language support vs. resource requirements (tradeoff between generalizability and cost).
- **Failure signatures:** Inability to parse structured documents, failure to cite relevant passages, incorrect refusal of answerable questions, poor performance on complex reasoning tasks.
- **First 3 experiments:** 1) Evaluate model performance on answer correctness across document formats (HTML vs. plain-text), 2) Assess refusal accuracy for unanswerable questions, 3) Measure citation accuracy for generated answers.

## Open Questions the Paper Calls Out
- How can models be improved to better handle structured document understanding and complex reasoning in RAG scenarios?
- What strategies can be developed to enhance answer refusal capabilities, ensuring models appropriately decline unanswerable questions?
- How can citation accuracy be improved to ensure generated answers are reliably supported by source documents?
- What are the best practices for adapting CReSt to evaluate new document formats and languages?

## Limitations
- CReSt focuses on structured documents and may not fully capture the challenges of unstructured or semi-structured data.
- The benchmark's reliance on human-verified examples may limit its scalability and adaptability to new domains.
- Experimental results are based on a specific set of models and may not generalize to all RAG systems.

## Confidence
- **Method soundness:** High
- **Benchmark design:** High
- **Experimental results:** Medium
- **Generalizability:** Medium

## Next Checks
1. Evaluate model performance on a wider range of document formats (e.g., PDFs, JSON) to assess generalizability.
2. Test CReSt with additional languages and domains to ensure cross-lingual and cross-domain robustness.
3. Investigate the impact of different retrieval strategies (e.g., dense vs. sparse retrieval) on model performance in CReSt scenarios.