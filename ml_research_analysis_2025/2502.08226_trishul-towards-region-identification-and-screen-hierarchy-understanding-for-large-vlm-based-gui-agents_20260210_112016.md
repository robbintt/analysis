---
ver: rpa2
title: 'TRISHUL: Towards Region Identification and Screen Hierarchy Understanding
  for Large VLM based GUI Agents'
arxiv_id: '2502.08226'
source_url: https://arxiv.org/abs/2502.08226
tags:
- trishul
- gpt-4v
- https
- gpt-4o
- elements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRISHUL, a training-free agentic framework
  for large vision-language model (LVLM)-based GUI agents. TRISHUL addresses limitations
  in existing GUI agents by providing comprehensive GUI understanding through Hierarchical
  Screen Parsing (HSP) and Spatially Enhanced Element Description (SEED) modules.
---

# TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents

## Quick Facts
- **arXiv ID**: 2502.08226
- **Source URL**: https://arxiv.org/abs/2502.08226
- **Reference count**: 40
- **Primary result**: Training-free LVLM agent achieving 72.2% accuracy on ScreenSpot action grounding

## Executive Summary
This paper introduces TRISHUL, a training-free agentic framework for large vision-language model (LVLM)-based GUI agents. TRISHUL addresses limitations in existing GUI agents by providing comprehensive GUI understanding through Hierarchical Screen Parsing (HSP) and Spatially Enhanced Element Description (SEED) modules. HSP organizes GUI elements into hierarchical structures with Global Regions of Interest (GROIs) and local elements, while SEED generates spatially and semantically enriched functionality descriptions for GUI elements. TRISHUL achieves superior performance in action grounding across ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets, with 72.2% accuracy on ScreenSpot and 68.0% on VisualWebBench using GPT-4o. It also excels in GUI referring tasks, surpassing the ToL agent on the ScreenPR benchmark.

## Method Summary
TRISHUL is a training-free GUI agent that combines Hierarchical Screen Parsing (HSP) and Spatially Enhanced Element Description (SEED) to achieve comprehensive GUI understanding. HSP uses SAM and EasyOCR to detect elements, filters them by area thresholds, computes Information Scores to prioritize regions, and applies NMS to generate GROIs and local elements. SEED uses an LVLM with CoT+ICL prompting to classify elements and generate spatially enriched descriptions. Action grounding proceeds through GROI proposal followed by SoM-style grounding within the proposed region. The framework achieves strong cross-platform performance without requiring platform-specific training.

## Key Results
- 72.2% accuracy on ScreenSpot action grounding benchmark
- 68.0% accuracy on VisualWebBench using GPT-4o
- Outperforms ToL agent on ScreenPR GUI referring benchmark
- Strong cross-platform generalizability across web, desktop, and mobile interfaces

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Screen Parsing (HSP)
- Claim: Organizing GUI elements into a two-level hierarchy (GROIs + local elements) improves action grounding by reducing search space and providing semantic grouping.
- Mechanism: SAM generates candidate bounding boxes via point sampling; EasyOCR extracts text regions. Boxes are filtered by area thresholds into GROI candidates (large) and local elements (small). An Information Score S = N_inside / √(1 + N_inter × Area) prioritizes regions with more internal content relative to size. NMS with IoS (Intersection-over-Size) filtering removes redundant GROIs.
- Core assumption: GUIs have semantically coherent regions that can be identified via spatial clustering of detected elements; SAM's zero-shot segmentation generalizes across platforms without GUI-specific training.
- Evidence anchors:
  - [abstract] "HSP organizes GUI elements into hierarchical structures with Global Regions of Interest (GROIs) and local elements"
  - [section] Algorithm 1 (page 2) defines the full HSP pipeline with Information Score computation
  - [corpus] Related work "GUI-Spotlight" similarly uses iterative focus refinement, suggesting hierarchical attention is a convergent pattern; corpus evidence for HSP specifically is weak—no direct corroboration found
- Break condition: HSP degrades when (1) GUI regions lack semantic separation (e.g., mobile apps with uniform layouts—see Appendix A.3: "mobile regions are not semantically coherent"), or (2) SAM produces noisy/overlapping proposals that overwhelm filtering heuristics.

### Mechanism 2: Spatially Enhanced Element Description (SEED)
- Claim: Enriching GUI element descriptions with spatial context (nearby text/icons) improves LVLM grounding accuracy over pure visual appearance.
- Mechanism: SEED receives SoM-annotated image + bounding boxes + OCR text. An LVLM with CoT+ICL (6 examples from ScreenSpot) classifies each element as {paired, standalone, picture, actionable-text} and generates descriptions by associating nearby elements. Paired elements merge descriptors; standalone rely on visual cues alone.
- Core assumption: LVLMs can perform reliable spatial reasoning from annotated images; proximity implies semantic relationship; ICL examples generalize across platforms.
- Evidence anchors:
  - [abstract] "SEED generates spatially and semantically enriched functionality descriptions for GUI elements"
  - [section] Ablation (Table 2): Removing SEED drops accuracy 8.5% (GPT-4V) and 2.9% (GPT-4o) on ScreenSpot
  - [corpus] "Learning GUI Grounding with Spatial Reasoning" corroborates spatial reasoning for GUI tasks; corpus evidence for CoT+ICL prompting specifically is weak
- Break condition: SEED fails when (1) OCR misses critical text (low LEE scores—see Figure 5 discussion), (2) nearby elements are semantically unrelated (false associations), or (3) LVLM reasoning is unreliable for complex layouts.

### Mechanism 3: Two-Stage GROI-Guided Grounding
- Claim: Coarse-to-fine grounding (first identify GROI, then local element within it) outperforms full-image grounding by constraining LVLM attention.
- Mechanism: Given instruction Is and annotated image, LVLM proposes the most relevant GROI ID from cropped GROI set. SEED descriptions are generated only for elements within the proposed GROI. Final grounding uses SoM prompting on the cropped region with SEED descriptors.
- Core assumption: Correct target element's midpoint lies within the proposed GROI (validated at 82-93% accuracy—Table 1); LVLMs perform better with reduced visual clutter.
- Evidence anchors:
  - [abstract] "TRISHUL achieves superior performance in action grounding across ScreenSpot, VisualWebBench, AITW, and Mind2Web"
  - [section] Ablation (Table 2): Removing GROI-based grounding drops accuracy 2.9% (GPT-4V) and 1.1% (GPT-4o)
  - [corpus] "GUI-Actor" and "GUI-Spotlight" both employ multi-stage refinement, supporting coarse-to-fine as a design pattern
- Break condition: Pipeline breaks when (1) GROI proposal fails (ground truth outside proposed GROI—see 7-18% failure rate in Table 1), (2) GROIs are not semantically useful (mobile interfaces—"GROI coverage lowest for mobile"—Appendix A.3).

## Foundational Learning

- Concept: **Set-of-Marks (SoM) Prompting**
  - Why needed here: TRISHUL uses SoM-style ID tags to annotate GUI elements for LVLM input; understanding how visual markers enable grounding is essential.
  - Quick check question: Given an image with numbered bounding boxes, can you explain how an LVLM maps a text instruction to a specific box ID?

- Concept: **Non-Maximum Suppression (NMS) with Scoring**
  - Why needed here: HSP uses a custom Information Score to rank GROI candidates before NMS; standard IoU-based NMS is insufficient when regions have varying information density.
  - Quick check question: Why would naively applying IoU-based NMS discard informative GROIs that overlap with less informative ones?

- Concept: **In-Context Learning (ICL) with Chain-of-Thought (CoT)**
  - Why needed here: SEED uses 6 ICL examples + CoT prompting to generate descriptions; the quality of these examples and prompt structure directly affects output quality.
  - Quick check question: What failure modes might occur if ICL examples are not representative of the target GUI platform (e.g., only web examples for mobile grounding)?

## Architecture Onboarding

- Component map:
  Input Image → [SAM + EasyOCR] → Candidate BBoxes → [Area Filtering + Heuristic Cleanup] → GROI Candidates + Local Elements → [Information Score + NMS] → Filtered GROIs → [LVLM GROI Proposal] → Selected GROI → [SEED (LVLM + CoT + ICL)] → Enhanced Element Descriptions → [SoM Grounding Prompt] → Predicted BBox

- Critical path:
  1. HSP quality (LEE score) → determines if ground truth is even detectable
  2. GROI proposal accuracy (Table 1: 82-93%) → upper bound for pipeline success
  3. SEED description quality → disambiguates visually similar elements

- Design tradeoffs:
  - **Area thresholds (A_thresh-GROI, A_thresh-Icon)**: Platform-specific; mobile requires different values than web/desktop. Paper uses fixed thresholds—tuning required for new domains.
  - **Information Score formula**: Emphasizes internal density over pure area; may underweight sparse but critical regions (e.g., navigation bars).
  - **Training-free vs. accuracy**: OmniParser + SEED (Table 2) achieves 73.7% on ScreenSpot vs. TRISHUL's 72.2%, suggesting specialized training can still outperform pure prompting.

- Failure signatures:
  - **Low LEE score** → HSP missing elements; check SAM/OCR output quality (Figure 5 shows Mind2Web has lowest LEE)
  - **GROI proposal fails** → Instruction ambiguous or spans multiple GROIs; inspect proposal prompt
  - **SEED produces generic descriptions** → ICL examples insufficient; add platform-specific examples
  - **Mobile performance lower than web/desktop** → Expected per Appendix A.3; GROIs less semantically coherent on mobile

- First 3 experiments:
  1. **Validate HSP element detection**: Run HSP on 20 screenshots from each platform (mobile/web/desktop). Compute LEE scores using ground truth bounding boxes from ScreenSpot. Target: LEE > 90% for all platforms.
  2. **Ablate SEED vs. baseline descriptions**: On ScreenSpot subset (100 samples), compare (a) OCR-only descriptions, (b) BLIPv2 captions (OmniParser style), (c) SEED descriptions. Measure grounding accuracy delta.
  3. **Cross-platform GROI proposal test**: Run GROI proposal on VisualWebBench using GPT-4o.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Hierarchical Screen Parsing (HSP) module be enhanced to improve the exhaustiveness of local element detection in complex web environments?
- Basis in paper: [inferred] Section 4.2 identifies the limited exhaustiveness of local elements detected by HSP (low LEE scores) as a primary "bottleneck" and "key factor constraining TRISHUL’s effectiveness" on the Mind2Web dataset.
- Why unresolved: The current combination of SAM and OCR fails to capture all actionable elements in dense web interfaces, creating an upper bound on performance regardless of the LVLM's reasoning capability.
- What evidence would resolve it: An improved parsing module that achieves significantly higher Local Element Exhaustiveness scores on Mind2Web, resulting in a proportional increase in Step Success Rates.

### Open Question 2
- Question: Can purely visual, training-free agents achieve parity with HTML-based methods in web navigation tasks?
- Basis in paper: [inferred] Section 3.3 notes that while TRISHUL outperforms other image-based baselines, it "trails state-of-the-art HTML-based methods" because "predicting bounding boxes is a more complex task than selecting HTML elements."
- Why unresolved: The structural clarity provided by DOM trees (available to methods like MindAct) currently offers a distinct advantage over pixel-based bounding box prediction, a gap TRISHUL has not yet closed.
- What evidence would resolve it: A training-free visual agent matching the Element Accuracy and Step Success Rate of HTML-reliant baselines on the Mind2Web benchmark without accessing source code metadata.

### Open Question 3
- Question: How can hierarchical region identification be optimized for mobile interfaces where semantic separation is minimal?
- Basis in paper: [inferred] Section 3.1 and Appendix A.3 state that GROI-based grounding is "less pronounced in mobile interfaces" because regions have "minimal semantic separation," leading to less dense GROI coverage.
- Why unresolved: The current Information Score heuristic for GROIs relies on semantic clustering which appears less effective for the distinct layout patterns of mobile screens compared to web or desktop platforms.
- What evidence would resolve it: A modified region proposal strategy that yields significantly higher accuracy improvements on the mobile subsets of ScreenSpot and AITW compared to the current GROI approach.

## Limitations
- Underspecified implementation details (SAM sampling parameters, area thresholds, prompt templates) may hinder exact reproduction
- Training-free approach may be outperformed by fine-tuned models on specific platforms (OmniParser + SEED achieved 73.7% vs. TRISHUL's 72.2% on ScreenSpot)
- Mobile GUI performance notably weaker due to less semantically coherent regions
- Assumes SAM generalizes well across platforms, which may not hold for non-standard or highly dynamic interfaces

## Confidence

- **High confidence**: Hierarchical Screen Parsing mechanism (HSP) improves grounding by reducing search space and providing semantic grouping, supported by ablation results and Information Score prioritization
- **Medium confidence**: Spatially Enhanced Element Description (SEED) improves accuracy through enriched spatial context, though effectiveness depends heavily on LVLM reasoning quality and representative ICL examples
- **Medium confidence**: Two-stage GROI-guided grounding outperforms full-image grounding by constraining LVLM attention, with proposal accuracy (82-93%) providing a strong upper bound

## Next Checks

1. **Validate HSP element detection**: Run HSP on 20 screenshots from each platform (mobile/web/desktop). Compute LEE scores using ground truth bounding boxes from ScreenSpot. Target: LEE > 90% for all platforms.
2. **Ablate SEED vs. baseline descriptions**: On ScreenSpot subset (100 samples), compare (a) OCR-only descriptions, (b) BLIPv2 captions (OmniParser style), (c) SEED descriptions. Measure grounding accuracy delta.
3. **Cross-platform GROI proposal test**: Run GROI proposal on VisualWebBench using GPT-4o.