---
ver: rpa2
title: Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials
arxiv_id: '2511.11361'
source_url: https://arxiv.org/abs/2511.11361
tags:
- data
- learning
- force
- training
- multi-fidelity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training machine learning
  force fields (MLFFs) for lithium-ion battery cathode materials, where complex electronic
  structures and scarce high-quality datasets limit performance. The authors propose
  a multi-fidelity graph neural network framework based on CHGNet that simultaneously
  leverages low-fidelity non-magnetic and high-fidelity magnetic DFT datasets to improve
  data efficiency.
---

# Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials

## Quick Facts
- arXiv ID: 2511.11361
- Source URL: https://arxiv.org/abs/2511.11361
- Authors: Guangyi Dong; Zhihui Wang
- Reference count: 40
- Primary result: Multi-fidelity graph neural network improves MLFF accuracy for LMFP cathode materials, especially when high-fidelity data is limited

## Executive Summary
This paper addresses the challenge of training machine learning force fields (MLFFs) for lithium-ion battery cathode materials, where complex electronic structures and scarce high-quality datasets limit performance. The authors propose a multi-fidelity graph neural network framework based on CHGNet that simultaneously leverages low-fidelity non-magnetic and high-fidelity magnetic DFT datasets to improve data efficiency. The method introduces fidelity-dependent atomic embeddings, message passing, readout layers, and composition models to differentiate data quality while preserving shared interaction trends. Experiments on lithium manganese iron phosphate (LMFP) cathode materials show that multi-fidelity training significantly improves prediction accuracy across energy, forces, stress, and magnetic moments, particularly when high-fidelity data is limited. The framework outperforms transfer learning approaches, demonstrating strong generalizability and potential for efficient modeling of complex battery materials.

## Method Summary
The method extends CHGNet by incorporating fidelity information through four components: fidelity-dependent atomic embeddings (additive element-specific plus fidelity-specific vectors), one-hot fidelity encoding in message-passing layers (AtomConv, BondConv, AngleUpdate), fidelity-specific readout MLPs, and fidelity-dependent composition models. The model assigns fidelity labels (f=1 for non-magnetic, f=2 for magnetic) and trains on mixed batches, activating only the parameters corresponding to each sample's fidelity. Loss weights are balanced for energy, forces, stress, and magnetic moments, with low-fidelity data providing complementary information to high-fidelity data. The framework maintains shared parameters across fidelities to capture universal interatomic physics while allowing fidelity-specific refinements for systematic energy offsets and interaction strengths.

## Key Results
- Multi-fidelity training with 50-100 high-fidelity frames plus all low-fidelity data reduces force MAE by ~20-30% compared to single-fidelity training
- Outperforms transfer learning approaches, particularly when high-fidelity data is scarce
- Fidelity-dependent composition models are necessary but not sufficient - must be combined with fidelity-conditioned message passing and readout
- Strong generalizability demonstrated on LMFP cathode materials with mixed Mn/Fe oxidation states

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fidelity-dependent embeddings enable simultaneous learning from datasets with different accuracy levels without conflating their distinct potential energy surfaces.
- **Mechanism:** Atomic embeddings are decomposed into element-specific and fidelity-specific components: v⁰ᵢ = Xᵢ + X_f. This additive formulation preserves inter-element relationships across fidelities while allowing each fidelity to develop distinct interaction patterns. The model treats "Fe at fidelity-1" and "Fe at fidelity-2" as related but distinguishable atomic species.
- **Core assumption:** Low-fidelity and high-fidelity potential energy surfaces share fundamental structural dependencies but differ systematically in their energy zero-points and interaction strengths.
- **Evidence anchors:**
  - [abstract]: "introduces fidelity-dependent atomic embeddings, message passing, readout layers, and composition models to differentiate data quality while preserving shared interaction trends"
  - [Section 2, Equation 4]: Explicit formulation showing additive embedding structure
  - [Section 2, page 3]: "introducing the fidelity embedding vector X_f in an additive form ensures that, for any two different elements, the difference between their embedding vectors remains the same across data of different precisions"
  - [corpus]: Weak direct evidence—neighbor papers discuss multi-fidelity concepts generally but don't validate this specific embedding strategy
- **Break condition:** If fidelity embeddings grow too large relative to atomic embeddings, the model may fail to transfer structural knowledge across fidelities, treating them as entirely unrelated datasets.

### Mechanism 2
- **Claim:** Shared message-passing layers with fidelity-conditioned message generation capture universal interatomic physics while adapting to fidelity-specific refinements.
- **Mechanism:** One-hot fidelity encoding f_g is concatenated to inputs at three submodules (AtomConv, BondConv, AngleUpdate), creating fidelity-dependent weight columns W_F that activate only during training on corresponding data. Most parameters remain shared across fidelities, preserving learned interaction physics.
- **Core assumption:** The geometric and bonding patterns governing interatomic forces are largely fidelity-invariant; differences manifest primarily in the quantitative mapping from features to energies.
- **Evidence anchors:**
  - [Section 2, Equations 6-8]: Shows how fidelity encoding is injected into linear layers and message functions
  - [Section 2, page 5]: "retaining most of the shared parameters of the original architecture to describe the common overall trends of different potential energy surfaces"
  - [Figure 4]: Ablation showing M (message-passing fidelity) contributes to performance, though F (full) is best
  - [corpus]: Neighbor paper 20985 discusses MLFF fundamentals but doesn't address multi-fidelity message passing specifically
- **Break condition:** If high-fidelity data is extremely scarce (< 50 structures in experiments), fidelity-specific message parameters may underfit, though the framework still outperforms single-fidelity training.

### Mechanism 3
- **Claim:** Separate composition models and readout layers per fidelity isolate systematic energy offsets, allowing the shared GNN backbone to focus on interaction physics.
- **Mechanism:** Composition model E^f_c = Σ(w_a,f · x_a) captures fidelity-dependent atomic reference energies through linear fitting. Fidelity-specific readout MLPs L^f_readout map final atom features to energies. This prevents the GNN from needing to learn different absolute energy scales.
- **Core assumption:** Energy zero-points vary significantly between computational methods (e.g., non-magnetic vs. magnetic DFT), but these offsets are largely composition-dependent rather than structure-dependent.
- **Evidence anchors:**
  - [Section 2, Equation 9-10]: Explicit formulation of fidelity-dependent composition model and readout
  - [Section 2, page 5]: "datasets generated by different computational methods are not suitable for direct joint training" without composition model
  - [Figure 4]: C (composition-only) performs poorly alone, but combining with other modules (F) achieves best results—composition model is "necessary but not sufficient"
  - [corpus]: No direct corpus validation of this decomposition strategy for MLFFs
- **Break condition:** Assumption: If reference energies have significant structure-dependent components (not just composition-dependent), the linear composition model will fail to fully isolate offsets.

## Foundational Learning

- **Concept:** Graph Neural Networks for Materials
  - **Why needed here:** CHGNet uses crystal graphs with atoms as nodes, bonds as edges, and a separate bond graph for angles. Understanding message passing across these structures is essential for following the fidelity modifications.
  - **Quick check question:** Can you explain why a bond graph (bonds as nodes, angles as edges) is needed in addition to the atom graph for capturing 3-body interactions?

- **Concept:** Magnetic Moments in DFT
  - **Why needed here:** Cathode materials involve transition metals with variable oxidation states. The paper distinguishes "non-magnetic" (spin-unpolarized, cheaper) from "magnetic" (spin-polarized, more accurate) calculations—understanding this tradeoff is critical.
  - **Quick check question:** Why would non-magnetic DFT calculations fail to capture the correct physics of LiMnFePO₄ during delithiation?

- **Concept:** Transfer Learning vs. Multi-Fidelity Learning
  - **Why needed here:** The paper explicitly compares against transfer learning and shows superior performance. Understanding why sequential pre-training then fine-tuning suffers from "catastrophic forgetting" clarifies the multi-fidelity advantage.
  - **Quick check question:** In transfer learning for force fields, what information is lost when you freeze early layers during fine-tuning?

## Architecture Onboarding

- **Component map:** Input (structure + fidelity label f) -> Embedding Layer: X_Z[atomic_num] + X_f[f] → v⁰ᵢ -> Message Passing (×4 layers): AtomConv(vᵢ, vⱼ, eᵢⱼ, f_g) → atom updates, BondConv(eᵢⱼ, eⱼₖ, aᵢⱼₖ, f_g) → bond updates, AngleUpdate(aᵢⱼₖ, f_g) → angle updates -> Readout Layer: L^f_readout(vⁿᴸᵢ) → atomic energies -> Composition Model: E^f_c = Σ(w_a,f · x_a) -> Output: E_total = E^f_c + Σ(atomic_energies) -> Forces/Stress: Via automatic differentiation -> Mag. Moments: From penultimate layer via L_m

- **Critical path:**
  1. Assign fidelity labels: f=1 for non-magnetic, f=2 for magnetic data
  2. Initialize fidelity-specific parameters: embedding X_f, message weights W_F, readout MLPs, composition weights w_a,f
  3. Joint training: each batch includes fidelity label, only corresponding parameters activate
  4. Monitor: Track separate MAE for each fidelity's test set

- **Design tradeoffs:**
  - **More fidelity levels vs. parameter efficiency:** Each additional fidelity adds parameters proportional to embedding dimension × number of layers. Paper uses nF=2; scaling requires care.
  - **Shared vs. separate readouts:** Ablation shows readout fidelity-dependence matters, but message-passing fidelity contributes more to force accuracy.
  - **Composition model complexity:** Linear model is simple; if reference energies have configurational dependence, this becomes a bottleneck.

- **Failure signatures:**
  - **Catastrophic imbalance:** If low-fidelity data >> high-fidelity data (>100:1 ratio) without careful loss weighting, model may ignore high-fidelity signal
  - **Fidelity collapse:** If fidelity embeddings are too small, model treats all data as same fidelity—check by plotting embedding distances
  - **Negative transfer:** If low-fidelity physics contradicts high-fidelity (e.g., wrong magnetic ordering), adding low-fidelity data degrades performance—Figure 2 shows this doesn't occur for LMFP

- **First 3 experiments:**
  1. **Baseline single-fidelity:** Train CHGNet on only magnetic data (N condition in paper). Measure energy/force/stress/magnetic moment MAE on held-out magnetic test set. This establishes the upper bound you're trying to match with less data.
  2. **Minimal multi-fidelity:** Train multi-fidelity model with all non-magnetic data + 50-100 magnetic structures. Compare to baseline. Paper shows ~20-30% force MAE reduction at 100 high-fidelity frames.
  3. **Ablation sweep:** Test F (full), E-only, M-only, R-only, C-only, and combinations from Figure 4. Confirm that message-passing fidelity (M) and readout fidelity (R) are the dominant contributors, and that C alone is insufficient.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this multi-fidelity framework be generalized to broader material systems beyond the specific Li-Mn-Fe-Phosphate (LMFP) cathode tested?
- **Basis in paper:** [explicit] The conclusion states future work involves "extending to broader material systems."
- **Why unresolved:** The experimental validation was restricted solely to the LMFP cathode material system.
- **What evidence would resolve it:** Successful training and prediction accuracy on diverse chemical systems (e.g., NMC, LCO) using the same framework.

### Open Question 2
- **Question:** Is it possible to develop a general multi-fidelity pre-trained model for battery materials using this architecture?
- **Basis in paper:** [explicit] The conclusion suggests "a general multi-fidelity pre-trained model for battery materials could be developed."
- **Why unresolved:** The current study trained task-specific models rather than a universal foundation model.
- **What evidence would resolve it:** A single pre-trained model capable of fine-tuning to new battery materials with minimal high-fidelity data.

### Open Question 3
- **Question:** How can quantifiable data quality evaluation standards be established to define granular fidelity levels within the model?
- **Basis in paper:** [explicit] The conclusion calls for "exploring more granular fidelity level divisions, and establishing quantifiable data quality evaluation standards."
- **Why unresolved:** The current implementation uses a simple integer $f$ based on magnetic vs. non-magnetic calculations.
- **What evidence would resolve it:** A continuous or multi-level fidelity metric that correlates with prediction error reduction.

## Limitations
- Experimental validation limited to single material system (LMFP) and two fidelity levels, generalizability untested
- Linear composition model assumption may not hold for all computational method pairs
- Impact of severe dataset imbalance (low-fidelity vastly outnumbering high-fidelity) on model performance requires further investigation

## Confidence

- **High confidence**: The additive embedding structure (Mechanism 1) is mathematically sound and directly supported by the paper's equations and ablation study.
- **Medium confidence**: The shared message-passing approach (Mechanism 2) is well-motivated and shows strong experimental support, though the exact contribution of each fidelity-specific component is difficult to isolate.
- **Medium confidence**: The composition model isolation strategy (Mechanism 3) is necessary based on ablation results, but the linear approximation may be too restrictive for some computational method pairs.

## Next Checks

1. **Cross-material validation**: Apply the multi-fidelity framework to a different cathode system (e.g., NMC, LCO) with distinct electronic structures to test generalizability beyond LMFP.
2. **Ablation on composition model**: Replace the linear composition model with a nonlinear alternative (e.g., small MLP) to assess whether the linear assumption is limiting performance.
3. **Scalability test**: Systematically vary the ratio of low-fidelity to high-fidelity data (e.g., 10:1, 100:1, 1000:1) to identify the break point where low-fidelity data begins to dominate and obscure high-fidelity signal.