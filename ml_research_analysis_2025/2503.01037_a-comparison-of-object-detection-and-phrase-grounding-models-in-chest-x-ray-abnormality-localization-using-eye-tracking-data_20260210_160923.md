---
ver: rpa2
title: A Comparison of Object Detection and Phrase Grounding Models in Chest X-ray
  Abnormality Localization using Eye-tracking Data
arxiv_id: '2503.01037'
source_url: https://arxiv.org/abs/2503.01037
tags:
- data
- abnormality
- bounding
- chest
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares object detection and phrase grounding models
  for localizing abnormalities in chest X-rays using eye-tracking data. The authors
  propose an automatic pipeline to generate bounding boxes for report sentences using
  radiologists' eye-tracking data, creating a benchmark for explainability in chest
  X-ray abnormality localization models.
---

# A Comparison of Object Detection and Phrase Grounding Models in Chest X-ray Abnormality Localization using Eye-tracking Data

## Quick Facts
- arXiv ID: 2503.01037
- Source URL: https://arxiv.org/abs/2503.01037
- Reference count: 0
- Primary result: Phrase grounding (MedRPG) outperforms object detection (YOLO-v5) in chest X-ray abnormality localization, achieving mIoU of 36% vs 20% and higher explainability (containment ratio 48% vs 26%).

## Executive Summary
This paper compares object detection and phrase grounding models for localizing abnormalities in chest X-rays using eye-tracking data. The authors propose an automatic pipeline to generate bounding boxes for report sentences using radiologists' eye-tracking data, creating a benchmark for explainability in chest X-ray abnormality localization models. They evaluate two models - YOLO-v5 (object detection) and MedRPG (phrase grounding) - on the REFLACX dataset. The phrase grounding model outperforms the object detection model in both performance (mIoU: 36% vs 20%) and explainability (containment ratio: 48% vs 26%), demonstrating the effectiveness of text in enhancing chest X-ray abnormality localization. The eye-tracking bounding boxes generated by the pipeline are shown to be learnable by deep neural networks and contain 75% of radiologists' annotations.

## Method Summary
The authors develop an automatic pipeline to generate bounding boxes from radiologists' eye-tracking data during report generation. Fixations from 1.5 seconds before sentence start through sentence end are aggregated into Gaussian heatmaps weighted by fixation duration, thresholded at 40% intensity, and enclosed by axis-aligned rectangles. They repurpose the REFLACX dataset by filtering sentences with "normal" (unless "abnormal" present) and "no," concatenating sentences implying ≥1 BB label, and randomly selecting highest-certainty BB for multi-BB statements. Two models are evaluated: YOLO-v5 for object detection and MedRPG with BioClinical BERT for phrase grounding. Performance is measured using mIoU, accuracy at IoU thresholds, and containment ratio against eye-tracking regions.

## Key Results
- Phrase grounding model (MedRPG) achieves mIoU of 36% compared to 20% for object detection (YOLO-v5)
- Phrase grounding model shows higher explainability with containment ratio of 48% vs 26% for object detection
- Eye-tracking bounding boxes contain 75% of radiologists' ground truth annotations and are learnable by deep neural networks
- The dataset repurposing strategy successfully creates learnable triplets for phrase grounding (1409 samples) and object detection (1998 samples)

## Why This Works (Mechanism)

### Mechanism 1: Text-Semantic Enrichment for Localization
- Claim: Phrase grounding outperforms object detection for abnormality localization when textual descriptions accompany images.
- Mechanism: Statements encode spatial cues (right, left, lower), severity descriptors, and relational context that constrain the search space and disambiguate visually similar regions. The text encoder (BioClinical BERT pretrained) maps these descriptions to a joint embedding space where visual features can be aligned more precisely than class labels alone permit.
- Core assumption: Radiology report sentences contain diagnostically relevant spatial and semantic information that directly corresponds to abnormality locations.
- Evidence anchors:
  - [abstract] "The better performance - mIoU = 0.36 vs. 0.20 - and explainability - Containment ratio 0.48 vs. 0.26 - of the phrase grounding model infers the effectiveness of text in enhancing chest X-ray abnormality localization."
  - [section 2.3] "A statement may contain information about location (right, left, central, lower, upper), size (small, medium, large), severity (mild, moderate, severe)... confidence (probable, possible, likely, unlikely), and evidence..."
  - [corpus] Related work on "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions" supports the role of textual knowledge in improving grounding, though corpus evidence on direct OD vs PG comparison is limited.
- Break condition: If report sentences lack spatial specificity or are generated without radiologist oversight, the semantic enrichment advantage degrades.

### Mechanism 2: Eye-Tracking Fixation to Bounding Box Pipeline
- Claim: Radiologist gaze fixations collected during report dictation can be algorithmically converted to learnable bounding box annotations.
- Mechanism: Fixations from a pre-sentence interval (1.5s before sentence start through sentence end) are aggregated into Gaussian heatmaps proportional to fixation duration, thresholded at 40% intensity, small regions removed (<1/400 image area), then enclosed by axis-aligned rectangles.
- Core assumption: Eye-tracking fixations during report generation spatially correlate with the abnormality being described, and the temporal alignment between fixations and spoken/written sentences is sufficiently precise.
- Evidence anchors:
  - [section 2.2] "We performed experiments with different values to set the thresholds and visually optimized them to enclose the ET fixations."
  - [section 3.2] "We trained and evaluated MedRPG using ET bounding boxes (test mIoU = 36.07), demonstrating that despite their inherent noise... these annotations remain learnable."
  - [corpus] "Gaze-based attention to improve the classification of lung diseases" (citation 16) and "Gaze-guided graph neural network for chest x-ray classification" (citation 4) show related gaze-integration approaches, though automatic BB generation from ET is novel per authors.
- Break condition: If radiologists routinely look at non-diagnostic regions (habitual scanning patterns) or if dictation delay exceeds the PSI window, fixations may not correspond to described abnormalities.

### Mechanism 3: Containment Ratio as Explainability Proxy
- Claim: A model whose predictions fall within radiologist eye-tracking regions is more explainable, as those regions represent clinically salient attention.
- Mechanism: Containment Ratio (CR) = area(predicted BB ∩ ET BB) / area(predicted BB). Higher CR indicates the model attends to regions radiologists deemed informative. This decouples localization accuracy (IoU with ground truth) from clinical plausibility (overlap with attention regions).
- Core assumption: Eye-tracking regions represent the minimal sufficient evidence a radiologist uses for diagnosis; predictions outside these regions are less explainable even if they overlap with ground truth annotations.
- Evidence anchors:
  - [section 2.3] "Therefore, if a model prediction for a label or statement falls within the ET bounding boxes, it is explainable."
  - [table 3] GT ET BBs contain 74.84% of radiologist annotations; MedRPG predictions achieve 47.55-56.14% CR vs YOLO-v5's 25.67-27%.
  - [corpus] No direct corpus evidence for CR as an explainability metric; this appears to be a novel contribution.
- Break condition: If ET bounding boxes systematically miss peripheral abnormalities (filtered out during thresholding), CR may penalize valid predictions.

## Foundational Learning

- Concept: **Phrase Grounding vs Object Detection**
  - Why needed here: The paper's central comparison requires understanding that OD predicts BBs given class labels, while PG predicts BBs given free-form text. PG can leverage richer semantic context but requires aligned image-text training data.
  - Quick check question: Given an image of a chest X-ray with a sentence "small left pleural effusion," would a phrase grounding model output one BB or multiple, and what information does it use that an object detector for "pleural effusion" cannot access?

- Concept: **Eye-Tracking Fixation Dynamics in Radiology**
  - Why needed here: The pipeline depends on understanding that fixations represent sustained attention (not saccades), are timestamped relative to report generation, and the pre-sentence interval captures anticipatory gaze.
  - Quick check question: Why does the pipeline collect fixations from 1.5 seconds before a sentence starts rather than only during the sentence?

- Concept: **IoU vs Containment Ratio**
  - Why needed here: These measure different things—IoU evaluates overlap with ground truth annotations (accuracy), while CR evaluates overlap with attention regions (explainability). A prediction can have high IoU but low CR.
  - Quick check question: A predicted BB has IoU=0.27 with the ground truth abnormality BB but CR=1.0 with the ET BB. What does this indicate about the prediction's accuracy versus explainability?

## Architecture Onboarding

- Component map:
  Raw inputs: Chest X-ray image + Timestamped report + Eye-tracking fixations
       ↓
  [Pipeline Step 1] Sentence-fixation temporal alignment (PSI=1.5s)
       ↓
  [Pipeline Step 2] Gaussian heatmap generation (duration-weighted)
       ↓
  [Pipeline Step 3] Thresholding (40%) + small region removal
       ↓
  [Pipeline Step 4] BB extraction (axis-aligned enclosure)
       ↓
  Generated ET BBs → Training data for PG model (MedRPG)
       ↓
  Inference: MedRPG(image, statement) → Predicted BB
       ↓
  Evaluation: IoU vs. ground truth BB; CR vs. ET BB

- Critical path:
  1. PSI parameter tuning directly affects which fixations map to which sentences
  2. Threshold and minimum region size determine BB size and noise filtering
  3. Text encoder pretraining (BioClinical BERT) drives phrase grounding performance—Table 1 shows 50.52 mIoU with pretraining vs 45.4 without

- Design tradeoffs:
  - Larger threshold → smaller, more precise BBs but risks filtering out legitimate fixations (Table 3 shows 25% of annotations not contained)
  - PSI window: longer captures more context but increases cross-sentence contamination
  - OD vs PG: OD handles multiple instances natively; PG produces one BB per statement but could benefit from confidence scores (noted in Discussion)

- Failure signatures:
  - OD model outputs IoU ≈ 0 on rare classes with <20 samples (Table 2: Acute fracture, Hiatal hernia, Pneumothorax)
  - PG model confused when one statement describes multiple BBs (section 2.1 notes random selection of highest-certainty BB)
  - ET BBs fail to contain annotations when thresholding removes small regions (25% containment loss)

- First 3 experiments:
  1. **Reproduce ET BB generation with varied PSI values (1.0s, 1.5s, 2.0s)** and measure impact on annotation containment rate and downstream PG mIoU to validate the 1.5s choice.
  2. **Ablate text encoder pretraining** by comparing BioClinical BERT vs. random initialization on the repurposed REFLACX triplets to quantify the semantic enrichment contribution.
  3. **Evaluate multi-BB prediction for PG** by modifying MedRPG to output top-k BBs with confidence scores for statements describing multiple abnormalities, comparing containment and IoU against the current single-BB approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does directly integrating eye-tracking data as an input modality into phrase grounding models improve localization performance and explainability beyond using eye-tracking solely for generating ground-truth bounding boxes?
- Basis in paper: [explicit] The authors state that "integrating ET into a phrase grounding model to enhance its performance and explainability" is a specific future direction of this study.
- Why unresolved: The current study uses eye-tracking data only to automatically generate bounding boxes (ET bounding boxes) for training and evaluation, rather than fusing gaze information directly into the model's architecture.
- What evidence would resolve it: A study implementing an architecture that fuses visual, textual, and gaze features, demonstrating higher mIoU or Containment Ratios compared to the MedRPG baseline.

### Open Question 2
- Question: Can modifying phrase grounding architectures to predict multiple bounding boxes with associated confidence scores for a single statement improve alignment with clinical diagnoses?
- Basis in paper: [explicit] The authors note that "predicting multiple bounding boxes for each statement perfectly matches the nature of medical domain and the confidence score corresponds to the certainty level."
- Why unresolved: The utilized MedRPG model is designed to output a single bounding box per statement, whereas medical findings described in a sentence may span multiple distinct regions.
- What evidence would resolve it: A modified phrase grounding model that outputs multiple regions for a single description, validated against annotations showing higher recall for multi-focal abnormalities.

### Open Question 3
- Question: Do the observed performance gaps between phrase grounding and object detection models hold when trained on significantly larger datasets?
- Basis in paper: [explicit] The authors list the "small size of the dataset" as a main limitation and suggest "applying these approaches to a bigger dataset will produce more reliable results."
- Why unresolved: The experiments were conducted on a dataset split with only 915 training images, which may disadvantage the object detection model (YOLO-v5) more than the pre-trained phrase grounding model.
- What evidence would resolve it: Replicating the comparative experiment on a larger dataset (e.g., the full MIMIC-CXR) to see if the object detection model narrows the performance gap with the phrase grounding model.

### Open Question 4
- Question: How does the absence of radiologist verification for the automatically repurposed sentence-bounding box pairs impact the training reliability of the phrase grounding model?
- Basis in paper: [explicit] The authors identify "the lack of a radiologist collaborator to approve statements we extracted for bounding boxes in dataset repurposing" as a limitation.
- Why unresolved: The dataset repurposing relied on automated keyword filtering (e.g., removing sentences with "no") and random selection of bounding boxes for multi-label sentences, potentially introducing label noise.
- What evidence would resolve it: An ablation study comparing model performance when trained on the automatically repurposed dataset versus a dataset verified/cleaned by a radiologist.

## Limitations
- The automatic ET bounding box generation pipeline relies on assumptions about fixation-heatmap correspondence that lack direct validation—the 1.5s pre-sentence interval and 40% threshold were "visually optimized" rather than empirically determined.
- The single-BB output constraint for multi-abnormality statements introduces annotation ambiguity, with random selection potentially masking systematic biases.
- The containment ratio as an explainability metric, while intuitively grounded in clinical attention, lacks corpus support as a validated explainability measure.

## Confidence
- **High confidence**: The comparative performance gap between phrase grounding (mIoU 36%) and object detection (mIoU 20%) is clearly established and reproducible given the dataset and methodology.
- **Medium confidence**: The claim that eye-tracking regions represent clinically salient attention sufficient for explainability evaluation—while supported by 74.84% annotation containment, this assumes ET fixations always correspond to diagnostic reasoning rather than habitual scanning.
- **Low confidence**: The generalizability of the ET BB pipeline beyond REFLACX dataset conditions, given the specific radiologist reporting workflow and dataset curation.

## Next Checks
1. Perform ablation study varying the pre-sentence interval (PSI) from 0.5s to 2.5s in 0.5s increments to quantify sensitivity of ET BB generation and downstream model performance.
2. Conduct radiologist review of a stratified sample of ET-generated BBs to validate clinical relevance and identify systematic failure modes in the automatic pipeline.
3. Implement multi-BB prediction for MedRPG using confidence scores to handle statements describing multiple abnormalities, comparing containment ratios against the current single-BB approach.