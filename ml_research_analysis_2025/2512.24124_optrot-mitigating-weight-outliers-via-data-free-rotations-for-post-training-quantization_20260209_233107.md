---
ver: rpa2
title: 'OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training
  Quantization'
arxiv_id: '2512.24124'
source_url: https://arxiv.org/abs/2512.24124
tags:
- quantization
- optrot
- weight
- rotations
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OptRot, a data-free method for mitigating weight
  outliers in large language models to improve post-training quantization. The key
  idea is to learn fusible rotations by minimizing the element-wise fourth power of
  rotated weights, which provably reduces weight incoherence and improves quantization
  error bounds.
---

# OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization

## Quick Facts
- arXiv ID: 2512.24124
- Source URL: https://arxiv.org/abs/2512.24124
- Authors: Advait Gadhikar; Riccardo Grazzi; James Hensman
- Reference count: 40
- Primary result: Data-free fusible rotations that reduce weight outliers and improve post-training quantization, outperforming Hadamard rotations and matching data-dependent methods in W4A8 settings.

## Executive Summary
OptRot introduces a data-free method for mitigating weight outliers in large language models to improve post-training quantization. The approach learns fusible rotations by minimizing the element-wise fourth power of rotated weights, which provably reduces weight incoherence and improves quantization error bounds. The method achieves better incoherence and weight quantization performance while being quantization-agnostic and data-free, matching or outperforming more expensive, data-dependent alternatives like SpinQuant in W4A8 settings.

## Method Summary
OptRot learns orthogonal rotations R₁ and R₂,l that are applied to specific weight matrices in transformer layers before quantization. R₁ is shared across all layers and applied to query/key/gate/up projections, while R₂,l is layer-specific for value and output projections. The rotations are optimized using Cayley-SGD on the Stiefel manifold to minimize the element-wise fourth power of rotated weights (∥vec(W̃)∥₄⁴), which serves as a smooth proxy for weight incoherence. After optimization, the rotations are fused into the weights, eliminating runtime overhead. The method is evaluated on Llama and Qwen model families with GPTQ quantization, showing consistent improvements in KL divergence and downstream task performance.

## Key Results
- Outperforms Hadamard rotations and matches SpinQuant in W4A8 settings while being data-free
- Reduces weight incoherence by up to 50% in top-50 layers, achieving 80% of full-layer gains
- Data-dependent extension (OptRot+) further improves performance by incorporating activation covariance via Hessian upper bounds
- Trade-off observed: performs worse than SpinQuant in W4A4 settings where activation outliers dominate error

## Why This Works (Mechanism)

### Mechanism 1: Fourth-Power Minimization as Smooth Incoherence Proxy
- Claim: Minimizing the element-wise fourth power of rotated weights reduces weight incoherence, which upper-bounds quantization error for both RTN and GPTQ.
- Mechanism: Weight incoherence µ_W = √(mn)·w_max/||W||_F depends non-smoothly on w_max. Replacing w_max with the 4-norm ∥vec(W̃)∥₄₄ provides a smooth, differentiable proxy that can be optimized via gradient descent while still penalizing outliers. The fourth power specifically relates to kurtosis, a measure of tailedness.
- Core assumption: Lower weight incoherence translates to lower actual quantization error in practice, not just tighter bounds.
- Evidence anchors:
  - [abstract]: "OptRot, which reduces weight outliers simply by minimizing the element-wise fourth power of the rotated weights."
  - [section 3.1, Theorem 3.1]: RTN error bound scales with µ²_W.
  - [section 3.1, Theorem 3.4]: GPTQ error bound also scales with µ²_W.
  - [corpus]: Kurtail (Akhondzadeh et al., 2025) similarly uses kurtosis-based outlier penalization.
- Break condition: When activation outliers dominate error (observed in W4A4 setting—Table 8 shows OptRot underperforms SpinQuant here).

### Mechanism 2: Fusible Rotation Placement Preserves Function Without Inference Overhead
- Claim: Rotations R1 and R2 can be learned at specific transformer layer boundaries and fused with weights, eliminating runtime cost.
- Mechanism: Rotations applied as W̃ = R₁ᵀW (for projection weights) or W̃ = R₁ᵀWR₂ (for value/output projections) preserve the forward pass when counter-rotations are applied to activations or fused into adjacent layers. R1 is shared across all layers; R2,l is layer-specific.
- Core assumption: LLM computation graphs are rotation-invariant at the specified insertion points (after RMSNorm, before attention projections).
- Evidence anchors:
  - [abstract]: "learn fusible rotations by minimizing principled and cheap proxy objectives to the weight quantization error."
  - [Figure 1]: Schematic showing R1, R2 as fusible (yellow) vs R3, R4 as online (red/orange).
  - [corpus]: QuaRot and SpinQuant use the same fusible rotation principle; ParoQuant extends to pairwise rotations.
- Break condition: If architectural modifications (e.g., non-rotational normalization, RoPE interference) break invariance.

### Mechanism 3: Hessian Upper Bound Term Captures Feature Correlation (OptRot+)
- Claim: Adding a data-dependent Hessian term UB = tr(H) - ||H||²_off/(2·tr(H)) to the objective improves rotation quality by accounting for activation covariance structure.
- Mechanism: The GPTQ error bound includes tr(HLᵀL) from the LDL decomposition. Rather than backpropagating through Cholesky (O(n³)), the paper uses the smooth upper bound UB which is O(n²), parallelizable, and rotation-sensitive via ||H||_off.
- Core assumption: Higher feature correlation (larger off-diagonal Hessian entries) indicates structured activations that GPTQ can exploit more effectively.
- Evidence anchors:
  - [section 4.2, Eq. 9]: Derivation of tr(HLᵀL) ≤ 2UB.
  - [Figure 2]: Sweep showing UB·||w||₄ achieves lowest KL divergence among joint objectives.
  - [corpus]: OSTQuant and FlatQuant also leverage Hessian/covariance information but with different parameterizations.
- Break condition: When Hessian computation is prohibitive (large models, limited calibration data) or when UB poorly approximates tr(D) (Figure 5 shows gaps for non-Hadamard eigenvectors).

## Foundational Learning

- **Weight Incoherence (µ_W)**:
  - Why needed here: Core metric the method optimizes; measures outlier prevalence via max/mean ratio.
  - Quick check question: If all weights in a matrix were equal to some constant c, what would µ_W equal? (Answer: 1, the minimum)

- **LDL Decomposition in GPTQ**:
  - Why needed here: GPTQ quantizes weights column-by-column using Hessian information; the L matrix appears in error bounds.
  - Quick check question: Why does GPTQ use LDL rather than direct inversion of the Hessian H? (Answer: H is often singular or ill-conditioned; LDL provides stable triangular solves)

- **Stiefel Manifold / Cayley Transform**:
  - Why needed here: Rotations must remain orthogonal during optimization; Cayley-SGD maintains this constraint.
  - Quick check question: What happens if you update a rotation matrix with standard SGD without projection? (Answer: It drifts from orthogonality, invalidating the fuse-and-preserve-property)

## Architecture Onboarding

- **Component map**:
  - R1: Single shared rotation applied before all q/k/gate/up projections; fused into weights.
  - R2,l: Per-layer rotation for value and output projections; fused into W_v and W_o.
  - R3, R4: Online rotations (not learned, Hadamard) applied during inference if activation quantization is needed.
  - Objective: ∑_l ∥vec(W̃^(l))∥₄₄ (data-free) or ∑_l UB^(l)·∥vec(W̃^(l))∥₄⁴ (data-dependent).

- **Critical path**:
  1. Initialize R1, R2,l as identity or Hadamard.
  2. Compute rotated weights W̃ for all linear layers.
  3. Evaluate loss (4th-power sum ± UB term).
  4. Backpropagate via Cayley-SGD for 1000 steps (lr=1).
  5. Fuse final rotations into weights.
  6. Apply GPTQ with calibration set (512 samples × 256 tokens from C4).

- **Design tradeoffs**:
  - OptRot (data-free) vs OptRot+ (data-dependent): ~5-10% KL improvement for OptRot+, but requires Hessian computation.
  - Top-50 vs all layers: Top-50 gives 80% of gains with fraction of compute (Table 2).
  - W4A8 vs W4A4: Method excels at weight quantization; activation quantization at 4-bit shows trade-off where reducing weight incoherence can worsen activation quantization (Table 8).

- **Failure signatures**:
  - KL divergence not decreasing: Check Cayley learning rate (try 0.1-10 range); verify orthogonal constraint maintained.
  - Worse than baseline in W4A4: Expected; activation outliers dominate—use SpinQuant or add online rotations.
  - OptRot+ unstable: Loss scaling may be needed (authors scale initial loss to match OptRot magnitude).

- **First 3 experiments**:
  1. Reproduce Figure 3 on a single layer: Plot µ_W before/after OptRot optimization to verify incoherence reduction.
  2. Ablate p-norm choice: Compare ∥w∥₄ vs ∥w∥₆ vs ∥w∥∞ approximation on WikiText perplexity.
  3. Compare data-free vs data-dependent: Run OptRot and OptRot+ on Llama-3.2-1B with W4 quantization; measure KL gap and wall-clock time for rotation learning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the fundamental trade-off between minimizing weight incoherence and preserving activation quantization performance be resolved to enable effective W4A4 (4-bit weight, 4-bit activation) quantization?
- **Basis in paper:** [explicit] The authors state in the Abstract and Conclusion that OptRot performs worse in the W4A4 setting, "highlighting a trade-off between weight and activation quantization."
- **Why unresolved:** The paper establishes that reducing weight outliers via rotation worsens activation outliers in low-bit regimes, but does not propose a mechanism to balance or optimize both simultaneously.
- **What evidence would resolve it:** A modified rotation objective or regularization term that maintains low weight incoherence while minimizing activation outlier emergence in W4A4 benchmarks.

### Open Question 2
- **Question:** How can the theoretical error bounds be refined to better account for the discrepancy between the "constrained LDL" used in proofs and the standard LDL decomposition used in practical GPTQ implementations?
- **Basis in paper:** [explicit] Appendix C.1 notes a "significant gap" between the theoretical constrained LDL and the true LDL observed in practice, leaving the tightness of the bounds in Theorem 3.4 somewhat ambiguous.
- **Why unresolved:** The derivation relies on a constrained decomposition (Definition 3.2) that differs from the implementation, meaning the theoretical guarantee does not strictly map to the empirical method.
- **What evidence would resolve it:** A refined theorem that bounds the error of standard LDL with clamping, or empirical proof that the constrained LDL is a sufficiently tight approximation for all layers.

### Open Question 3
- **Question:** Can a data-free proxy objective be designed to capture cross-layer weight interactions, similar to how end-to-end methods like SpinQuant (W4) compensate for quantization errors?
- **Basis in paper:** [inferred] Appendix G notes that SpinQuant (W4) outperforms OptRot on RTN tasks because it "can exploit interactions between weights," whereas OptRot minimizes a layerwise upper bound independently.
- **Why unresolved:** OptRot treats layers independently to remain efficient and data-free, potentially missing global error corrections that learned interactions provide.
- **What evidence would resolve it:** An extension of OptRot that optimizes rotations for multiple layers jointly (without data) and matches or exceeds the performance of interaction-aware methods like SpinQuant.

## Limitations
- Performance trade-off in W4A4 settings where activation quantization dominates error, limiting applicability to low-bit activation regimes.
- Reliance on standard transformer architectures; effectiveness on non-transformer models or alternative attention mechanisms remains untested.
- Hessian computation for OptRot+ may become prohibitive for very large models or when calibration data is scarce.

## Confidence

- **High confidence**: The theoretical bounds connecting weight incoherence to quantization error (Theorems 3.1 and 3.4) are well-established and the experimental validation on Llama and Qwen models is comprehensive for the tested settings.
- **Medium confidence**: The mechanism by which minimizing the fourth power of rotated weights translates to practical quantization improvements is plausible but relies on the assumption that µ_W reduction directly correlates with lower actual quantization error, which may not hold uniformly across all weight distributions.
- **Low confidence**: The optimality of the 4-norm proxy for w_max and the choice of 1000 optimization steps with fixed learning rate are empirical decisions without theoretical justification for their specific values.

## Next Checks

1. **Cross-architecture validation**: Test OptRot on a non-transformer architecture (e.g., Mamba or RWKV) to verify that the fusible rotation approach generalizes beyond standard attention-based models.
2. **Ablation on norm choice**: Systematically compare the 4-norm proxy against 6-norm and infinity-norm approximations across multiple weight matrices to quantify sensitivity to this design choice.
3. **Activation quantization synergy**: Investigate whether combining OptRot with activation outlier mitigation techniques (e.g., scale optimization or outlier clipping) can close the W4A4 performance gap with SpinQuant.