---
ver: rpa2
title: Efficient Contrastive Decoding with Probabilistic Hallucination Detection -
  Mitigating Hallucinations in Large Vision Language Models -
arxiv_id: '2504.12137'
source_url: https://arxiv.org/abs/2504.12137
tags:
- hallucination
- decoding
- hallucinations
- ours
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Efficient Contrastive Decoding (ECD), a method
  to mitigate hallucinations in large vision-language models (LVLMs) by leveraging
  probabilistic hallucination detection. Instead of relying on contrastive decoding
  with distorted inputs, ECD uses a lightweight classifier trained on internal LVLM
  features to estimate hallucination scores.
---

# Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -

## Quick Facts
- arXiv ID: 2504.12137
- Source URL: https://arxiv.org/abs/2504.12137
- Authors: Laura Fieback; Nishilkumar Balar; Jakob Spiegelberg; Hanno Gottschalk
- Reference count: 40
- Primary result: Training-free hallucination mitigation method achieving up to 5.74 pp reduction in hallucination rates while maintaining computational efficiency

## Executive Summary
This work introduces Efficient Contrastive Decoding (ECD), a novel method to mitigate hallucinations in Large Vision Language Models (LVLMs) through probabilistic hallucination detection. Unlike traditional contrastive decoding approaches that rely on distorted inputs, ECD leverages internal LVLM features to train a lightweight classifier that estimates hallucination scores. These scores are then used to penalize hallucinated tokens during decoding, effectively shifting the output distribution toward more accurate responses. The method is training-free, applicable to any open-source LVLM, and adds minimal computational overhead.

The approach demonstrates significant improvements across multiple benchmarks, reducing hallucination rates by up to 5.74 percentage points (32%) in open-ended captioning tasks and improving F1 scores by up to 23.02 percentage points (33%) in discriminative VQA benchmarks. ECD consistently outperforms state-of-the-art methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) across various LVLMs, including LLaVA 1.5, InstructBLIP, and MiniGPT-4, while maintaining lower computational costs.

## Method Summary
ECD addresses LVLM hallucinations by using a lightweight classifier trained on internal model features to estimate hallucination scores. During inference, these scores are incorporated into the decoding process through a modified probability distribution that penalizes tokens with high hallucination likelihood. The method operates entirely during inference without requiring LVLM fine-tuning, making it broadly applicable. The hallucination classifier is trained on annotated hallucination data using internal LVLM features as input, creating a probabilistic detection mechanism that guides the decoding process toward more accurate outputs.

## Key Results
- Reduces hallucination rates by up to 5.74 percentage points (32%) in open-ended captioning tasks
- Improves F1 scores by up to 23.02 percentage points (33%) in discriminative VQA benchmarks
- Outperforms state-of-the-art methods like VCD and ICD across multiple LVLM architectures while maintaining lower computational costs

## Why This Works (Mechanism)
The method works by exploiting the internal representations of LVLMs to detect when they are likely to hallucinate. By training a classifier on these internal features, ECD can estimate the probability that a given token will be hallucinated. This probabilistic information is then used to modify the token distribution during decoding, effectively steering the model away from likely hallucinated outputs. The contrastive nature of the approach allows it to balance between accuracy and hallucination reduction without requiring extensive model retraining.

## Foundational Learning
- **Contrastive Decoding**: Technique that compares model outputs with distorted versions to improve accuracy - needed for understanding how ECD builds on existing methods; quick check: verify that ECD maintains contrastive principles while reducing computational overhead
- **Vision-Language Model Internal Features**: The intermediate representations learned by LVLMs that capture visual and linguistic information - needed for understanding how ECD extracts hallucination signals; quick check: confirm that feature extraction doesn't significantly impact inference speed
- **Probabilistic Token Scoring**: Method of assigning confidence scores to tokens based on likelihood of correctness - needed for understanding how ECD modifies the decoding distribution; quick check: validate that hallucination scores correlate with actual error rates
- **Inference-time Optimization**: Techniques that improve model performance without retraining - needed for understanding ECD's practical advantages; quick check: measure the additional latency introduced by hallucination detection
- **Cross-modal Alignment**: The ability of LVLMs to properly associate visual inputs with textual outputs - needed for understanding hallucination sources; quick check: assess whether ECD improves alignment metrics
- **Token-level Confidence Estimation**: Process of determining per-token reliability during generation - needed for understanding how ECD applies penalties; quick check: verify that confidence scores are stable across similar inputs

## Architecture Onboarding

Component Map:
Vision Encoder -> LVLM Backbone -> Feature Extractor -> Hallucination Classifier -> Modified Decoder

Critical Path:
Input Image → Vision Encoder → LVLM Backbone → Feature Extractor → Hallucination Classifier → Probability Modification → Output Generation

Design Tradeoffs:
- Training-free approach vs. need for hallucination-annotated data
- Computational efficiency vs. potential accuracy improvements from fine-tuning
- Generic applicability vs. potential gains from model-specific optimization
- Probabilistic detection vs. deterministic hallucination filtering

Failure Signatures:
- Inconsistent hallucination scores across similar inputs
- Performance degradation on tasks with high inherent ambiguity
- Computational overhead that negates efficiency benefits
- Over-penalization leading to overly conservative outputs

First Experiments:
1. Measure inference latency overhead with and without ECD on a representative LVLM
2. Compare hallucination detection accuracy of ECD classifier vs. human annotations
3. Evaluate performance degradation on out-of-distribution inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Requires annotated hallucination data for training the lightweight classifier
- Performance gains show variability across different model architectures and task types
- Contrastive nature may still introduce some computational overhead compared to standard decoding
- Limited evaluation on multilingual datasets and non-English LVLMs
- Uncertainty about effectiveness on long-form generation tasks

## Confidence
- High Confidence: Core methodology of using internal LVLM features for hallucination detection and demonstrated effectiveness in reducing hallucination rates
- Medium Confidence: Computational efficiency claims relative to existing contrastive decoding methods, based on reported numbers but needing more extensive ablation studies
- Medium Confidence: Generalization capabilities across diverse LVLM architectures and downstream tasks, based on limited set of models and benchmarks

## Next Checks
1. Conduct extensive ablation studies to quantify the impact of different hallucination classifier architectures and training strategies on final performance
2. Evaluate the method's effectiveness on multilingual datasets and non-English LVLMs to assess cross-lingual generalization
3. Test the approach on long-form generation tasks to determine if hallucination detection remains effective over extended output sequences