---
ver: rpa2
title: Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models
arxiv_id: '2502.15950'
source_url: https://arxiv.org/abs/2502.15950
tags:
- loss
- data
- mixture
- mixtures
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Mixture of Data Experts (MDE) as an efficient
  approximation method to optimize language model pre-training data mixtures. The
  core idea is to train individual expert models on each training domain and then
  form an ensemble model weighted by the candidate mixture proportions.
---

# Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models

## Quick Facts
- **arXiv ID**: 2502.15950
- **Source URL**: https://arxiv.org/abs/2502.15950
- **Reference count**: 28
- **Primary result**: MDE improves mixture ranking quality (Spearman 0.65→0.95) and achieves 3x computational savings vs full regression methods

## Executive Summary
This paper addresses the challenge of optimizing pre-training data mixtures for large language models by proposing Mixture of Data Experts (MDE) as an efficient approximation method. The approach trains individual expert models on each training domain and uses their weighted probability ensemble to approximate the loss of a model trained on any candidate mixture. This MDE loss serves as a highly predictive feature for regression models that rank mixture performance, substantially outperforming prior methods that use only mixture weights as input features. Experiments demonstrate that MDE-optimized mixtures lead to better downstream task performance compared to heuristic baselines and prior optimization methods.

## Method Summary
The method trains k expert models (one per domain) and uses their weighted probability outputs to approximate the cross-entropy loss of any candidate mixture λ. For each validation domain, per-token probabilities from all experts are cached and combined via λ-weighted averaging to compute MDE losses. These losses, along with mixture weights, serve as features in regression models (Linear, GBM, MTGP) that predict validation losses for candidate mixtures. An optimizer (Vizier) then searches for λ minimizing predicted aggregate loss. The approach can be used standalone with just k proxy models (achieving 3x computational savings) or combined with regression models for improved accuracy.

## Key Results
- MDE features improve Spearman correlation from 0.65 to 0.95 for linear regressors
- MDE standalone achieves comparable performance to full regression at 3x less computational cost
- Optimizing mixtures using end-task validation losses (AVG-ET) correlates 0.772 with downstream accuracy vs 0.320 for training-domain losses
- MDE-optimized mixtures achieve 38.2% average accuracy vs 35.2% for prior methods on downstream tasks

## Why This Works (Mechanism)

### Mechanism 1: Probability-Weighted Ensemble Approximation
Aggregating predictions from domain-specific expert models approximates the loss of a model jointly trained on a data mixture. The ensemble forms next-token distribution as weighted average of expert probability outputs: P_MDE(x_t|x_{<t}, λ) = Σ λ_i · P_{θ*_i}(x_t|x_{<t}). This ensemble's cross-entropy on validation domains approximates what a jointly-trained model would achieve. The core assumption is that the distribution minimizing pre-training loss for a λ-weighted mixture can be expressed as a weighted combination of domain-optimal distributions, with weights related to λ_i (idealized when domains share marginal prefix distributions).

### Mechanism 2: Feature Enrichment for Regression Models
MDE loss estimates provide highly predictive features that substantially improve regression model accuracy for predicting mixture performance. Instead of using only mixture weights λ as input features (prior work), MDE adds k × m features (expert-based loss estimates per validation domain). These capture non-linear interactions between domains that pure weight-based features miss. The relationship between mixture weights and validation loss has structure that MDE features partially capture but raw weights cannot.

### Mechanism 3: End-Task Validation Signal Alignment
Optimizing mixtures using validation loss on end-task domains correlates more strongly with downstream task accuracy than optimizing on training-domain validation loss alone. End-task validation examples provide signal about capabilities actually evaluated downstream, whereas training-domain losses may optimize for domain-specific perplexity without transfer. Cross-entropy loss on task-formatted validation examples predicts generation/ranking accuracy on related test tasks.

## Foundational Learning

- **Concept: Cross-entropy loss as training and evaluation metric**
  - Why needed: MDE approximates cross-entropy loss; understanding what it measures (next-token prediction quality) is essential for interpreting results
  - Quick check: Can you explain why lower cross-entropy on a validation domain indicates better model fit, and why it may not directly correspond to task accuracy?

- **Concept: Ensemble methods (probability averaging vs. parameter averaging)**
  - Why needed: MDE uses probability-weighted ensembles; the paper explicitly contrasts this with parameter averaging, which fails for independently-trained models
  - Quick check: Why does averaging model parameters from independently pre-trained experts fail (Spearman -0.14 in Table 12), while averaging their output probabilities succeeds (Spearman 0.95)?

- **Concept: Bi-level optimization and proxy-based methods**
  - Why needed: Data mixture optimization is inherently bi-level (inner: train model; outer: select mixture); proxy models make this tractable
  - Quick check: What tradeoffs are involved in choosing proxy model size and training duration, and how does the paper determine 280M/5B tokens is sufficient (Figure 3)?

## Architecture Onboarding

- **Component map**: Data Experts -> MDE Approximation -> Regression Model -> Optimizer
- **Critical path**: 1) Train k data experts on individual domains 2) For each validation domain, cache per-token probabilities from all experts 3) Sample ~25 mixture configurations, train proxy models, observe losses 4) Fit regression model using λ + MDE features 5) Run optimizer to find λ minimizing predicted aggregate loss 6) Train target model (1B/100B tokens) with optimized mixture
- **Design tradeoffs**: Proxy size: Smaller proxies (70M) work for single-domain ranking but fail on aggregate metrics (Figure 3); 280M recommended; MDE standalone vs. MDE+regression: Standalone requires only k models but is less accurate; regression adds ~25 proxy training runs; Validation objective: AVG-SP optimizes training-domain perplexity; AVG-ET optimizes for downstream transfer
- **Failure signatures**: Parameter averaging of experts yields near-random rankings (Table 12: Spearman -0.14); Very small proxies (<6K steps) fail to predict aggregate metrics correctly; Optimizing only AVG-SP can hurt downstream accuracy (MTGP-MDE-SP: 35.2% vs. token-proportional baseline: 37.0%)
- **First 3 experiments**: 1) Validate MDE approximation: For held-out mixtures, compare MDE-predicted losses against actual proxy model losses; expect Spearman >0.90 on SlimPajama domains 2) Ablate regression features: Train regression with λ-only vs. λ+MDE features; verify MDE improves Spearman by ≥0.2 (Table 1) 3) Proxy scale sensitivity: Train proxies at 70M, 150M, 280M on same mixtures; confirm 280M is needed for aggregate ranking (Figure 3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an iterative Bayesian optimization process using MTGP uncertainty estimates find optimal mixtures with fewer pre-training runs than the current batch approach?
- Basis: The Conclusion proposes leveraging MTGP confidence measures to balance exploitation and exploration dynamically, rather than generating all mixtures in a single batch
- Why unresolved: The current study relies on a static set of proxy models; the proposed dynamic feedback loop has not been implemented or benchmarked for sample efficiency
- What evidence would resolve it: Experiments demonstrating that an iterative selection process converges to an optimal mixture using fewer total proxy training steps compared to the fixed-sample method

### Open Question 2
- Question: Can MDE features effectively predict downstream task accuracy metrics directly, rather than relying on cross-entropy loss as a proxy?
- Basis: The Conclusion suggests extending the regression models to predict downstream task generation accuracy directly, utilizing MDE features from sequence probabilities
- Why unresolved: The current work optimizes for cross-entropy loss and relies on its correlation with downstream performance; direct accuracy prediction remains untested
- What evidence would resolve it: A regression model trained to predict task-specific Exact Match scores using MDE features, which subsequently outperforms loss-based optimization in downstream evaluations

### Open Question 3
- Question: Does the efficiency and rank-invariance of the MDE approximation hold for models significantly larger than 1B parameters or training runs exceeding 100B tokens?
- Basis: The authors limit experiments to 1B parameters and 100B tokens, noting that assessing effectiveness on larger scales is an important area for future research
- Why unresolved: It is unclear if the linear relationship and proxy quality hold at massive scales, particularly given potential "diminishing returns from data repetition"
- What evidence would resolve it: Validation of MDE ranking quality (Spearman correlation) when predicting mixtures for 7B+ parameter models or multi-trillion token training runs

## Limitations

- The theoretical justification relies on idealized conditions where domains share marginal prefix distributions, which rarely holds in real-world heterogeneous datasets
- Computational overhead of training k expert models plus proxy models can be substantial, particularly for large k or when using larger expert sizes than minimum viable 280M parameters
- End-task validation optimization requires carefully curated validation sets that mirror downstream task formats, which may not always be available or representative

## Confidence

**High Confidence**: MDE ensemble probability averaging substantially outperforms parameter averaging of independently trained experts (Spearman correlation improvement from -0.14 to 0.95); Adding MDE features to regression models significantly improves mixture ranking accuracy compared to weight-only features (Spearman 0.65 → 0.95 for linear regressors); MDE on its own with k proxy models achieves comparable performance to full regression approaches at 3x less computational cost; Optimizing mixtures using end-task validation losses (AVG-ET) correlates more strongly with downstream accuracy than training-domain optimization (AVG-SP correlation 0.772 vs 0.320)

**Medium Confidence**: The 280M parameter proxy size is optimal for aggregate metric prediction, as smaller proxies fail on multi-domain aggregation (Figure 3); MDE-optimized mixtures consistently outperform heuristic baselines and prior optimization methods on downstream tasks (38.2% vs 35.2% accuracy); The theoretical approximation quality depends on domain prefix distribution similarity, which is plausible but not extensively validated across diverse datasets

**Low Confidence**: MDE's effectiveness scales to 100B+ parameter models without modification (only validated up to 1B parameters); The method generalizes to datasets with vastly different domain characteristics than SlimPajama (e.g., highly imbalanced or multimodal domains); End-task validation optimization consistently improves transfer without overfitting to specific validation tasks

## Next Checks

1. **Domain Similarity Stress Test**: Systematically evaluate MDE approximation quality as a function of domain dissimilarity in prefix distributions. Create controlled experiments with domains that have increasingly divergent marginal distributions and measure how MDE's approximation error scales. This would validate the theoretical assumptions about when MDE works and establish practical guidelines for dataset selection.

2. **Scale-Up Validation**: Extend experiments to 10B+ parameter models to verify that MDE's computational efficiency advantages (3x reduction with standalone MDE) persist at scale. Additionally, test whether the 280M proxy size remains optimal or requires adjustment for larger target models, and measure any degradation in approximation quality.

3. **End-Task Validation Robustness**: Conduct ablation studies where end-task validation sets are progressively corrupted or reduced in size to determine the minimum viable validation set size and quality requirements. This would establish practical constraints on when AVG-ET optimization is feasible versus when it risks overfitting to unrepresentative validation data.