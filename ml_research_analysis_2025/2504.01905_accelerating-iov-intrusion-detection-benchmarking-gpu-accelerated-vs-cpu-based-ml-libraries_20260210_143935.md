---
ver: rpa2
title: 'Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based
  ML Libraries'
arxiv_id: '2504.01905'
source_url: https://arxiv.org/abs/2504.01905
tags:
- detection
- scikit-learn
- cuml
- datasets
- intrusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares GPU-accelerated (cuML) versus CPU-based (scikit-learn)
  machine learning for intrusion detection in the Internet of Vehicles (IoV), focusing
  on computational efficiency. Using three IoV datasets (OTIDS, GIDS, CICIoV2024)
  and models like Random Forest, KNN, Logistic Regression, and XGBoost, it found GPU
  implementations dramatically accelerated training (up to 159x faster) and prediction
  (up to 95x faster) while preserving detection accuracy.
---

# Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries

## Quick Facts
- arXiv ID: 2504.01905
- Source URL: https://arxiv.org/abs/2504.01905
- Reference count: 23
- Primary result: GPU implementations achieved up to 159x faster training and 95x faster prediction while preserving detection accuracy

## Executive Summary
This study benchmarks GPU-accelerated (cuML) versus CPU-based (scikit-learn) machine learning libraries for intrusion detection in Internet of Vehicles (IoV) networks. Using three IoV datasets and models including Random Forest, KNN, Logistic Regression, and XGBoost, the research demonstrates that GPU implementations dramatically accelerate training (up to 159x faster) and prediction (up to 95x faster) while maintaining comparable detection accuracy and F1 scores. The findings highlight GPU acceleration as a critical enabler for real-time IoV security, though cuML has some limitations like missing parameters for handling imbalanced data. A hybrid approach using GPU for computationally intensive tasks and CPU for final model deployment is recommended.

## Method Summary
The study compares GPU-accelerated cuML library against CPU-based scikit-learn for IoV intrusion detection using three CAN bus datasets: OTIDS (4.6M samples), GIDS (14.4M samples), and CICIoV2024 (1.4M samples). Data preprocessing involved converting hex payloads to decimal, handling NaN values, removing timestamp/ID columns, and applying LabelEncoder and StandardScaler. Four machine learning models (Random Forest, KNN, Logistic Regression, XGBoost) were trained and evaluated using consistent hyperparameters across both libraries. Performance metrics included accuracy, F1-score, training time, and prediction time. Hardware configuration used Intel i9-10900X CPU and NVIDIA RTX A4000 GPU (16GB VRAM).

## Key Results
- KNN achieved the highest speedups: 159x faster training and 95x faster prediction on GPU
- Random Forest showed 11x-45x training acceleration with minor accuracy trade-offs (82.37%→77.23% on OTIDS)
- Detection accuracy and F1 scores remained comparable between libraries for most models
- GPU acceleration enables real-time threat detection capabilities critical for IoV security

## Why This Works (Mechanism)

### Mechanism 1: GPU Parallelization of Distance-Based Computations
KNN achieves the most dramatic speedups (up to 159x training, 56-58x prediction) because distance computations across all training samples are highly parallelizable on GPU architectures. GPUs execute these independent distance calculations simultaneously across thousands of CUDA cores, whereas CPUs process them sequentially. Speedup magnitude correlates with the degree of parallelizable operations; KNN's O(n) prediction complexity per sample becomes parallel work.

### Mechanism 2: Tree Ensemble Parallelization via Independent Tree Construction
Random Forest achieves substantial speedups (11x-45x training) because individual decision trees in the ensemble can be constructed independently and in parallel. RF trains multiple decision trees on bootstrapped samples, and GPU implementations distribute tree construction across parallel threads, reducing sequential training time while maintaining statistical independence of trees.

### Mechanism 3: Accuracy Preservation via Algorithmic Equivalence
Detection accuracy and F1 scores remain comparable between cuML and scikit-learn because both libraries implement the same underlying algorithms with mathematically equivalent operations. Given identical hyperparameters and data preprocessing, algorithmic equivalence ensures similar model behavior regardless of hardware; GPU acceleration affects computation speed, not the mathematical operations themselves.

## Foundational Learning

- Concept: **GPU Architecture (CUDA Cores, SIMT, Memory Bandwidth)**
  - Why needed here: Understanding why KNN/RF speed up more than LR requires knowing that GPUs excel at parallel operations across many data points simultaneously
  - Quick check question: Why would KNN benefit more from GPU parallelization than Logistic Regression?

- Concept: **Algorithm Complexity and Parallelizability**
  - Why needed here: Interpreting the 159x vs 6.7x speedup difference requires distinguishing embarrassingly parallel algorithms (KNN) from sequentially constrained ones (gradient-based optimization in LR)
  - Quick check question: Which operation in Random Forest can be parallelized: tree construction, feature splitting within a node, or both?

- Concept: **CAN Bus and IoV Attack Taxonomy**
  - Why needed here: The datasets contain attack types (DoS, Fuzzy, Spoofing, Impersonation) specific to vehicular networks; understanding these informs feature engineering and model selection
  - Quick check question: Why might timestamp and ID columns be excluded from training data for CAN bus intrusion detection?

## Architecture Onboarding

- Component map:
  Raw CAN Data → Preprocessing (hex→decimal, NaN handling, label encoding) → Feature Scaling (StandardScaler) → Train/Test Split (80/20 stratified) → Model Training (cuML GPU or scikit-learn CPU) → Evaluation (Accuracy, F1, Time metrics)

- Critical path:
  1. Ensure CUDA compatibility (cuML version matches CUDA driver)
  2. Validate data preprocessing produces identical inputs for both libraries
  3. Benchmark single model first (KNN on CICIoV2024 shows clearest speedup signal)
  4. Compare hyperparameter parity before interpreting accuracy differences

- Design tradeoffs:
  - **cuML**: Faster training/prediction; fewer tuning parameters (no class_weight); requires NVIDIA GPU + CUDA setup
  - **scikit-learn**: Broader parameter support; CPU-only; slower on large datasets
  - **Hybrid approach (recommended)**: Use cuML for hyperparameter search/feature selection speed; train final model on scikit-learn for deployment flexibility

- Failure signatures:
  - CUDA out-of-memory errors when dataset exceeds GPU VRAM (16GB in study)
  - Accuracy drops >5% between libraries suggest hyperparameter mismatch, not hardware
  - Environment incompatibility (cuML requires specific CUDA/cuDNN versions)
  - Missing class_weight causing poor performance on imbalanced datasets (CICIoV2024: 86.89% benign)

- First 3 experiments:
  1. Replicate KNN speedup on smallest dataset (CICIoV2024, ~1.4M samples) to validate environment setup—expect 50x+ training speedup
  2. Test RFC with balanced vs imbalanced class handling to quantify cuML's class_weight limitation impact on GIDS dataset
  3. Measure prediction latency under simulated real-time constraints (single-sample inference) to assess deployment viability; GPU overhead may reduce speedup for small batch sizes

## Open Questions the Paper Calls Out

- **Open Question 1**: How can hybrid CPU-GPU architectures be optimized to dynamically allocate tasks based on real-time IoV workload characteristics?
  - Basis in paper: "Future research should focus on developing hybrid CPU-GPU architectures"
  - Why unresolved: The paper recommends a hybrid approach but provides no empirical validation or framework for determining optimal task partitioning between GPU (training/hyperparameter tuning) and CPU (deployment)
  - What evidence would resolve it: A comparative study implementing hybrid architectures with dynamic task scheduling, measuring end-to-end latency, resource utilization, and detection accuracy across varying IoV traffic loads

- **Open Question 2**: Can cuML's missing class_weight parameter be effectively compensated through preprocessing techniques without sacrificing the speed advantages demonstrated (up to 159x training speedup)?
  - Basis in paper: "The library currently lacks specific parameters (e.g., class_weight crucial for imbalanced data)"
  - Why unresolved: IoV datasets like CICIoV2024 show severe class imbalance (86.89% benign vs 0.70% gas attacks), yet the paper does not test whether alternative approaches (oversampling, cost-sensitive learning) maintain cuML's speedup while achieving comparable accuracy to scikit-learn's weighted implementations
  - What evidence would resolve it: Experiments applying SMOTE, class-balanced sampling, or ensemble methods to cuML implementations, comparing both computational efficiency and F1-scores on minority attack classes against weighted scikit-learn baselines

- **Open Question 3**: Do the GPU speedups generalize to resource-constrained vehicular edge computing units with limited VRAM and lower compute capability?
  - Basis in paper: The paper acknowledges "challenges remain in implementing these solutions in resource-constrained environments" but all experiments used a high-end RTX A4000 (16GB VRAM)
  - Why unresolved: Real IoV deployment targets embedded systems with minimal GPU resources; the 159x speedup on RTX A4000 may not transfer to edge-grade GPUs or may encounter memory constraints with large datasets like GIDS (14.4M samples)
  - What evidence would resolve it: Benchmarking cuML performance across a spectrum of GPU hardware (Jetson modules, automotive-grade SoCs) with memory profiling to identify VRAM thresholds where speedup degrades or fails

## Limitations
- Limited hyperparameter transparency restricts precise replication
- cuML's missing class_weight parameter for imbalanced datasets may underestimate real-world performance
- GPU memory constraints (16GB VRAM) limit scalability for larger IoV datasets

## Confidence
- **High Confidence**: Training and prediction time speedups (measured directly, consistent across datasets)
- **Medium Confidence**: Accuracy preservation claims (near-identical results reported, but minor RF degradation observed)
- **Low Confidence**: Generalizability to other IoV attack scenarios (tested only on three specific datasets)

## Next Checks
1. Verify hyperparameter parity between cuML and scikit-learn implementations, particularly for RF and XGBoost
2. Test prediction latency on single-sample inference to assess real-time deployment viability
3. Benchmark with additional imbalanced datasets to quantify impact of missing class_weight parameter