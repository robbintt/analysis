---
ver: rpa2
title: 'From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in
  In-Context Learning on Social Media Health Discourse'
arxiv_id: '2510.03636'
source_url: https://arxiv.org/abs/2510.03636
tags:
- sentiment
- support
- examples
- poisoning
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examined how data poisoning affects in-context learning\
  \ (ICL) for sentiment analysis of HMPV-related tweets. Small adversarial perturbations\
  \ in support examples\u2014such as synonym replacement, negation insertion, and\
  \ randomized modifications\u2014caused sentiment label flips in up to 67% of cases."
---

# From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse

## Quick Facts
- arXiv ID: 2510.03636
- Source URL: https://arxiv.org/abs/2510.03636
- Reference count: 12
- This study examined how data poisoning affects in-context learning (ICL) for sentiment analysis of HMPV-related tweets. Small adversarial perturbations in support examples—such as synonym replacement, negation insertion, and randomized modifications—caused sentiment label flips in up to 67% of cases. To counter this, a spectral signature defense was applied, filtering poisoned examples while preserving semantic integrity. After defense, ICL accuracy remained stable at approximately 46.7%, and logistic regression on cleaned embeddings achieved 100% sentiment accuracy, indicating effective mitigation of poisoning effects. The results highlight ICL's fragility under attack and demonstrate that spectral defense can help maintain dataset reliability in high-stakes public health monitoring contexts.

## Executive Summary
This paper investigates data poisoning attacks on In-Context Learning (ICL) for sentiment analysis of Human Metapneumovirus (HMPV) related tweets, demonstrating that small adversarial perturbations in support examples can cause significant prediction instability. The authors implement three attack strategies—synonym replacement, negation insertion, and randomized modifications—and show they can flip sentiment labels in up to 67% of cases. To counter these attacks, they apply a spectral signature defense that filters poisoned examples while preserving semantic integrity, achieving 100% accuracy with logistic regression on cleaned embeddings. The study reveals ICL's vulnerability to poisoning attacks and demonstrates that spectral defense can help maintain dataset reliability in high-stakes public health monitoring contexts.

## Method Summary
The study analyzes HMPV-related tweets, manually annotated into Positive/Negative/Neutral classes, using 5-shot In-Context Learning with the Zephyr-7B-β model. Three poisoning attack strategies are implemented: synonym replacement (p=0.3 using WordNet), negation insertion, and randomized modifications applied to support examples only. A spectral signature defense is then applied, using all-MiniLM-L6-v2 embeddings, z-score normalization, and Truncated SVD to flag the top 2% outliers along the dominant variance direction. The defense's effectiveness is validated by measuring ICL accuracy before and after filtering, and by training a logistic regression classifier on cleaned embeddings to achieve 100% accuracy.

## Key Results
- Small adversarial perturbations in ICL support examples caused sentiment label flips in up to 67% of cases
- Spectral signature defense filtered poisoned examples while preserving semantic integrity
- After defense, ICL accuracy remained stable at approximately 46.7%
- Logistic regression on cleaned embeddings achieved 100% sentiment accuracy

## Why This Works (Mechanism)

### Mechanism 1: ICL Sensitivity to Support Set Perturbations
ICL relies on pattern induction from few-shot examples in the prompt context. When support examples are perturbed via synonym replacement, negation insertion, or randomized modifications, the model's attention over context shifts, altering the inferred pattern and causing label flips. The perturbations do not modify target inputs—only the contextual examples the model uses for analogical reasoning.

### Mechanism 2: Spectral Signature Detection via SVD Projection
Poisoned examples can be identified as outliers along dominant variance directions in embedding space. After z-score normalization and truncated SVD, projections along the top singular vector reveal outliers; the top 2% by magnitude are flagged as suspicious.

### Mechanism 3: Semantic Integrity Preservation Post-Filtering
By flagging only extreme outliers (~1.74% of 50,285 samples), the defense preserves the majority of clean data. Post-defense, sentiment polarity remains stable (~0.05), KMeans clustering shows consistent topic distributions, and a logistic regression classifier on cleaned embeddings achieves 100% accuracy—indicating the semantic signal remains intact.

## Foundational Learning

- **In-Context Learning (ICL)**: Why needed here: ICL is the attack surface; understanding how LLMs generalize from prompt-embedded examples without gradient updates is essential to grasping why perturbations propagate. Quick check question: How does ICL differ from fine-tuning in terms of where the "learning" signal is stored?
- **Data Poisoning Attack Vectors in NLP**: Why needed here: The study uses specific perturbation types (synonym replacement, negation insertion, randomized); understanding these clarifies why small changes have outsized effects. Quick check question: Why might replacing "cases" with "instances" or inserting "is not" flip a sentiment prediction?
- **Spectral Methods for Anomaly Detection**: Why needed here: The defense relies on SVD to expose poisoned samples as variance outliers. Quick check question: What does the top singular vector represent in a matrix of embeddings, and why would poisoned samples have high projections along it?

## Architecture Onboarding

- **Component map**: Dataset pipeline (HMPV tweet collection → preprocessing → manual sentiment annotation) -> ICL prompt builder (5-shot support examples + target tweet → structured prompt) -> Model inference (Zephyr-7B-β generates sentiment predictions via next-token completion) -> Perturbation module (applies synonym replacement, negation insertion, or random selection to support examples only) -> Defense pipeline (SentenceTransformer embeddings → z-score normalization → truncated SVD → outlier scoring → filtering) -> Evaluation (Accuracy, label flip rate, logistic regression on cleaned embeddings, t-SNE visualization)

- **Critical path**: 1. Establish clean ICL baseline accuracy 2. Inject perturbations into support set → measure flip rate (67% observed) 3. Apply spectral defense → filter flagged samples 4. Re-evaluate ICL accuracy (~46.7%) and validate via logistic regression (100% on embeddings)

- **Design tradeoffs**: Filtering threshold (Flagging only 2% is conservative—reduces false positives but may miss subtle or distributed poisoning), ICL vs. embedding classifier (Post-defense, ICL remains fragile (46.7%) while logistic regression on cleaned embeddings achieves 100%; suggests ICL has residual sensitivity not captured by spectral detection), Detection vs. poisoning ratio (Detection rate drops as poisoning ratio increases (6.95% at 25% poisoning → 1.74% at 100%), indicating fixed-threshold spectral defense scales poorly)

- **Failure signatures**: Label flip rate >50% indicates successful poisoning attack, Post-defense ICL accuracy not recovering suggests residual context contamination, Detection rate declining as poisoning ratio increases indicates threshold misalignment, Divergence between ICL accuracy and embedding classifier accuracy signals ICL-specific fragility

- **First 3 experiments**: 1. Baseline characterization: Run ICL on clean support set to establish reference accuracy and per-class performance 2. Ablation by perturbation type: Isolate synonym replacement vs. negation insertion vs. random to quantify which attack vector causes the highest flip rates 3. Defense threshold sweep: Vary the outlier flagging threshold (e.g., 1%, 2%, 5%) and measure tradeoff between detection rate and post-defense ICL accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Can hybrid defenses combining spectral filtering with adversarial training better detect subtle poisoning patterns? Basis in paper: Future Work suggests this to address the "limited adaptiveness" of the standalone spectral method. Why unresolved: The defense flagged a fixed number of samples regardless of the poisoning ratio, potentially missing distributed attacks. What evidence would resolve it: Comparative experiments showing improved detection rates on the HMPV dataset.

### Open Question 2
Can robust prompting strategies like chain-of-thought reasoning improve post-defense In-Context Learning (ICL) accuracy? Basis in paper: Authors note ICL accuracy plateaued at ~46.7% and suggest exploring robust prompting in Future Work. Why unresolved: A performance gap exists: logistic regression achieved 100% accuracy on cleaned embeddings, but ICL remained low. What evidence would resolve it: ICL accuracy significantly exceeding 46.7% on the cleaned dataset using advanced prompting.

### Open Question 3
Can dynamic spectral thresholds improve detection across varying poisoning intensities? Basis in paper: Future Work calls for "adaptive thresholds" to improve upon the conservative filtering observed. Why unresolved: The study used a fixed threshold that flagged a constant number of examples across all conditions. What evidence would resolve it: A mechanism that scales flagged samples proportionally to the actual corruption level.

## Limitations
- Spectral signature defense's effectiveness is primarily demonstrated on a single NLP task (HMPV tweet sentiment analysis) and may not generalize to other domains or attack types
- The 100% logistic regression accuracy on cleaned embeddings was achieved on a single train-test split without cross-validation or confidence intervals
- The defense's fixed 2% outlier threshold appears to degrade as poisoning ratio increases, suggesting potential scalability issues
- The study does not evaluate whether more sophisticated poisoning attacks (e.g., optimized to evade spectral detection) could bypass the defense

## Confidence
- **High Confidence**: ICL's fundamental sensitivity to support set perturbations (67% flip rate observed), basic spectral defense methodology (SVD-based outlier detection), and the general framework of poisoning ICL support examples
- **Medium Confidence**: The specific 2% outlier threshold selection, the claim that semantic integrity is preserved post-filtering, and the logistic regression classifier's 100% accuracy as validation of defense effectiveness
- **Low Confidence**: Generalization of results to other NLP tasks, defense effectiveness against adaptive poisoning attacks, and long-term stability of ICL accuracy at 46.7%

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary the outlier flagging threshold (1%, 2%, 5%, 10%) and measure the tradeoff between detection rate and post-defense ICL accuracy across different poisoning ratios to identify optimal thresholds
2. **Cross-Domain Generalization**: Apply the same poisoning and defense pipeline to a different NLP task (e.g., movie review sentiment or news headline classification) to test whether the 67% flip rate and 46.7% post-defense accuracy are task-specific or represent general ICL vulnerabilities
3. **Adaptive Attack Evaluation**: Implement poisoning attacks specifically designed to evade spectral detection (e.g., constrained perturbations that minimize variance in embedding space) and measure whether the current defense maintains effectiveness against these more sophisticated attacks