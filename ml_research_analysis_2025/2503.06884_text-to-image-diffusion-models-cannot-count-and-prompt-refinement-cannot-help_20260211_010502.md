---
ver: rpa2
title: Text-to-Image Diffusion Models Cannot Count, and Prompt Refinement Cannot Help
arxiv_id: '2503.06884'
source_url: https://arxiv.org/abs/2503.06884
tags:
- counting
- uni00000013
- uni00000011
- figure
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces T2ICountBench, a comprehensive benchmark
  designed to rigorously evaluate the counting ability of state-of-the-art text-to-image
  diffusion models. The benchmark covers 15 models, including both open-source and
  private systems, and uses human evaluation to ensure high reliability.
---

# Text-to-Image Diffusion Models Cannot Count, and Prompt Refinement Cannot Help

## Quick Facts
- arXiv ID: 2503.06884
- Source URL: https://arxiv.org/abs/2503.06884
- Reference count: 40
- Primary result: Text-to-image diffusion models consistently fail at counting tasks, with accuracy dropping below 10% for 11-15 objects across all tested models

## Executive Summary
This paper presents a comprehensive evaluation of counting abilities in text-to-image diffusion models through the newly introduced T2ICountBench benchmark. The study systematically tests 15 state-of-the-art models, including both open-source and private systems, using human evaluation to ensure reliable assessment. Results demonstrate that all models struggle significantly with counting tasks, particularly as the number of objects increases, with accuracy falling below 10% for 11-15 objects. An exploratory study on prompt refinement techniques (multiplicative decomposition, additive decomposition, grid prior, and position guidance) found that these approaches generally degrade rather than improve counting performance, suggesting fundamental limitations in numerical understanding within diffusion models.

## Method Summary
The authors created T2ICountBench, a comprehensive benchmark for evaluating counting abilities in text-to-image diffusion models. The benchmark includes 15 models ranging from open-source systems like Stable Diffusion to private models like GPT-4V and Midjourney. Human evaluation was employed to ensure high reliability in assessing the generated images against counting prompts. The study tested various prompt refinement strategies to determine if they could improve counting accuracy, including multiplicative decomposition, additive decomposition, grid prior, and position guidance techniques. Results were analyzed across different object quantities to understand how counting accuracy degrades with increasing complexity.

## Key Results
- All tested diffusion models failed to accurately count objects, with accuracy below 10% for prompts requiring 11-15 objects
- Counting accuracy decreases sharply as the number of objects increases across all models
- Prompt refinement techniques generally degraded performance rather than improving counting accuracy
- Human evaluation ensured high reliability in the benchmark results

## Why This Works (Mechanism)
The fundamental limitation appears to stem from how diffusion models process numerical information. Rather than understanding numbers as discrete quantities, these models likely learn statistical patterns between textual descriptions and visual features without developing true numerical comprehension. The generation process relies on gradual denoising guided by text embeddings, which may not capture the precise quantitative relationships required for counting. When prompts specify multiple objects, the model may generate a plausible scene with a reasonable number of objects based on learned correlations, but cannot reliably produce the exact requested quantity.

## Foundational Learning
- **Text-to-image diffusion fundamentals**: Understanding how diffusion models progressively denoise random noise into coherent images based on text prompts is crucial for grasping why counting is challenging in this framework
- **Why needed**: Provides context for understanding the generation process and its limitations
- **Quick check**: Review how text embeddings guide the denoising process in diffusion models

- **Prompt engineering techniques**: Familiarity with methods like multiplicative decomposition, additive decomposition, grid prior, and position guidance helps understand the prompt refinement approaches tested
- **Why needed**: These techniques represent common strategies for improving model outputs through better prompt formulation
- **Quick check**: Examine how each prompt refinement method attempts to improve counting accuracy

- **Human evaluation in AI benchmarks**: Understanding the role and methodology of human evaluation in assessing model outputs is important for interpreting the benchmark results
- **Why needed**: Human evaluation provides more reliable assessment than automated metrics for subjective tasks like counting
- **Quick check**: Review how human evaluators were instructed to assess counting accuracy in generated images

## Architecture Onboarding

**Component map**: Text prompt → Text encoder → Diffusion denoising U-Net → Image output

**Critical path**: The generation process flows from text encoding through iterative denoising steps, with each step attempting to reduce noise while maintaining coherence with the prompt. The counting challenge emerges because numerical information must be preserved through all these transformations without explicit numerical reasoning capabilities.

**Design tradeoffs**: Diffusion models prioritize visual coherence and prompt alignment over precise numerical accuracy. The architecture is optimized for generating realistic images that match textual descriptions in a general sense, but lacks mechanisms for exact counting. This tradeoff enables impressive creative generation but fails on tasks requiring discrete numerical reasoning.

**Failure signatures**: Models generate plausible scenes with reasonable numbers of objects but cannot reliably produce exact counts. Accuracy degrades exponentially with increasing object numbers. Prompt refinements that work for other tasks often worsen counting performance, indicating the problem is fundamental rather than solvable through better prompting.

**First experiments**:
1. Test counting accuracy with different object types (animals, vehicles, household items) to determine if the failure is object-category dependent
2. Evaluate whether spatial constraints in prompts (specifying exact positions) improve counting accuracy
3. Compare counting performance across different diffusion model architectures to identify if certain designs perform better

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark focuses exclusively on counting discrete objects, potentially overlooking other numerical reasoning capabilities
- Tested prompt refinement techniques represent a limited subset of possible approaches
- Performance differences between open-source and private models were not thoroughly explored
- The study does not investigate whether these counting failures extend to other generative modalities beyond text-to-image

## Confidence
- High confidence: The finding that all models struggle with counting regardless of architecture or training data, due to comprehensive evaluation across 15 diverse models
- Medium confidence: The claim that prompt refinement cannot help counting, as tested techniques showed consistent degradation rather than improvement, but the space of possible refinements remains vast
- Medium confidence: The observation that accuracy drops sharply with increasing object counts, though the exact shape of this degradation curve may vary with different object types or contexts

## Next Checks
1. Test whether incorporating spatial coordinate information directly into the diffusion model architecture improves counting accuracy beyond prompt-based refinements
2. Evaluate counting performance across different object categories (animate vs. inanimate, small vs. large) to determine if the failure is object-type dependent
3. Investigate whether fine-tuning diffusion models on counting-specific datasets can overcome the observed limitations without catastrophic forgetting of other generation capabilities