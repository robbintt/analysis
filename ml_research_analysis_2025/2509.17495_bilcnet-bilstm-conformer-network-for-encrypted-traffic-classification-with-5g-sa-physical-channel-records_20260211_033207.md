---
ver: rpa2
title: 'BiLCNet : BiLSTM-Conformer Network for Encrypted Traffic Classification with
  5G SA Physical Channel Records'
arxiv_id: '2509.17495'
source_url: https://arxiv.org/abs/2509.17495
tags:
- traffic
- classification
- channel
- learning
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses encrypted traffic classification in 5G SA
  networks, where traditional methods like DPI and port-based identification are ineffective.
  The authors propose BiLCNet, a hybrid BiLSTM-Conformer architecture that integrates
  sequential modeling from BiLSTM with spatial feature extraction from Conformer blocks.
---

# BiLCNet : BiLSTM-Conformer Network for Encrypted Traffic Classification with 5G SA Physical Channel Records

## Quick Facts
- **arXiv ID**: 2509.17495
- **Source URL**: https://arxiv.org/abs/2509.17495
- **Reference count**: 18
- **Primary result**: Hybrid BiLSTM-Conformer architecture achieves 93.9% accuracy on 5G SA encrypted traffic classification using physical channel data

## Executive Summary
This paper addresses the challenge of classifying encrypted traffic in 5G Standalone (SA) networks where traditional methods like DPI and port-based identification fail. The authors propose BiLCNet, a novel hybrid architecture that combines BiLSTM's sequential modeling with Conformer blocks' spatial feature extraction. By leveraging physical channel data from the wireless air interface, the model infers traffic types without requiring payload decryption. Evaluated on a controlled 5G SA dataset, BiLCNet demonstrates strong performance (93.9% accuracy) and robust generalization across diverse channel conditions in zero-shot transfer learning scenarios.

## Method Summary
BiLCNet is a hybrid architecture that processes 5G SA physical channel records through a custom preprocessing pipeline. The model takes 10 ms radio frames containing 10 subframes with over 60 physical channel features (MCS, HARQ feedback, PRB allocation, SNR) from PUCCH, PUSCH, PDSCH, and PDCCH channels. Key feature engineering creates three derived metrics: ERR (Error Rate Ratio), EFF_PDSCH (PDSCH Efficiency), and MVI (Modulation Variability Index). The architecture consists of a BiLSTM layer for bidirectional temporal modeling, followed by stacked Conformer blocks that combine depthwise convolution and multi-head self-attention, attention pooling across time, and a fully connected classifier. The model is trained with cross-entropy loss using AdamW optimizer with early stopping.

## Key Results
- Achieves 93.9% classification accuracy on the noise-limited 5G SA dataset
- Outperforms conventional ML models (LR, CART, RF) and DL models (LSTM, Transformer baselines)
- Demonstrates strong generalization with 83.4% average accuracy in zero-shot transfer learning across diverse channel conditions
- Shows optimal performance in the 66-80 dB transmission gain range

## Why This Works (Mechanism)

### Mechanism 1
Physical channel data from the 5G air interface enables traffic classification without requiring payload decryption. Upper-layer application behavior induces characteristic patterns in physical layer parameters (MCS, HARQ feedback, PRB allocation, SNR). By extracting features from PUCCH, PUSCH, PDSCH, and PDCCH channels within a 10 ms radio frame window, the model infers traffic type from how the network adapts its physical transmission—information that persists regardless of encryption at higher layers. This mapping holds across the 64–84 dB transmission gain range tested, though performance degrades at transmission gain extremes.

### Mechanism 2
The hybrid BiLSTM-Conformer architecture improves classification by jointly capturing temporal dependencies and spatial feature correlations within channel feature matrices. BiLSTM processes the 10-subframe sequence bidirectionally, encoding positional context into hidden states—effectively replacing positional embeddings. Conformer blocks then apply depthwise convolution (local patterns across features) followed by multi-head self-attention (global dependencies across subframes), with residual connections and layer normalization stabilizing training. The fixed 10-subframe window is sufficient to capture traffic signatures when temporal dependencies are strong.

### Mechanism 3
Custom feature engineering (ERR, EFF_PDSCH, MVI) enhances discriminability beyond raw physical channel features. Cumulative Error Rate captures transmission reliability patterns associated with traffic type. PDSCH Efficiency quantifies downlink resource utilization, distinguishing latency-sensitive services. Modulation Variability Index measures adaptive modulation fluctuations, reflecting dynamic channel adaptation specific to traffic demands. The 10 ms observation window provides sufficient time for stable statistical estimates of these derived metrics.

## Foundational Learning

- **5G Physical Channels and Frame Structure**: The preprocessing pipeline assumes understanding of 5G SA numerology (μ=1), subframe/slot structure, and channel roles (PUCCH/PUSCH/PDSCH/PDCCH) to construct valid feature matrices. *Quick check: Given a 10 ms radio frame with μ=1, how many subframes and slots are present, and which channels carry user data versus control information?*

- **Bidirectional LSTM Sequence Modeling**: The BiLSTM component replaces positional embeddings; understanding how hidden states encode forward/backward context is essential for debugging temporal feature extraction. *Quick check: How does a BiLSTM differ from a unidirectional LSTM in terms of the information available at each timestep?*

- **Conformer Block (Convolution-Augmented Transformer)**: The architecture relies on Conformer blocks to fuse local (convolutional) and global (attention) feature extraction. Understanding residual connections, layer normalization, and multi-head self-attention is required to modify or debug this component. *Quick check: In a Conformer block, what is the order of operations, and why does convolution precede self-attention in the standard formulation?*

## Architecture Onboarding

- **Component map**: Channel Feature Matrix (T=10 subframes × D=60+ features) → BiLSTM Layer → Stacked Conformer Blocks → Attention Pooling → Fully Connected Classifier

- **Critical path**: Verify preprocessing produces valid feature matrices (check for missing channels, correct zero-padding) → Confirm BiLSTM output dimensions match Conformer input expectations → Monitor attention pooling weights to ensure the model attends to informative subframes

- **Design tradeoffs**: Accuracy vs. real-time (paper prioritizes offline accuracy over inference speed) → Fixed window vs. flexibility (10-subframe window may not capture long-duration traffic patterns) → Feature engineering vs. end-to-end learning (custom features improve interpretability but require domain expertise)

- **Failure signatures**: Low accuracy on "Call" and "Meeting" classes (may indicate class imbalance or feature similarity) → Zero-shot performance drop at transmission gain extremes (suggests insufficient coverage of edge channel conditions) → Training instability (if BiLCNet shows similar instability to Transformer baseline, check learning rate, batch normalization, or gradient clipping)

- **First 3 experiments**: 1) Baseline sanity check: Train a simple LSTM-only model on the same preprocessed data to quantify Conformer contribution 2) Ablation on derived features: Train BiLCNet with raw features only (excluding ERR, EFF_PDSCH, MVI) to measure feature engineering impact 3) Transfer learning probe: Train on a subset of transmission gain values (e.g., 70–80 dB only) and test on held-out gains to map generalization boundaries systematically

## Open Questions the Paper Calls Out
- Can BiLCNet maintain its classification performance when deployed in real-world 5G SA environments outside controlled laboratory settings?
- How does BiLCNet scale to multi-device and multi-user scenarios with concurrent traffic flows?
- What architectural or training modifications would improve BiLCNet's robustness at extreme transmission gain values (below 66 dB and above 80 dB)?
- Can BiLCNet achieve real-time classification latency suitable for inline network management without sacrificing accuracy?

## Limitations
- Dataset not publicly released and no synthetic data generator provided
- Results derived from controlled laboratory environment in Faraday cage, not validated in real-world deployments
- Claims of real-world robustness and cross-network generalization not experimentally verified
- No latency measurements or computational complexity analysis provided

## Confidence
- **High**: Physical-layer feature utility (93.9% accuracy on held-out data); BiLSTM-Conformer design rationale; documented failure modes for class imbalance and channel extremes
- **Medium**: Zero-shot transfer-learning gains (relative to baselines); custom feature-engineering contributions (ERR, EFF_PDSCH, MVI)
- **Low**: Claims of real-world robustness and cross-network generalization; exact reproducibility of reported numbers

## Next Checks
1. **Data simulation and preprocessing audit**: Generate a synthetic 5G SA channel dataset matching the stated feature set and windowing scheme. Verify that the engineered features (ERR, EFF_PDSCH, MVI) are stable and discriminative across the 64–84 dB gain range.

2. **Ablation study with documented hyperparameters**: Implement BiLCNet with fully specified architecture and optimizer settings. Run controlled ablations (BiLSTM-only, raw features only, Conformer-only) to isolate each mechanism's contribution, ensuring fair comparison by fixing training duration and data splits.

3. **Zero-shot transfer under synthetic shifts**: Hold out entire gain ranges and simulate realistic domain shifts (e.g., different traffic mixes, SNR distributions). Measure classification accuracy and confusion matrices to identify failure modes and guide model adaptation or data augmentation strategies.