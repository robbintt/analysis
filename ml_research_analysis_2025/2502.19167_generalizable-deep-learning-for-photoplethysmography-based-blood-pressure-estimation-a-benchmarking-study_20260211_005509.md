---
ver: rpa2
title: Generalizable deep learning for photoplethysmography-based blood pressure estimation
  -- A Benchmarking Study
arxiv_id: '2502.19167'
source_url: https://arxiv.org/abs/2502.19167
tags:
- vital
- aami
- calibfree
- training
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks deep learning models for photoplethysmography
  (PPG)-based blood pressure (BP) estimation, focusing on out-of-distribution (OOD)
  generalization. The authors trained five models on the PulseDB dataset and evaluated
  their performance both in-distribution and on external datasets.
---

# Generalizable deep learning for photoplethysmography-based blood pressure estimation -- A Benchmarking Study

## Quick Facts
- arXiv ID: 2502.19167
- Source URL: https://arxiv.org/abs/2502.19167
- Reference count: 40
- Best model (XResNet1d101) achieved MAEs of 9.4/6.0 mmHg (systolic/diastolic) on PulseDB with subject-specific calibration, and 14.0/8.5 mmHg without.

## Executive Summary
This paper benchmarks deep learning models for photoplethysmography (PPG)-based blood pressure (BP) estimation, focusing on out-of-distribution (OOD) generalization. The authors trained five models on the PulseDB dataset and evaluated their performance both in-distribution and on external datasets. The best model (XResNet1d101) achieved MAEs of 9.4/6.0 mmHg (systolic/diastolic) on PulseDB with subject-specific calibration, and 14.0/8.5 mmHg without. On external datasets, MAEs ranged from 15.0-25.1 mmHg (SBP) and 7.0-10.4 mmHg (DBP) without calibration. The study found that performance is strongly influenced by differences in BP distributions between datasets. A simple domain adaptation approach using sample-based importance weighting was explored, showing modest improvements in OOD performance, particularly for AAMI datasets. The results emphasize the importance of evaluating models on external datasets and developing domain adaptation techniques for robust BP estimation from PPG signals.

## Method Summary
The study benchmarks five deep learning models (XResNet1d101, LeNet1D, Inception1D, S4) for BP estimation from 125 Hz PPG signals using the PulseDB dataset (combining MIMIC-III and VitalDB). Models were trained with AdamW optimizer (lr=0.001, batch=512) for 50 epochs using MSE loss. The task is framed as a multi-task regression with two output nodes for SBP and DBP. Performance was evaluated both in-distribution on PulseDB test sets and out-of-distribution on four external datasets (Sensors, UCI, BCG, PPGBP) used in prior work [3]. A domain adaptation approach using sample-based importance weighting was explored to improve OOD generalization by reweighting training samples based on target label distribution differences.

## Key Results
- XResNet1d101 achieved the best performance with 9.4/6.0 mmHg MAE (SBP/DBP) on PulseDB with subject-specific calibration and 14.0/8.5 mmHg without calibration.
- On external datasets without calibration, MAEs ranged from 15.0-25.1 mmHg (SBP) and 7.0-10.4 mmHg (DBP), showing significant performance degradation.
- VitalDB-based models showed better OOD generalization than MIMIC-based models, suggesting the importance of training data source characteristics.
- Simple domain adaptation using importance weighting improved OOD performance modestly, with mean improvements of 3.39 mmHg for SBP and 1.32 mmHg for DBP on external datasets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deep learning models (specifically CNNs) extract hierarchical features from raw PPG waveforms that correlate with blood pressure, but this mapping is highly sensitive to the training distribution.
- **Mechanism:** Convolutional layers (ResNet/Inception) act as feature extractors, identifying morphological patterns in the PPG signal (e.g., pulse width, amplitude) that statistically correlate with SBP/DBP in the training set. However, if the test set has a different distribution of these patterns or BP labels, the mapping fails.
- **Core assumption:** The relationship between the input PPG morphology and output BP is consistent within the training distribution but varies across domains (covariate shift).
- **Evidence anchors:**
  - [abstract] "deep learning models... to infer BP from the raw PPG waveform... performance is strongly influenced by the differences in BP distributions."
  - [section III.B] Describes the use of ResNet and Inception architectures for "hierarchical feature extraction."
  - [corpus] "ArterialNet: Reconstructing Arterial Blood Pressure Waveform..." validates the use of deep learning for hemodynamic monitoring but notes sensitivity to individual variability.
- **Break condition:** If the PPG signal contains artifacts or physiological features not represented in the training set, the feature map becomes invalid, leading to high MAE (e.g., >25 mmHg).

### Mechanism 2
- **Claim:** Importance weighting based on label distributions can partially correct for domain shift by re-prioritizing training samples that resemble the target domain.
- **Mechanism:** By calculating the ratio of target-to-source probabilities in label space (SBP/DBP histograms), the model up-weights training samples that fall into under-represented "tails" of the target distribution. This minimizes the predictive error on the test set by aligning the empirical risk with the target distribution.
- **Core assumption:** The primary driver of performance degradation is the mismatch in label distributions (BP ranges) rather than just signal morphology differences.
- **Evidence anchors:**
  - [abstract] "simple domain adaptation approach using sample-based importance weighting... showing modest improvements."
  - [section III.D] "we utilize the sample weights wi for the loss calculation... derived from the SBP distribution."
  - [section IV.D] "The mean improvement... is given by 3.39 mmHg for SBP... substantially larger than... within PulseDB."
- **Break condition:** If the domain shift is caused by sensor noise or physiological differences not captured by the label distribution (histograms), this weighting will fail to improve generalization.

### Mechanism 3
- **Claim:** Training on the VitalDB subset (surgical patients) yields better out-of-distribution generalization than the MIMIC subset (ICU patients), potentially due to differences in data quality or patient population variability.
- **Mechanism:** The VitalDB dataset may contain signal characteristics or a distribution of physiological states that act as a better "universal" basis for PPG-BP mapping than MIMIC, which might overfit to the specific hemodynamics of critical care patients.
- **Core assumption:** Generalizability is determined by the specific characteristics of the source data domain, not just dataset size.
- **Evidence anchors:**
  - [section IV.C] "Vital subset... seems to be an important component for good generalization... MIMIC-based models show a poor OOD generalization performance."
  - [section VI] "MIMIC-based models show poor OOD generalization. This puts into question the use of MIMIC as the predominant training dataset."
  - [corpus] Corpus evidence is limited regarding the specific comparison of MIMIC vs. VitalDB for generalization; this finding appears specific to this study's experimental setup.
- **Break condition:** If the target deployment environment is specifically an ICU (similar to MIMIC), the VitalDB model's advantage might diminish or reverse.

## Foundational Learning

- **Concept:** **Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** The paper demonstrates that In-Distribution (ID) performance is an "overly optimistic" proxy for real-world utility. An engineer must understand that low validation loss on held-out training data (ID) does not guarantee clinical viability on new hardware or patient groups.
  - **Quick check question:** If I achieve 6 mmHg MAE on a held-out test set from the same hospital, can I expect the same performance on data collected from a wearable watch at home? (Answer: No, expect significant degradation).

- **Concept:** **Domain Adaptation (Sample Reweighting)**
  - **Why needed here:** The study uses a specific, lightweight technique (importance weighting) to mitigate distribution shift without complex model retraining. This is a practical strategy for deploying models when only summary statistics (label distributions) of the new target population are known.
  - **Quick check question:** How do you adjust a model trained on a dataset with mostly healthy BP values to perform well on a hypertensive population without collecting new raw training data? (Answer: Reweight the loss function based on the ratio of hypertensive vs. healthy label probabilities).

- **Concept:** **Earth Mover's Distance (EMD)**
  - **Why needed here:** The paper quantifies "dataset similarity" using EMD. This metric is crucial for predicting generalization gapsâ€”higher EMD between training and test sets correlates with higher error.
  - **Quick check question:** If I select a new external validation set, what metric can I compute *before* running inference to estimate the likely drop in model accuracy? (Answer: EMD of the SBP/DBP distributions).

## Architecture Onboarding

- **Component map:** Input (125 Hz PPG segments) -> Backbone (XResNet1d101 with global average pooling) -> Heads (2 output nodes for SBP/DBP) -> Loss (Weighted MSE) -> Optimizer (AdamW)

- **Critical path:**
  1. **Data Ingestion:** PulseDB (VitalDB + MIMIC) vs. External (Sensors, UCI, etc.).
  2. **Distribution Check:** Calculate EMD between source (Train) and target (Test) BP histograms.
  3. **Weighting (Optional):** Apply sample importance weights if label distributions diverge (specifically useful for AAMI protocols).
  4. **Inference:** Predict SBP/DBP.

- **Design tradeoffs:**
  - **Vital vs. MIMIC:** VitalDB training data provides better OOD robustness but may lack the extreme pathology depth of MIMIC.
  - **Calib vs. CalibFree:** "Calib" scenarios (subject-specific fine-tuning) yield much lower MAE (~9 mmHg) but require subject data; "CalibFree" is harder (~14 mmHg ID) but fully generalizable.
  - **S4 vs. CNN:** Structured State Space (S4) models underperformed compared to ResNets, suggesting long-range dependencies in raw PPG are less critical than local morphological features for this specific task.

- **Failure signatures:**
  - **High MAE (>20 mmHg) on External Datasets:** Indicates severe distribution shift (likely training on MIMIC only).
  - **DBP better than SBP:** Diastolic errors are typically lower, but relative performance (MASE) might be worse due to lower variance in ground truth DBP.
  - **Degradation on BCG/PPGBP:** Smaller datasets with different sensor physics cause specific failures.

- **First 3 experiments:**
  1. **Baseline ID Evaluation:** Train XResNet1d101 on "Combined Calib" and test on "Combined Test" to establish the optimistic upper bound (target: ~9-14 mmHg MAE).
  2. **OOD Stress Test:** Train on "CalibFree Vital" and test on "Sensors" and "UCI" datasets to measure the generalization gap without calibration.
  3. **Domain Adaptation Ablation:** Retrain the model from Experiment 2 using importance weighting derived from the target dataset's label distribution to quantify the reduction in MAE (expected: ~1-4 mmHg improvement).

## Open Questions the Paper Calls Out

- Can reweighting using a predefined, fixed label distribution (such as a flat distribution or one inferred from a population cohort) improve model robustness when the target distribution is unknown?
- To what extent do large pretrained foundation models (e.g., Papagei, Siamquality) improve OOD generalization for PPG-based BP estimation compared to models trained from scratch?
- Can clinical metadata be utilized to predict whether a specific unseen sample will fall into the high-accuracy group (IEEE grades A-B) or the low-accuracy group (Grade D)?

## Limitations
- The superiority of VitalDB over MIMIC for OOD generalization is observed but lacks a mechanistic explanation.
- Domain adaptation via importance weighting shows consistent but modest improvements (~1-4 mmHg).
- The technique is sensitive to accurate estimation of target label distributions, which may not be available in practice.

## Confidence
- **High:** The benchmarking methodology (MAE/MASE metrics, multiple external datasets) is robust and the relative performance ranking of models (XResNet1d101 > others) is likely reliable.
- **Medium:** The domain adaptation technique's efficacy is demonstrated but the gains are modest and may not generalize to all domain shifts (e.g., sensor noise vs. BP distribution).
- **Low:** The specific reason why VitalDB-based models generalize better than MIMIC-based models is not fully explained and requires further investigation.

## Next Checks
1. **Ablation on Training Data Source:** Retrain the best model (XResNet1d101) on MIMIC-only, VitalDB-only, and their combinations to isolate the source of the generalization gap.
2. **Domain Adaptation on Sensor Noise:** Apply the importance weighting technique to a controlled experiment where only sensor noise (not BP labels) differs between training and test sets to test its limits.
3. **Dataset Similarity Prediction:** Use the reported EMD values to predict the performance on a new, held-out external dataset before running inference, validating the utility of EMD as a generalization gap estimator.