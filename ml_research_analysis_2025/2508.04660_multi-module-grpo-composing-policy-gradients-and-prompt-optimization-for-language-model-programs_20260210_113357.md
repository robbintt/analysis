---
ver: rpa2
title: 'Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for
  Language Model Programs'
arxiv_id: '2508.04660'
source_url: https://arxiv.org/abs/2508.04660
tags:
- grpo
- mmgrpo
- program
- module
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying Group Relative Policy
  Optimization (GRPO) to multi-module language model (LM) programs, which involve
  multiple LM calls with distinct prompt templates. The authors introduce mmGRPO,
  a multi-module generalization of GRPO that groups LM calls by module across rollouts
  and handles variable-length and interrupted trajectories.
---

# Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs

## Quick Facts
- arXiv ID: 2508.04660
- Source URL: https://arxiv.org/abs/2508.04660
- Reference count: 40
- Primary result: mmGRPO improves accuracy by 11% on average vs. post-trained LM, and 5% vs. prompt optimization alone

## Executive Summary
This paper addresses the challenge of applying Group Relative Policy Optimization (GRPO) to multi-module language model (LM) programs, which involve multiple LM calls with distinct prompt templates. The authors introduce mmGRPO, a multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories. By aligning structurally comparable module calls, mmGRPO enables GRPO-style policy gradient updates without requiring shared histories or module-level inputs across rollouts. Experiments across classification, many-hop search, and privacy-preserving delegation tasks show that mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average compared to the post-trained LM and by 5% compared to prompt optimization alone. The authors open-source mmGRPO in DSPy as the dspy.GRPO optimizer.

## Method Summary
mmGRPO extends GRPO to multi-module LM programs by grouping LM calls at the module level across rollouts. The key innovation is aligning module calls based on module identity and relative invocation order (e.g., "second call to query generator"), then computing advantages within each module-level group using the final program reward. The method handles variable-length and interrupted trajectories through padding and diversity-preserving group construction. It optionally pads smaller groups to a fixed size or uses variance-based element selection to maintain uniform group sizes while maximizing reward diversity. The approach is composed with prompt optimization (MIPROv2) followed by weight optimization, yielding better performance than either alone. The implementation uses DSPy's dspy.GRPO optimizer with LoRA adapters and trains for 750 steps with 4 examples per step and 12 rollouts per example.

## Key Results
- mmGRPO improves accuracy by 11% on average compared to the post-trained LM
- mmGRPO achieves 5% better accuracy than prompt optimization alone when composed with it
- Validated across three diverse tasks: Banking77 classification, HoVer4-HOP multi-hop retrieval, and PAPILLON privacy delegation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping at the module level enables GRPO-style policy gradient updates for multi-module LM programs without requiring shared inputs across rollouts.
- Mechanism: mmGRPO relaxes GRPO's requirement for shared prompts by aligning module calls based on module identity and relative invocation order (e.g., "second call to query generator"), then computing advantages within each module-level group using the final program reward.
- Core assumption: Credit assignment from final program reward to individual module calls is sufficient for learning without intermediate supervision.
- Evidence anchors:
  - [abstract] "mmGRPO enables GRPO-style policy gradient updates without requiring shared histories or module-level inputs across rollouts"
  - [Section 3] "MMGRPO aligns module calls across trajectories based on both the module identifier and the relative order in which it appears within the trajectory"
  - [corpus] Weak direct evidence; related GRPO variants focus on normalization and stability, not multi-module composition
- Break condition: If intermediate module outputs are critical for final success but this signal is washed out by uniform credit assignment, learning may be inefficient or unstable.

### Mechanism 2
- Claim: Variable-length and interrupted trajectories can be handled through padding and diversity-preserving group construction.
- Mechanism: When rollouts differ in structure (early termination, parsing failures), mmGRPO optionally pads smaller groups or uses variance-based element selection to maintain uniform group sizes while maximizing reward diversity within groups.
- Core assumption: Diversity in prompt-output pairs within groups improves generalization (assumption supported by contemporaneous work).
- Evidence anchors:
  - [Section 3] "MMGRPO optionally pads smaller groups to a fixed size"
  - [Appendix A.3] "SELECT K DIVERSE ELEMENTS... favoring selections that increase reward variance"
  - [corpus] Xu et al. (2025) propose similar variance-based selection, suggesting the mechanism has independent support
- Break condition: If padding strategies introduce noise or if early-failure trajectories provide misleading credit signals, performance may degrade.

### Mechanism 3
- Claim: Staged composition of prompt optimization followed by weight optimization yields better performance than either alone.
- Mechanism: Prompt optimization (MIPROv2) first improves the policy's prompt templates, generating higher-quality rollouts; mmGRPO then fine-tunes weights on these improved rollouts, benefiting from a stronger training signal early in training.
- Core assumption: High-quality initial rollouts lead to more robust gradient estimates during RL training.
- Evidence anchors:
  - [abstract] "mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average... and by 5% against prompt optimization on its own"
  - [Section 4] "performing PO generates stronger rollouts, leading to a more robust training signal early in the training runs"
  - [corpus] No direct BetterTogether evaluations in related work; composition strategy appears novel to this paper
- Break condition: If prompt optimization pushes the policy into a region where subsequent RL optimization is unstable (e.g., overfitting to prompt patterns), composition may underperform.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: mmGRPO extends GRPO's core mechanism—computing advantages from relative rewards within groups—to multi-module programs. Without understanding GRPO's baseline/normalization approach, the extension's design choices are opaque.
  - Quick check question: Given 4 rollouts with rewards [0.2, 0.6, 0.4, 0.8], what is the advantage for the second rollout after normalization?

- Concept: **Multi-module LM programs**
  - Why needed here: The paper's central challenge arises from programs where rollouts can differ in structure (variable module invocations, early termination). Understanding this modularity is essential for grasping why standard GRPO fails.
  - Quick check question: Why can't you simply concatenate all module outputs into a single sequence and apply standard GRPO?

- Concept: **Prompt optimization vs. weight optimization**
  - Why needed here: The BetterTogether strategy combines two fundamentally different approaches (discrete prompt search vs. continuous weight updates). Understanding their complementary nature is key to interpreting results.
  - Quick check question: If prompt optimization is much cheaper (1.4 GPU-hours vs. 18.7), when might you still prefer mmGRPO alone?

## Architecture Onboarding

- Component map:
  - Student program Φ (multi-module LM program with prompt templates πM and weights θM for each module M)
  -> Teacher programs T (optional list for off-policy sampling)
  -> FORM MODULE LEVEL GROUPS (core function constructing module-level GRPO groups)
  -> PADGROUPS (handles variable-length trajectories via truncate or fill strategies)
  -> SELECT K DIVERSE ELEMENTS (ensures groups have exactly G elements while maximizing diversity)
  -> HuggingFace GRPOTrainer (underlying training infrastructure with LoRA rank 16)

- Critical path:
  1. Sample batch from training set
  2. Generate rollouts from teacher programs (default: student only)
  3. Compute final reward for each trajectory
  4. Group module calls by (module_id, relative_invocation_order)
  5. Pad/truncate groups to uniform size G
  6. Apply GRPO loss independently to each module-level group
  7. Update weights via LoRA

- Design tradeoffs:
  - **Uniform vs. learned credit assignment**: Paper uses uniform (final reward assigned to all module calls); learned credit could be more precise but requires additional supervision
  - **Group size G**: Larger groups provide better advantage estimates but require more rollouts; paper uses G=12
  - **Padding mode**: "fill" preserves data but may introduce noise; "truncate" is cleaner but discards data from early-failure trajectories

- Failure signatures:
  - **Low variance in rewards within groups**: If all rollouts succeed or fail together, advantage estimates will be near-zero, yielding weak gradients
  - **High module invocation variance**: If some rollouts call a module 4 times and others 0 times, padding dominates and signal degrades
  - **Parsing errors in early modules**: Cascade failures produce incomplete trajectories with fallback rewards, potentially biasing learning

- First 3 experiments:
  1. **Single-module validation**: Run mmGRPO on Banking77 (single CoT module) and verify it reduces to standard GRPO behavior
  2. **Group size ablation**: Test G ∈ {4, 8, 12, 16} on HoVer4-HOP to measure sensitivity to group size
  3. **Padding mode comparison**: Compare "truncate" vs. "fill" on PAPILLON (where delegation failures are common) to assess robustness to trajectory variance

## Open Questions the Paper Calls Out
None

## Limitations
- The assumed sufficiency of final-program reward for module-level credit assignment without intermediate supervision may not scale to programs with deeper module nesting or more complex failure modes
- Padding and diversity mechanisms may introduce noise in programs with highly variable control flow
- The composition strategy with prompt optimization assumes both components are beneficial, but their interaction could be task-dependent

## Confidence

- **High Confidence**: The mmGRPO algorithm extension from GRPO to multi-module programs is well-defined and technically sound. The core mechanism of grouping by module identity and relative invocation order is clearly specified.
- **Medium Confidence**: The empirical improvements over baselines (11% vs. post-trained LM, 5% vs. prompt optimization) are well-documented, but the sample size of three tasks limits generalizability.
- **Medium Confidence**: The claim that staged composition of prompt optimization followed by weight optimization yields better performance than either alone is supported by experiments, though the interaction effects warrant further study.

## Next Checks

1. **Group size sensitivity**: Test mmGRPO with G ∈ {4, 8, 12, 16} on HoVer4-HOP to measure how group size affects performance and determine optimal settings for different program complexities.

2. **Padding strategy ablation**: Compare "truncate" vs. "fill" padding modes on PAPILLON to quantify the impact of trajectory variance handling on final performance, particularly in programs with high early-failure rates.

3. **Credit assignment analysis**: Implement an ablation where module calls receive rewards proportional to their contribution (e.g., based on intermediate success signals) rather than uniform final-program reward, to test whether more granular credit assignment improves learning efficiency.