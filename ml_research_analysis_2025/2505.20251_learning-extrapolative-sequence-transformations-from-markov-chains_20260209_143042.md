---
ver: rpa2
title: Learning Extrapolative Sequence Transformations from Markov Chains
arxiv_id: '2505.20251'
source_url: https://arxiv.org/abs/2505.20251
tags:
- training
- sequence
- learning
- mcmc
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of extrapolating beyond training
  data in sequence generation tasks such as protein engineering, sentiment control,
  and text anonymization. Traditional Monte Carlo methods like MCMC can explore the
  space but are computationally expensive and inefficient.
---

# Learning Extrapolative Sequence Transformations from Markov Chains

## Quick Facts
- **arXiv ID**: 2505.20251
- **Source URL**: https://arxiv.org/abs/2505.20251
- **Reference count**: 40
- **Primary result**: Proposed method achieves better or comparable extrapolation performance to MCMC with significantly fewer steps across protein engineering, sentiment control, and text anonymization tasks.

## Executive Summary
This paper addresses the challenge of extrapolating beyond training data in sequence generation tasks by learning an autoregressive model from Markov chains generated by MCMC. Traditional MCMC methods can explore sequence spaces but are computationally expensive. The proposed approach distills the search behavior of MCMC into a sample-efficient autoregressive model by training on selected state transitions from the chains. Experiments demonstrate the method outperforms or matches MCMC performance with significantly fewer steps: 74.8% success below -5 ddG in 3 steps versus MCMC's 27% in 83 steps for protein engineering, 73.4% in the extrapolation range in 1 step versus MCMC's 80.9% in 496 steps for sentiment control, and EER 0.221 and SBERT 0.839 in 4 steps versus MCMC's 0.393 and 0.835 in 4498 steps for anonymization.

## Method Summary
The method learns an autoregressive model from Markov chains generated by MCMC search. First, MCMC samples sequences using a Masked Language Model as the proposal distribution and an energy-based model (EBM) as the target density. The accepted states from these chains are stored as training data. Next, episodes are created by pruning chains using strategies like "First/Best" or "Fixed-length Δ Energy" selection. The autoregressive model $q_θ$ is then fine-tuned on these episodes, learning to map initial states to optimized states. During inference, the model generates sequences iteratively or in single steps, conditioned on the history of previous states and scores. The approach leverages the exploration of MCMC while achieving the efficiency of direct generation.

## Key Results
- Achieved 74.8% success rate below -5 ddG (extrapolation range) in 3 steps versus MCMC's 27% in 83 steps for protein engineering
- Reached 73.4% in the extrapolation range in 1 step versus MCMC's 80.9% in 496 steps for sentiment control
- Obtained EER 0.221 and SBERT 0.839 in 4 steps versus MCMC's 0.393 and 0.835 in 4498 steps for anonymization

## Why This Works (Mechanism)

### Mechanism 1: Search Distillation via Trajectory Pruning
If an autoregressive model is trained on high-value state transitions extracted from Markov chains, it may approximate the search efficiency of Monte Carlo methods with significantly lower inference cost. MCMC explores the sequence space stochastically, accepting or rejecting proposals based on an energy function. By recording the chains, pruning low-quality transitions, and training a model $q_θ$ on the remaining "successful" paths (e.g., transitions that strictly lower energy), the model learns a direct mapping from initial state to optimized state, bypassing the stochastic random walk. The core assumption is that the MCMC process successfully discovers trajectories that improve the target property, and these trajectories contain learnable patterns rather than random noise. If the MCMC chains fail to reach the extrapolation region (e.g., getting stuck in local modes), the training data will consist only of sub-optimal transitions, and the distilled model will fail to extrapolate.

### Mechanism 2: Energy-Guided Curriculum Learning
Conditioning the training episodes on relative improvements in energy (Δ energy) appears to improve the model's ability to navigate the state space compared to uniform sub-sampling. Rather than treating all MCMC steps equally, selecting transitions based on the magnitude of energy decrease prioritizes "high-gradient" steps. This creates a curriculum where the model learns to associate specific sequence edits with significant property improvements, effectively learning a greedy optimization policy. The core assumption is that the "energy" or score function $s(x)$ is a reliable proxy for the true target property (oracle) even in the extrapolation range. If the score function is misaligned with the true objective in the extrapolation range, optimizing for Δ energy will lead to "reward hacking" rather than valid extrapolation.

### Mechanism 3: History-Conditioned Refinement
Conditioning the generative process on the history of previous states and their scores allows the model to perform iterative, non-autoregressive refinement. The model $q_θ$ generates token sequences $x_t$ conditioned on $(x_0, ..., x_{t-1})$ and scores $(s_0, ..., s_{t-1})$. This provides feedback loops, allowing the model to correct previous errors or adjust the trajectory based on the cumulative "reward-to-go," similar to offline reinforcement learning. The core assumption is that the sequence of edits required for extrapolation is Markovian or sufficiently dependent on the immediate history to be captured by the context window. If the sequence requires complex planning exceeding the model's context length or capacity, the iterative refinement may oscillate or diverge.

## Foundational Learning

**Concept: Metropolis-Hastings Acceptance Criterion**
- **Why needed here**: This defines how the "teacher" MCMC process generates the training data. You must understand how the proposal $q(x'|x)$ interacts with the energy function $p(x)$ to accept or reject a candidate sequence.
- **Quick check question**: If the energy of a proposed sequence is lower (worse) than the current sequence, is it always rejected in MH? (Answer: No, accepted with probability $\exp(\Delta E)$).

**Concept: Masked Language Models (MLMs) as Proposal Distributions**
- **Why needed here**: The paper uses pre-trained MLMs (like BERT/T5) to generate candidate edits (proposals) for the MCMC sampler. Understanding the in-filling objective is crucial for implementing the sampler.
- **Quick check question**: Why is an MLM better suited for proposing local edits than a standard left-to-right autoregressive model?

**Concept: Amortized Inference / Distillation**
- **Why needed here**: The core contribution is "amortizing" the expensive MCMC search into a cheap autoregressive model.
- **Quick check question**: Does the student model ($q_θ$) learn the *distribution* of the MCMC chain or the *trajectory* of optimization?

## Architecture Onboarding

**Component map**: Oracle/Guide -> MCMC Sampler -> Buffer -> Episode Creator -> Student Model ($q_θ$)

**Critical path**: The **Episode Creation** strategy. As shown in Table 4, the choice of how to subsample the MCMC chains (e.g., Fixed-length Δ Energy vs. Thinning) causes >10% variance in extrapolation success.

**Design tradeoffs**:
- **Real vs. Predicted Scores**: The paper predicts scores during inference to avoid expensive oracle calls (Appendix A), but this introduces drift if the predictor is inaccurate.
- **Episode Length**: Longer episodes allow multi-step reasoning but increase inference latency and training complexity (Section 3.4/C).

**Failure signatures**:
- **Mode Collapse**: The model outputs the same high-reward sequence regardless of input (check diversity via BLEU/Uniques).
- **Stuck in Training Range**: The model interpolates well but fails to cross the boundary into the extrapolation zone (likely due to insufficient MCMC exploration during data generation).
- **Semantic Drift**: The score improves, but the sequence loses meaning (monitor Fluency/SBERT scores).

**First 3 experiments**:
1. **Sanity Check (Toy Task)**: Implement the binary sequence optimizer described in Section 2 to verify the distillation pipeline works in a controlled environment.
2. **Ablation on Episode Strategy**: Compare "Uniform Thinning" vs. "Δ Energy" on the Sentiment task to reproduce the sensitivity observed in Table 5.
3. **Efficiency Boundary**: Measure the minimum MCMC chain length required to train a successful $q_θ$ (Appendix D analysis) to establish data generation costs.

## Open Questions the Paper Calls Out

**Open Question 1**: Can further on-policy fine-tuning of the learned policy $q_θ$ (after initialization via MCMC) improve extrapolation performance?
- **Basis in paper**: [explicit] "Another interesting avenue for future work would be to perform further on-policy fine-tuning of our policy after initializing it using the proposed approach, which we expect could further improve performance."
- **Why unresolved**: The current method treats the MCMC-derived data as a fixed dataset for supervised fine-tuning. The authors hypothesize that incorporating reinforcement learning updates (on-policy tuning) could refine the policy, but this mechanism was not implemented or tested in the current study.
- **What evidence would resolve it**: An experiment where $q_θ$ generates samples, receives feedback from the oracle/guide, and updates its weights iteratively, compared against the static supervised baseline.

**Open Question 2**: What properties of the search space cause "variable-length Δ energy" training episodes to fail in protein engineering while succeeding in sentiment control?
- **Basis in paper**: [inferred] In Table 4, the variable-length Δ energy method significantly underperforms fixed-length methods in the protein task (e.g., 42.4% vs 74.8% success at -5 ddG), whereas in Table 5, it performs competitively in the sentiment task. The paper notes the weakness in protein tasks is "not found in the results for sentiment," leaving the cause unexplained.
- **Why unresolved**: The authors acknowledge the discrepancy but do not analyze why selecting states based on specific energy thresholds works for text but not biological sequences. It suggests the granularity of improvement differs fundamentally between discrete text edits and protein mutations.
- **What evidence would resolve it**: An analysis of the distribution of energy improvements (Δ scores) per step in protein chains versus text chains, specifically measuring the density of high-reward transitions relative to the total chain length.

**Open Question 3**: How can the extrapolation capabilities of this method be fairly compared against state-of-the-art instruction-tuned LLMs?
- **Basis in paper**: [explicit] "Due to the fact that our extrapolation tasks require methods to have not been explicitly tuned on data in the extrapolation range, we are unable to compare to many state-of-the-art baselines, such as prompting."
- **Why unresolved**: Large foundation models are typically trained on massive datasets that likely include the "extrapolation range" (e.g., 5-star reviews), making them unsuitable baselines for a true out-of-distribution generalization study. This creates a gap in understanding how this method stacks up against general-purpose models.
- **What evidence would resolve it**: A benchmark using a "hold-out" property or domain that was explicitly scrubbed from the training data of the baseline LLMs, or evaluating on a temporally shifted dataset where the "extrapolation" concept did not exist during the LLM's pre-training.

## Limitations
- **Oracle Score Reliability**: The method critically depends on the quality of the proxy scorer $s(x)$, which must be accurate within the training range but unreliable in the extrapolation zone.
- **Efficiency Claims Context**: While the method demonstrates significantly fewer steps than MCMC, this comparison assumes MCMC is run to completion for each sample, which may not reflect practical usage.
- **Generalization to Different Sequence Types**: The experiments focus on discrete, relatively short sequences (proteins, reviews, Reddit posts), leaving effectiveness for longer or more complex tasks untested.

## Confidence

- **High Confidence**: The core distillation mechanism (learning from MCMC transitions) is well-supported by the experimental results. The efficiency gains are reproducible and the ablation studies demonstrate sensitivity to episode selection strategy.
- **Medium Confidence**: The extrapolation claims are supported for the specific domains tested, but the definition of "extrapolation" may not capture all aspects of successful generalization.
- **Low Confidence**: The long-term stability and diversity of the generated sequences beyond the immediate extrapolation range is not evaluated.

## Next Checks

1. **Scorer Uncertainty Validation**: Implement a controlled experiment where the proxy scorer is deliberately made unreliable in the extrapolation range (e.g., random noise added beyond threshold). Verify that the method still produces meaningful improvements rather than optimizing for noise.

2. **Early-Stopping MCMC Baseline**: Implement an early-stopping criterion for MCMC (e.g., stop when no improvement after k steps) and compare efficiency against the proposed method to establish a fairer baseline.

3. **Diversity and Semantic Coherence Test**: Generate 100 samples from both MCMC and the proposed method for each task. Compute diversity metrics (unique n-grams) and semantic coherence (SBERT similarity to reference distribution) to ensure the method doesn't sacrifice quality for efficiency.