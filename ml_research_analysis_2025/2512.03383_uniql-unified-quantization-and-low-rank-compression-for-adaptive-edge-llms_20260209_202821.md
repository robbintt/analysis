---
ver: rpa2
title: 'UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs'
arxiv_id: '2512.03383'
source_url: https://arxiv.org/abs/2512.03383
tags:
- uniql
- pruning
- quantization
- compression
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniQL, a unified post-training quantization
  and structured pruning framework designed to enable adaptive deployment of large
  language models (LLMs) on resource-constrained edge devices. The key challenges
  addressed include limited memory, shared computational resources, and dynamic workloads
  that make fixed-size models impractical.
---

# UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs

## Quick Facts
- **arXiv ID**: 2512.03383
- **Source URL**: https://arxiv.org/abs/2512.03383
- **Reference count**: 40
- **Primary result**: Achieves 4×–5.7× memory reduction and 2.7×–3.4× token throughput on edge devices while maintaining accuracy within 5% of original models at 15% pruning

## Executive Summary
UniQL introduces a unified post-training quantization and structured pruning framework designed to enable adaptive deployment of large language models (LLMs) on resource-constrained edge devices. The framework addresses challenges including limited memory, shared computational resources, and dynamic workloads that make fixed-size models impractical. UniQL supports Transformers, State Space Models (SSMs), and hybrid architectures, performing weight-sorting, masked fine-tuning, and quantization in a single cloud pass while allowing on-device configurable pruning rates up to 35%.

The core method uses efficient structured weight-sorting algorithms—including pseudo-inverse-free MLP decomposition, quantization-aware SVD for attention layers, state-aware sorting for SSMs, and fused RoPE kernels—to rank and prune model channels. Masked LoRA fine-tuning adapts the sorted model to multiple pruning rates in one training run, and 4-bit quantization minimizes memory usage. On-device pruning is then applied based on current resource availability.

## Method Summary
UniQL performs a single cloud-based pass of weight-sorting, masked LoRA fine-tuning, and quantization, then enables runtime adaptation on edge devices. The framework uses pseudo-inverse-free MLP sorting via ridge leverage scores, quantization-aware SVD for attention layers, state-aware sorting for SSMs, and symmetric sorting for RoPE-compatible attention. A single masked LoRA fine-tuning run with randomly sampled pruning masks allows the model to support multiple pruning rates (0–35%) at deployment time. The method employs 4-bit quantization and layer-wise pruning rates determined by budget importance scores.

## Key Results
- Achieves 4×–5.7× memory reduction and 2.7×–3.4× token throughput improvement compared to baselines
- Maintains accuracy within 5% of original models at 15% pruning across six diverse LLM architectures
- Supports adaptive pruning rates up to 35% on-device with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-inverse-free MLP Weight Sorting
- Replaces expensive pseudo-inverse with ridge leverage scores for 20× faster weight sorting while maintaining pruning quality
- Uses intermediate activations X_int to compute channel correlation matrix C = X_int^T X_int, then derives ridge leverage scores diag(C(C + λI)^{-1}) to rank channel importance
- Core assumption: Ridge leverage scores provide sufficiently accurate proxy for channel importance compared to theoretical pseudo-inverse bounds

### Mechanism 2: Quantization-Aware SVD for Attention Layers
- Fuses diagonal eigenvalue matrix Σ into U to reduce INT4 quantization error by ensuring eigenvalues serve as per-column scaling factors
- Prevents outlier eigenvalues from dominating quantization grid for all columns
- Core assumption: INT4 group-wise symmetric quantization is sensitive to within-group numerical distribution variance

### Mechanism 3: Masked LoRA Fine-Tuning for Multi-Rate Adaptation
- Single LoRA fine-tuning run with randomly sampled pruning masks enables support for multiple pruning rates (0–35%) at deployment time
- At each step, randomly samples global pruning rate P_t from predetermined set to mask pruned channels
- Core assumption: Low-rank adapters can learn rate-invariant corrections that generalize across sampled pruning distribution

## Foundational Learning

- **Singular Value Decomposition (SVD)**
  - Why needed here: Core to attention layer compression; eigenvalue truncation determines which channels are pruned
  - Quick check question: Given W = UΣV^T, which matrix contains the singular values that rank channel importance?

- **Ridge Regression / Leverage Scores**
  - Why needed here: MLP sorting uses ridge leverage scores diag(C(C + λI)^{-1}) as proxy for channel importance without pseudo-inverse
  - Quick check question: Why does adding λI to correlation matrix before inversion improve numerical stability?

- **Rotary Position Embedding (RoPE)**
  - Why needed here: Fused RoPE kernel and symmetric sorting strategy depend on understanding how RoPE pairs dimensions for rotation
  - Quick check question: RoPE splits dimensions into pairs [x1, x2] for rotation—why does this require symmetric index sorting?

- **Group-wise Symmetric Quantization**
  - Why needed here: INT4 compression uses per-group scaling factors; QSVD design directly addresses quantization group sensitivity
  - Quick check question: If weights in quantization group have high variance, what happens to quantization error under symmetric uniform quantization?

## Architecture Onboarding

- **Component map**:
  - MLP blocks: up_proj, gate_proj, down_proj → sorted via ridge leverage scores (Algorithm 1)
  - MHSA blocks: Q/K projections → symmetric sorted with fused RoPE (Algorithm 2/6); V/O projections → quantization-aware SVD (Algorithm 3/7)
  - Mamba blocks: B/C projections → Δ-aware correlation sorting (Algorithm 4); z/x/o projections → state-aware ridge scores (Algorithm 5)
  - Calibration pipeline: wikitext2 (128 samples, 2048 seq) → Alpaca (128 samples) for sorting → Alpaca (51.8k samples) for fine-tuning

- **Critical path**:
  1. Collect calibration activations from all layer types
  2. Compute layer-wise BI scores → allocate per-layer pruning rates
  3. Run structured weight-sorting (per-block algorithms)
  4. Apply masked LoRA fine-tuning (5 epochs, random rate sampling)
  5. Fuse Hadamard transforms (except pruned dimensions)
  6. Apply GPTQ-style INT4 quantization (group size 128)
  7. Deploy with on-demand channel masking

- **Design tradeoffs**:
  - Pruning rate vs. accuracy: 15% pruning stays within 5% of baseline; 35% drops 10–15%
  - Calibration set choice: Alpaca outperforms WikiText2 for weight-sorting
  - Hadamard fusion: Disabled for pruned channels to enable flexible masking; may slightly reduce quantization quality
  - LoRA rank (r=8): Low rank keeps fine-tuning cheap but may limit recovery at high pruning rates

- **Failure signatures**:
  - Accuracy collapse (>20% drop) at 25%+ pruning without fine-tuning → insufficient masked FT coverage
  - OOM on edge device at 0% pruning → verify embedding/output layers are quantized
  - Latency worse than baseline → check if fused RoPE kernel is actually invoked
  - SVD numerical instability on Qwen-2.5 → large intermediate dimension can cause ill-conditioned matrices

- **First 3 experiments**:
  1. **Smoke test**: Run full pipeline on Llama-3.1-8B at 0% and 15% pruning; verify average accuracy on 5 zero-shot tasks matches Table 4 (73.6%, 71.4%)
  2. **Ablation**: Disable QSVD on attention layers at 25% pruning; confirm ~7% accuracy drop per Table 10
  3. **Latency validation**: Profile TPOT on target edge device (e.g., Orin Nano) at 0%, 25%, 35% pruning; compare against TAO-HQQ baseline per Table 8

## Open Questions the Paper Calls Out
- No specific open questions were explicitly called out in the paper.

## Limitations
- Performance degradation at extreme pruning rates beyond 35% where masked LoRA fine-tuning cannot recover accuracy
- Numerical stability concerns for extremely large intermediate dimensions in MLP sorting
- Incomplete Hadamard fusion due to pruned dimensions potentially affecting quantization quality

## Confidence
- **High confidence**: Memory reduction (4×–5.7×) and token throughput (2.7×–3.4×) improvements vs. baselines—directly measured on specified hardware (A6000, Orin Nano 8G) with clear experimental protocols
- **Medium confidence**: Accuracy retention within 5% at 15% pruning—based on zero-shot evaluations across five tasks, but limited to six tested architectures without ablation on pruning rate sensitivity
- **Low confidence**: The 20× speedup claim for pseudo-inverse-free sorting—while Table 1 shows 22× speedup for specific matrix sizes, general-case performance across all MLP dimensions is not characterized

## Next Checks
1. **Numerical robustness test**: Run UniQL on Qwen-2.5-14B with varying calibration batch sizes; measure accuracy stability and detect any SVD conditioning warnings during sorting
2. **Quantization quality ablation**: Disable QSVD on attention layers for Llama-3.1-8B at 25% pruning; verify the ~7% accuracy drop per Table 10 to confirm quantization-aware SVD's contribution
3. **Latency validation on target edge device**: Profile TPOT on Orin Nano 8G at 0%, 25%, 35% pruning; compare against TAO-HQQ baseline per Table 8 to confirm claimed 2.7×–3.4× throughput gains