---
ver: rpa2
title: 'Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes'
arxiv_id: '2508.05019'
source_url: https://arxiv.org/abs/2508.05019
tags:
- clinical
- soap
- notes
- note
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents skin-SOAP, a weakly supervised multimodal framework
  for generating structured SOAP notes from limited dermatologic inputs. The approach
  uses synthetic data generation via retrieval-augmented generation, parameter-efficient
  fine-tuning (QLoRA), and supervised fine-tuning to adapt Vision-LLaMA 3.2 for structured
  clinical note generation.
---

# Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes

## Quick Facts
- arXiv ID: 2508.05019
- Source URL: https://arxiv.org/abs/2508.05019
- Authors: Sadia Kamal; Tim Oates; Joy Wan
- Reference count: 10
- Primary result: Skin-SOAP achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro on clinical SOAP note generation

## Executive Summary
This paper introduces skin-SOAP, a weakly supervised multimodal framework that generates structured SOAP notes from dermatologic images with limited clinical inputs. The approach leverages synthetic data generation via retrieval-augmented generation (RAG), parameter-efficient fine-tuning (QLoRA), and supervised fine-tuning to adapt Vision-LLaMA 3.2 for clinical documentation. The framework addresses the challenge of generating structured medical notes without extensive expert annotations, achieving performance comparable to leading foundation models while maintaining clinical coherence and structural integrity.

## Method Summary
The method employs a three-stage pipeline: (1) GPT-3.5 converts structured clinical metadata into natural language captions, (2) RAG retrieves relevant medical context from a curated vector database, and (3) Vision-LLaMA 3.2 generates SOAP notes conditioned on the multimodal input. QLoRA adaptation with rank=8 enables efficient fine-tuning while preserving base model capabilities. The framework is trained on PAD-UFES-20 dataset using cross-entropy loss minimization, with synthetic captions serving as pseudo-labels. Evaluation uses novel metrics including MedConceptEval (ClinicalBERT similarity) and Clinical Coherence Score (CCS), alongside Flow-Judge qualitative assessment.

## Key Results
- Skin-SOAP achieves perfect Flow-Judge scores (20/20) on structure, readability, completeness, and medical relevance
- Clinical BERT F1 scores demonstrate high semantic alignment with clinical concepts
- Performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro on SOAP note generation tasks
- Two-way ANOVA shows significant effects of SOAP section on semantic alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-augmented grounding reduces clinical hallucinations and improves factual reliability of generated SOAP notes.
- **Mechanism:** Generated captions query a curated vector database (ChromaDB) indexing authoritative medical sources (NCI, ACS, NHS, South Texas Skin Cancer Institute). Retrieved context is concatenated with captions before being passed to Vision-LLaMA, anchoring outputs in validated clinical knowledge rather than relying solely on parametric memory.
- **Core assumption:** The retrieval corpus is comprehensive, current, and sufficiently aligned with the target dermatologic conditions to provide relevant context.
- **Evidence anchors:**
  - [abstract] "reduces reliance on manual annotations, enabling scalable, clinically grounded documentation"
  - [section 3.2] "addresses common limitations of pre-trained language models, such as outdated knowledge and hallucinated reasoning"
  - [corpus] CLI-RAG paper confirms RAG improves clinical text generation but notes challenges with unstructured patient data
- **Break condition:** Retrieval fails when lesion presentations are rare/atypical and not well-represented in the indexed corpus; retrieval quality degrades if query embeddings poorly match document semantics.

### Mechanism 2
- **Claim:** Synthetic weak supervision via GPT-3.5 captioning creates scalable training data without expert annotation.
- **Mechanism:** Structured clinical metadata (26 features per lesion) is converted to natural language captions by GPT-3.5. These captions, combined with RAG-retrieved context, serve as pseudo-labels for training Vision-LLaMA. The model learns to map multimodal inputs → structured SOAP format through cross-entropy loss minimization.
- **Core assumption:** GPT-3.5-generated captions sufficiently capture clinically relevant information; errors in synthetic labels do not cascade significantly into fine-tuned model.
- **Evidence anchors:**
  - [abstract] "weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs"
  - [section 3.2] "we employ a weak supervision strategy to synthesize training data"
  - [corpus] SynGP500 and related work show synthetic clinical data generation is emerging but validation remains limited
- **Break condition:** Systematic errors in GPT-3.5 captioning propagate through the pipeline; domain shift between synthetic and real clinical documentation patterns.

### Mechanism 3
- **Claim:** Low-rank adaptation (QLoRA) enables efficient domain specialization while preserving base model capabilities.
- **Mechanism:** LoRA injects trainable low-rank matrices (rank=8, α=16) into transformer projections (q, k, v, o, gate, up, down). Original weights remain frozen; only ΔW = AB is updated. This constrains the adaptation to a low-dimensional subspace, reducing overfitting risk on the small dermatology dataset (2,298 images).
- **Core assumption:** The clinical documentation task lies within the representational capacity of low-rank perturbations to the base model.
- **Evidence anchors:**
  - [section 3.3] "reduces computational costs without compromising model performance"
  - [section 3.4] Training completed in 1.5 hours on single A100 for fine-tuning
  - [corpus] Multiple recent papers confirm LoRA effectiveness for clinical LLM adaptation
- **Break condition:** Complex clinical reasoning patterns requiring higher-rank representations may not be captured; performance ceiling limited by base model's pre-trained medical knowledge.

## Foundational Learning

- **Concept:** SOAP note structure (Subjective, Objective, Assessment, Plan)
  - **Why needed here:** The entire framework is optimized to produce this specific 4-section format; understanding what belongs in each section is essential for debugging outputs.
  - **Quick check question:** Given a patient's chief complaint of "bleeding mole," which SOAP section would contain this vs. the diagnosis?

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** Core mechanism for clinical grounding; the ChromaDB vector store and retrieval logic are central to reducing hallucinations.
  - **Quick check question:** If retrieval returns no relevant documents for a query, what happens to the model's output quality?

- **Concept:** Parameter-efficient fine-tuning (LoRA/QLoRA)
  - **Why needed here:** The training setup uses specific hyperparameters (r=8, α=16); understanding how low-rank matrices work helps diagnose underfitting/overfitting.
  - **Quick check question:** What is the tradeoff between increasing LoRA rank vs. training time and memory?

## Architecture Onboarding

- **Component map:** Image + metadata → GPT-3.5 caption → RAG retrieval → Vision-LLaMA (QLoRA-adapted) → SOAP output
- **Critical path:** The pipeline from image and structured metadata through GPT-3.5 captioning, RAG retrieval, and QLoRA-adapted Vision-LLaMA to structured SOAP output is the core sequence. Failure at any stage cascades downstream; caption quality gates everything.
- **Design tradeoffs:**
  - Weak supervision vs. annotation cost: synthetic labels scale but may lack clinical nuance
  - RAG grounding vs. inference latency: retrieval adds ~100-500ms per query (not reported in paper)
  - LoRA rank vs. expressivity: r=8 chosen empirically; higher ranks may improve performance but increase memory
- **Failure signatures:**
  - Structural errors (Fig 4a): diagnosis placed in Subjective instead of Assessment → indicates insufficient SFT on format
  - Low MedConceptEval scores for SCC/BCC (Table 1): model may struggle with subtle morphological features
  - CCS higher for generated than ground truth: model may be overfitting to caption vocabulary rather than deeper clinical reasoning
- **First 3 experiments:**
  1. **Ablate RAG:** Generate SOAP notes with/without retrieval; compare MedConceptEval scores to quantify grounding contribution.
  2. **LoRA rank sweep:** Test r∈{4,8,16,32} on validation set; plot Clinical BERT F1 vs. training time to find efficiency frontier.
  3. **Error analysis by lesion type:** Stratify outputs by BCC/MEL/SCC/ACK/SEK/NEV; identify which conditions have lowest semantic alignment and examine retrieval coverage for those classes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does Skin-SOAP generalize to diverse datasets with varying metadata standards?
- **Basis in paper:** [explicit] The authors acknowledge using only a single dataset (PAD-UFES-20) and that "variations in metadata across different sources posed challenges for standardization."
- **Why unresolved:** The current validation is restricted to one specific data structure, limiting claims of scalability.
- **What evidence would resolve it:** Successful evaluation on heterogeneous, multi-source dermatology datasets with distinct metadata schemas.

### Open Question 2
- **Question:** Can human-in-the-loop strategies effectively bridge the gap in clinical reasoning depth?
- **Basis in paper:** [explicit] Future work proposes "incorporating human-in-the-loop refinement strategies"; [inferred] CCS analysis showed generated notes lack the "depth and complexity" of expert-written notes.
- **Why unresolved:** While the model is fluent, it does not yet fully replicate the complex reasoning found in ground-truth documentation.
- **What evidence would resolve it:** Qualitative and quantitative improvements in reasoning depth when comparing baseline output to HITL-refined notes.

### Open Question 3
- **Question:** How can evaluation benchmarks better capture the progression of clinical reasoning across multiple encounters?
- **Basis in paper:** [explicit] Future work suggests "developing evaluation benchmarks that capture the progression of clinical reasoning across multiple encounters."
- **Why unresolved:** Current metrics (MedConceptEval, CCS) are designed for single-instance notes rather than longitudinal patient care.
- **What evidence would resolve it:** A new longitudinal benchmark assessing diagnostic consistency and plan evolution over multiple patient visits.

## Limitations
- The evaluation framework relies heavily on synthetic weak supervision without direct validation against ground truth clinical documentation patterns
- RAG grounding effectiveness depends on the quality, coverage, and currency of indexed medical sources, which may not represent rare/atypical presentations
- QLoRA with rank=8 constrains adaptation to low-rank perturbations, potentially limiting capture of complex clinical reasoning patterns

## Confidence
- **High confidence:** SOAP structure generation capability (Flow-Judge scores of 20/20) and effectiveness of parameter-efficient fine-tuning for reducing computational costs
- **Medium confidence:** Clinical semantic alignment claims (MedConceptEval scores of 0.8-0.9) and overall comparability to GPT-4o/Claude/DeepSeek Janus Pro
- **Low confidence:** Claim that RAG grounding significantly reduces clinical hallucinations due to lack of ablation studies

## Next Checks
1. **RAG ablation study:** Generate SOAP notes with and without retrieval augmentation on a held-out validation set. Compare MedConceptEval scores and conduct qualitative expert review to quantify the grounding contribution and identify hallucination patterns.

2. **Descriptor bank validation:** Have clinical experts review the curated descriptor banks for each lesion class to verify clinical accuracy and completeness. Identify any missing concepts or outdated terminology that could bias the MedConceptEval scores.

3. **Rank sensitivity analysis:** Systematically vary the LoRA rank parameter (r ∈ {4, 8, 16, 32}) while holding other hyperparameters constant. Plot Clinical BERT F1 vs. training time and memory usage to identify the optimal efficiency frontier and assess whether r=8 is truly optimal for this task.