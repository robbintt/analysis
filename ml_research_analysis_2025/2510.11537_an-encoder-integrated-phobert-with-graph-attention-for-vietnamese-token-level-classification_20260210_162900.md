---
ver: rpa2
title: An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level
  Classification
arxiv_id: '2510.11537'
source_url: https://arxiv.org/abs/2510.11537
tags:
- phobert
- vietnamese
- graph
- entity
- disfluency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TextGraphFuseGAT, a hybrid model that integrates
  a pretrained Vietnamese language model (PhoBERT) with Graph Attention Networks (GAT)
  and a Transformer decoder for token-level classification tasks. The approach constructs
  a fully connected graph over PhoBERT token embeddings and applies GAT to capture
  rich inter-token dependencies, followed by a Transformer decoder for further refinement.
---

# An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification

## Quick Facts
- arXiv ID: 2510.11537
- Source URL: https://arxiv.org/abs/2510.11537
- Authors: Ba-Quang Nguyen
- Reference count: 6
- Primary result: Proposed model achieves state-of-the-art performance on Vietnamese token-level classification tasks across three benchmarks

## Executive Summary
This paper introduces TextGraphFuseGAT, a hybrid model that integrates PhoBERT with Graph Attention Networks (GAT) and a Transformer decoder for Vietnamese token-level classification. The model constructs a fully connected graph over PhoBERT token embeddings and applies GAT to capture inter-token dependencies, followed by a Transformer decoder for further refinement. Evaluated on PhoNER-COVID19, PhoDisfluency, and VietMed-NER datasets, the model consistently outperforms strong baselines including transformer-only and conventional neural models like BiLSTM + CNN + CRF.

The approach demonstrates significant improvements across all three Vietnamese benchmarks, achieving Micro-F1 scores of 0.984 (word-level) and 0.982 (syllable-level) on PhoNER-COVID19, near-perfect scores on PhoDisfluency, and 0.893 on the challenging VietMed-NER dataset. Ablation studies confirm that both the GAT layer and Transformer decoder contribute to improved performance, particularly on complex, imbalanced datasets. The method validates the effectiveness of combining pretrained semantic features with graph-based relational modeling for token classification in Vietnamese NLP.

## Method Summary
TextGraphFuseGAT is a hybrid model that combines pretrained language modeling with graph-based relational learning for Vietnamese token-level classification. The architecture begins with PhoBERT as the encoder to generate contextualized token embeddings from input text. These embeddings serve as node features in a fully connected graph where each token is connected to every other token. Graph Attention Networks process this graph structure, allowing tokens to attend to relevant contextual information from other tokens while learning to weight these relationships dynamically. The GAT output is then fed into a Transformer decoder for additional sequence refinement. Finally, a classification layer maps the refined token representations to label predictions. The model is trained end-to-end using standard cross-entropy loss on labeled Vietnamese datasets.

## Key Results
- Achieves Micro-F1 scores of 0.984 (word-level) and 0.982 (syllable-level) on PhoNER-COVID19, substantially exceeding PhoBERT large
- Attains near-perfect Micro-F1 scores of 0.994 (word-level) and 0.993 (syllable-level) on PhoDisfluency detection
- Achieves Micro-F1 of 0.893 on VietMed-NER with 18 entity types, outperforming all baselines including transformer-only models

## Why This Works (Mechanism)
The model works by combining the semantic richness of pretrained language models with the structural awareness of graph neural networks. PhoBERT provides strong contextualized embeddings that capture language-specific patterns in Vietnamese. The GAT layer enables tokens to explicitly model dependencies and relationships with other tokens in the sequence, capturing long-range dependencies that may be difficult for standard transformers to learn efficiently. The Transformer decoder then refines these graph-enhanced representations through self-attention, allowing the model to further process the relational information. This multi-stage approach leverages the strengths of each component: PhoBERT's language understanding, GAT's structural reasoning, and Transformer's sequence modeling capabilities.

## Foundational Learning

**PhoBERT**: A pretrained Vietnamese language model based on the RoBERTa architecture, trained on large Vietnamese corpora. Why needed: Provides strong contextualized embeddings specific to Vietnamese linguistic patterns. Quick check: Ensure the model has been properly pretrained on sufficient Vietnamese data.

**Graph Attention Networks (GAT)**: Neural networks that operate on graph-structured data, computing node representations by attending to neighbors with learned weights. Why needed: Enables modeling of inter-token dependencies and relationships beyond local context. Quick check: Verify attention coefficients are properly normalized and contribute to meaningful node updates.

**Transformer Decoder**: The autoregressive component of transformer architecture that processes sequences through self-attention. Why needed: Further refines graph-enhanced representations through sequence modeling capabilities. Quick check: Confirm proper masking is applied to prevent information leakage in the decoder.

**Token-level Classification**: The task of assigning labels to individual tokens in a sequence, common in NER and disfluency detection. Why needed: The target application domain for the proposed method. Quick check: Verify label consistency between word-level and syllable-level tokenization schemes.

## Architecture Onboarding

**Component Map**: Input Text -> PhoBERT Encoder -> Fully Connected Graph -> GAT Layer -> Transformer Decoder -> Classification Layer -> Output Labels

**Critical Path**: The most performance-critical components are the PhoBERT encoder (provides semantic foundation), GAT layer (captures relational information), and Transformer decoder (refines representations). Bottlenecks may occur in the fully connected graph construction for long sequences and in the GAT computation which scales quadratically with sequence length.

**Design Tradeoffs**: The fully connected graph construction maximizes relational modeling but introduces O(n²) complexity. Using a pretrained model (PhoBERT) provides strong initialization but limits adaptability to other languages. The three-stage pipeline (pretrained encoder → GAT → Transformer decoder) adds complexity but captures complementary information at each stage.

**Failure Signatures**: Performance degradation may occur on very long sequences due to quadratic scaling in graph construction and GAT computation. The model may struggle with domain shifts if pretrained on different Vietnamese corpora than the target tasks. Over-smoothing in GAT could lead to indistinguishable token representations, particularly in homogeneous sequences.

**First Experiments**:
1. Run the model on a small subset of training data to verify basic functionality and check for NaN or gradient explosion issues
2. Compare attention weight distributions from the GAT layer to identify whether meaningful relationships are being learned
3. Evaluate the model's performance on sequences of increasing length to identify computational bottlenecks and potential over-smoothing effects

## Open Questions the Paper Calls Out
None

## Limitations
- Requires substantial computational resources due to the combination of PhoBERT, GAT layers, and Transformer decoder, potentially limiting practical deployment on resource-constrained systems
- The fully connected graph construction may introduce unnecessary computational overhead and could lead to over-smoothing effects in longer sequences
- Evaluation scope remains narrow, focusing exclusively on Vietnamese datasets across three specific tasks, with unproven generalizability to other languages or broader NLP tasks

## Confidence
- Claims about performance improvements on Vietnamese datasets: High
- Claims about broader effectiveness for general token-level classification: Medium
- Claims about specific contributions of GAT layer versus Transformer decoder: Medium

## Next Checks
1. Evaluate the model's performance on non-Vietnamese languages and diverse token-level classification tasks beyond NER and disfluency detection to assess cross-linguistic generalizability
2. Conduct controlled experiments varying graph connectivity patterns (sparse vs. fully connected) and GAT layer depth to isolate their specific contributions to performance gains
3. Test the model's robustness on sequences of varying lengths and on datasets with different class imbalance ratios to understand its limitations in real-world scenarios