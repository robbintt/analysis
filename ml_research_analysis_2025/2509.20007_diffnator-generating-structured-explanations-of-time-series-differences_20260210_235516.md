---
ver: rpa2
title: 'DiffNator: Generating Structured Explanations of Time-Series Differences'
arxiv_id: '2509.20007'
source_url: https://arxiv.org/abs/2509.20007
tags:
- time-series
- series
- type
- differences
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes DiffNator, a framework for generating structured
  explanations of differences between two time series. The method uses a JSON schema
  to represent differences, which are classified into two types: phenomena appearing
  in only one series (Type 1) or differing in intensity (Type 2).'
---

# DiffNator: Generating Structured Explanations of Time-Series Differences

## Quick Facts
- arXiv ID: 2509.20007
- Source URL: https://arxiv.org/abs/2509.20007
- Reference count: 0
- Primary result: Achieves field accuracies above 95% for type and function identification, and match accuracy above 91% when maximum number of differences is one.

## Executive Summary
This paper proposes DiffNator, a framework for generating structured JSON explanations of differences between two univariate time series. The method uses a frozen LLM with time-series encoders to output explanations classified as Type 1 (phenomenon appearing in only one series) or Type 2 (differing intensity). Experiments on the TORI dataset show substantial improvements over visual question answering and retrieval baselines, with strong performance even as the number of differences increases.

## Method Summary
DiffNator generates structured JSON explanations of differences between two time series using a frozen LLM with trainable encoders and adapters. The framework computes pointwise differences in representation space before LLM fusion, uses explicit JSON schema constraints to reduce output complexity, and trains on synthetic data generated from real signals with controlled perturbations. The method achieves high accuracy in identifying difference types, functions, and parameters while maintaining valid JSON output structure.

## Key Results
- Field accuracies exceed 95% for type and function identification
- Match accuracy remains above 91% when maximum number of differences is one
- Performance shows modest degradation (to 66.4% overall match accuracy) when maximum differences increase to four
- Trend and Event categories show 20+ point improvements with query attention over mean pooling

## Why This Works (Mechanism)

### Mechanism 1
Explicit difference encoding via learned merging operations enables the LLM to reason about comparative patterns rather than raw signals. The framework computes pointwise differences in representation space before LLM fusion, with query attention learning localized difference patterns while mean pooling aggregates global differences. This forces the encoder to learn comparative rather than absolute features.

### Mechanism 2
JSON schema constraints reduce output space complexity and enable structured decoding from frozen LLMs without fine-tuning. The schema enforces 7 categorical fields with limited vocabularies, and the adapter projects time-series embeddings into LLM embedding space. Only adapter and encoder are trained; LLM remains frozen.

### Mechanism 3
Synthetic data generation from real signals with controlled perturbations creates learnable difference distributions. Real TORI signals are perturbed by adding 27 component functions with controlled parameters, creating elementary differences with known ground-truth JSON. This creates a training distribution that covers meaningful differences in IoT deployments.

## Foundational Learning

- **Attention-based difference detection in sequence representations**
  - Why needed here: Query attention on difference is the core merging operation; understanding how learnable queries capture localized patterns is essential for debugging
  - Quick check question: Given two time series with a spike at t=100 in target only, would mean pooling or query attention better isolate the spike's contribution to D_t?

- **LLM embedding alignment via adapter MLPs**
  - Why needed here: The adapter projects from encoder dimension to LLM embedding space; poor alignment causes the LLM to ignore time-series signals
  - Quick check question: If the adapter outputs were randomly initialized and frozen, would the LLM generate valid JSON? Why or why not?

- **Structured decoding with schema constraints**
  - Why needed here: JSON schema enforcement happens via prompt engineering, not architectural constraints; understanding prompt-design trade-offs is critical for extending the schema
  - Quick check question: What happens if the prompt specifies an invalid JSON schema (e.g., circular field dependencies)?

## Architecture Onboarding

- **Component map**: Input → Encoder (both series independently) → Merger (compute D_t) → Adapter → Fuse with prompt embeddings → LLM → JSON decode

- **Critical path**: The time series flow through separate encoders, merge via difference computation, adapt to LLM space, and generate structured JSON output

- **Design tradeoffs**:
  - Informer vs. TCN: Informer has attention mechanisms better for long-range dependencies; TCN is faster for inference
  - Mean pooling vs. Query attention: Attention adds 6 learnable queries and improves Event/Trend localization by 20+ points but adds computation
  - Frozen LLM vs. Fine-tuning: Freezing reduces training cost but may limit adaptation to schema-specific patterns

- **Failure signatures**:
  - High IoU but low match accuracy: Correct interval localization but wrong function classification—suggests encoder-LLM alignment issue
  - Low presence/param accuracy with high type accuracy: Correct difference detection but wrong attribute prediction—suggests merger loses detail
  - Valid JSON with wrong field counts: LLM hallucinates or misses differences—prompt schema may be ambiguous

- **First 3 experiments**:
  1. Ablate merging strategy: Replace query attention with simple concatenation and measure match accuracy degradation on Event category
  2. Test adapter initialization: Compare random vs. pre-trained adapter weights to assess encoder representation compatibility
  3. Stress-test schema scaling: Add 5 new component functions to the taxonomy and measure generalization gap

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several critical questions emerge from the methodology and results.

## Limitations
- Synthetic data reliance may create domain gaps between training perturbations and real-world IoT difference patterns
- 27-component function taxonomy may not exhaustively cover all meaningful difference types in deployment scenarios
- Performance degradation with multiple simultaneous differences suggests limitations in handling complex, overlapping phenomena

## Confidence
- **High confidence**: Match accuracy results on single-difference cases (K_max = 1) with >91% accuracy
- **Medium confidence**: Generalizability to real-world IoT deployment scenarios given synthetic training data approach
- **Medium confidence**: Performance claims on multi-difference cases (K_max > 1) where modest degradation is observed

## Next Checks
1. **Domain transfer validation**: Test the trained model on a held-out set of real-world IoT difference pairs to measure performance degradation and identify specific difference types where synthetic training distribution fails
2. **Schema extensibility test**: Add 5-10 new component functions to the taxonomy representing edge-case phenomena and measure zero-shot or few-shot generalization performance
3. **Baseline comparison on real data**: Apply the same synthetic-data training protocol to visual question answering and retrieval baselines to establish whether DiffNator's advantage stems from architecture or synthetic data alignment