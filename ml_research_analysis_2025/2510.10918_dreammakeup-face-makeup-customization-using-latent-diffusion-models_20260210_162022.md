---
ver: rpa2
title: 'DreamMakeup: Face Makeup Customization using Latent Diffusion Models'
arxiv_id: '2510.10918'
source_url: https://arxiv.org/abs/2510.10918
tags:
- makeup
- facial
- diffusion
- dreammakeup
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DreamMakeup introduces a training-free diffusion-based framework
  for face makeup customization that overcomes limitations of existing GAN-based and
  fine-tuned diffusion approaches. The method uses early-stopped DDIM inversion to
  preserve facial structure and identity, while enabling extensive customization through
  diverse conditioning inputs including reference images, specific RGB colors, and
  textual descriptions.
---

# DreamMakeup: Face Makeup Customization using Latent Diffusion Models

## Quick Facts
- arXiv ID: 2510.10918
- Source URL: https://arxiv.org/abs/2510.10918
- Reference count: 40
- Primary result: Training-free diffusion framework achieving LPIPS of 0.0667 and CLIP-I of 0.7694 on makeup transfer tasks

## Executive Summary
DreamMakeup introduces a training-free diffusion-based framework for face makeup customization that overcomes limitations of existing GAN-based and fine-tuned diffusion approaches. The method uses early-stopped DDIM inversion to preserve facial structure and identity, while enabling extensive customization through diverse conditioning inputs including reference images, specific RGB colors, and textual descriptions. DreamMakeup achieves superior identity preservation and realistic makeup application without requiring expensive fine-tuning, demonstrating compatibility with Large Language Models for personalized recommendations.

## Method Summary
DreamMakeup employs early-stopped DDIM inversion to preserve facial structure and identity while enabling makeup customization. The framework first inverts the source image to latent space using early-stopped DDIM (t* = 200-400 steps), then applies pixel-space transformations (RGB color transfer, histogram matching, or warping) to the decoded estimate using segmentation masks. A latent interpolation regularizer blends the transformed image with the inverted latent during reverse sampling, while cross-attention composition allows independent control of different makeup attributes through textual prompts.

## Key Results
- LPIPS of 0.0667 and CLIP-I of 0.7694 on makeup transfer tasks, outperforming state-of-the-art GAN and diffusion methods
- Superior identity preservation with less than 4 seconds for color transfer operations
- Fine-grained control over makeup application with compatibility for Large Language Models and integration with Stable Makeup and Stable Diffusion 3.0

## Why This Works (Mechanism)

### Mechanism 1
Early-stopped DDIM inversion preserves facial structure and identity better than full inversion. By terminating inversion at t* ≤ T (typically 200-400 steps), the method retains a latent that decodes faithfully to the original image while maintaining editing flexibility. The denoised estimate ẑ₀(t*) preserves sufficient structural information for faithful reconstruction.

### Mechanism 2
Pixel-space transformations before reverse sampling provide fine-grained color matching. After decoding ẑ₀(t*) to pixel space, domain-specific transformations (histogram matching, RGB color transfer with α scaling, warping) are applied directly on facial regions defined by segmentation masks. This bypasses the diffusion model's tendency to alter colors during standard denoising.

### Mechanism 3
Interpolation-guided sampling in latent space regularizes identity preservation without introducing artifacts. During reverse sampling, the method interpolates between the current denoised estimate and the transformed source latent: z̃₀(t) = (1-λ)ẑ₀(t) + λE(T(x₀)). This keeps the trajectory anchored to original facial details that pure DDIM inversion loses.

## Foundational Learning

- **DDIM Inversion and Tweedie's Formula**: Understanding deterministic mapping of images back through diffusion timesteps is essential for the early-stopping strategy. Tweedie's formula provides the denoised estimate needed at each step. Quick check: Given noisy latent z_t, can you derive ẑ₀ using the noise prediction ε_θ(z_t, t)?

- **Cross-Attention Mechanisms in U-Net**: The cross-attention composition requires understanding how queries, keys, and values from text conditioning interact with spatial features. Quick check: How does weighted composition of multiple attention maps allow independent control of different makeup attributes?

- **Latent vs Pixel Space Operations**: The method explicitly switches between spaces—pixel for color matching, latent for interpolation—requiring understanding of when each domain is appropriate. Quick check: Why does pixel-space interpolation cause artifacts while latent-space interpolation preserves coherence?

## Architecture Onboarding

- **Component map**: Input Image x₀ → Encoder E → z₀ → Early-stopped DDIM Inversion → z_t* → Tweedie Denoising → ẑ₀(t*) → Decoder D → x̂₀(t*) → Pixel-space Transform T → x̂_new → Re-encode → z̃_t* → Reverse DDIM Sampling ← Cross-attention composition + Interpolation guidance → Decoder D → Output x̃₀

- **Critical path**: Early-stopped inversion timestep t* is the most sensitive parameter—start with t*=300 for SD v1.5. Incorrect t* causes either insufficient editing flexibility or identity loss.

- **Design tradeoffs**:
  - t* (early-stop step): Lower = more identity preservation, less editing capacity
  - α (transfer scale): Controls makeup intensity per region; independent per attribute
  - λ (interpolation weight): 0.15 default; balances realism vs makeup fidelity
  - α_s (cross-attention weights): 0.1-0.7 per prompt; negative for suppression prompts

- **Failure signatures**:
  - "Plastic skin" appearance: DDIM inversion ran too deep (t* too high)—reduce to 250-300
  - Color bleeding across regions: Segmentation mask refinement failed—check BiSeNet output quality
  - Identity shift: λ too low or cross-attention weights too aggressive—reduce α_s values
  - Unnatural edges: Gradation smoothing not applied to eyeshadow/lip masks

- **First 3 experiments**:
  1. Reproduce single-attribute RGB transfer (lips only) with t*=300, α=0.5, λ=0.15 on 5 test images to validate baseline identity preservation (target LPIPS <0.07)
  2. Ablate t* across {200, 300, 400, 500} measuring LPIPS and CLIP-I to find optimal early-stop point for your base model
  3. Test cross-attention composition with conflicting prompts ("matte lips" + "glossy lips") to verify independent attribute control and understand weight sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the pixel-space customization module to errors in the BiSeNet segmentation mask, particularly under heavy occlusion or extreme poses? The paper notes the approach may inherit intrinsic limitations of foundational models regarding segmentation, but does not analyze failure rates specifically attributable to segmentation mask bleed or misclassification.

### Open Question 2
Does the strict identity preservation enforced by early-stopped DDIM inversion inherently limit the generation of heavy or structural makeup styles that significantly alter facial perception? While the method preserves identity, it is unclear if the rigid structural constraint prevents the model from executing aggressive contouring or artistic styles that require modifying perceived face shape.

### Open Question 3
Can the LLM integration generalize makeup recommendations to diverse ethnicities without requiring retraining on specific demographic datasets? The evaluation focuses on Asian faces, leaving the performance gap for different skin tones or facial structures unexplored.

## Limitations
- The early-stopping strategy lacks comprehensive ablation studies across different diffusion model architectures
- Cross-attention composition weights are tuned empirically without theoretical grounding for generalization
- The warping algorithm for reference-based eye alignment is described but not fully specified

## Confidence
- **High**: Identity preservation through early-stopped inversion (supported by quantitative LPIPS/CLIP-I metrics)
- **Medium**: Pixel-space transformation effectiveness (demonstrated on controlled datasets but lacks validation on challenging cases)
- **Medium**: Cross-attention composition for multi-prompt control (shows independent attribute manipulation but weight sensitivity requires further exploration)
- **Low**: Generalization to non-makeup customization tasks (framework architecture supports extension but no empirical validation)

## Next Checks
1. **Ablation study across diffusion models**: Test early-stopped inversion on Stable Diffusion v2.1 and SDXL using identical t* parameters to quantify architecture sensitivity
2. **Robustness to segmentation failures**: Systematically evaluate makeup transfer quality when BiSeNet produces imperfect masks to identify failure thresholds
3. **Cross-attention weight sensitivity analysis**: Conduct controlled experiments varying α_s from 0.05 to 1.0 for single and conflicting prompts to establish weight ranges