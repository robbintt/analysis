---
ver: rpa2
title: 'RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning'
arxiv_id: '2507.07451'
source_url: https://arxiv.org/abs/2507.07451
tags:
- training
- experience
- learning
- policy
- rlep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLEP tackles the instability and energy inefficiency of reinforcement
  learning for large language models by introducing a two-phase framework that first
  collects verified reasoning trajectories and then replays them during training.
  At each update step, the model is optimized on mini-batches blending fresh rollouts
  with previously successful solutions.
---

# RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning

## Quick Facts
- arXiv ID: 2507.07451
- Source URL: https://arxiv.org/abs/2507.07451
- Reference count: 17
- Primary result: Improves AIME-2024 accuracy from 38.2% to 39.9% on Qwen2.5-Math-7B

## Executive Summary
RLEP addresses the instability and inefficiency of reinforcement learning for large language models by introducing a two-phase framework that combines verified reasoning trajectory collection with experience replay. The method alternates between gathering high-quality solutions and training on mini-batches that mix new rollouts with previously successful experiences. This approach helps the model avoid unproductive exploration paths and accelerates convergence on challenging math reasoning tasks.

Applied to Qwen2.5-Math-7B, RLEP achieves faster convergence than baselines while reaching higher final accuracy on competition math problems. The method demonstrates particular effectiveness on AIME and AMC benchmarks, showing both efficiency gains and performance improvements through strategic reuse of verified reasoning paths.

## Method Summary
RLEP introduces a two-phase reinforcement learning framework for LLM reasoning that addresses common training instabilities. In the first phase, the model collects verified reasoning trajectories through exploration. In the second phase, training proceeds by sampling mini-batches that combine fresh rollouts with previously successful solutions stored in a replay buffer. This experience replay mechanism allows the model to learn from proven reasoning paths while continuing to explore new solutions. The approach is specifically designed to accelerate convergence and improve accuracy on complex reasoning tasks by steering learning away from unproductive exploration patterns.

## Key Results
- Matches baseline peak accuracy in significantly fewer training steps
- Improves AIME-2024 accuracy from 38.2% to 39.9%
- Improves AIME-2025 accuracy from 19.8% to 22.3%
- Improves AMC-2023 accuracy from 77.0% to 82.2%

## Why This Works (Mechanism)
Experience replay in reinforcement learning for LLMs works by maintaining a buffer of high-quality, verified reasoning trajectories that the model can revisit during training. When the model encounters new situations, it can draw upon previously successful solution paths rather than starting exploration from scratch. This mechanism is particularly valuable in reasoning tasks where finding correct solutions is rare and expensive, as it allows the model to focus on productive exploration directions identified through prior experience.

## Foundational Learning
- **Reinforcement Learning with Experience Replay**: Stores and reuses successful trajectories to improve sample efficiency; needed because reasoning tasks have sparse rewards and expensive rollouts; quick check: compare training curves with and without replay buffer
- **Solution Verification Mechanisms**: Validates reasoning paths before storing them; needed to ensure replay buffer contains only useful examples; quick check: measure false positive rate in stored trajectories
- **Curriculum Learning Through Replay**: Gradually introduces more complex reasoning paths; needed to build from simple to complex problems; quick check: track success rate progression across difficulty levels
- **Exploration-Exploitation Balance**: Manages trade-off between trying new approaches and using known solutions; needed to avoid getting stuck in local optima; quick check: measure diversity of generated solutions over time

## Architecture Onboarding

Component Map:
Experience Collection -> Solution Verification -> Replay Buffer -> Training Updates -> Performance Evaluation

Critical Path:
The critical path flows from experience collection through verification to replay storage, then to training updates. Fresh rollouts are generated, verified for correctness, and either discarded or stored. During training, mini-batches are constructed by sampling from both the replay buffer and new rollouts, with the model updating weights based on this mixed data.

Design Tradeoffs:
The primary tradeoff involves replay buffer size versus training stability. Larger buffers provide more diverse experiences but risk including outdated or suboptimal solutions. The verification mechanism adds computational overhead but ensures quality. Balancing exploration of new solutions against exploitation of proven paths requires careful reward shaping and sampling strategies.

Failure Signatures:
Training instability manifests as oscillating performance or failure to converge. This typically occurs when the replay buffer contains too many low-quality solutions or when the exploration rate is improperly tuned. Performance plateaus may indicate insufficient diversity in stored experiences or overly conservative exploitation strategies.

First Experiments:
1. Verify replay buffer sampling strategy maintains solution diversity
2. Test verification accuracy threshold sensitivity
3. Measure convergence speed with varying buffer sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single 7B model (Qwen2.5-Math-7B) and three math datasets
- No ablation studies to isolate experience replay's specific contribution
- Lacks direct energy consumption measurements for efficiency claims
- No robustness checks for replay buffer size or training stability over extended periods

## Confidence
- **High** for claims about accelerated convergence in LLM reasoning tasks
- **Medium** for claims about superior final accuracy on tested benchmarks
- **Low** for broader claims about energy efficiency and generalizability

## Next Checks
1. Conduct ablation experiments comparing RLEP with and without experience replay to isolate its specific impact on convergence and accuracy
2. Evaluate RLEP on diverse reasoning datasets (e.g., GSM8K, MATH) and across multiple model sizes (e.g., 7B, 13B, 33B) to test scalability and robustness
3. Measure and report per-iteration energy consumption and wall-clock time under identical compute environments to substantiate efficiency claims