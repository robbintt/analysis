---
ver: rpa2
title: A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous
  Control
arxiv_id: '2507.02712'
source_url: https://arxiv.org/abs/2507.02712
tags:
- learning
- network
- replay
- deep
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Forget-and-Grow (FoG), a novel deep RL algorithm\
  \ addressing primacy bias\u2014where agents overfit to early experiences in the\
  \ replay buffer\u2014which limits sample efficiency and generalizability. Inspired\
  \ by infantile amnesia in neuroscience, FoG incorporates two mechanisms: Experience\
  \ Replay Decay (ER Decay), which reduces sampling probability of older transitions\
  \ to \"forget\" outdated experiences, and Network Expansion, which dynamically adds\
  \ new parameters early in training to \"grow\" neural capacity and improve adaptation\
  \ to data shifts."
---

# A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control

## Quick Facts
- **arXiv ID**: 2507.02712
- **Source URL**: https://arxiv.org/abs/2507.02712
- **Reference count**: 40
- **Primary result**: FoG achieves state-of-the-art performance on 41 continuous control tasks by mitigating primacy bias through Experience Replay Decay and Network Expansion

## Executive Summary
This paper introduces Forget-and-Grow (FoG), a novel deep RL algorithm that addresses primacy bias—where agents overfit to early experiences in the replay buffer. Inspired by infantile amnesia in neuroscience, FoG incorporates two mechanisms: Experience Replay Decay (ER Decay), which reduces sampling probability of older transitions, and Network Expansion, which dynamically adds new parameters early in training to improve adaptation to data shifts. Experiments on 41 tasks across four continuous control benchmarks show FoG significantly outperforms state-of-the-art model-free and model-based baselines.

## Method Summary
FoG is built on the OBAC backbone with scaled networks and replay ratios. It combines Experience Replay Decay (sampling weights decay by (1-ε)^t with ε=1e-5) to "forget" outdated experiences, and Network Expansion that dynamically adds residual blocks to the critic at specific intervals (50k and 200k steps post-reset). The algorithm uses a replay ratio of 10 and incorporates periodic network resets. This approach addresses the fundamental problem of primacy bias in high-replay-ratio RL, where early random exploration data dominates learning and prevents effective adaptation to better later data.

## Key Results
- FoG significantly outperforms state-of-the-art model-free (BRO, SimBa) and model-based (TD-MPC2) baselines on 41 tasks across four benchmarks
- Achieves high performance with up to 23M parameters, demonstrating favorable model size scaling
- Shows superior data efficiency and adaptability compared to prior methods
- Ablations confirm both ER Decay and Network Expansion are essential for the performance gains

## Why This Works (Mechanism)

### Mechanism 1: Experience Replay Decay (ER Decay)
- Reduces sampling probability of older transitions to create bounded sampling expectation
- Implements decay factor ε on sampling weights w_{t,i} = max(τ, (1 - ε)^{t-i})
- Prevents overfitting to early, potentially low-quality data
- Shifts data distribution toward recent experiences, approximating a "moving window" of relevance

### Mechanism 2: Network Expansion (Dynamic Capacity)
- Adds fresh parameters (neurons) to restore plasticity
- Critic starts small and expands by adding residual blocks at 50k and 200k steps
- New parameters are "unburdened" by gradients of early experiences
- Addresses plasticity loss measurable by "dormant neurons"

### Mechanism 3: Scaled Replay Ratio with Resets
- High replay ratios (10×) require periodic network resets to clear primacy bias
- Resets alone are insufficient without correcting data imbalance
- ER Decay ensures re-learning focuses on current data after resets
- Prevents excessive catastrophic growth in loss after network resets

## Foundational Learning

- **Concept: Primacy Bias in RL**
  - Why needed: Central failure mode FoG addresses—agents overfit to first experiences collected during random exploration
  - Quick check: Why does "resetting" a network not fully solve primacy bias if the replay buffer still contains mostly early data?

- **Concept: Network Plasticity & Dormant Neurons**
  - Why needed: "Grow" mechanism is a solution to plasticity loss; understanding "dormant" neurons explains why adding new parameters is more effective than just resetting old ones
  - Quick check: How does adding a new residual block differ mathematically from re-initializing an existing block?

- **Concept: Off-Policy Actor-Critic (OBAC/SAC)**
  - Why needed: FoG is built on OBAC; understanding separation of Actor (policy) and Critic (value) is vital as FoG primarily modifies the Critic architecture
  - Quick check: Does FoG apply network expansion to the Actor, the Critic, or both? (Answer: Primarily the Critic)

## Architecture Onboarding

- **Component map**: Replay Buffer (with ER Decay) -> Critic Network (with Residual Blocks) -> Scheduler (manages Reset List and Expansion List) -> OBAC Core

- **Critical path**: 
  1. Initialize Critic with depth=2 (small)
  2. Train with Replay Ratio=10 using ER Decay sampling
  3. Trigger Reset: Re-initialize network weights (depth returns to 2)
  4. Trigger Expansion: At 50k/200k steps post-reset, add a block
  5. Decay learning rate proportional to parameter increase

- **Design tradeoffs**:
  - Depth vs. Stability: Starting small and growing mitigates loss explosions under high replay ratios
  - ε (Decay Rate) vs. Sample Efficiency: Aggressive decay helps primacy bias but may hurt performance if old data was useful

- **Failure signatures**:
  - Loss Explosion: Occurs if large network is trained with high replay ratios without Forget-and-Grow schedule
  - Stagnation: Occurs if expansion is disabled; agent cannot improve on complex tasks despite scaling parameters

- **First 3 experiments**:
  1. Ablation on Sampling: Run FoG on HalfCheetah comparing Uniform Sampling vs. PER vs. ER Decay
  2. Plasticity Test: Run FoG on Humanoid-Walk; plot "Dormant Ratio" over time
  3. Scaling Limit: Train FoG with max parameters (23M) vs. BRO on Dog-Trot to verify continued scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the formal theoretical convergence guarantees for the Forget-and-Grow (FoG) strategy?
- Basis: The conclusion states that "future works include seeking theoretical guarantees for the FoG strategies," acknowledging current support is primarily empirical
- Why unresolved: Paper provides proofs on sample count bounds but not on convergence to optimal policy or sample complexity characterization
- What evidence would resolve it: A formal proof showing FoG update rules converge to local/global optimum under standard RL assumptions

### Open Question 2
- Question: Why does FoG fail to improve performance when integrated with the BRO algorithm, and can this incompatibility be resolved?
- Basis: Appendix B.4 notes "FoG-BRO yielded performance very similar to that of the original BRO"
- Why unresolved: Authors observe conflict but don't isolate which specific component of BRO neutralizes FoG's benefits
- What evidence would resolve it: Ablation study identifying the specific architectural/optimization component in BRO causing conflict

### Open Question 3
- Question: How does the synchronization between ER Decay and Network Expansion specifically mitigate "primacy bias" in the gradient descent process?
- Basis: Paper admits to "lacking a thorough and in-depth investigation into their mechanisms" despite proposing intuitive theoretical insights
- Why unresolved: Paper posits mechanisms but doesn't detail mathematical interaction of how expanded parameters adapt to shifting data distribution
- What evidence would resolve it: Study of gradient landscape or Neural Tangent Kernel showing how expanded parameters adapt to shifting data distribution

## Limitations
- Relies heavily on empirical comparisons; theoretical convergence guarantees are not established
- Performance depends on specific hyperparameters (decay rate ε, reset schedule) that may not generalize
- Neurobiological inspiration from infantile amnesia is presented more as motivation than rigorous mapping

## Confidence

- **High Confidence**: Basic mechanism of ER decay reducing early-data over-sampling and its mathematical justification; empirical performance gains over baselines
- **Medium Confidence**: Claim that network expansion specifically addresses plasticity loss versus simply providing more capacity; optimal schedule for resets and expansions
- **Low Confidence**: Direct translation of neurobiological concepts to RL context; mechanism of synchronization between ER Decay and Network Expansion

## Next Checks

1. **Transfer Test**: Apply FoG to a non-control domain (e.g., Atari or sparse-reward gridworld) to verify primacy bias mitigation generalizes beyond continuous control

2. **Reset-Only Baseline**: Run an ablation where FoG performs resets with uniform sampling (no ER decay) to isolate the benefit of forgetting from network resets

3. **Single-Shot Learning**: Create a task where optimal policy requires a single key experience (e.g., finding goal location once) to test whether ER decay's forgetting harms this scenario