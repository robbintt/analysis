---
ver: rpa2
title: 'FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained
  Wildfire Spread Forecasting'
arxiv_id: '2512.03369'
source_url: https://arxiv.org/abs/2512.03369
tags:
- fire
- video
- mask
- prediction
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FireSentry, a high-resolution multi-modal
  wildfire dataset featuring sub-meter spatial and sub-second temporal granularity
  captured via UAV platforms. It provides synchronized visible/infrared video streams,
  environmental telemetry, and validated fire masks.
---

# FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting

## Quick Facts
- arXiv ID: 2512.03369
- Source URL: https://arxiv.org/abs/2512.03369
- Reference count: 40
- Primary result: Multi-modal UAV wildfire dataset with diffusion-based forecasting approach

## Executive Summary
This paper introduces FireSentry, a high-resolution multi-modal wildfire dataset featuring sub-meter spatial and sub-second temporal granularity captured via UAV platforms. It provides synchronized visible/infrared video streams, environmental telemetry, and validated fire masks. The authors propose FiReDiff, a generative diffusion-based paradigm that predicts future infrared videos and then segments fire masks, diverging from conventional mask-only approaches.

## Method Summary
The authors present FireSentry, a multi-modal dataset for fine-grained wildfire spread forecasting, collected via UAV platforms with sub-meter spatial and sub-second temporal resolution. The dataset includes synchronized visible/infrared video streams, environmental telemetry, and validated fire masks. They propose FiReDiff, a generative diffusion-based approach that predicts future infrared videos and segments fire masks from these predictions, contrasting with traditional mask-only forecasting methods. Extensive experiments demonstrate FiReDiff's superior performance over physics-based, data-driven, and generative models across multiple metrics including PSNR, SSIM, LPIPS, FVD for video quality, and AUPRC, F1, IoU, MSE for mask accuracy. Ablation studies highlight the critical role of environmental modalities in model performance.

## Key Results
- FiReDiff achieves 39.2% PSNR, 36.1% SSIM, 50.0% LPIPS, 29.4% FVD improvements for video quality
- FiReDiff achieves 3.3% AUPRC, 59.1% F1, 42.9% IoU, 62.5% MSE improvements for mask accuracy
- Ablation studies confirm critical role of environmental modalities in model performance

## Why This Works (Mechanism)
The generative diffusion-based approach enables learning complex spatio-temporal patterns in wildfire dynamics by modeling the full video generation process rather than just predicting final masks. This allows the model to capture intermediate physical states and transitions, leading to more accurate and physically plausible predictions. The multi-modal inputs provide rich contextual information about environmental conditions that directly influence fire behavior.

## Foundational Learning
- **Diffusion Models**: Why needed - to generate realistic future video frames by learning the reverse diffusion process; Quick check - validate with standard denoising benchmarks
- **Spatio-temporal Modeling**: Why needed - to capture both spatial fire spread patterns and temporal dynamics; Quick check - test with synthetic video sequences
- **Multi-modal Fusion**: Why needed - to integrate visual, thermal, and environmental data for comprehensive fire understanding; Quick check - verify sensor synchronization and alignment
- **Segmentation Post-processing**: Why needed - to convert predicted video frames into actionable fire masks; Quick check - validate with ground truth masks

## Architecture Onboarding
- **Component Map**: Input Video -> Diffusion Model -> Predicted Video -> Segmentation Network -> Fire Masks
- **Critical Path**: The diffusion-based video prediction stage is critical as it generates the temporal context needed for accurate mask segmentation
- **Design Tradeoffs**: Generative approach vs. direct mask prediction - generative models provide richer temporal context but require more computational resources
- **Failure Signatures**: Over-smoothed predictions in diffusion model lead to imprecise fire boundaries; missing environmental features reduce physical plausibility
- **3 First Experiments**: 1) Validate diffusion model on synthetic fire sequences, 2) Test segmentation accuracy with perfect input videos, 3) Evaluate multi-modal fusion with controlled environmental variations

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset limited to small experimental areas due to UAV platform constraints
- Controlled burn conditions may not capture full complexity of real wildfires
- Environmental telemetry may introduce sensor noise and synchronization challenges

## Confidence
- Dataset specifications and baseline metrics: High
- Superiority of generative diffusion approach: Medium
- Scalability and real-world applicability: Low

## Next Checks
1. Test FiReDiff on real-world wildfire data beyond controlled burns to assess practical utility
2. Evaluate model performance across diverse terrain types and vegetation conditions to establish generalization capabilities
3. Conduct ablation studies on environmental telemetry components to quantify their actual contribution versus computational overhead