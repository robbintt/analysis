---
ver: rpa2
title: 'BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute'
arxiv_id: '2506.22716'
source_url: https://arxiv.org/abs/2506.22716
tags:
- cost
- response
- routing
- arxiv
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently deploying large
  language models (LLMs) at scale by proposing a novel query routing framework that
  dynamically selects models and allocates computational resources based on query
  difficulty and quality requirements. The core method, BEST-Route, combines a cost-efficient
  multi-headed router that predicts query difficulty with a test-time optimal compute
  strategy using best-of-n sampling to enhance small-model performance.
---

# BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute

## Quick Facts
- **arXiv ID:** 2506.22716
- **Source URL:** https://arxiv.org/abs/2506.22716
- **Reference count:** 24
- **Primary result:** Achieves up to 60% cost reduction with less than 1% performance drop versus always using reference model

## Executive Summary
This paper introduces BEST-Route, a novel query routing framework that dynamically selects both the optimal LLM and the number of responses to generate for each query, achieving significant cost savings while maintaining response quality. The system combines a multi-headed router that predicts query difficulty with a test-time optimal compute strategy using best-of-n sampling to enhance small-model performance. Experiments on large-scale real-world datasets demonstrate that BEST-Route can reduce inference costs by up to 60% while preserving nearly identical quality to using the most powerful reference model (GPT-4o).

## Method Summary
BEST-Route operates by first predicting which combination of model and response count will match the reference model's quality for each incoming query. A multi-headed router (DeBERTa-v3-small backbone with K×N heads) predicts match probabilities for all model/n combinations. The system then filters by a quality threshold, estimates costs using average output lengths, and selects the lowest-cost valid option. For small models, best-of-n sampling generates multiple responses and uses a proxy reward model (DeBERTa-v3-large) to select the highest-quality response. The entire pipeline is trained end-to-end on a 10K instruction dataset with 20 responses per query scored by armoRM.

## Key Results
- Achieves up to 60% cost reduction compared to always using reference model
- Maintains less than 1% quality drop on large-scale real-world datasets
- Successfully routes 70-80% of queries to smaller models while preserving quality
- Demonstrates consistent improvement across question answering, coding, and safety domains

## Why This Works (Mechanism)

### Mechanism 1: Best-of-n Sampling with Proxy Reward Model
The proxy reward model learns to score candidate responses generated by smaller models, selecting the best among n candidates to match larger model quality. During inference, it scores n candidates and returns the highest-scoring response, maintaining quality while remaining cheaper than a single large-model response.

### Mechanism 2: Multi-Head Router for Match Probability Prediction
A shared encoder with lightweight heads efficiently predicts whether (model, n) combinations will match reference model quality. The router selects the lowest-cost combination above threshold t, enabling adaptive routing based on query difficulty.

### Mechanism 3: Threshold-Based Cost-Quality Control
A single threshold parameter enables tunable tradeoffs between cost savings and quality preservation. The system filters valid options by predicted match probability, estimates costs, and selects minimum-cost valid option, falling back to reference model if none meet threshold.

## Foundational Learning

- **Concept: Best-of-n Sampling**
  - Why needed here: Core strategy for boosting small model quality without exceeding large model cost
  - Quick check question: Why does increasing n help more for harder queries than simply using a larger model?

- **Concept: Pairwise Ranking Loss**
  - Why needed here: Proxy RM learns relative quality ordering, not absolute scores
  - Quick check question: Why train on (worst, median), (median, best) pairs instead of all C(n,2) pairs?

- **Concept: Shared-Backbone Multi-Task Learning**
  - Why needed here: Router must predict K×N probabilities efficiently without deploying K×N separate models
  - Quick check question: What is the memory and latency advantage of sharing a backbone across all (model, n) heads?

## Architecture Onboarding

- **Component map:**
  - Multi-Head Router (DeBERTa-v3-small, 44M) -> Predicts match probabilities for all (model, n)
  - Proxy Reward Model (DeBERTa-v3-large, 300M) -> Scores candidate responses
  - Model Pool: K small models + 1 reference model (e.g., GPT-4o)
  - Cost Estimator: avg_output_length × token_price

- **Critical path:**
  1. Query → Router predicts p_k,n for all combinations
  2. Filter by threshold → compute costs for valid options
  3. Select min-cost valid (model, n); fallback to M_ref if none
  4. Generate n responses → Proxy RM scores → return best

- **Design tradeoffs:**
  - Router size vs. prediction accuracy (44M backbone is fast but may underfit complex queries)
  - Max n vs. latency (higher n improves quality but increases generation time)
  - Threshold t vs. cost savings (higher t preserves quality, reduces savings)
  - Assumption: Cost estimation relies on training-set average output lengths

- **Failure signatures:**
  - Router defaults to M_ref excessively → threshold too high or router undertrained
  - Quality drops unexpectedly → proxy RM misaligned with ground truth
  - Cost savings minimal → router overestimates difficulty
  - Latency spikes → n too large or proxy RM bottleneck

- **First 3 experiments:**
  1. Validate proxy RM: plot armoRM score vs. n (replicate Figure 2) to confirm no reward hacking
  2. Sweep threshold t: measure cost reduction vs. quality drop curve (replicate Table 1 at multiple t values)
  3. OOD test: evaluate on MT-Bench to assess generalization beyond training distribution

## Open Questions the Paper Calls Out

### Open Question 1
How does the BEST-Route framework scale computationally and performance-wise when the model pool expands to hundreds of Large Language Models (LLMs)? The current multi-headed router design requires training K×N classification heads, which may face diminishing returns or training difficulties as the feature space becomes too sparse or complex when K grows to 100+.

### Open Question 2
To what extent does potential misalignment between the proxy reward model and ground-truth human preferences lead to suboptimal routing or reward hacking? The system relies on a proxy to approximate expensive ground-truth scores, and if the proxy prioritizes specific linguistic features over semantic correctness, the "best-of-n" selection might pick responses that score well artificially but fail user satisfaction.

### Open Question 3
How does the static estimation of output token length impact routing efficiency when query responses exhibit high variance in verbosity? The router estimates costs using average training data lengths, but if specific queries trigger much longer responses than historical averages (e.g., complex coding tasks), the cost calculation will underestimate true cost, potentially violating cost-quality trade-off constraints.

## Limitations

- **Reward model misalignment:** Potential misalignment between proxy reward model and ground-truth human preferences may result in suboptimal response selection in certain cases
- **Output length estimation:** Static estimation of output token length using average training data may lead to inaccurate cost calculations for queries with atypical verbosity
- **Scalability concerns:** Effectiveness in handling extremely large model pools (hundreds of LLMs) remains unexplored and may require architectural modifications

## Confidence

- **High confidence:** The mechanism of best-of-n sampling with proxy reward models is well-established in the literature and the experimental setup is clearly specified
- **Medium confidence:** The cost reduction claims (up to 60%) are supported by in-domain experiments, but the robustness to distribution shift is not thoroughly validated
- **Low confidence:** The long-term viability of the routing system given potential reward model misalignment and changing query patterns

## Next Checks

1. **Proxy RM alignment validation:** Generate a validation set of 1,000 queries with 20 responses each from multiple models, score with armoRM, and measure the Spearman correlation between proxy RM rankings and armoRM rankings. A correlation below 0.8 would indicate potential quality degradation in production.

2. **Distribution shift stress test:** Evaluate the full BEST-Route system on MT-Bench or other truly out-of-domain datasets with at least 500 queries. Compare the cost-quality tradeoff curve against in-domain performance to identify any degradation patterns.

3. **Threshold sensitivity analysis:** Systematically sweep the routing threshold t from 0.1 to 0.9 in increments of 0.1 on the validation set. Plot the Pareto frontier of cost reduction vs. quality drop to identify the optimal operating point and understand the tradeoff sensitivity.