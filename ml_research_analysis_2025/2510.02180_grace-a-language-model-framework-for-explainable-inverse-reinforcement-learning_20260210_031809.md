---
ver: rpa2
title: 'GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning'
arxiv_id: '2510.02180'
source_url: https://arxiv.org/abs/2510.02180
tags:
- reward
- state
- grace
- code
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GRACE (Generating Rewards As CodE) is a framework for interpretable\
  \ Inverse Reinforcement Learning that uses Large Language Models within an evolutionary\
  \ search to recover executable, code-based reward functions from expert demonstrations.\
  \ The method achieves high accuracy in reward recovery\u2014reaching 1.0 fitness\
  \ on BabyAI and AndroidWorld benchmarks\u2014while requiring only 8 expert trajectories\
  \ per task, compared to 2000+ for traditional IRL baselines."
---

# GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.02180
- Source URL: https://arxiv.org/abs/2510.02180
- Reference count: 40
- Achieves 1.0 fitness on BabyAI and AndroidWorld benchmarks with only 8 expert trajectories

## Executive Summary
GRACE (Generating Rewards As CodE) is a framework for interpretable Inverse Reinforcement Learning that uses Large Language Models within an evolutionary search to recover executable, code-based reward functions from expert demonstrations. The method achieves high accuracy in reward recovery—reaching 1.0 fitness on BabyAI and AndroidWorld benchmarks—while requiring only 8 expert trajectories per task, compared to 2000+ for traditional IRL baselines. It matches or outperforms PPO with ground-truth rewards and GAIL across diverse domains including BabyAI, MuJoCo, and AndroidWorld.

## Method Summary
GRACE addresses the challenge of Inverse Reinforcement Learning by framing reward recovery as a code generation problem. The framework uses LLMs to generate Python-based reward functions as code strings, which are then evaluated and refined through an evolutionary search process. The evolutionary search explores the space of possible reward functions by generating mutations and recombinations of code strings, evaluating their fitness based on how well they explain expert demonstrations. This approach produces interpretable, executable reward functions that can be inspected and debugged, unlike traditional black-box IRL methods. The modular code library design supports multi-task generalization across different environments.

## Key Results
- Achieves perfect fitness (1.0) on BabyAI and AndroidWorld benchmarks
- Requires only 8 expert trajectories per task versus 2000+ for traditional IRL baselines
- Matches or outperforms PPO with ground-truth rewards and GAIL across BabyAI, MuJoCo, and AndroidWorld domains

## Why This Works (Mechanism)
GRACE leverages LLMs' capability to generate syntactically valid Python code while using evolutionary search to navigate the space of possible reward functions. The framework converts the ill-posed IRL problem into a structured code search task where fitness evaluation guides the search toward reward functions that accurately explain expert behavior. The evolutionary process allows exploration of diverse reward function structures while maintaining code validity, and the interpretable nature of the generated code enables inspection of what the model has learned.

## Foundational Learning

**Evolutionary Search**
- Why needed: To systematically explore the space of possible reward functions and avoid local optima
- Quick check: Verify that mutation and crossover operations maintain syntactic validity of Python code

**Code-based Reward Representation**
- Why needed: Enables human interpretability and debugging of learned reward functions
- Quick check: Confirm that generated code executes without errors and produces valid reward values

**Large Language Model Code Generation**
- Why needed: Provides diverse initial candidates and handles the complexity of reward function structure
- Quick check: Test LLM output for syntactic correctness and basic reward function properties

## Architecture Onboarding

**Component Map**
LLM -> Evolutionary Search -> Reward Function -> Policy Learning

**Critical Path**
Expert demonstrations → LLM code generation → Fitness evaluation → Evolutionary selection → Final reward function → Policy training

**Design Tradeoffs**
Evolutionary search provides robustness to local optima but increases computational cost compared to gradient-based methods. Code-based rewards offer interpretability but may limit expressiveness compared to neural network representations.

**Failure Signatures**
Poor fitness scores indicate insufficient coverage of expert demonstrations in generated rewards. Syntactic errors in generated code suggest LLM generation issues. Policy performance degradation despite high fitness may indicate reward function ambiguity.

**First Experiments**
1. Test LLM code generation with synthetic reward functions to verify basic functionality
2. Evaluate fitness landscape smoothness with small evolutionary search populations
3. Compare policy performance using ground-truth versus generated reward functions

## Open Questions the Paper Calls Out

None

## Limitations
- Evolutionary search introduces computational overhead and depends on initialization randomness
- Performance evaluation limited to controlled environments with relatively simple task complexity
- Reward ambiguity analysis based on a small set of manually defined alternatives

## Confidence
- High confidence in code-based reward recovery performance claims, supported by quantitative results across multiple benchmarks
- Medium confidence in generalization and multi-task capabilities, as these are demonstrated but not extensively analyzed
- Medium confidence in interpretability claims, as the code is human-readable but no user studies validate actual human comprehension

## Next Checks
1. Conduct ablation studies removing the evolutionary search components to isolate the contribution of LLM-generated code versus search optimization
2. Test GRACE on more complex, open-world environments with longer task horizons and larger state/action spaces
3. Perform systematic reward ambiguity analysis using automated generation of diverse alternative reward functions rather than manual variants