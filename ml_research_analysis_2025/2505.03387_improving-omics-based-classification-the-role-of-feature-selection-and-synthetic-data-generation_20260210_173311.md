---
ver: rpa2
title: 'Improving Omics-Based Classification: The Role of Feature Selection and Synthetic
  Data Generation'
arxiv_id: '2505.03387'
source_url: https://arxiv.org/abs/2505.03387
tags:
- data
- classification
- feature
- samples
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of achieving high accuracy and
  interpretability in omics-based classification when sample sizes are small. The
  authors propose a machine learning framework that combines LASSO-based feature selection
  with synthetic data generation using Gaussian noise, followed by kernel support
  vector machine classification.
---

# Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation

## Quick Facts
- arXiv ID: 2505.03387
- Source URL: https://arxiv.org/abs/2505.03387
- Reference count: 11
- Proposed method reduces selected features while maintaining accuracy in lung cancer miRNA classification

## Executive Summary
This study addresses the challenge of achieving high accuracy and interpretability in omics-based classification when sample sizes are small. The authors propose a machine learning framework that combines LASSO-based feature selection with synthetic data generation using Gaussian noise, followed by kernel support vector machine classification. Evaluated on a lung cancer miRNA expression dataset across six binary classification scenarios, the proposed method significantly reduces the number of selected features while maintaining accuracy comparable to a baseline LASSO model. Data augmentation improves model performance, particularly with limited training data. Cross-validation confirms that the framework generalizes well to larger test sets, offering a balanced approach to high accuracy and interpretable decision-making in omics-based disease classification.

## Method Summary
The proposed framework combines LASSO-based feature selection with synthetic data generation using Gaussian noise augmentation, followed by kernel support vector machine (KSVM) classification. The method was evaluated on the E-MTAB-8026 lung cancer miRNA expression dataset, balancing samples to 500 per class (LCa, NTLD, OD, Healthy Controls). Synthetic samples were generated by adding Gaussian noise (std = 10% of feature's std) to create 200 synthetic samples per class. LASSO feature selection used C=0.01 across multiple runs, retaining features with non-zero coefficients in >50% of runs. The KSVM with polynomial kernel was trained on selected features. The experiment used bootstrap analysis with training sizes [10, 25, 100, 150, 250, 300, 350] per class across 100 repetitions, comparing three methods: L1-KSVM with augmentation, L1-KSVM without augmentation, and baseline LASSO.

## Key Results
- The proposed method significantly reduced the number of selected miRNAs compared to baseline LASSO while maintaining comparable accuracy
- Data augmentation improved model performance, especially with limited training data (10-25 samples per class)
- Cross-validation confirmed the framework generalizes well to larger test sets
- The approach achieved a balance between high accuracy and interpretable decision-making in omics-based disease classification

## Why This Works (Mechanism)
The framework works by addressing the small sample size problem in omics data through two complementary approaches: feature selection and data augmentation. LASSO feature selection reduces dimensionality and improves interpretability by identifying the most relevant miRNAs for classification. Synthetic data generation using Gaussian noise augmentation expands the training set, helping the model generalize better and reducing overfitting. The combination of these techniques allows the KSVM to learn more robust decision boundaries with fewer but more informative features, leading to both high accuracy and improved interpretability.

## Foundational Learning
1. **LASSO (Least Absolute Shrinkage and Selection Operator)**: Why needed - to perform feature selection in high-dimensional omics data; Quick check - verify L1 penalty coefficient (C) and feature voting threshold are appropriate for the dataset size.
2. **Gaussian noise augmentation**: Why needed - to expand small training sets and improve model generalization; Quick check - evaluate impact of different noise scales (5%, 10%, 15% of feature std) on performance.
3. **Bootstrap analysis**: Why needed - to assess model stability and variance across different training set sizes; Quick check - monitor feature overlap and accuracy variance across bootstrap iterations.
4. **Kernel SVM with polynomial kernel**: Why needed - to handle non-linear decision boundaries in omics data; Quick check - tune kernel degree and regularization parameters (C, gamma, coef0) for optimal performance.

## Architecture Onboarding
**Component Map**: E-MTAB-8026 data -> Preprocessing (hsa-* miRNAs, balancing) -> Synthetic data generation (Gaussian noise) -> LASSO feature selection -> KSVM classification -> Evaluation (accuracy, confusion matrices)

**Critical Path**: Data preprocessing → Feature selection (LASSO) → Data augmentation (Gaussian noise) → Classification (KSVM) → Evaluation

**Design Tradeoffs**: The framework trades some potential model complexity for improved interpretability through feature selection. The synthetic data generation approach is computationally efficient but may not fully capture biological variability in miRNA expression.

**Failure Signatures**: 
- Overfitting with very small training sets (n=10-25): High training accuracy but low test accuracy, large variance across bootstrap iterations
- Unstable feature selection: Low feature overlap across bootstrap runs
- Ineffective augmentation: Synthetic data degrading performance compared to non-augmented runs

**3 First Experiments**:
1. Evaluate impact of different LASSO C values (0.001, 0.01, 0.1) on feature selection stability and classification accuracy
2. Compare Gaussian noise augmentation with alternative methods (e.g., SMOTE) for small omics datasets
3. Perform hyperparameter tuning for KSVM (kernel degree, C, gamma) to optimize performance across all six classification scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of specific SVM hyperparameters (kernel degree, C, gamma, coef0) may lead to variations in reproducibility
- Unclear number of LASSO simulation runs for feature voting threshold could affect feature selection consistency
- Unspecified cross-validation strategy and random seed settings may impact exact replication of results
- Synthetic data generation using Gaussian noise may not fully capture true biological variability in miRNA expression

## Confidence
- Major claims (accuracy improvement, feature reduction): Medium
- Methodology (LASSO + augmentation + KSVM): High
- Reproducibility (without hyperparameter details): Low
- Generalizability to other omics datasets: Medium

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary SVM kernel degree, C, and gamma to assess impact on accuracy and feature selection stability
2. **Feature selection robustness**: Track feature overlap across multiple bootstrap iterations to quantify stability of selected miRNAs
3. **Synthetic data ablation**: Compare performance with different noise scales (e.g., 5%, 10%, 15% of feature std) and evaluate impact on both accuracy and overfitting