---
ver: rpa2
title: Spatially-Aware Evaluation of Segmentation Uncertainty
arxiv_id: '2506.16589'
source_url: https://arxiv.org/abs/2506.16589
tags:
- uncertainty
- metrics
- boundary
- calibration
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of current uncertainty evaluation
  metrics in medical image segmentation, which treat voxels independently and fail
  to capture spatial context and anatomical structure. This can lead to identical
  scores for qualitatively distinct uncertainty patterns, such as scattered versus
  boundary-aligned uncertainty, obscuring clinically important distinctions.
---

# Spatially-Aware Evaluation of Segmentation Uncertainty

## Quick Facts
- arXiv ID: 2506.16589
- Source URL: https://arxiv.org/abs/2506.16589
- Authors: Tal Zeevi; Eléonore V. Lieffrig; Lawrence H. Staib; John A. Onofrey
- Reference count: 17
- Key outcome: Three spatially-aware metrics (BUC, BA-ECE, SPACE) outperform traditional voxel-wise metrics in discriminating meaningful vs. spurious uncertainty patterns in prostate zonal segmentation, with SPACE achieving 95.83% accuracy.

## Executive Summary
Current uncertainty evaluation metrics in medical image segmentation treat voxels independently, failing to capture spatial context and anatomical structure. This limitation can lead to identical scores for qualitatively distinct uncertainty patterns, such as scattered versus boundary-aligned uncertainty, obscuring clinically important distinctions. The authors propose three spatially-aware metrics that explicitly incorporate boundary information and spatial correlations to address this gap.

The proposed metrics were validated on prostate zonal segmentation data from the Medical Segmentation Decathlon. Results demonstrate that spatially-aware metrics, particularly SPACE, outperform traditional voxel-wise metrics in discriminating between meaningful and spurious uncertainty patterns. These metrics show promise for improving the assessment of segmentation reliability in clinical applications by better capturing uncertainty in critical boundary regions.

## Method Summary
The method introduces three spatially-aware metrics that incorporate boundary information and spatial correlations to evaluate segmentation uncertainty. BUC quantifies whether uncertainty is localized at predicted boundaries by computing the ratio of mean uncertainty within a boundary region to total mean uncertainty. BA-ECE extends calibration error analysis by binning uncertainty scores based on distance to the ground-truth boundary with distance-based weighting. SPACE evaluates local alignment between uncertainty and actual errors using spatial smoothing with Gaussian kernel convolution. The metrics were validated on prostate zonal segmentation data using Monte Carlo dropout to generate uncertainty maps, comparing performance against traditional voxel-wise metrics.

## Key Results
- SPACE achieved 95.83% accuracy in discriminating between meaningful and spurious uncertainty patterns
- BA-ECE showed the highest effect size (1.83) and mean difference (55.8%), representing a 68% effect size increase over standard ECE
- The spatially-aware metrics outperformed traditional voxel-wise metrics (ECE, MCE, AUC-ROC, AUC-PR, PAvPU, AU-ARC, voxel accuracy) in cross-validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating boundary proximity into uncertainty evaluation improves discrimination between clinically meaningful and spurious uncertainty patterns.
- Mechanism: BUC computes the ratio of mean uncertainty within a boundary region R to total mean uncertainty, concentrating evaluation on anatomically critical zones where segmentation ambiguity is clinically expected.
- Core assumption: Uncertainty near anatomical boundaries reflects legitimate ambiguity rather than model failure, whereas scattered uncertainty indicates noise.
- Evidence anchors:
  - [abstract] "spatially aware metrics that incorporate structural and boundary information"
  - [section 3.1] BUC formula: μR_υ / (μR_υ + μR̄_υ), where R is the set of voxels within a chosen distance to the predicted boundary
  - [corpus] Related work on aleatoric uncertainty (arXiv:2507.22418) suggests modeling natural variability among expert annotators at boundaries—provides theoretical grounding but no direct validation of BUC specifically.
- Break condition: If boundary regions are incorrectly defined (e.g., poor boundary extraction), BUC may misclassify legitimate uncertainty as spurious.

### Mechanism 2
- Claim: Spatial smoothing reveals local correspondence between uncertainty and errors that voxel-wise comparisons miss.
- Mechanism: SPACE applies Gaussian kernel convolution Gσ to both uncertainty map U and error map E, then computes mean absolute difference. This captures whether uncertain regions spatially co-locate with actual errors within local neighborhoods.
- Core assumption: Clinically useful uncertainty should exhibit spatial coherence with errors, not just correlate at the voxel level.
- Evidence anchors:
  - [section 3.3] "SPACE = mean(|(Gσ * U) - (Gσ * E)|)"
  - [section 4.4] SPACE achieved 95.83% accuracy, highest among all metrics
  - [corpus] No direct corpus validation of spatial smoothing for uncertainty; related Earth observation segmentation work (arXiv:2510.19586) addresses uncertainty but not spatial metrics specifically.
- Break condition: Choice of σ critically affects sensitivity—too small reverts to voxel-wise behavior; too large over-smooths and loses localization.

### Mechanism 3
- Claim: Distance-based calibration binning focuses evaluation on regions where calibration errors are clinically consequential.
- Mechanism: BA-ECE partitions voxels into K distance bands from ground-truth boundary, computes calibration error per band, and weights by inverse distance. Near-boundary miscalibration is penalized more heavily.
- Core assumption: Calibration quality matters more near anatomical boundaries than in interior regions.
- Evidence anchors:
  - [section 3.2] "wi is a distance-based weight inversely proportional to the average distance of voxels in band bi from the boundary"
  - [section 4.4] BA-ECE showed highest effect size (1.83) and mean difference (55.8%), +68% effect size increase over standard ECE
  - [corpus] Clinical interpretability work (arXiv:2512.07224) emphasizes boundary importance but does not validate BA-ECE directly.
- Break condition: Requires ground-truth boundary for binning—not available at inference time, limiting deployment scenarios.

## Foundational Learning

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: BA-ECE extends ECE; understanding binning and calibration error computation is prerequisite.
  - Quick check question: Given predicted confidences [0.7, 0.8, 0.9] and outcomes [correct, incorrect, correct], how would you compute calibration error in the 0.7-0.9 bin?

- Concept: **Monte Carlo Dropout Uncertainty**
  - Why needed here: Paper uses MC dropout (n=30) to generate uncertainty maps for validation.
  - Quick check question: Why does running multiple forward passes with dropout produce uncertainty estimates? What does variance across passes represent?

- Concept: **Distance Transform for Boundary Proximity**
  - Why needed here: Both BUC and BA-ECE require computing shortest distance from each voxel to boundary.
  - Quick check question: Given a binary segmentation mask, how would you efficiently compute d(x) = shortest distance to boundary for all voxels?

## Architecture Onboarding

- Component map:
  Input: Segmentation prediction + Ground truth (for validation)
  │
  ├── Uncertainty Generation (MC Dropout, n passes)
  │      └── Variance map U
  │
  ├── Boundary Extraction
  │      └── Distance map d(x)
  │
  ├── Metric Computation:
  │      ├── BUC: Boundary region mask → μR_υ / (μR_υ + μR̄_υ)
  │      ├── BA-ECE: Distance binning → weighted calibration error
  │      └── SPACE: Gσ * U, Gσ * E → mean absolute difference
  │
  └── Output: Scalar metric values

- Critical path: Ground-truth boundary extraction → Distance transform → BUC/BA-ECE computation. For SPACE: Gaussian smoothing kernel selection is the critical hyperparameter.

- Design tradeoffs:
  - BUC uses *predicted* boundary; BA-ECE uses *ground-truth* boundary—BUC is deployment-ready, BA-ECE is validation-only.
  - SPACE's σ controls locality vs. noise tolerance—no single optimal value across anatomical scales.
  - Distance band granularity (K) in BA-ECE: more bands increase precision but reduce samples per band.

- Failure signatures:
  - BUC near 0.5 with scattered uncertainty → boundary region poorly defined or model has no boundary awareness.
  - BA-ECE undefined → distance bands empty due to very small structures or incorrect mask.
  - SPACE identical to voxel-wise ECE → σ too small relative to voxel spacing.

- First 3 experiments:
  1. **Sanity check**: Generate synthetic uncertainty maps (pure noise vs. boundary-aligned) on a single case; verify metrics correctly rank them.
  2. **Hyperparameter sweep**: Vary SPACE σ (e.g., 1, 3, 5, 7 voxels) and BA-ECE K (e.g., 5, 10, 20 bands); plot metric stability vs. discrimination power.
  3. **Cross-anatomy validation**: Apply to a different Medical Segmentation Decathlon task (e.g., liver, lung) to assess generalizability beyond prostate—paper explicitly notes this as future work.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed spatially-aware metrics generalize across different anatomical structures, imaging modalities, and clinical datasets beyond prostate zonal segmentation?
- Basis in paper: [explicit] "Further exploration is needed to assess generalizability across different datasets and imaging modalities. Future work may examine these metrics on other anatomical structures."
- Why unresolved: Validation was limited to 36 prostate MRI cases from a single dataset; performance on other organs (e.g., brain, cardiac) or modalities (CT, ultrasound) with different boundary characteristics remains unknown.
- What evidence would resolve it: Systematic evaluation across diverse medical imaging benchmarks showing consistent metric discrimination between clean and noisy uncertainty maps.

### Open Question 2
- Question: Can spatially-aware metrics be integrated into model training as loss functions or regularizers to encourage learning of clinically meaningful uncertainty patterns?
- Basis in paper: [explicit] "Future work may... investigate integrating them into model training to encourage clinically meaningful uncertainty patterns."
- Why unresolved: Current work uses metrics only for post-hoc evaluation; differentiability and optimization behavior when used as training objectives is unexplored.
- What evidence would resolve it: Demonstrating improved boundary-aligned uncertainty in models trained with differentiable approximations of BUC, BA-ECE, or SPACE.

### Open Question 3
- Question: How sensitive are the proposed metrics to their hyperparameters (Gaussian kernel σ for SPACE, distance band count K and weights for BA-ECE, boundary region definition for BUC)?
- Basis in paper: [inferred] Paper uses fixed choices (e.g., "a chosen distance," σ unspecified) without ablation studies on parameter sensitivity or recommendations for different segmentation tasks.
- Why unresolved: Optimal parameter selection may depend on image resolution, anatomical scale, and boundary sharpness, but no guidance is provided.
- What evidence would resolve it: Ablation studies showing metric stability across parameter ranges and task-adaptive selection criteria.

### Open Question 4
- Question: Do spatially-aware metrics provide actionable information for clinical decision-making that improves downstream patient outcomes compared to traditional voxel-wise metrics?
- Basis in paper: [inferred] Paper claims clinical relevance but evaluates only discriminative power between synthetic high/low-quality uncertainty maps, not actual clinical utility in treatment planning or error detection.
- Why unresolved: No clinician evaluation or correlation with clinically meaningful outcomes (e.g., inter-observer variability, treatment margin adequacy) was conducted.
- What evidence would resolve it: User studies with clinicians demonstrating that spatially-aware metrics better predict segmentation failures relevant to clinical workflow, or correlate with expert-assessed boundary ambiguity.

## Limitations
- Validation limited to prostate zonal segmentation from a single dataset, restricting generalizability to other anatomical structures and imaging modalities
- Several hyperparameters lack specification, including dropout rate for low-quality maps, distance band count for BA-ECE, and Gaussian kernel σ for SPACE
- BA-ECE requires ground-truth boundaries for binning, making it unsuitable for deployment-time evaluation where only predictions are available

## Confidence
- **High Confidence**: The core insight that voxel-wise uncertainty metrics fail to capture clinically meaningful spatial patterns; the SPACE metric's superior discrimination performance (95.83% accuracy) is well-supported by results.
- **Medium Confidence**: The relative performance of BUC and BA-ECE, as these metrics show strong but context-dependent results (BA-ECE excels in effect size but requires ground truth).
- **Low Confidence**: Generalizability to other anatomies and imaging modalities, as cross-validation beyond prostate was not performed.

## Next Checks
1. **Cross-anatomy validation**: Apply the three metrics to at least two additional Medical Segmentation Decathlon tasks (e.g., liver, lung) to assess performance stability across different organ systems and boundary complexities.
2. **Hyperparameter sensitivity analysis**: Systematically vary SPACE σ (1, 3, 5, 7 voxels) and BA-ECE K (5, 10, 20 bands) to determine optimal settings and metric robustness across parameter ranges.
3. **Synthetic pattern validation**: Generate controlled synthetic uncertainty maps with known patterns (pure noise vs. boundary-aligned) and verify all three metrics correctly discriminate between these qualitatively distinct patterns, establishing baseline behavior.