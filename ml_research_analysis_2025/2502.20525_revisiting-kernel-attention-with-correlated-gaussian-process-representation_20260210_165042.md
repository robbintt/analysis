---
ver: rpa2
title: Revisiting Kernel Attention with Correlated Gaussian Process Representation
arxiv_id: '2502.20525'
source_url: https://arxiv.org/abs/2502.20525
tags:
- attention
- cgpt
- kernel
- sgpa
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Correlated Gaussian Process Transformers (CGPT)
  to address the limitation of existing GP-based transformers that require symmetric
  attention kernels, which reduces model representation capacity. CGPT models self-attention
  units as cross-covariance between two correlated GPs, allowing asymmetric kernels
  while preserving uncertainty quantification.
---

# Revisiting Kernel Attention with Correlated Gaussian Process Representation

## Quick Facts
- arXiv ID: 2502.20525
- Source URL: https://arxiv.org/abs/2502.20525
- Reference count: 31
- Primary result: CGPT outperforms symmetric kernel baselines on CIFAR10/100 and CoLA with better calibration and OOD detection

## Executive Summary
This paper introduces Correlated Gaussian Process Transformers (CGPT), which reformulate self-attention units as cross-covariance between two correlated Gaussian Processes. This allows asymmetric attention kernels while preserving uncertainty quantification through the GP framework. The method addresses a key limitation of existing GP-based transformers that require symmetric kernels, reducing representational capacity. A sparse approximation (SCGPT) is derived for better scalability. Experiments show CGPT and SCGPT outperform state-of-the-art GP-based transformers in accuracy, calibration metrics, and OOD detection across CIFAR10/100 and CoLA datasets.

## Method Summary
CGPT models self-attention as the expectation of a query Gaussian Process conditioned on a key Gaussian Process, where both are correlated transformations of a common canonical GP. This cross-covariance formulation enables asymmetric kernels (Wq ≠ Wk) while maintaining analytical uncertainty quantification. The predictive variance provides uncertainty estimates that are incorporated into training via log-marginal likelihood regularization. For scalability, SCGPT applies deterministic training conditional sparse approximation using inducing points, reducing computational complexity from O(n³) to O(κ³ + n²κ).

## Key Results
- CGPT achieves 93.31% accuracy on CIFAR10 (2.1% absolute improvement over symmetric kernel baseline) and 73.06% on CIFAR100 (1.3% improvement)
- SCGPT demonstrates superior memory efficiency with comparable runtime to sparse GP alternatives while maintaining competitive performance
- Both CGPT and SCGPT show better calibration (lower ECE, MCE, NLL) and OOD detection (higher AUROC, AUPR) than competing methods
- CGPT converges faster than standard Transformer on CoLA, achieving higher MCC (0.611 vs 0.575) with better uncertainty estimates

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Kernel Attention via Correlated GP Cross-Covariance
- Claim: CGPT enables asymmetric attention kernels (Wq ≠ Wk) by modeling attention as cross-covariance between correlated GPs, restoring representational capacity while preserving uncertainty quantification
- Mechanism: Two GPs zk(x) and zq(x) are affine transformations of a common canonical GP zo(x). The attention output is the expectation E[zq|zk], involving cross-covariance matrices κkq(x,x') and κqk(x,x') that are inherently asymmetric due to different projection matrices
- Evidence: Abstract states "models self-attention units as cross-covariance between two correlated GPs... allows asymmetries in attention"; Section 3.3 provides mathematical derivation showing Wq ≠ Wk is possible

### Mechanism 2: Uncertainty Calibration via CGP Predictive Variance and Log-Likelihood Regularization
- Claim: CGP provides analytical predictive variance that can be minimized during training by maximizing log-marginal likelihood, acting as a regularizer
- Mechanism: Predictive variance V[zq|zk] is derived in closed form. A term proportional to log probability log p(zq=νa,zk) is added to the loss, encouraging parameters that produce high-likelihood (low-uncertainty) predictions
- Evidence: Section 3.4.2 shows the regularization term; abstract mentions "preserving uncertainty quantification"

### Mechanism 3: Scalability via Sparse CGP with Inducing Points
- Claim: Applying DTC sparse approximation reduces computational complexity from O(n³) to O(κ³ + n²κ)
- Mechanism: The expectation E[zq|zk] is computed through nested integrals, approximated using inducing points {si} and {s'i} to create low-rank approximations
- Evidence: Section 4 derives the complexity reduction; abstract mentions "sparse approximation (SCGPT) is derived for better scalability"

## Foundational Learning

### Concept: Gaussian Process (GP) & Kernel Functions
- Why needed: The paper frames attention as GP inference; understanding prior, mean, and kernel is essential for CGP derivation
- Quick check: In a standard GP, what two functions fully specify the prior over functions?

### Concept: Self-Attention in Transformers
- Why needed: The method modifies core self-attention unit; understanding Q, K, V matrices and softmax operation is necessary
- Quick check: In the standard formulation V⁺ = softmax((QKᵀ)/√d)V, what does the product QKᵀ represent?

### Concept: Bayesian Regularization & Uncertainty Quantification
- Why needed: The key contribution uses GP framework to calibrate uncertainty; understanding predictive variance and log-likelihood regularization is crucial
- Quick check: If a model's predictions have low uncertainty (predictive variance) but are incorrect, what is this an example of?

## Architecture Onboarding

### Component map:
Input X → (Linear Projs: Wk, Wq, Wv) → K, Q, V → **CGP Attention Unit** (computes V⁺ as predictive mean E[zq|zk] and accumulates uncertainty term U) → Feed-Forward → Output

### Critical path:
1. Forward pass computes attention outputs V⁺ using CGP-based kernel attention
2. During forward pass, uncertainty/regularization term U is computed per attention block
3. Final classification output is produced
4. Total loss L(θ) = loss(νa) - α·U is computed
5. Backpropagation updates all parameters

### Design tradeoffs:
- **CGPT vs. SCGPT**: CGPT is exact but scales as O(n³); SCGPT scales as O(κ³ + n²κ) but introduces approximation error
- **Asymmetry vs. Complexity**: Standard GP attention is symmetric (tied Wk, Wq) but simpler; CGP unties them, increasing capacity and derivation complexity
- **Regularization Strength (α)**: High α strongly penalizes uncertainty but may hurt task performance; low α may not calibrate well

### Failure signatures:
- High NLL/MCE/ECE on validation: Uncertainty calibration failing; adjust α or check CGP variance computation
- OOM with long sequences: CGPT is full-rank; must switch to SCGPT with appropriate inducing points
- Performance drop vs. standard Transformer: CGP regularizer may be too strong (α too high) or symmetric baseline was poor comparison

### First 3 experiments:
1. **Sanity Check**: Implement CGP-based attention for single layer on small dataset; compare output with standard kernel attention to verify Eq. 23-25 correspondence
2. **Ablation on α**: Train CGPT on CIFAR10 validation split, sweep α values (0.1, 0.5, 1.0); plot accuracy and ECE to find optimal trade-off
3. **Scalability Test**: Compare training memory and time of CGPT vs. SCGPT on CIFAR10 as sequence length n increases (n=32, 64, 128); confirm O(n³) vs. O(n²κ) scaling behavior

## Open Questions the Paper Calls Out
- Improving efficiency using random features or sparse GP approximations beyond current inducing point method
- Applying CGPT framework to large-scale pre-training tasks and modern LLMs
- Analyzing theoretical impact of optimizing lower bound vs. exact log marginal likelihood on uncertainty quality
- Incorporating complex or non-stationary kernel functions beyond squared exponential kernel.

## Limitations
- Performance gains demonstrated against symmetric kernel baselines rather than current state-of-the-art vision transformers
- Sparse approximation with 16 inducing points validated but trade-off between inducing points count and performance unexplored
- Experiments limited to ViT architectures with 5-6 layers; results on deeper architectures or decoder-only models unknown

## Confidence

**High Confidence**: Mathematical derivation of CGP-based attention (Eq. 25) is correct and follows from standard GP conditioning formulas; uncertainty calibration mechanism is theoretically sound

**Medium Confidence**: Empirical performance gains are significant but could be partially attributed to improved architecture choice (asymmetric kernels) rather than CGP framework itself

**Medium Confidence**: Scalability claims are mathematically correct but practical impact depends on implementation details not fully specified

## Next Checks
1. **Architecture Ablation**: Compare CGPT against asymmetric kernel attention without CGP uncertainty regularization to isolate contribution of uncertainty calibration vs. architectural changes
2. **Deeper Model Evaluation**: Test CGPT/SCGPT on ViT models with 12+ layers to verify scalability and performance consistency in deeper architectures
3. **Overestimation Analysis**: Evaluate calibration metrics (ECE, MCE) on clean validation data to determine if CGPT systematically underestimates uncertainty on easy examples or maintains well-calibrated predictions across difficulty levels