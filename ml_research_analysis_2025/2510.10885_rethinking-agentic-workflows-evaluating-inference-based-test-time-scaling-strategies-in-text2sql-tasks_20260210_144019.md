---
ver: rpa2
title: 'Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling
  Strategies in Text2SQL Tasks'
arxiv_id: '2510.10885'
source_url: https://arxiv.org/abs/2510.10885
tags:
- reasoning
- workflow
- workflows
- scaling
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks six lightweight, industry-oriented test-time
  scaling strategies and four large language models (two general-purpose and two reasoning-focused)
  on the BIRD Mini-Dev Text-to-SQL benchmark. The evaluation measures SQL accuracy
  (Soft-F1 score), inference latency, and token consumption.
---

# Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks

## Quick Facts
- arXiv ID: 2510.10885
- Source URL: https://arxiv.org/abs/2510.10885
- Reference count: 7
- Primary result: Divide-and-Conquer instructions with few-shot demonstrations consistently improve Text2SQL accuracy across both general-purpose and reasoning-focused models.

## Executive Summary
This study benchmarks six lightweight, industry-oriented test-time scaling strategies and four large language models (two general-purpose and two reasoning-focused) on the BIRD Mini-Dev Text-to-SQL benchmark. The evaluation measures SQL accuracy (Soft-F1 score), inference latency, and token consumption. Results show that Divide-and-Conquer instructions combined with few-shot demonstrations consistently improve performance across all models, with reasoning models also benefiting from explicit procedural guidance. Adding a result verification step provides the most reliable accuracy gains among more complex workflows, while retrieval-based methods often underperform. Gemini Flash models achieve significantly lower latency (5.02-12.03s) compared to GPT-4o and o4-mini (15.70-18.43s). The findings highlight that base model selection is critical, and that balancing accuracy with efficiency requires careful workflow design and potentially product-level user experience considerations.

## Method Summary
The study evaluates six inference-based agentic workflows (baseline, Divide-and-Conquer with 3-shot, Divide-and-Conquer with 5-shot, Parallel Scaling, Retrieval-Enhanced, and Result Verification) across four LLMs (Gemini 1.5/2.5 Flash, GPT-4o, o4-mini) on the BIRD Mini-Dev benchmark (500 samples). The core method involves a ReAct loop (SQL Writer > Executor <> SQL Refiner) enhanced by different test-time scaling strategies. Each workflow is measured on Soft-F1 score, Execution Accuracy, inference time, and token count. The Divide-and-Conquer approach decomposes complex queries into sequential subproblems with few-shot demonstrations, while the verification workflow adds a Feedback Provider agent to catch semantically incorrect but syntactically valid SQL.

## Key Results
- Divide-and-Conquer instructions with few-shot demonstrations consistently improve performance across all tested models, with GPT-4o improving from 61.1 to 64.4 Soft-F1 and o4-mini from 56.3 to 65.5.
- Result verification provides the most consistent accuracy gains among complex workflows, increasing Soft-F1 for Gemini 1.5 Flash from 62.58 to 63.63 and GPT-4o from 64.44 to 64.95.
- Retrieval-based schema filtering generally harms performance by depriving models of essential information needed for complex reasoning, increasing Schema Linking Errors for Gemini 1.5 Flash from 14.9% to 16.4% and o4-mini from 10.4% to 12.2%.
- Gemini Flash models achieve significantly lower latency (5.02-12.03s) compared to GPT-4o and o4-mini (15.70-18.43s), with incorrect answers taking 19.58% longer than correct ones.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Divide-and-Conquer instructions with few-shot demonstrations improve Text2SQL accuracy across both general-purpose and reasoning-focused models.
- Mechanism: Decomposing complex SQL generation into sequential subproblems (schema linking → SQL generation → refinement) combined with concrete examples reduces cognitive load on the model, providing explicit procedural scaffolding that guides reasoning even for models already fine-tuned for reasoning tasks.
- Core assumption: The benefit stems from reducing problem complexity per inference step rather than simply increasing total compute.
- Evidence anchors:
  - [abstract] "Divide-and-Conquer prompting and few-shot demonstrations consistently enhance performance for both general-purpose and reasoning-focused LLMs."
  - [section 3/RQ1] "GPT-4o saw its Soft-F1 increase from 61.1 in the baseline to 64.4. Similarly, the reasoning model o4-mini improved from a baseline of 56.3 to 65.5."
  - [corpus] Related work on test-time scaling confirms decomposition benefits, though specific Text2SQL evidence in neighbors is limited.
- Break condition: Expected to diminish when questions are simple single-table queries where decomposition adds overhead without benefit, or when few-shot examples poorly match the target schema structure.

### Mechanism 2
- Claim: Result verification provides the most consistent accuracy gains among complex workflow additions by catching semantically incorrect but syntactically valid SQL.
- Mechanism: A Feedback Provider agent receives execution output and evaluates semantic correctness, triggering refinement when results don't match question intent—addressing the gap between executable queries and correct answers.
- Core assumption: The verification agent can reliably detect semantic mismatches without ground-truth reference.
- Evidence anchors:
  - [section 3/RQ2] "Verification method proved most effective... It increased the Soft-F1 score for Gemini 1.5 Flash (from 62.58 to 63.63), Gemini 2.5 Flash (68.12 to 68.44), and GPT-4o (64.44 to 64.95)."
  - [section 2.1] "This workflow aims to handle the situation where the generated SQL query is syntactically correct but semantically incorrect."
  - [corpus] "On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows" suggests feedback mechanisms are critical for agentic success, supporting this pattern.
- Break condition: Expected to fail when verification agent lacks domain knowledge to judge semantic correctness, or when multiple valid SQL interpretations exist for ambiguous questions.

### Mechanism 3
- Claim: Retrieval-based schema filtering harms performance when correct column selection requires reasoning beyond semantic similarity.
- Mechanism: LSH-based entity retrieval and semantic column filtering can exclude relevant tables/columns that require multi-hop reasoning to identify, depriving the model of information needed for correct query construction and increasing Schema Linking Errors.
- Core assumption: The retrieval mechanism's similarity matching approximates the reasoning needed for column selection.
- Evidence anchors:
  - [section 3/RQ2] "The retrieval-enhanced method was generally counterproductive... if identifying the correct columns requires complex reasoning rather than simple similarity matching, the model may be deprived of essential information."
  - [section 3/RQ2/Figure 3] "Retrieval method also increased the rate of Schema Linking Errors for Gemini 1.5 Flash (from 14.9% to 16.4%) and o4-mini (from 10.4% to 12.2%)."
  - [corpus] No direct corpus contradiction or support found; retrieval-augmentation evidence in neighbors focuses on RAG broadly, not schema filtering specifically.
- Break condition: Expected to work better on databases with clear semantic correspondence between question terms and column names, where similarity matching suffices.

## Foundational Learning

- Concept: Test-time scaling strategies
  - Why needed here: The paper evaluates methods that increase inference compute (parallel execution, verification, decomposition) rather than training—understanding this paradigm is essential for interpreting results.
  - Quick check question: Can you explain why test-time scaling might help reasoning models that were already trained with similar techniques?

- Concept: ReAct agent pattern (Reasoning + Acting)
  - Why needed here: The baseline workflow uses iterative "think, act, observe" loops for SQL refinement; understanding this pattern is prerequisite to evaluating enhancements.
  - Quick check question: How does the ReAct loop handle execution errors in Text2SQL workflows?

- Concept: Execution Accuracy vs. Soft-F1 metrics
  - Why needed here: The paper uses Soft-F1 as primary metric because it's less strict than Execution Accuracy—metric choice affects conclusions about which methods "work."
  - Quick check question: Why might Soft-F1 be more appropriate than exact execution match for evaluating Text2SQL systems?

## Architecture Onboarding

- Component map: SW (SQL Writer) -> EX (Executor) -> SR (SQL Refiner), with optional FP (Feedback Provider) for verification, and KE/ER/CR (Keyword Extraction/Entity Retrieval/Column Retrieval) for retrieval-based workflows

- Critical path: For production deployment, the DC 3-shot+ReAct workflow offers best accuracy-efficiency balance. Add Verification only if latency budget permits (adds ~5-10s). Avoid retrieval-based approaches unless schema is simple and well-named.

- Design tradeoffs:
  - Accuracy vs. Latency: Incorrect answers are 19.58% slower than correct ones—complex workflows extend wait times without guaranteeing correctness
  - Model selection vs. Workflow complexity: Gemini 2.5 Flash baseline outperforms GPT-4o's best workflow, suggesting model choice matters more than workflow sophistication
  - Parallel scaling: 5x compute for ~1-2 point Soft-F1 gain, inconsistent across models

- Failure signatures:
  - High Schema Linking Errors: Indicates model struggles to map question terms to database schema—consider DC prompting
  - Wrong Query Logic (most common): Procedural decomposition helps but retrieval-based filtering may worsen this
  - Execution errors after refinement loop: Suggests fundamental misunderstanding of schema or question intent

- First 3 experiments:
  1. Establish baseline with CoT+ReAct on your schema using Gemini 2.5 Flash—measure Soft-F1, latency, and error types.
  2. Add DC 3-shot prompting to same setup—quantify accuracy gain and latency increase; if >3 Soft-F1 points, proceed.
  3. Add Result Verification only if accuracy remains below target—measure whether FP agent catches logic errors your domain experts identify as critical.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the performance gains from Divide-and-Conquer prompting and Result Verification transfer effectively to semantic parsing tasks beyond Text-to-SQL?
- Basis in paper: [explicit] The conclusion explicitly states future work should explore "the applicability of these strategies to other tasks."
- Why unresolved: This study isolated Text-to-SQL; the dependency on SQL-specific syntax execution for verification leaves generalizability unconfirmed.
- What evidence would resolve it: Benchmarking these specific workflows on non-SQL code generation or generic reasoning datasets (e.g., HumanEval, GSM8k).

### Open Question 2
- Question: Do the efficiency and accuracy trade-offs observed in lightweight workflows persist when evaluated on the full BIRD Dev set or other large-scale benchmarks?
- Basis in paper: [explicit] The authors note evaluation was "confined to the BIRD Mini-Dev benchmark" and future work requires "validifying these findings across a broader range of datasets."
- Why unresolved: The Mini-Dev subset may not capture the full distribution of database complexity or question difficulty found in the complete benchmark.
- What evidence would resolve it: Re-running the experiments on the full BIRD Dev set and cross-domain datasets like Spider to confirm consistency.

### Open Question 3
- Question: Can retrieval-based structured reasoning be redesigned to avoid depriving models of necessary schema information during complex reasoning steps?
- Basis in paper: [inferred] The results show retrieval-based methods underperform because "if identifying correct columns requires complex reasoning... the model may be deprived of essential information."
- Why unresolved: The study identified the failure mode (information loss) but did not test alternative retrieval thresholds or hybrid retrieval strategies.
- What evidence would resolve it: Ablation studies on retrieval granularity or hybrid retrieval strategies that allow full schema access during the generation phase.

## Limitations
- The study is limited to a single simplified Text2SQL benchmark (BIRD Mini-Dev with 500 samples) which may not generalize to more complex schemas or multi-turn dialogues.
- Specific prompt templates for Divide-and-Conquer and exact few-shot demonstrations are not provided, making exact reproduction challenging.
- The "Executor" component is treated as a black box, so execution or result interpretation errors could confound accuracy measurements.

## Confidence
**High Confidence**: Divide-and-Conquer instructions with few-shot demonstrations consistently improve performance across all tested models; result verification provides most reliable accuracy gains; Gemini Flash latency significantly outperforms GPT-4o/o4-mini.

**Medium Confidence**: Retrieval-based schema filtering harms performance (based on limited evidence); base model selection is more critical than workflow sophistication (requires broader coverage); incorrect answers taking 19.58% longer (needs replication).

**Low Confidence**: Generalization about balancing accuracy with efficiency requiring careful workflow design (too broad for narrow experimental scope); specific claims about which error types each workflow addresses most effectively (based on aggregated data without per-error-type analysis).

## Next Checks
1. **Benchmark Generalization Test**: Run the same six workflows on the full Spider benchmark to verify whether Divide-and-Conquer benefits hold for more complex schemas and whether retrieval-based approaches perform differently on open-domain questions.

2. **Feedback Provider Validation**: Implement a controlled experiment where ground-truth semantic correctness is available to evaluate the Feedback Provider's actual accuracy in detecting errors, rather than assuming it works based on downstream Soft-F1 improvements.

3. **Cost-Effectiveness Analysis**: Calculate the cost per correct answer for each workflow (tokens × API price × attempts until correct) to determine whether accuracy gains justify increased compute costs, particularly for Parallel Scaling and Result Verification workflows.