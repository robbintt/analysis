---
ver: rpa2
title: Reinforcement Learning for Durable Algorithmic Recourse
arxiv_id: '2509.22102'
source_url: https://arxiv.org/abs/2509.22102
tags:
- recourse
- candidates
- time
- recommendations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of algorithmic recourse in competitive,
  resource-constrained environments, where recommendations can become invalid over
  time due to shifting applicant pools. The authors introduce a reinforcement learning-based
  framework that models the dynamic feedback between recommendations and candidate
  behavior.
---

# Reinforcement Learning for Durable Algorithmic Recourse

## Quick Facts
- arXiv ID: 2509.22102
- Source URL: https://arxiv.org/abs/2509.22102
- Authors: Marina Ceccon; Alessandro Fabris; Goran Radanović; Asia J. Biega; Gian Antonio Susto
- Reference count: 40
- Primary result: Reinforcement learning framework achieves superior balance between feasibility and long-term validity of algorithmic recourse recommendations in competitive environments

## Executive Summary
This paper addresses algorithmic recourse in competitive, resource-constrained settings where recommendations become invalid due to shifting applicant pools. The authors introduce a reinforcement learning framework that explicitly models dynamic feedback between recommendations and candidate behavior over a predefined validity horizon T. Their approach incorporates adaptive estimation of feature modification difficulties and employs a hierarchical decomposition separating goal selection from counterfactual generation, enabling recommendations that remain valid despite delayed reapplication and evolving competition.

## Method Summary
The method uses a hierarchical reinforcement learning approach with two policies: a predictor policy μ that recommends target scores based on competitive dynamics, and a recourse recommender policy φ that generates feature modifications to reach those targets. Both policies are trained using Soft Actor-Critic (SAC) in POMDP environments. The system maintains online estimates of feature modification difficulties that are updated based on observed success/failure outcomes. The framework explicitly accounts for temporal validity by designing recommendations to remain valid over a predefined horizon T, allowing candidates to delay reapplication while maintaining recourse guarantees.

## Key Results
- Outperforms existing baselines substantially in synthetic simulation environments
- Achieves superior balance between feasibility and long-term validity of recommendations
- Hierarchical decomposition improves computational tractability in high-dimensional action spaces
- Validity horizon T enables robust recommendations against shifting competition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition of the recourse problem into separate goal-selection and counterfactual-generation policies improves computational tractability
- Mechanism: The predictor policy (μ) learns adaptive target scores based on competitive dynamics, while the recourse recommender policy (φ) learns how to modify features to reach any given score. Pre-training φ reduces the action space for μ to a single scalar (target score) per candidate, making the overall optimization feasible.
- Core assumption: The optimal target score and optimal path to reach it can be decoupled and learned independently without significant loss of optimality
- Evidence anchors:
  - [abstract] "authors introduce a reinforcement learning-based framework that models the dynamic feedback between recommendations and candidate behavior"
  - [section 4] "Directly learning the counterfactual matrix XCF_t is computationally expensive due to its high dimensionality and variable size. To address this, we adopt a hierarchical approach that separates counterfactual generation from goal selection"
  - [corpus] Corpus neighbors address recourse but none employ this specific hierarchical decomposition
- Break condition: If optimal target scores depend critically on the specific paths available (e.g., when candidates have heterogeneous feature constraints), the decomposition may yield suboptimal joint solutions

### Mechanism 2
- Claim: Online estimation of feature modification difficulties enables recommendations that prioritize easier-to-change features, improving feasibility
- Mechanism: The system maintains difficulty estimates (d̂_i) for each feature, updated via error signals from observed success/failure outcomes. The recommender's cost function penalizes large changes weighted by these difficulties, steering recommendations toward more attainable modifications.
- Core assumption: Feature modification difficulties are quasi-stationary and can be estimated from limited observations
- Evidence anchors:
  - [abstract] "Their method explicitly accounts for feature modification difficulties"
  - [section 4.1] "Difficulty estimates are learned adaptively... the agent's estimate of the difficulty of modifying feature i... updated using a decaying learning rate"
  - [corpus] No corpus papers explicitly address adaptive difficulty estimation for recourse
- Break condition: If difficulties vary across individuals or time (e.g., personal financial circumstances), global estimates become unreliable

### Mechanism 3
- Claim: Explicitly modeling a validity horizon T enables recommendations that remain valid despite delayed reapplication and evolving competition
- Mechanism: The predictor learns to recommend target scores that account for anticipated competition over T steps. The reward (RR_T^t) measures reliability over this window, and candidates can delay reapplication up to T steps while retaining validity guarantees.
- Core assumption: Competitive dynamics are sufficiently predictable over T steps for the policy to learn anticipatory strategies
- Evidence anchors:
  - [abstract] "designs recommendations to remain valid over a predefined time horizon T... durability allows individuals to confidently reapply after taking time to implement the suggested changes"
  - [section 5.2] "Guaranteeing recourse over a longer horizon imposes a more stringent requirement, forcing the agent to recommend more challenging feature changes... feasibility must decrease to guarantee large reliability over an increasing time horizon"
  - [corpus] "Optimal Robust Recourse" addresses robustness via adversarial perturbations but not temporal validity horizons
- Break condition: If competitive dynamics are chaotic or exogenous shocks occur, T-step validity guarantees become unreliable

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The environment is partially observable—candidates' feature modifications and dropout decisions remain hidden until they reapply. The agent must reason under uncertainty about unobserved population states.
  - Quick check question: What specific information is hidden from the agent at each time step, and how does the observation function O(o|s',a) formalize this?

- **Concept: Soft Actor-Critic (SAC)**
  - Why needed here: SAC handles continuous action spaces and provides sample-efficient off-policy learning, critical for training both policies in complex stochastic environments.
  - Quick check question: Why does the paper note that SAC lacks theoretical convergence guarantees, and what practical implications does this have?

- **Concept: Pareto Optimization (Feasibility-Validity Tradeoff)**
  - Why needed here: The method explicitly balances multiple competing objectives—high validity (reliable outcomes) requires harder recommendations, reducing feasibility (implementation success).
  - Quick check question: Why can't the system simply maximize validity by recommending extremely high target scores?

## Architecture Onboarding

- **Component map:**
  - Synthetic dataset generation -> Decision model M(·) training -> Recourse recommender policy φ training -> Predictor policy μ training -> Evaluation

- **Critical path:**
  1. Generate synthetic dataset → train M(·)
  2. Train φ in single-candidate POMDP (3,000 episodes error-only, then 20,000 error+cost)
  3. Train μ in multi-candidate POMDP with frozen φ (7,000 timesteps)
  4. Evaluate Pareto fronts across (α, τ) hyperparameter sweeps

- **Design tradeoffs:**
  - Longer T → lower feasibility at fixed validity (stricter requirement)
  - Lower β → sharper validity-feasibility tradeoff (modifications harder)
  - SAC offers no convergence guarantees (Appendix H)
  - Baseline (threshold-based) prioritizes feasibility but achieves RR ≈ 0.4 (Figure 2)

- **Failure signatures:**
  - Recourse Reliability RRT_t < 0.5 → predictor not anticipating competition
  - High Gini index → inequitable score dispersion across candidates
  - Slow convergence for T=5 vs T=1 (Figure 4) → insufficient exploration

- **First 3 experiments:**
  1. Replicate synthetic experiments with T=1, β=0.05; verify Pareto front matches Figure 2a before varying parameters
  2. Ablate adaptive difficulty estimation by fixing d̂_i = 0.5; quantify feasibility degradation vs full method
  3. Stress-test: train μ with β=0.05, evaluate at β=0.01 to assess generalization to harder settings

## Open Questions the Paper Calls Out
None

## Limitations
- Hierarchical decomposition may yield suboptimal solutions when feature constraints interact strongly with competitive dynamics
- Adaptive difficulty estimation assumes quasi-stationary modification costs across candidates
- Validity horizon approach assumes predictable competitive dynamics, vulnerable to exogenous shocks
- Performance gains validated only in synthetic environments, not on real-world datasets

## Confidence
- **High confidence**: The hierarchical approach significantly improves computational tractability and enables training in high-dimensional action spaces
- **Medium confidence**: Adaptive difficulty estimation meaningfully improves recommendation feasibility in practice
- **Medium confidence**: The validity horizon T provides genuine robustness against shifting competition

## Next Checks
1. Evaluate performance when feature modification difficulties vary across individuals (e.g., person-specific d̂_i sampled from distributions)
2. Test robustness to exogenous competition shocks by introducing sudden score distribution shifts mid-training
3. Compare against state-of-the-art robust recourse methods on real-world datasets to validate synthetic performance gains