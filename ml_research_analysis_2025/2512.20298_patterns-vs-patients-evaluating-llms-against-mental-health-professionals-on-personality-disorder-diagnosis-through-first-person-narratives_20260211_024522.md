---
ver: rpa2
title: 'Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals
  on Personality Disorder Diagnosis through First-Person Narratives'
arxiv_id: '2512.20298'
source_url: https://arxiv.org/abs/2512.20298
tags:
- human
- diagnostic
- personality
- mental
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study directly compared state-of-the-art LLMs to mental health
  professionals in diagnosing Borderline and Narcissistic Personality Disorders using
  Polish-language first-person narratives. LLMs achieved an overall diagnostic accuracy
  of 65.48%, surpassing human experts who scored 43.57%.
---

# Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives

## Quick Facts
- arXiv ID: 2512.20298
- Source URL: https://arxiv.org/abs/2512.20298
- Reference count: 30
- Models achieved 65.48% accuracy, surpassing human experts at 43.57%

## Executive Summary
This study directly compared 16 state-of-the-art LLMs to mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders using Polish-language first-person narratives. LLMs achieved 65.48% overall diagnostic accuracy, outperforming human experts who scored 43.57%. While both groups excelled at identifying BPD, LLMs severely underdiagnosed NPD, likely due to the stigmatizing connotations of "narcissism." Models provided confident, elaborate justifications focused on patterns and formal categories, while human experts offered concise, cautious reasoning emphasizing patient self-experience. The findings demonstrate LLMs' competence at interpreting complex first-person clinical data but highlight critical reliability and bias issues requiring collaborative human-AI frameworks.

## Method Summary
The study evaluated 16 LLMs (9 closed-source: Gemini 2.5/3 Pro, Claude Opus 4.1, GPT-4o, GPT-4.1, GPT-5 variants; 7 open-source: Gemma 3 27B, Llama 3.3 70B, DeepSeek R1/v3.1, Qwen 3 32B) against 6 mental health professionals using 7 Polish-language first-person narratives (3 BPD, 3 NPD, 1 Healthy Control) from 50-70 minute autobiographical interviews. Each model completed 3 runs per case (21 trials total), while humans completed 1 run. A 6-step prompt template collected categorical diagnosis, confidence, justification, severity rating, severity confidence, and severity justification. Semantic embeddings (BAAI/bge-multilingual-gemma2) and lexical analysis (ConvoKit) compared justification patterns. Weighted log-odds ratio identified characteristic features, while MDS/UMAP visualized semantic gaps.

## Key Results
- LLMs achieved 65.48% diagnostic accuracy versus 43.57% for human experts
- Models severely underdiagnosed NPD (F1 = 6.7) despite strong BPD performance (F1 = 83.4)
- Models never used the lowest confidence rating (1 = "guessing") while humans used it 19% of the time
- GPT family showed "depathologizing bias," misclassifying 10 cases as healthy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs outperform humans on categorical diagnosis by leveraging pattern-matching across formal diagnostic criteria extracted from training corpora.
- Mechanism: Models map patient narratives onto DSM/ICD category representations learned during pre-training, identifying symptom clusters without requiring temporal or experiential understanding. The paper shows models excel when diagnostic categories align with prevalent training data (BPD: F1 = 83.4).
- Core assumption: Diagnostic accuracy reflects effective pattern recognition rather than clinical reasoning.
- Evidence anchors:
  - [abstract] "models provided confident, elaborate justifications focused on patterns and formal categories"
  - [section 3.2.2] "heavy focus on seeing patterns in patients' testimonies: patterns" identified as model-characteristic lexical feature
  - [corpus] MindEval benchmark notes similar limitations including "sycophancy or overvalidation" in mental health LLM applications
- Break condition: When diagnostic categories carry social stigma conflicting with RLHF alignment (e.g., "narcissist"), models suppress accurate classification despite pattern recognition capability.

### Mechanism 2
- Claim: RLHF alignment creates systematic diagnostic avoidance for stigmatized conditions.
- Mechanism: Preference optimization rewards non-confrontational, agreeable outputs. Assigning value-laden labels like "narcissist" to first-person narrators conflicts with these rewards, causing models to avoid NPD diagnoses (F1 = 6.7) despite demonstrating dimensional sensitivity to severity (recall = 40.97%).
- Core assumption: The term "narcissism" carries sufficient negative semantic load in training corpora to trigger alignment-induced avoidance.
- Evidence anchors:
  - [abstract] "models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term 'narcissism'"
  - [section 4] "assigning a stigmatized label to a first-person narrator—essentially 'calling the user a narcissist'—conflicts with these preferences"
  - [corpus] MindEval confirms "sycophancy" as a documented failure mode in mental health LLM applications
- Break condition: If alignment training explicitly included clinical diagnostic scenarios with appropriate label acceptance, this suppression would likely diminish.

### Mechanism 3
- Claim: Models lack uncertainty calibration appropriate for clinical contexts.
- Mechanism: LLMs never utilized the lowest confidence rating (1 = "guessing") whereas human experts used it in 19% of diagnostic ratings, suggesting models cannot appropriately express epistemic uncertainty even when objective performance warrants caution.
- Core assumption: Verbalized confidence reflects internal metacognitive state rather than generated text patterns.
- Evidence anchors:
  - [abstract] "models provided confident, elaborate justifications... while human experts remained concise and cautious"
  - [section 3.1] "models never utilized the lowest certainty score (1 – 'guessing') whereas human practitioners used it in 19% of diagnostic ratings"
  - [corpus] CounselBench similarly notes gaps in LLM clinical question-answering reliability
- Break condition: When task requires expressing doubt to prevent patient harm, current models would fail to trigger appropriate cautionary responses.

## Foundational Learning

- Concept: **Dimensional vs. Categorical Diagnosis**
  - Why needed here: The study evaluates both frameworks. Categorical assumes discrete diagnostic entities (BPD present/absent); dimensional conceptualizes severity along a continuum. Models showed preference for categorical (71.4% accuracy) over dimensional (66.7%) tasks.
  - Quick check question: Given patient impairment evidence without clear category fit, would your system assign dimensional severity or default to "no disorder"?

- Concept: **RLHF Alignment Bias**
  - Why needed here: Explains why NPD underdiagnosis occurs despite pattern recognition capability. Models prioritize agreeable outputs over clinically accurate stigmatizing labels.
  - Quick check question: Has your alignment training explicitly addressed scenarios where accurate negative labels serve patient welfare?

- Concept: **Metacognitive Calibration**
  - Why needed here: Models over-express confidence relative to objective performance. Clinical safety requires systems that express uncertainty appropriately.
  - Quick check question: Does your architecture include mechanisms for verbalized uncertainty that correlate with actual error rates?

## Architecture Onboarding

- Component map:
  Input layer: First-person narrative text (Polish-language autobiographical accounts, 50–70 minute interviews transcribed) → Diagnostic module: 6-step structured output (categorical diagnosis, confidence, justification, severity, severity confidence, severity justification) → Embedding analysis: BAAI/bge-multilingual-gemma2 for semantic comparison of justifications → Evaluation: MDS/UMAP for semantic gap visualization; weighted log-odds ratio for lexical feature analysis

- Critical path:
  1. Narrative input → diagnostic classification (categorical + dimensional)
  2. Justification generation (pattern-focused vs. patient-centered)
  3. Confidence calibration (currently broken: never expresses lowest confidence)
  4. Bias detection (NPD suppression, BPD overdiagnosis, GPT-family depathologizing)

- Design tradeoffs:
  - Generalist vs. domain-specific models: Study excluded medical fine-tuned models due to context window limitations; general-purpose models performed adequately but with reliability issues
  - Reasoning effort: GPT-5 variants with high reasoning effort showed lowest certainty (M = 2.57) but reasoning did not consistently improve accuracy
  - Sample depth vs. breadth: N = 7 narratives enabled qualitative depth but limits statistical power

- Failure signatures:
  - NPD recall collapse (3.5%) with dimensional recall preserved (40.97%) indicates label-specific suppression, not capability absence
  - GPT-family depathologizing bias: GPT-4o misclassified 10 cases as healthy
  - Semantic outlier models (GPT-4o, Llama 3.3 70B, Gemma 3 27B, Qwen variants) showed both poor performance and atypical justification semantics

- First 3 experiments:
  1. Debiasing probe: Present NPD cases with neutral framing (remove first-person perspective) to isolate alignment-induced label avoidance from pattern recognition capability
  2. Confidence calibration: Force models to express uncertainty on cases where 3-run consistency < 100%, correlating verbalized confidence with actual trial variance
  3. Collaborative framework test: Present model diagnoses to human experts as decision support; measure whether human-AI combination achieves higher validity than either alone (as the paper recommends but does not test)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms drive the "metacognitive dissociation" where agents report higher subjective certainty for dimensional ratings despite achieving higher objective accuracy on categorical tasks?
- Basis in paper: [explicit] Discussion section notes the paradox and suggests "Future studies could investigate the underlying mechanisms of this metacognitive dissociation."
- Why unresolved: The study observed that both humans and models were more confident in severity assessments (dimensional) than diagnosis (categorical), even though they performed better on the latter, but did not explain the cause.
- What evidence would resolve it: Comparative studies measuring the calibration between confidence scores and objective accuracy across different diagnostic task types.

### Open Question 2
- Question: Does requiring models to generate diagnostic reasoning prior to assigning a score improve accuracy for models with weak linguistic fluency?
- Basis in paper: [explicit] Discussion section regarding Llama 3.3 70B asks to "investigate how the performance of this model would change if its diagnostic reasoning were generated before assigning diagnostic scores."
- Why unresolved: The authors found Llama 3.3 70B produced accurate diagnoses but incoherent justifications; it is unclear if the reasoning process is flawed or if the post-hoc generation is the issue.
- What evidence would resolve it: Controlled experiments comparing diagnostic performance when models are prompted to output reasoning before the label versus after.

### Open Question 3
- Question: Do the observed diagnostic biases, such as the underdiagnosis of NPD and the "depathologizing bias" in GPT models, persist across different cultural and linguistic contexts?
- Basis in paper: [explicit] Limitations section states: "Future research should explore whether these findings remain consistent across different cultural settings."
- Why unresolved: The study was limited to Polish-language narratives; biases may be exacerbated or mitigated by English-centric training data or different cultural expressions of personality disorders.
- What evidence would resolve it: Replication of the evaluation protocol using first-person narratives from diverse cultural cohorts and languages.

## Limitations
- Small sample size (N = 7 narratives) limits statistical power and generalizability
- Single language (Polish) and clinical setting may not generalize across cultures
- Closed-label dataset prevents independent verification and reproduction
- Cannot definitively determine whether model alignment biases reflect genuine RLHF effects or dataset-specific artifacts

## Confidence
- **High Confidence**: Models outperforming human experts in overall diagnostic accuracy (65.48% vs 43.57%) and the pattern-focused nature of model justifications
- **Medium Confidence**: RLHF-induced NPD underdiagnosis mechanism is plausible but requires further testing to confirm it's not dataset-specific
- **Low Confidence**: Cross-cultural generalizability of findings and specific role of model architecture in performance differences

## Next Checks
1. **Stigma Effect Isolation**: Test NPD suppression hypothesis by presenting same clinical cases with neutral third-person framing rather than first-person narratives
2. **Confidence Calibration Experiment**: Implement forced uncertainty expression by running each model case 5+ times with temperature variation, then requiring models to provide confidence scores based on run-to-run consistency
3. **Collaborative Framework Validation**: Conduct human-AI diagnostic experiment where human experts receive model outputs as decision support, measuring whether combination achieves higher diagnostic validity than either approach alone