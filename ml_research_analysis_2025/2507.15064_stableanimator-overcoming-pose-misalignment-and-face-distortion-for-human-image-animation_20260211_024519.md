---
ver: rpa2
title: 'StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human
  Image Animation'
arxiv_id: '2507.15064'
source_url: https://arxiv.org/abs/2507.15064
tags:
- image
- face
- video
- reference
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents StableAnimator++, a diffusion-based video
  generation framework for human image animation that preserves identity consistency
  even when reference images and driving poses differ significantly in body size or
  position. The method introduces three key innovations: a learnable SVD-guided pose
  alignment that predicts accurate similarity transformation matrices between reference
  and driven poses, a global content-aware Face Encoder that refines face embeddings
  by incorporating full reference context, and a distribution-aware ID Adapter that
  maintains identity through distribution alignment.'
---

# StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation

## Quick Facts
- arXiv ID: 2507.15064
- Source URL: https://arxiv.org/abs/2507.15064
- Authors: Shuyuan Tu; Zhen Xing; Xintong Han; Zhi-Qi Cheng; Qi Dai; Chong Luo; Zuxuan Wu; Yu-Gang Jiang
- Reference count: 40
- Primary result: Outperforms state-of-the-art methods on MisAlign100 benchmark for identity-consistent human image animation under pose misalignment

## Executive Summary
StableAnimator++ addresses the critical challenge of maintaining identity consistency in human image animation when reference images and driving poses differ significantly in body size or position. The framework introduces three key innovations: a learnable SVD-guided pose alignment that predicts accurate similarity transformations between reference and driven poses, a global content-aware Face Encoder that refines face embeddings by incorporating full reference context, and a distribution-aware ID Adapter that maintains identity through distribution alignment. Extensive experiments demonstrate superior performance on both TikTok and MisAlign100 benchmarks, with significant improvements in video fidelity and identity preservation compared to existing methods.

## Method Summary
The method uses Stable Video Diffusion as backbone and introduces a learnable SVD-guided pose alignment module that predicts similarity transformation matrices between reference and driven poses. The Face Encoder refines ArcFace embeddings using global reference context via cross-attention. A distribution-aware ID Adapter aligns face and image cross-attention outputs to counteract temporal layer interference. During inference, Hamilton-Jacobi-Bellman-based face optimization is integrated into the denoising process to enhance facial fidelity. The framework is trained on 5K internet videos and 9M images, with pre-training of the alignment block for 50 epochs followed by 20 epochs of full model training.

## Key Results
- Achieves CSIM score of 0.802 on MisAlign100 benchmark, significantly outperforming competitors (0.391-0.575)
- Maintains strong performance on TikTok dataset with CSIM of 0.891
- Reduces L1 reconstruction error to 0.0507 on MisAlign100 compared to 0.0813-0.1099 for competitors
- Inference time of 84 seconds for 16 frames with HJB optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learnable pose alignment guided by SVD-derived transformations improves robustness to body size and position discrepancies between reference and driving poses.
- **Mechanism:** SVD computes rotation matrix R from covariance of centered keypoints; learnable Transformer encoders model motion patterns; cross-attention injects SVD guidance; MLP predicts refined transformation matrices (R', S', t').
- **Core assumption:** SVD provides a useful intermediate transformation state even when imperfect, serving as a training signal for the learnable layers to correct and refine.
- **Evidence anchors:**
  - [abstract] "uses learnable layers to predict the similarity transformation matrices between the reference image and the driven poses via injecting guidance from Singular Value Decomposition"
  - [section III-A] Equations 2-8 detail SVD computation; Equation 10 shows MLP prediction of refined matrices
  - [corpus] TruePose paper addresses pose-guided synthesis but without learnable alignment; PMMD mentions pose misalignment but uses different approach

### Mechanism 2
- **Claim:** Distribution alignment between face and image cross-attention outputs counteracts interference from temporal layers while preserving identity.
- **Mechanism:** Compute μ and σ for both cross-attention outputs (z_img and z_face); normalize face features to match image feature distribution via (z_face - μ_face)/σ_face × σ_img + μ_img; element-wise add aligned features.
- **Core assumption:** Temporal layers cause distribution shift in spatial features; aligning face embeddings to image embedding distribution reduces this interference.
- **Evidence anchors:**
  - [abstract] "distribution-aware ID Adapter that counteracts interference caused by temporal layers while preserving ID via distribution alignment"
  - [section III-B] Equation 14 shows distribution alignment formula; explains "temporal modeling does not impede the ID information"
  - [corpus] Weak corpus evidence—no direct comparisons to distribution alignment in related pose/animation papers

### Mechanism 3
- **Claim:** HJB equation-based optimization during inference enhances facial fidelity by guiding denoising trajectory toward optimal ID consistency.
- **Mechanism:** For each denoising step, optimize predicted sample x_pred by minimizing cosine distance between ArcFace embeddings of decoded prediction and reference; use Adam optimizer for 10 gradient steps; mathematical proof shows HJB solution corresponds to diffusion SDE.
- **Core assumption:** The optimization landscape is smooth enough that gradient descent improves ID similarity without destabilizing denoising.
- **Evidence anchors:**
  - [abstract] "Hamilton-Jacobi-Bellman (HJB) based face optimization integrated into the denoising process"
  - [section III-C] Algorithm 1 shows optimization loop; Equations 15-24 derive connection between HJB and SDE formulation
  - [corpus] Weak corpus evidence—HJB optimization for diffusion is novel in this domain

## Foundational Learning

- **Singular Value Decomposition for Point Alignment:**
  - Why needed here: Core to pose alignment mechanism; SVD decomposes covariance matrix to extract rotation between point sets.
  - Quick check question: Can you explain why SVD of covariance matrix X_d·X_r^T yields the optimal rotation between two centered point sets?

- **Diffusion Denoising (EDM Framework):**
  - Why needed here: Understanding denoising timesteps, noise schedules, and where optimization can be injected without breaking the sampling process.
  - Quick check question: What happens if you modify x_pred mid-denoising without adjusting the noise schedule?

- **Cross-Attention and Feature Injection:**
  - Why needed here: Multiple components (Face Encoder, ID Adapter) use cross-attention to inject conditional information.
  - Quick check question: How does adding temporal attention after spatial attention change feature distributions?

## Architecture Onboarding

- **Component map:** Reference image → VAE latent + CLIP image embedding + ArcFace face embedding → Face Encoder → refined face embeddings → ID Adapter → distribution alignment → added to U-Net → PoseNet → aligned poses → added to latents → denoising with HJB optimization

- **Critical path:**
  1. DWPose extracts keypoints from reference and driving video
  2. Learnable alignment predicts transformation matrices
  3. Aligned poses → PoseNet → added to latents
  4. Reference → three pathways (VAE latent, CLIP image embedding, ArcFace face embedding)
  5. Face embedding refined by Face Encoder with global context
  6. ID Adapter injects face/image embeddings with distribution alignment
  7. During inference: HJB optimization runs for first 10 denoising steps

- **Design tradeoffs:**
  - Training alignment block separately (50 epochs, 9M images) vs end-to-end: reduces complexity but requires two-stage training
  - HJB optimization only in first 10 steps: balances quality vs latency (84s inference for 16 frames)
  - Face mask weighting in loss (1+M): prioritizes face regions but may cause overfitting

- **Failure signatures:**
  - Body/face distortion when pose alignment fails (visual: stretched limbs, blurred faces)
  - CSIM score drops significantly on MisAlign100 vs TikTok dataset (0.802 vs competitors' 0.391-0.575)
  - Over-sharpened faces if HJB optimization steps >10

- **First 3 experiments:**
  1. **Ablate alignment methods:** Compare ControlNeXt alignment vs SVD-only vs learnable SVD-guided on MisAlign100; measure Dis (Euclidean distance to ground truth) and CSIM.
  2. **Validate distribution alignment:** Replace ID Adapter distribution alignment with simple addition or normalization; expect FVD degradation per Table V (412→382→349).
  3. **Test HJB optimization steps:** Run inference with optimization steps k ∈ {0, 4, 8, 10, 15}; plot CSIM vs latency to find optimal tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the HJB-based face optimization be distilled into a lightweight, feed-forward network to eliminate the computational latency overhead during inference?
- Basis in paper: [explicit] The authors note that "increasing the number of optimization steps introduces higher inference latency," requiring an empirical cap of 10 steps to balance efficiency and quality.
- Why unresolved: The current method requires iterative gradient updates via an optimizer (Adam) at each denoising step, which is computationally expensive and limits real-time application.
- What evidence would resolve it: Experiments demonstrating a distillation method that mimics the HJB gradient guidance in a single pass, achieving comparable CSIM scores on MisAlign100 with significantly reduced inference time.

### Open Question 2
- Question: How can the optimization process be adaptively controlled to prevent "over-sharpening" of facial details without relying on a fixed step count?
- Basis in paper: [explicit] The paper states that "excessive steps tend to over-sharpen facial details," implying the optimization trajectory is not self-regulating.
- Why unresolved: A fixed step count (10 steps) is a heuristic that may be suboptimal for varying degrees of misalignment or reference quality; an adaptive stopping criterion is not provided.
- What evidence would resolve it: Ablation studies using perceptual quality metrics (like NIQE or FID) to show that an adaptive stopping mechanism maintains optimal texture quality better than the fixed setting.

### Open Question 3
- Question: Does the reliance on similarity transformations (rotation, scale, translation) for pose alignment limit performance in scenarios involving significant perspective distortion?
- Basis in paper: [inferred] The method explicitly predicts "similarity transformation matrices," which are mathematically incapable of modeling perspective warping (shear/non-uniform depth scaling) often present in real-world camera movements.
- Why unresolved: While the method handles body size and position, the rigidity of similarity transforms may force suboptimal alignments for dynamic camera angles.
- What evidence would resolve it: Qualitative and quantitative comparisons on a dataset specifically curated with perspective-varying driving videos (e.g., extreme low/high angles) against methods using affine or homography-based alignment.

### Open Question 4
- Question: Does the Distribution-aware ID Adapter effectively mitigate identity leakage or feature mixing in multi-person scenarios where facial regions overlap or occlude?
- Basis in paper: [inferred] Figure 13 demonstrates multi-person animation, but the ID Adapter relies on aligning the *distribution* of face and image embeddings. This global alignment might struggle to preserve distinct identity boundaries in crowded or occluded frames.
- Why unresolved: The interaction between distribution alignment and the attention mechanism in multi-instance scenarios is not deeply analyzed in the ablation studies.
- What evidence would resolve it: Evaluation on a multi-person benchmark measuring identity consistency for secondary characters and checking for attribute transfer (e.g., clothing/face mixing) between subjects.

## Limitations

- The method relies on DWPose for keypoint detection, which may fail when subjects are occluded or keypoints are missing (>70% as noted in filtering criteria), potentially causing SVD guidance to become unreliable.
- The HJB optimization adds inference latency (84s for 16 frames), making real-time applications challenging without optimization.
- The model shows strong performance on the MisAlign100 benchmark but may not generalize equally well to all pose misalignment scenarios, particularly those involving extreme body size differences.

## Confidence

- **High Confidence:** The SVD-based pose alignment mechanism and its mathematical foundation are well-documented and theoretically sound. The experimental results on the MisAlign100 benchmark (CSIM scores significantly outperforming competitors) provide strong empirical validation.
- **Medium Confidence:** The distribution alignment approach in the ID Adapter is conceptually clear but lacks extensive ablation studies comparing alternative feature fusion methods. The choice of distribution matching over other normalization techniques could benefit from additional justification.
- **Low Confidence:** The HJB optimization implementation details are sparse. While the mathematical connection to diffusion SDE is presented, the practical impact of optimization hyperparameters (learning rate, number of steps) on identity preservation versus artifact introduction needs more rigorous analysis.

## Next Checks

1. **Distribution Alignment Ablation:** Replace the distribution alignment in ID Adapter with alternative feature fusion methods (simple addition, normalization, or concatenation) and measure the impact on FVD and CSIM scores across both TikTok and MisAlign100 datasets.

2. **Pose Alignment Robustness:** Test the SVD-guided alignment module on artificially degraded keypoint data (randomly remove 30-70% of keypoints) and measure performance degradation compared to using only DWPose-extracted poses.

3. **HJB Optimization Tradeoff Analysis:** Systematically vary the number of HJB optimization steps (k ∈ {0, 2, 5, 8, 10, 15}) and plot the tradeoff curve between CSIM scores and inference latency to identify the optimal balance for practical deployment.