---
ver: rpa2
title: Optimizing LLMs Using Quantization for Mobile Execution
arxiv_id: '2512.06490'
source_url: https://arxiv.org/abs/2512.06490
tags:
- quantization
- llama
- mobile
- arxiv
- gguf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a practical workflow for deploying large language
  models on mobile devices through post-training quantization. The authors compress
  Meta's Llama 3.2 3B model using 4-bit quantization via BitsAndBytes and convert
  it to GGUF format for mobile execution.
---

# Optimizing LLMs Using Quantization for Mobile Execution

## Quick Facts
- **arXiv ID**: 2512.06490
- **Source URL**: https://arxiv.org/abs/2512.06490
- **Reference count**: 22
- **Primary result**: 68.66% model size reduction (6.00GB→1.88GB) enables Llama 3.2 3B deployment on Android via 4-bit PTQ and GGUF format

## Executive Summary
This paper presents a practical workflow for deploying large language models on mobile devices through post-training quantization. The authors compress Meta's Llama 3.2 3B model using 4-bit quantization via BitsAndBytes and convert it to GGUF format for mobile execution. The resulting model achieves significant size reduction while maintaining qualitative inference capabilities. Testing on an Android device via Termux and Ollama framework demonstrates successful deployment with competitive performance metrics compared to other small models.

## Method Summary
The authors implement a two-stage quantization pipeline: first applying BitsAndBytes 4-bit Normal Float (nf4) quantization to the Llama 3.2 3B model, then converting the result to GGUF format using llama.cpp tools with q4_k_m optimization. The quantized model is deployed on an Android device (OnePlus Nord CE 5G, 12GB RAM) via Termux and Ollama framework. Performance is evaluated using perplexity on WikiText-2 (8.57), BLEU score on DailyMail (0.45), and MMLU benchmark (61.8%), along with qualitative inference testing.

## Key Results
- 68.66% model size reduction from 6.00GB to 1.88GB through 4-bit quantization and GGUF conversion
- WikiText-2 perplexity of 8.57 and BLEU score of 0.45 demonstrate maintained inference quality
- Successful qualitative deployment on Android device using Termux and Ollama framework
- Competitive performance compared to other small models while enabling offline, privacy-preserving AI applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: 4-bit post-training quantization reduces Llama 3.2 3B model size by 68.66% while preserving functional inference capability.
- **Mechanism**: BitsAndBytes library applies Normal Float 4-bit (nf4) quantization, mapping 16-bit floating-point weights to 4-bit representations using learned scale factors and zero-points. This exploits weight distributions that are approximately normal, allowing information-preserving compression at low bit-widths.
- **Core assumption**: Weight distributions are sufficiently clustered around zero that 4-bit precision captures most information relevant to inference quality.
- **Evidence anchors**: [abstract] "achieves a 68.66% reduction in model size, reducing storage from 6.00GB to 1.88GB"; [section 4.2, Table 1] shows staged compression with 68.66% reduction; related work notes PTQ strategies underperform at <3 bits due to weight-quantization gaps.
- **Break condition**: Sub-4-bit quantization shows "substantial degradation"; 1-bit quantization requires output alignment techniques to remain viable.

### Mechanism 2
- **Claim**: GGUF format conversion enables efficient mobile inference through runtime-optimized tensor layout and memory mapping.
- **Mechanism**: llama.cpp conversion process restructures quantized weights into GGUF format designed for memory-mapped file access, allowing on-demand loading of required weight shards rather than full model loading.
- **Core assumption**: The target inference runtime can efficiently decode GGUF's quantized tensor format without hardware acceleration.
- **Evidence anchors**: [abstract] "converted to GGUF format using llama.cpp tools for optimized mobile inference"; [section 3.3] describes GGUF conversion for mobile-optimized runtimes; corpus notes GGUF has become "standard for memory-efficient deployment."
- **Break condition**: If model architecture uses unsupported operations or if mobile RAM is insufficient even with memory-mapped loading, inference will fail or OOM.

### Mechanism 3
- **Claim**: Termux + Ollama provides a viable deployment environment for running quantized LLMs on consumer Android hardware.
- **Mechanism**: Termux emulates Linux-like userspace on Android, enabling execution of compiled binaries (llama.cpp/Ollama) that would otherwise require root or NDK compilation. Ollama wraps llama.cpp with model management.
- **Core assumption**: The device has sufficient RAM (12GB in test device) to hold at least partial model weights and activations during inference.
- **Evidence anchors**: [abstract] "demonstrate the feasibility of running the final quantized GGUF model on an Android device using the Termux environment and the Ollama framework"; [section 3.2] deployment on OnePlus Nord CE 5G (Snapdragon 750G, 12GB RAM); corpus provides parallel evidence on resource-constrained edge devices.
- **Break condition**: Devices with <6GB RAM or without ARM64 support will likely fail to load even the 1.88GB quantized model due to runtime overhead.

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed here: Understanding why PTQ was chosen over QAT informs tradeoff decisions between compression quality and implementation cost.
  - Quick check question: Can you explain why PTQ requires no original training data while QAT does?

- **Concept: Quantization precision hierarchy (FP16 → INT8 → INT4)**
  - Why needed here: The paper's results depend on understanding where 4-bit sits on the accuracy-efficiency frontier.
  - Quick check question: At approximately what compression rate does INT4 quantization typically show "noticeable degradation" per the paper's literature review?

- **Concept: Memory-mapped inference**
  - Why needed here: GGUF's efficiency on mobile relies on this technique; misunderstanding it leads to incorrect RAM requirements estimation.
  - Quick check question: How does memory-mapped model loading differ from loading the entire model into RAM before inference?

## Architecture Onboarding

- **Component map**: Hugging Face Hub (model source) → Transformers + BitsAndBytes (4-bit nf4 quantization) → llama.cpp convert.py (GGUF format conversion) → llama.cpp quantize (q4_k_m scheme) → Ollama (inference runtime) → Termux (Android Linux environment)

- **Critical path**: The quantization stage (BitsAndBytes nf4 → GGUF q4_k_m) determines both size reduction and accuracy preservation. Errors here propagate irreversibly; reconversion requires starting from the original BF16 weights.

- **Design tradeoffs**:
  - nf4 vs. q4_k_m: nf4 prioritizes training/fine-tuning compatibility; q4_k_m optimizes for inference quality per byte. The paper applies both sequentially.
  - PTQ simplicity vs. QAT accuracy: PTQ chosen for accessibility (no training data/compute), but QAT may offer better accuracy at 4-bit precision.

- **Failure signatures**:
  - Model loads but produces incoherent output → quantization corrupted weight distributions; verify checksums post-conversion.
  - OOM during inference despite "sufficient" RAM → activation memory exceeds estimates; reduce context length or use more aggressive quantization.
  - Slow inference (>10 sec first token) → memory thrashing; confirm model is memory-mapped, not fully loaded.

- **First 3 experiments**:
  1. Replicate the quantization pipeline on Llama 3.2 3B in a Colab environment, measuring size at each stage (BF16 → 4-bit → GGUF) to validate the 68.66% reduction claim.
  2. Run WikiText-2 perplexity evaluation on both original and quantized models to reproduce the 8.57 perplexity result and quantify degradation.
  3. Deploy the quantized model on a target Android device via Termux/Ollama, measuring first-token latency and tokens-per-second to establish baseline performance expectations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do inference latency (tokens/second) and energy consumption compare across different mobile hardware configurations when running the 4-bit quantized model?
- **Basis in paper**: [explicit] Section 5.3 identifies "Unsystematic Performance Profiling" as a limitation, noting that inference speed was observed qualitatively rather than measured.
- **Why unresolved**: The authors successfully demonstrated feasibility but lacked systematic instrumentation to measure power draw or precise generation speed on the Android device.
- **What evidence would resolve it**: A benchmark study reporting tokens/second and battery discharge rates across diverse mobile chipsets (e.g., different Snapdragon or Tensor generations).

### Open Question 2
- **Question**: How does the specific BitsAndBytes/GGUF workflow compare against alternative quantization methods like GPTQ, AWQ, or SmoothQuant in terms of accuracy retention on mobile targets?
- **Basis in paper**: [explicit] Section 6.2 explicitly lists comparing "Alternative Quantization Techniques" such as GPTQ and AWQ as a direction for future work.
- **Why unresolved**: The study isolated a single workflow (BitsAndBytes + llama.cpp), leaving the relative efficiency of other compression algorithms unknown.
- **What evidence would resolve it**: A comparative analysis benchmarking the MMLU and perplexity scores of Llama 3.2 3B quantized via GPTQ and AWQ against the current GGUF results.

### Open Question 3
- **Question**: Can hybrid compression strategies combining PTQ with pruning or knowledge distillation achieve higher compression rates without compromising the model's qualitative coherence?
- **Basis in paper**: [explicit] Section 6.2 proposes investigating "Hybrid Compression Strategies" to combine quantization with pruning or distillation for greater compression.
- **Why unresolved**: The current approach relied solely on quantization; the authors did not test if removing redundant weights (pruning) or distilling knowledge could further reduce the 1.88GB footprint.
- **What evidence would resolve it**: Experiments applying magnitude-based pruning to the Llama model prior to 4-bit quantization to observe if size decreases while maintaining the BLEU score of 0.45.

## Limitations
- The deployment testing is limited to a single Android device (OnePlus Nord CE 5G with 12GB RAM), constraining generalizability to lower-end mobile hardware.
- The qualitative testing on mobile devices lacks systematic benchmarking metrics, making it difficult to assess real-world performance variability.
- The paper doesn't address potential security vulnerabilities introduced by quantization, despite known attack vectors in GGUF formats.

## Confidence

**High Confidence**: The size reduction metrics (68.66% reduction from 6.00GB to 1.88GB) and the staged compression pathway are well-documented with verifiable intermediate values. The BitsAndBytes quantization mechanism is standard practice with extensive literature support.

**Medium Confidence**: The performance metrics (perplexity of 8.57, BLEU of 0.45) are reported without error bounds on mobile deployment, and the qualitative inference testing lacks systematic evaluation criteria.

**Low Confidence**: The claim that the quantized model maintains "qualitative inference capabilities" on mobile devices is based on subjective assessment rather than standardized mobile LLM evaluation benchmarks.

## Next Checks
1. **Cross-device validation**: Deploy the quantized model on multiple Android devices with varying RAM specifications (4GB, 6GB, 8GB) to establish minimum viable hardware requirements and measure performance degradation patterns.

2. **Security audit**: Conduct a security assessment of the GGUF quantized model using known attack vectors from "Mind the Gap: A Practical Attack on GGUF Quantization" to identify potential vulnerabilities in the quantization scheme.

3. **Systematic mobile evaluation**: Develop a standardized mobile LLM evaluation protocol including first-token latency, tokens-per-second, memory usage patterns, and generation quality metrics to replace the current qualitative assessment.