---
ver: rpa2
title: Informed Initialization for Bayesian Optimization and Active Learning
arxiv_id: '2510.23681'
source_url: https://arxiv.org/abs/2510.23681
tags:
- hipe
- optimization
- batch
- learning
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HIPE, a novel acquisition function for initializing
  Bayesian Optimization and Active Learning. HIPE addresses the limitation of traditional
  space-filling designs, which often fail to balance coverage of the input space with
  effective hyperparameter learning.
---

# Informed Initialization for Bayesian Optimization and Active Learning

## Quick Facts
- **arXiv ID:** 2510.23681
- **Source URL:** https://arxiv.org/abs/2510.23681
- **Reference count:** 35
- **Primary result:** HIPE acquisition function outperforms standard initialization strategies (Sobol, Random, BALD, NIPV) in large-batch, few-shot settings for both Bayesian Optimization and Active Learning.

## Executive Summary
This paper introduces HIPE, a novel acquisition function for initializing Bayesian Optimization and Active Learning. Traditional space-filling designs like Sobol sequences often fail to balance input space coverage with effective hyperparameter learning, particularly in large-batch, few-shot settings. HIPE addresses this by optimizing for both predictive uncertainty reduction and hyperparameter identification using information-theoretic principles. The authors derive a closed-form expression for HIPE in the Gaussian Process setting and implement a practical Monte Carlo approximation for batched optimization. Extensive experiments demonstrate that HIPE consistently achieves better predictive accuracy, hyperparameter identification, and subsequent optimization performance compared to standard baselines, especially in high-dimensional settings and when accurate model calibration is critical.

## Method Summary
HIPE is an acquisition function that combines Expected Predictive Information Gain (EPIG) and Bayesian Active Learning by Disagreement (BALD) objectives with an adaptive weighting β = EIG(y(x*); θ|D). The method uses Monte Carlo estimation with M=12 hyperparameter samples, T=1024 test points, and N=128 posterior samples to evaluate the acquisition function. For initialization, HIPE jointly optimizes over the entire q-batch in qD-dimensional space using multi-start L-BFGS-B. The approach employs a fully Bayesian Gaussian Process with ARD kernels and NUTS sampling for hyperparameter inference. Priors are specified as ℓd ~ LN(0.75+log(D)/2, 0.75), σε ~ LN(-5.5, 0.75), and c ~ N(0, 0.25). The method is designed specifically for large-batch (q≥8), few-shot (B=2-5 batches) settings where joint batch selection is critical.

## Key Results
- HIPE consistently outperforms Sobol, Random, BALD, and NIPV baselines across synthetic functions (Ackley 4D, Hartmann 4D/6D/8D/12D, Ishigami) and real-world HPO surrogate tasks
- The method shows particular advantage in large-batch settings (q≥8) and high-dimensional spaces (D≥20)
- HIPE achieves better hyperparameter identification, with post-initialization lengthscales accurately reflecting dimension importance
- Out-of-sample inference performance after HIPE initialization surpasses all baselines in Bayesian Optimization tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining predictive uncertainty reduction (EPIG) with hyperparameter learning (BALD) yields better initialization than either objective alone.
- **Mechanism:** HIPE maximizes joint information gain over test function values AND model hyperparameters. The acquisition surface naturally selects points that are both well-distributed (reducing predictive variance across the space) and axis-aligned (resolving lengthscale uncertainty). This counters the tendency of pure space-filling methods to sample boundaries in high dimensions while avoiding BALD's tendency to cluster queries.
- **Core assumption:** Accurate hyperparameter estimates (particularly lengthscales) causally improve downstream BO acquisition decisions by reducing influence of unimportant dimensions.
- **Evidence anchors:**
  - [abstract]: "HIPE optimizes for both predictive uncertainty reduction and hyperparameter identification using information-theoretic principles."
  - [Section 4.1, Eq. 7]: HIPEβ(X) combines EPIG and BALD objectives with weighting parameter β.
  - [corpus]: Related work (BioBO, OASI) addresses domain-informed initialization but does not validate this specific joint formulation.
- **Break condition:** If the true objective has no lengthscale variation across dimensions (isotropic), the hyperparameter learning component provides diminishing returns.

### Mechanism 2
- **Claim:** Setting β = EIG(y(x*); θ|D) automatically balances the two objectives proportional to their effect on downstream prediction.
- **Mechanism:** The weighting β quantifies the mutual information between hyperparameters θ and test points y(x*). When hyperparameters are highly informative about predictions (large β), BALD dominates; when hyperparameter knowledge barely affects predictions (small β), EPIG dominates. This removes the need for manual tuning while adapting to problem structure.
- **Core assumption:** The mutual information between hyperparameters and test predictions meaningfully captures the practical value of hyperparameter learning for the specific problem.
- **Evidence anchors:**
  - [Section 4.1, Eq. 9-10]: Definition of β = EIG(y(x*); θ|D) and the final HIPE formulation.
  - [Section 4.1]: "β does not depend on the candidate set X and can thus be pre-computed."
  - [corpus]: No direct validation of this specific weighting scheme in related literature.
- **Break condition:** If hyperparameter priors are extremely tight (low uncertainty), EIG(y(x*); θ|D) → 0, and HIPE collapses to pure EPIG.

### Mechanism 3
- **Claim:** Batch joint optimization via Monte Carlo estimation produces coherent designs that avoid greedy redundancy.
- **Mechanism:** HIPE estimates the acquisition function using M hyperparameter samples, T test locations, and N predictive samples, then jointly optimizes over the entire q-batch in qD-dimensional space. This allows the optimizer to place points that collectively reduce uncertainty rather than independently selecting redundant high-information points.
- **Core assumption:** The nested MC estimator provides sufficient signal for gradient-based optimization despite stochasticity.
- **Evidence anchors:**
  - [Section 4.2, Eqs. 11-12]: MC estimators for BALD and EPIG components.
  - [Section 4.2]: "Using Sample Average Approximation, the HIPE objective is deterministic and auto-differentiable."
  - [corpus]: High-dimensional surrogate modeling work (arxiv:2512.11705) uses similar batch BO but doesn't validate this specific acquisition.
- **Break condition:** For very large batch sizes (q >> 64) or high dimensions (D >> 40), computational cost may become prohibitive relative to function evaluation time.

## Foundational Learning

- **Concept: Mutual Information / Expected Information Gain**
  - **Why needed here:** HIPE is defined entirely in terms of EIG between variables (predictions, hyperparameters, observations). Understanding that EIG = H[ξ] - E[H[ξ|y]] (entropy reduction) is essential.
  - **Quick check question:** Can you explain why EIG is symmetric and can be computed either as entropy reduction over parameters or over observations?

- **Concept: Gaussian Process hyperparameters and ARD kernels**
  - **Why needed here:** HIPE specifically targets learning lengthscales ℓi that control dimension importance. The mechanism assumes poor lengthscale estimation harms BO performance.
  - **Quick check question:** What happens to a GP's predictions if a lengthscale is severely overestimated for an important dimension?

- **Concept: Batch Bayesian Optimization vs. sequential**
  - **Why needed here:** HIPE is designed for the large-batch, few-shot setting where joint batch selection is critical because there are few sequential correction opportunities.
  - **Quick check question:** Why does greedy sequential selection fail when you need to choose all points simultaneously before observing any outcomes?

## Architecture Onboarding

- **Component map:** Hyperparameter sampler -> Test distribution p* -> EPIG estimator -> BALD estimator -> β pre-computer -> Batch optimizer
- **Critical path:**
  1. Define GP model structure and hyperpriors
  2. Pre-compute β = EIG(y(x*); θ|D) using MC integration
  3. Initialize candidate batch X via Sobol samples
  4. Estimate HIPE(X) using MC samples of θ, test points, and predictions
  5. Optimize X jointly via gradient descent
  6. Return optimized batch for evaluation
- **Design tradeoffs:**
  - MC sample counts (M, T, N): Higher values reduce estimator variance but increase runtime. Paper uses M=12, T=1024, N=128.
  - Batch size q: Larger batches improve coverage but scale optimization cost as O(qD). HIPE shows advantage particularly for q ≥ 8.
  - Test distribution p*: Uniform is default; non-uniform p* encodes prior knowledge about important regions
- **Failure signatures:**
  - Clustered queries: BALD component too strong (β too large relative to EPIG scale) → queries collapse to axes
  - Boundary-heavy samples: EPIG component too weak → behaves like Sobol, over-samples boundaries in high D
  - Runtime explosion: Check M × T × N product; reduce if acquisition optimization exceeds function evaluation time
  - Poor hyperparameter identification post-init: Likely β underweighting or insufficient hyperparameter prior uncertainty
- **First 3 experiments:**
  1. **Sanity check on 2D synthetic function with known lengthscales:** Run HIPE vs. Sobol vs. BALD; visualize acquisition surfaces and selected points. Verify HIPE produces spread-but-axis-aligned designs.
  2. **Ablation on β:** Compare HIPE with β=0 (pure EPIG), β=1, β=EIG(y(x*); θ|D), and β=10 (near-pure BALD) on Hartmann-6D. Plot RMSE and NLL after initialization batch.
  3. **Two-shot BO on LCBench task:** Run full pipeline (HIPE init → single qLogNEI batch) on Fashion-MNIST surrogate. Compare out-of-sample inference performance against Random/Sobol/NIPV baselines with matched compute budget.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead from MCMC sampling and joint qD optimization may limit scalability beyond q=32 or D=40
- Method assumes ARD kernel structure where individual lengthscales matter; performance may degrade for isotropic or highly correlated dimensions
- Batch joint optimization approach lacks comparison to sequential or batch-conditioned alternatives

## Confidence
- **High confidence:** HIPE's core mechanism of combining EPIG and BALD objectives and its empirical advantage in large-batch, few-shot settings
- **Medium confidence:** The specific adaptive β formulation effectively balances objectives across problems without manual tuning
- **Low confidence:** Computational scalability beyond presented dimensions and batch sizes, and generalizability to non-ARD kernel structures

## Next Checks
1. **Ablation study on β weighting:** Compare HIPE with fixed β values (0, 0.5, 1, 10) on Hartmann-6D to validate that the adaptive EIG-based weighting consistently outperforms manual tuning.
2. **Scalability test:** Evaluate HIPE on D=60-80 dimensional problems with q=32-64 batches to identify computational bottlenecks and assess whether the information-theoretic benefits persist at scale.
3. **Kernel structure robustness:** Test HIPE with Matérn kernels and isotropic priors on synthetic problems where true lengthscales are either uniform or highly correlated across dimensions.