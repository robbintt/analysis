---
ver: rpa2
title: Linear socio-demographic representations emerge in Large Language Models from
  indirect cues
arxiv_id: '2512.10065'
source_url: https://arxiv.org/abs/2512.10065
tags:
- gender
- user
- representations
- class
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models encode user sociodemographic
  attributes from indirect cues like names and occupations. The authors train linear
  probes on LLM activation patterns to detect gender, race, and class representations,
  achieving AUC-ROC values exceeding 0.98 for explicit disclosure across multiple
  models.
---

# Linear socio-demographic representations emerge in Large Language Models from indirect cues

## Quick Facts
- arXiv ID: 2512.10065
- Source URL: https://arxiv.org/abs/2512.10065
- Authors: Paul Bouchaud; Pedro Ramaciotti
- Reference count: 40
- Models encode demographics from names/occupations with AUC-ROC > 0.98

## Executive Summary
This study reveals that large language models develop linear representations of user demographics in their activation space, encoding attributes like gender, race, and class from both explicit disclosure and indirect cues such as names and occupations. The authors train logistic regression probes on residual stream activations to detect these representations, achieving classification performance exceeding 0.98 AUC-ROC. Critically, these implicit demographic representations actively shape downstream behavior—models recommend gender-stereotypical careers at rates comparable to explicit gender disclosure, despite refusing explicit demographic reasoning queries. The findings expose an "alignment gap" where safety training suppresses explicit demographic queries while leaving implicit inference pathways intact.

## Method Summary
The authors generate synthetic dialogues with explicit demographic disclosure (2,500 prompts across intersectional combinations of gender, race, and class) and extract residual stream activations at each layer for the final token. Logistic regression probes with L2 regularization (λ=0.01) are trained via 5-fold cross-validation to classify demographic attributes, with optimal layers identified per model (Magistral: layer 20/40, Qwen: layer 33/40, GPT-OSS: layer 13/28, OLMo: layer 13/16). These trained probes are then applied to implicit cues (names, occupations) and validated against real-world statistics (US Census, Bureau of Labor Statistics). Steering interventions along identified demographic directions test causal influence on downstream behavior.

## Key Results
- Linear probes achieve AUC-ROC > 0.98 for gender, race, class classification from explicit disclosure across multiple models
- Implicit cues (names, occupations) activate census-aligned demographic representations (Spearman ρ > 0.78 with real-world statistics)
- Inference-time steering along demographic directions shifts model outputs in stereotype-consistent ways
- GPT-5 refuses explicit demographic queries (>98%) but produces stereotypical recommendations when gender is implicitly signaled

## Why This Works (Mechanism)

### Mechanism 1: Linear Representation of Demographics
- **Claim:** Sociodemographic attributes encode as interpretable geometric directions in residual stream activation space.
- **Mechanism:** During forward pass, indirect cues (names, occupations) activate pattern-matched representations formed during pre-training on distributional text statistics. These representations reside in linear subspaces, recoverable via logistic regression probes trained on explicit-disclosure activations.
- **Core assumption:** The linear representation hypothesis holds for user demographic attributes, meaning high-level concepts encode as directions rather than nonlinear manifolds.
- **Evidence anchors:**
  - [abstract]: "LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions"
  - [Results]: Probes achieved AUC-ROC exceeding 0.98 for gender, race, class across Magistral, Qwen, GPT-OSS
  - [Methods]: "Having identified which subspaces encode user attributes, we then monitor their activity across diverse prompts"
- **Break condition:** If nonlinear probes significantly outperform linear probes on held-out demographic prediction, the linear hypothesis may not hold for these attributes.

### Mechanism 2: Statistical Association Transfer from Training Data
- **Claim:** Demographic representations in activation space correlate with training corpus co-occurrence statistics.
- **Mechanism:** Pre-training on web text exposes models to stereotypical name-demography and occupation-demography co-occurrences. During inference, cue words activate latent representations shaped by these statistical priors.
- **Core assumption:** Training data demographic distributions are the primary source of learned associations; other factors (architecture, fine-tuning) play secondary roles.
- **Evidence anchors:**
  - [Results]: OLMo training data analysis shows Pearson r=0.795 between training-corpus gender fractions and internal representations for occupations
  - [Results]: CommonCrawl gender fractions correlate with representations across Magistral (r=0.739), Qwen (r=0.623), GPT-OSS (r=0.710)
  - [corpus]: Corpus evidence on mechanism origin is moderate; the OLMo open-data analysis is strong, but closed-model training data access is limited
- **Break condition:** If models trained on debiased or synthetic corpora still exhibit equivalent demographic representations, training-data statistics may not be the primary driver.

### Mechanism 3: Causal Influence on Downstream Behavior
- **Claim:** Demographic representations functionally shape output generation, not merely correlate with inputs.
- **Mechanism:** Residual stream directions encoding demographic attributes are read by downstream layers during token generation. Intervening on these directions alters output distributions in stereotype-consistent ways.
- **Core assumption:** Residual stream directions are causally implicated in generation, not epiphenomenal byproducts of processing.
- **Evidence anchors:**
  - [Results]: Inference-time steering along race direction shifts country-of-origin predictions from Nigeria to France proportionally to intervention strength
  - [Results]: Gender steering alters predicted first names from male-associated to female-associated
  - [abstract]: "Implicit demographic representations actively shape downstream behavior, such as career recommendations"
- **Break condition:** If steering interventions produce incoherent outputs or fail to transfer across prompt templates, the representations may be prompt-specific artifacts rather than general mechanisms.

### Mechanism 4: Alignment Gap Between Explicit Refusal and Implicit Action
- **Claim:** Safety training suppresses explicit demographic reasoning but leaves implicit inference-behavior pathways intact.
- **Mechanism:** Alignment procedures target surface-level refusal patterns for explicit demographic queries. However, implicit cues (names, preferences) bypass these guardrails by activating demographic representations without triggering refusal classifiers.
- **Core assumption:** Refusal mechanisms operate on semantic content detection rather than internal representation monitoring.
- **Evidence anchors:**
  - [Results]: GPT-5 refused >98% of explicit "as a man/woman" career queries but produced stereotypical recommendations when gender was implicitly signaled via sport preferences
  - [abstract]: "Models refuse explicit demographic-based reasoning while implicitly encoding and acting upon demographics inferred from contextual cues"
  - [corpus]: Related work (Hofmann et al., Bai et al.) documents similar explicit-implicit gaps in bias benchmarks
- **Break condition:** If future models refuse implicit-cue scenarios at equivalent rates to explicit disclosure, the alignment gap mechanism is addressed.

## Foundational Learning

- **Concept: Linear Representation Hypothesis**
  - Why needed here: Interpreting probe results requires understanding that high-level concepts may encode as directions in activation space, enabling linear decoding and steering.
  - Quick check question: Given a probe achieving 0.99 AUC-ROC on demographic classification, what does this suggest about the geometry of that concept in activation space?

- **Concept: Residual Stream Architecture**
  - Why needed here: Probes target residual stream activations at specific layers; understanding the read/write structure explains why intermediate layers encode richer representations.
  - Quick check question: Why might middle layers (e.g., layer 20 of 40) yield higher probe accuracy than early or final layers?

- **Concept: Activation Steering / Inference-Time Intervention**
  - Why needed here: Causal claims depend on intervention experiments; understanding how to add/subtract direction vectors explains the steering methodology.
  - Quick check question: If adding direction vector v_gender to residual stream h_i shifts outputs toward female-stereotypical completions, what does this imply about v_gender's role?

## Architecture Onboarding

- **Component map:** Synthetic prompts -> Transformer with L layers -> Residual stream activations at layer i -> Logistic regression probe -> Demographic label prediction -> Steering intervention (h_i ← h_i + α · v_attribute)

- **Critical path:**
  1. Generate labeled explicit-disclosure dataset (2,500 prompts across demographic intersections)
  2. Record residual stream at each layer for all prompts
  3. Train logistic regression probes per layer per attribute
  4. Identify optimal layer via cross-validation (max AUC-ROC)
  5. Apply trained probes to implicit-cue prompts
  6. Validate causality via steering: h_i ← h_i + α · v_attribute

- **Design tradeoffs:**
  - Binary vs. continuous demographics: Binary labels simplify probing but fail to capture identity nuance
  - Synthetic vs. organic prompts: Synthetic enables controlled experiments; generalization to real users uncertain
  - Linear vs. nonlinear probes: Linear enables interpretability; may underfit complex representations

- **Failure signatures:**
  - Probes fail to generalize across languages or prompt templates (overfitting to disclosure phrasing)
  - Steering produces incoherent outputs rather than systematic shifts
  - High probe accuracy on explicit disclosure but near-chance on implicit cues (representations not shared)

- **First 3 experiments:**
  1. **Probe training validation:** Train linear probes on explicit-disclosure activations across all layers. Verify AUC-ROC > 0.95 at optimal layers. Plot accuracy vs. layer to confirm mid-layer peak.
  2. **Implicit-cue generalization:** Apply trained probes to name-only and occupation-only prompts. Compare probe-predicted demographics against census/BLS ground truth (compute Spearman ρ).
  3. **Steering causality check:** For one attribute (e.g., gender), run steering intervention across α ∈ [-20, 20] on neutral prompts. Verify monotonic output shift (e.g., name predictions from male-associated to female-associated).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the linear demographic representations identified from synthetic prompts generalize to authentic, multi-turn user conversations?
- Basis in paper: [explicit] Authors state "whether findings fully generalize to organic user interactions remains an open question" in the limitations section.
- Why unresolved: Experiments used carefully controlled synthetic dialogues; real conversations have unpredictable structures, varied disclosure patterns, and contextual factors not captured in experimental prompts.
- What evidence would resolve it: Studies probing models during live human-AI interactions, comparing activation patterns to those observed with synthetic prompts.

### Open Question 2
- Question: Can inference-time interventions (activation steering) effectively reduce stereotypical outputs without degrading model capabilities in deployed systems with persistent memory?
- Basis in paper: [explicit] "whether such approaches can effectively mitigate bias without compromising model capabilities—particularly in the context of persistent, cross-conversation memory—remains an open and pressing question for future work."
- Why unresolved: The paper demonstrates steering works causally but does not test whether interventions persist across sessions or impair other model functions.
- What evidence would resolve it: Longitudinal experiments measuring both bias reduction and task performance metrics after applying steering across extended multi-session interactions.

### Open Question 3
- Question: How do non-US models (e.g., Qwen, Magistral) acquire US Census-aligned demographic stereotypes—through shared English training corpora, cultural transfer, or other mechanisms?
- Basis in paper: [inferred] Authors observe that Chinese and French models encode US stereotypes, "suggesting globalization of bias through shared English-language training resources, though deeper cross-cultural analysis is needed."
- Why unresolved: The study correlating OLMo representations with training data cannot be replicated for closed models; Common Crawl is an imperfect proxy for proprietary corpora.
- What evidence would resolve it: Controlled training experiments varying cultural sources, or probing intermediate checkpoints to trace when during training stereotypes emerge.

### Open Question 4
- Question: Do demographic representations compound over successive interactions in systems with persistent memory, amplifying stereotyping effects beyond single-session levels?
- Basis in paper: [inferred] The paper notes representations "may persist and compound across interactions" but only tested stability over 5 turns in synthetic conversations, not cumulative amplification across sessions.
- Why unresolved: Experimental design did not simulate realistic memory accumulation over days or weeks of user activity.
- What evidence would resolve it: Longitudinal studies measuring stereotype strength in outputs after accumulating user information across multiple distinct conversations.

## Limitations

- Synthetic prompts and binary demographic labels constrain external validity—real-world demographic complexity and intersectional identities may not map cleanly onto studied representations.
- Access to model internals remains restricted, limiting reproducibility of GPT-OSS and GPT-5 results.
- Steering intervention experiments show causal evidence but require broader validation across multiple downstream tasks and prompt templates.

## Confidence

- **High Confidence:** Linear probe methodology, statistical correlations with real-world demographics, and core finding that LLMs encode demographics from indirect cues.
- **Medium Confidence:** Causal influence of demographic representations on downstream behavior, particularly given steering intervention results.
- **Low Confidence:** Claims about the precise mechanism by which safety alignment creates an "alignment gap" between explicit refusal and implicit action.

## Next Checks

1. **Prompt Template Generalization:** Apply trained probes to diverse, real-world prompt templates beyond synthetic disclosure format. Test whether demographic representations transfer across prompt styles and languages, particularly for languages with grammatical gender or honorific systems.

2. **Intersectional Identity Validation:** Extend analysis beyond binary demographic attributes to capture intersectional identities. Train probes on multi-dimensional demographic vectors and test whether combined representations encode as independent directions or nonlinear combinations.

3. **Downstream Behavior Scaling:** Systematically measure the magnitude and consistency of steering-induced behavioral shifts across multiple downstream tasks (not just career recommendations). Quantify whether implicit demographic inference produces effects comparable to explicit disclosure across different model families and task domains.