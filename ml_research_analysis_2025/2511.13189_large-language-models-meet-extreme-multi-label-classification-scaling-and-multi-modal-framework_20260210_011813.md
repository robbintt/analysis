---
ver: rpa2
title: 'Large Language Models Meet Extreme Multi-label Classification: Scaling and
  Multi-modal Framework'
arxiv_id: '2511.13189'
source_url: https://arxiv.org/abs/2511.13189
tags:
- vixml
- learning
- text
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of Extreme Multi-label Classification
  (XMC), where queries must be associated with relevant labels from extremely large
  label spaces while balancing efficiency and performance. The authors propose two
  key contributions: (1) a dual-decoder learning approach that effectively adapts
  large decoder-only language models (up to 7B parameters) for XMC by embedding texts
  within structured prompt templates, and (2) the Vision-enhanced eXtreme Multi-label
  Learning (ViXML) framework, which efficiently incorporates visual metadata by using
  a single image embedding per image concatenated with text token embeddings.'
---

# Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework

## Quick Facts
- **arXiv ID:** 2511.13189
- **Source URL:** https://arxiv.org/abs/2511.13189
- **Reference count:** 13
- **Primary result:** ViXML framework achieves state-of-the-art P@1 scores (+5.07% to +8.21%) on XMC tasks by integrating visual metadata with small encoders outperforming billion-parameter text-only models

## Executive Summary
This paper addresses the challenge of Extreme Multi-label Classification (XMC) by proposing two key contributions: a dual-decoder learning approach that adapts large decoder-only language models (up to 7B parameters) for XMC through structured prompting, and the Vision-enhanced eXtreme Multi-label Learning (ViXML) framework that efficiently incorporates visual metadata. The framework demonstrates that single image embeddings concatenated with text tokens, processed through early fusion with frozen vision encoders, can deliver performance gains equivalent to adding billions of text-only parameters. Extensive experiments on four public datasets show state-of-the-art results, with ViXML using small 66M parameter encoders outperforming text-only decoder models in most cases.

## Method Summary
The method combines structured prompting for decoder models with early-fusion multi-modal integration. Text inputs are wrapped in structured templates ("This product text [text] <\|endoftext\|>") to contextualize them for pretrained decoders, while images are processed through frozen vision encoders (SigLIP2 or ViT) to produce single embeddings that concatenate with text token embeddings before transformer processing. Training uses triplet loss with NGAME hard negative mining, and inference employs Maximum Inner Product Search against precomputed label embeddings. The framework supports both encoder and decoder architectures while maintaining computational efficiency through frozen vision encoders and precomputed feature banks.

## Key Results
- ViXML with small encoders (66M parameters) outperforms text-only decoder models in most cases, demonstrating visual metadata's effectiveness
- Qwen2.5-7B-I achieves 48.06 P@1 vs BERT's 45.87 on LF-AmazonTitles-131K (text-only)
- Early fusion consistently outperforms MUFIN's late fusion by ~1.5% P@1 across vision encoder configurations
- Adding visual metadata improves P@1 by +5.07% to +8.21% over text-only baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured prompting enables decoder-only LLMs to produce effective embeddings for extreme multi-label classification, surpassing traditional encoder-only approaches.
- **Core assumption:** Decoder-only LLMs' parametric knowledge from pretraining transfers to embedding-based retrieval tasks when properly prompted and fine-tuned, despite their unidirectional attention.
- **Evidence anchors:** Qwen2.5-7B-I achieves 48.06 P@1 vs BERT's 45.87 on LF-AmazonTitles-131K; prompting without structure scores 43.14 vs 44.66 with prefix and EOS.

### Mechanism 2
- **Claim:** Frozen vision encoders with single image embeddings provide computational efficiency while delivering performance gains equivalent to adding billions of text-only parameters.
- **Core assumption:** Single pooled image embeddings capture sufficient visual semantics for XMC tasks; fine-grained token-level visual features are unnecessary for product-to-product recommendation.
- **Evidence anchors:** DistilBERT (66M) + ViXML achieves 49.55 P@1 vs Qwen2.5-3B-I text-only at 47.42; preserves efficiency by eliminating need to train visual encoders.

### Mechanism 3
- **Claim:** Early fusion of visual and text representations outperforms late fusion by enabling cross-modal attention during encoding rather than post-hoc combination.
- **Core assumption:** Joint encoding allows text tokens to contextualize image representations and vice versa, resolving ambiguities that separate encoding cannot address.
- **Evidence anchors:** ViXML (early fusion) + PRIME + ViT-32: 53.30 P@1 vs MUFIN (late fusion) + PRIME + ViT-32: 52.62 P@1.

## Foundational Learning

- **Concept: Contrastive Learning with Triplet Loss**
  - Why needed here: The core training objective that shapes embeddings so positive query-label pairs have higher inner product than negative pairs.
  - Quick check question: Given embeddings h_q (query), h_p (positive label), h_n (negative label), and margin m=0.1, compute the triplet loss when h_q · h_p = 0.6, h_q · h_n = 0.55. Answer: [0.55 - 0.6 + 0.1]+ = [0.05]+ = 0.05

- **Concept: Maximum Inner Product Search (MIPS)**
  - Why needed here: At inference, XMC reduces to finding labels with highest embedding similarity to the query.
  - Quick check question: If query embedding is [0.6, 0.8] and three label embeddings are L1=[1,0], L2=[0,1], L3=[0.5, 0.5], which label ranks first? Answer: L1=0.6, L2=0.8, L3=0.7 → L2 ranks first

- **Concept: Frozen Pretrained Encoders + Feature Banks**
  - Why needed here: ViXML's efficiency relies on precomputing vision embeddings once and reusing them, avoiding redundant forward passes through the 1.14B parameter vision encoder.
  - Quick check question: A dataset has 500K queries with 2 images each. SigLIP2 (1.14B params, 4.28ms/image) takes how long to precompute all embeddings? Answer: 500K × 2 × 4.28ms = 4,280,000ms ≈ 71 minutes (one-time cost)

## Architecture Onboarding

- **Component map:**
  Text Input → Tokenizer → [Text Prefix] → Text Token Embeddings
                                        ↓
  Image Input → Frozen Vision Encoder (SigLIP2/ViT) → Linear Projection → Image Embedding
                                        ↓
  [Concatenate] → Transformer (Encoder OR Decoder with structured prompt + LoRA)
                                        ↓
  Mean Pooling → L2 Normalize → Sentence Embedding (hq or hl)
                                        ↓
  [Training: Triplet Loss with NGAME Hard Negative Mining]
  [Inference: MIPS against precomputed label embeddings]

- **Critical path:**
  1. Vision encoder selection and projection initialization: Wrong projection init causes training instability
  2. Prompt template design for decoder models: Incorrect ordering (images before text without prefix) causes attention sink issues
  3. Negative mining hyperparameters: NGAME requires appropriate hard negative ratio
  4. LoRA rank and alpha: Small values (<64) produced weak results

- **Design tradeoffs:**
  - Encoders (DistilBERT, 66M) are 30x faster to train but decoders (Qwen2.5-3B) achieve 2-3 points higher P@1
  - ViT-32 (86M) is faster but SigLIP2 (1.14B) provides +2 points P@1
  - 1 image yields +2.11 P@1 over text-only; 3 images yield +2.63 (diminishing returns after 3)
  - PRIME's multi-positive option improves 0.09 P@1 but increases training time 40%

- **Failure signatures:**
  - Training loss plateaus early with decoder models: Likely insufficient epochs
  - ViXML underperforms text-only baseline: Check if visual metadata available at test time
  - Retrieval-augmented inference degrades PSP@1: Expected behavior due to ground-truth bias
  - Decoder P@1 drops when image tokens precede text: Attention sink on first token

- **First 3 experiments:**
  1. Train DistilBERT with PRIME on LF-AmazonTitles-131K (text-only) for 300 epochs. Target: ~44.86 P@1
  2. Add frozen SigLIP2 with linear projection, single image per sample, train for 150 epochs. Target: ~49.55 P@1
  3. Train Qwen2.5-0.5B-Instruct with structured prompts for 30 epochs. Target: ~44.84 P@1

## Open Questions the Paper Calls Out

None

## Limitations

- Reliance on visual metadata being available at both training and inference time, with models trained with ViXML underperforming text-only models when visual metadata is unavailable at test time
- Single-image embedding approach may not capture fine-grained visual details that multiple token-level embeddings could provide
- Computational efficiency claims are qualified by the need to precompute and store vision embeddings as a feature bank, which could become prohibitive for extremely large-scale applications

## Confidence

- **High Confidence:** Core performance improvements of ViXML over text-only baselines (P@1 gains of +5.07% to +8.21%) are well-supported by extensive experimental results across four datasets with clear ablation studies.
- **Medium Confidence:** Claim that decoder-only LLMs with structured prompting can effectively replace encoder models for XMC tasks is supported but requires more diverse domain testing.
- **Low Confidence:** Assertion that a single image embedding is "worth billions of parameters" is somewhat overstated and lacks rigorous ablation studies comparing different visual feature granularities.

## Next Checks

1. **Cross-domain robustness test:** Evaluate ViXML on non-e-commerce XMC datasets (e.g., Wikipedia categories, scientific paper classification) to assess whether the structured prompting approach generalizes beyond product recommendation contexts.

2. **Vision encoder granularity ablation:** Systematically compare single pooled image embeddings against multi-token visual features (e.g., using Vision Transformer patch embeddings) to quantify the information loss from the current approach.

3. **Dynamic vs. frozen vision encoder efficiency:** Implement and benchmark a variant where the vision encoder is fine-tuned rather than frozen, measuring both training time and retrieval accuracy.