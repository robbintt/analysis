---
ver: rpa2
title: Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure
arxiv_id: '2601.07342'
source_url: https://arxiv.org/abs/2601.07342
tags:
- infrastructure
- service
- agent
- impact
- root
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an agentic diagnostic framework for root
  cause analysis (RCA) and impact propagation in telecom and datacenter infrastructures.
  Instead of hard-coded algorithms, an LLM agent uses Model Context Protocol (MCP)
  tools to navigate a typed infrastructure ontology through structured steps: service
  lookup, resource enumeration, evidence analysis, and impact assessment.'
---

# Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure

## Quick Facts
- arXiv ID: 2601.07342
- Source URL: https://arxiv.org/abs/2601.07342
- Reference count: 10
- Primary result: LLM agent achieves 100% accuracy in telecom/datacenter RCA and impact analysis using tool-constrained reasoning

## Executive Summary
This paper introduces an agentic diagnostic framework for root cause analysis (RCA) and impact propagation in telecom and datacenter infrastructures. Instead of hard-coded algorithms, an LLM agent uses Model Context Protocol (MCP) tools to navigate a typed infrastructure ontology through structured steps: service lookup, resource enumeration, evidence analysis, and impact assessment. Experiments show Claude Haiku 3.5 achieves 100% accuracy in identifying root causes and impacted parties across 10 test cases, with perfect faithfulness to tool outputs. Larger open-source models (GPT-OSS-120B) perform nearly as well with faster inference, while smaller models struggle with protocol adherence. The approach ensures grounded reasoning, reproducibility, and operational safety, laying the foundation for autonomous incident resolution and proactive change impact mitigation.

## Method Summary
The framework implements an agentic loop using LLM agents (Claude Haiku 3.5, Llama 3.1 8B, GPT-OSS-120B) connected to a Model Context Protocol server that exposes typed tools for infrastructure interaction. The agent follows a 6-step investigation protocol: Lookup Service → Get Implementation → Analyze Events/Notes → Get Impact → Publish. The MCP server acts as a safety boundary, preventing direct graph access and ensuring all reasoning is grounded in tool outputs. The system is evaluated on a synthetic infrastructure ontology with 10 specific test scenarios covering various failure modes and evidence types.

## Key Results
- Claude Haiku 3.5 achieves 100% accuracy across all RCA and IA test cases
- GPT-OSS-120B matches Claude's accuracy with faster inference times
- Smaller models (Llama 8B) show 21% investigation failure rates due to malformed tool calls
- Perfect faithfulness to tool outputs eliminates entity hallucination when using sufficiently capable models
- The framework successfully handles both structured graph data and unstructured operational notes/events

## Why This Works (Mechanism)

### Mechanism 1: Tool-Constrained Grounding
Constraining the LLM to interact with infrastructure data solely through a typed MCP tool interface appears to eliminate entity hallucination and enforce safety boundaries, provided the model has sufficient instruction-following capability. The system replaces direct graph access with primitives (e.g., LOOKUPSERVICE, GETEVENTS), forcing the LLM to synthesize answers strictly from tool outputs. This decouples reasoning from data integrity, creating an auditable safety boundary where the agent physically cannot access unexposed data.

### Mechanism 2: Proceduralizing Diagnostic Logic
Diagnostics emerge from a strict "Investigation Protocol" rather than free-form reasoning, guiding the model to emulate traditional graph traversal algorithms. The paper defines a deterministic workflow: Extract Service → Resolve → Get Implementation → Analyze Evidence → Get Impact → Publish. By forcing the LLM to follow this chain, the system ensures the agent covers all dependency paths just as a hardcoded algorithm would.

### Mechanism 3: Semantic Analysis of Unstructured Evidence
The system leverages the LLM's semantic capabilities to interpret unstructured operational data (Notes/Events), allowing it to identify root causes that purely structured graph traversals would miss. Unlike rule-based engines that rely on structured alarms, the agent retrieves Note or Event objects containing natural language (e.g., "memory leak," "scheduled maintenance") and analyzes this text to discriminate between healthy and faulty resources.

## Foundational Learning

**Concept: Infrastructure Ontology (SID Model)**
- Why needed here: The agent navigates a specific graph structure. You must distinguish between Services (what is sold), Resources (what runs it), and Parties (who consumes it) to understand the traversal logic.
- Quick check question: If a server crashes, does it directly affect a Party or a Service? (Answer: It affects the Resource, which implements the Service, which is allocated to the Party).

**Concept: Tool Use / Function Calling**
- Why needed here: The agent does not "think" about the graph directly; it acts via API calls. Understanding the tool definitions (Definition 8) is required to debug why an agent might fail to find data.
- Quick check question: Which tool would the agent call to find out which customers are affected by a crashed router? (Answer: GETUSAGE on the impacted Services).

**Concept: Service Implementation (σ) vs. Resource Impact (ρ)**
- Why needed here: These are the core mathematical mappings for RCA. σ(s): Traces down from service to resources (to find the cause). ρ(r): Traces up from resource to services (to find the blast radius).
- Quick check question: To perform Root Cause Analysis on a degraded email service, do you compute σ or ρ? (Answer: σ(EmailService)).

## Architecture Onboarding

**Component map:** LLM Agent -> MCP Server -> Infrastructure Ontology
- **LLM Agent:** The reasoning engine (e.g., Claude Haiku, GPT-OSS)
- **MCP Server:** The middleware exposing typed tools (GETIMPACTEDSERVICES, PUBLISH)
- **Infrastructure Ontology:** The backend graph (Digital Twin/Neo4j) storing the Service/Resource/Party topology

**Critical path:** The RCA Investigation Protocol (Section 5.1). The agent must sequentially: Lookup Service → Get Implementation → Analyze Events/Notes → Get Impact → Publish.

**Design tradeoffs:**
- **Model Size:** Larger models (Claude, GPT-120B) ensure 99-100% protocol adherence and grounding but are slower/costlier. Smaller models (Llama 8B) are fast but suffer 21% investigation failure rates due to malformed tool calls.
- **Hybrid vs. Pure Graph:** The system trades raw graph access for "safety" via the MCP abstraction layer.

**Failure signatures:**
- **Tool Hallucination:** Agent invents a tool name (e.g., GETCUSTOMER instead of GETUSAGE)
- **Entity Hallucination:** Agent invents a Resource ID not returned by GETIMPLEMENTATION
- **Protocol Drift:** Agent skips the impact analysis phase and publishes early

**First 3 experiments:**
1. Protocol Adherence Test: Implement a minimal MCP server with stub data. Run "Test Case 1" (Simple Storage Failure) to see if the model can follow the 6-step sequence without errors.
2. Unstructured Data Validation: Run "Test Case 3" (Server Issue via Note) to verify if the model can extract a root cause from a text note rather than a structured event.
3. Temporal Reasoning Stress Test: Run "Test Case 9" (Multiple Events) to check if the model identifies the recent incident versus the old maintenance event.

## Open Questions the Paper Calls Out
- How can the investigation protocol be augmented to handle temporal dependencies and resolution events to prevent misidentification of resolved faults as active root causes?
- What pre-filtering or summarization strategies are required to maintain diagnostic accuracy when service implementations σ(s) exceed the LLM's context window?
- Can the framework be extended to output calibrated confidence scores rather than binary conclusions to better support human-in-the-loop validation?

## Limitations
- The framework evaluates on synthetic, controlled ontologies and may struggle with real-world dynamic topologies and noisy operational data
- Larger models demonstrate near-perfect protocol adherence but at significantly higher computational cost
- The system explicitly acknowledges limitations in handling temporal ambiguity in Events/Notes
- Cost-benefit trade-off for deploying large models versus engineering more robust protocols for smaller models is not quantified

## Confidence
**High Confidence:** The core mechanism of tool-constrained grounding (MCP) to prevent hallucination and ensure safety is well-supported by the 0% hallucination rate in Claude Haiku and static analyzer results.

**Medium Confidence:** The reported 100% accuracy across all models is based on synthetic benchmarks; generalization to real-world data is uncertain.

**Low Confidence:** The feasibility of maintaining operational safety and protocol adherence in production environments with high-volume, noisy data is not demonstrated.

## Next Checks
1. **Real-World Data Validation:** Deploy the agent on real-world incident datasets (anonymized telecom logs or public datacenter failure reports) to test performance on unstructured, noisy, and temporally complex evidence against traditional diagnostic engines.

2. **Dynamic Topology Stress Test:** Create test cases where the infrastructure graph dynamically changes during an incident to measure the agent's ability to maintain accurate context without a global state snapshot.

3. **Cost-Performance Frontier Analysis:** Systematically evaluate the trade-off between model size, inference cost, and protocol adherence to identify a "minimum viable model" achieving >95% protocol adherence at lower cost than larger models.