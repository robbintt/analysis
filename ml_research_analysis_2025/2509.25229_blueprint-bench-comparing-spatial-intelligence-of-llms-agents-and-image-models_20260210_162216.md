---
ver: rpa2
title: 'Blueprint-Bench: Comparing spatial intelligence of LLMs, agents and image
  models'
arxiv_id: '2509.25229'
source_url: https://arxiv.org/abs/2509.25229
tags:
- image
- floor
- room
- generation
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Blueprint-Bench evaluates spatial reasoning in AI by converting
  apartment photos into 2D floor plans. The benchmark tests whether models can infer
  room layouts, connectivity, and scale from images despite these tasks being outside
  their training scope.
---

# Blueprint-Bench: Comparing spatial intelligence of LLMs, agents and image models

## Quick Facts
- arXiv ID: 2509.25229
- Source URL: https://arxiv.org/abs/2509.25229
- Reference count: 5
- Most models perform at or below random baseline on spatial reasoning tasks

## Executive Summary
Blueprint-Bench evaluates spatial reasoning in AI by converting apartment photos into 2D floor plans. The benchmark tests whether models can infer room layouts, connectivity, and scale from images despite these tasks being outside their training scope. It scores similarity between generated and ground-truth floor plans using room connectivity graphs and size rankings. We tested leading LLMs, image generation models, and agent systems on 50 apartments with ~20 interior images each. Results show most models perform at or below random baseline, with image generation models particularly struggling with instruction following. Agent-based iterative refinement showed no meaningful improvement over single-pass generation. Human performance remains substantially superior. Blueprint-Bench provides the first numerical framework for comparing spatial intelligence across different model architectures.

## Method Summary
Blueprint-Bench uses apartment photographs as inputs and requires models to generate 2D floor plans following strict formatting rules. The scoring algorithm extracts connectivity graphs and size rankings from both ground truth and generated floor plans, then computes weighted similarity scores. The benchmark tests LLMs, image generation models, and agent systems on 50 apartments with ~20 images each, comparing performance against random baseline and human evaluation.

## Key Results
- Most models perform at or below random baseline on spatial reasoning tasks
- Image generation models struggle particularly with instruction following, consistently including forbidden details like furniture and windows
- Agent-based iterative refinement showed no meaningful improvement over single-pass generation
- Human performance remains substantially superior to all AI models tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Blueprint-Bench isolates spatial reasoning capability by using in-distribution inputs (photographs) with out-of-distribution output requirements (floor plans).
- Mechanism: The benchmark presents apartment photographs—common in training data—but requires models to transform these into structured 2D floor plans via SVG code or image generation. This transformation demands inferring room layouts, connectivity relationships, and consistent scale—capabilities not explicitly trained.
- Core assumption: If models possessed genuine spatial intelligence, they could perform this transformation even without specific training, similar to how humans can.
- Evidence anchors:
  - [abstract] "While the input modality (photographs) is well within the training distribution... the task of spatial reconstruction requires genuine spatial intelligence"
  - [section 1] "This task is very alien to an LLM... ARC is brilliant because it is one of the few benchmarks that can demonstrate a blind spot in LLM capabilities"
  - [corpus] NavSpace benchmark similarly isolates spatial perception by controlling for semantic understanding in navigation tasks
- Break condition: If models were specifically trained on floor plan generation tasks, results would reflect training rather than emergent spatial reasoning.

### Mechanism 2
- Claim: The structured output format (9 rules) enables automated, reproducible scoring while revealing instruction-following failures.
- Mechanism: Floor plans must follow strict formatting rules (black walls 3px wide, green doors, red room center dots 10×10px). The scoring algorithm uses computer vision to extract connectivity graphs and size rankings, then computes weighted similarity (50% edge overlap, 20% degree correlation, etc.).
- Core assumption: Models that cannot follow formatting rules would also struggle with spatial reasoning even if format constraints were removed.
- Evidence anchors:
  - [section 2.1] Lists 9 explicit formatting rules
  - [section 3] "NanoBanana particularly struggled with the rule of ignoring all other details. It constantly included furniture, windows, etc."
  - [corpus] Weak/missing—no corpus papers specifically address the tradeoff between instruction-following constraints and spatial reasoning evaluation
- Break condition: If a model had strong spatial reasoning but poor instruction following, this benchmark would understate its capabilities.

### Mechanism 3
- Claim: Agent-based iterative refinement failed to improve performance because agents did not meaningfully engage in multi-step spatial reasoning.
- Mechanism: Agents were given Docker environments with images, could view images multiple times, and iterate on drawings. Claude Code did iterate (identifying issues like incorrect room counts), while Codex CLI did not. Neither achieved above-baseline results.
- Core assumption: Assumption: The limitation is in spatial reasoning capability itself, not in the single-pass output format.
- Evidence anchors:
  - [section 3] "Claude Code with Claude 4 Opus... did show this behavior; see Figure 8... However, it still wasn't very good"
  - [section 3] Codex "never even looked at the image it created before submitting"
  - [corpus] iVISPAR and ORIGAMISPACE similarly find that interactive/iterative setups don't automatically solve spatial reasoning deficits in VLMs
- Break condition: If agents had better spatial feedback mechanisms (e.g., 3D visualization tools), iterative approaches might show gains.

## Foundational Learning

- Concept: **Connectivity graphs as spatial representation**
  - Why needed here: The scoring algorithm converts floor plans into graphs where nodes=rooms and edges=door connections. Understanding this abstraction is essential for interpreting results.
  - Quick check question: Given a floor plan with kitchen, living room, and bathroom, can you draw which rooms connect to which?

- Concept: **In-distribution inputs vs. out-of-distribution tasks**
  - Why needed here: Blueprint-Bench's key insight is using familiar inputs (photos) for unfamiliar tasks (floor plans). This distinguishes spatial intelligence from mere pattern recognition.
  - Quick check question: Why would GPT-5 perform well on image captioning but poorly on Blueprint-Bench if both use photographs?

- Concept: **Weighted composite scoring metrics**
  - Why needed here: The final score combines 6 components with different weights. Understanding this helps prioritize what matters (connectivity) vs. what matters less (door orientation).
  - Quick check question: If a model gets room sizes wrong but connectivity perfect, would its score be closer to 0.5 or 0.8?

## Architecture Onboarding

- Component map: Input: ~20 apartment images → Model (LLM/image-gen/agent) → Floor plan output → Scoring pipeline: Image → CV extraction (red blobs, flood-fill, door detection) → JSON (connectivity graph, size rankings, door data) → Weighted similarity computation → Score [0,1]

- Critical path:
  1. Model must follow 9 formatting rules exactly (or scoring fails)
  2. Extraction algorithm detects room centers via HSV filtering + flood-fill from red dots
  3. Connectivity extracted by scanning wall boundaries for green pixels
  4. Score = weighted average of 6 similarity components

- Design tradeoffs:
  - **Strict formatting vs. expressiveness**: Rules enable robust scoring but penalize models with spatial reasoning but poor instruction following
  - **Size-rank labeling vs. room-type labeling**: Size-based IDs avoid LLM prior contamination (e.g., assuming living rooms are largest) but cascade errors if size ranking is wrong
  - **Connectivity focus vs. shape accuracy**: Ignores room shape to avoid unpredictable penalties from wall-distance metrics

- Failure signatures:
  - Score ≈ 0 with valid format → Complete spatial misunderstanding
  - Scores below random baseline → Model generating generic floor plans without using image evidence
  - "Striped" patterns in results → Image models including forbidden details (furniture, windows, windows)
  - Agent doesn't view generated output → Not using iterative capability

- First 3 experiments:
  1. **Baseline sanity check**: Run extraction/scoring on ground truth floor plans with synthetic noise (permuted room IDs, added/removed doors) to validate scoring sensitivity
  2. **Format compliance test**: Give a strong LLM (GPT-5) a single room image and explicit floor plan text description; test if it can produce a rule-compliant output (isolates instruction-following from spatial reasoning)
  3. **Human comparison protocol**: Replicate human evaluation with 12-apartment subset, logging time-per-image and revision patterns to establish whether agent iteration strategies match human strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Scoring sensitivity may overstate performance of models that get connectivity right but misrepresent room shapes and sizes
- Human baseline had small sample size (n=3) and uncontrolled conditions, potentially biasing results
- Some spatial relationships may be occluded or impossible to infer from photographs alone, creating inherent performance ceilings

## Confidence
- Spatial reasoning deficit is real and general (High)
- Agent iteration provides no benefit (Medium)
- Image models struggle with instruction following (High)

## Next Checks
1. Generate synthetic apartment layouts with known connectivity graphs, render as photographs, and test model reconstruction to validate whether poor performance reflects spatial reasoning deficits or inherent limitations of image-to-floor-plan transformation

2. Replicate human evaluation with strict time limits (e.g., 5 minutes per apartment) and larger sample size (n=10+) to establish whether current human performance represents ceiling or whether iterative refinement could improve results

3. Apply same scoring methodology to simpler spatial tasks like room adjacency prediction or room size comparison to isolate whether spatial reasoning deficit is specific to floor plan generation or reflects more general spatial understanding limitations