---
ver: rpa2
title: Unlocking Speech Instruction Data Potential with Query Rewriting
arxiv_id: '2507.08603'
source_url: https://arxiv.org/abs/2507.08603
tags:
- speech
- https
- text
- data
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building high-quality speech
  instruction datasets for training end-to-end Large Speech Language Models (LSLMs).
  The authors propose a query rewriting framework with multi-LLM knowledge fusion,
  along with multi-agent annotation and data quality validation.
---

# Unlocking Speech Instruction Data Potential with Query Rewriting

## Quick Facts
- **arXiv ID:** 2507.08603
- **Source URL:** https://arxiv.org/abs/2507.08603
- **Reference count:** 25
- **Primary result:** Query rewriting framework increases speech instruction dataset usability from 72% to 93% with 5% improvement in semantic similarity

## Executive Summary
This paper addresses the challenge of building high-quality speech instruction datasets for training end-to-end Large Speech Language Models (LSLMs). The authors propose a multi-LLM query rewriting framework combined with multi-agent annotation and data quality validation. By using multiple LLMs to rewrite text instructions for TTS compatibility and employing multiple ASR and embedding models for validation, the approach significantly improves data usability while maintaining semantic integrity. The method demonstrates particular advantages in handling complex rewriting tasks requiring context-aware understanding and knowledge integration.

## Method Summary
The approach uses three instruction-tuned LLMs (Llama-3-8B, Phi-3-small, Qwen2-7B) to independently rewrite text instructions into TTS-compatible formats. Each rewritten text, along with the original, is synthesized using Parler-TTS with GPT-4-generated speaker descriptions. A multi-agent validation pipeline then uses three ASR models and three embedding models to compute similarity scores between synthesized speech transcriptions and original text. The highest-quality candidate exceeding a 0.9 similarity threshold is selected for the dataset. Failed cases are retried using a knowledge fusion model fine-tuned via LoRA on successful rewrite pairs.

## Key Results
- Data usability increases from 72% to 93% through the rewriting and validation pipeline
- Semantic similarity between synthesized speech and original text improves by 5%
- Multi-agent validation achieves average WER of 8.36% versus 9.64-10.21% for individual ASR models
- Knowledge fusion via LoRA fine-tuning successfully handles complex cases that individual LLMs fail

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-agent annotation reduces single-model ASR errors when validating synthesized speech quality.
- **Mechanism:** Three ASR models with different architectures independently transcribe synthesized speech; multiple embedding models compute similarity to original text; the maximum similarity score is selected as the quality metric. Orthogonal ASR errors are avoided through diversity.
- **Core assumption:** ASR models with similar performance but different architectures will make different errors on edge cases, enabling cross-validation.
- **Evidence anchors:** Section 4.1 shows multi-agent method achieves average WER of 8.36% vs. 9.64-10.21% for single ASR models.

### Mechanism 2
- **Claim:** Multiple LLMs performing zero-shot query rewriting capture complementary linguistic transformations for TTS compatibility.
- **Mechanism:** Llama-3-8B-Instruct, Phi-3-small-8k-instruct, and Qwen2-7B-Instruct independently rewrite text instructions; TTS generates candidate speech for each; the highest-quality output is selected via the multi-agent validation pipeline.
- **Core assumption:** Different LLMs trained on different data will produce meaning-preserving rewrites that vary in TTS compatibility.
- **Evidence anchors:** Section 5.4 shows "the performance of using a single LLM is inferior to that of using multiple LLMs together."

### Mechanism 3
- **Claim:** Knowledge fusion via LoRA fine-tuning on successful rewrites enables handling of complex cases that individual LLMs fail.
- **Mechanism:** Successful rewrite pairs (quality > 0.9) are collected; Meta-Llama-3-8B-Instruct is fine-tuned using LoRA on these pairs; the fused model re-attempts failed cases; new outputs undergo same validation pipeline.
- **Core assumption:** Patterns from successful rewrites transfer to structurally similar failed cases.
- **Evidence anchors:** Section 4.3 describes the knowledge fusion training loop with LoRA (r=8, α=16).

## Foundational Learning

- **Concept: Embedding Space Similarity for Semantic Equivalence**
  - **Why needed here:** Traditional WER fails to capture semantic similarity when surface forms differ; the paper uses embedding cosine similarity as the quality metric.
  - **Quick check question:** Can you explain why two texts with identical meaning but different word choices might have high embedding similarity but high WER?

- **Concept: Zero-Shot Generalization in LLMs**
  - **Why needed here:** Query rewriting must generalize to unseen instruction formats without task-specific training.
  - **Quick check question:** What makes an LLM suitable for zero-shot text transformation tasks?

- **Concept: LoRA Fine-Tuning**
  - **Why needed here:** Knowledge fusion requires efficient adaptation; LoRA enables training with limited GPU memory (r=8, α=16, 4 A40 GPUs with gradient checkpointing).
  - **Quick check question:** How does LoRA reduce trainable parameters compared to full fine-tuning?

## Architecture Onboarding

- **Component map:** Original Text → [3 LLM Rewriters] → Candidate Texts (4 variants) → GPT-4 Style Description → [TTS Model] → Candidate Speech → [3 ASR Models] → Recognition Results → [3 Embedding Models] → Similarity Scores → Max Similarity Selection → Quality Threshold (α=0.9) → [Pass] → Speech Instruction Dataset / [Fail] → Knowledge Fusion Training → Retry

- **Critical path:** ASR → Embedding similarity calculation; if this fails, all downstream filtering is unreliable. Validate ASR diversity before production deployment.

- **Design tradeoffs:**
  - Threshold α = 0.9: Higher threshold improves quality but reduces data yield; paper shows 93% pass rate achieved.
  - Multi-speaker vs. Single-speaker: Multi-speaker (192 styles via GPT-4) improves diversity but increases synthesis cost ~4x (534 vs. 82 GPU hours).
  - Three LLMs vs. single: Tripled inference cost but quality gains diminish without orthogonality.

- **Failure signatures:**
  - Low pass rate (<80%) despite rewriting: Check ASR model agreement; likely systematic TTS issue with specific text patterns.
  - Embedding similarity consistently high but downstream task performance poor: Semantic drift in rewrites; lower threshold or add NLI validation.
  - Knowledge fusion shows no improvement: Insufficient successful examples or failed cases lack common patterns.

- **First 3 experiments:**
  1. **Baseline validation:** Run original text through TTS → ASR pipeline; measure pass rate and WER. Compare to Table 2 "Original" row (72% average pass rate).
  2. **Single LLM ablation:** Test each LLM (Phi-3, Qwen2, Llama-3) independently; verify orthogonality by checking which samples each passes/fails. Expect non-overlapping failure modes.
  3. **Threshold sensitivity:** Run full pipeline with α ∈ {0.85, 0.90, 0.95}; measure pass rate vs. downstream task ROUGE-L. Confirm α=0.90 optimal per Table 3.

## Open Questions the Paper Calls Out
- How can query rewriting frameworks be adapted to handle long-form spoken instructions exceeding 100 words while maintaining semantic integrity and TTS compatibility?
- Can the multi-LLM rewriting and multi-agent validation framework generalize effectively to low-resource languages where high-performance ASR and TTS models are scarce?
- Does the reliance on embedding-space similarity (SIM) for validation fail to capture subtle factual distortions or hallucinations introduced during the LLM rewriting process?

## Limitations
- The framework is optimized for short instructions (under 100 words) and may not scale to long-form speech
- Heavy computational requirements with 534 GPU hours for synthesis across 192 speaker styles
- Limited validation of knowledge fusion generalization to completely unseen instruction types

## Confidence
- **High confidence:** Multi-LLM rewriting approach with quality threshold validation is technically sound and the 72% to 93% usability improvement is well-documented
- **Medium confidence:** Knowledge fusion component shows logical coherence but limited empirical validation across diverse instruction types
- **Low confidence:** Long-term stability and generalization to completely different domains remains unproven

## Next Checks
1. **Cross-dataset generalization test:** Apply the pipeline to a completely different instruction dataset (e.g., mathematical reasoning or code generation) and measure whether the 93% pass rate holds
2. **Error pattern analysis:** Conduct systematic analysis of the 7% of samples that fail quality threshold despite rewriting to identify fundamental limitations
3. **Computational efficiency benchmark:** Measure end-to-end processing time and cost for the full pipeline and compare against simpler single-model approaches