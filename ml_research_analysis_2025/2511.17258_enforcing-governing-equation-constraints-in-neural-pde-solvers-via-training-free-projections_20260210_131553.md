---
ver: rpa2
title: Enforcing governing equation constraints in neural PDE solvers via training-free
  projections
arxiv_id: '2511.17258'
source_url: https://arxiv.org/abs/2511.17258
tags:
- projection
- constraint
- constraints
- linear
- pino
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of enforcing governing equation
  constraints in neural PDE solvers, which often violate physical laws despite good
  performance on standard metrics. The authors propose two training-free, post-hoc
  projection methods to enforce these constraints on approximate solutions: a nonlinear
  optimization-based projection using LBFGS and a local linearization-based projection
  using Jacobian-vector products.'
---

# Enforcing governing equation constraints in neural PDE solvers via training-free projections

## Quick Facts
- **arXiv ID:** 2511.17258
- **Source URL:** https://arxiv.org/abs/2511.17258
- **Reference count:** 35
- **Primary result:** Two training-free projection methods (LBFGS nonlinear optimization and Jacobian-vector linearization) significantly reduce constraint violations and improve accuracy in neural PDE solvers.

## Executive Summary
This paper addresses the challenge of enforcing governing equation constraints in neural PDE solvers, which often violate physical laws despite good performance on standard metrics. The authors propose two training-free, post-hoc projection methods to enforce these constraints on approximate solutions: a nonlinear optimization-based projection using LBFGS and a local linearization-based projection using Jacobian-vector products. The method formulates the projection as finding a physically consistent solution u that satisfies the PDE constraints h(u) = c given the neural network output รป.

Experiments on three dynamical systems (Lorenz ODEs, Kuramoto-Shivashinsky PDE, and Navier-Stokes PDE) demonstrate that both projections substantially reduce constraint violations and improve accuracy over physics-informed baselines. LBFGS achieves near-perfect constraint satisfaction when the initial approximation is reasonable, while linearization-based methods offer computational efficiency at the cost of accuracy. Results show constraint violation reductions of 70% or more and significant improvements in Mean Squared Error across all tested systems.

## Method Summary
The paper introduces a post-hoc projection framework that enforces governing equation constraints on neural PDE solver outputs. The approach treats the neural network output รป as an initial guess and solves an optimization problem to find a physically consistent solution u that satisfies the PDE constraints h(u) = c. Two projection methods are proposed: a nonlinear optimization approach using LBFGS and a linearization-based approach using Jacobian-vector products. The nonlinear method provides exact constraint satisfaction but is computationally expensive, while the linearization method offers computational efficiency with approximate constraint satisfaction. The projections are training-free and can be applied to any neural PDE solver output without modifying the training process.

## Key Results
- LBFGS nonlinear optimization achieves near-perfect constraint satisfaction (violations reduced by 70% or more) across all tested systems when initial approximations are reasonable
- Linearization-based projection offers 10-100x computational speedup compared to LBFGS but with higher residual constraint violations (10^-3 to 10^-4 range)
- Both methods significantly improve Mean Squared Error compared to physics-informed baselines across Lorenz ODEs, Kuramoto-Shivashinsky PDE, and Navier-Stokes PDE
- Constraint violation reductions exceed 70% across all test cases, with LBFGS providing the most accurate results

## Why This Works (Mechanism)
The projection methods work by treating the neural network output as an initial guess and iteratively adjusting it to satisfy the governing equations. The LBFGS method performs nonlinear optimization in the space of solutions, effectively "correcting" the neural network output to find a physically consistent state. The linearization approach approximates the constraint satisfaction problem locally using Jacobian-vector products, providing a computationally efficient but approximate solution. Both methods leverage the fact that neural networks can produce reasonable approximations that can be refined to satisfy exact physical constraints.

## Foundational Learning

**Partial Differential Equations:** Why needed - Understanding the governing equations that describe physical systems. Quick check - Can you identify the PDE form h(u) = c in the proposed method?

**Neural PDE Solvers:** Why needed - The method builds on existing neural approaches to solving PDEs. Quick check - Can you explain how neural networks approximate PDE solutions?

**Optimization Methods:** Why needed - The projection relies on optimization to enforce constraints. Quick check - Can you describe the difference between LBFGS and gradient descent?

**Jacobian Computation:** Why needed - Linearization method requires efficient Jacobian-vector products. Quick check - Can you explain why Jacobian-vector products are more efficient than full Jacobian computation?

**Constraint Satisfaction:** Why needed - The core problem of ensuring physical consistency. Quick check - Can you define what it means for a solution to satisfy PDE constraints?

## Architecture Onboarding

**Component Map:** Neural Network Output (รป) -> Projection Method (LBFGS/Linearization) -> Constrained Solution (u)

**Critical Path:** The critical path involves computing the initial neural network approximation, evaluating constraint violations, and applying the projection method to enforce physical consistency. The most computationally intensive step is the constraint evaluation and gradient computation.

**Design Tradeoffs:** Nonlinear optimization (LBFGS) provides exact constraint satisfaction but is computationally expensive, while linearization offers speed but with approximate solutions. The choice depends on the application's tolerance for constraint violations versus computational resources.

**Failure Signatures:** Linearization methods fail when the constraint landscape is highly nonlinear or the initial guess is poor, leading to constraint violations. LBFGS may fail to converge for initial guesses far from feasible solutions or for highly constrained problems.

**First Experiments:** 1) Test projection on a simple ODE system with known analytical solution, 2) Compare constraint satisfaction between LBFGS and linearization on a moderate-sized PDE, 3) Evaluate computational overhead of each method on increasing problem sizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can efficient differentiable projection operators be developed to enable end-to-end training of neural PDE solvers?
- Basis in paper: The conclusion states that current methods "cannot be directly integrated as differentiable layers within neural networks due to the computational cost of backpropagation," and lists "developing differentiable projection operators" as future work.
- Why unresolved: The optimization process (specifically LBFGS) used for projection is computationally expensive to unroll and differentiate through, currently limiting the method to post-hoc application.
- What evidence would resolve it: A projection implementation that supports gradient flow with acceptable memory/time overhead, demonstrating improved convergence or accuracy when used as a trainable layer.

### Open Question 2
- Question: Can these projection techniques be effectively extended to generative models to ensure temporal coherence in sampled trajectories?
- Basis in paper: The conclusion identifies "extending these techniques to generative models" as an opportunity for "improving temporal coherence by projecting sampled trajectories onto physically consistent manifolds."
- Why unresolved: The current study validates the method on deterministic solvers (MLP, FNO, PINO) but does not test it on stochastic sampling paths or generative architectures.
- What evidence would resolve it: Experiments applying the projection operators to outputs from generative architectures (e.g., diffusion models) showing reduced constraint violations in sampled solutions.

### Open Question 3
- Question: Would iterative relinearization approaches (like Sequential Quadratic Programming) offer a better trade-off between the accuracy of nonlinear projection and the speed of linear projection?
- Basis in paper: The analysis of the constraint landscape (Fig. 2) shows linear approximations degrade quickly, leading the authors to suggest "iterative relinearization approaches... could improve performance if implemented efficiently."
- Why unresolved: The paper evaluates single-step linearization and full nonlinear optimization, but does not implement intermediate iterative linear methods.
- What evidence would resolve it: A comparison of convergence speed and constraint satisfaction accuracy between standard LBFGS, single-step linear projection, and an SQP-style iterative linear solver.

## Limitations
- Linearization methods show significant variability in constraint satisfaction, with violations remaining at 10^-3 to 10^-4 levels in some cases
- Performance heavily depends on the quality of initial neural network approximations, with poor initial guesses leading to convergence failures
- LBFGS projection computational overhead can be substantial for large-scale problems, limiting real-time applications

## Confidence
- **High**: The core methodology and mathematical formulation are sound and well-presented
- **Medium**: The experimental results show consistent improvements, but the generalizability across diverse PDE types needs further validation
- **Low**: The computational complexity analysis and scalability claims require more rigorous examination

## Next Checks
1. Test the projection methods on higher-dimensional PDEs and systems with multiple coupled equations to assess scalability
2. Evaluate performance when initial neural network approximations are deliberately degraded (e.g., through noisy training data or limited training)
3. Benchmark the computational overhead against alternative constraint enforcement methods across varying problem sizes and complexities