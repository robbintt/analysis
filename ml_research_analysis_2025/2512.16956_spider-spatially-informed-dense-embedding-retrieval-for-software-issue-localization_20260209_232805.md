---
ver: rpa2
title: 'SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization'
arxiv_id: '2512.16956'
source_url: https://arxiv.org/abs/2512.16956
tags:
- retrieval
- spider
- recall
- code
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SpIDER, a dense embedding retrieval approach
  that incorporates graph-based spatial exploration and LLM-based reasoning to improve
  code function localization. The method first retrieves top-K functions by semantic
  similarity, then explores the local graph neighborhood around the top-C centers
  to find relevant functions that are structurally proximate but may have lower semantic
  similarity scores.
---

# SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization

## Quick Facts
- arXiv ID: 2512.16956
- Source URL: https://arxiv.org/abs/2512.16956
- Reference count: 40
- Primary result: SpIDER improves recall and accuracy by at least 13% and 14% over standard dense retrieval across Python, Java, JavaScript, and TypeScript

## Executive Summary
SpIDER addresses code localization by combining dense embedding retrieval with graph-based spatial exploration. The method retrieves top-K functions by semantic similarity, explores the local graph neighborhood around top-C centers, and uses an LLM to filter candidates within a fixed retrieval budget. Experiments on a new multilingual benchmark show consistent improvements in recall and accuracy across datasets and languages, with gains preserved after LLM reranking. The approach leverages structural proximity of buggy functions and semantic filtering to enhance retrieval quality.

## Method Summary
SpIDER augments dense embedding retrieval with graph-based spatial exploration and LLM-based reasoning. It first retrieves top-K functions by semantic similarity, then explores the local graph neighborhood around the top-C centers using BFS along "contains" edges. Candidates are filtered in two stages: first by semantic similarity (top-N), then by an LLM binary selector. The final set is constructed within a fixed retrieval budget K by replacing low-ranked semantic candidates with validated neighbors. The method is evaluated on a new multilingual benchmark covering Python, Java, JavaScript, and TypeScript.

## Key Results
- SpIDER improves recall and accuracy by at least 13% and 14% over standard dense retrieval methods
- Improvements are consistent across all languages (Python, Java, JavaScript, TypeScript) and datasets
- Gains are preserved after LLM reranking, with hyperparameter tuning (C=5, d=4, N=500, K=20) providing optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Spatial Locality Exploitation via Graph Neighborhoods
Buggy functions cluster structurally in codebases, and top semantic matches often reside near actual targets. By selecting top-C semantic matches as seeds and performing BFS up to depth d along "contains" edges, SpIDER captures structurally proximate functions that may rank lower on semantic similarity alone. This works when relevance is non-negatively correlated with graph proximity and at least one seed lands in the relevant subgraph.

### Mechanism 2: Two-Stage Neighbor Filtering (Semantic + LLM)
Constraining exploration to semantically plausible candidates, then applying LLM reasoning, improves precision without excessive LLM cost. Stage 1 restricts BFS candidates to top-N by semantic score, and Stage 2 uses an LLM binary selector on source code plus issue description to retain relevant neighbors. This two-stage approach acts as a guardrail against semantic drift in expansion.

### Mechanism 3: Fixed-Budget Replacement
Replacing low-ranked semantic candidates with validated neighbors improves recall while preserving retrieval budget K. After LLM selection, validated neighbors are inserted below their centers, displacing the lowest-ranked non-center nodes in the top-K list. This works when displaced nodes are less likely to be relevant than LLM-validated neighbors.

## Foundational Learning

- **Dense embedding retrieval and bi-modal encoders**: Understanding embedding similarity scoring (cos(F(v), F(Q))) is prerequisite for SpIDER's augmentation of dense retrieval. Quick check: Given a query embedding q and candidate embeddings c1, c2, can you compute cosine similarity and rank candidates?
- **Graph traversal (BFS) on code structure**: SpIDER uses BFS along "contains" edges; depth d controls exploration scope. Quick check: For a node v at depth 0, what nodes are at depth 2 along "contains" edges in a typical file→class→function hierarchy?
- **LLM-based binary relevance classification**: Stage 2 filtering relies on LLM returning L(u) ∈ {0, 1}; understanding prompt design and failure modes matters. Quick check: If an LLM returns "relevant" for 80% of candidates regardless of actual relevance, what happens to SpIDER's precision?

## Architecture Onboarding

- **Component map**: Graph construction (Tree-sitter/ast) → nodes (functions, classes, files, dirs) + edges (contains, invokes, imports, inherits) → Semantic retrieval → relevance scores sQ(v) = cos(F(v), F(Q)) → Seed selection → top-C from top-K → Neighborhood exploration → BFS to depth d → Two-stage filtering → top-N intersection + LLM binary selection → Output construction → validated neighbors + remaining top-K by score
- **Critical path**: Graph construction (offline, once per codebase) → embedding computation (offline, once) → per-query: semantic ranking → BFS → LLM filtering → output assembly. LLM calls per center (C calls per query) are the latency bottleneck.
- **Design tradeoffs**: C (centers): More seeds → broader exploration, more LLM calls. d (depth): Deeper → more candidates, higher token cost. N (semantic threshold): Higher N → more candidates reach LLM, higher cost. K (budget): Larger K → higher recall/accuracy, but downstream tasks may have context limits.
- **Failure signatures**: Near-zero neighbors passing LLM filter → check if N is too restrictive or LLM prompt mis-specified. High false positives → LLM selector may be too permissive. Low recall despite deep exploration → seeds may not land in relevant subgraph. Multi-language graph construction errors → JavaScript/TypeScript class/function parsing may miss split definitions.
- **First 3 experiments**: 1) Replicate Figure 2 on SWE-PolyBench Python split comparing DER vs. SpIDER at K=10, 20, 30. 2) Ablate C ∈ {1, 3, 5, 7} with fixed d=4, N=500, K=20 to verify saturation point. 3) Profile token consumption and latency varying d ∈ {2, 4, 6, 8} with N=500, C=5, K=20 on a 50-instance sample.

## Open Questions the Paper Calls Out

### Open Question 1
Does incorporating call-graph edges (e.g., 'invokes', 'imports') outperform the hierarchical 'contains' edges currently used for neighborhood exploration? The paper notes it can incorporate other edges but restricts to 'contains' to capture hierarchical structure. Ablation studies comparing BFS traversal using only 'contains' vs. only 'invokes' vs. mixed graph schemas would resolve this.

### Open Question 2
How does the method perform on polyglot repositories where resolving an issue requires edits across multiple programming languages? The paper's graph construction uses only files written in the "primary programming language," explicitly ignoring files in other languages. Evaluation on multi-language benchmarks where ground truth requires modifying files in both primary and secondary languages would resolve this.

### Open Question 3
Can the LLM-based filtering stage be replaced by a lightweight classifier or static heuristic without sacrificing the observed performance gains? While RQ3 addresses efficiency, ablation studies show token consumption scales linearly with exploration depth and neighborhood size. A comparative study substituting the LLM selector with smaller encoder-based classifiers or static analysis rules would measure performance degradation versus cost savings.

### Open Question 4
Can the insertion strategy be refined to prevent the observed degradation in Mean Reciprocal Rank (MRR)? The paper notes SpIDER may reduce MRR in cases where the first relevant function is a non-center node, as the algorithm ranks neighbors of centers ahead of other semantically similar candidates. Developing a ranking score that combines semantic similarity and graph distance would optimize both Recall and MRR simultaneously.

## Limitations
- Performance hinges on two critical assumptions (structural subgraph existence and LLM selector reliability) that are not empirically validated
- Reliance on proprietary LLM (Claude Sonnet 4) introduces reproducibility challenges
- Graph construction pipeline for JavaScript/TypeScript with split class definitions is mentioned but not detailed sufficiently

## Confidence
- **High Confidence**: Recall and accuracy improvements over dense retrieval baselines are well-supported by Figure 2 across all languages and datasets
- **Medium Confidence**: Claim that structural proximity correlates with relevance relies on indirect evidence from recall gains; LLM filtering effectiveness is assumed rather than validated through ablation
- **Low Confidence**: Generalization to codebases with different structural properties is not explored; fixed-budget replacement strategy's optimality is asserted without comparison to alternatives

## Next Checks
1. Measure the average structural distance between ground truth functions in the test set to quantify actual clustering of relevant functions in the graph
2. Replace the LLM selector with a simple semantic similarity threshold or smaller open-source model to assess marginal contribution of LLM-based filtering
3. Apply SpIDER to a codebase with known structural characteristics that challenge assumptions (e.g., functional programming language repository or highly modular codebase) to evaluate robustness beyond the multilingual benchmark