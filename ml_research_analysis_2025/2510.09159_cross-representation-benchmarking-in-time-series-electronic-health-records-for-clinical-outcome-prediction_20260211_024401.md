---
ver: rpa2
title: Cross-Representation Benchmarking in Time-Series Electronic Health Records
  for Clinical Outcome Prediction
arxiv_id: '2510.09159'
source_url: https://arxiv.org/abs/2510.09159
tags:
- event
- data
- clinical
- stream
- time-series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first cross-representation benchmark for
  Electronic Health Records (EHRs), systematically comparing multivariate time-series,
  event streams, and textual event streams across two clinical settings (ICU and longitudinal
  care) using standardized data curation and evaluation. The authors evaluate appropriate
  model families for each representation and analyze the impact of feature pruning
  based on missingness.
---

# Cross-Representation Benchmarking in Time-Series Electronic Health Records for Clinical Outcome Prediction

## Quick Facts
- arXiv ID: 2510.09159
- Source URL: https://arxiv.org/abs/2510.09159
- Reference count: 33
- This paper presents the first cross-representation benchmark for Electronic Health Records (EHRs), systematically comparing multivariate time-series, event streams, and textual event streams across two clinical settings (ICU and longitudinal care) using standardized data curation and evaluation.

## Executive Summary
This paper introduces the first systematic cross-representation benchmark for electronic health records, comparing multivariate time-series, event streams, and textual event streams across ICU and longitudinal care settings. The authors evaluate appropriate model families for each representation and analyze the impact of feature pruning based on missingness. Through standardized data curation and evaluation, the study reveals that event stream models consistently outperform other representations, with pre-trained models showing high sample efficiency in few-shot settings. The research provides practical guidance for selecting EHR representations based on clinical context and data availability.

## Method Summary
The study establishes a unified benchmark pipeline comparing three EHR representations: multivariate time-series (MT), event streams (ES), and textual event streams (TES). The authors curated two clinical datasets (MIMIC-IV for ICU and eICU for longitudinal care) and designed seven prediction tasks covering mortality, length of stay, and decompensation. For each representation, they selected appropriate model families - Transformer-based models for ES/TES and RNNs for MT. The benchmark includes feature pruning strategies based on missingness rates and evaluates model performance across different data regimes using 5-fold cross-validation. All models were trained from scratch except for pre-trained variants like CLMBR and Llama-3, which were evaluated in few-shot settings.

## Key Results
- Event stream models consistently outperformed multivariate time-series and textual representations across all clinical tasks
- Pre-trained models like CLMBR demonstrated superior sample efficiency in few-shot settings (1-5 patients)
- Feature pruning strategies required clinical context adaptation: sparsity pruning improved ICU predictions while retaining sparse features was critical for longitudinal tasks
- Simple count-based models became competitive with sophisticated pre-trained models as training data increased

## Why This Works (Mechanism)
The superior performance of event stream representations stems from their ability to capture the discrete, irregular nature of clinical events without forcing artificial temporal discretization. Event streams naturally model the asynchronous timing of medical interventions and observations, preserving temporal gaps and event sequences that carry clinical significance. The Transformer architectures, particularly when pre-trained on large EHR corpora, learn to recognize clinically meaningful patterns across diverse event types and time scales. Feature pruning based on missingness removes noise from unreliable measurements while preserving clinically relevant sparse events that may indicate critical interventions or diagnoses.

## Foundational Learning
- **EHR Data Representations**: Understanding multivariate time-series, event streams, and textual formats is crucial for selecting appropriate modeling approaches. Quick check: Can you explain when to use each representation for a given clinical task?
- **Clinical Event Modeling**: Events in healthcare are inherently asynchronous and irregular, requiring specialized temporal modeling. Quick check: How does event stream modeling differ from fixed-interval time-series approaches?
- **Feature Sparsity in Clinical Data**: Medical measurements often have highly variable completeness across patients and time. Quick check: What percentage of features in your dataset have >50% missing values?
- **Few-shot Learning in Healthcare**: Clinical data scarcity makes sample-efficient models critical for real-world deployment. Quick check: What's the minimum number of training examples needed for your model to achieve baseline performance?
- **Representation Transfer Learning**: Pre-trained models on large EHR corpora can generalize to downstream clinical tasks. Quick check: How does pre-training on diverse clinical data improve performance on specific prediction tasks?

## Architecture Onboarding

**Component Map**: Data Curation -> Feature Engineering -> Model Selection -> Training Pipeline -> Evaluation Framework

**Critical Path**: Feature selection and representation choice directly determine model family selection, which then drives training strategy and hyperparameter optimization. The evaluation framework must be consistent across all representations to enable fair comparison.

**Design Tradeoffs**: Multivariate time-series offers temporal continuity but requires imputation and may lose event timing information. Event streams preserve timing but lose measurement precision. Textual representations enable pre-training but add tokenization complexity. Feature pruning reduces noise but risks removing rare but important clinical signals.

**Failure Signatures**: Poor performance with multivariate time-series often indicates inadequate imputation strategies or loss of temporal patterns through discretization. Event stream failures typically stem from insufficient sequence length or inappropriate tokenization. Feature pruning failures occur when clinically important rare events are removed as "sparse."

**First 3 Experiments**:
1. Compare baseline multivariate time-series RNN with event stream Transformer on a simple mortality prediction task
2. Evaluate feature pruning impact by comparing performance with different missingness thresholds
3. Test pre-trained model sample efficiency by training with 1, 5, and 10 patients versus full dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation covers only two clinical settings (ICU and longitudinal care), limiting broader applicability across different medical domains
- Feature pruning strategies, while effective in these settings, may not translate to specialties with different data sparsity patterns
- The relatively small number of tasks (7) constrains the robustness of cross-task conclusions

## Confidence

**High confidence**: The finding that event stream representations outperform multivariate time-series and textual formats across multiple clinical tasks is well-supported by the experimental results. The observed superiority of pre-trained models like CLMBR in few-shot settings is consistently demonstrated.

**Medium confidence**: The recommendation for feature pruning strategies shows promise but requires validation across diverse clinical contexts. The identified trade-off between sparsity pruning in ICU versus longitudinal settings is plausible but needs broader testing.

**Low confidence**: The practical guidance for representation selection based on data regime is promising but preliminary, as the study's data volumes may not reflect real-world clinical deployment scenarios.

## Next Checks

1. Extend the benchmark to additional clinical domains (e.g., oncology, chronic disease management) to validate the generalizability of representation performance patterns across diverse medical contexts.

2. Evaluate model performance with substantially larger datasets (>100,000 patients) to confirm whether simple count-based models truly become competitive with pre-trained models at scale.

3. Implement real-world deployment studies to assess whether benchmark performance translates to clinical utility, particularly examining the practical impact of different feature pruning strategies on model interpretability and clinical decision support.