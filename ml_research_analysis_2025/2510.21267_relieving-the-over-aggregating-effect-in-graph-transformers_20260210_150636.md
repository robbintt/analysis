---
ver: rpa2
title: Relieving the Over-Aggregating Effect in Graph Transformers
arxiv_id: '2510.21267'
source_url: https://arxiv.org/abs/2510.21267
tags:
- attention
- nodes
- graph
- entropy
- wideformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new phenomenon in graph transformers termed
  over-aggregating, where global attention aggregates a large volume of messages with
  low discrimination, leading to message dilution and information loss. To address
  this, the authors propose Wideformer, a plug-and-play method that divides the aggregation
  of source nodes into parallel processes based on attention similarity, and guides
  the model to focus on informative clusters through sorting and weighting.
---

# Relieving the Over-Aggregating Effect in Graph Transformers

## Quick Facts
- **arXiv ID:** 2510.21267
- **Source URL:** https://arxiv.org/abs/2510.21267
- **Reference count:** 40
- **Primary result:** Proposes Wideformer to address over-aggregating in graph transformers, achieving consistent improvements across 13 datasets when integrated with GraphGPS, SGFormer, and Polynormer.

## Executive Summary
This paper identifies a new phenomenon in graph transformers termed over-aggregating, where global attention aggregates a large volume of messages with low discrimination, leading to message dilution and information loss. To address this, the authors propose Wideformer, a plug-and-play method that divides the aggregation of source nodes into parallel processes based on attention similarity, and guides the model to focus on informative clusters through sorting and weighting. Wideformer consistently improves performance across thirteen real-world datasets when integrated with three popular linear methods (GraphGPS, SGFormer, Polynormer), achieving superior results compared to baseline methods while effectively reducing attention entropy.

## Method Summary
Wideformer addresses over-aggregating in graph transformers by dividing source node aggregation into parallel processes based on attention similarity. The method uses K-Means++ to select m cluster centers from query features, assigns source nodes to clusters via key-center similarity, and aggregates within each cluster separately. After divided aggregation, Wideformer computes cluster-level attention scores, sorts and weights cluster outputs by these scores, then concatenates the results. The approach is designed to work as a plug-and-play module with linear attention methods, replacing single n-input aggregations with m smaller aggregations that preserve informative messages while reducing message dilution.

## Key Results
- Wideformer consistently improves accuracy across 13 real-world datasets when integrated with GraphGPS, SGFormer, and Polynormer backbones
- Attention entropy is reduced by approximately 0.2-0.4 normalized entropy across all three backbones
- Performance gains peak at 3-5 clusters, with diminishing returns beyond m=6 due to information dispersion
- On heterophilic graphs (e.g., Minesweeper, Tolokers), Wideformer with Polynormer achieves ROC-AUC improvements of 2.3-4.7% over baselines

## Why This Works (Mechanism)

### Mechanism 1: Over-Aggregating via Attention Entropy
- **Claim:** When aggregating from many nodes, global attention tends toward uniform scores, diluting informative messages.
- **Mechanism:** Attention entropy admits a lower bound that increases monotonically with node count n (Theorem 3.1). As n grows, attention scores become more uniform, reducing the model's ability to discriminate informative vs. noisy sources.
- **Core assumption:** Attention optimization gradients weaken as uniform attention spreads signal across many nodes (Eq. 3-4).
- **Evidence anchors:**
  - [abstract] "Over-aggregating arises when a large volume of messages is aggregated into a single node with less discrimination, leading to the dilution of the key messages and potential information loss."
  - [section 3.1] Fig. 1(b) shows attention entropy increasing with number of nodes across multiple datasets.
  - [corpus] Limited direct corroboration; related work on attention dynamics in transformers exists but doesn't explicitly address entropy scaling with node count.
- **Break condition:** If attention can be explicitly regularized or structured to maintain low entropy, over-aggregating should diminish. The paper shows entropy regularization improves accuracy (Tab. 1) but is computationally infeasible for large graphs.

### Mechanism 2: Divided Aggregation Limits Message Dilution
- **Claim:** Partitioning source nodes into clusters before aggregation preserves key messages by limiting input volume per aggregation process.
- **Mechanism:** Wideformer selects m cluster centers from query features via K-Means++ (Algorithm 1), assigns source nodes to clusters via key-center similarity (Eq. 5), then aggregates within each cluster separately (Eq. 6). This replaces a single n-input aggregation with m smaller aggregations.
- **Core assumption:** Informative messages cluster together in query-key space, allowing selective aggregation.
- **Evidence anchors:**
  - [section 4.1] "Only a limited volume of messages is aggregated for each cluster, preventing the dilution of the key messages."
  - [section 5.3.3] Fig. 7 shows performance gains peak at m=3-8 clusters, declining beyond as information disperses too thinly.
  - [corpus] ParaFormer (2512.14619) addresses over-smoothing via global attention but uses different structural mechanisms; limited direct validation of cluster-based aggregation in graph transformers.
- **Break condition:** If clusters are too many (m > 6), informative messages scatter across clusters, reducing attention effectiveness. If clusters are too few, dilution persists.

### Mechanism 3: Attention Guidance Prioritizes Informative Clusters
- **Claim:** Sorting and weighting cluster outputs by cluster-level attention scores focuses target nodes on informative subsets.
- **Mechanism:** After divided aggregation, Wideformer computes cluster-level attention scores ᾱ via softmax over query vs. averaged cluster keys (Eq. 7), then sorts and weights cluster outputs by these scores (Eq. 8) before concatenation.
- **Core assumption:** Cluster-level attention provides a useful proxy for cluster importance; sorting provides consistent ordering across targets.
- **Evidence anchors:**
  - [section 4.2] "The sorted and weighted aggregation results...inject the importance of each cluster into the aggregation, guiding the focus of the target nodes."
  - [section 5.1] Fig. 3(a): Wideformer without guidance maintains high entropy; with guidance, entropy drops significantly across all three backbones.
  - [corpus] Weak corpus support; attention-guided clustering appears in vision transformers (ENACT) but underperforms Wideformer in graph settings (Tab. S9).
- **Break condition:** If cluster attention scores become uniform, guidance provides no differentiation. Simple weighting may fail when few source nodes contain informative messages (Tab. 6 shows mixed results for divided-only ablation).

## Foundational Learning

- **Attention Entropy in Graphs**
  - Why needed here: The paper's core diagnostic relies on entropy as a proxy for over-aggregating. Without understanding entropy-as-confidence, the mechanism is opaque.
  - Quick check question: If attention scores are [0.25, 0.25, 0.25, 0.25] vs. [0.7, 0.1, 0.1, 0.1], which has higher entropy and why does it indicate potential information loss?

- **K-Means++ Center Selection**
  - Why needed here: Algorithm 1 uses a specific initialization strategy (maximize minimum distance to prior centers) rather than random initialization.
  - Quick check question: Why does K-Means++ select the point minimizing maximum similarity to existing centers, rather than maximizing diversity directly?

- **Linear vs. Sparse Attention Trade-offs**
  - Why needed here: Wideformer targets linear attention methods that maintain global receptive fields but suffer over-aggregating; contrast with sparse methods (Exphormer) that avoid this via limited receptive fields.
  - Quick check question: What complexity do linear attention methods achieve, and what architectural assumption do they make about the attention matrix structure?

## Architecture Onboarding

- **Component map:** Input Features (X) → Q, K, V projections → Center Selection (Algorithm 1, m centers from Q) → Source Assignment (Eq. 5, K vs. centers) → Divided Aggregation (Eq. 6, m parallel aggregations) → Cluster Attention (Eq. 7, Q vs. averaged cluster K) → Sorting & Weighting (Eq. 8) → Concatenation → downstream backbone

- **Critical path:** Center selection quality → cluster assignment coherence → cluster-level attention discrimination. If centers don't separate informative from noisy nodes, downstream sorting provides no benefit.

- **Design tradeoffs:**
  - m=2-8 clusters: Lower m preserves more context per cluster; higher m reduces dilution but risks fragmenting informative signals.
  - Non-iterative center selection: Faster but suboptimal vs. iterative refinement (Tab. 5 shows +0.6-0.7% accuracy with learnable centers).
  - Simple weighting vs. learned gating: Current approach uses fixed softmax; learnable cluster gates could adapt per-layer but add parameters.

- **Failure signatures:**
  - High entropy despite Wideformer: Check if guidance step is disabled or cluster attention scores are near-uniform.
  - Performance degradation vs. baseline: Likely m is too large for the dataset; try m=2-4.
  - OOM on cluster attention: Cluster attention computes Q × K̄ᵀ (n × m), should remain tractable; if failing, check if m accidentally set too high or intermediate dimensions expanded.

- **First 3 experiments:**
  1. **Entropy validation:** Run backbone (GraphGPS/SGFormer/Polynormer) with and without Wideformer on a medium dataset (e.g., CoauthorCS). Measure attention entropy before/after; expect ~0.2-0.4 reduction normalized entropy.
  2. **Cluster count sweep:** Test m ∈ {2, 4, 6, 8} on 2-3 datasets. Confirm performance peaks at m=3-5 as reported; document where this breaks down (very small or very large graphs).
  3. **Ablation by component:** Disable guidance (Eq. 8), run with divided aggregation only. Compare to full Wideformer; expect entropy reduction to weaken significantly (per Fig. 3a "w/o Guide").

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a parameterized or learnable center selection strategy replace the KMeans++ initialization to resolve the trade-off between computational efficiency and model performance?
- Basis in paper: [explicit] Section 5.3.3 and Appendix I state that "modeling centers as parameters" could address efficiency-performance trade-offs, and note the lack of exploration into "learnable division strategies."
- Why unresolved: The authors currently use a non-iterative KMeans++ approach for speed, observing that iterative clustering improves performance but increases cost.
- What evidence would resolve it: A method that learns cluster centers as trainable parameters, achieving the higher accuracy of iterative methods without the associated time complexity increase.

### Open Question 2
- Question: How can an adaptive weighting mechanism be designed to outperform the current simple sorting strategy in scenarios with low ratios of informative source nodes?
- Basis in paper: [explicit] Section 5.3.3 notes that the simple guiding strategy leads to "limited benefit capability" when fewer source nodes contain informative messages, identifying "exploring alternative guiding strategies" as future work.
- Why unresolved: The current weighting relies on basic sorting (argsort) of cluster attention scores, which may not sufficiently prioritize sparse informative signals.
- What evidence would resolve it: A dynamic guiding mechanism that demonstrates significant performance improvements on datasets where the current "Div. + All" method shows marginal gains over the baseline (e.g., CoauthorCS).

### Open Question 3
- Question: Is there a theoretical relationship between graph topology (e.g., node count, homophily) and the optimal number of clusters ($m$) that can eliminate the need for empirical grid search?
- Basis in paper: [inferred] Section 5.3.3 empirically determines the optimal cluster count is 3–5, noting that excessive partitioning disperses critical information, but does not provide a theoretical rule for selection.
- Why unresolved: The authors select $m$ via grid search, suggesting the optimal value is currently data-dependent rather than theoretically derived.
- What evidence would resolve it: A theoretical framework or heuristic formula that prescribes the optimal number of clusters based on graph scale or structural properties, correlating with peak empirical performance.

## Limitations

- The core theoretical claim about attention entropy bounds relies on assumptions about uniform attention distributions that may not hold in practice
- The sorting and weighting mechanism lacks formal theoretical grounding, with fixed softmax-based attention guidance that doesn't adapt to layer-specific patterns
- Computational overhead of cluster attention scales linearly with cluster count, potentially prohibitive for very large graphs with many clusters

## Confidence

- **High confidence**: The empirical effectiveness of Wideformer in reducing attention entropy and improving performance across diverse datasets (13 real-world graphs, three different backbones)
- **Medium confidence**: The theoretical analysis of over-aggregating via attention entropy bounds, with sound mathematical framework but partially speculative practical implications
- **Medium confidence**: The mechanism by which cluster-level attention effectively prioritizes informative sources, supported empirically but with alternative explanations not fully ruled out

## Next Checks

1. **Causality validation**: Run controlled experiments disabling the sorting/weighting guidance while maintaining divided aggregation, and vice versa. Measure both attention entropy and performance to establish which component drives improvements.

2. **Entropy-performance correlation**: Systematically vary m (cluster count) across datasets and measure the relationship between achieved entropy reduction and performance gains. Test whether entropy reduction alone predicts accuracy improvements.

3. **Alternative guidance mechanisms**: Replace the fixed softmax cluster attention with learned cluster gating parameters or attention regularization terms. Compare performance and computational overhead to assess whether simpler approaches could achieve similar benefits.