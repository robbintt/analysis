---
ver: rpa2
title: 'Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset
  for Evaluating Large Language Models and Agents on RTL Design and Verification'
arxiv_id: '2506.14074'
source_url: https://arxiv.org/abs/2506.14074
tags:
- code
- design
- generation
- operand
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The CVDP benchmark addresses the need for realistic, challenging
  hardware design and verification tasks for evaluating LLMs. It includes 783 human-authored
  problems across 13 categories, covering RTL generation, verification, debugging,
  and comprehension in both single-turn and agentic formats.
---

# Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification

## Quick Facts
- arXiv ID: 2506.14074
- Source URL: https://arxiv.org/abs/2506.14074
- Reference count: 25
- 783 human-authored Verilog problems across 13 categories, max 34% pass@1 for SOTA models

## Executive Summary
CVDP is a comprehensive benchmark dataset for evaluating LLMs and agents on RTL design and verification tasks. Authored by experienced hardware engineers and rigorously filtered, it addresses the need for realistic, challenging hardware design problems that expose current LLM limitations. The benchmark includes 783 problems across RTL generation, verification, debugging, and comprehension in both single-turn and agentic formats, revealing significant capability gaps particularly in design verification tasks.

## Method Summary
The benchmark was created by engaging ~35 hardware engineers with 4+ years of Verilog experience to author problems from scratch. A two-stage quality filtering process reduced 1,313 initial problems to 783 high-quality datapoints through sanity checks and LLM-based judging for ambiguity, consistency, and behavioral match. Evaluation uses open-source EDA tools (Icarus Verilog, Yosys, Verilator) with CocoTB test harnesses, running in Docker containers for isolation. Agentic tasks are currently evaluated in converted Non-Agentic format due to lack of established agentic frameworks.

## Key Results
- Max 34% pass@1 for top models on code generation tasks
- Agentic tasks show particularly low performance, especially for RTL reuse and verification
- Design verification categories (cid12-14) achieve 0-7% pass@1, indicating fundamental LLM struggles
- Failure clustering reveals category-specific weaknesses, with verification tasks showing more distinct error patterns

## Why This Works (Mechanism)

### Mechanism 1
Expert-authored problems with multi-stage quality filtering produce a benchmark with substantial headroom for measuring LLM improvements in hardware design. ~35 hardware engineers authored problems from scratch; two-stage filtering reduced 1,313 problems to 783 high-quality datapoints. This yields problems that better reflect real-world complexity than scraped public repositories.

### Mechanism 2
Distinguishing Non-Agentic (single-turn) from Agentic (multi-turn, tool-using) evaluation surfaces different capability gaps. Non-Agentic problems provide full context in one prompt; Agentic problems run in Docker containers where agents inspect mini-repositories and invoke EDA tools. Agentic contexts average larger token counts, reflecting real-world iterative design requirements.

### Mechanism 3
Automated failure clustering reveals category-specific LLM weaknesses that aggregate metrics obscure. Failed cases are reflected upon by a reasoning LLM, embedded via SentenceTransformer, clustered with K-means, and summarized. Design verification categories show more distinct failure clusters than RTL coding categories, providing actionable error patterns.

## Foundational Learning

- **RTL (Register-Transfer Level) Design**: Why needed: Core code generation tasks require understanding behavioral specifications mapping to synthesizable Verilog constructs. Quick check: Explain the difference between blocking (`=`) and non-blocking (`<=`) assignments in Verilog and when each should be used.

- **SystemVerilog Testbenches and Verification**: Why needed: Design verification categories are where models perform worst; understanding testbench structure, checkers, and assertions is prerequisite to improving these scores. Quick check: What is the difference between a testbench stimulus generator and a checker? What role do SystemVerilog Assertions (SVA) play?

- **Cycle-Accurate Timing and Latency Contracts**: Why needed: Models struggle with cycle-accurate control sequencing—e.g., updating `pass_cnt` one cycle late in brick sort, violating latency specifications. Quick check: Given a sorting FSM that performs one compare-swap per cycle across N passes, how would you compute total latency and where would you update state counters?

## Architecture Onboarding

- **Component map**: JSONL files (Non-Agentic: 617 problems; Agentic: 166 problems) -> Python benchmark runner with model/agent callbacks -> Mini-repository context instantiation -> Prompt model/agent -> CocoTB test harness (Icarus Verilog + simulator) -> Pass/fail signal collection -> LLM-based scoring for comprehension tasks.

- **Critical path**: Load datapoint from JSONL → Instantiate mini-repository context → Prompt model/agent → Run generated code through test harness (CocoTB + simulator) → Collect pass/fail signal → (Comprehension tasks) LLM-based scoring vs. reference.

- **Design tradeoffs**: Open-source vs. commercial tools (most tasks use open-source, but verification requires Cadence Xcelium); Oracle vs. full-repository context (current uses minimal relevant context); Pass@1 vs. pass@k (reported as pass@1 with n=5 samples).

- **Failure signatures**: Testbench generation (syntax errors, missing timescale, unmatched blocks, insufficient coverage); Assertion generation (misplaced SVA, flawed timing, operator errors); RTL generation (bit-width mismatches, mixed blocking/non-blocking assignments, off-by-one timing errors); Module reuse (models excel at single-module generation but struggle to compose multiple existing modules).

- **First 3 experiments**: 1) Baseline a Non-Agentic model on cid02 (Code Completion) to establish Claude 3.7 Sonnet's 34% pass@1 reference point; 2) Compare Non-Agentic vs. Agentic performance on cid03 (NL Spec-to-Code) to measure drop-off when navigating larger contexts; 3) Target cid13 (Testbench Checker) with specialized prompting, experimenting with decomposition strategies to improve success.

## Open Questions the Paper Calls Out

1. **Agentic evaluation frameworks**: Can agentic evaluation frameworks significantly improve LLM performance on hardware design verification tasks compared to single-turn inference? The paper evaluates agentic problems in single-turn format only, leaving multi-turn, tool-using performance unmeasured.

2. **SystemVerilog testbench generation**: What architectural or training improvements could address the fundamental difficulty LLMs face with SystemVerilog testbench generation? The paper identifies the problem but does not propose solutions for procedural verification patterns.

3. **RTL-to-specification tasks**: Would a specification-creation-from-RTL task category provide better differentiation of LLM code comprehension capabilities? This category was suggested but not implemented in the current benchmark release.

## Limitations

- Reliance on commercial EDA tools (Cadence Xcelium) for verification categories may restrict reproducibility
- Agentic evaluation methodology remains underspecified due to lack of established frameworks
- Dataset size may be insufficient to capture full diversity of real-world hardware design challenges

## Confidence

**High Confidence (95%+)**: CVDP provides more challenging and realistic hardware design tasks than prior benchmarks, supported by expert authoring and significantly lower pass rates.

**Medium Confidence (70-90%)**: Agentic tasks are particularly difficult, but evaluation methodology is incomplete; failure clustering provides useful insights but LLM-based reflection introduces uncertainty.

**Low Confidence (below 70%)**: Specific failure mode claims are based on limited case studies and may not generalize.

## Next Checks

1. Replicate baseline results: Run Claude 3.7 Sonnet on Non-Agentic cid02 (Code Completion) with exact configuration to verify 34% pass@1 baseline.

2. Test open-source vs commercial tool performance gap: Evaluate testbench generation tasks using open-source tools versus Xcelium where possible to quantify commercial tool contribution.

3. Implement and evaluate a simple agentic framework: Develop a minimal agent that can invoke Icarus Verilog and Yosys through system calls, then compare performance on agentic tasks versus converted Non-Agentic format.