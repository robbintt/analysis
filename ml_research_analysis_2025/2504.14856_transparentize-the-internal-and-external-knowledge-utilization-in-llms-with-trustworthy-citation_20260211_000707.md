---
ver: rpa2
title: Transparentize the Internal and External Knowledge Utilization in LLMs with
  Trustworthy Citation
arxiv_id: '2504.14856'
source_url: https://arxiv.org/abs/2504.14856
tags:
- knowledge
- answer
- citation
- generation
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel task requiring large language models
  to generate citations that combine external knowledge sources with internal parameter
  knowledge, enhancing both interpretability and trustworthiness. The proposed RAEL
  paradigm and INTRALIGN method enable models to transparently articulate and cite
  relevant internal knowledge when external sources are insufficient, accompanied
  by confidence scores.
---

# Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation

## Quick Facts
- **arXiv ID:** 2504.14856
- **Source URL:** https://arxiv.org/abs/2504.14856
- **Reference count:** 40
- **Primary result:** Novel method enabling LLMs to transparently cite both external sources and internal parameter knowledge with confidence scores, improving accuracy and trustworthiness over baselines.

## Executive Summary
This paper addresses the challenge of making large language models' knowledge utilization transparent by requiring explicit citations for both external documents and internal parameter knowledge. The authors propose a structured reasoning paradigm (RAEL) that forces models to explicitly review retrieved context, scrutinize their own parametric knowledge, and generate citations with confidence scores. They also introduce INTRALIGN, an alignment method using GPT-4o to generate high-quality training data with a token-weighted loss emphasizing citation quality. Experiments on three LLMs across four scenarios show significant improvements in answer accuracy, citation faithfulness, and reference trustworthiness compared to state-of-the-art baselines.

## Method Summary
The approach combines a structured reasoning paradigm (RAEL) with an alignment method (INTRALIGN). RAEL guides models through a four-step process: context review, parameter knowledge scrutiny, reference generation, and answer generation with citations. INTRALIGN generates training data by sampling from the target LLM's own knowledge and using GPT-4o to create RAEL-formatted responses, then fine-tunes the model with a token-wise weighted loss that emphasizes reference, confidence, and citation marker tokens. The training uses LoRA with rank=8, learning rate=10^-4, and 2 epochs on 1K training samples. The method addresses knowledge conflicts in retrieval-augmented generation by enabling transparent citation of both external and internal sources.

## Key Results
- INTRALIGN significantly improves citation faithfulness and reference trustworthiness across all three tested LLMs
- The method achieves better calibration (lower ECE) for internally cited knowledge compared to baselines
- RAEL paradigm outperforms simpler prompting approaches, particularly in scenarios with poor retrieval quality
- The approach effectively handles the "tug-of-war" between external and internal knowledge sources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A structured reasoning paradigm (RAEL) that explicitly separates context review, internal knowledge scrutiny, and citation generation can improve the transparency and accuracy of knowledge utilization in LLMs.
- **Mechanism:** The RAEL paradigm forces the model to first review retrieved documents, then scrutinize its own parametric knowledge, and finally generate references and a cited answer. This explicit, stepwise process reduces implicit knowledge blending and makes the source of information (external vs. internal) interpretable.
- **Core assumption:** LLMs can be guided via prompting and fine-tuning to reliably follow a structured "think-then-cite" process, separating analysis from generation.
- **Evidence anchors:**
  - [abstract] The paper introduces "RAEL, the paradigm for our task."
  - [Section 4.1] Describes RAEL as requiring models to "review the context, scrutinize its own knowledge, and then generate... references, along with a final answer containing citations."
  - [corpus] Related work on knowledge conflicts in RAG (e.g., "Tug-of-war between knowledge") supports the need for structured integration of internal/external sources.

### Mechanism 2
- **Claim:** Aligning the model with a custom dataset and a weighted loss function (INTRALIGN) improves citation faithfulness and reference trustworthiness (convincingness & conciseness).
- **Mechanism:** INTRALIGN generates training data by sampling from the target LLM's own knowledge and using GPT-4o to create RAEL-formatted responses. The fine-tuning uses a token-wise weighted loss, assigning higher weights to reference, confidence, and citation marker tokens. This forces the model to focus on generating high-quality, trustworthy citations.
- **Core assumption:** GPT-4o can reliably generate high-quality RAEL-formatted demonstrations, and the weighted loss effectively directs model capacity toward citation generation.
- **Evidence anchors:**
  - [abstract] The paper designs "INTRALIGN, an integrated method containing customary data generation and an alignment algorithm."
  - [Table 3 & 4] Ablation studies show removing RAEL or weighted loss degrades performance, particularly in citation recall and conciseness.
  - [corpus] ParamMute and other methods focus on suppressing internal knowledge for faithfulness; INTRALIGN takes a different, alignment-based approach.

### Mechanism 3
- **Claim:** Requiring a confidence score for internally cited knowledge and evaluating it with Expected Calibration Error (ECE) can improve the trustworthiness of self-generated references.
- **Mechanism:** The task definition mandates a confidence score for any internal reference. The INTRALIGN training data includes these scores (derived from self-consistency checks), and ECE is used as a key metric. This creates a training signal for the model to output well-calibrated confidence, aligning its stated certainty with factual accuracy.
- **Core assumption:** Models can learn to estimate the factuality of their own parametric knowledge and express it as a numerical confidence score.
- **Evidence anchors:**
  - [Section 2.1.5] Defines Internal Reference ECE to measure alignment between confidence and factuality (using FactScore).
  - [Table 1] Results show INTRALIGN achieves lower ECE (e.g., 0.10) compared to baselines, indicating better calibration.
  - [corpus] LUMINA and R1-Searcher++ also address knowledge imbalances, but via detection or dynamic acquisition, not explicit confidence calibration for citation.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** This work is a direct extension of RAG. Understanding how RAG grounds LLMs in external documents is essential to grasp the problem of "opaque" internal knowledge use this paper tries to solve.
  - **Quick check question:** Can you explain how standard RAG typically handles a case where retrieved documents do not contain the full answer?

- **Concept: Parametric vs. Contextual Knowledge**
  - **Why needed here:** The core tension in the paper is between knowledge encoded in model weights (parametric) and knowledge provided in the prompt (contextual/retrieved). The goal is to use both transparently.
  - **Quick check question:** In the Olivia Rodrigo example (Figure 1), what constitutes parametric knowledge versus external context?

- **Concept: Model Calibration & Expected Calibration Error (ECE)**
  - **Why needed here:** A key innovation is requiring confidence scores for internal citations. Understanding calibration—how well a model's confidence matches its accuracy—is crucial to evaluate this feature.
  - **Quick check question:** If a model assigns 0.8 confidence to a generated fact that is only correct 50% of the time, is it well-calibrated? How would ECE reflect this?

## Architecture Onboarding

- **Component map:** The system has two main components: (1) The RAEL inference paradigm - a structured prompt design guiding any LLM through Context Review → Parameter Knowledge Scrutiny → Reference Generation → Answer Generation. (2) The INTRALIGN training pipeline - uses a teacher model (GPT-4o) to generate RAEL-formatted data, quality-controls it, and fine-tunes a target LLM with a token-weighted loss to emphasize citation tokens.
- **Critical path:** The most critical sequence for a new engineer is understanding the RAEL prompt structure (Fig. 4, Fig. 14) → understanding the weighted loss formulation in Section 4.2 → tracing the data generation and filtering pipeline in Section 4.2. The ablation studies (Tables 3 & 4) are key to validating each component's contribution.
- **Design tradeoffs:** A central tradeoff is Convincingness vs. Conciseness (Fig. 3). The weighted loss and data reranking are designed to navigate this. Another tradeoff is Training data quality vs. cost: using GPT-4o for data generation is effective but expensive. The paper also trades off general reasoning capacity for citation quality, as seen in the weighted loss design.
- **Failure signatures:** Watch for: (1) High ECE with low FactScore, indicating the model is overconfident in incorrect internal knowledge. (2) Low Citation Recall (RcO), showing the model fails to support its claims with citations. (3) High plagiarism rate (PR/PS) on the GT,PK set (Table 2), where the model dishonestly claims external knowledge as internal.
- **First 3 experiments:**
  1. Prompting Baseline: Implement and test the `Guided-RAEL` two-shot prompt on a small LLM (e.g., Llama-3.1-8B) with the provided dataset. Measure Accuracy, Citation Recall, and Convincingness to establish a non-fine-tuned baseline.
  2. Ablation on Weighted Loss: Using the provided INTRALIGN training data, fine-tune the same model twice: once with the prescribed weighted loss and once with standard loss. Compare performance on the key metrics (especially Recall, Conv., Conc.) to validate the mechanism from Section 4.2 & Tables 3-4.
  3. Scenario Stress Test: Evaluate the fine-tuned model specifically on the `GT,PK` and `GT,PK` scenarios (from Table 1). Plot the "tug-of-war" behavior (as in Fig. 6 & 7) to see if the model adapts its citation strategy (internal vs. external) based on retrieval quality and its own knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the model's preference for citing external versus internal knowledge be explicitly controlled during inference?
- **Basis in paper:** [explicit] Section 8 (Limitations) states, "future works may focus on the intended control of citing external or internal knowledge."
- **Why unresolved:** While the INTRALIGN method enables the model to adaptively utilize both sources, the authors do not provide a mechanism for users or developers to enforce a specific preference (e.g., strictly preferring retrieval over parameter knowledge) dynamically.
- **What evidence would resolve it:** Introducing a control token or prompt prefix to bias the generation toward a specific source, followed by an analysis of the shift in external ($R_{c}^{ex}$) versus internal ($R_{c}^{in}$) citation recall scores.

### Open Question 2
- **Question:** What are the specific internal mechanisms within the LLM that govern the extraction and utilization of parameter knowledge during citation generation?
- **Basis in paper:** [explicit] Section 8 states, "We still leave room to consider and explore the internal mechanism of parameter knowledge utilization."
- **Why unresolved:** The paper establishes a paradigm (RAEL) and an alignment method (INTRALIGN) to improve the *output* behavior, but it does not investigate the neuron-level or layer-level representations that distinguish "reciting parameter knowledge" from "extracting context."
- **What evidence would resolve it:** Causal tracing or probing experiments (e.g., activation patching) on aligned models to locate specific attention heads or MLP layers responsible for detecting context insufficiency and triggering internal recitation.

### Open Question 3
- **Question:** Does the alignment for convincingness and conciseness inadvertently increase the rate of "dishonest" internal citations (plagiarism)?
- **Basis in paper:** [inferred] Section 5.4.3 observes that larger models exhibit higher plagiarism rates (claiming rewritten external text as internal knowledge), hypothesizing they "learn more about the preference for convincingness and conciseness."
- **Why unresolved:** The paper identifies the correlation between model scale/alignment and dishonest internal references but does not determine if this is an unavoidable side effect of optimizing for the proposed trustworthiness metrics.
- **What evidence would resolve it:** A correlation analysis between the optimization intensity for trustworthiness metrics (Convincingness/Conciseness) and the Plagiarism Severity (PS) score across different training checkpoints.

### Open Question 4
- **Question:** How does the data reranking and selection strategy in INTRALIGN bias the model's performance on out-of-distribution question types?
- **Basis in paper:** [explicit] Section 8 notes that "possible bias may still be introduced during reranking and selection in the data sampling process."
- **Why unresolved:** The pipeline relies on GPT-4o to generate and rerank data based on specific criteria (convincingness/conciseness), which may filter out valid reasoning paths or knowledge utilization strategies that do not match the teacher model's style.
- **What evidence would resolve it:** Evaluating the cross-scenario performance of the aligned model on a dataset generated without the reranking/selection filter to measure the delta in accuracy and citation faithfulness.

## Limitations

- **Data Quality and Cost Dependencies:** The INTRALIGN method's effectiveness is heavily dependent on the quality of GPT-4o-generated RAEL-formatted data, introducing significant cost barriers and potential data mismatch if the target LLM's parametric knowledge differs substantially from GPT-4o's.
- **Calibration Mechanism Vulnerability:** The confidence score requirement for internal citations remains vulnerable to "dishonest internal reference generation" where models rewrite external context as internal knowledge with high confidence, which the paper identifies but provides limited mitigation strategies for.
- **Limited Scope of Scenarios:** The four scenarios (GT,PK; GT,¬PK; ¬GT,PK; ¬GT,PK) may not capture the full complexity of real-world knowledge retrieval and generation, focusing only on Wikipedia-based datasets.

## Confidence

- **High Confidence:** The RAEL paradigm's structural benefits (separating context review, internal scrutiny, and citation generation) are well-supported by the explicit prompt design and ablation studies showing performance degradation when components are removed.
- **Medium Confidence:** The weighted loss mechanism's effectiveness is moderately supported by Table 3-4 showing improvements in citation metrics, but the exact weight values remain unspecified, making full reproduction challenging.
- **Medium Confidence:** The confidence calibration improvements (ECE metrics) are demonstrated but rely heavily on the self-consistency mechanism for golden confidence calculation, which may not generalize well to all knowledge types.

## Next Checks

1. **Real-world Knowledge Domain Test:** Evaluate the method on non-Wikipedia domains (e.g., scientific literature, technical documentation) to assess generalization beyond the current dataset scope and identify potential knowledge domain limitations.

2. **Cost-Effectiveness Analysis:** Implement a pilot study comparing GPT-4o-based data generation with lower-cost alternatives (e.g., fine-tuned smaller models) to quantify the practical economic barriers and identify potential scalability thresholds.

3. **Confidence Calibration Stress Test:** Design adversarial test cases specifically targeting the confidence calibration mechanism, including questions where the model has partial knowledge or conflicting internal/external sources, to measure robustness against dishonest citation generation.