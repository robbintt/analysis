---
ver: rpa2
title: 'Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted
  Evidence Synthesis'
arxiv_id: '2509.00038'
source_url: https://arxiv.org/abs/2509.00038
tags:
- prompt
- research
- language
- prompts
- dspy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a declarative framework for reproducible
  LLM-assisted systematic literature reviews (SLRs), addressing the fragility of manual
  prompt engineering. By adapting prompt optimization techniques from DSPy, GRPO,
  and GEPA, it proposes a four-step programmatic workflow: defining the research goal,
  codifying a quality standard with gold-standard examples, automatically compiling
  an optimal prompt, and packaging the result as a verifiable digital artefact.'
---

# Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis

## Quick Facts
- arXiv ID: 2509.00038
- Source URL: https://arxiv.org/abs/2509.00038
- Reference count: 40
- Key outcome: Introduces a declarative framework for reproducible LLM-assisted systematic literature reviews using DSPy/GRPO prompt optimization, with a working Python implementation achieving reliable abstract screening performance.

## Executive Summary
This paper addresses the reproducibility crisis in AI-assisted evidence synthesis by replacing manual prompt engineering with a declarative compilation framework. The approach treats prompt generation as an optimization problem, using gold-standard examples and automated search to produce LLM-agnostic, deterministic artifacts. By fixing model parameters and packaging the entire workflow, it aims to create verifiable digital objects that can be reliably reproduced across research contexts. A proof-of-concept implementation using DSPy demonstrates compiling an abstract screening module that achieves consistent performance without the fragility of ad-hoc prompting.

## Method Summary
The framework follows a four-step programmatic workflow: (1) Define the research goal by specifying inputs, outputs, and evaluation criteria; (2) Codify quality standards using expert-labeled gold-standard examples; (3) Automatically compile an optimal prompt using DSPy's MIPROv2 optimizer with fixed model parameters (temperature=0.0, seed=42, top_p=1.0); and (4) Package the compiled artifact as a verifiable digital object containing the prompt text, configuration, and logs. The approach treats prompt generation as hyperparameter tuning, iterating over instruction templates and exemplars against a validation set to maximize accuracy.

## Key Results
- Replaces brittle manual prompt engineering with a structured, auditable compilation process
- Achieves reproducible performance through fixed model parameters and deterministic settings
- Demonstrates first application of declarative DSPy methods to SLR workflows
- Produces verifiable digital artifacts that maintain consistency across runs

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Intent from Implementation
Treating prompts as compiled artifacts rather than manually crafted strings reduces variance from linguistic ambiguity. The framework enforces a declarative paradigm where researchers define "what" (signatures, schemas) while the compiler determines "how" (instruction phrasing, exemplar selection). This isolates scientific goals from model-specific syntactic sensitivities. Core assumption: automated optimizers can explore prompt space more effectively than human intuition. Evidence: subtle prompt variations can cause up to 76% accuracy differences; declarative approaches have shown promise in DSPy literature. Break condition: ambiguous task definitions or insufficient model capacity prevent stable compilation.

### Mechanism 2: Metric-Driven Compilation (Prompt Tuning)
The system treats prompt generation as hyperparameter tuning, iterating over instruction templates and few-shot exemplars against a gold-standard validation set to maximize specific metrics like accuracy. This replaces manual "prompt alchemy" with an automated search loop. Core assumption: gold-standard examples are representative and unbiased enough to guide the optimizer toward generalizable solutions. Evidence: compilation process is "analogous to hyperparameter tuning" systematically searching for optimal configurations. Break condition: if evaluation metrics fail to capture nuances like "Unsure" vs. "Exclude" distinctions, compiled prompts become brittle in edge cases.

### Mechanism 3: Artifact Packaging for Determinism
Strictly pinning model parameters and versioning prompt artifacts enables reproducibility impossible with standard LLM chat interfaces. The workflow mandates fixing variables typically left fluid (temperature, seed, model build) and packaging them into verifiable bundles (config files, prompt text, logs). This attempts to remove the "luck" element from manual prompting. Core assumption: LLM providers respect pinned seeds and model versions over time, or local models are used where this can be strictly enforced. Evidence: code implementation explicitly sets temperature=0.0, seed=42, top_p=1.0 for deterministic behavior. Break condition: silent model updates by providers invalidate reproducibility guarantees.

## Foundational Learning

- **Concept: Declarative vs. Imperative Programming**
  - Why needed: The paper shifts from "writing instructions" (imperative) to "defining signatures and goals" (declarative). Understanding this distinction is vital for structuring input data correctly.
  - Quick check: Are you defining *how* the model should think step-by-step, or are you defining the *input schema* and *output label* you want the compiler to achieve?

- **Concept: Prompt Optimization / "Compiling"**
  - Why needed: The core engine is the optimizer (e.g., MIPROv2) that searches for the best prompt, not the LLM itself. You must understand you're tuning a program, not chatting.
  - Quick check: If the compiler returns a prompt that looks grammatically strange but passes the metric, do you accept it or edit it manually? (Hint: Accept it to maintain the "compiled" guarantee).

- **Concept: Gold-Standard Validation Sets**
  - Why needed: The system is only as good as the examples you grade for it. The "Compiler" cannot function without a "Gold Standard" to test against.
  - Quick check: Have you curated a set of labeled examples that includes difficult edge cases (e.g., "Unsure" classifications), or only clear-cut "Include/Exclude" cases?

## Architecture Onboarding

- **Component map:** Signature (Schema) -> Metric (Optimizer Logic) -> Compiler (DSPy/GEPA) -> Artifact (JSON/YAML)
- **Critical path:** Creation of the Gold Standard dataset. This is the highest-friction step. You cannot "compile" without labeled data. If this data is low quality, the compiled prompt will effectively overfit to noise.
- **Design tradeoffs:**
  - Rigidity vs. Flexibility: Compiled prompts are deterministic and high-performing on defined tasks but may lack flexibility for out-of-domain queries.
  - Effort Front-loading: Requires significant time to build test suite and gold standards before screening begins, unlike ad-hoc prompting which is fast to start but chaotic to maintain.
- **Failure signatures:**
  - The "Unsure" Loop: Ambiguous gold standards may drive the optimizer to classify everything as "Unsure" to maximize safety, minimizing false positives but killing utility.
  - Overfitting: Compiled prompts perform perfectly on gold-standard sets but fail on new abstracts because training data was too narrow.
- **First 3 experiments:**
  1. **Hello World Compile:** Implement the `ScreenAbstract` signature with 5 simple examples. Run the compiler and verify it saves a `.json` file.
  2. **Metric Sensitivity Test:** Change the metric to penalize "False Negatives" more heavily than "False Positives." Re-compile and observe how the `decision` logic shifts.
  3. **Adversarial Injection:** Add a deliberately tricky abstract to the gold standard (e.g., matches keywords but wrong study design). Run compilation to see if the prompt learns to distinguish design from keywords.

## Open Questions the Paper Calls Out
- Does the declarative compilation framework yield reliable performance gains for complex SLR tasks beyond abstract screening, such as data extraction and risk of bias assessment?
- What is the minimum effective size and diversity of the gold-standard dataset required to prevent overfitting during prompt compilation?
- To what degree does a prompt compiled for one specific LLM architecture retain its performance when deployed on different model families?

## Limitations
- The framework's generalization across different SLR topics and contexts remains unproven with only a single demonstration
- Long-term sustainability of compiled artifacts is uncertain due to potential silent model updates and evolving LLM architectures
- Practical implementation heavily depends on curating robust gold-standard datasets, which the paper only gestures at without providing

## Confidence
- **High Confidence:** The conceptual shift from manual prompt engineering to declarative prompt compilation is sound and technically feasible
- **Medium Confidence:** The framework's ability to generalize across different SLR topics and contexts is plausible but unproven
- **Low Confidence:** The long-term sustainability of compiled artifacts is uncertain given potential silent model updates

## Next Checks
1. **Gold-Standard Robustness:** Test the compiled prompt's performance on a held-out set of abstracts from a different SLR domain than the one used for compilation to assess generalization
2. **Reproducibility Audit:** Run the compiled artifact across two different LLM providers (e.g., OpenAI and Anthropic) with identical settings to check for cross-provider determinism
3. **Long-Term Stability:** Re-run the compiled artifact after 6 months (or simulate a model update) to measure any degradation in accuracy or consistency