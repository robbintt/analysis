---
ver: rpa2
title: Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR
  Imagery
arxiv_id: '2503.22516'
source_url: https://arxiv.org/abs/2503.22516
tags:
- segmentation
- remote
- sensing
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates ten remote sensing foundation models for sea
  ice type segmentation using Sentinel-1 SAR imagery, addressing the challenge of
  limited labeled datasets in polar regions. The research systematically benchmarks
  models including Prithvi-600M, CROMA, and others, comparing their performance across
  seasonal and spatial variations.
---

# Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery

## Quick Facts
- arXiv ID: 2503.22516
- Source URL: https://arxiv.org/abs/2503.22516
- Reference count: 40
- Primary result: LoRA adaptation enables effective transfer of foundation models to polar SAR data, with Prithvi-600M outperforming baselines when using three input channels.

## Executive Summary
This study evaluates ten remote sensing foundation models for sea ice type segmentation using Sentinel-1 SAR imagery, addressing the challenge of limited labeled datasets in polar regions. The research systematically benchmarks models including Prithvi-600M, CROMA, and others, comparing their performance across seasonal and spatial variations. Key findings show Prithvi-600M outperforms baseline models when using three input channels, while CROMA achieves similar performance through full fine-tuning. The study introduces a systematic methodology for selecting foundation models and provides insights into their generalization capabilities across different sea ice conditions. LoRA adaptation emerges as a robust approach for efficient model adaptation.

## Method Summary
The study evaluates ten remote sensing foundation models on the AI4Arctic Sea Ice Challenge Dataset using Sentinel-1 C-band SAR EW mode imagery. Models are adapted using three strategies: frozen encoder, LoRA adaptation (rank-4, alpha-16), and full fine-tuning. The UPerNet decoder architecture is consistently used across all models. Input channels include HH, HV, and their ratio (3-channel) or just HH and HV (2-channel). Training employs AdamW optimizer with cross-entropy loss and StepLR scheduler. Models are evaluated on 20 test scenes using weighted F1-score, IoU, and per-class metrics.

## Key Results
- Prithvi-600M with 3-channel input achieves the highest F1-score (0.747) using LoRA adaptation
- SAR-specific pre-trained models underperform due to IW vs EW mode mismatch in acquisition
- LoRA adaptation delivers the most significant performance boost, outperforming both frozen encoder and full fine-tuning strategies

## Why This Works (Mechanism)

### Mechanism 1
LoRA adaptation enables effective transfer of foundation models to polar SAR data by selectively updating encoder parameters while preserving pre-trained representations. This allows the model to adapt to domain-specific features—such as sea ice textures, SAR speckle noise, and backscatter patterns—without catastrophic forgetting of the spatial hierarchies learned during pre-training.

### Mechanism 2
Larger model scale improves performance only when paired with an appropriate fine-tuning strategy and input channel configuration. The 600M-parameter Prithvi model possesses greater representational capacity to model complex sea ice patterns, which is effectively leveraged when provided with a 3-channel input (HH, HV, and their ratio).

### Mechanism 3
SAR-specific pre-training provides no guaranteed advantage for this task due to a fundamental mismatch in acquisition modes between pre-training and target domains. Models pre-trained on IW mode data from lower latitudes cannot directly transfer to EW mode data from polar regions due to different spatial resolutions, polarization combinations, and characteristic noise patterns.

## Foundational Learning

- **Concept: Transfer Learning with Foundation Models**
  - Why needed here: Sea ice segmentation is hampered by scarce, expert-labeled datasets. Foundation models, pre-trained on massive Earth observation datasets, provide a powerful initialization to reduce data requirements.
  - Quick check question: Why might a model pre-trained on optical RGB satellite imagery still provide a useful starting point for a task using single-polarization SAR data?

- **Concept: Fine-Tuning Strategies (Frozen, LoRA, Full)**
  - Why needed here: The paper demonstrates that the choice of fine-tuning strategy is often more critical than model choice. A poorly chosen strategy (e.g., full fine-tuning a large model on small data) leads to failure.
  - Quick check question: For a model pre-trained on generic remote sensing data, why might a frozen encoder strategy fail when adapting to a specialized domain like sea ice?

- **Concept: Domain Shift in SAR Imagery**
  - Why needed here: The dominant failure mode is not a lack of model capacity but a domain mismatch between pre-training data (lower-latitude, IW mode) and target data (polar, EW mode).
  - Quick check question: What are the primary differences between Sentinel-1's IW and EW acquisition modes that could impact a segmentation model's performance?

## Architecture Onboarding

- **Component map:** Sentinel-1 SAR data -> 224×224 patches -> Foundation model encoder (ViT/Swin) -> LoRA modules (optional) -> UPerNet decoder -> 6-class semantic map

- **Critical path:**
  1. Process Sentinel-1 EW GRD data to generate 224×224 patches
  2. Create 3rd channel (HH/HV ratio) for models like Prithvi
  3. Load pre-trained foundation model encoder and attach UPerNet decoder
  4. Inject LoRA modules into encoder's attention layers
  5. Train using cross-entropy loss with AdamW optimizer and StepLR scheduler

- **Design tradeoffs:**
  - 2-channel vs. 3-channel input: 2-channel uses native SAR polarizations but limits performance; 3-channel boosts performance for large models but requires feature engineering
  - LoRA vs. Full Fine-tuning: LoRA is computationally efficient and guards against overfitting; full fine-tuning can achieve top performance for some models but is risky for larger models on limited data
  - Model Scale: Larger models offer higher potential performance but are more sensitive to fine-tuning strategy

- **Failure signatures:**
  - Low F1 score with frozen encoder: Pre-trained features don't transfer to polar SAR domain
  - Performance collapse during training: Likely overfitting or catastrophic forgetting
  - High performance on easy classes but near-zero recall on hard classes: Class imbalance problem
  - Poor performance specifically in winter or Canadian Arctic: Poor generalization across seasons/regions

- **First 3 experiments:**
  1. Establish baselines: Train U-Net and DeepLabV3 models on 2-channel and 3-channel data
  2. LoRA adaptation sweep: Apply LoRA to Prithvi-600M and sweep rank (r=4, 8, 16)
  3. Ablate fine-tuning strategies: Compare frozen, LoRA, and full fine-tuning for top 2-3 models

## Open Questions the Paper Calls Out

### Open Question 1
Can a foundation model pre-trained specifically on Sentinel-1 Extra Wide (EW) mode SAR imagery from polar regions outperform models pre-trained on Interferometric Wide (IW) mode data from lower latitudes? No current foundation model is pre-trained on polar EW mode SAR data.

### Open Question 2
What hybrid fine-tuning strategies would optimally balance preservation of pre-trained features with domain adaptation for sea ice segmentation? The study found conflicting results between models, suggesting different architectures may benefit from different combined approaches.

### Open Question 3
How can model performance be improved for winter conditions when young ice classes exhibit visually ambiguous or transitional features? Winter F1-scores dropped to approximately 0.47–0.53 across all models, indicating a fundamental challenge with transitional ice states.

### Open Question 4
What performance threshold must foundation models achieve for reliable operational deployment in National Ice Centers where manual mapping remains standard practice? While the best FMs approach baseline U-Net performance (F1~0.75), operational requirements have not been defined or met.

## Limitations

- The evaluation was conducted on a single sea ice segmentation dataset, which may not capture full diversity of polar SAR imagery conditions
- The focus on UPerNet decoder architecture may not represent optimal decoder choices for all foundation models
- SAR-specific pre-training advantage was not fully realized due to IW vs EW mode mismatch, suggesting domain-specific pretraining alone is insufficient

## Confidence

- **High Confidence:** Effectiveness of LoRA adaptation for foundation models on limited datasets; importance of choosing appropriate fine-tuning strategies
- **Medium Confidence:** Superiority of Prithvi-600M with 3-channel input; conclusion that SAR-specific pre-training provides no advantage due to IW/EW mode mismatch

## Next Checks

1. **Cross-Dataset Validation:** Evaluate top-performing models on independent sea ice dataset from different geographic regions to assess generalization beyond AI4Arctic

2. **Decoder Architecture Ablation:** Systematically test alternative decoder architectures (DeepLabV3+, Mask2Former) with top foundation models to determine if UPerNet is optimal

3. **Domain-Specific Pretraining Test:** Fine-tune subset of models using Sentinel-1 EW mode data from lower latitudes before adapting to polar sea ice task to directly test whether bridging IW/EW gap improves performance