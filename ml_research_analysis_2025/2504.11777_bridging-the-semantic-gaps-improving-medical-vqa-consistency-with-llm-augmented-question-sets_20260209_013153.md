---
ver: rpa2
title: 'Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented
  Question Sets'
arxiv_id: '2504.11777'
source_url: https://arxiv.org/abs/2504.11777
tags:
- questions
- datasets
- question
- consistency
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of linguistic variability undermining
  the consistency of medical visual question answering (MVQA) systems. The authors
  propose a Semantically Equivalent Question Augmentation (SEQA) framework that uses
  large language models (LLMs) to generate diverse yet semantically equivalent rephrasings
  of questions, thereby enriching linguistic diversity while preserving semantic meaning.
---

# Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets

## Quick Facts
- arXiv ID: 2504.11777
- Source URL: https://arxiv.org/abs/2504.11777
- Reference count: 32
- Primary result: SEQA framework improves MVQA consistency (TAR-SC) by 11.61% and accuracy by 19.35% through LLM-generated semantically equivalent question augmentations

## Executive Summary
This study addresses the challenge of linguistic variability undermining the consistency of medical visual question answering (MVQA) systems. The authors propose a Semantically Equivalent Question Augmentation (SEQA) framework that uses large language models (LLMs) to generate diverse yet semantically equivalent rephrasings of questions, thereby enriching linguistic diversity while preserving semantic meaning. To evaluate consistency, they introduce the Total Agreement Rate with Semantically Equivalent Input and Correct Answer (TAR-SC) metric, along with three dataset-specific diversity metrics: ANQI, ANQA, and ANQS. Using the SEQA framework, they augmented three benchmark MVQA datasets (SLAKE, VQA-RAD, and PathVQA), achieving significant improvements in all three metrics. Experiments with three MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and fine-tuning settings showed that fine-tuned models achieved an average accuracy improvement of 19.35%, while TAR-SC improved by an average of 11.61%, indicating substantial enhancement in model consistency.

## Method Summary
The SEQA framework uses Gemini 1.5 Flash to generate 10 paraphrased questions per original question while preserving semantic meaning and ground truth answers. The augmented datasets are then used to fine-tune three MVQA models (M2I2, MUMC, BiomedGPT) under zero-shot and fine-tuning settings. The method computes TAR-SC by grouping variant questions by original question and measuring mean accuracy per group, along with dataset diversity metrics (ANQI, ANQA, ANQS).

## Key Results
- SEQA framework achieved an average accuracy improvement of 19.35% on fine-tuned MVQA models
- TAR-SC improved by an average of 11.61%, indicating substantial enhancement in model consistency
- Dataset augmentation increased QA pairs from ~7k to ~68k (SLAKE) and ~3.5k to ~19k (VQA-RAD)
- ANQI/ANQA/ANQS metrics showed significant improvements across all three benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on linguistically diverse rephrasings of identical visual queries appears to regularize Medical VQA (MVQA) models, reducing their sensitivity to syntax variations.
- **Mechanism:** The SEQA framework leverages an LLM (Gemini 1.5 Flash) to expand the "neighborhood" of valid queries for a single visual answer. By mapping varied syntactic forms to the same ground truth, the model is constrained to learn a semantic representation that is invariant to phrasing.
- **Core assumption:** The generating LLM produces paraphrases that are strictly semantically equivalent and does not introduce factual drift or hallucinations during the augmentation process.
- **Evidence anchors:**
  - [abstract] The approach "enriches linguistic diversity while preserving semantic meaning."
  - [section 3.1] The prompt explicitly constrains the LLM: "Do not change the answer... ensure that newly generated questions are semantically equivalent."
  - [corpus] Related work ("Knowing or Guessing?") confirms that Med-VLMs exhibit fragility to phrasing fluctuations, validating the need for this regularization.
- **Break condition:** If the generating LLM introduces subtle semantic shifts (e.g., changing "Is the mass present?" to "Is the large mass present?"), the ground truth label becomes invalid for the new query, introducing label noise rather than robustness.

### Mechanism 2
- **Claim:** Optimizing for agreement across augmented variants (measured by TAR-SC) aligns the model's decision boundary with the core semantic intent of the question rather than superficial token patterns.
- **Mechanism:** TAR-SC penalizes models that are consistent but wrong, or correct but inconsistent. Fine-tuning to maximize this metric forces the visual-language alignment to be robust against the "semantic gaps" caused by synonymy and sentence structure changes.
- **Core assumption:** High consistency on rephrased questions correlates with general reliability in clinical deployment, a relationship assumed but not strictly proven by the metric itself.
- **Evidence anchors:**
  - [abstract] Fine-tuned models achieved an average TAR-SC improvement of 11.61%, indicating "substantial enhancement in model consistency."
  - [section 3.2] TAR-SC calculates the "mean score of correct answers for each set of semantically similar questions," bridging accuracy and consistency.
  - [corpus] The paper "Knowing or Guessing?" supports this by emphasizing that "consistent answering across diverse question phrasings is essential for reliable diagnosis."
- **Break condition:** If the test set augmentation is generated differently than the training set (distribution shift), or if the model overfits to the specific rephrasing style of the generator LLM, the metric may not generalize to human clinical questions.

### Mechanism 3
- **Claim:** Increasing the density of QA pairs per image (high ANQI/ANQA) improves the model's ability to disentangle visual features from linguistic priors.
- **Mechanism:** By presenting a single image with a high volume of varied questions (avg increase of ~85 questions per image), the model cannot rely on simple correlations between specific keywords and image types. It is forced to attend more strictly to the visual content to satisfy the varied linguistic conditions.
- **Core assumption:** The underlying visual features in the dataset are sufficiently distinct to support the discrimination required by the more granular question sets.
- **Evidence anchors:**
  - [section 5.1] The augmented datasets show a massive increase in Average Number of QA Items per Image (ANQI) and Average Number of Questions per Image with the Same Answer (ANQA).
  - [section 5.4] The authors note this density exposes models to "more linguistic diversity," reducing the influence of syntax/structure.
  - [corpus] "BioD2C" discusses constraining multimodal interaction, which aligns with the need to ground text in visual features more strictly.
- **Break condition:** If the visual encoder is weak or the image resolution is too low to support the fine-grained distinctions implied by the dense question sets, the model may resort to hallucination or overfitting to the training data distribution.

## Foundational Learning

- **Concept: Semantic Invariance in Multimodal Space**
  - **Why needed here:** The core problem is the "semantic gap" where different vectors in text space (different questions) must map to the same point in answer space. Learners must understand that a model must learn an embedding space where $d(\text{"What organ is this?"}, \text{"Identify the organ"}) \approx 0$.
  - **Quick check question:** Can you explain why a standard cross-entropy loss on a non-augmented dataset might fail to penalize a model that answers "Liver" correctly for "What is this?" but incorrectly for "Identify the organ"?

- **Concept: Consistency vs. Accuracy**
  - **Why needed here:** The paper distinguishes between reliability (general correctness) and consistency (stability). A model can be 90% accurate but dangerously inconsistent (e.g., randomly failing on paraphrases), which is unacceptable in clinical settings.
  - **Quick check question:** If a model achieves 100% Accuracy but 50% TAR-SC, what does that imply about its behavior on semantically equivalent rephrasings?

- **Concept: Black-box Reliability Estimation**
  - **Why needed here:** The authors use an external LLM to generate test neighborhoods because they cannot access the internal weights of the VLM (e.g., GPT-4 based systems). Learners must grasp how to probe opaque models using input perturbations.
  - **Quick check question:** Why is "neighborhood consistency" a viable proxy for reliability when internal confidence scores (softmax probabilities) are often miscalibrated?

## Architecture Onboarding

- **Component map:** Augmentor (LLM) -> Dataset Builder -> Target VLM -> Evaluator
- **Critical path:** The **Prompt Engineering** in the Augmentor (Section 3.1). The prompt must strictly forbid adding information ("Do not add additional information") and changing the answer. If this fails, the entire training signal is corrupted by noise.
- **Design tradeoffs:**
  - **Cost vs. Diversity:** Using Gemini 1.5 Flash is cost-effective but may result in less creative or diverse rephrasings compared to GPT-4, potentially limiting the robustness of the augmentation.
  - **Dataset Size vs. Training Time:** Increasing QA pairs from ~7k to ~68k (SLAKE) significantly increases training epochs and memory requirements for fine-tuning.
- **Failure signatures:**
  - **High Accuracy, Low TAR-SC:** The model learned the training data patterns but failed to generalize semantic understanding; it is brittle to syntax changes.
  - **Semantic Drift in Dataset:** Analysis of the augmented set shows questions like "Is the mass malignant?" generated from "Is there a mass?", causing incorrect ground truth alignment.
- **First 3 experiments:**
  1. **Consistency Baseline:** Run the target VLM (Zero-shot) on the unaugmented test set vs. the SEQA-augmented test set to quantify the initial "semantic gap" (performance drop).
  2. **Ablation on Generator:** Generate augmented datasets using different LLMs (e.g., Llama 3 vs. GPT-4) and measure the correlation between ANQS (diversity metric) and final TAR-SC to determine if "more diversity" always equals "better consistency."
  3. **Fine-tuning Validation:** Fine-tune the VLM on the SEQA dataset and evaluate TAR-SC specifically on *human-generated* paraphrases (if available) to ensure the model isn't just overfitting to the specific style of the LLM generator.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the SEQA framework maintain performance improvements when applied to larger and more diverse datasets like OmniMedVQA or PathVQA?
- Basis in paper: [explicit] The authors state, "Future work will focus on expanding the framework to test on larger and more diverse datasetsâ€” for example, by integrating datasets such as OmniMedVQA, PathVQA, and VQA-Med."
- Why unresolved: The current study limited experiments to SLAKE and VQA-RAD, which are smaller and may restrict the generalizability of the findings to broader clinical settings.
- What evidence would resolve it: Experimental results showing TAR-SC and accuracy improvements on these larger datasets when using the SEQA augmentation.

### Open Question 2
- Question: How does the potential for error propagation from LLM-generated rephrasings quantitatively affect the quality of augmented datasets and model performance?
- Basis in paper: [explicit] The authors note, "LLMs may introduce errors during the generation process... If these errors are not carefully addressed, they could... compromising its quality and, in turn, affecting the performance of the trained models."
- Why unresolved: While the limitation is identified, the paper does not quantify the rate of LLM generation errors or isolate their specific impact on the final model's consistency or accuracy.
- What evidence would resolve it: A comparative analysis of model performance using SEQA datasets with and without manual filtering or automated validation of the LLM-generated questions.

### Open Question 3
- Question: Does fine-tuning with the SEQA framework translate to improved clinical utility when evaluated by medical professionals in real-world scenarios?
- Basis in paper: [explicit] The authors plan to "evaluate our model in real-world clinical scenarios, incorporating feedback from medical professionals to refine their utility and safety in healthcare environments."
- Why unresolved: Current evaluation relies on automated metrics (Accuracy, TAR-SC), which the authors admit "may not fully capture certain nuances relevant to real-world clinical applications."
- What evidence would resolve it: Qualitative feedback and quantitative error analysis from clinicians using the models in a simulated or live clinical workflow.

## Limitations

- **Semantically equivalent generation quality**: The framework relies on LLM-generated paraphrases maintaining strict semantic equivalence. The paper acknowledges this assumption but does not provide quantitative evaluation of semantic drift in generated questions, which could introduce label noise during training.
- **Clinical relevance of consistency**: While TAR-SC measures agreement across rephrased questions, the clinical utility of this metric remains uncertain. High TAR-SC scores may not necessarily translate to better diagnostic reliability in real-world scenarios.
- **Dataset composition**: The augmented datasets show massive increases in question density (ANQI, ANQA), but the paper doesn't analyze whether this density introduces bias toward certain question types or medical conditions.

## Confidence

- **High confidence**: The methodology for dataset augmentation and the introduction of TAR-SC as a consistency metric are well-defined and reproducible. The significant improvements in TAR-SC (11.61% average) and accuracy (19.35% average) are supported by experimental results.
- **Medium confidence**: The claim that semantic invariance training improves general clinical reliability assumes that the augmented question distribution represents realistic clinical variability, which may not be fully validated.
- **Medium confidence**: The assumption that Gemini 1.5 Flash produces high-quality paraphrases without hallucinations is reasonable but unverified through external validation.

## Next Checks

1. **Semantic drift analysis**: Manually audit 100 randomly sampled augmented question pairs to measure the rate of semantic drift or hallucination introduction by the LLM.
2. **Cross-LLM generalization**: Repeat the augmentation process using different LLMs (e.g., GPT-4, Llama 3) to test whether improvements in TAR-SC are consistent across generation models.
3. **Human clinical validation**: Have medical experts evaluate model responses on the SEQA-augmented test set to determine if TAR-SC improvements correlate with clinically meaningful consistency.