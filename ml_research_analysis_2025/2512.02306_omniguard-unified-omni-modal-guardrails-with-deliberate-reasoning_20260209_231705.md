---
ver: rpa2
title: 'OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning'
arxiv_id: '2512.02306'
source_url: https://arxiv.org/abs/2512.02306
tags:
- safety
- https
- omni-modal
- audio
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniGuard, the first family of unified omni-modal
  guardrail models designed to perform safety moderation across text, images, videos,
  and audio with deliberate reasoning. To support training, the authors construct
  a large-scale omni-modal safety dataset of over 210K samples, covering unimodal
  and cross-modal inputs, each annotated with safety labels, violation categories,
  and expert-generated reasoning critiques via targeted distillation.
---

# OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning

## Quick Facts
- arXiv ID: 2512.02306
- Source URL: https://arxiv.org/abs/2512.02306
- Reference count: 40
- Primary result: OmniGuard-7B outperforms strong baselines including GPT-4o on 15 benchmarks; OmniGuard-3B matches or exceeds 235B models

## Executive Summary
OmniGuard introduces the first unified omni-modal guardrail family designed for safety moderation across text, images, videos, and audio. The approach employs deliberate reasoning through targeted distillation of expert-generated safety critiques, achieving superior performance compared to both proprietary and open-source baselines. The system outputs structured safety assessments including violation categories and natural language explanations, addressing the growing need for explainable safety moderation in multi-modal AI applications.

## Method Summary
OmniGuard uses full-parameter instruction tuning on Qwen2.5-Omni base models (7B/3B) with a mission-focused safety moderation template. The training process employs targeted distillation where expert teacher models (gpt-oss-120b for text, Qwen3-VL-235B for visual, Kimi-Audio-7B-Instruct for audio) generate safety reasoning critiques. The student model learns through a joint loss combining classification, category prediction, and autoregressive critique generation. Training uses SWIFT on 8×H100 GPUs with specific hyperparameters including AdamW optimizer, cosine scheduler, and gradient accumulation.

## Key Results
- OmniGuard-7B consistently outperforms baselines including GPT-4o, Qwen3-235B, and Qwen3-VL-235B on 15 benchmarks
- OmniGuard-3B achieves competitive performance, matching or exceeding much larger models
- Reasoning-augmented training improves cross-modal generalization compared to label-only SFT
- Cross-modal transfer shows minimal performance degradation between seen and unseen modalities (81.8 vs 79.4 F1)

## Why This Works (Mechanism)

### Mechanism 1: Targeted Distillation Transfers Reasoning Capacity
Distilling expert-generated safety critiques into a unified guardrail model improves both detection accuracy and cross-modal generalization compared to label-only supervision. Teacher models generate detailed natural language critiques conditioned on input, safety label, and violation categories. The student model minimizes a joint loss: L_total = L_cls + L_cat + L_critique, where L_critique is autoregressive generation loss aligning student critiques with teacher explanations. This transfers policy-aligned reasoning patterns rather than just decision boundaries.

### Mechanism 2: Mission-Focused Instruction Tuning Concentrates Model Capacity
Fixing the instruction template to omni-modal safety moderation while diversifying inputs maximizes safety reasoning capacity compared to general-purpose instruction tuning. Unlike general instruction tuning with diverse task types, this approach constrains the instruction space to a single mission (safety moderation), allowing the model to allocate representational capacity toward modality-variant input understanding and fine-grained policy reasoning rather than task-switching overhead.

### Mechanism 3: Cross-Modal Shared Representation Enables Zero-Shot Transfer
Training on three modalities transfers safety reasoning to a held-out fourth modality with minimal performance degradation. The omni-modal base model (Qwen2.5-Omni) maps heterogeneous inputs to a shared latent space. Safety reasoning supervision on seen modalities learns modality-invariant unsafe content representations that activate for semantically equivalent content in unseen modalities.

## Foundational Learning

- Concept: Omni-modal representation alignment
  - Why needed here: Understanding how Qwen2.5-Omni maps text, image, video, and audio to a shared embedding space is prerequisite for diagnosing cross-modal transfer failures.
  - Quick check question: Can you explain why semantically similar content across modalities produces comparable activations in the shared latent space?

- Concept: Knowledge distillation loss composition
  - Why needed here: The joint objective (L_cls + L_cat + L_critique) requires understanding how classification and generation losses interact during training.
  - Quick check question: If L_critique dominates training, what symptom would you expect in category classification accuracy?

- Concept: Policy-guided safety taxonomy
  - Why needed here: The model outputs violation categories from a predefined set C={c_1, ..., c_m}; understanding taxonomy design is essential for extending to new policies.
  - Quick check question: How would adding a new violation category require changes to the training data and model output structure?

## Architecture Onboarding

- Component map: Input layer (Qwen2.5-Omni encoder) -> Fusion (shared latent space) -> Task head (three-output decoder: safety_label, violation_categories, reasoning_critique) -> Training pipeline (targeted distillation → SFT on constructed dataset)

- Critical path: 1) Dataset construction (aggregate 210K+ samples across modalities) → 2) Teacher critique generation (modality-specific expert models) → 3) Joint loss training (L_cls + L_cat + L_critique minimization) → 4) Inference with structured output parsing

- Design tradeoffs: Inference latency vs. interpretability (reasoning generation adds overhead), Parameter efficiency vs. modality coverage (3B model competitive with 235B baselines), Dataset diversity vs. taxonomy consistency (unifying multiple sources required)

- Failure signatures: All-safe predictions (L_cls collapse, check class balance), Missing violation categories (L_cat underweighting or imbalance), Generic critiques without modality-specific reasoning (insufficient critique diversity), Cross-modal performance drop on specific modality pair (missing cross-modal training samples)

- First 3 experiments: 1) Ablate reasoning component: Train label-only variant and compare to full model on cross-modal benchmarks to validate L_critique contribution, 2) Held-out modality test: Train on three modalities, evaluate on fourth to measure transfer gap, 3) Latency profiling: Measure inference time with and without critique generation to quantify interpretability cost

## Open Questions the Paper Calls Out

- How can the trade-off between safety reasoning depth and inference latency be optimized for resource-constrained or real-time omni-modal applications?
- How can guardrail models effectively moderate complex, interleaved multimodal safety scenarios where inputs are temporally mixed rather than static?
- How can cross-modal safety training mitigate negative transfer or dependency when semantically equivalent data are not jointly present during training?

## Limitations
- Dataset generalization concerns due to aggregated sources with potentially inconsistent annotation standards
- Heavy dependency on proprietary teacher models for reasoning critique generation
- Cross-modal transfer assumptions may not hold for modality-specific harm patterns
- Policy rigidity limits adaptability to evolving safety guidelines

## Confidence

- OmniGuard performance superiority: Medium confidence
- Reasoning augmentation improves cross-modal generalization: Medium confidence
- Cross-modal transfer effectiveness: Low confidence

## Next Checks

1. Independent Teacher Model Validation: Replicate the reasoning critique generation using alternative teacher models on a held-out sample set to verify performance improvements are not artifacts of specific teacher model reasoning patterns.

2. Real-World Deployment Test: Deploy OmniGuard on an external safety moderation dataset from a different domain to assess performance degradation and identify modality-specific failure modes.

3. Policy Evolution Stress Test: Conduct a simulated policy update experiment where violation categories are modified or new categories added, then measure the retraining requirements and performance recovery time compared to a more flexible architecture.