---
ver: rpa2
title: Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive
  Evidence Re-ranking
arxiv_id: '2512.05012'
source_url: https://arxiv.org/abs/2512.05012
tags:
- retrieval
- contrastive
- evidence
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving retrieval accuracy
  and factual reliability in Retrieval-Augmented Generation (RAG) systems, particularly
  in safety-critical domains like healthcare. The proposed method, Self-Explaining
  Contrastive Evidence Re-ranking (CER), combines triplet-based contrastive learning
  with token-level attribution rationales to fine-tune embeddings.
---

# Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking

## Quick Facts
- arXiv ID: 2512.05012
- Source URL: https://arxiv.org/abs/2512.05012
- Authors: Francielle Vargas; Daniel Pedronette
- Reference count: 18
- One-line primary result: CER improves retrieval accuracy by restructuring embedding space to separate factual from non-evidential content in clinical trial reports.

## Executive Summary
This paper addresses the challenge of improving retrieval accuracy and factual reliability in Retrieval-Augmented Generation (RAG) systems, particularly in safety-critical domains like healthcare. The proposed method, Self-Explaining Contrastive Evidence Re-ranking (CER), combines triplet-based contrastive learning with token-level attribution rationales to fine-tune embeddings. This approach restructures the embedding space so that factual evidence is pulled closer to the query while subjective or misleading content is pushed apart. Hard negatives are selected using a subjectivity-based criterion to enhance discrimination.

Evaluated on clinical trial reports, CER significantly improved retrieval accuracy by creating clearer separation between evidential and non-evidential embeddings, as shown in UMAP projections. The fine-tuned model achieved better intra-class cohesion and inter-class separation compared to the baseline, with average distances for positive pairs (Intra-Pos) increasing from 0.5716 to 0.7766 and for negative pairs (Intra-Neg) from 0.5977 to 0.8141, while the inter-class distance (Inter) increased from 0.5901 to 0.8110. The method also demonstrated robustness across both cosine and Euclidean distance metrics, providing transparent, evidence-based retrieval to mitigate hallucinations and enhance reliability.

## Method Summary
CER fine-tunes Contriever using a cosine and euclidean triplet loss to restructure the embedding space, pulling factual rationales closer while pushing subjective or misleading explanations apart. The method uses a two-stage pipeline: (1) contrastive retrieval with triplet-based contrastive learning and subjectivity-based hard negative selection, and (2) self-explaining re-ranking with token-level attribution rationales. The approach is evaluated on clinical trial reports, showing improved retrieval accuracy through clearer separation between evidential and non-evidential embeddings in UMAP projections.

## Key Results
- CER improved embedding separation: Intra-Pos increased from 0.5716 to 0.7766, Intra-Neg from 0.5977 to 0.8141, and Inter from 0.5901 to 0.8110.
- The method achieved better intra-class cohesion and inter-class separation compared to baseline.
- CER demonstrated robustness across both cosine and Euclidean distance metrics.
- UMAP visualizations showed clearer separation between evidential (positive) and non-evidential (negative) embeddings after fine-tuning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Triplet-based contrastive learning with subjectivity-aware hard negatives restructures embedding space to separate evidential from non-evidential content.
- Mechanism: The model is trained using triplet loss where anchors (queries) are paired with factual evidence (positives) and subjective/misleading passages (hard negatives). This explicitly maximizes inter-class distance while maintaining intra-class cohesion.
- Core assumption: Subjectivity-based negative selection meaningfully correlates with non-evidential content; the criterion accurately identifies misleading passages.
- Evidence anchors:
  - [abstract] "Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart."
  - [section 4] "The UMAP projections show that our fine-tuned model, trained with the proposed subjectivity-based triplet selection, produces a much clearer separation between evidential (positive) and non-evidential (negative) embeddings."
  - [corpus] Limited direct corpus support for this specific subjectivity-criterion mechanism; related work (Sriram et al.) uses GPT-4 distillation and human annotations for contrastive fact-checking but not subjectivity-based selection.
- Break condition: If subjectivity criterion fails to distinguish misleading from factual content (e.g., objective-sounding misinformation), hard negatives become noisy, degrading embedding separation.

### Mechanism 2
- Claim: Fine-tuning Contriever with combined cosine and Euclidean triplet loss yields robust distance metrics across embedding space operations.
- Mechanism: Dual-metric optimization prevents overfitting to a single distance formulation, ensuring retrieval quality generalizes across similarity computations used in downstream RAG pipelines.
- Core assumption: Both metrics capture complementary geometric properties relevant to evidential similarity.
- Evidence anchors:
  - [abstract] "We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy..."
  - [section 3] "CER fine-tunes Contriever [7] using a cosine and euclidean triplet loss to restructure the embedding space"
  - [section 4] "We also evaluated the efficiency between applying Euclidean and cosine distance metrics"
  - [corpus] Weak corpus signal on dual-metric training; related papers focus on single-metric retrieval quality.
- Break condition: If downstream retrieval uses a different distance metric (e.g., dot product without normalization), performance gains may not transfer.

### Mechanism 3
- Claim: Token-level attribution rationales provide transparency and ground re-ranking decisions in verifiable evidence spans.
- Mechanism: Self-explaining component generates fine-grained attribution for each retrieved passage, enabling interpretable selection of passages whose rationales align with query intent.
- Core assumption: Token-level attributions accurately reflect evidence contribution to query relevance.
- Evidence anchors:
  - [abstract] "generating token-level attribution rationales for each retrieved passage"
  - [section 3] "self-explaining re-ranking, which provides fine-grained attribution signals to identify the most query-aligned passages"
  - [limitations] "no assessment has yet been conducted on the quality of the explanations generated by the self-explaining component"
  - [corpus] No corpus evidence on attribution quality for this method; faithfulness-aware UQ work suggests attribution evaluation remains an open challenge.
- Break condition: If attributions are unfaithful (highlighting spurious tokens), transparency claim fails and re-ranking may select non-evidential passages.

## Foundational Learning

- Concept: **Triplet Loss and Contrastive Learning**
  - Why needed here: Core training objective that restructures embedding space via anchor-positive-negative comparisons.
  - Quick check question: Given query Q, factual passage P, and misleading passage N, can you sketch how triplet loss would update embeddings to satisfy margin constraints?

- Concept: **Dense Retrieval and Embedding Spaces**
  - Why needed here: CER operates on Contriever embeddings; understanding distance metrics (cosine, Euclidean) and clustering is essential.
  - Quick check question: Why might cosine similarity and Euclidean distance produce different rankings for the same embedding set?

- Concept: **RAG Retrieval-Generation Coupling**
  - Why needed here: The method addresses how noisy retrieval propagates hallucinations to generation outputs.
  - Quick check question: If retrieval returns topically similar but non-evidential passages, what failure modes emerge in LLM generation?

## Architecture Onboarding

- Component map: Document chunking -> Contriever fine-tuning (triplet loss with subjectivity-based hard negatives) -> Contextual retrieval -> Token-level attribution generation -> Passage re-ranking based on rationale-query alignment -> Grounded response

- Critical path: Quality of hard negative selection -> Embedding space separation -> Retrieval precision@K -> Final answer factual grounding

- Design tradeoffs:
  - Subjectivity criterion specificity vs. coverage (overly strict may miss hard negatives; overly loose introduces noise)
  - Dual-metric training complexity vs. single-metric simplicity
  - Rationale generation overhead vs. transparency benefit

- Failure signatures:
  - Embedding overlap in UMAP (baseline signature: Intra-Pos ≈ Intra-Neg ≈ Inter ≈ 0.59)
  - Topical similarity conflated with evidential adequacy (e.g., "lemon" and "cancer" co-occurrence without clinical evidence)
  - Unfaithful attributions highlighting non-evidential tokens

- First 3 experiments:
  1. Replicate UMAP visualization on held-out clinical trial subset; verify inter-class distance increases from baseline ~0.59 toward ~0.81.
  2. Ablate subjectivity criterion: replace hard negatives with random negatives; measure precision@5 and recall@5 degradation.
  3. Evaluate attribution faithfulness manually on 50 retrieved passages: do highlighted tokens correspond to actual evidence spans supporting the query?

## Open Questions the Paper Calls Out

- How does the quality of token-level attribution rationales generated by the self-explaining component affect human verification efficiency in safety-critical contexts?
  - Basis in paper: [explicit] The authors explicitly state in the Limitations section that "no assessment has yet been conducted on the quality of the explanations generated by the self-explaining component."
  - Why unresolved: While the paper claims transparency is a core outcome, the actual utility, readability, and fidelity of these rationales remain unverified, leaving a gap between the proposed mechanism and the claimed benefit of interpretable, evidence-based retrieval.
  - What evidence would resolve it: A human evaluation study measuring the speed and accuracy with which medical experts can verify claims using the generated rationales compared to baseline retrieval methods.

- To what extent does the improved embedding separation achieved by CER translate into reduced hallucination rates in the final generated text?
  - Basis in paper: [explicit] The authors acknowledge that "More robust experiments are still required to thoroughly evaluate the proposed method... [on] the re-ranking stage" and that current findings are "preliminary evidence."
  - Why unresolved: The paper demonstrates improved geometric separation in the embedding space (UMAP projections) and retrieval metrics, but it does not present quantitative metrics on the factual consistency or hallucination rates of the final LLM outputs.
  - What evidence would resolve it: End-to-end evaluation using factuality metrics (e.g., FACTSCORE or precision/recall of generated claims) on the downstream generation task.

- How robust is the automated subjectivity-based criterion for hard negative selection across diverse medical datasets?
  - Basis in paper: [inferred] The method relies on a specific "subjectivity-based criterion" to identify hard negatives, but the paper provides no ablation study or error analysis regarding the accuracy of this automated selection.
  - Why unresolved: If the criterion misclassifies objective evidence as subjective (or vice versa), the contrastive learning objective could inadvertently push factual evidence apart, degrading retrieval quality in unseen corpora.
  - What evidence would resolve it: An ablation study comparing model performance when using automated subjectivity-based negatives versus human-curated hard negatives on a distinct clinical dataset.

## Limitations
- The subjectivity-based criterion for hard negative selection is not specified, making it impossible to reproduce the exact methodology.
- No assessment has been conducted on the quality of the explanations generated by the self-explaining component.
- The method is evaluated exclusively on clinical trial reports, limiting generalizability to other domains.

## Confidence
- High Confidence: The contrastive learning mechanism with triplet loss for embedding space restructuring is well-established and the reported distance metric improvements are statistically significant and verifiable through UMAP visualization.
- Medium Confidence: The dual-metric (cosine + Euclidean) training approach is theoretically sound, but the practical benefit over single-metric training remains unclear without ablation studies.
- Low Confidence: The subjectivity-based hard negative selection algorithm, the quality of generated attribution rationales, and the method's performance on non-clinical domains all lack sufficient evidence or validation to support the claimed benefits.

## Next Checks
1. **Subjectivity Criterion Validation**: Implement and test multiple subjectivity detection approaches (lexical heuristics, fine-tuned classifiers, GPT-4-based assessment) on the clinical trial corpus. Measure precision/recall of identifying genuinely misleading vs. factual passages and correlate with downstream retrieval performance degradation when using random vs. subjectivity-based negatives.

2. **Attribution Faithfulness Assessment**: Conduct a human evaluation study where domain experts rate the faithfulness of token-level attributions on 100 randomly sampled retrieved passages. Calculate the percentage of attributions that correctly highlight evidence spans supporting the query versus spurious or irrelevant tokens.

3. **Cross-Domain Generalization Test**: Apply CER to a non-clinical domain (e.g., legal case law or news articles) using the same training procedure. Measure whether the embedding separation improvements (Intra-Pos, Intra-Neg, Inter distance changes) and retrieval accuracy gains transfer, or if the subjectivity criterion requires domain-specific adaptation.