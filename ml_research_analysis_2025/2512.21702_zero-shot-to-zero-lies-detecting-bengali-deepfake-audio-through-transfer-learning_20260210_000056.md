---
ver: rpa2
title: 'Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer
  Learning'
arxiv_id: '2512.21702'
source_url: https://arxiv.org/abs/2512.21702
tags:
- audio
- detection
- deepfake
- arxiv
- bengali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting deepfake audio
  in the Bengali language, a task that has received little attention despite the growing
  threat of synthetic speech. We investigate both zero-shot and fine-tuned approaches
  using the BanglaFake dataset.
---

# Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning

## Quick Facts
- arXiv ID: 2512.21702
- Source URL: https://arxiv.org/abs/2512.21702
- Reference count: 37
- This study presents the first systematic benchmark for Bengali audio deepfake detection, showing that fine-tuning pretrained models achieves 79.17% accuracy versus 53.80% for zero-shot approaches.

## Executive Summary
This study addresses the challenge of detecting deepfake audio in the Bengali language, a task that has received little attention despite the growing threat of synthetic speech. We investigate both zero-shot and fine-tuned approaches using the BanglaFake dataset. Zero-shot inference with pretrained models such as Wav2Vec2-XLSR-53, Whisper, and WavLM shows limited performance, with the best accuracy at 53.80% and EER of 46.20%. Fine-tuning six architectures, including Wav2Vec2-Base, ResNet18, and ViT-B16, yields substantial improvements, with ResNet18 achieving 79.17% accuracy, 79.12% F1 score, 84.37% AUC, and 24.35% EER. The results demonstrate that fine-tuning is essential for effective detection in low-resource languages. This work provides the first systematic benchmark for Bengali audio deepfake detection and highlights the effectiveness of deep learning models when adapted to domain-specific data.

## Method Summary
The study uses the BanglaFake dataset containing 12,260 real and 13,260 synthetic Bengali audio files (6-7 seconds each at 22,050 Hz, resampled to 16 kHz). Models are evaluated using 70:15:15 train/validation/test splits. Mel-spectrograms are extracted (64-128 bands, converted to dB scale, normalized) and resized to 224×224 images with 3 channels for CNN and ViT architectures. Six architectures are fine-tuned: Wav2Vec2-Base, ResNet18, ViT-B16, LCNN, LCNN-Attention, and CNN-BiLSTM. Training uses Adam optimizer (LR=0.0001, except 0.00005 for Wav2Vec2), binary cross-entropy loss, batch sizes of 4-32, and early stopping. Performance metrics include accuracy, F1 score, Equal Error Rate (EER), and AUC.

## Key Results
- Fine-tuned ResNet18 achieves 79.17% accuracy, 79.12% F1 score, 24.35% EER, and 84.37% AUC on BanglaFake test set
- Zero-shot inference with Wav2Vec2-XLSR-53 achieves only 53.80% accuracy and 46.20% EER
- LCNN-Attention achieves best AUC (88.48%) and lowest EER (23.01%) but requires 14 epochs
- Wav2Vec2-Base shows highest recall (98.49%) but lowest precision (53.37%), indicating bias toward "fake" predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning pretrained models on domain-specific data significantly improves Bengali deepfake detection compared to zero-shot inference.
- **Mechanism:** Pretrained models encode general audio representations from high-resource languages, but fine-tuning adapts feature extractors and classification boundaries to Bengali-specific acoustic patterns and synthesis artifacts present in the BanglaFake dataset.
- **Core assumption:** Bengali acoustic characteristics and VITS-based deepfake artifacts differ sufficiently from pretraining distributions that task-specific adaptation is required.
- **Evidence anchors:**
  - [abstract]: "Experimental results confirm that fine-tuning significantly improves performance over zero-shot inference"
  - [Table IV]: Fine-tuned ResNet18 achieves 79.17% accuracy vs. zero-shot Wav2Vec2-XLSR-53 at 53.80%
  - [corpus]: Limited direct evidence—related papers focus on dataset construction (BanglaFake) rather than fine-tuning mechanisms for Bengali

### Mechanism 2
- **Claim:** Residual connections in ResNet18 enable effective hierarchical feature extraction from mel-spectrograms for spectro-temporal artifact detection.
- **Mechanism:** Skip connections allow the network to learn both low-level acoustic inconsistencies and high-level synthetic patterns while mitigating gradient degradation in deeper layers processing spectrogram representations.
- **Core assumption:** Deepfake artifacts manifest as detectable patterns in the spectral domain that benefit from multi-scale hierarchical processing.
- **Evidence anchors:**
  - [section IV]: "Mel-spectrograms are expanded to 3-channel images. Features are extracted using a pretrained ResNet18 backbone. A classifier layer produces logits for deepfake detection"
  - [Table IV]: ResNet18 achieves highest accuracy (79.17%) and F1 (79.12%) among all tested architectures
  - [corpus]: Related work [12], [13] reports ResNet18 with LFCC/CQCC features effective for deepfake detection

### Mechanism 3
- **Claim:** Attention mechanisms over the time axis improve detection by emphasizing temporally localized deepfake artifacts.
- **Mechanism:** Attention layers learn dynamic weights across time steps, amplifying regions with stronger synthetic inconsistencies while suppressing irrelevant or clean audio segments.
- **Core assumption:** Deepfake artifacts are non-uniformly distributed across audio duration, concentrating in specific temporal regions.
- **Evidence anchors:**
  - [section IV]: "Attention is applied over the time axis to highlight important temporal features. Outputs pass through fully connected layers for binary classification"
  - [Table IV]: LCNN-Attention achieves 78.43% accuracy and 23.01% EER vs. LCNN baseline at 48.36% accuracy and 61.23% EER
  - [corpus]: Related work [13] mentions attention mechanisms improving robustness in noisy conditions

## Foundational Learning

- **Concept: Transfer Learning and Domain Shift**
  - Why needed here: The study demonstrates that pretrained models fail at zero-shot Bengali detection, requiring fine-tuning to address domain shift
  - Quick check question: Why does Wav2Vec2-XLSR-53, trained on 53 languages, still achieve only 53.8% accuracy on Bengali deepfake detection without fine-tuning?

- **Concept: Mel-Spectrogram as Image Representation**
  - Why needed here: Image-based architectures (ResNet18, ViT-B16) process audio via spectrogram-to-image conversion, requiring understanding of this transformation
  - Quick check question: How does converting 1D audio to a 2D spectrogram enable use of vision architectures for audio classification?

- **Concept: Equal Error Rate (EER) for Spoof Detection**
  - Why needed here: EER is the primary security-focused metric; lower EER indicates better discrimination at the optimal threshold
  - Quick check question: If a model has 79% accuracy but 24% EER, what does this suggest about its real-world reliability for security applications?

## Architecture Onboarding

- **Component map:**
  Raw Audio (16kHz, 5 sec fixed) → Mel-Spectrogram (64-128 bands) → dB scale → Normalize → Resize to 224×224, replicate to 3 channels → [Model Selection] → Binary Output (Real/Fake logits)

- **Critical path:**
  1. Resample all audio to 16kHz, truncate/pad to 5 seconds
  2. Generate mel-spectrograms with 64-128 bands, convert to dB, normalize
  3. For ResNet18: expand to 3-channel 224×224 images
  4. Fine-tune with Adam (LR=0.0001), binary cross-entropy loss, early stopping
  5. Evaluate on held-out 15% test split using accuracy, F1, EER, AUC

- **Design tradeoffs:**
  - ResNet18: Best accuracy (79.17%), fastest convergence (3 epochs), no explicit temporal modeling—best for compute-constrained deployment
  - LCNN-Attention: Best AUC (88.48%) and lowest EER (23.01%), but requires 14 epochs—best for security-critical applications prioritizing low false acceptance
  - ViT-B16: Strong EER (22.26%), smallest batch size (8) required—best when memory is limited but transformer inductive bias desired
  - Wav2Vec2-Base: Highest recall (98.49%), lowest precision (53.37%), computationally expensive (1 epoch only)—best when missing fakes is costlier than false alarms

- **Failure signatures:**
  - Zero-shot models cluster near 50% accuracy → indicates pretrained representations lack Bengali deepfake-specific features
  - High recall / low precision (e.g., Wav2Vec2-Base: 98.49% recall, 53.37% precision) → model biased toward "fake" predictions, likely due to class imbalance or decision threshold
  - LCNN without attention: 48.36% accuracy (below random) → architectural insufficiency for this task
  - EER > 40% for all zero-shot models → unreliable for security applications without fine-tuning

- **First 3 experiments:**
  1. **Reproduce ResNet18 baseline:** Train ResNet18 on BanglaFake with mel-spectrogram input (224×224, 3-channel), Adam optimizer, LR=0.0001, batch=32, 3 epochs. Target: ~79% accuracy, EER <25%.
  2. **Quantify attention contribution:** Train LCNN and LCNN-Attention with identical hyperparameters (LR=0.0001, batch=16, 14 epochs). Expected delta: ~30% accuracy improvement from attention mechanism.
  3. **Validate zero-shot ceiling:** Run Wav2Vec2-XLSR-53 inference on BanglaFake test split without fine-tuning. Confirm ~54% accuracy and ~46% EER to establish baseline for improvement claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Bengali deepfake detection models generalize to synthesis techniques not seen during training?
- Basis in paper: [explicit] "Future work should expand the dataset to cover more speakers and diverse synthesis techniques... Robustness against adversarial attacks and unseen deepfake generation methods should be investigated."
- Why unresolved: The BanglaFake dataset contains only VITS-based synthetic audio; models were not evaluated against other TTS systems, voice conversion methods, or emerging LLM-based audio synthesis.
- What evidence would resolve it: Benchmark fine-tuned models on Bengali deepfakes generated by diverse, unseen synthesizers (e.g., Tacotron, FastSpeech, voice cloning systems) and report cross-synthesis performance.

### Open Question 2
- Question: How can cross-lingual transfer learning improve deepfake detection for low-resource languages like Bengali?
- Basis in paper: [explicit] "Cross-lingual transfer learning can be explored to improve performance in low-resource conditions."
- Why unresolved: Zero-shot models pretrained on multilingual data performed poorly (best 53.8% accuracy); the paper did not investigate whether strategic transfer from high-resource languages could close this gap.
- What evidence would resolve it: Train models using multi-stage transfer (e.g., English→multilingual→Bengali) and compare against Bengali-only fine-tuning.

### Open Question 3
- Question: Can lightweight architectures achieve comparable detection performance for real-time deployment?
- Basis in paper: [explicit] "Lightweight models are required for real-time applications and deployment in resource-constrained environments."
- Why unresolved: The best-performing models (ResNet18, ViT-B16) have substantial computational requirements; no efficiency-performance tradeoff analysis was conducted.
- What evidence would resolve it: Evaluate compact architectures (e.g., MobileNet, distilled transformers) measuring latency, memory, and accuracy on Bengali deepfake detection.

### Open Question 4
- Question: Would integrating prosodic and linguistic features improve detection reliability beyond acoustic-only approaches?
- Basis in paper: [explicit] "Future research should also integrate prosodic and linguistic features with acoustic cues to enhance detection reliability."
- Why unresolved: Current models rely solely on acoustic/spectrogram features; Bengali-specific prosodic patterns and linguistic structure remain unexplored for deepfake detection.
- What evidence would resolve it: Develop multimodal models combining acoustic features with prosodic features (pitch, duration patterns) and compare against acoustic-only baselines.

## Limitations

- The study lacks specification of mel-spectrogram extraction parameters (FFT size, hop length) beyond "standard" settings, making exact reproduction challenging.
- Zero-shot classification methodology for pretrained models is underspecified—particularly how models like Whisper and AST generate binary predictions without classification heads.
- The LCNN architecture details, including MFM layer configurations, are incomplete.
- The study focuses exclusively on VITS-based synthesis artifacts, limiting generalizability to other deepfake generation methods.

## Confidence

- High confidence: Fine-tuning effectiveness (clear quantitative improvement from ~54% to ~79% accuracy)
- Medium confidence: Architectural comparisons (relative performance differences are robust, but absolute numbers depend on unspecified hyperparameters)
- Low confidence: Zero-shot methodology details (insufficient description of how pretrained models produce binary classifications)

## Next Checks

1. **Reproduce core finding:** Fine-tune ResNet18 on BanglaFake following the described preprocessing pipeline (16 kHz, 5 sec, mel-spectrogram, 224×224, 3-channel) with Adam optimizer (LR=0.0001, batch=32, 3 epochs) to verify the 79.17% accuracy claim

2. **Validate architectural contribution:** Train both LCNN and LCNN-Attention with identical hyperparameters to quantify the attention mechanism's ~30% accuracy improvement

3. **Establish zero-shot baseline:** Run Wav2Vec2-XLSR-53 inference on the BanglaFake test set without fine-tuning to confirm the ~54% accuracy and ~46% EER ceiling for pretrained models