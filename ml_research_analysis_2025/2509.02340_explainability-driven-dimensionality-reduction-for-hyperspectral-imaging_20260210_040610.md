---
ver: rpa2
title: Explainability-Driven Dimensionality Reduction for Hyperspectral Imaging
arxiv_id: '2509.02340'
source_url: https://arxiv.org/abs/2509.02340
tags:
- bands
- band
- spectral
- hyperspectral
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of high-dimensionality in hyperspectral
  imaging, which introduces computational burden and redundancy. The authors propose
  an exploratory study applying post-hoc explainability methods to guide band selection,
  reducing spectral dimension while preserving predictive performance.
---

# Explainability-Driven Dimensionality Reduction for Hyperspectral Imaging

## Quick Facts
- arXiv ID: 2509.02340
- Source URL: https://arxiv.org/abs/2509.02340
- Authors: Salma Haidar; José Oramas
- Reference count: 40
- One-line primary result: Explainability-guided band selection reduces HSI spectral dimension to ~30 bands while maintaining or improving classification accuracy (91-99%) on public benchmarks.

## Executive Summary
This work addresses the challenge of high-dimensionality in hyperspectral imaging, which introduces computational burden and redundancy. The authors propose an exploratory study applying post-hoc explainability methods to guide band selection, reducing spectral dimension while preserving predictive performance. Their four-stage methodology involves classification, explainability, evaluation, and band selection/retraining. Experiments on two public benchmarks (Pavia University and Salinas) demonstrate that classifiers trained on as few as 30 selected bands match or exceed full-spectrum baselines. The resulting subsets align with physically meaningful, highly discriminative wavelength regions, indicating that model-aligned, explanation-guided band selection is a principled route to effective dimensionality reduction for HSI.

## Method Summary
The methodology involves four stages: (1) Train baseline CNN models (Tri-CNN or HSI-CNN) on full spectral data; (2) Apply post-hoc explainability methods (LRP, SHAP, RISE) to generate band-wise relevance scores; (3) Evaluate explanation faithfulness using deletion/insertion tests to rank bands by influence; (4) Select top-k bands (typically 30) and retrain models on reduced spectral subsets. The approach uses overlapping patches from Pavia University (103 bands) and Salinas (204 bands) datasets, with specific train/test splits for each benchmark.

## Key Results
- Classification accuracy of 91-99% achieved with only 30 selected bands versus full spectra
- Band subsets align with physically meaningful wavelength regions (e.g., chlorophyll absorption at 0.67 µm)
- Performance plateaus after 30 bands, with negligible accuracy gains from adding more bands (<1% increase at 50 bands)
- Tri-CNN architecture shows better robustness to spectral reduction than HSI-CNN

## Why This Works (Mechanism)

### Mechanism 1: Model-Aligned Sensitivity Mapping
If an explainability method correctly identifies the spectral bands a trained model relies on, removing those bands should cause a rapid decline in prediction confidence. Post-hoc explainers (LRP, SHAP, RISE) assign relevance scores to input bands. By iteratively removing bands with the highest scores (Deletion test) and measuring the area under the confidence curve (AUC), the method isolates bands that causally drive the model's output. This transforms feature attribution into a selection filter. The core assumption is that the model has learned to associate specific spectral signatures with class labels, rather than relying on spatial artifacts or noise, and the explainer faithfully captures this reliance.

### Mechanism 2: Physical-Semantic Convergence
Model-derived importance aligns with known physical absorption features (e.g., chlorophyll, SWIR) because the CNN must exploit these discriminative regions to minimize loss. The CNN learns weights that maximize class separation. Since classes (e.g., vegetation vs. asphalt) are physically defined by chemical composition, the network's internal representation naturally weights the corresponding wavelengths (e.g., red-edge, moisture bands) higher. XAI methods expose this hidden weighting. The core assumption is that the spectral resolution is sufficient to capture distinct physical signatures, and the training data distribution covers the variance of these signatures.

### Mechanism 3: Redundancy Elimination via Semantic Aggregation
High-dimensional HSI data contains significant inter-band correlation; selecting bands based on "influence scores" preserves the decision boundary while discarding correlated or uninformative neighbors. Instead of selecting bands based on statistical variance (PCA) or clustering, this method selects based on decision contribution. If Band 50 and Band 52 are highly correlated but Band 50 has 10x the relevance score for the final class logit, the method implicitly keeps the "causal" band and discards the redundant one during the top-k selection. The core assumption is that decision-contribution is a more robust proxy for information content than statistical variance alone.

## Foundational Learning

- **Concept: Spectral Signatures & Absorption Features**
  - **Why needed here:** To interpret why the model selects specific bands (e.g., 0.67 µm for chlorophyll) and validate that the XAI isn't hallucinating.
  - **Quick check question:** If a model selects bands around 1.4 µm or 1.9 µm for vegetation analysis, is this likely a valid feature or an artifact of water absorption noise?

- **Concept: Perturbation-based Explanation (Deletion/Insertion)**
  - **Why needed here:** The paper relies on these metrics to prove the XAI method is "faithful." You must understand that a lower Deletion AUC (fast confidence drop) means a better explanation.
  - **Quick check question:** If a Deletion curve stays flat (high AUC) until the very end, what does that tell you about the ranked relevance of the removed bands?

- **Concept: Post-hoc XAI (LRP vs. SHAP vs. RISE)**
  - **Why needed here:** To troubleshoot why the paper uses three methods and why they might select slightly different bands (LRP is gradient-based, SHAP is perturbation-based).
  - **Quick check question:** Which method (black-box perturbation vs. white-box gradient) would you trust more if the model is overfitting to noise?

## Architecture Onboarding

- **Component map:** Input patches → Teacher CNN → Explainers (LRP, SHAP, RISE) → Evaluator (Deletion/Insertion) → Selector (Top-k Ranker) → Student CNN (retrained)
- **Critical path:** The evaluation loop (Stage 3). If the explainer ranks bands poorly (low faithfulness), the Student model will be trained on garbage data. You must verify the Deletion AUC is low before retraining.
- **Design tradeoffs:** Tri-CNN vs. HSI-CNN: Tri-CNN is more robust to spectral reduction (multi-scale spatial features help), whereas HSI-CNN (2D converted) loses more accuracy when bands are cut. 30 vs. 50 Bands: 30 is the "efficiency sweet spot." Moving to 50 adds compute cost with negligible accuracy gain (<1%).
- **Failure signatures:** Flat Deletion Curve: The XAI method is no better than random guessing; do not use these bands. Physically Incoherent Selection: If top bands fall in known water-absorption zones or are scattered randomly across the spectrum, the model has likely learned sensor artifacts. High Variance in Retraining: If accuracy fluctuates wildly across runs with selected bands, the subset is likely too small (information loss) or unstable.
- **First 3 experiments:** Sanity Check: Train a baseline on random 30 bands vs. explainability-selected 30 bands. The gap quantifies the value added by the XAI stage. Ablation by Band Count: Retrain Student models on Top-15, Top-30, and Top-50 bands to plot the saturation curve (Figure 4) for your specific dataset. Cross-Model Consistency: Use LRP to extract bands from Model A, then train Model B on those bands. This tests if the selected bands are "universally" good features or just specific to the Teacher model's idiosyncrasies.

## Open Questions the Paper Calls Out
None

## Limitations
- The faithfulness of XAI methods (LRP, SHAP, RISE) in capturing the model's true decision boundaries is assumed but not empirically validated against ground-truth causal features.
- The methodology relies on CNNs trained on specific HSI datasets (Pavia, Salinas) with fixed patch sizes. Generalization to different sensors, spatial resolutions, or spectral ranges remains untested.
- The claim that explainability-driven selection is "more principled" than statistical methods lacks direct comparison with PCA, band clustering, or other dimensionality reduction techniques on identical benchmarks.

## Confidence
- **High confidence**: Classification accuracy improvements (91-99%) when using 30 selected bands versus full spectra, as these are direct experimental results.
- **Medium confidence**: Physical alignment of selected bands (e.g., 0.67 µm for chlorophyll) - while consistent with known spectral signatures, the paper doesn't establish causal relationships between band selection and physical properties.
- **Low confidence**: The claim that explainability-driven selection is "more principled" than statistical methods - the paper lacks direct comparison with PCA-based band selection on identical benchmarks.

## Next Checks
1. **Cross-sensor validation**: Apply the selected 30-band subset from Pavia to a completely different HSI dataset (e.g., Indian Pines) to test generalization across sensors and domains.
2. **Ground-truth feature ablation**: For datasets with known physical ground truth (e.g., mineral mapping), systematically remove bands corresponding to known absorption features and measure accuracy degradation to validate the XAI method's physical alignment claims.
3. **Statistical comparison baseline**: Implement PCA-based band selection on identical datasets and directly compare classification performance, computational efficiency, and interpretability against the explainability-driven approach.