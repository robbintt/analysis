---
ver: rpa2
title: 'There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance'
arxiv_id: '2510.20119'
source_url: https://arxiv.org/abs/2510.20119
tags:
- timeseries
- invariance
- data
- domains
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental limitation of current time-series
  foundation models (TSFMs): their pretraining datasets lack the human-semantic completeness
  found in vision and language corpora, exemplified by the absence of recognizable
  concepts like "apple." This incompleteness prevents TSFMs from achieving emergent
  reasoning and zero-shot generalization. The authors argue that progress requires
  moving beyond opportunistic data aggregation to principled dataset design that systematically
  spans the space of invariance preserving temporal semantics.'
---

# There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance

## Quick Facts
- **arXiv ID**: 2510.20119
- **Source URL**: https://arxiv.org/abs/2510.20119
- **Reference count**: 3
- **Key outcome**: Time-series foundation models lack human-semantic completeness, preventing emergent reasoning and zero-shot generalization.

## Executive Summary
This paper identifies a fundamental limitation of current time-series foundation models (TSFMs): their pretraining datasets lack the human-semantic completeness found in vision and language corpora, exemplified by the absence of recognizable concepts like "apple." This incompleteness prevents TSFMs from achieving emergent reasoning and zero-shot generalization. The authors argue that progress requires moving beyond opportunistic data aggregation to principled dataset design that systematically spans the space of invariance preserving temporal semantics. They propose constructing a formal ontology of time-series invariances—such as spectral, amplitude, shape, elastic morphological, distributional, and parametric invariances—derived from first principles using transformation groups. Such an ontology would guide both dataset curation and model architecture to ensure representational completeness.

## Method Summary
The paper proposes building a formal ontology of time-series invariances from first principles using transformation groups, where T(g·x) = T(x) defines invariance under transformation g. The method involves cataloging invariance types across existing time-series domains, implementing a transformation library covering primitive operations (time warping, amplitude scaling, frequency shifts, temporal permutations), and designing a balanced synthetic dataset generator that systematically covers invariance space. The approach aims to create a "world-complete" time-series corpus by ensuring representational completeness through invariance coverage, which would then guide both dataset curation and model architecture design for TSFMs.

## Key Results
- Current TSFM pretraining datasets lack human-semantic completeness, preventing emergent zero-shot reasoning
- Time-series data is complementary to vision/language modalities, capturing dynamics not expressible semantically
- A formal invariance ontology derived from first principles can guide both dataset curation and model architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Principled dataset design spanning invariance classes may improve TSFM generalization.
- Mechanism: Curating or generating data to cover specific invariance types (spectral, amplitude, shape, elastic morphological, distributional, parametric) exposes the model to transformations that preserve temporal semantics, encouraging the learning of robust representations rather than superficial correlations.
- Core assumption: Models trained on invariance-diverse data will generalize better across domains than models trained on opportunistically aggregated data.
- Evidence anchors: [abstract] "We posit that progress demands a shift from opportunistic aggregation to principled design: constructing datasets that systematically span the space of invariance that preserve temporal semantics."
- Break condition: If empirical studies show that large-scale, opportunistically aggregated datasets outperform smaller, invariance-curated ones; or if downstream tasks reward spurious correlations over invariant features.

### Mechanism 2
- Claim: Time-series data lacks human-semantic completeness (the "apple" concept), preventing emergent zero-shot reasoning similar to NLP/Vision FMs.
- Mechanism: Web-scale text and image corpora contain dense human-relevant semantic concepts, enabling FMs to learn shared world models (PRH). Time-series data is complementary, capturing dynamics (e.g., traffic patterns) not easily expressed semantically, and lacking dense concept coverage, so TSFMs cannot learn a "world model" from scraping alone.
- Core assumption: Emergent zero-shot reasoning in FMs requires pretraining data with dense human-semantic completeness.
- Evidence anchors: [abstract] "There are no timeseries dataset that contains the concept apple."
- Break condition: If future work demonstrates emergent zero-shot reasoning in TSFMs trained on massive but uncurated time-series data, or shows time-series has equivalent "semantic completeness."

### Mechanism 3
- Claim: A formal invariance ontology can guide both dataset curation and model architecture for TSFMs.
- Mechanism: By defining invariances (transformation groups *g* where *T(g·x) = T(x)*), one can systematically enumerate, balance, and synthetically generate data covering these symmetries. This ontology also informs architecture choices (e.g., spectral-invariant layers for audio, amplitude-equivariant normalizations).
- Core assumption: Such an ontology can be constructed and practically applied; it maps meaningfully to downstream task performance.
- Evidence anchors: [abstract] "To this end, we suggest that the ontology of timeseries invariances should be built based on first principles."
- Break condition: If the ontology is too complex/intractable to define or use; if applying it yields no practical gains over heuristic data balancing.

## Foundational Learning

- **Invariance and Equivariance**: The paper's central argument hinges on identifying and leveraging domain-specific invariances (e.g., spectral, amplitude) to define data completeness and guide model design.
  - Quick check: For a time-series classification task, if you apply a transformation (e.g., amplitude scaling) and the prediction remains the same, what type of property is the model exhibiting?

- **Transformation Groups**: The paper proposes building an invariance ontology from first principles using transformation groups (closing under composition).
  - Quick check: If transformation set S = {time-shift, amplitude-scale}, what does the "group generated by S" include?

- **Platonic Representation Hypothesis (PRH)**: The paper uses the PRH to argue for the semantic completeness of text/image data and the incompleteness of time-series data.
  - Quick check: What does PRH predict about representations learned by models trained on different modalities (e.g., text and images)?

## Architecture Onboarding

- **Component map**: Invariance Ontology Definition -> Dataset Curation/Generation Module -> Model Architecture Alignment
- **Critical path**: 1) Define/Select an invariance ontology (subset of spectral, amplitude, shape, elastic, distributional, parametric). 2) Audit/curate existing data or generate synthetic data to balance across ontology classes. 3) Train/evaluate TSFM on invariance-balanced vs. unbalanced corpora. 4) *(Future)* Integrate invariance-aware architectural components.
- **Design tradeoffs**: Completeness vs. Cost (resource-intensive construction), Generality vs. Specificity (broad invariance classes may not apply to all domains), Synthetic vs. Real Data (synthetic ensures coverage but lacks real-world complexity).
- **Failure signatures**: Overgeneralized Invariance (applying invariance where it doesn't hold, e.g., treating amplitude as invariant in energy demand), Invariance Coverage Gaps (uneven coverage leading to biased representations), Ontology Rigidity (overly strict ontology failing to accommodate novel domains).
- **First 3 experiments**: 1) Invariance Coverage Audit: Quantify distribution of existing TSFM pretraining datasets across proposed invariance classes to identify gaps. 2) Ablation on Synthetic Invariance Injection: Train baseline TSFM on standard corpus, then variants with synthetically generated data augmenting under-represented invariances (e.g., elastic morphological). Compare zero-shot performance. 3) Cross-Domain Invariance Transfer: Evaluate if models trained on domains with strong invariance X (e.g., spectral in audio) perform better on other domains sharing X (e.g., vibration), vs. domains with different invariance profiles.

## Open Questions the Paper Calls Out

- **Open Question 1**: Which specific combination of time-series domains, when aggregated, can approximate a "world-complete" representation of temporal reality?
  - Basis: [explicit] The authors state directly that "Identifying the combination of timeseries domains that together approximate a world-complete representation of reality remains an open research question."
  - Why unresolved: The paper establishes the problem (semantic incompleteness of opportunistic aggregation) but does not enumerate which domains or how many are needed for completeness.
  - What evidence would resolve it: A systematic mapping of invariance properties across existing time-series archives, followed by empirical tests measuring zero-shot generalization when models are trained on progressively more invariance-diverse corpora.

- **Open Question 2**: How can a formal, complete ontology of time-series invariances be systematically derived from first principles using transformation groups?
  - Basis: [explicit] The paper proposes that "an ontology of timeseries invariance from first principles could be a good starting point" and outlines transformation categories (time-axis, value-space, index-space, dynamical), but no such ontology yet exists.
  - Why unresolved: Constructing this ontology requires formalizing the full space of transformations and their closure under composition—a non-trivial theoretical undertaking.
  - What evidence would resolve it: A complete, axiomatically-grounded taxonomy of invariance classes with formal proofs of closure properties, validated by showing it can classify and differentiate real-world time-series domains.

- **Open Question 3**: Can principled invariance-coverage-guided dataset curation alone induce emergent reasoning and robust zero-shot generalization in TSFMs?
  - Basis: [inferred] The paper's core claim is that aligned structure and emergent behavior arise only from invariance-completeness, but this remains a hypothesis without empirical validation.
  - Why unresolved: No TSFM has yet been trained on an explicitly invariance-balanced corpus; current datasets are assembled opportunistically.
  - What evidence would resolve it: Training TSFMs on datasets curated using the invariance ontology and demonstrating emergent behaviors (e.g., zero-shot cross-domain transfer, causal reasoning) that exceed current baselines.

## Limitations

- No concrete definition of "world-complete" dataset coverage or how to measure invariance coverage quantitatively
- No established benchmarks to validate whether invariance-based design improves TSFM generalization
- Proposed ontology may be too rigid to accommodate novel or mixed-domain time-series data

## Confidence

- **Medium confidence** in the core argument that opportunistic aggregation is insufficient for TSFM pretraining
- **Low confidence** in the proposed invariance ontology's practical applicability and construction methodology
- **Medium confidence** in the claim that systematic invariance coverage could improve generalization

## Next Checks

1. **Invariance Coverage Audit**: Quantify the distribution of existing TSFM pretraining datasets (Time Series Pile, LOTSA, UTSD, Monash Archive, Google Trends, JD supply-chain, DataDog telemetry) across the paper's proposed invariance classes to identify gaps and validate the claim of incompleteness.

2. **Ablation on Synthetic Invariance Injection**: Train a baseline TSFM on a standard corpus, then train variants with synthetically generated data augmenting specific under-represented invariances (e.g., elastic morphological). Compare zero-shot performance on downstream tasks to test whether invariance coverage improves generalization.

3. **Cross-Domain Invariance Transfer**: Evaluate whether models trained on domains with strong invariance X (e.g., spectral in audio) perform better on other domains sharing X (e.g., vibration) versus domains with different invariance profiles, testing the transferability claim of the invariance ontology.