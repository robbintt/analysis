---
ver: rpa2
title: Generating Query-Relevant Document Summaries via Reinforcement Learning
arxiv_id: '2508.08404'
source_url: https://arxiv.org/abs/2508.08404
tags:
- product
- relevance
- search
- summaries
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ReLSum, a reinforcement learning framework
  for generating query-relevant summaries of product descriptions to improve search
  relevance in e-commerce systems. The method uses relevance scores as rewards to
  train a large language model to produce concise summaries that capture essential
  attributes not present in product titles, addressing the limitations of both verbose
  full descriptions and insufficient titles.
---

# Generating Query-Relevant Document Summaries via Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.08404
- Source URL: https://arxiv.org/abs/2508.08404
- Authors: Nitin Yadav; Changsung Kang; Hongwei Shang; Ming Sun
- Reference count: 6
- One-line primary result: RL-based summarization improves e-commerce search relevance by 0.13% R@90P and 0.80% NDCG@5 on full dataset, with larger gains on tail queries.

## Executive Summary
This paper introduces ReLSum, a reinforcement learning framework for generating query-relevant summaries of product descriptions to improve search relevance in e-commerce systems. The method uses relevance scores as rewards to train a large language model to produce concise summaries that capture essential attributes not present in product titles, addressing the limitations of both verbose full descriptions and insufficient titles. ReLSum was compared to baselines and evaluated using recall at 90% precision (R@90P) and NDCG@5 on golden datasets. The ReLSumGRPO variant achieved gains of +0.13% in R@90P and +0.80% in NDCG@5 on the full dataset, with larger gains (+0.51% R@90P, +1.03% NDCG@5) on the tail dataset. Online interleaving tests showed a 0.22% lift in add-to-cart actions (p=0.000), and A/B tests revealed improvements across business metrics including +0.34% in units and +0.26% in orders per visitor.

## Method Summary
ReLSum employs reinforcement learning to generate query-relevant summaries of product descriptions for e-commerce search. The LLM (Mistral-7B-Instruct-v0.3 with LoRA) generates summaries from product title and description. A frozen BERT cross-encoder computes a relevance score given the query and concatenated [title; summary]. The reward is defined as −|r(q, [t; s]) − l|, where l is the ground-truth relevance label. Policy updates are performed via GRPO or DPO. Training data is filtered to retain examples where the description meaningfully improves relevance compared to title alone. This query-blind approach enables precomputation of summaries for scalable deployment.

## Key Results
- ReLSumGRPO achieved +0.13% R@90P and +0.80% NDCG@5 on Golden-full dataset compared to baseline.
- Larger gains observed on Golden-tail: +0.51% R@90P and +1.03% NDCG@5.
- Online A/B tests showed +0.22% lift in add-to-cart actions (p=0.000) and improvements in units (+0.34%) and orders per visitor (+0.26%).

## Why This Works (Mechanism)

### Mechanism 1: RL-based Reward Alignment for Task-Specific Summarization
Using relevance scores as rewards aligns the summarization objective with the downstream ranking task, overcoming the misalignment problem of generic summarization. The LLM generates summaries conditioned on product description and title. A frozen BERT cross-encoder computes a relevance score given the query and [title; summary]. The reward is defined as −|r(q, [t; s]) − l|, where l is the ground-truth relevance label. Policy updates via GRPO or DPO optimize the LLM to produce summaries that maximize this reward.

### Mechanism 2: Query-Blind Policy for Scalable Deployment
Constraining the policy to product-only inputs (no query at inference) enables precomputation and deployment to billions of products while still producing query-relevant summaries. During training, the environment (queries and ranking system) provides rewards, but the policy πθ only receives [title; description]. This allows summaries to be generated offline per product and reused across all queries.

### Mechanism 3: Training Data Filtering for Signal Amplification
Filtering training examples to retain those where r(q, [t; d]) differs noticeably from r(q, t) focuses learning on cases where the description truly matters. The training dataset is constructed by collecting query-product pairs from search logs, computing offline relevance with and without description, and filtering for large differences. This concentrates gradient signal on informative examples.

## Foundational Learning

- Concept: Cross-Encoder vs Bi-Encoder Architecture
  - Why needed here: The paper uses a BERT cross-encoder as the frozen reward model; understanding its quadratic attention complexity explains why full descriptions are infeasible and summaries are necessary.
  - Quick check question: Given a query of 10 tokens and a document of 200 tokens, how many attention computations does a cross-encoder require compared to a bi-encoder?

- Concept: Policy Gradient and Proximal Optimization (GRPO/DPO)
  - Why needed here: The core contribution uses GRPO and DPO to optimize the summarization policy; understanding clipping, KL penalties, and advantage normalization is essential to debug training.
  - Quick check question: In GRPO, what role does the clipping parameter ϵ play, and why is the KL divergence term β·D_KL included?

- Concept: Reward Engineering for Text Generation
  - Why needed here: The reward is derived from a frozen relevance model's output; understanding how to define and normalize rewards impacts policy convergence.
  - Quick check question: If the reward model outputs scores in [0, 1] but ground-truth labels are sparse, what might happen to gradient variance during training?

## Architecture Onboarding

- Component map: LLM -> Frozen BERT Cross-Encoder -> Reward Computation -> Policy Optimizer
- Critical path:
  1. Prepare filtered training dataset with proxy labels.
  2. Initialize LLM (π_ref) and freeze cross-encoder.
  3. Sample G summaries per product from π_old.
  4. Compute rewards via cross-encoder.
  5. Update π_θ via GRPO or DPO.
  6. Deploy: precompute summaries offline; serve with cross-encoder at runtime.

- Design tradeoffs:
  - GRPO vs DPO: GRPO uses 4 samples and clipping (β=0.0, ϵ=0.2); DPO uses pairwise preference (β=0.1). GRPO slightly outperforms on NDCG@5 (+0.80% vs +0.68%) on Golden-full.
  - Sample count G: Higher G improves signal but increases compute; paper found G=4 optimal.
  - Summary length: Truncation applied during evaluation; longer summaries improve relevance but increase latency due to cross-encoder attention.

- Failure signatures:
  - No metric improvement over baseline (None): Summary may not add missing attributes; check prompt effectiveness.
  - Worse performance on head queries: Overfitting to tail attributes; rebalance training data.
  - High training variance: Reward normalization or sample count issues; verify advantage computation.
  - Latency violations: Summaries too long; enforce stricter token limits.

- First 3 experiments:
  1. Reproduce offline results: Train ReLSum_GRPO on a sampled dataset, evaluate R@90P and NDCG@5 against baseline (None) and Desc.
  2. Ablate training data filtering: Compare performance when using filtered vs unfiltered training sets to validate the filtering hypothesis.
  3. Compare GRPO vs DPO hyperparameter sensitivity: Vary β and ϵ to understand robustness; confirm GRPO's advantage on tail queries.

## Open Questions the Paper Calls Out
- **Question:** Does an iterative training framework, where the downstream relevance model is fine-tuned alongside the summarization LLM, yield further performance gains?
- **Question:** Can the ReLSum framework effectively incorporate multimodal inputs, such as product images, to improve relevance predictions for visually-driven queries?
- **Question:** To what extent does the RL policy "game" the frozen reward model by generating summaries that maximize relevance scores at the expense of factual consistency?

## Limitations
- The frozen BERT cross-encoder may have systematic biases that summaries could overfit to rather than true relevance.
- Query-blind summaries may underperform for polysemous queries where different intents require different emphasized attributes.
- The exact impact of summary length constraints on relevance is not quantified in detail.

## Confidence
- **High Confidence:** The core RL framework (GRPO/DPO with frozen cross-encoder reward) is technically sound and reproducible.
- **Medium Confidence:** The mechanism that query-blind summaries generalize across queries is plausible but lacks direct empirical validation.
- **Low Confidence:** The exact impact of the summary length constraint on relevance is not quantified.

## Next Checks
1. **Reward Model Ablation:** Replace the BERT cross-encoder with a simpler keyword-based relevance model and retrain ReLSum. Compare performance to isolate how much of the gain depends on the sophistication of the reward model.
2. **Query-Blind Generalization Test:** For a subset of products, generate multiple summaries conditioned on different query intents (e.g., "cheap", "high-quality", "fast shipping"). Evaluate whether query-blind summaries underperform query-specific ones, validating the scalability vs. precision trade-off.
3. **Filtering Sensitivity Analysis:** Vary the threshold for training data filtering (e.g., Δr > 0.05, 0.1, 0.2) and measure impact on both tail and head query performance. This would confirm whether the filtering strategy is robust or overly sensitive to parameter choice.