---
ver: rpa2
title: 'ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model
  for Traditional Chinese Medicine Formula'
arxiv_id: '2509.05867'
source_url: https://arxiv.org/abs/2509.05867
tags:
- graphrag
- formula
- zhifangdantai
- fine-tuning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ZhiFangDanTai, a framework that combines Graph-based
  Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning for Traditional Chinese
  Medicine (TCM) formula generation. The model retrieves and synthesizes structured
  TCM knowledge into concise summaries, constructs enhanced instruction datasets,
  and fine-tunes LLMs to integrate external information effectively.
---

# ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula

## Quick Facts
- arXiv ID: 2509.05867
- Source URL: https://arxiv.org/abs/2509.05867
- Reference count: 0
- This paper presents ZhiFangDanTai, a framework that combines Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning for Traditional Chinese Medicine (TCM) formula generation. The model retrieves and synthesizes structured TCM knowledge into concise summaries, constructs enhanced instruction datasets, and fine-tunes LLMs to integrate external information effectively. Theoretical proofs demonstrate that this approach reduces generalization error and hallucination rates. Experimental results on both collected and clinical datasets show significant improvements over state-of-the-art models, achieving a CCHR score of 0.9303 and FS score of 0.9741 on collected data, and a CCHR score of 0.9340 and FS score of 0.9371 on clinical data. The model is open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

## Executive Summary
This paper presents ZhiFangDanTai, a framework that combines Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning for Traditional Chinese Medicine (TCM) formula generation. The model retrieves and synthesizes structured TCM knowledge into concise summaries, constructs enhanced instruction datasets, and fine-tunes LLMs to integrate external information effectively. Theoretical proofs demonstrate that this approach reduces generalization error and hallucination rates. Experimental results on both collected and clinical datasets show significant improvements over state-of-the-art models, achieving a CCHR score of 0.9303 and FS score of 0.9741 on collected data, and a CCHR score of 0.9340 and FS score of 0.9371 on clinical data. The model is open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

## Method Summary
The method constructs a TCM knowledge graph from 80,000 formulas enriched by DeepSeek-7B, clustering into 7 fine-grained communities using Leiden algorithm. GraphRAG retrieves relevant information from these communities for symptom queries and synthesizes global answers. The framework fine-tunes LLaMA3.2-7B using Supervised Fine-Tuning (SFT) with LoRA rank=12, then applies Direct Preference Optimization (DPO) with β=0.2. The training pipeline uses LlamaIndex and LlamaFactory, with 2x RTX 4090 GPUs. The model achieves high performance on both collected and clinical datasets while reducing hallucination rates.

## Key Results
- Achieves CCHR score of 0.9303 and FS score of 0.9741 on collected dataset
- Achieves CCHR score of 0.9340 and FS score of 0.9371 on clinical dataset from Haodf.com
- Outperforms state-of-the-art models including OpenTCM and TCM-LLaMA in hallucination reduction and formula compatibility metrics
- Theoretical proofs demonstrate reduction in generalization error and hallucination rates

## Why This Works (Mechanism)

### Mechanism 1: GraphRAG-Enhanced Knowledge Integration
The framework uses GraphRAG to structure TCM knowledge into a graph with fine-grained communities (Diseases, Formulas, Herbs) and perform map-reduce retrieval to synthesize coherent global answers. This structured context is used to fine-tune the LLM via SFT and DPO, teaching it to better integrate and utilize external information. The quality depends on the accuracy of the knowledge graph and effectiveness of fine-tuning.

### Mechanism 2: Theoretical Reduction of Generalization Error
Mathematical proofs show that combining GraphRAG with fine-tuning (SFT+DPO) yields lower bounds on generalization error compared to fine-tuning alone. The proofs model the problem using conditional mutual information, with Proposition 1 showing information gain from GraphRAG reduces SFT error, and Proposition 2 showing DPO further reduces error by optimizing for preferred answers.

### Mechanism 3: Mitigation of Hallucination via Constrained Generation
Hallucination is reduced through grounding (GraphRAG provides factual context) and preference alignment (DPO optimizes for factual, coherent answers). Propositions 3 & 4 provide mathematical bounds on hallucination probability. Experimental metrics show CCHR=0.9303 and FS=0.9741 on collected data.

## Foundational Learning

- **Concept: GraphRAG (Graph-based Retrieval-Augmented Generation)**
  - Why needed here: This is the core knowledge retrieval engine. It structures unstructured TCM texts into a graph of entities (herbs, symptoms, formulas) and relationships, enabling nuanced, multi-hop retrieval that simple vector similarity search might miss.
  - Quick check question: How does GraphRAG's use of community detection and map-reduce synthesis differ from standard RAG's chunk retrieval?

- **Concept: Supervised Fine-Tuning (SFT) & Direct Preference Optimization (DPO)**
  - Why needed here: These are the methods used to adapt the general-purpose LLM to the TCM domain. SFT teaches the model the basic task format using the GraphRAG-augmented dataset. DPO further refines the model to align with human judgment about answer quality.
  - Quick check question: What is the key difference between the optimization objectives of SFT (Eq. 1) and DPO (Eq. 2) in this framework?

- **Concept: TCM Domain Specifics (Formula Composition, Syndrome Differentiation)**
  - Why needed here: The model's output is evaluated on domain-specific metrics like Correct Sovereign-minister-assistant-courier Compatibility Rate (CSCR). Understanding the "monarch, minister, assistant, courier" hierarchy of herbs in a formula is essential to interpreting the model's performance.
  - Quick check question: In the evaluation metrics, what does the "Compatibility Compliance Rate (CCR)" check for, and why is it critical for a TCM formula model?

## Architecture Onboarding

- **Component map:** Data Pipeline (collects raw TCM formula data → extracts entities → constructs TCM Knowledge Graph) → GraphRAG Module (retrieves and synthesizes global answers) → Training Pipeline (SFT + DPO on LLaMA3.2-7B) → Inference Engine (generates TCM formulas with explanations)

- **Critical path:** The quality of the entire system hinges on the GraphRAG retrieval quality. If this step fails to retrieve relevant and accurate context, both the training data (for SFT/DPO) and the final inference will be flawed.

- **Design tradeoffs:**
  - Retrieval Granularity (top-k): Higher k provides more context but increases noise and inference time. The paper found top-k=2 a good balance.
  - Training Stages: Including Generative Pre-training (GPT) was shown to degrade performance, likely due to distributional shift. The SFT+DPO path is more effective.
  - Base LLM Choice: Using LLaMA3.2-7B vs. Qwen2.5-7B showed different strengths. LLaMA was generally better for TCM-specific metrics, while Qwen performed better on some translation metrics.

- **Failure signatures:**
  - High Hallucination (Low CCHR/FS): Indicates poor retrieval (irrelevant context) or ineffective DPO training.
  - Low Compatibility Compliance (Low CCR/CSCR): Suggests the model has not learned TCM formulation principles, possibly due to insufficient or poor-quality training data.
  - Slow Inference: High top-k or inefficient graph database queries in the GraphRAG module.

- **First 3 experiments:**
  1. Reproduce Baseline vs. Full Model: Implement the simplest baseline (LLaMA3.2-7B Plug-and-play) and the full ZhiFangDanTai model on the provided test split. Compare key metrics (CCHR, FS) to validate the reported gains.
  2. Ablate the GraphRAG Component: Replace the GraphRAG module with a standard RAG retriever using the same base LLM and fine-tuning pipeline. Compare performance to isolate the contribution of the graph-based retrieval structure.
  3. Vary the top-k Retrieval Parameter: Run inference with top-k = 1, 2, 3 and measure the trade-off between answer quality (BLEU, ROUGE, TCM metrics) and inference time.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's performance is fundamentally dependent on the accuracy and completeness of the constructed TCM knowledge graph.
- The theoretical proofs for error reduction rely on specific mathematical assumptions that may not hold in real-world TCM data distribution.
- The DPO mechanism relies on accurately labeled preference pairs, and if the scoring or selection is noisy, hallucination reduction may be less effective.

## Confidence
- **High Confidence**: Experimental results demonstrating performance improvements on provided datasets are directly verifiable if the model and data are accessible.
- **Medium Confidence**: The mechanism by which GraphRAG enhances knowledge integration is well-described and supported by related work, but the specific impact versus standard RAG is not definitively isolated.
- **Medium Confidence**: Claims about reducing generalization error and hallucination rates are supported by both theoretical proofs and experimental metrics, but the theoretical guarantees are conditional and experiments only show improvements on tested datasets.

## Next Checks
1. Validate GraphRAG vs. Standard RAG: Implement the full ZhiFangDanTai pipeline but replace the GraphRAG module with a standard vector similarity-based RAG system using the same base LLM and fine-tuning procedure. Compare key performance metrics (CCHR, FS, CCR) to isolate the contribution of the graph-based community structure.
2. Stress-Test Knowledge Graph Quality: Systematically introduce controlled errors (e.g., incorrect herb-symptom relationships) into the TCM knowledge graph and measure the degradation in GraphRAG retrieval quality and downstream model performance.
3. Analyze DPO Preference Data: Conduct a qualitative analysis of a sample of the preference pairs used for DPO training. Check for potential labeling errors or biases in what is classified as "hallucinated" versus "preferred."