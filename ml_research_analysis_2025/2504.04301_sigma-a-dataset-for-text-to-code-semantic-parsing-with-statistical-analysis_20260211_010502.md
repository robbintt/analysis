---
ver: rpa2
title: 'Sigma: A dataset for text-to-code semantic parsing with statistical analysis'
arxiv_id: '2504.04301'
source_url: https://arxiv.org/abs/2504.04301
tags:
- dataset
- code
- statistical
- questions
- python
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The SIGMA dataset introduces a novel approach to semantic parsing
  by incorporating statistical analysis into text-to-code tasks. It includes 6,000
  questions paired with Python code labels across 160 databases, covering 44 distinct
  patterns, including 40 statistical analysis patterns and 4 SQL query types.
---

# Sigma: A dataset for text-to-code semantic parsing with statistical analysis

## Quick Facts
- arXiv ID: 2504.04301
- Source URL: https://arxiv.org/abs/2504.04301
- Reference count: 27
- Dataset includes 6,000 questions paired with Python code labels across 160 databases

## Executive Summary
SIGMA introduces a novel dataset for text-to-code semantic parsing that integrates statistical analysis capabilities. The dataset contains 6,000 questions paired with Python code labels across 160 databases, covering 44 distinct patterns including 40 statistical analysis patterns and 4 SQL query types. This work addresses the gap between traditional Text-to-SQL tasks and the need for advanced statistical capabilities in semantic parsing. The authors evaluate three baseline models (LGESQL, SLSQL, and SmBoP) and demonstrate that LGESQL with ELECTRA achieves the highest structure accuracy of 83.37%, while SmBoP with GraPPa and T5 reaches 76.38% execution accuracy.

## Method Summary
The authors developed SIGMA through a systematic dataset construction process involving 160 databases and 44 distinct patterns. The dataset covers both SQL query types and Python-based statistical analysis functions. Three baseline models were implemented and evaluated: LGESQL using ELECTRA encoder, SLSQL, and SmBoP using GraPPa and T5 models. The evaluation framework measures both structure accuracy (syntactic correctness) and execution accuracy (functional correctness of generated code). The dataset aims to bridge the gap between traditional Text-to-SQL tasks and the need for statistical analysis capabilities in semantic parsing applications.

## Key Results
- LGESQL with ELECTRA achieved highest structure accuracy of 83.37%
- SmBoP with GraPPa and T5 reached 76.38% execution accuracy
- Dataset contains 6,000 questions across 160 databases with 44 distinct patterns

## Why This Works (Mechanism)
The dataset works by providing structured pairing of natural language questions with executable Python code that includes statistical functions. This approach leverages the rich statistical capabilities of Python libraries while maintaining the structured nature of semantic parsing. The combination of SQL query types and Python statistical functions creates a comprehensive framework for handling both data retrieval and analysis tasks. The systematic construction across multiple databases ensures diversity in both schema complexity and statistical operations required.

## Foundational Learning

1. **Text-to-Code Semantic Parsing** - Converting natural language to executable code
   - Why needed: Bridges human intent and machine execution
   - Quick check: Can model handle novel queries accurately?

2. **Statistical Analysis Functions** - Python's statistical capabilities
   - Why needed: Enables complex data analysis beyond basic queries
   - Quick check: Does generated code produce correct statistical results?

3. **Structure vs Execution Accuracy** - Dual evaluation metrics
   - Why needed: Separates syntactic correctness from functional correctness
   - Quick check: Are high structure scores reflected in execution performance?

## Architecture Onboarding

Component map: Natural Language Query -> Encoder -> Decoder -> Python Code

Critical path: Input text → Model processing → Code generation → Execution

Design tradeoffs: Structure accuracy vs execution accuracy; Model complexity vs performance; Python-specific vs language-agnostic approaches

Failure signatures: High structure accuracy but low execution accuracy indicates code generation issues; Low structure accuracy suggests parsing difficulties

First experiments:
1. Test baseline models on held-out validation set from same database schemas
2. Evaluate cross-database generalization performance
3. Conduct ablation studies on model components (encoder, decoder, decoding strategy)

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset construction relies on 160 databases that may not represent full diversity of real-world scenarios
- Limited to 44 distinct patterns, potentially constraining real-world applicability
- Focus on Python may limit applicability to other programming languages for statistical analysis

## Confidence

- Dataset quality and representativeness: Medium confidence
- Baseline model performance: High confidence
- Model architecture recommendations: Medium confidence

## Next Checks

1. Test baseline models on a held-out validation set from the same database schemas to verify generalization within the dataset
2. Evaluate model performance on cross-database generalization by testing on schemas not seen during training
3. Conduct ablation studies to determine the impact of different model components on statistical analysis accuracy