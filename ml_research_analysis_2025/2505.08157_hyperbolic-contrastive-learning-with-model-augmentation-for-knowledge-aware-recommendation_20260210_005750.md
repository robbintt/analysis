---
ver: rpa2
title: Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware
  Recommendation
arxiv_id: '2505.08157'
source_url: https://arxiv.org/abs/2505.08157
tags:
- learning
- graph
- hyperbolic
- contrastive
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in knowledge-aware recommendation
  systems that use graph neural networks and contrastive learning. Existing methods
  struggle to capture hierarchical structures in user-item bipartite graphs and knowledge
  graphs, and often perturb graph structures for positive sampling, potentially shifting
  user preferences.
---

# Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation

## Quick Facts
- arXiv ID: 2505.08157
- Source URL: https://arxiv.org/abs/2505.08157
- Reference count: 38
- Primary result: HCMKR improves NDCG@10 by 2.33%-11.03% over state-of-the-art baselines

## Executive Summary
This paper addresses limitations in knowledge-aware recommendation systems by introducing Hyperbolic Contrastive Learning with Model-augmentation (HCMKR). The framework tackles two key challenges: capturing hierarchical structures in user-item bipartite graphs and knowledge graphs, and avoiding preference shifts when generating positive samples for contrastive learning. HCMKR employs Lorentzian knowledge aggregation in hyperbolic space and introduces three model-level augmentation techniques (Dropout, cross-layer outputs, and model-pruning) to generate positive samples without perturbing graph structures.

## Method Summary
HCMKR maps Euclidean embeddings to the Lorentzian manifold using exponential mapping, then performs knowledge aggregation through tangent-space projections with relation-aware attention. The bipartite GNN encodes user-item interactions using multiple layers, with model-level augmentation generating positive pairs through dropout masks, different GNN layer outputs, or weight pruning. The model is trained with a joint loss combining BPR ranking loss and contrastive loss. The cross-layer augmentation variant (HCMKR-C) consistently outperformed other variants, achieving maximum NDCG@10 improvements of 11.03% across three benchmark datasets.

## Key Results
- HCMKR achieves maximum NDCG@10 improvement of 11.03% over state-of-the-art baselines
- Cross-layer augmentation (HCMKR-C) consistently outperforms dropout and pruning variants
- Model-level augmentation preserves user preference context while avoiding structure-level preference shifts
- Training time efficiency comparable to existing methods despite added complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hyperbolic space (Lorentz model) captures hierarchical structures in power-law distributed graphs more effectively than Euclidean space, enabling better user/item representations.
- **Mechanism:** Euclidean embeddings are mapped to the Lorentzian manifold via exponential mapping (Eq. 5). Knowledge aggregation then operates through tangent-space projections: embeddings are logged to tangent space, aggregated with relation-aware attention (Eq. 6–11), then exponentiated back. This preserves the intrinsic hierarchy that Euclidean methods distort.
- **Core assumption:** User-item and knowledge graphs exhibit power-law degree distributions (hierarchical structure); Bourgain's theorem implies Euclidean space cannot embed such graphs with low distortion.
- **Evidence anchors:**
  - [abstract]: "difficulties in effectively capturing the underlying hierarchical structure within user-item bipartite graphs and knowledge graphs"
  - [section]: Figure 1 shows degree distributions approximating power-law for Amazon-Book and MovieLens-20M; Section 3.1 cites Bourgain's theorem limitations.
  - [corpus]: "Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring" corroborates hyperbolic geometry's effectiveness for hierarchically organized data.
- **Break condition:** If the graph lacks hierarchical structure (e.g., near-uniform degree distribution), hyperbolic gains diminish; Euclidean methods may suffice.

### Mechanism 2
- **Claim:** Model-level augmentation avoids the preference shift caused by structure-level perturbations because the original graph encodes the supervision signal.
- **Mechanism:** Instead of edge dropping (which removes items from user history), augmentation perturbs the encoding function f(·): (1) Dropout uses different random masks on two forward passes; (2) Cross-layer outputs take different GNN layers as views; (3) Pruning masks low-magnitude weights. The graph G remains intact, preserving preference semantics.
- **Core assumption:** The user-item interaction graph is the ground-truth preference signal; altering it changes what the model learns to predict.
- **Evidence anchors:**
  - [abstract]: "commonly generate positive samples for contrastive learning by perturbing the graph structure, which may lead to a shift in user preference learning"
  - [section]: Figure 2(e) illustrates edge dropping removing preference for electronic products.
  - [corpus]: Limited direct corpus evidence on model-level augmentation for recommendation; this is a relatively novel approach.
- **Break condition:** If the task explicitly benefits from structural diversity (e.g., robustness to missing edges), structure-level augmentation may be preferable.

### Mechanism 3
- **Claim:** Cross-layer augmentation consistently outperforms dropout and pruning variants by leveraging multi-scale representations without additional forward passes.
- **Mechanism:** Different GNN layers capture different receptive fields—shallow layers encode local structure, deeper layers capture higher-order connectivity. Using outputs from layers k₁ and k₂ as positive pairs provides natural augmentation that requires only one forward pass (Eq. 14).
- **Core assumption:** Different layers encode complementary yet semantically consistent information about the same node.
- **Evidence anchors:**
  - [abstract]: "cross-layer augmentation method (HCMKR-C) consistently performed best, improving NDCG@10 by 2.33% to 11.03%"
  - [section]: Figure 6(c) shows layer combination "1-3" achieves best Yelp2018 performance.
  - [corpus]: Weak corpus evidence; cross-layer augmentation for recommendation is underexplored in retrieved neighbors.
- **Break condition:** If deep layers suffer over-smoothing, cross-layer pairs may become uninformative.

## Foundational Learning

- **Concept:** Lorentzian manifold operations (exponential/logarithmic mapping, tangent space)
  - **Why needed here:** All aggregation occurs via tangent-space projections; understanding Eq. 3–5 is essential for implementation.
  - **Quick check question:** Why must aggregation be performed in tangent space rather than directly on the manifold?

- **Concept:** Contrastive learning view generation
  - **Why needed here:** The core contribution is *how* positive pairs are generated; misunderstanding this leads to incorrect augmentation logic.
  - **Quick check question:** If you drop edges from the user-item graph, what semantic change occurs compared to dropping model weights?

- **Concept:** Relation-aware attention in knowledge graphs
  - **Why needed here:** Eq. 7–11 compute attention using relation embeddings and hyperbolic distance.
  - **Quick check question:** Why is the Fermi-Dirac function introduced in Eq. 11?

## Architecture Onboarding

- **Component map:**
  Input: User-item bipartite graph G₁, Knowledge graph G₂
  Space mapping: Euclidean → Hyperbolic (Eq. 5)
  Lorentzian knowledge aggregation: Attention-weighted KG propagation (Eq. 6–11)
  Bipartite GNN encoding: Layer-wise aggregation in tangent space (Eq. 12)
  Model augmentation: Dropout / Cross-layer / Pruning (Eq. 13–15)
  Training: Joint BPR + contrastive loss (Eq. 17–20)

- **Critical path:**
  1. Initialize Euclidean embeddings → map to Lorentz manifold
  2. Aggregate KG neighbors with relation-aware hyperbolic attention
  3. Propagate through bipartite GNN (3 layers default)
  4. Generate augmented views via chosen augmentation type
  5. Compute recommendation loss + contrastive loss

- **Design tradeoffs:**
  - Lorentz over Poincaré: Numerical stability (distance avoids fractions)
  - No feature transformation/nonlinearity: Reduces overfitting on sparse graphs (following LightGCN)
  - Mean aggregation in tangent space: Fréchet mean has no closed-form in hyperbolic space

- **Failure signatures:**
  - Dropout ratio > 0.5: Overly dissimilar views degrade performance (Fig. 6a)
  - Pruning ratio too high: Similar degradation (Fig. 6b)
  - HCMKR-P underperforms: Dropout disabled for pruning evaluation (Table 2)
  - Poor layer combinations: "1-3" works best on Yelp; others may underperform (Fig. 6c)

- **First 3 experiments:**
  1. **Ablation:** Run with/without hyperbolic geometry, with/without model augmentation (Fig. 5) to isolate each component's contribution.
  2. **Augmentation comparison:** Compare HCMKR-C, HCMKR-D, HCMKR-P on all three datasets (Table 2) to validate cross-layer superiority.
  3. **Hyperparameter sweep:** Vary dropout ratio, pruning ratio, layer combination, and λ (Fig. 6) to identify stable operating ranges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative monotonicity functions compare to the Fermi-Dirac function in Lorentzian knowledge aggregation?
- Basis in paper: [explicit] Footnote 2 states the authors leave the exploration of other functions to change monotonicity for future work.
- Why unresolved: The choice of function affects the attention mechanism in aggregation; alternatives might offer better convergence or representation quality.
- What evidence would resolve it: Ablation studies replacing the Fermi-Dirac function with other transformations and comparing NDCG/Recall metrics.

### Open Question 2
- Question: Can advanced pruning strategies improve model-augmentation performance over the implemented magnitude pruning?
- Basis in paper: [explicit] Footnote 3 notes that complex pruning strategies could be applied but were omitted in favor of simple magnitude pruning.
- Why unresolved: Magnitude pruning might discard important semantic features, potentially explaining why HCMKR-P underperforms compared to other variants.
- What evidence would resolve it: Experiments implementing gradient-based or structured pruning methods within the HCMKR-P framework to measure performance deltas.

### Open Question 3
- Question: Is the lower performance of HCMKR-P due to the pruning strategy itself or the experimental necessity of disabling dropout modules?
- Basis in paper: [inferred] Section 4.2 notes dropout was disabled in HCMKR-P to isolate pruning effects, introducing a confound compared to other variants.
- Why unresolved: Disabling dropout removes a key regularization component, making it difficult to determine if the pruning augmentation is inherently less effective.
- What evidence would resolve it: Experiments running HCMKR-P with active dropout modules to isolate the specific contribution of pruning to performance.

## Limitations

- Theoretical grounding for preference preservation is mostly intuitive rather than rigorously proven
- Model assumes knowledge graphs are available and well-aligned with user-item interactions
- Limited exploration of alternative monotonicity functions for Lorentzian aggregation

## Confidence

- Hyperbolic geometry effectiveness for hierarchical graphs: **High** - supported by Bourgain's theorem, power-law degree distributions in datasets, and consistent performance gains
- Model-level augmentation preserving preference: **Medium** - logical argument and ablation support, but limited theoretical proof and minimal related work for comparison
- Cross-layer augmentation superiority: **High** - consistent experimental results across all three datasets with clear performance margins
- Training efficiency claims: **Medium** - stated as comparable to existing methods but no explicit timing experiments shown

## Next Checks

1. **Ablation with non-hierarchical data:** Test HCMKR on datasets with near-uniform degree distributions to verify hyperbolic gains specifically stem from hierarchical structure rather than general representation improvements.

2. **Preference preservation experiment:** Implement both structure-level (edge dropping) and model-level augmentation variants, then measure explicit preference shift through controlled perturbations to validate the core claim about preference preservation.

3. **Transfer learning evaluation:** Test whether HCMKR representations transfer across datasets or domains to assess if learned hyperbolic embeddings capture generalizable hierarchical patterns versus dataset-specific structures.