---
ver: rpa2
title: Compressed Models are NOT Trust-equivalent to Their Large Counterparts
arxiv_id: '2508.13533'
source_url: https://arxiv.org/abs/2508.13533
tags:
- compressed
- large
- calibration
- https
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether compressed models are trust-equivalent
  to their original large counterparts. The authors define trust-equivalence through
  two dimensions: interpretability alignment (whether models use the same input features
  for predictions) and calibration similarity (whether models exhibit similar reliability
  in predicted probabilities).'
---

# Compressed Models are NOT Trust-equivalent to Their Large Counterparts

## Quick Facts
- arXiv ID: 2508.13533
- Source URL: https://arxiv.org/abs/2508.13533
- Reference count: 34
- Primary result: Compressed models exhibit low interpretability alignment and different calibration profiles compared to large models, despite comparable accuracy

## Executive Summary
This paper challenges the assumption that compressed models with similar accuracy to their large counterparts are suitable drop-in replacements. Through experiments with BERT-base and four compressed variants (Distil-BERT, BERT-Medium, BERT-Mini, BERT-Tiny) on natural language inference and paraphrase identification tasks, the authors demonstrate that compressed models differ substantially in how they make predictions and express confidence. Using LIME and SHAP for feature attribution analysis and calibration metrics like ECE, they show that compressed models achieve comparable accuracy but rely on different input features and exhibit altered probability reliability profiles, making them not trust-equivalent to their large counterparts.

## Method Summary
The study fine-tuned BERT family models (BERT-base, Distil-BERT, BERT-Medium, BERT-Mini, BERT-Tiny) on SNLI dataset for natural language inference and QQP dataset for paraphrase identification. Models were trained using Hugging Face Transformers with batch size 32, AdamW optimizer, learning rate 1e-5, for 5-10 epochs. Interpretability alignment was measured using Jaccard similarity between top-10 features identified by LIME and SHAP explainers. Calibration similarity was assessed using Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Brier Score, and reliability diagrams with 10 equal-width confidence bins. Experiments were conducted on a single NVIDIA A100-SXM4 GPU with 80GB memory.

## Key Results
- Jaccard similarity scores between BERT-base and compressed models ranged from 0.372 to 0.669, all below the 0.7 threshold for interpretability alignment
- BERT-base showed highly peaked confidence distribution (88% of NLI predictions in 0.9-1.0 bin), while compressed models had more distributed confidence scores
- BERT-Tiny achieved better calibration (lower ECE) than BERT-base but with worse accuracy, demonstrating the uncoupling of accuracy from behavioral metrics
- Distil-BERT preserved calibration better than architectural compression but still failed to achieve interpretability alignment with BERT-base

## Why This Works (Mechanism)

### Mechanism 1: Divergent Feature Attribution Under Compression
- Claim: Compressed models, even with similar accuracy, base predictions on different input features than large models
- Mechanism: Reducing model capacity forces compressed models to learn more efficient decision boundaries that rely on subsets of features that are easier to learn but not necessarily the same ones the large model used
- Evidence anchors: Jaccard coefficients between BERT-base and compressed models range from 0.372 to 0.669; Table 2 shows BERT-base focusing on different words than BERT-Mini despite both predicting correctly
- Break condition: If compression techniques are specifically designed to preserve feature attributions, this mechanism may be mitigated

### Mechanism 2: Altered Probability Calibration Profiles
- Claim: Compression changes the distribution of a model's confidence scores, typically making smaller models less confident or differently confident than large models
- Mechanism: Large models are often overconfident with high proportion of predictions near 1.0; compression reduces this overconfidence, spreading predictions across lower confidence bins
- Evidence anchors: BERT-base assigns 88% of NLI predictions to 0.9-1.0 confidence bin; this drops significantly for smaller models (BERT-Tiny: 44.23% for NLI); reliability diagrams show different calibration profiles
- Break condition: This mechanism relies on large models being overconfident; if a large model were perfectly calibrated, the effect might differ

### Mechanism 3: Uncoupling of Accuracy from Behavioral Metrics
- Claim: Model accuracy is an insufficient proxy for behavioral trust-equivalence
- Mechanism: Accuracy is a single aggregate metric; trust-equivalence requires matching granular behavioral properties: which specific features are salient for each prediction and how reliable the probability scores are
- Evidence anchors: BERT-Tiny has worse accuracy but better ECE than BERT-base, directly showing a trade-off between metrics; compressed models achieve similar accuracy by taking different "paths" (different features) and with different "pace" (different confidence)
- Break condition: This mechanism is robust but its impact depends on application's tolerance for behavioral differences

## Foundational Learning

- Concept: **Model Calibration (Expected Calibration Error - ECE)**
  - Why needed here: Core metric used to define second dimension of trust-equivalence; understanding it is crucial to interpret findings on probability reliability
  - Quick check question: A model predicts "Class A" with 90% confidence. Out of 100 such predictions, it is correct 70 times. What is the calibration error for this specific confidence bin?

- Concept: **Feature Attribution Methods (LIME/SHAP)**
  - Why needed here: Tools used to measure first dimension of trust-equivalence (interpretability alignment); knowing their strengths and weaknesses is key
  - Quick check question: Why does this paper treat the top-K features from LIME/SHAP as an unordered set rather than relying on their precise rank order?

- Concept: **Jaccard Similarity Coefficient**
  - Why needed here: Specific metric used to quantify overlap between feature sets from large and compressed models
  - Quick check question: Set A contains {word1, word2, word3}. Set B contains {word2, word3, word4}. What is the Jaccard Similarity between Set A and Set B?

## Architecture Onboarding

- Component map: BERT-family model (BERT-base, Distil-BERT, etc.) -> Trust-equivalence evaluation framework -> Interpretability Alignment Module (LIME/SHAP + Jaccard) and Calibration Similarity Module (Confidence bins + ECE/MCE)

- Critical path:
  1. Large model (e.g., BERT-base) is trained/fine-tuned for a task
  2. Compressed model is proposed as efficient substitute
  3. Trust-Equivalence Audit: Run both models on same test set; apply Interpretability Alignment Module; apply Calibration Similarity Module
  4. Go/No-Go Decision: If alignment scores are low (< 0.7) and calibration profiles differ significantly, compressed model is NOT a trust-equivalent drop-in replacement

- Design tradeoffs:
  - Accuracy vs. Calibration: Extreme compression (BERT-Tiny) can improve calibration while hurting accuracy; trade-off must be made explicit
  - Efficiency vs. Behavioral Fidelity: Knowledge distillation preserves calibration better than architectural reduction but still fails interpretability alignment; different compression methods have different fidelity profiles

- Failure signatures:
  - Silent Failure: Compressed model with ~90% accuracy is deployed as drop-in replacement; no errors flagged but model's reasoning is based on spurious or different features (e.g., person's name instead of financial data in loan application), and confidence scores are misleading

- First 3 experiments:
  1. Reproduce Interpretability Alignment: Take BERT-base and Distil-BERT fine-tuned on MRPC dataset; for 50 random test sentences, use LIME to identify top 5 influential words; calculate average Jaccard similarity (expect 0.5-0.7)
  2. Measure Calibration Shift: Using same two models, collect prediction confidence scores for entire test set; plot histograms of confidence for both models; calculate and compare ECE (expect Distil-BERT's confidence histogram to be less peaked at 1.0)
  3. Task-Sensitivity Probe: Repeat calibration measurement for sentiment analysis task (like SST-2) instead of paraphrase identification; investigate if calibration shift is consistent across different task types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed failures in trust-equivalence generalize to decoder-only large language models (LLMs) and non-text modalities like image and audio?
- Basis in paper: [explicit] Authors state "It will be interesting to see if the same trends carry over to popular decoder-only large language models and other modalities, such as image and audio"
- Why unresolved: Current study is restricted to encoder-only Transformer models and text classification tasks
- What evidence would resolve it: Replicating interpretability alignment and calibration similarity experiments on generative decoder models (e.g., GPT, Llama) and Vision Transformers

### Open Question 2
- Question: Can model compression techniques be specifically designed to preserve trust-equivalence alongside accuracy?
- Basis in paper: [explicit] Authors identify need for future research "to design model compression methods that preserve the trust-equivalence"
- Why unresolved: Existing compression paradigms optimize primarily for performance parity, often degrading calibration or altering feature attributions
- What evidence would resolve it: Novel compression objective function that successfully minimizes divergence of feature attributions (Jaccard similarity) and calibration error (ECE) between teacher and student models

### Open Question 3
- Question: Is the finding of low interpretability alignment an artifact of the instability inherent in LIME and SHAP methods?
- Basis in paper: [inferred] Paper acknowledges "LIME and SHAP have known instabilities" yet relies on them to quantify primary dimension of trust-equivalence
- Why unresolved: If explainability methods themselves are unstable, observed low Jaccard similarity scores might reflect measurement noise rather than fundamental divergence in model decision processes
- What evidence would resolve it: Comparative analysis using ground-truth benchmarks or more stable explainability techniques to confirm feature attribution divergence is real

## Limitations

- The paper assumes LIME/SHAP feature attributions are reliable proxies for model decision-making, though both methods are known to have instabilities and can produce inconsistent results across runs
- Generalizability of results beyond BERT-family models and the specific NLP tasks tested remains unclear
- Random seed selection for fine-tuning and data shuffling was not specified, introducing potential variance in reproducibility
- Trade-off between behavioral fidelity and efficiency was not quantified in terms of practical deployment costs

## Confidence

- High confidence in core claim that compressed models differ behaviorally from large models, supported by consistent empirical evidence across multiple metrics and tasks
- Medium confidence in generalizability of interpretability alignment findings, as results depend on specific feature attribution methods used
- Medium confidence in calibration findings, as relationship between model capacity and confidence distribution may vary across different model architectures

## Next Checks

1. Reproduce Jaccard similarity scores using multiple random seeds to quantify variance in interpretability alignment measurements
2. Test trust-equivalence evaluation on a non-BERT architecture (e.g., RoBERTa or DeBERTa) to assess generalizability
3. Implement alternative compression method (e.g., quantization or pruning) and compare its behavioral fidelity profile to knowledge distillation