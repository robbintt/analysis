---
ver: rpa2
title: Policy Optimization for Dynamic Heart Transplant Allocation
arxiv_id: '2512.12497'
source_url: https://arxiv.org/abs/2512.12497
tags:
- policy
- transplant
- allocation
- heart
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a simulator for heart transplant allocation
  using UNOS data and machine learning models to predict offer acceptance, graft survival,
  and waitlist survival. The status quo allocation policy was shown to be inferior
  to a simple myopic policy that maximizes estimated years gained per transplant.
---

# Policy Optimization for Dynamic Heart Transplant Allocation

## Quick Facts
- arXiv ID: 2512.12497
- Source URL: https://arxiv.org/abs/2512.12497
- Reference count: 40
- The status quo heart transplant allocation policy is considerably inferior to a myopic policy that maximizes life years gained per transplant.

## Executive Summary
This paper develops a simulator for heart transplant allocation using UNOS data and machine learning models to predict offer acceptance, graft survival, and waitlist survival. The status quo allocation policy was shown to be inferior to a simple myopic policy that maximizes estimated years gained per transplant. The paper introduces potentials—a long-term utility measure—to account for the dynamic nature of allocation, leading to further performance gains. Batching a small number of donors also improved outcomes by enabling better matching.

## Method Summary
The authors develop a heart transplant allocation simulator using UNOS historical data (1987-present) filtered to adult patients. They train three machine learning models: a LightGBM classifier for offer acceptance (AUROC=0.895), Cox proportional hazards regression models for graft survival (C-index=0.600) and waitlist survival (C-index=0.726). The simulator processes donor arrivals and waitlist dynamics to evaluate allocation policies. They implement and compare multiple policies including the status quo, a myopic policy maximizing life years gained per transplant, potential-based policies that account for future matchability, and batching policies that allocate multiple donors jointly.

## Key Results
- The status quo policy achieves 932-1081 life years per month versus 1317-1644 for the myopic policy across 3 test months
- Potential-based policies gain an additional 31 life years per month on average compared to myopic
- Batching donors (size 5) yields 20-57 additional life years compared to sequential allocation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A myopic policy that maximizes estimated life years gained per transplant outperforms the current tier-based status quo allocation system.
- Mechanism: The myopic policy explicitly computes δ(t) = GraftSurv(donor, patient) − WaitlistSurv(patient) for each candidate, selecting the patient with the highest positive benefit. This incorporates both post-transplant survival predictions and pretransplant urgency, whereas the status quo relies on coarse tiers based primarily on medical status, geography, and waitlist time without directly optimizing for survival outcomes.
- Core assumption: The Cox regression models for graft and waitlist survival provide sufficiently accurate estimates of true survival times; the difference between predicted graft survival and waitlist survival meaningfully captures transplant benefit.
- Evidence anchors:
  - [abstract] "the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant"
  - [Section 3.3, Table 2] Myopic policy achieves 1317–1644 life years per month vs. 932–1081 for status quo (January–March 2019)
  - [corpus] The related paper on waitlist mortality prediction (arxiv:2507.07339) addresses longitudinal modeling for the same UNOS dataset, supporting the feasibility of survival prediction but not validating this specific policy comparison
- Break condition: If graft survival C-index degrades significantly on out-of-distribution donor-patient pairs (e.g., rare blood type combinations), the myopic policy's advantage over status quo may narrow or reverse.

### Mechanism 2
- Claim: Incorporating "potentials"—a measure of a patient's future matchability based on blood type—improves upon the myopic policy by accounting for the dynamic allocation process.
- Mechanism: The potential-based policy modifies the objective to δ_pot(t) = GraftSurv − WaitlistSurv + θ^T x_pot, where θ parameters are learned from historical trajectories via SMAC optimization. Blood type AB patients have higher future matchability (can accept from all donor types), so the policy may deprioritize them when an equivalent blood type O candidate exists, preserving harder-to-match patients for future opportunities.
- Core assumption: Blood type alone captures sufficient information about future matchability; parameters learned from 3 months of training trajectories generalize to held-out months; offer acceptance rates can be treated as 100% during training without significantly biasing learned potentials.
- Evidence anchors:
  - [abstract] "develop improved policies that account for the dynamic nature of the allocation process through the use of potentials"
  - [Section 3.4, Table 3] Potential-based policy gains 31 additional life years per month on average across 9 test months
  - [corpus] No directly comparable evidence in corpus for potentials in heart allocation; kidney exchange literature (Dickerson et al. 2012) is cited as the conceptual origin
- Break condition: If higher-dimensional potentials (beyond blood type) are needed for meaningful gains, training becomes computationally prohibitive—the paper notes this as a bottleneck.

### Mechanism 3
- Claim: Batching a small number of brain-dead donors (DBD, non-DCD) and allocating them jointly improves outcomes over sequential single-donor dispatch.
- Mechanism: By waiting until B donors accumulate or 48 hours elapse, the policy solves a maximum weighted bipartite matching problem between the donor batch and current waitlist. This partially converts an online problem to an offline optimization, enabling better donor-patient pairings that would be missed by greedy sequential allocation.
- Core assumption: DBD donors remain in approximately stable condition for 1–2 days; the Hungarian algorithm's optimal matching translates to real-world survival gains; no significant deterioration in organ quality occurs during the batching window.
- Evidence anchors:
  - [abstract] "batching together even a handful of donors—which is a viable option for a certain type of donors—further enhances performance"
  - [Section 3.5, Table 4] Batch size 5 yields 20–57 additional life years compared to batch size 1 across 3 months
  - [corpus] No corpus papers address batching in organ allocation; mechanism remains specific to this work
- Break condition: If real-world ischemic times during batched allocation exceed model assumptions, or if clinical workflows cannot accommodate delayed dispatch, gains may not materialize in practice.

## Foundational Learning

- Concept: Cox proportional hazards regression and survival analysis with right censorship
  - Why needed here: Both graft survival and waitlist survival models use Cox regression to handle censored outcomes (patients still alive or lost to follow-up). Understanding hazard functions, baseline hazard estimation (Breslow's method), and C-index evaluation is essential for interpreting model reliability.
  - Quick check question: Given a Cox model with θ = [0.5, -0.3] for covariates [age, blood_pressure], does a patient with age=60, BP=120 have higher or lower hazard than one with age=50, BP=140?

- Concept: Gradient boosting decision trees (LightGBM) for binary classification
  - Why needed here: The offer acceptance predictor uses LightGBM with 200 estimators. Understanding how GBDT handles mixed categorical/numerical features, prevents overfitting via validation-based early stopping, and produces calibrated probabilities is necessary for debugging prediction quality.
  - Quick check question: If AUROC drops from 0.895 to 0.750 on a new hospital's data, what are three plausible causes related to feature distribution shifts?

- Concept: Maximum weighted bipartite matching (Hungarian algorithm)
  - Why needed here: The batching policy formulates allocation as assigning B donors to patients to maximize total δ scores. Understanding the Hungarian algorithm's O(n³) complexity and constraint handling is necessary for scaling to larger batch sizes.
  - Quick check question: If you have 5 donors and 100 patients with computed δ scores for each pair, what is the size of the bipartite graph, and does the Hungarian algorithm directly apply?

## Architecture Onboarding

- Component map:
UNOS Historical Data (1987–present) -> Data Cleaning (SRTR protocol) -> Offer Acc/Graft Surv/Waitlist Surv (GBDT/Cox) -> Simulator (patient entry/exit/status updates) -> Policy Layer (myopic → potentials → batching) -> Evaluation (life years gained over time window)

- Critical path:
  1. Rebuild survival models first—C-index values of 0.600 (graft) and 0.726 (waitlist) indicate modest predictive power; any policy gains are contingent on improving these or quantifying uncertainty.
  2. Validate offer acceptance model on recent data distributions before trusting simulation fidelity.
  3. Start with myopic policy experiments before attempting potential training (computationally expensive).

- Design tradeoffs:
  - Distance vs. ischemic time: Paper finds gains up to 1000–2000 NM, but graft survival model has limited training data at high distances (selection bias).
  - Myopic vs. non-myopic: Potentials improve outcomes but require expensive SMAC training; interpretability is high (blood type only) but extensibility is limited.
  - Batching vs. urgency: Delaying allocation for DBD donors may harm urgent patients; paper does not model status-based exceptions.

- Failure signatures:
  - Graft survival C-index near 0.5 on validation data → model no better than random; policy optimization meaningless.
  - Potential parameters θ converging to near-zero → blood type not providing useful signal; need richer covariates.
  - Batching experiments showing no gain or negative gain → organ quality deterioration or matching formulation error.

- First 3 experiments:
  1. Replicate Cox regression models on a held-out year of UNOS data; report C-index with confidence intervals. Flag if values deviate >0.02 from paper's reported means.
  2. Run myopic policy simulation with α=1 (current acceptance rates) and max_distance=1000 NM for a 3-month window; compare life years gained to paper's Table 2 values. Investigate if results differ by >10%.
  3. Ablate the offer acceptance model by setting all acceptance probabilities to 1.0 (α=0) for a single month; quantify the performance delta to isolate the contribution of acceptance modeling vs. survival modeling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the allocation framework be extended to balance life years gained with fairness objectives, such as max-min fairness for disadvantaged groups?
- Basis in paper: [explicit] The conclusion states that "extending our framework to address fairness considerations is another important direction for the future," noting the current objective may conflict with fairness.
- Why unresolved: The current study optimizes exclusively for life years gained and does not model equity constraints for demographics or clinical groups like highly sensitized patients.
- What evidence would resolve it: A modified optimization algorithm that incorporates fairness constraints and demonstrates a viable trade-off between efficiency and equity in simulations.

### Open Question 2
- Question: Does combining donor batching with potential-based policies yield additive performance improvements?
- Basis in paper: [explicit] Section 3.5 states that "combining potentials with batching is a natural direction," but the authors did not perform the experiment due to computational restrictions.
- Why unresolved: The paper evaluates batching and potentials independently; the interaction effect between these two methods remains unknown.
- What evidence would resolve it: Simulation results showing life years gained for a policy that simultaneously utilizes small-batch matching and the potentials function.

### Open Question 3
- Question: To what extent are high offer rejection rates driven by misaligned incentives between transplant centers and national efficiency?
- Basis in paper: [explicit] Section 3.2 posits that "investigating these issues [incentives] in more depth is an interesting direction for future research," suggesting current evaluation metrics may prioritize center quality over system quantity.
- Why unresolved: The paper identifies the phenomenon of rejections but lacks the causal analysis to determine why centers reject offers that the model predicts would be beneficial.
- What evidence would resolve it: An empirical analysis correlating transplant center evaluation metrics with their organ offer acceptance patterns.

## Limitations

- The survival models show modest C-indices (0.600 for graft, 0.726 for waitlist), raising questions about whether policy improvements would translate to real-world outcomes
- The potential-based policy relies solely on blood type to capture future matchability, which may oversimplify the complex dynamics of donor-recipient compatibility
- Batching assumes DBD donors remain viable for 1–2 days, but ischemic time thresholds and organ quality degradation during delays are not explicitly modeled

## Confidence

- **High confidence** in the mechanistic claim that a myopic policy outperforms status quo by directly optimizing for life years gained, as supported by direct comparison on real UNOS data
- **Medium confidence** in the effectiveness of potentials, given the strong assumptions about blood type sufficiency and the lack of external validation beyond UNOS data
- **Low confidence** in the clinical applicability of batching gains without real-world ischemic time data and center acceptance behavior modeling

## Next Checks

1. Replicate the survival model training on a temporally separated UNOS test set and report C-index with 95% confidence intervals. Investigate whether modest gains persist when model uncertainty is quantified.

2. Conduct an ablation study isolating the impact of offer acceptance modeling by running the myopic policy with acceptance probabilities set to 1.0; measure the delta in life years gained to assess model dependencies.

3. Validate the potential-based policy on a cross-validation fold not used for parameter training (e.g., 2020 data if trained on 2019) to test generalization beyond the original training window.