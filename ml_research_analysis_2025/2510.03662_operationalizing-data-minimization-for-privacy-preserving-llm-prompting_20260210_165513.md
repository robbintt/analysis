---
ver: rpa2
title: Operationalizing Data Minimization for Privacy-Preserving LLM Prompting
arxiv_id: '2510.03662'
source_url: https://arxiv.org/abs/2510.03662
tags:
- weighted
- type
- information
- abstract
- redact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal framework for data minimization
  in LLM prompting, aiming to quantify the least privacy-revealing disclosure needed
  to maintain utility. The method uses a priority-queue tree search over a privacy-ordered
  action space to find minimal prompts that preserve task performance.
---

# Operationalizing Data Minimization for Privacy-Preserving LLM Prompting

## Quick Facts
- **arXiv ID:** 2510.03662
- **Source URL:** https://arxiv.org/abs/2510.03662
- **Reference count:** 40
- **Key outcome:** A formal framework that uses priority-queue tree search to find minimal privacy-revealing prompts while preserving task utility across four datasets and nine LLMs.

## Executive Summary
This paper introduces a formal framework for data minimization in LLM prompting, aiming to quantify the least privacy-revealing disclosure needed to maintain utility. The method uses a priority-queue tree search over a privacy-ordered action space to find minimal prompts that preserve task performance. Experiments across four datasets with nine LLMs show that larger frontier models tolerate significantly more data minimization than smaller ones (e.g., 85.7% redaction for GPT-5 vs. 19.3% for Qwen2.5-0.5B), and that directly predicting minimal disclosure is challenging for LLMs. Adversarial audits confirm strong privacy protection after minimization, with low recoverability of redacted or abstracted information. The results demonstrate data minimization as a promising paradigm for reducing privacy exposure in LLM interactions.

## Method Summary
The paper proposes a two-stage "Freeze-Then-Search" algorithm to minimize privacy-revealing information in LLM prompts. Stage 1 "freezes" entities that are essential for utility (redaction/abstraction breaks performance). Stage 2 uses a priority-queue tree search to explore the privacy-ordered action space, starting from the most private state and iteratively relaxing constraints. A distilled Privacy Comparator (Qwen2.5-7B) ranks privacy sensitivity of prompt variants, while a utility predicate (GPT-4o judge) verifies task success. The method is evaluated on four datasets with nine LLMs, measuring redaction/abstraction percentages and conducting adversarial recovery audits.

## Key Results
- Larger frontier models tolerate significantly more data minimization (85.7% redaction for GPT-5) than smaller models (19.3% for Qwen2.5-0.5B).
- Redaction provides stronger privacy guarantees than abstraction against inference attacks, with lower recovery rates.
- LLMs struggle to directly predict optimal data minimization strategies, tending to "overshare" via abstraction.
- The search-based approach successfully finds minimal prompts that maintain utility while significantly reducing privacy exposure.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Systematically searching a privacy-ordered action space locates the minimal disclosure point better than single-pass prediction.
- **Mechanism:** The "Freeze-Then-Search" algorithm treats data minimization as a tree search. It starts at the most private state (maximum redaction) and iteratively "relaxes" constraints (Redact → Abstract → Retain) based on a priority queue. This ensures the system finds the first valid utility state that requires the least amount of information, effectively reversing the "abstraction bias" found in standard LLMs.
- **Core assumption:** Privacy sensitivity can be reliably ordered (lattice structure), and utility is a binary predicate (pass/fail) that can be evaluated at each step.
- **Evidence anchors:** [Abstract] "propose a priority-queue tree search to locate this optimal point within a privacy-ordered transformation space." [Section 4.1] "Stage 2: Privacy-Comparator Priority-Queue Tree Search... returns the first action profile a that satisfies the utility predicate."
- **Break condition:** If the utility predicate is noisy or the Privacy Comparator fails to align with human judgment, the search may converge on a suboptimal or unsafe node.

### Mechanism 2
- **Claim:** Data minimization feasibility scales with model capability.
- **Mechanism:** Frontier models (e.g., GPT-5) possess stronger internal knowledge and inference capabilities, allowing them to maintain utility even when specific spans (like names or locations) are redacted or abstracted. Smaller models (e.g., Qwen-0.5B) lack this "inference gap" and thus require nearly all original information (Retain) to function, limiting minimization potential.
- **Core assumption:** The utility loss observed in smaller models is due to a lack of capability rather than an inherent ambiguity in the prompt.
- **Evidence anchors:** [Abstract] "larger frontier LLMs can tolerate stronger data minimization while maintaining task quality than smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for Qwen2.5-0.5B)." [Section 6.1] "This suggests not just a privacy gap, but a capability gap: models may lack awareness of what information they actually need to solve a task."
- **Break condition:** If a small model is heavily quantized or corrupted, the required "Retain" ratio might artificially inflate, falsely indicating low minimization potential.

### Mechanism 3
- **Claim:** Redaction offers strictly stronger privacy guarantees than abstraction against inference attacks.
- **Mechanism:** While abstraction preserves semantic meaning (e.g., "New York" → "a city in the US"), this semantic residue allows adversaries to infer the original value. Redaction removes the token entirely (or replaces it with a generic tag), breaking the contextual link required for inference.
- **Core assumption:** Adversaries rely on local context within the text to recover information, rather than external knowledge bases alone.
- **Evidence anchors:** [Section 6.1] "Abstraction consistently yields higher overall recovery than redaction... Redaction is more robust to on-text inference than abstraction." [Table 3] Shows p_corr for Abstraction (0.149) vs. Redaction (0.051) on ShareGPT.
- **Break condition:** If the redacted entity is highly predictable from the task alone (e.g., "capital of France"), redaction provides little marginal privacy benefit over abstraction.

## Foundational Learning

- **Concept: Data Minimization (GDPR Art. 5(1)(c))**
  - **Why needed here:** This is the theoretical foundation of the paper. You must understand that the goal is not anonymity (which might break utility), but limiting disclosure to what is necessary.
  - **Quick check question:** Does removing the user's email address break the "book a flight" task? If yes, minimization requires retaining it (or abstracting it); if no, it should be redacted.

- **Concept: Ordinal Lattice (Action Space)**
  - **Why needed here:** The algorithm relies on a strict ordering of privacy states: RETAIN ≺ ABSTRACT ≺ REDACT. You need to understand this to implement the "relaxation" steps in the search.
  - **Quick check question:** In a valid relaxation step, can you jump from REDACT directly to RETAIN? (Answer: No, the paper defines a lattice path).

- **Concept: Best-First Search (Priority Queue)**
  - **Why needed here:** The core algorithm is not a random search but a prioritized traversal of a tree.
  - **Quick check question:** If Node A is more privacy-preserving than Node B, which node does the priority queue dequeue first?

## Architecture Onboarding

- **Component map:** PII Detector → Action Space → Privacy Comparator → Utility Predicate → Search Engine
- **Critical path:** The interaction between the Search Engine and the Utility Predicate. The search proposes a minimal prompt; the predicate checks it. If it fails, the search must "relax" the prompt (reveal more info). Performance depends on the speed of this loop (latency of the utility judge).
- **Design tradeoffs:**
  - *Search Depth vs. Latency:* The paper notes complexity ≲ c T log T · t_C. Aggressive minimization (large search space) increases latency significantly.
  - *Abstraction vs. Redaction:* Abstraction preserves utility better for small models but leaks privacy (Mechanism 3). Redaction is safer but harder for weak models.
- **Failure signatures:**
  - "Frozen" Utility: If Stage 1 "Freeze" detects that redacting even a single entity breaks utility, the system may force RETAIN on critical data, minimizing privacy gains.
  - Comparator Misalignment: If the distilled Comparator prioritizes the wrong variant, the search explores the wrong branch of the tree, wasting compute or selecting a sub-optimal prompt.
  - Reasoning Model Bias: GPT-5/reasoning models may "overshare" via abstraction if asked to predict directly (Section 6.2).
- **First 3 experiments:**
  1. **Unit Test the Comparator:** Validate the distilled Qwen model against the human-labeled ground truth (Table 1) to ensure it aligns with the 71% accuracy benchmark before running the full search.
  2. **Sanity Check the "Freeze" Stage:** Run a prompt where a specific entity (e.g., a specific date) is mathematically required for the answer. Verify the "Freeze" stage correctly marks it as immutable (forced RETAIN).
  3. **End-to-End Redaction vs. Abstraction:** Run the full pipeline on a sample from ShareGPT. Compare the recovery rate of the "Redact-Heavy" output vs. "Abstract-Heavy" output using the provided adversarial audit script.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What causal mechanisms explain why larger frontier models tolerate significantly higher data minimization than smaller models?
- **Basis in paper:** [explicit] The authors "call for research to investigate the underlying causes of the varied levels of information 'redundancy' across models," noting that determining information necessity is an "open research frontier."
- **Why unresolved:** The paper empirically observes that GPT-5 tolerates 85.7% redaction while Qwen-0.5B tolerates only 19.3%, but does not explain the architectural or internal reasoning differences driving this gap.
- **What evidence would resolve it:** Causal tracing or probing studies that identify how "frozen" vs. "non-frozen" entities are processed differently across model sizes during inference.

### Open Question 2
- **Question:** Does test-set contamination artificially inflate the utility scores of data-minimized prompts?
- **Basis in paper:** [explicit] The conclusion states that "the potential impact of test set contamination... should be carefully taken into consideration in future investigations."
- **Why unresolved:** Models may answer minimized prompts correctly not by generalizing from the minimal context, but by relying on memorized associations from public datasets (e.g., MedQA) present in their training data.
- **What evidence would resolve it:** Re-evaluating the minimization framework using fresh data known to be absent from training corpora or applying contamination detection metrics to the benchmark results.

### Open Question 3
- **Question:** Can small, specialized models be trained to predict minimization strategies without defaulting to the observed "abstraction bias"?
- **Basis in paper:** [explicit] Results show LLMs "struggle to predict optimal data minimization" (oversharing via abstraction), and the authors advocate for "model-specific predictors" for on-device deployment.
- **Why unresolved:** The study demonstrates that zero-shot prediction fails, but it remains untested whether supervised finetuning on the oracle search results can yield efficient, accurate predictors for edge devices.
- **What evidence would resolve it:** Training lightweight models on the oracle outputs and measuring their prediction accuracy (Fit vs. Overshare rates) against the search-derived benchmarks.

## Limitations
- The Privacy Comparator's 71% accuracy on human-labeled data represents a significant drop from the zero-shot o3 baseline (89%), suggesting possible overfitting to the training distribution.
- The utility predicate's dependence on GPT-4o judge introduces both latency (9.73s average) and potential subjectivity in open-ended task evaluation.
- The search algorithm's computational complexity (c·T·log T·t_C) may become prohibitive for longer prompts with many PII spans.

## Confidence

**High Confidence:** The core algorithmic approach (Freeze-Then-Search with priority queue) is technically sound and well-specified. The comparative results showing larger models tolerating more data minimization (85.7% vs 19.3% redaction) are directly supported by experimental measurements.

**Medium Confidence:** The claim that redaction provides stronger privacy than abstraction relies on specific adversarial evaluation conditions and may not generalize to more sophisticated inference attacks. The generalizability of findings across different task types and PII categories is supported but not exhaustively tested.

**Low Confidence:** The direct prediction of minimal disclosure by LLMs (Section 6.2) shows poor performance, but the evaluation methodology may not fully capture the potential of chain-of-thought or reasoning approaches.

## Next Checks

1. **Comparator Robustness Test:** Systematically evaluate the Privacy Comparator on adversarial examples where privacy sensitivity judgments are ambiguous or context-dependent, measuring both accuracy and consistency across multiple runs.

2. **Search Algorithm Scalability:** Profile the Freeze-Then-Search algorithm on prompts with varying numbers of PII spans (10, 25, 50) to empirically validate the claimed computational complexity and identify performance bottlenecks.

3. **Cross-Model Generalization:** Replicate the data minimization experiment across a broader range of task types (beyond the current four datasets) and with additional PII categories not well-represented in the current evaluation, particularly focusing on temporal and demographic attributes.