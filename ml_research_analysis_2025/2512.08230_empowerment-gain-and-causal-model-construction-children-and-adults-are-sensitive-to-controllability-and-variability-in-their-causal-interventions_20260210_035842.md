---
ver: rpa2
title: 'Empowerment Gain and Causal Model Construction: Children and adults are sensitive
  to controllability and variability in their causal interventions'
arxiv_id: '2512.08230'
source_url: https://arxiv.org/abs/2512.08230
tags:
- causal
- learning
- machine
- children
- empowerment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the relationship between empowerment and causal\
  \ learning, bridging reinforcement learning and causal inference. The authors propose\
  \ that empowerment\u2014maximizing mutual information between actions and outcomes\u2014\
  can serve as an intrinsic reward that guides causal model construction."
---

# Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions

## Quick Facts
- arXiv ID: 2512.08230
- Source URL: https://arxiv.org/abs/2512.08230
- Authors: Eunice Yiu; Kelsey Allen; Shiry Ginosar; Alison Gopnik
- Reference count: 3
- Key outcome: Humans prefer systems combining controllability and variability for causal learning, with context-dependent shifts between goal-directed and exploratory modes.

## Executive Summary
This paper investigates how humans use empowerment—maximizing mutual information between actions and outcomes—as an intrinsic reward to guide causal model construction. Through two studies with children (ages 5-10) and adults, the authors demonstrate that people prefer causal systems offering both controllability and variability over systems with only one dimension. The research bridges reinforcement learning and causal inference by showing how seeking empowerment leads agents to discover genuine causal relations. Results reveal that while both groups prefer controllable-and-variable systems for goal-directed tasks, they shift toward purely variable systems during exploratory play contexts.

## Method Summary
Participants observed demonstrations from three star machines: one always produced the same output (pure control), one produced random outputs (pure variability), and one produced variable but controllable outputs where slot size predicted output size. After demonstrations, participants completed generalization tasks requiring them to select the correct slot to produce novel outputs (e.g., extra small stars, hats of specific sizes, light bulbs of specific brightness). Study 2 contrasted two features (hue vs. size) within single machines where one feature was controllable and the other random. Both children and adults completed tasks in work (goal-directed) and play (exploratory) contexts to test context-dependent preferences.

## Key Results
- Both children and adults showed significant preference for the controllable-and-variable machine over purely variable or purely controllable machines in goal-directed generalization tasks.
- Children selected correct slots on controllable machines 56.26% of the time when producing novel outputs, significantly above chance.
- Adults and children shifted preference toward the purely variable machine in exploratory play contexts but not in goal-directed work contexts.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Seeking empowerment (high mutual information between actions and outcomes) leads agents to discover causal relations.
- **Mechanism:** Agents maximize the mutual information between their actions and environmental outcomes. This naturally identifies relations where interventions predictably change effects—the defining feature of causal structure on the interventionist account.
- **Core assumption:** Causal relations are precisely those where intervening on variable X reliably changes variable Y; this matches the formal definition of high action-outcome mutual information.
- **Evidence anchors:**
  - [abstract] "If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model."
  - [section] "The basic structure of causal relations on the interventionist view is that we can intervene systematically on X to systematically change the value of Y... This is well captured by the notion of empowerment."
  - [corpus] Related work "Towards Empowerment Gain through Causal Structure Learning in Model-Based RL" explicitly tests empowerment as intrinsic motivation for causal discovery, providing convergent computational evidence.
- **Break condition:** If the environment contains high-mutual-information relations that are not causal (e.g., engineered correlations without intervention support), empowerment-seeking could converge on spurious structure.

### Mechanism 2
- **Claim:** Humans preferentially select and generalize from systems combining both controllability and variability over systems offering only one.
- **Mechanism:** Learners evaluate candidate causal systems along two dimensions—whether actions reliably produce effects (control) and whether different actions produce different effects (variability). High empowerment requires both.
- **Core assumption:** Learners can separately estimate reliability of action-outcome mappings and the entropy of outcome distributions.
- **Evidence anchors:**
  - [abstract] "Participants observed three star machines: one always produced the same output (pure control), one produced random outputs (pure variability), and one produced variable but controllable outputs... both children and adults preferred the controllable-and-variable machine."
  - [section] Study 1 Results: "When asked to generalize to a new object by producing hats of different sizes, children selected the correct slots on the controllable machines 56.26% of the time... showed a significant preference for the variable and controllable machine over the purely variable machine and over the purely controllable machine."
  - [corpus] Weak direct corpus evidence on this specific two-dimensional preference; most related work focuses on empowerment as single metric rather than decomposed factors.
- **Break condition:** If estimating either dimension becomes computationally intractable (e.g., high-dimensional action spaces with sparse feedback), the mechanism may fail to guide selection.

### Mechanism 3
- **Claim:** Humans shift between empowerment-seeking and pure variability-seeking depending on whether the context is goal-directed (work) or exploratory (play).
- **Mechanism:** Contextual cues modulate the relative weighting of controllability vs. variability in action selection. Goal-directed contexts upweight controllability; exploratory contexts increase tolerance for random variability.
- **Core assumption:** Agents maintain context-sensitive value functions that reweight intrinsic motivations.
- **Evidence anchors:**
  - [abstract] "Children and adults shifted toward the random machine in exploratory contexts but not in goal-directed contexts."
  - [section] Machine Preference Results: "There was a significant shift in adults' machine preferences between the work and play contexts... adults were significantly more likely to select the variable and controllable machine in the work context compared to the play context, while their preference for the purely variable machine increased in the play context."
  - [corpus] "Intrinsically-Motivated Humans and Agents in Open-World Exploration" compares human and agent exploration but does not explicitly test context-switching mechanisms.
- **Break condition:** If contextual cues are ambiguous or the agent lacks a representation of "goal-directed vs. exploratory," the modulation may not engage correctly.

## Foundational Learning

- **Concept: Mutual Information**
  - **Why needed here:** Empowerment is formally defined as the mutual information between action sequences and outcome sequences. Without this, you cannot implement or evaluate empowerment-seeking behavior.
  - **Quick check question:** Given two random variables A (actions) and O (outcomes), can you compute I(A;O) and explain what it measures?

- **Concept: Causal Bayes Nets (Interventionist Framework)**
  - **Why needed here:** The paper's theoretical contribution explicitly bridges empowerment to the interventionist account of causation in the Bayes Net formalism. Understanding do(X) and its role in identifying causal structure is prerequisite.
  - **Quick check question:** What does it mean to perform an "intervention" on variable X in a causal graph, and how does this differ from merely observing X?

- **Concept: Intrinsic vs. Extrinsic Rewards in RL**
  - **Why needed here:** Empowerment is proposed as an *intrinsic* epistemic reward, distinct from classical extrinsic utility maximization. Understanding this distinction is necessary to implement empowerment-driven agents.
  - **Quick check question:** In an RL setting, how would an intrinsic reward for empowerment differ from a curiosity bonus based on prediction error?

## Architecture Onboarding

- **Component map:** Action encoder -> Outcome observer -> Mutual information estimator -> Context classifier -> Action selector

- **Critical path:**
  1. Collect (action, outcome) pairs from environment interaction.
  2. Estimate mutual information for candidate action-outcome mappings.
  3. Rank available actions/interventions by empowerment.
  4. Modulate selection based on detected context (work vs. play).
  5. Update internal causal model if high-empowerment relation confirmed.

- **Design tradeoffs:**
  - **Exact vs. approximate MI estimation:** Exact calculation is intractable in high dimensions; KRNN or variational approximations are common but introduce bias.
  - **Exploration-exploitation balance:** Pure empowerment-seeking may ignore high-reward but low-control options; hybrid objective functions are often needed.
  - **Context detection:** Hard-coded cues (e.g., task instructions) vs. inferred from environment statistics.

- **Failure signatures:**
  - **Noisy TV problem:** Agent fixates on maximally variable but uncontrollable sources (pure noise). Paper explicitly notes this as failure mode for novelty-only intrinsic rewards.
  - **Gaming the empowerment metric:** Agent finds trivial action-outcome correlations that do not reflect genuine causal structure (e.g., self-generated noise loops).
  - **Context misclassification:** Treating exploratory play as goal-directed suppresses useful random exploration.

- **First 3 experiments:**
  1. **Replicate "three machines" paradigm in silico:** Implement agents that choose among three simulated systems (pure control, pure variability, combined). Verify that empowerment-seeking agents prefer the combined system in goal contexts.
  2. **Ablate controllability or variability:** Test whether removing either dimension from the reward degrades generalization performance on transfer tasks (new objects, new modalities).
  3. **Test context-switching:** Train agents with explicit "work" vs. "play" context signals; verify preference shifts toward random exploration in play mode without degradation in work-task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a formal computational bridge be constructed between Causal Bayes Net learning and empowerment-driven reinforcement learning?
- Basis in paper: [explicit] The authors state that "A formal bridge between Bayes net causal learning and empowerment gain in RL would be very desirable," noting that current formalisms separate model-building from action.
- Why unresolved: While the paper argues the concepts are linked, Bayes nets typically treat interventions as exogenous, whereas RL treats them as endogenous actions, leaving their integration an unsolved formal problem.
- What evidence would resolve it: A computational model where maximizing empowerment directly constrains the hypothesis space for Bayesian inference, demonstrating tractable causal discovery.

### Open Question 2
- Question: Do children under age five prefer controllable variability (empowerment) over pure variability or pure control?
- Basis in paper: [inferred] The study tested 5-10 year olds, yet the introduction notes that infants as young as 3 months engage in "conjugate reinforcement" to gain control.
- Why unresolved: The developmental trajectory of the specific preference for *controllable variability* (as distinct from simple agency or novelty) remains untested in toddlers and infants.
- What evidence would resolve it: Adapting the "Star Machine" paradigm for younger children using looking-time or simple reaching measures to assess preferences between random and empowered outcomes.

### Open Question 3
- Question: Why do participants shift their preference toward the random, purely variable machine in "play" contexts but not "work" contexts?
- Basis in paper: [explicit] The results show adults and children significantly increased their preference for the purely variable machine in play contexts; the authors suggest this may reflect sensitivity to "information gain" but do not confirm the mechanism.
- Why unresolved: The study identifies the context-dependent behavioral shift but does not isolate whether the drive is for high-entropy novelty, diagnostic information, or a relaxation of utility constraints.
- What evidence would resolve it: A study manipulating the information-theoretic value of the random machine independently of its novelty to see which factor drives the "play" preference.

## Limitations
- Small sample sizes (N=16 adults, N=18 children in Study 1; N=28 adults, N=28 children in Study 2) limit generalizability.
- No computational model implementation demonstrating how empowerment maximization leads to causal model construction.
- Theoretical mechanism linking empowerment to causal learning remains verbal rather than formalized.

## Confidence
- **High confidence:** The empirical finding that humans prefer controllable-and-variable systems over pure control or pure variability alone in goal-directed contexts.
- **Medium confidence:** The context-dependent shift between goal-directed and exploratory preferences, as this relies on smaller effect sizes and more complex behavioral patterns.
- **Medium confidence:** The theoretical claim that empowerment maximization naturally leads to causal model construction, as this requires bridging reinforcement learning and causal inference frameworks that have different methodological traditions.

## Next Checks
1. Replicate the three-machine preference paradigm with larger sample sizes and pre-registered analysis plans to confirm the core behavioral effects.
2. Implement computational models that directly test whether agents maximizing empowerment converge on causal models rather than spurious correlations, particularly in environments with non-causal high-mutual-information relations.
3. Test the context-modulation hypothesis by manipulating explicit goal instructions versus environmental cues to see which more reliably shifts preferences between controllable and purely variable systems.