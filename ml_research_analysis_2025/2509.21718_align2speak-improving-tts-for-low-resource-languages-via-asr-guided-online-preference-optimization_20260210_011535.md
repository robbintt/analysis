---
ver: rpa2
title: 'Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online
  Preference Optimization'
arxiv_id: '2509.21718'
source_url: https://arxiv.org/abs/2509.21718
tags:
- languages
- speech
- grpo
- language
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a GRPO-based framework for adapting multilingual
  TTS models to low-resource languages by leveraging a pretrained ASR model as a reward
  signal. The method proceeds in three stages: multilingual pretraining with IPA tokens,
  fine-tuning on limited paired data, and online preference optimization using ASR-derived
  CER, speaker similarity, and PESQ scores as rewards.'
---

# Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization

## Quick Facts
- arXiv ID: 2509.21718
- Source URL: https://arxiv.org/abs/2509.21718
- Reference count: 0
- Primary result: GRPO with ASR rewards substantially improves TTS intelligibility and speaker consistency for low-resource languages, outperforming fine-tuning alone and offline DPO alignment.

## Executive Summary
This work introduces a GRPO-based framework for adapting multilingual TTS models to low-resource languages by leveraging a pretrained ASR model as a reward signal. The method proceeds in three stages: multilingual pretraining with IPA tokens, fine-tuning on limited paired data, and online preference optimization using ASR-derived CER, speaker similarity, and PESQ scores as rewards. Experiments show that combining fine-tuning with GRPO yields substantial gains in intelligibility (CER drops from 33.00% to 3.94% for Portuguese with 30 min data) and speaker consistency (SSIM improves from 0.50 to 0.79), outperforming fine-tuning alone. GRPO also surpasses offline alignment (DPO) in high-resource languages, delivering superior intelligibility, speaker similarity, and audio quality.

## Method Summary
The framework trains a Koel-TTS model with IPA tokenization on multilingual data (21k hours across 6 languages), then fine-tunes on low-resource paired data with 5× upsampling. GRPO samples 12 outputs per prompt at temperature 0.7, computes multi-objective rewards (CER, SSIM, PESQ) using external models, and updates the policy via group-relative advantages. The reward weights are 0.45/0.45/0.1 for CER/SSIM/PESQ, with no KL penalty. The approach enables adaptation without paired data by decoupling speaker prompts from text content.

## Key Results
- GRPO combined with fine-tuning reduces CER from 33.00% to 3.94% for Portuguese with 30 minutes of data
- SSIM improves from 0.50 to 0.79 for Portuguese with 30 minutes of data
- GRPO outperforms DPO in high-resource languages, achieving CER 0.56% vs 0.89% and SSIM 0.759 vs 0.667 for English

## Why This Works (Mechanism)

### Mechanism 1
Online GRPO with multi-objective rewards outperforms offline preference alignment for TTS adaptation. GRPO samples K outputs per prompt, computes rewards via ASR/speaker/PESQ models, centers rewards as group-relative advantages, and upweights higher-reward outputs. This creates a dynamic feedback loop unavailable in offline DPO. Core assumption: reward models generalize reliably to low-resource languages. Evidence: Table 1 shows Base+GRPO achieves CER 0.56% vs. Base+DPO 0.89% (English). Break condition: If reward models exhibit systematic bias, GRPO amplifies bias rather than quality.

### Mechanism 2
IPA-based tokenization enables cross-lingual phonetic transfer for zero-shot or few-shot adaptation. IPA maps orthographically diverse languages to shared phonetic space (~256 byte-level tokens). The model learns universal acoustic-to-phonetic mappings during multilingual pretraining, which transfer when generating unseen IPA sequences in new languages. Core assumption: IPA accurately represents target phoneme inventory. Evidence: Baseline model shows "limited generalizability despite IPA tokenization, evident from its high CER scores." Break condition: If target language contains phonemes absent from pretraining languages, synthesis degrades.

### Mechanism 3
Decoupled speaker prompts enable preference optimization without paired text-speech data. GRPO prompts require only (reference audio, text transcript) where reference audio content need not match text. This allows leveraging unpaired speech recordings and text corpora as training data. Core assumption: The TTS model can extract speaker characteristics from arbitrary reference audio independent of phonetic content. Evidence: "applying our multi-objective GRPO directly on this baseline substantially reduces CER and boosts SSIM, even without access to paired text–speech data in the target language." Break condition: If speaker embedding extraction is content-dependent, speaker similarity rewards become noisy.

## Foundational Learning

- **Autoregressive codec-based TTS**: The model generates audio tokens autoregressively at 21.5 FPS using NanoCodec; understanding token prediction and cross-attention conditioning is essential for debugging synthesis failures.
  - Quick check: Given a codec with codebook size 1024 and 8 codebooks per frame, what is the per-step vocabulary size for the AR decoder?

- **Policy gradient methods and advantage estimation**: GRPO uses group-relative advantages (reward minus group mean) to weight log-likelihood updates; misunderstanding this leads to incorrect implementation of the loss function.
  - Quick check: In GRPO, if K=4 samples yield rewards [0.3, 0.5, 0.7, 0.9], what are the corresponding advantages A_i,k?

- **Speech quality metrics (CER, SSIM, PESQ)**: The reward function is a weighted combination (w_cer=0.45, w_ssim=0.45, w_pesq=0.1) of normalized metrics; understanding their ranges and failure modes is critical for reward engineering.
  - Quick check: Why might optimizing purely for CER lead to degraded naturalness or speaker similarity?

## Architecture Onboarding

- Component map: Non-autoregressive encoder -> Autoregressive decoder -> NanoCodec audio tokens; External reward models (Whisper ASR, Titanet speaker verification, PESQ estimator) used during GRPO only

- Critical path: 1) Pretrain baseline on 6 languages (21k hours) with IPA tokenization and next-frame prediction loss; 2) Fine-tune on mixed data (original + upsampled low-resource) with learning rate 1e-5; 3) Run GRPO: for each of 15k prompts per language, sample K=12 outputs at temperature 0.7, compute rewards, update with learning rate 2e-7

- Design tradeoffs: Upsampling factor (5×) during fine-tuning balances exposure vs. overfitting; reward weights (0.45/0.45/0.1) emphasize intelligibility over quality; omitting KL penalty enables faster convergence but risks mode collapse

- Failure signatures: High CER after fine-tuning indicates insufficient upsampling or learning rate too high; SSIM plateaus suggests speaker verification model doesn't generalize; PESQ degrades while CER improves indicates over-optimization for intelligibility; training instability suggests reward variance too high

- First 3 experiments: 1) Ablate reward components (CER-only, SSIM-only, combined) on held-out language; 2) Vary fine-tuning data amount (15min, 30min, 1hr, 5hr) with and without GRPO; 3) Apply GRPO to baseline model (no fine-tuning) on language with zero paired data

## Open Questions the Paper Calls Out

### Open Question 1
Does the improvement in automatic metrics (CER, SSIM, PESQ) correlate with human perception of naturalness and speaker identity in low-resource languages? The evaluation relies exclusively on automated metrics without human subjective evaluations. Proxy metrics often fail to capture subtle prosodic errors or artifacts that human listeners find objectionable. A human listening study comparing fine-tuned baselines against GRPO-optimized models would validate perceptual gains.

### Open Question 2
Can this GRPO-based alignment framework be effectively applied to non-autoregressive or diffusion-based TTS architectures? The methodology is implemented exclusively on Koel-TTS, an autoregressive transformer model, while diffusion models are acknowledged as competing standards. GRPO relies on generating and ranking multiple discrete token sequences; it's unclear if reward normalization translates to continuous latent spaces. Applying the pipeline to diffusion-based TTS would measure performance deltas.

### Open Question 3
Does the use of a frozen ASR model as the primary reward signal introduce "reward hacking," where the TTS learns to generate artifacts that minimize ASR loss without improving true intelligibility? The framework uses Whisper Large V3 as the "judge" for intelligibility. Reinforcement learning agents often exploit such proxies (e.g., generating noise patterns the ASR ignores). The paper reports improved CER but doesn't analyze failure cases or specific phonetic distortions that might "fool" the reward model while degrading actual speech quality.

### Open Question 4
How sensitive is the model to the specific weighting of the multi-objective reward function ($w_{cer}, w_{ssim}, w_{pesq}$) across different target languages? The authors set fixed weights (0.45, 0.45, 0.1) for all experiments without ablation studies. It's unclear if this balance represents a robust global optimum or if speaker similarity was artificially suppressed for the specific test set. Ablation studies showing the trade-off frontier of CER vs. SSIM as weights vary would clarify sensitivity.

## Limitations

- The evaluation focuses on only three languages (Polish, Portuguese, Hindi) at limited data scales (30 minutes to 5 hours), leaving open questions about performance on typologically distinct languages
- The reliance on commercial ASR (Whisper Large V3) and speaker verification (Titanet) models without error analysis obscures whether improvements reflect true quality gains or reward hacking
- The absence of human perceptual studies means objective metrics may not correlate with actual listening experience, particularly for naturalness and speaker consistency

## Confidence

- **High Confidence**: The experimental methodology is sound, with clear ablation studies showing GRPO outperforms fine-tuning alone across all metrics
- **Medium Confidence**: The claimed superiority over DPO in high-resource languages is supported by results but lacks statistical significance testing
- **Low Confidence**: The assumption that IPA tokenization enables robust cross-lingual transfer is weakly supported, as the paper doesn't benchmark against grapheme-based tokenization

## Next Checks

1. **Reward Model Robustness Test**: Systematically evaluate Whisper ASR and Titanet speaker verification on held-out samples from target low-resource languages to quantify error rates, accent bias, and speaker coverage gaps. Compare objective improvements against human listening scores to assess metric reliability.

2. **IPA Tokenization Coverage Analysis**: For each target language, analyze the IPA conversion pipeline's phoneme coverage, particularly for non-Latin scripts. Benchmark TTS performance using grapheme tokenization as an alternative to IPA to isolate the contribution of phonetic representation to cross-lingual transfer.

3. **DPO Hyperparameter Sweep**: Conduct an exhaustive comparison between GRPO and DPO across English and Spanish, varying DPO's KL penalty coefficient, reward scaling, and sampling temperature. Determine whether GRPO's advantage persists under optimal DPO configurations or reflects implementation differences.