---
ver: rpa2
title: 'Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages:
  A Comparison of Reasoning and Non-Reasoning Architectures'
arxiv_id: '2512.16287'
source_url: https://arxiv.org/abs/2512.16287
tags:
- translation
- language
- languages
- reasoning
- uralic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates OpenAI GPT models for translating between
  Finnish and four endangered Uralic languages (Komi-Zyrian, Moksha, Erzya, Udmurt).
  The research systematically compares reasoning versus non-reasoning model architectures
  using refusal rate analysis on a parallel corpus of literary texts.
---

# Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures

## Quick Facts
- arXiv ID: 2512.16287
- Source URL: https://arxiv.org/abs/2512.16287
- Reference count: 6
- Primary result: Reasoning models show 16 percentage points lower refusal rates than non-reasoning models for Uralic language translation

## Executive Summary
This study evaluates OpenAI GPT models for translating between Finnish and four endangered Uralic languages (Komi-Zyrian, Moksha, Erzya, Udmurt). The research systematically compares reasoning versus non-reasoning model architectures using refusal rate analysis on a parallel corpus of literary texts. Results show reasoning models exhibit 16 percentage points lower refusal rates than non-reasoning models, with the o4-mini-2025-04-16 model achieving the best performance at 8.3% refusal rate. Moksha presented the greatest challenge with 63.6% refusal rate, while Erzya showed the lowest at 27.3%. The findings demonstrate that reasoning models are significantly more willing to attempt translation of morphologically complex languages, providing valuable insights for endangered language preservation efforts and highlighting the potential of reasoning architectures for low-resource language tasks.

## Method Summary
The study uses a parallel corpus of literary texts from Finnish to four Uralic languages (Komi-Zyrian, Moksha, Erzya, Udmurt). Five sentences per target language (20 total) are translated using five different GPT models: GPT-4, GPT-4o, GPT-4o-mini (non-reasoning) via Chat Completions API, and o3-2025-04-16, o4-mini-2025-04-16 (reasoning) via Responses API. Prompts use direct translation format with temperature=0.1 for non-reasoning models. Responses are categorized into four refusal patterns: Direct Refusal, Short Response, Attempted Translation, and Deflection. Refusal rates are computed per model and per language.

## Key Results
- Reasoning models show 16 percentage points lower refusal rates than non-reasoning models (o4-mini: 8.3% vs. non-reasoning: 40-50%)
- o4-mini-2025-04-16 achieved the best performance with 0% refusal for Komi-Zyrian and Udmurt
- Moksha showed highest refusal rate at 63.6%, Erzya lowest at 27.3%
- Morphological complexity correlates with refusal rates, with Moksha's agglutinative structure requiring more sophisticated processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning architectures reduce refusal rates for low-resource language translation tasks compared to direct-generation models.
- Mechanism: Reasoning models (o3, o4-mini) employ internal deliberation before output generation, which appears to increase confidence in attempting unfamiliar linguistic tasks. The paper reports a 16 percentage point reduction in refusal rates for reasoning models versus non-reasoning models.
- Core assumption: The internal reasoning process allows models to decompose morphologically complex translation challenges rather than defaulting to refusal when training data is sparse.
- Evidence anchors:
  - [abstract] "reasoning models exhibit 16 percentage points lower refusal rates than non-reasoning models"
  - [section 5.3, Table 3] o4-mini-2025-04-16 achieved 8.3% refusal rate; non-reasoning models ranged 40-50%
  - [corpus] Weak direct evidence—neighboring papers discuss low-resource evaluation but not reasoning-specific refusal mechanisms
- Break condition: If refusal behavior is driven primarily by safety filters rather than confidence/competence, the reasoning advantage may not generalize to other task types.

### Mechanism 2
- Claim: Morphological complexity correlates with translation refusal rates across model architectures.
- Mechanism: Uralic languages feature agglutinative morphology with extensive case systems. Moksha, which demonstrated the highest refusal rate (63.6%), has "rich verbal inflection patterns" and complex agglutinative structures that require sophisticated morphological processing.
- Core assumption: Models refuse when they cannot confidently decompose or generate morphological variants unseen in training data.
- Evidence anchors:
  - [section 5.2, Table 2] Language-specific refusal rates: Moksha 63.6%, Erzya 27.3%
  - [section 6.2] "This variation correlates with morphological complexity, as Moksha's rich agglutinative structure requires more sophisticated linguistic processing"
  - [corpus] No direct corpus evidence on Uralic morphology-refusal correlation; neighboring papers address different language families
- Break condition: If refusal rates are instead driven by training data volume rather than morphological complexity per se, the mechanism would predict performance by corpus size, not structural features.

### Mechanism 3
- Claim: Model size effects are secondary to architecture type for low-resource translation willingness.
- Mechanism: The smaller o4-mini reasoning model (8.3% refusal) outperformed the larger o3 reasoning model (50% refusal), suggesting reasoning capability is more determinative than parameter count for this task class.
- Core assumption: Reasoning processes compensate for reduced model capacity when facing unfamiliar linguistic patterns.
- Evidence anchors:
  - [section 5.1, Table 1] o4-mini achieved 0% refusal for Komi-Zyrian and Udmurt; o3 showed 33.3% and 33.3% respectively
  - [section 6.1] "reasoning models demonstrate superior willingness to attempt Uralic language translation"
  - [corpus] Related paper "Reasoning Transfer for an Extremely Low-Resource and Endangered Language" suggests reasoning transfer mechanisms may support low-resource scenarios, but direct evidence is limited
- Break condition: If o4-mini's advantage stems from different training data exposure rather than reasoning efficiency, the size-architecture tradeoff would not replicate.

## Foundational Learning

- Concept: Agglutinative morphology
  - Why needed here: Uralic languages build words by stringing together morphemes (case markers, possessive suffixes, verbal inflections). Models must decompose or generate these sequences to translate successfully.
  - Quick check question: Can you explain why "rich verbal inflection patterns" (Moksha) might trigger refusal in models trained primarily on analytic languages?

- Concept: Refusal rate analysis
  - Why needed here: The paper measures model "willingness" rather than translation quality. This is distinct from BLEU/METEOR and captures a different failure mode.
  - Quick check question: What is the difference between a "direct refusal" and an "attempted translation" in the paper's taxonomy?

- Concept: Reasoning vs. non-reasoning architectures
  - Why needed here: Reasoning models (o3, o4-mini) use internal deliberation before output; non-reasoning models generate directly. This architectural distinction produces measurably different refusal behavior.
  - Quick check question: Which API endpoints do the authors use for reasoning vs. non-reasoning models, and why might this matter for reproducibility?

## Architecture Onboarding

- Component map:
  - Source: Finnish literary sentences (5 per target language)
  - Models: Non-reasoning (GPT-4o, GPT-4o-mini, GPT-4) via Chat Completions API; Reasoning (o3, o4-mini) via Responses API
  - Prompt: Direct translation format ("Translate the following Finnish text to [target language]: [sentence]")
  - Evaluation: Refusal classification into four categories (direct refusal, short response, attempted translation, deflection)

- Critical path:
  1. Preprocess parallel corpus (sentence alignment, UTF-8 encoding, 100-token limit)
  2. Submit identical prompts across all 5 models for each language
  3. Classify responses by refusal pattern
  4. Aggregate refusal rates by language and by architecture type

- Design tradeoffs:
  - Temperature: Non-reasoning models set to 0.1 for determinism; reasoning models use default settings (API constraint)
  - Sample size: 5 sentences × 4 languages × 5 models = 100 translation attempts—small but systematic
  - Metric choice: Refusal rate measures willingness, not translation quality (authors note this limitation)

- Failure signatures:
  - High refusal rate for specific language (e.g., Moksha at 63.6%) indicates morphological complexity exceeds model confidence
  - Non-reasoning model variability (GPT-4: 0-20% refusal; GPT-4o-mini: 20-80% refusal) suggests inconsistent handling of low-resource targets
  - Degenerate repetition in outputs (see Appendix A, Example C) indicates instability even when not refusing

- First 3 experiments:
  1. Replicate the refusal rate comparison using the same prompt format on a different low-resource language family (e.g., Formosan languages from corpus neighbor "FormosanBench") to test generalization.
  2. Add few-shot examples (2-3 translation pairs) to the prompt and measure refusal rate reduction—if reasoning models already benefit from internal deliberation, few-shot may provide diminishing returns.
  3. Evaluate translation quality (BLEU, chrF) for attempted translations to determine whether lower refusal rates produce usable output or confident but incorrect translations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reasoning and non-reasoning models compare on actual translation quality metrics (e.g., BLEU, METEOR, chrF) rather than just refusal rates?
- Basis in paper: [explicit] The authors state in Section 6.3: "the focus on refusal rates rather than translation quality metrics limits our understanding of actual translation performance when models do attempt translation" and in Section 6.4 call for "development of specialized evaluation metrics for reasoning model translation quality."
- Why unresolved: The study design intentionally measured only willingness to attempt translation (refusal rate), not semantic accuracy, morphological correctness, or fluency of outputs.
- What evidence would resolve it: Human evaluation of translation quality and/or automatic metric scores (BLEU, METEOR, chrF++) on the attempted translations across all model-language pairs.

### Open Question 2
- Question: Does few-shot prompting reduce refusal rates and improve translation quality for Uralic languages, and does this benefit reasoning models differentially?
- Basis in paper: [explicit] Section 6.4 states: "Few-shot learning approaches comparing reasoning versus non-reasoning architectures could reveal optimal strategies for adapting models to new language families."
- Why unresolved: The study used only zero-shot direct translation prompts without providing in-context examples.
- What evidence would resolve it: A controlled experiment comparing zero-shot vs. few-shot (1-shot, 3-shot, 5-shot) prompting across reasoning and non-reasoning architectures on the same Uralic languages.

### Open Question 3
- Question: What causal factors explain the substantial variation in refusal rates across Uralic languages (Moksha 63.6% vs. Erzya 27.3%)?
- Basis in paper: [inferred] The authors note that variation "correlates with morphological complexity and available training data" (Section 5.2) but do not investigate which factor is causal or whether other factors (e.g., tokenization, script, web presence) contribute.
- Why unresolved: The study is correlational and does not isolate the contribution of morphology, corpus size, or training data representation.
- What evidence would resolve it: Regression analysis controlling for morphological complexity metrics, training corpus size estimates, and tokenization efficiency; or controlled synthetic experiments manipulating these factors.

### Open Question 4
- Question: Does incorporating linguistic knowledge (morphological analyzers, grammatical rules) into reasoning model prompts improve translation quality for agglutinative Uralic languages?
- Basis in paper: [explicit] Section 6.4 proposes: "Integration of linguistic knowledge into reasoning model prompts may enhance their ability to handle complex morphological structures."
- Why unresolved: The study employed simple direct prompts without linguistic annotation or morphology-aware prompting.
- What evidence would resolve it: Experiments comparing baseline prompts against linguistically-augmented prompts (e.g., with morpheme boundaries, case information, or glosses) for the same translation pairs.

## Limitations
- Small sample size (5 sentences per language) limits generalizability to full-document translation
- Exclusive focus on refusal rates rather than translation quality metrics
- Use of different API endpoints for reasoning vs. non-reasoning models introduces potential confounding variables

## Confidence
- High confidence: The 16 percentage point reasoning advantage is well-supported by experimental data
- Medium confidence: Morphological complexity-refusal correlation lacks direct corpus evidence for Uralic languages
- Low confidence: o4-mini performance advantage may stem from training data differences rather than reasoning efficiency

## Next Checks
1. Replicate the refusal rate experiment using 5 Formosan language sentences to test generalization to other agglutinative families
2. Compute BLEU/chrF scores for attempted translations to verify usable output quality, especially for Moksha
3. Annotate test sentences with Uralic morphological features and correlate feature density with refusal rates to test the morphological complexity hypothesis directly