---
ver: rpa2
title: Text Embedded Swin-UMamba for DeepLesion Segmentation
arxiv_id: '2508.06453'
source_url: https://arxiv.org/abs/2508.06453
tags:
- text
- segmentation
- lesion
- dataset
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates integrating text descriptions from radiology
  reports into the Swin-UMamba architecture for lesion segmentation on CT scans. The
  authors developed Text-Swin-UMamba, which incorporates short-form report text through
  a Text Tower encoder into the Swin-UMamba segmentation backbone.
---

# Text Embedded Swin-UMamba for DeepLesion Segmentation

## Quick Facts
- arXiv ID: 2508.06453
- Source URL: https://arxiv.org/abs/2508.06453
- Reference count: 0
- Dice score of 82.64% and Hausdorff distance of 6.34 pixels achieved on DeepLesion segmentation

## Executive Summary
This study investigates integrating text descriptions from radiology reports into the Swin-UMamba architecture for lesion segmentation on CT scans. The authors developed Text-Swin-UMamba, which incorporates short-form report text through a Text Tower encoder into the Swin-UMamba segmentation backbone. Using the ULS23 DeepLesion dataset, their approach achieved a Dice score of 82.64% and Hausdorff distance of 6.34 pixels. The model outperformed prior methods, showing 37.79% improvement over LLM-driven LanGuideMedSeg and 2.58% over xLSTM-UNet. The results demonstrate that incorporating text-based semantic features can enhance lesion segmentation performance, with text providing complementary clinical context beyond imaging features alone.

## Method Summary
The Text-Swin-UMamba architecture combines a Swin-UMamba backbone with a Text Tower encoder for multimodal lesion segmentation. The Text Tower processes short-form radiology report sentences using a BioLord pretrained model, tokenizer, and max pooling to produce 768-dimensional embeddings. These text features are fused at five decoder stages of the Swin-UMamba backbone through linear projection layers. The model is trained on the ULS23 DeepLesion dataset with Dice and cross-entropy loss, using AdamW optimizer with cosine annealing scheduler for 1500 epochs. The approach leverages text descriptions to provide clinical context that supplements visual features, particularly helpful for lesions with ambiguous boundaries or when surrounding anatomy creates visual confusion.

## Key Results
- Achieved Dice score of 82.64% and Hausdorff distance of 6.34 pixels on DeepLesion segmentation
- Outperformed LLM-driven LanGuideMedSeg by 37.79% and xLSTM-UNet by 2.58%
- Demonstrated that text-based semantic features provide complementary clinical context beyond imaging features alone

## Why This Works (Mechanism)

### Mechanism 1: Text-Guided Semantic Disambiguation
Short-form radiology text provides complementary clinical context that reduces visual ambiguity in lesion boundaries and anatomical localization. The Text Tower encodes semantic attributes from reports into a 768-dim vector, which is projected and fused at each decoder stage. This global context acts as a soft prior on what/where to segment, supplementing local visual features that may have low contrast or ambiguous boundaries.

### Mechanism 2: Multi-Scale Language-Image Fusion in Decoder
Injecting text embeddings at multiple decoder stages allows language features to modulate segmentation predictions across spatial resolutions. After the Text Tower produces a pooled embedding, dedicated linear projection layers match channel dimensions at five decoder stages. The fused features guide the Swin-UMamba backbone with text-based semantic information during mask generation.

### Mechanism 3: Max Pooling Preserves Strongest Semantic Activations
Max pooling over token embeddings selects the most salient semantic feature per dimension, providing a robust fixed-size representation for short clinical sentences. After BioLord contextualizes tokens, max pooling takes the maximum activation across all tokens for each hidden dimension. This retains the "strongest feature value" rather than averaging or weighting.

## Foundational Learning

- **Mamba / State Space Models in Vision**: Why needed here - Swin-UMamba replaces pure attention with selective state space modeling for efficient long-range dependencies in medical images. Quick check question: Can you explain how Mamba's selective scan differs from standard self-attention in terms of computational complexity?

- **nnU-Net Self-Configuration Framework**: Why needed here - Swin-UMamba is derived from nnU-Net's automated configuration (preprocessing, architecture, training). Understanding this baseline clarifies what the text tower adds. Quick check question: What three configurations does nnU-Net automatically generate based on dataset fingerprinting?

- **Multimodal Fusion in Medical Imaging**: Why needed here - The paper fuses text embeddings into image decoder features. Understanding where and how to fuse (early, mid, late; additive vs. concatenative) is essential for reproducing results. Quick check question: Why might late fusion (at decoder stages) be preferred over early fusion (input-level) for segmentation tasks?

## Architecture Onboarding

- **Component map**: Image Encoder (Swin-UMamba) -> Image Decoder (Swin-UMamba) -> Text Tower (Tokenizer → BioLord encoder → Max pooling → 768-dim embedding) -> Lang Fusion Layers (Linear projections matching text embedding to decoder channel dimensions at each stage)

- **Critical path**: 1) Preprocess CT slice (512×512 → 256×256 centered on lesion) 2) Extract short-form report sentence 3) Tokenize text and pass through BioLord → max pool → 768-dim vector 4) Project text vector at each decoder stage; add/concatenate to image features 5) Decoder outputs segmentation mask; compute Dice + CE loss

- **Design tradeoffs**: Short vs. long-form text: Short reports simplify pooling but limit semantic richness; long-form requires more complex encoding (planned Phase 2). Decoder-only vs. full injection: Decoder-only is simpler; full injection adds <0.2% gain—likely not worth complexity for short texts. 2D vs. 3D: Current 2D design limits clinical realism; 3D extension non-trivial for Mamba architectures.

- **Failure signatures**: Dice <70% on validation: Check text-image alignment; ensure reports correspond to correct slices. Text tower outputs near-zero variance: Verify BioLord weights loaded correctly; check tokenizer on medical vocabulary. Marginal improvement over baseline (<1%): Expected for centered/cropped lesions—text cues less discriminative when localization is trivialized.

- **First 3 experiments**: 1) Baseline reproduction: Train Swin-UMamba without text on ULS23; verify Dice ~81.5% matches paper baseline. 2) Ablation on pooling: Compare max vs. mean vs. weighted-average pooling on validation set; expect <0.5% difference. 3) Decoder-only vs. full injection: Test both fusion strategies; if gain <0.2%, prefer simpler decoder-only approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of Text-Swin-UMamba degrade when applied to full-size CT volumes where lesions are not centered or pre-cropped?
- Basis in paper: [explicit] The authors state the experimental design relies on the ULS23 dataset where "lesions are centered on 256×256 images," which constrains generalizability to realistic scenarios where lesions are small relative to the anatomy.
- Why unresolved: The model currently benefits from a simplified localization task, and it is unknown if text embeddings are sufficient to locate lesions in complex, uncropped full-field views.
- What evidence would resolve it: Benchmarking the model on the original 512x512 DeepLesion slices or external datasets without center-cropping.

### Open Question 2
- Question: Can extending the architecture to 3D volumetric data significantly improve results given that radiologic descriptors are inherently 3D concepts?
- Basis in paper: [explicit] The authors note the approach is currently designed only for 2D images, while descriptors like "irregular margins" are 3D concepts, and extension to 3D "has not been attempted in this pilot work."
- Why unresolved: 2D slices lack depth context, potentially limiting the model's ability to fully utilize text descriptions of volumetric shape and texture.
- What evidence would resolve it: Implementing a 3D Text-Swin-UMamba and comparing segmentation accuracy against the 2D baseline on volumetric datasets.

### Open Question 3
- Question: Does integrating long-form radiology reports and a text decoder for report generation yield reciprocal improvements in segmentation accuracy?
- Basis in paper: [explicit] The authors identify the lack of a text decoder and long-form analysis as a limitation, stating future work will focus on "encoding long-form radiology reports" and enabling "full report generation."
- Why unresolved: It is unclear if the mutual reinforcement between generating reports and generating masks (multi-task learning) offers substantial gains over the current input-embedding approach.
- What evidence would resolve it: Evaluating a unified architecture that performs both segmentation and report generation to see if Dice scores improve compared to the segmentation-only model.

## Limitations

- The study relies on short-form radiology descriptions (single sentences per slice) rather than comprehensive reports, potentially limiting the clinical context available to the model
- The approach is currently designed only for 2D images, while radiologic descriptors like "irregular margins" are inherently 3D concepts
- The model benefits from pre-centered/cropped lesions, raising questions about performance on full-size CT volumes where lesions are not localized

## Confidence

- **High Confidence**: The experimental methodology and metric calculations appear sound. The Dice score improvement over LLM-driven LanGuideMedSeg (37.79%) and xLSTM-UNet (2.58%) is well-documented and reproducible given access to the dataset and code.
- **Medium Confidence**: The claim that text provides "complementary clinical context beyond imaging features alone" is supported by the observed performance gains, but the magnitude of benefit (~1% Dice gain) suggests this advantage may be dataset-dependent and less pronounced when lesions are already well-centered and cropped.
- **Low Confidence**: The assertion that max pooling is superior to mean pooling for short clinical sentences lacks strong empirical support, with only a 0.30% Dice difference observed. This suggests the pooling mechanism may not be critical for short-form text processing.

## Next Checks

1. **Dataset Dependency Analysis**: Evaluate Text-Swin-UMamba performance on a dataset where lesions are not pre-centered/cropped, to determine if text guidance provides greater benefit when visual localization is more challenging.

2. **Long-Form Text Extension**: Implement and test the planned Phase 2 extension using comprehensive radiology reports to assess whether richer semantic context yields substantially improved segmentation performance beyond the marginal gains observed with short-form text.

3. **Computational Efficiency Assessment**: Compare the computational overhead of the Text Tower fusion approach against simpler alternatives (e.g., decoder-only injection) to determine if the marginal performance gains justify the added architectural complexity and training time.