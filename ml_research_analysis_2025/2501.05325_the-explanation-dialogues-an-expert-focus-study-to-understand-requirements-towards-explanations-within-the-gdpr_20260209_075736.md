---
ver: rpa2
title: 'The explanation dialogues: an expert focus study to understand requirements
  towards explanations within the GDPR'
arxiv_id: '2501.05325'
source_url: https://arxiv.org/abs/2501.05325
tags:
- explanations
- explanation
- data
- information
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how legal experts perceive and reason about
  explainable AI (XAI) methods for automated decision-making, focusing on compliance
  with GDPR explainability requirements. Through a combination of an online questionnaire
  and follow-up interviews with legal scholars, the research evaluates four XAI methods
  (global/local SHAP, DICE, LORE) using a credit approval use case.
---

# The explanation dialogues: an expert focus study to understand requirements towards explanations within the GDPR

## Quick Facts
- arXiv ID: 2501.05325
- Source URL: https://arxiv.org/abs/2501.05325
- Reference count: 40
- Legal experts found XAI methods insufficient for GDPR compliance, with a preference for hybrid global-local approaches with clear textual explanations.

## Executive Summary
This study evaluates how legal experts perceive and reason about explainable AI (XAI) methods for automated decision-making in the context of GDPR Article 22. Through an online questionnaire and follow-up interviews with 12 legal scholars, the research tested four XAI methods (SHAP, DICE, LORE) using a credit approval scenario. The findings reveal that experts found the explanations hard to understand, incomplete in information, and insufficient for exercising data subject rights. While no single method fully satisfied GDPR requirements, experts preferred a combination of global and local methods with clear textual explanations. The study identifies key areas for improvement in presentation format, accessibility, content depth, and technical risk management.

## Method Summary
The study employed a mixed-method approach combining an online questionnaire with follow-up interviews. Legal experts evaluated four XAI methods (Global/Local SHAP, DICE, LORE) using a credit approval use case based on the South German Credit Dataset. A Random Forest classifier was trained to achieve 0.815 accuracy, and explanations were generated for specific test instances. Data preparation included converting currency DM to EUR and modernizing features. The evaluation focused on GDPR compliance requirements, particularly the suitability for exercising data subject rights and the completeness of information provided.

## Key Results
- Legal experts found XAI explanations generally hard to understand and cognitively misleading for average consumers
- Explanations were deemed insufficient for exercising data subject rights and contesting automated decisions
- A combination of global and local methods with clear textual explanations was preferred over single-method approaches
- Experts highlighted concerns about intellectual property disclosure and the need for actionable counterfactuals

## Why This Works (Mechanism)

### Mechanism 1: Completeness via Methodological Hybridization
Global explanations provide systemic context while local explanations offer specific case details. Legal reasoning requires both the general "rules of the game" and specific "moves" to assess fairness. Experts found global methods lacking individual case reference while local methods missed broader logic verification. The combination addresses this gap but risks information overload affecting understandability.

### Mechanism 2: The Narrative Layer as a Translation Bridge
Technical artifacts alone are insufficient for legal validity. Experts flagged plots and rules as hard to understand for laypeople, requiring transformation of probabilistic outputs into causal justifications. The gap lies in converting technical output into "plain language narratives" answering "Why?" rather than just "How?" This risks introducing unsupported inferences if not carefully managed.

### Mechanism 3: Actionability as the Compliance Proxy
Legal experts evaluate explanations based on their ability to facilitate user contestation rather than technical fidelity. Contrastive explanations scored higher on actionability by showing what to change, but were critiqued if changes were unrealistic or discriminatory. The goal of GDPR Article 22 is contestability, making non-actionable explanations fail compliance tests.

## Foundational Learning

- **GDPR Article 22 (Automated Decision-Making)**: The legal benchmark defining "right to contest" and "right to explanation." Quick check: Does the explanation allow the data subject to "express their point of view" and "contest the decision"?
- **Global vs. Local Explainability**: Global explains model behavior generally (e.g., "Savings is usually most important") while Local explains specific predictions (e.g., "Your savings caused rejection"). Quick check: Does a Global SHAP plot explain why a specific individual was rejected, or how the model generally behaves?
- **Grounded Theory**: The qualitative method used to derive codes like "Suitability for exercising rights" from expert interviews. Quick check: Were the codes pre-defined or did they emerge from the interviews?

## Architecture Onboarding

- **Component map:** ADM Core (Random Forest) → XAI Layer (Global SHAP, Local SHAP, DICE, LORE) → Translation Layer (missing narrative bridge) → User Interface
- **Critical path:** 1) Generate Local Explanation (e.g., "Feature X contributed +0.4 to risk") → 2) Crucial Step: Translate to Actionable Narrative (e.g., "Your low account balance was main reason...") → 3) Verify Actionability (Is suggestion feasible and legal?)
- **Design tradeoffs:** Completeness vs. Understandability, Intellectual Property vs. Transparency, Accuracy vs. Actionability
- **Failure signatures:** "Black Box" Denial (technically correct but legally unusable), Non-actionable Recourse (suggesting impossible changes), "Happy Path" Bias (assuming developer understanding equals consumer understanding)
- **First 3 experiments:**
  1. Baseline Compliance Test: Present raw SHAP/DICE outputs to legal reviewer and measure "perceived contestability"
  2. Narrative Ablation Study: Compare [Plot Only] vs. [Plot + Text Summary] vs. [Text Summary Only] on "Suitability for exercising rights"
  3. Actionability Verification: Generate counterfactuals and manually validate if they are legally distinct from discriminatory suggestions

## Open Questions the Paper Calls Out

### Open Question 1
To what extent can XAI methods be manipulated by data controllers to superficially satisfy GDPR transparency requirements while obscuring the true logic of automated decisions? The study deliberately assumed truthful explanations to isolate understandability issues, leaving adversarial use unexplored. Evidence would require user studies detecting manipulation in adversarial settings.

### Open Question 2
Does a systematically designed combination of global and local explanation methods with natural language text significantly improve legal experts' assessment of GDPR compliance compared to single methods? The study evaluated methods largely in isolation without testing standardized composite frameworks. Evidence would require controlled experiments with combined interfaces.

### Open Question 3
How do the explainability requirements of the GDPR align with or contradict the specific right to explanation in the European AI Act? The study was designed before the AI Act was finalized and restricted its premise to GDPR Article 22. Evidence would require comparative legal analysis against both GDPR Article 22 and AI Act Article 86.

## Limitations
- Based on expert perceptions rather than empirical testing with actual data subjects
- Evaluation criteria derived from expert interviews rather than formal legal standards
- Sample size limited to 12 experts focused on German legal scholars

## Confidence

- **High Confidence:** Experts found explanations hard to understand and incomplete (well-supported by direct quotes)
- **Medium Confidence:** Hybrid global-local approaches recommended (strongly supported by expert consensus)
- **Medium Confidence:** Explanations insufficient for exercising data subject rights (supported by expert feedback but lacks empirical validation)

## Next Checks

1. Conduct A/B testing with actual data subjects comparing technical explanations versus narrative-based explanations to measure real-world understandability and actionability.

2. Test the proposed hybrid approach (global + local + textual) against GDPR requirements using formal legal review rather than expert perception surveys.

3. Evaluate the risk of "non-actionable" counterfactuals by having legal experts classify generated recommendations as discriminatory, impossible, or genuinely actionable.