---
ver: rpa2
title: 'TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for
  Forecasting with Covariates'
arxiv_id: '2509.13906'
source_url: https://arxiv.org/abs/2509.13906
tags:
- foundation
- forecasting
- covariates
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TFMAdapter introduces a lightweight, instance-level adapter that
  enhances foundation models for time series forecasting with covariates without fine-tuning.
  The key challenge is integrating covariate information while preserving the foundation
  model's generalization capabilities, using only the limited historical data available
  during a single inference call.
---

# TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates

## Quick Facts
- arXiv ID: 2509.13906
- Source URL: https://arxiv.org/abs/2509.13906
- Authors: Afrin Dange; Sunita Sarawagi
- Reference count: 37
- 24-27% improvement over foundation models alone with minimal computational overhead

## Executive Summary
TFMAdapter introduces a lightweight, instance-level adapter that enhances foundation models for time series forecasting with covariates without fine-tuning. The key challenge is integrating covariate information while preserving the foundation model's generalization capabilities, using only the limited historical data available during a single inference call. TFMAdapter addresses this through a two-stage approach: first generating pseudo-forecasts using a regression model trained on representative windows of the historical data, then refining these forecasts with a Gaussian Process adapter that incorporates covariates and auxiliary features. This approach enables effective utilization of the full historical context while limiting expensive foundation model calls.

## Method Summary
TFMAdapter works by first generating pseudo-forecasts for the entire historical window using a simple regression model (Bayesian Ridge) trained on representative subsets of the history where foundation model outputs are available. These pseudo-forecasts, combined with covariates and auxiliary features, are then used to train a Gaussian Process adapter that learns to refine the foundation model's predictions. The adapter operates at the instance level, meaning it only uses the single time series being forecasted without access to external datasets. During inference, an uncertainty-based filtering mechanism falls back to the raw foundation model prediction when the GP posterior variance exceeds a threshold.

## Key Results
- Achieves 24-27% improvement over base foundation models alone
- Outperforms supervised baselines while maintaining computational efficiency
- Particularly excels at leveraging covariate information that most foundation models struggle to incorporate
- Maintains effectiveness across diverse datasets with varying history lengths (288-2016 steps)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-forecasts enable training adapters on full history with minimal foundation model calls
- Mechanism: A lightweight regression model (Bayesian Ridge) learns to approximate foundation model behavior from just 3 representative windows, then generates proxy forecasts for all historical time steps, providing sufficient training data for the adapter without expensive repeated TSFM invocations
- Core assumption: The foundation model's forecasting behavior can be locally approximated by a simple regression model using lag features and positional encodings
- Evidence anchors:
  - [abstract] "generating pseudo-forecasts with a simple regression model... enables training on the full historical context while limiting TSFM invocations"
  - [Section 4.1] Describes partitioning history into windows, selecting K=3 representative windows by z-score, training Bayesian Ridge on TSFM outputs
  - [corpus] No direct corpus validation; related work (CoRA, UniCA) addresses covariate adaptation but not pseudo-forecast approximation
- Break condition: If foundation model behavior is highly non-linear or context-dependent in ways lag features cannot capture, pseudo-forecasts diverge from true TSFM outputs, degrading adapter training

### Mechanism 2
- Claim: Gaussian Process adapter learns to correct foundation model outputs using covariates
- Mechanism: GP with composite kernel (k₁ for pseudo-forecasts + lag + position, k₂ for covariates) learns a non-parametric mapping from (forecast, covariate) pairs to true values via closed-form posterior, with uncertainty-based filtering to fall back to raw TSFM predictions when variance exceeds threshold
- Core assumption: Covariates provide orthogonal information to TSFM forecasts that can be combined additively or multiplicatively in kernel space
- Evidence anchors:
  - [abstract] "training a Gaussian Process regressor to refine predictions using both pseudo- and TSFM forecasts alongside covariates"
  - [Section 4.2] Defines GP mapping y_t = g(ỹ_t, x_t, f_t) + ε with composite kernel, posterior predictive distribution
  - [corpus] CoRA paper addresses covariate adaptation but uses different architectural approach; no direct validation of GP-specific mechanism
- Break condition: If covariates are uninformative or already captured by TSFM's inductive bias, GP adds noise without benefit; if covariate-TSF relationship is highly discontinuous, kernel-based smoothing introduces bias

### Mechanism 3
- Claim: Instance-level adaptation preserves foundation model generalization while adding domain-specific covariate handling
- Mechanism: By training adapter parameters only on the single time series instance being forecasted (using only H historical points available at inference), the method avoids overfitting to specific domains while still learning the local covariate-target relationship
- Core assumption: Sufficient signal exists within a single instance's history to learn meaningful covariate corrections
- Evidence anchors:
  - [abstract] "instance-level adapter that augments TSFMs with covariate information without fine-tuning"
  - [Section 3.2] Defines instance-level constraint: adapter depends only on history provided during one invocation
  - [corpus] Multiple papers (CoRA, UniCA, STAMP) explore TSFM adaptation but primarily via dataset-level fine-tuning, not instance-level learning
- Break condition: If history H is too short relative to forecast horizon F or covariate dimensionality, adapter overfits to noise; if time series has structural breaks, historical patterns don't generalize to forecast horizon

## Foundational Learning

- Concept: **Foundation Model Inductive Bias**
  - Why needed here: TFMAdapter relies on TSFM already capturing temporal patterns; adapter only corrects for missing covariate integration
  - Quick check question: Does the base TSFM already outperform simple baselines on univariate forecasting for your data?

- Concept: **Gaussian Process Posterior**
  - Why needed here: The adapter uses GP's closed-form posterior for both prediction and uncertainty quantification; understanding kernel choice and hyperparameter effects is critical
  - Quick check question: Can you explain how the composite kernel separates pseudo-forecast influence from covariate influence?

- Concept: **Instance-Level vs Dataset-Level Adaptation**
  - Why needed here: This constraint defines what data adapter can access; misunderstanding leads to data leakage or insufficient training signal
  - Quick check question: If you have 100 similar time series, should you train one adapter per series or pool data across them?

## Architecture Onboarding

- Component map:
  - Input: History Y₁:H (H timesteps), Covariates X₁:H+F (H+F timesteps)
  - Window Selector: Partitions Yh:H into N windows, selects K=3 by z-score variance
  - Foundation Model M: Called K times on selected windows → labeled data
  - Pseudo-forecast Generator G: Bayesian Ridge trained on (lag features, position encodings, true values) → targets are TSFM outputs
  - Adapter A: GP with composite kernel, trained on (pseudo-forecasts, covariates, auxiliary features) → targets are true historical values
  - Uncertainty Filter: Falls back to raw TSFM forecast when GP posterior variance exceeds threshold

- Critical path:
  1. Window selection (determines TSFM call efficiency vs diversity tradeoff)
  2. Pseudo-forecast quality (if G diverges from M, adapter learns wrong corrections)
  3. GP kernel selection and hyperparameter tuning (directly affects final accuracy)

- Design tradeoffs:
  - K (number of TSFM calls): More calls → better pseudo-forecast training data but higher latency; paper shows K=3 sufficient
  - Window selection strategy: Latest windows capture recent dynamics; z-score windows capture diversity; paper shows similar final performance
  - GP kernel choice: Matern/RBF for smooth corrections, Linear for additive shifts; paper uses validation to select
  - Uncertainty threshold: Aggressive filtering → more fallback to TSFM; conservative → more adapter influence

- Failure signatures:
  - Adapter underperforms base TSFM: Check if covariates are actually informative (correlation analysis); verify pseudo-forecasts approximate TSFM outputs
  - High variance in adapter predictions: GP kernel may be mis-specified; reduce complexity or increase regularization
  - No improvement over baselines: History H may be insufficient; TSFM may already capture covariate-relevant patterns

- First 3 experiments:
  1. **Covariate importance ablation**: Train adapter with only pseudo-forecasts (no covariates) vs full setup to isolate covariate contribution; expect degradation if covariates are valuable
  2. **Window selection sensitivity**: Compare Latest vs Random vs Z-score strategies on pseudo-forecast quality (MAE vs true TSFM outputs) and final forecast accuracy; validate K=3 sufficiency
  3. **Uncertainty threshold calibration**: Sweep threshold values on validation set; plot adapter usage rate vs accuracy to find operating point that balances TSFM fallback with adapter benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does improving the fidelity of pseudo-forecasts (via z-score window selection) fail to yield significant improvements in the final adapter forecasts?
- Basis in paper: [explicit] Section 6.2 notes that "producing pseudo-forecasts that more closely resemble the true data distribution does not necessarily improve the final forecasts," suggesting a disconnect between pseudo-forecast quality and downstream utility.
- Why unresolved: The authors hypothesize that pseudo-forecasts may diverge from the foundation model's specific predictive distribution, but they do not isolate the mechanism causing this lack of correlation.
- Evidence: An ablation study analyzing the distributional alignment (e.g., KL-divergence) between the pseudo-forecasts and the foundation model's actual outputs, rather than just their proximity to ground truth values.

### Open Question 2
- Question: How does the Gaussian Process adapter's computational overhead scale with extremely long historical contexts, given the cubic complexity of the kernel inversion?
- Basis in paper: [inferred] While the method avoids extensive TSFM calls, it relies on a GP adapter (Section 4.2). Experiments only test histories up to ~2000 steps (Table 4), leaving the scalability limits for massive contexts unexplored.
- Why unresolved: The paper emphasizes efficiency regarding TSFM invocations but does not profile the computational cost or numerical stability of fitting the GP on the full history as $H$ increases significantly.
- Evidence: Runtime and memory benchmarks on datasets with history lengths exceeding 10,000 steps, comparing the GP adapter against linear or sparse-GP alternatives.

### Open Question 3
- Question: Can the instance-level adaptation framework be extended to multivariate forecasting where dependencies exist between multiple target series?
- Basis in paper: [explicit] The conclusion states "Future work could explore new methods for adapter models," and the introduction highlights that few foundation models extend to multivariate forecasting. The current method is restricted to univariate targets with exogenous covariates.
- Why unresolved: Modeling cross-series correlations would require moving beyond the current univariate GP adapter to a multi-output regression setting, which increases parametric complexity and data requirements.
- Evidence: Extending TFMAdapter to handle multiple target variables simultaneously and evaluating its ability to capture inter-series dynamics compared to specialized multivariate baselines.

## Limitations
- Pseudo-forecast approximation quality may degrade for highly non-linear foundation model behaviors
- GP adapter effectiveness depends on covariates being genuinely informative relative to foundation model's existing capabilities
- Instance-level constraint limits learning from broader domain knowledge that dataset-level adaptation could capture

## Confidence
- **High Confidence**: The two-stage architecture design, window selection methodology, and overall experimental methodology are well-specified and reproducible
- **Medium Confidence**: The pseudo-forecast quality and its impact on adapter training - while theoretically sound, the assumption that simple regression can accurately approximate foundation model behavior across diverse time series warrants empirical validation
- **Medium Confidence**: The GP adapter's effectiveness in combining covariates with pseudo-forecasts - the composite kernel approach is principled, but the specific contribution of each component and the robustness across different covariate types requires further investigation

## Next Checks
1. **Pseudo-forecast fidelity assessment**: For each dataset, measure the MAE between pseudo-forecasts and actual TSFM outputs on the selected representative windows. Analyze whether degradation in pseudo-forecast quality correlates with adapter underperformance.

2. **Covariate importance ablation across domains**: Systematically remove covariates from the GP adapter on each dataset and measure the performance drop. This will reveal whether the claimed improvements stem from genuine covariate integration or other factors.

3. **Uncertainty filtering effectiveness analysis**: Track the percentage of forecasts where the adapter falls back to the base TSFM versus when it uses the GP prediction. Compare the distribution of GP posterior variances between successful and failed forecasts to validate the uncertainty-based filtering mechanism.