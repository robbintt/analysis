---
ver: rpa2
title: Tree Ensemble Explainability through the Hoeffding Functional Decomposition
  and TreeHFD Algorithm
arxiv_id: '2510.24815'
source_url: https://arxiv.org/abs/2510.24815
tags:
- tree
- treehfd
- decomposition
- have
- treeshap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TreeHFD, a new algorithm to estimate the
  Hoeffding functional decomposition (HFD) of tree ensembles from data samples. The
  HFD breaks down black-box models into transparent, orthogonal components, but its
  practical estimation has been an open problem.
---

# Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm

## Quick Facts
- **arXiv ID:** 2510.24815
- **Source URL:** https://arxiv.org/abs/2510.24815
- **Reference count:** 40
- **Primary result:** TreeHFD algorithm estimates Hoeffding Functional Decomposition (HFD) of tree ensembles from data, providing interpretable explanations that outperform existing methods like TreeSHAP.

## Executive Summary
This paper introduces TreeHFD, a novel algorithm to estimate the Hoeffding Functional Decomposition (HFD) of tree ensembles from data samples. The HFD breaks down black-box models into transparent, orthogonal components, but practical estimation has been an open problem. TreeHFD addresses this by leveraging the piecewise constant structure of tree ensembles and using a least-squares approach to approximate the decomposition. Theoretical analysis shows that TreeHFD is convergent and inherits desirable properties like sparsity, orthogonality, and causal variable selection. Experiments on both simulated and real data demonstrate that TreeHFD accurately recovers the HFD, outperforming existing methods like TreeSHAP.

## Method Summary
TreeHFD estimates the HFD by exploiting the piecewise constant structure of tree ensembles. It discretizes orthogonality constraints over Cartesian tree partitions and solves a constrained least-squares problem to satisfy hierarchical orthogonality while fitting the tree's predictions. The algorithm works by parsing each tree to collect split values, building Cartesian partitions for relevant variable subsets, constructing constraint matrices, and solving per-tree least squares problems. The solution is aggregated across trees to form the final HFD components, with empty cells imputed from neighboring nonempty cells.

## Key Results
- TreeHFD accurately recovers the HFD on analytical cases with low residual MSE (~0.02)
- TreeHFD achieves better orthogonality than TreeSHAP, with correlation between interaction components and main effects < 0.05
- TreeHFD demonstrates convergence properties as sample size increases, validating its theoretical consistency guarantees

## Why This Works (Mechanism)

### Mechanism 1: Piecewise Constant Approximation with Discretized Orthogonality
TreeHFD estimates the HFD from samples by exploiting tree structure through piecewise constant approximation. Tree ensembles partition feature space into cells; TreeHFD discretizes orthogonality constraints over Cartesian tree partitions and solves a constrained least-squares problem to satisfy hierarchical orthogonality while fitting predictions. This works under Assumption 1 (input density bounded away from zero and infinity), which ensures well-defined conditional expectations and invertibility in the least-squares setup.

### Mechanism 2: Hierarchical Orthogonality Enforces Sparsity and Pure Interactions
Hierarchical orthogonality constraints require that any component η^{(J)} is uncorrelated with all η^{(I)} for I⊂J. This enforces sparsity via the reluctance principle—if lower-order terms suffice, higher-order terms vanish. Under these constraints, the decomposition produces unique, interpretable components where interactions do not absorb main effects. This holds when the density f varies little within each partition cell; otherwise global orthogonality is only approximate for the ensemble.

### Mechanism 3: Sample-Based Consistency via Empirical Risk Minimization
TreeHFD estimates converge to the theoretical tree-HFD as sample size grows through empirical risk minimization. The empirical loss (sample averages of squared decomposition error plus discretized orthogonality penalties) converges to the population loss by the law of large numbers. Minimizing this loss over a compact parameter set yields consistent estimates via M-estimation theory, assuming the input density assumption holds and hyperparameters are set appropriately.

## Foundational Learning

**Concept: Hoeffding/ANOVA Functional Decomposition**
- Why needed here: Core object TreeHFD estimates; decomposes f(X)=∑_{J⊆V_p} f^{(J)}(X^{(J)}) with hierarchical orthogonality
- Quick check question: If inputs are independent, does the Möbius formula f^{(J)}(X^{(J)})=∑_{I⊆J}(−1)^{|J|−|I|}E[f(X)|X^{(I)}] hold?

**Concept: Hierarchical Orthogonality**
- Why needed here: Ensures interactions are "pure" (free of main effects) and decomposition is unique
- Quick check question: For p=2, if E[f(X)|X^{(1)}]=f^{(1)}(X^{(1)}), what does orthogonality imply about the interaction f^{(1,2)}?

**Concept: Tree Partitions and Piecewise Constant Representations**
- Why needed here: Trees partition the feature space; HFD components are constrained to be piecewise constant over Cartesian tree partitions
- Quick check question: Given a tree's splits on X^{(1)} at 0.3 and 0.7, what is the Cartesian partition for X^{(1)}?

## Architecture Onboarding

**Component map:** Trained tree ensemble → extract splits → build Cartesian partitions A^{(J)}_ℓ for relevant J ∈ P^{(ℓ)}_p → construct constraint matrix C_n and target Z_n from sample counts → solve least squares ||Z_n−C_nβ||²_2 → aggregate β across trees to form HFD components

**Critical path:**
1. Parse each tree to collect split values per variable and build Cartesian partitions
2. For each tree, enumerate variable subsets P^{(ℓ)}_p (respecting d_I) and set up quadratic constraints encoding orthogonality
3. Solve per-tree least squares; impute empty cells from largest neighboring nonempty cell; sum components across trees

**Design tradeoffs:** Higher interaction order d_I improves fidelity but increases computation; pruning depth d_T reduces cost for deep trees at potential accuracy loss; ignoring density variation within cells trades exact orthogonality for tractability

**Failure signatures:** Large residual MSE indicates insufficient d_I or overly aggressive pruning; high correlation between main effects and interactions signals broken orthogonality; empty cells without valid neighbors cause imputation errors

**First 3 experiments:**
1. Reproduce the analytical sinusoidal+interaction setting (n=5000, M=100 xgboost trees, d_I=2) and report per-component MSE vs. the analytical HFD
2. On a real dataset (e.g., California Housing), compare TreeHFD and TreeSHAP: compute max absolute correlation between interaction components and their associated main effects to quantify orthogonality
3. Vary sample size (100→10000) on the analytical case and plot cumulative MSE for TreeHFD to empirically validate convergence trends

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided, but several important questions emerge from the analysis and implementation details.

## Limitations
- Discretization of orthogonality constraints introduces approximation errors that grow with within-cell density variation, potentially weakening orthogonality guarantees
- Convergence analysis assumes independence between training data for the tree ensemble and the sample used for TreeHFD estimation, which is not explicitly verified
- Computational complexity scales exponentially with tree depth, limiting applicability to shallow or pruned trees in practice

## Confidence

**Mechanism 1 (Piecewise Constant Approximation):** Medium - The discretized least-squares formulation is clearly defined, but corpus lacks direct validation of this specific tree-HFD mechanism

**Mechanism 2 (Hierarchical Orthogonality):** Medium - Orthogonality is theoretically well-founded, but the tree-specific discretized constraints lack direct corpus support and depend critically on small ∆_{A,f}

**Mechanism 3 (Sample-Based Consistency):** Medium - The M-estimation consistency argument is standard, but empirical validation of convergence rates is absent from the corpus

## Next Checks
1. **Reproduce Analytical Case:** Generate the sinusoidal+interaction data (n=5000, p=6), train XGBoost (M=100), run TreeHFD (d_I=2), and report per-component MSE vs. analytical HFD values

2. **Real Data Orthogonality Test:** On California Housing, compare TreeHFD and TreeSHAP by computing the maximum absolute correlation between interaction components and their associated main effects to quantify orthogonality

3. **Convergence Validation:** Vary sample size (100→10000) on the analytical case and plot cumulative MSE for TreeHFD to empirically validate convergence trends