---
ver: rpa2
title: 'FaCTR: Factorized Channel-Temporal Representation Transformers for Efficient
  Time Series Forecasting'
arxiv_id: '2506.05597'
source_url: https://arxiv.org/abs/2506.05597
tags:
- forecasting
- time
- factr
- temporal
- channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FaCTR introduces a lightweight transformer for multivariate time
  series forecasting by disentangling temporal and cross-channel dependencies. It
  employs a Factorization Machine to model low-rank pairwise interactions between
  channels and a gating mechanism to fuse temporal and spatial signals.
---

# FaCTR: Factorized Channel-Temporal Representation Transformers for Efficient Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2506.05597
- **Source URL**: https://arxiv.org/abs/2506.05597
- **Reference count**: 40
- **Primary result**: 50× smaller than spatiotemporal transformers, state-of-the-art on 11 benchmarks

## Executive Summary
FaCTR introduces a lightweight transformer for multivariate time series forecasting by disentangling temporal and cross-channel dependencies. It employs a Factorization Machine to model low-rank pairwise interactions between channels and a gating mechanism to fuse temporal and spatial signals. FaCTR achieves state-of-the-art performance across eleven benchmarks with close to 400K parameters—on average 50× smaller than competing spatiotemporal transformers. It also enables interpretable forecasting through cross-channel influence scores and supports effective self-supervised pretraining. The design aligns with the structural sparsity and invariance observed in real-world time series, yielding efficient, scalable, and explainable forecasts.

## Method Summary
FaCTR processes multivariate time series by separating temporal and spatial modeling into distinct pathways. Each channel's temporal dynamics are captured through a single-head self-attention module operating on 1D patches, while cross-channel relationships are modeled via a low-rank Factorization Machine. The resulting temporal and spatial representations are fused using a learned gating mechanism before passing through an embedding-wise MLP. The architecture processes 512-step lookback windows using 32-step patches, achieving strong performance with only ~400K parameters across standard forecasting benchmarks.

## Key Results
- Achieves state-of-the-art performance on 11 standard benchmarks with ~400K parameters
- Outperforms spatiotemporal transformers by 50× in parameter efficiency while maintaining or improving accuracy
- Enables interpretable forecasting through cross-channel influence scores derived from the Factorization Machine
- Supports effective self-supervised pretraining, improving generalization

## Why This Works (Mechanism)
FaCTR's efficiency stems from its factorization approach: temporal attention per channel avoids quadratic complexity in channel count, while the low-rank FM captures cross-channel dependencies without full pairwise interactions. The gating mechanism adaptively balances temporal and spatial information based on the input. This design exploits the natural sparsity and stationarity in real-world time series, where temporal patterns are often channel-specific and cross-channel relationships are low-rank. The separation of concerns allows each module to specialize, reducing computational overhead while maintaining expressiveness.

## Foundational Learning
- **Time series patch embedding**: Splits temporal sequence into fixed-size patches for transformer processing; needed to handle long sequences efficiently; quick check: verify patch overlap and stride match paper (P=32, S=32)
- **Factorization Machines**: Models pairwise feature interactions with low-rank assumption; needed to capture cross-channel dependencies without full O(C²) complexity; quick check: verify FM rank=8 produces stable cross-channel scores
- **Single-head temporal attention**: Processes each channel independently with reduced computational cost; needed to avoid quadratic scaling with channel count; quick check: confirm attention weights per channel are computed independently
- **RevIN normalization**: Channel-wise normalization using training statistics; needed to stabilize training across heterogeneous time series; quick check: ensure normalization statistics are computed per channel on training split only
- **Gated fusion**: Learns to weight temporal vs spatial representations dynamically; needed to balance complementary information sources; quick check: verify gate outputs are in [0,1] and sum to 1 across the two inputs
- **SAM optimization**: Sharpens decision boundaries for better generalization; needed to improve optimization stability; quick check: confirm SAM ρ values from provided tables are used per dataset/horizon

## Architecture Onboarding
- **Component map**: Input patches -> Temporal attention (per channel) -> FM cross-channel scores -> Gated fusion -> MLP -> Linear head to horizon
- **Critical path**: Patch embedding → Temporal attention → FM → Gated fusion → MLP → Forecast
- **Design tradeoffs**: Separates temporal and spatial modeling for efficiency vs. potential loss of joint modeling benefits; uses low-rank FM vs. full pairwise interactions for scalability
- **Failure signatures**: Traffic dataset underperforms due to over-smoothing sharp peaks (diagnose with channel-wise MSE); covariate misalignment degrades cross-channel scores; standard Adam without SAM yields worse generalization
- **First experiments**: 1) Train on ETTh1-h96 with 1 layer and verify parameter count (~400K); 2) Test cross-channel influence scores for interpretability; 3) Compare with/without SAM to confirm performance gap

## Open Questions the Paper Calls Out
- **Open Question 1**: How can FaCTR be extended to support future-known covariates during the forecasting horizon? The current architecture lacks a mechanism to ingest future-aligned exogenous variables without disrupting the temporal attention structure. Evidence would be a modified variant demonstrating performance improvements on datasets with strong future-leaking covariates.
- **Open Question 2**: How can FaCTR integrate hierarchical reconciliation to ensure forecast consistency across different spatial levels? While it models cross-channel dependencies, it lacks inductive bias to enforce coherence between aggregated lower-level and higher-level forecasts. Evidence would be an architectural variant minimizing variance between aggregated bottom-level and top-level predictions.
- **Open Question 3**: Can the spatial embedding layer be decoupled from input channel cardinality to facilitate cross-domain transfer learning? The current lookup-table embedding requires reinitialization when channel count changes, breaking transfer of learned structural representations. Evidence would be successful fine-tuning from a source to target domain with different channel counts using parameterized embeddings.

## Limitations
- Number of stacked layers is unspecified, creating uncertainty about model depth and capacity
- Exact positional encoding scheme and dropout configurations are not provided
- Paper lacks ablation on layer depth, making it unclear how performance scales with depth
- Static/dynamic covariate embedding sizes are inferred rather than explicitly stated

## Confidence
- **High confidence**: Core factorization approach (temporal attention + FM cross-channel interaction) and gating mechanism are well-described and replicable
- **Medium confidence**: Embedding dimensions (D=32), patch configuration (P=32, S=32), and training protocol (Adam + SAM, cosine annealing) are clearly specified
- **Low confidence**: Total number of layers/blocks is unknown, which may significantly impact performance and parameter efficiency claims

## Next Checks
1. Train FaCTR with 1, 3, and 5 layers on ETTh1-h96 and compare to Table 1 to empirically determine the correct depth
2. Verify the exact positional encoding implementation matches the paper's intent by comparing outputs with and without PE
3. Test whether using SAM with ρ values from the provided tables consistently improves over standard Adam across all datasets