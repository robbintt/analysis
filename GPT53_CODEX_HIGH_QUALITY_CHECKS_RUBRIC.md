# GPT-5.3-Codex-High Quality Checks Rubric

Status: **Core-focused rubric finalized** (tags ignored; metadata low-weight).

## Composite Executive Summary (Checkpoint Decision)

Using the current core-focused rubric:
- **New model cohort (10 papers, offset rule `3 + 44n`)**: mean **89.90**, std **6.30**, **6 PASS-High**, **4 PASS-Conditional**, **0 FAIL**.
- **Pony Alpha old-batch baseline (6 papers, Appendix B)**: mean **87.33**, std **5.20**, **3 PASS-High**, **3 PASS-Conditional**, **0 FAIL**.

### Decision
- On this rubric and sampled sets, the **new-model paper digests remain slightly stronger on average** than the Pony Alpha old-batch baseline.
- The largest practical quality spread is **within** the new-model cohort (lowest: 75, highest: 95), so per-paper QC still matters.
- Recommended policy: treat new-model digests as generally usable for synthesis, with targeted review for lower-scoring outliers and claim-calibration checks.


## Purpose
Define a consistent, auditable QA process for evaluating paper analysis `.md` reports against their source `.txt` files.

## Scope
- Applies to analyses generated by **gpt-5.3-codex-high**.
- Designed to support **interleaved evaluation** with outputs from a different model.
- Unit of evaluation: one pair (`report.md` vs `source.txt`) for the same arXiv/paper ID.

## Pairing Rules (sanity check)
1. Match by paper ID prefix (e.g., `2406.02856_*`).
2. Confirm report title roughly matches source title.
3. If multiple reports exist for same paper, evaluate the explicitly selected one.
4. If pairing is ambiguous, mark `BLOCKED` and do not score.

---

## Rubric (100 points total)

### 1) Paper Match Sanity Check (5 pts)
Intent: catch obvious pairing mistakes quickly.

Scoring:
- **5** = clearly same paper
- **3** = probably same paper, minor mismatch/noise
- **0** = obviously wrong paper/source pairing

Note: this is a rough human sanity check (not strict formal validation).

### 2) Core Factual Accuracy (30 pts)
Intent: verify the main story is true to source.

Quick checks:
- problem + contribution
- method framing
- direction of main findings
- no obvious fabricated central concepts

Scoring:
- **26–30**: Mostly accurate, no major factual errors
- **18–25**: Some drift/imprecision; core story still right
- **8–17**: **Multiple misleading statements**; core story only partially reliable
- **0–7**: Core summary is wrong or strongly misleading

### 3) Quantitative Claims Sanity (core-only) (15 pts)
Intent: penalize only numeric issues that mislead core conclusions.

Scoring:
- **13–15**: No misleading numeric issues on core claims
- **9–12**: Numeric imprecision exists, but peripheral; core interpretation unchanged
- **5–8**: At least one misleading numeric issue **on a core claim**
- **0–4**: Multiple misleading numeric issues **on core claims** or fabricated core quantitative results

### 4) Claim Discipline (Overreach Control) (20 pts)
Intent: penalize overreach only when it distorts core takeaway.

Scoring:
- **17–20**: Well-calibrated claims; no core overreach
- **12–16**: Some overstatement, but not core-distorting
- **6–11**: At least one **core-misleading overreach**
- **0–5**: Multiple core-misleading overreaches or strongly inflated conclusions

### 5) Metadata & Formatting Hygiene (5 pts)
Intent: track polish issues with low impact on total quality.

Required fields:
- `title`
- `arxiv_id`
- `source_url`

Scoring:
- **5** = all required fields present + clean formatting
- **4** = all required fields present + 1 minor issue
- **3** = all required fields present + 2–3 minor issues
- **2** = all required fields present + 1 major issue
- **1** = missing 1 required field
- **0** = missing 2+ required fields or unusable metadata

**Tags are ignored** (no scoring impact).

### 6) Unsupported / Fabricated Content (core-focused) (15 pts)
Intent: penalize unsupported content when it affects core interpretation.

Scoring:
- **15** = no core unsupported/fabricated claims
- **12** = 1 peripheral unsupported claim
- **9** = 2+ peripheral unsupported claims
- **6** = 1 core unsupported/fabricated claim
- **3** = 2 core unsupported/fabricated claims
- **0** = 3+ core unsupported/fabricated claims, or one fabricated central result

### 7) Evidence Anchoring Quality (10 pts)
Intent: ensure major judgments are quickly verifiable.

Major judgments (4):
1. core factual accuracy judgment
2. core numeric judgment
3. claim-overreach judgment
4. unsupported/fabricated-content judgment

Scoring:
- **10** = anchors for **4/4** major judgments
- **8** = anchors for **3/4** major judgments
- **6** = anchors for **2/4** major judgments
- **4** = anchors for **1/4** major judgments
- **2** = citations/quotes exist, but do not clearly support any major judgment
- **0** = no usable anchors

---

## Severity Labels
Use these for each issue found:
- **S0 (Critical):** wrong paper identity/pairing, or fabricated central result
- **S1 (Major):** core-misleading factual/quantitative/claim-discipline issue
- **S2 (Moderate):** meaningful but non-core-distorting issue
- **S3 (Minor):** wording/format/cosmetic issue

## Pass/Fail Thresholds
- **PASS-High:** 90–100
- **PASS-Conditional:** 75–89
- **FAIL:** <75

Guidance:
- A report with unresolved S0 or multiple S1 issues should not be marked PASS-High.

---

## Required Output Format Per Check

```md
### QA Record
- paper_id:
- report_path:
- source_path:
- evaluator_model: gpt-5.3-codex-high
- comparison_model: (if interleaved)
- timestamp:

### Score
- Paper Match Sanity Check (5):
- Core Factual Accuracy (30):
- Quantitative Claims Sanity (15):
- Claim Discipline / Overreach (20):
- Metadata & Formatting Hygiene (5):
- Unsupported/Fabricated Content (15):
- Evidence Anchoring (10):
- **Total (100):**
- Verdict: PASS-High | PASS-Conditional | FAIL

### Findings
1. [Severity: Sx] Finding summary
   - Report evidence:
   - Source evidence (quote/line anchor):
   - Impact:
   - Fix guidance:

### Final Judgment
- Reliability summary (2–4 lines)
- Whether this sample should be moved to `bad/` (Yes/No)
```

---

## Interleaving Plan (Template)

| order | paper_id | model_under_test | comparison_model | status |
|---|---|---|---|---|
| 1 |  | gpt-5.3-codex-high |  | pending |
| 2 |  | other-model |  | pending |
| 3 |  | gpt-5.3-codex-high |  | pending |
| 4 |  | other-model |  | pending |

Notes:
- Alternate models in sequence (A/B/A/B).
- Use similar paper difficulty where possible.
- Do not run batch checks until interleaving list is finalized.

---

## Operational Notes
- Keep QA writeups concise but evidence-backed.
- Prefer direct source quotes for disputed core claims.
- If source `.txt` appears OCR-corrupt or incomplete, mark as `BLOCKED` and skip scoring.
- This file is the QA policy baseline; revisions should be versioned.

---

## Appendix A — 3-Report Baseline Setup (Pony Alpha / GLM-5 Preview)

Purpose: establish a **relative-comparison baseline** across 3 reports generated by **Pony Alpha (GLM-5 Preview)**.

Sampling rule used:
- Source index set: `(13 + 40n)` for `n = 0, 1, 2`
- Interpreted as **1-indexed positions** in the **lexicographically sorted** `.md` file list from the 2025 analysis corpus.

Selected baseline samples:

1. **Position 13**
   - Report (`.md`):
     - `2011.03783_towards-a-resource-for-multilingual-lexicons-an-mt-assisted-and-human-in-the-loop-multilingual-parallel-corpus-with-multi-word-expression-annotation_20260210_034853.md`
   - Corresponding source (`.txt`):
     - `2011.03783.txt`

2. **Position 53**
   - Report (`.md`):
     - `2302.13849_optimal-prediction-using-expert-advice-and-randomized-littlestone-dimension_20260210_013041.md`
   - Corresponding source (`.txt`):
     - `2302.13849.txt`

3. **Position 93**
   - Report (`.md`):
     - `2308.08427_eliciting-risk-aversion-with-inverse-reinforcement-learning-via-interactive-questioning_20260210_051248.md`
   - Corresponding source (`.txt`):
     - `2308.08427.txt`

Baseline comparison mode:
- Treat these 3 as a **relative cohort** (compare each against the other two) before broader evaluation.
- Emphasize intra-cohort consistency on:
  - core factual accuracy,
  - core quantitative sanity,
  - claim discipline,
  - unsupported/fabricated content,
  using the rubric above.

Status:
- **Selection complete**
- **Comparative scoring complete** (baseline extended to 6 reports)

---

## Appendix B — Baseline Comparative Scoring Results (Pony Alpha / GLM-5 Preview)

Scored using the **current core-focused rubric** in this file.

### B.1 Per-Report Scores (current rubric, n=6)

| paper_id | Match (5) | Core Facts (30) | Quant Core (15) | Claim Discipline (20) | Metadata (5) | Unsupported/Fabricated (15) | Anchoring (10) | Total | Verdict |
|---|---:|---:|---:|---:|---:|---:|---:|---:|---|
| 2011.03783 | 5 | 24 | 15 | 14 | 4 | 12 | 10 | **84** | PASS-Conditional |
| 2302.13849 | 5 | 24 | 15 | 12 | 4 | 12 | 10 | **82** | PASS-Conditional |
| 2308.08427 | 5 | 26 | 15 | 18 | 4 | 15 | 10 | **93** | PASS-High |
| 2312.05827 | 5 | 26 | 15 | 17 | 4 | 15 | 10 | **92** | PASS-High |
| 2402.03903 | 5 | 23 | 15 | 13 | 4 | 12 | 10 | **82** | PASS-Conditional |
| 2403.10786 | 5 | 26 | 15 | 16 | 4 | 15 | 10 | **91** | PASS-High |

### B.2 Key Evidence Highlights (added 3 old-batch samples)

#### Sample 4 — 2312.05827
- **Core support confirmed**:
  - PULSE method + online Bayesian update framing + subspace projection are explicit in abstract/introduction (source lines 7–11, 60–67).
  - Real-time claim (<1ms update/inference) appears in abstract (source lines 16–17).
  - AUC and strategy superiority signals appear in results sections (e.g., source lines 908–915, 1008–1015, 1853–1856).
- **Issue level**:
  - No core fabricated or core-misleading claim identified.

#### Sample 5 — 2402.03903
- **Core support confirmed**:
  - Variance-reduction theorem and finite-time complexity connection are explicit (source lines 16–18, 528–542, 600–607).
  - Empirical signal includes significant wins in MinAtar and 7/9 PPO cases (source lines 897–904, 1018–1020).
- **Core-adjacent issue**:
  - Report method framing suggests PiLaR is the PPO mechanism, while source PPO section compares exact λ-returns vs n-step and states PiLaR is not necessary in that setting (source lines 947–953).

#### Sample 6 — 2403.10786
- **Core support confirmed**:
  - Contour-guided diffusion + SCGD + zero-shot framing are explicit (source lines 20–31, 298–301, 889–905).
  - DSC/HD95 gains are supported in quantitative section (source lines 705–707, 713–717).
- **Core-adjacent issue**:
  - Report claims superior FID/KID across all tested datasets; source indicates best FID on both datasets but KID is best on H&T and close second on lumbar (source lines 717–722).

### B.3 Cross-Sample Baseline Statistics (n=6)
- Total score mean: **87.33 / 100**
- Total score sample std. dev.: **5.20**
- Verdict distribution:
  - PASS-High: 3/6
  - PASS-Conditional: 3/6
  - FAIL: 0/6

### B.4 Divergence Pattern (current rubric signal, n=6)
Dominant recurring pattern in this expanded old-batch sample:
1. **Core factual reliability is generally good** across the cohort.
2. **Main residual risk is claim calibration / mechanism phrasing drift** (e.g., method-attribution overreach).
3. **Metadata drift exists but remains low-impact under this rubric**.

Interpretation:
- Under the core-focused rubric, Pony Alpha old-batch reports are generally usable for synthesis with targeted review on claim calibration.

## Appendix C — Unified New-Model Evaluation (10 reports, offset 3 + 44n)

### C.1 Sample Set and Scores

| order | arxiv_id | score | verdict | dominant note |
|---:|---|---:|---|---|
| 1 | 2401.02509 | 95 | PASS-High | Strong core fidelity; only low-impact metadata drift |
| 2 | 2401.13875 | 84 | PASS-Conditional | Minor mechanism/open-question calibration issues |
| 3 | 2402.01831 | 89 | PASS-Conditional | Broadly accurate; slight over-broad SOTA phrasing risk |
| 4 | 2402.05284 | 75 | PASS-Conditional | Lowest-scoring outlier; claim-calibration concerns |
| 5 | 2402.08078 | 93 | PASS-High | Core thesis aligned; minor explanatory certainty overreach |
| 6 | 2402.18139 | 93 | PASS-High | Strong core alignment; low-impact metadata issue |
| 7 | 2404.03622 | 94 | PASS-High | Strong quantitative fidelity; mild generalization overreach |
| 8 | 2405.11204 | 88 | PASS-Conditional | Good core bounds; one unsupported/open-question omission |
| 9 | 2406.00048 | 94 | PASS-High | Strong core mechanism alignment; minor calibration caution |
| 10 | 2411.00533 | 94 | PASS-High | Strong core faithfulness and numeric consistency |

### C.2 Aggregate Statistics (new-model cohort, n=10)
- Mean score: **89.90 / 100**
- Sample std. dev.: **6.30**
- Verdict distribution:
  - **PASS-High:** 6/10
  - **PASS-Conditional:** 4/10
  - **FAIL:** 0/10

### C.3 Relative Comparison vs Pony Alpha Old-Batch Baseline (Appendix B, n=6)
- Pony Alpha baseline mean: **87.33 / 100**
- New-model cohort mean: **89.90 / 100**
- Difference in means: **+2.57** (new-model higher on this rubric/sample)

Interpretation notes:
- Comparison is informative but not definitive due different sample sizes (10 vs 6) and sampling grids.
- Both cohorts show generally usable digest quality under core-focused criteria.
- New-model cohort shows broader spread; targeted review should focus on lower-scoring entries (especially 2402.05284).

### C.4 Consolidated Decision
Under the current rubric and sampled checkpoints, the new-model digests are **operationally usable and slightly stronger on average** than the expanded Pony Alpha old-batch baseline, with quality primarily limited by occasional claim-calibration drift rather than core factual failure.

