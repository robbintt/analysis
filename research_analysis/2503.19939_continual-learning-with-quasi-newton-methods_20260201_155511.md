# Continual Learning With Quasi-Newton Methods
*Steven Vander Eeckt; Hugo Van hamme*

---

## üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 40 |
| **Key Improvement** | +8% overall performance vs. EWC |
| **Forgetting Reduction** | 50% reduction in catastrophic forgetting |
| **Top Accuracy** | 66.39% on Split CIFAR-10/100 |
| **Memory Optimization** | Constant memory usage achieved via MRT |

---

## üìù Executive Summary

Continual learning faces the critical challenge of **catastrophic forgetting**, where neural networks lose proficiency in previously learned tasks upon acquiring new information. While **Elastic Weight Consolidation (EWC)** has been a dominant regularization-based solution, it suffers from a fundamental theoretical limitation: it relies on a Laplace approximation that assumes model parameters are uncorrelated. By simplifying the Hessian to only the diagonal of the Fisher information matrix, EWC ignores crucial off-diagonal interactions and parameter correlations. This simplification constrains performance, particularly in complex scenarios where retaining inter-parameter dependencies is essential for preventing knowledge loss.

The authors introduce **Continual Learning with Sampled Quasi-Newton (CSQN)**, a novel optimization framework designed to capture off-diagonal Hessian information without requiring architecture-specific modifications. The method replaces EWC's diagonal approximation with a more accurate low-rank Hessian approximation derived from Quasi-Newton methods (specifically SR1 and BFGS). By maximizing the posterior probability of parameters, CSQN integrates a regularization term weighted by this precise Hessian. To ensure computational feasibility and numerical stability, the algorithm factorizes the Hessian to minimize memory storage and employs Cholesky or Eigenvalue decomposition to maintain positive definiteness. Additionally, the authors introduce sophisticated scalability strategies‚Äî**Capped Truncation (CT)**, **Binary Tree (BTREE)**, and **Mixed Regularization Technique (MRT)**‚Äîto manage memory footprint while preserving performance.

Across four benchmarks, CSQN consistently outperformed EWC, Kronecker-Factored methods, and several rehearsal-based approaches. The method reduced catastrophic forgetting by **50%** and improved overall performance by an average of **8%** compared to EWC. This research establishes a new performance standard for regularization-based continual learning, demonstrating that rehearsal-free methods can significantly outperform existing state-of-the-art techniques by accurately modeling parameter correlations.

---

## üîç Key Findings

*   **Significant Performance Gain**: CSQN significantly outperforms EWC, reducing catastrophic forgetting by **50%** and improving overall performance by an average of **8%**.
*   **Benchmark Dominance**: Across four benchmarks, CSQN consistently outperformed EWC and other baselines (including rehearsal-based methods), securing superior results in three out of four scenarios.
*   **Robustness**: The method demonstrated particular robustness in the most challenging testing scenarios.
*   **Generalizability**: CSQN achieves these results across diverse tasks and architectures without requiring modifications specific to the model structure.

---

## üõ† Methodology

The researchers propose **Continual Learning with Sampled Quasi-Newton (CSQN)**, a method designed to improve upon the regularization techniques used in Elastic Weight Consolidation (EWC).

*   **Limitation of EWC**: EWC relies on a Laplace approximation that simplifies the Hessian to the diagonal of the Fisher information matrix. This assumes parameters are uncorrelated, which is often inaccurate.
*   **The CSQN Advantage**: CSQN leverages Quasi-Newton methods to compute more accurate Hessian approximations. This captures parameter interactions beyond the diagonal (i.e., parameter correlations) without architecture-specific modifications.
*   **Optimization Strategy**: The approach integrates Quasi-Newton methods with continual learning to provide better Hessian estimates for regularization.

---

## ‚öôÔ∏è Technical Details

### Core Algorithm
The CSQN approach addresses the continual learning problem by maximizing the posterior probability of parameters. Built upon EWC, it utilizes the Laplace approximation but replaces the diagonal Hessian with a more accurate approximation from Sampled Quasi-Newton (SQN) methods.

*   **Loss Function**: Combines the current task's cross-entropy loss with a regularization term weighted by the previous task's Hessian ("B").
*   **Algorithms**: Implements SR1 and BFGS algorithms for low-rank Hessian updates using sampling techniques.

### Memory and Stability
*   **Factorization**: To optimize memory, the Hessian is factorized as "B = B_0 + ZZ^T", storing only "Z" and "B_0".
*   **Positive Definiteness**: Maintained via Cholesky Factorization or, if undefined, via Eigenvalue Decomposition (zeroing negative eigenvalues) and QR Factorization.
*   **Fisher Integration**: Integrates the diagonal of the Fisher Information Matrix (FIM) for sampling covariance and initial Hessian estimation.

### Scalability Strategies
*   **Capped Truncation (CT)**
*   **Binary Tree (BTREE)**: Achieves logarithmic memory growth.
*   **Mixed Regularization Technique (MRT)**: Enables constant memory usage.

---

## üìà Results

### Comparative Performance
*   **vs. EWC**: CSQN consistently outperforms EWC, reducing catastrophic forgetting by 50% and improving overall performance by an average of 8%.
*   **vs. Kronecker-Factored (KF) Methods**: CSQN-S (20) achieved **66.39% ACC** on Split CIFAR-10/100 (outperforming KF's 61.05% and EWC's 61.42%).
*   **Rotated MNIST**: Achieved **86.22% ACC**.
*   **vs. Rehearsal Methods**: Against ER, DER++, and A-GEM, CSQN (BFGS) and MRT demonstrated superior stability on Split TinyImageNet.

### Specific Dataset Metrics
*   **Split TinyImageNet**: CSQN-S (20) + MRT achieved **47.94% ACC**.
*   **Vision Datasets**: Achieved **79.23% ACC**.

### Sensitivity and Scalability Analysis
*   **Hyperparameter Sensitivity**: Higher ranks ($M=20$) can lead to over-regularization; **$M=10$** performed better on Split TinyImageNet.
*   **Scalability**:
    *   **BTREE**: Achieved logarithmic memory growth with **<1% degradation**.
    *   **MRT**: Achieved constant memory usage with performance variances ranging from **<2% degradation** on MNIST to a **5.5% improvement** on Split TinyImageNet.

---

## üèÜ Research Contributions

1.  **Flaw Identification**: The paper identifies the "uncorrelated model parameters" assumption in EWC as a critical flaw and introduces a method to capture off-diagonal parameter interactions.
2.  **Novel Algorithm**: Introduces **CSQN**, a robust optimization strategy that integrates Quasi-Newton methods with continual learning to provide better Hessian estimates for regularization.
3.  **New Performance Standards**: The research establishes new performance standards for regularization-based continual learning, demonstrating that rehearsal-free methods can significantly outperform existing state-of-the-art approaches.