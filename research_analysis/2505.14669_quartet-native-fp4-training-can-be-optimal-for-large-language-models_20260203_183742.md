---
title: 'Quartet: Native FP4 Training Can Be Optimal for Large Language Models'
arxiv_id: '2505.14669'
source_url: https://arxiv.org/abs/2505.14669
generated_at: '2026-02-03T18:37:42'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quartet: Native FP4 Training Can Be Optimal for Large Language Models

*Roberto L. Castro; Andrei Panferov; Soroush Tabesh; Oliver Sieberling; Jiale Chen; Mahdi Nikdan; Saleh Ashkboos; Dan Alistarh*

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Target Architecture:** NVIDIA Blackwell (5th-gen Tensor Cores)
> *   **Peak Compute:** 18 PFLOPS (B200 GPU)
> *   **Format:** MXFP4 (1 Sign, 1 Mantissa, 2 Exponent)
> *   **Key Efficiency:** $\text{eff}_N = 0.65$, $\text{eff}_D = 0.95$

---

## Executive Summary

Training Large Language Models (LLMs) is computationally prohibitive, driving demand for lower-precision formats like FP4 to reduce memory bandwidth and energy consumption. However, existing FP4 algorithms face significant accuracy degradation and typically rely on mixed-precision fallbacks, such as maintaining master weights in FP16 or FP8, to maintain stability. This paper addresses the challenge of achieving accurate, stable training natively in FP4 without these fallback mechanisms, which is essential for fully realizing the theoretical efficiency gains of next-generation hardware accelerators.

The authors introduce **"Quartet,"** a novel algorithm designed for native Matrix-Multiply Floating Point 4 (MXFP4) training. Quartet executes all forward and backward pass matrix multiplications in the MXFP4 formatâ€”utilizing a 1-sign, 1-mantissa, 2-exponent structure with block-scaling (32 elements per 8-bit scale)â€”and is optimized for the 5th-gen Tensor Cores of the NVIDIA Blackwell architecture. The research further formulates a new low-precision scaling law that incorporates Parameter Efficiency ($\text{eff}_N$) and Data Efficiency ($\text{eff}_D$) coefficients. This law was derived through extensive evaluations on **Llama-type models** (specifically the Llama-2 architecture) to model how model size, data volume, and precision interact to determine training loss.

Quartet demonstrates superior performance over stable baselines like LUQ-INT4. It achieved a Parameter Efficiency ($\text{eff}_N$) of 0.65 and a Data Efficiency ($\text{eff}_D$) of 0.95, significantly outperforming LUQ-INT4, which scored 0.49 and 0.15, respectively. In validation loss tests on the C4 dataset at a 100x Data-to-Parameter ratio, Quartet achieved a loss of 3.29, a 10% relative improvement over the baseline. Hardware benchmarks on the Blackwell architecture indicate massive throughput gains, with the B200 GPU peaking at 18 PFLOPS of dense FP4 compute and the RTX 5090 achieving nearly 2x speedup relative to FP8 implementations.

This work proves that native FP4 training is competitive with higher-precision formats like FP16 and FP8, challenging the assumption that extreme quantization necessitates accuracy trade-offs. By combining a robust algorithmic framework with optimized CUDA kernels for the Blackwell architecture, the authors provide a practical path for the industry to drastically reduce training costs through the measured throughput improvements. The findings suggest that FP4 is poised to become a standard precision for future AI training workloads.

---

## Key Findings

*   **Native Viability:** Existing FP4 algorithms suffer from accuracy degradation and rely on mixed-precision fallbacks; Quartet proves native FP4 training is competitive with FP16/FP8.
*   **New Scaling Laws:** A new low-precision scaling law was identified to model loss based on Model Size, Data, and Precisions.
*   **Hardware Efficiency:** Hardware-supported FP4 offers better throughput and energy efficiency compared to higher precision formats.
*   **Algorithmic Superiority:** The proposed Quartet algorithm significantly outperforms current stable baselines in both parameter and data efficiency.

---

## Methodology

The authors conducted extensive evaluations on Llama-type models to investigate FP4 feasibility and derive scaling laws. Based on this analysis, they designed the **'Quartet' algorithm** for accurate low-precision computation. Finally, they implemented optimized CUDA kernels specifically for the NVIDIA Blackwell architecture to validate the theoretical findings in a real-world hardware environment.

---

## Contributions

*   **Quartet Algorithm:** Introduction of the Quartet algorithm for accurate, native FP4 training without mixed-precision fallbacks.
*   **Scaling Law Formulation:** Derivation of a low-precision scaling law incorporating Parameter Efficiency and Data Efficiency coefficients.
*   **Software Release:** Release of optimized CUDA kernels for the NVIDIA Blackwell architecture.
*   **Benchmarking:** Provision of empirical evidence benchmarking FP4 viability against higher-precision formats (FP16/FP8).

---

## Technical Details

**Algorithm & Architecture**
*   **Core Logic:** Native MXFP4 training executing all matrix multiplications (forward and backward) in FP4 without fallback.
*   **Hardware Target:** Optimized for NVIDIA Blackwell architecture (5th-gen Tensor Cores).

**Numerical Format**
*   **Structure:** 4 bits per value (1 sign, 1 mantissa, 2 exponent).
*   **Scaling:** Block-scaling where groups of 32 elements share an 8-bit power-of-two scaling factor.

**Training Configuration**
*   **Models:** Llama-2 architecture.
*   **Dataset:** C4 dataset.
*   **Optimizer:** AdamW with weight decay and gradient clipping.
*   **Metrics:** Incorporates Parameter Efficiency ($\text{eff}_N$) and Data Efficiency ($\text{eff}_D$) coefficients to model loss.

---

## Results

**Efficiency Metrics**
*   **Quartet:** Achieved a Parameter Efficiency ($\text{eff}_N$) of **0.65** and Data Efficiency ($\text{eff}_D$) of **0.95**.
*   **Baseline (LUQ-INT4):** Scored 0.49 and 0.15 respectively.

**Validation Performance**
*   **Loss:** Achieved a loss of **3.29** on C4 at a 100x D/N ratio.
*   **Improvement:** Represents a **10% relative improvement** over LUQ-INT4.

**Hardware Benchmarks**
*   **Speedup:** Almost **2x speedup** relative to FP8 on an RTX 5090.
*   **Peak Compute:** B200 GPU peaking at **18 PFLOPS** of dense FP4 compute.