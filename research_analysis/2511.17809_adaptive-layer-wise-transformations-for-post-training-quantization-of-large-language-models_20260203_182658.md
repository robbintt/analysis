---
title: Adaptive Layer-Wise Transformations for Post-Training Quantization of Large
  Language Models
arxiv_id: '2511.17809'
source_url: https://arxiv.org/abs/2511.17809
generated_at: '2026-02-03T18:26:58'
quality_score: 8
citation_count: 5
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Adaptive Layer-Wise Transformations for Post-Training Quantization of Large Language Models

*Cuong Pham; Hoang Anh Dung; Cuong C. Nguyen; Trung Le; Gustavo Carneiro; Jianfei Cai; Thanh-Toan Do*

---

> ### ðŸ“Š Quick Facts
>
> *   **Models Tested:** LLaMA Family (Specifically LLaMA-2-7B)
> *   **Quantization Regime:** Aggressive (W3A3K3V3)
> *   **Key Metric Gains (vs. FlatQuant):** +4.58 PPL improvement; +2.11% Zero-shot accuracy
> *   **Core Mechanism:** Kurtosis-based heuristic for layer selection
> *   **Efficiency:** Bypasses expensive differentiable searches
> *   **Quality Score:** 8/10

---

## Executive Summary

This paper addresses the limitations of current post-training quantization (PTQ) methods for Large Language Models (LLMs), specifically the reliance on homogeneous transformation settings. Existing approaches typically apply a uniform transformation typeâ€”such as Affine or Rotationâ€”across all layers of a network. This strategy is suboptimal because it fails to account for the heterogeneous statistical properties and weight distributions present across different layers. Consequently, these fixed settings lead to significant performance degradation under aggressive quantization regimes (e.g., W3A3), where maintaining accuracy is critical for practical deployment.

The authors introduce an adaptive, heterogeneous framework that selects the optimal transformation type on a per-layer basis. Instead of relying on a computationally expensive differentiable search, the method leverages a discovered correlation between weight distribution kurtosis and transformation effectiveness. The system dynamically chooses between Affine transformations, which flatten distributions using a learnable matrix, and Rotation transformations, which redistribute outliers using an orthogonal matrix. A lightweight proxy utilizing robust z-score normalization guides this selection: for Attention layers, high kurtosis favors Affine transformation, while for Feed-Forward Network (FFN) layers, high kurtosis favors Rotation transformation.

In experiments on LLaMA-2-7B using aggressive W3A3K3V3 quantization, the adaptive method significantly outperformed fixed baseline strategies. It achieved a WikiText-2 perplexity (PPL) of 7.26 and a C4 PPL of 9.42, with a Zero-shot average accuracy of 61.15%. Compared to a fixed Affine approach, this resulted in a 0.28 improvement in WikiText-2 PPL and a 1.26 percentage point gain in zero-shot accuracy. Furthermore, the method demonstrated substantial improvements over the FlatQuant baseline, achieving gains of up to 4.58 points in PPL and 2.11% in zero-shot accuracy, validating the efficacy of layer-wise adaptability.

This research sets a new benchmark for aggressive PTQ by demonstrating that layer-specific heterogeneous transformations are necessary for preserving LLM performance at low bit-widths. By establishing a computationally efficient mechanism to determine optimal transformations based on statistical properties like kurtosis, the authors eliminate the need for expensive differentiable searches. This advancement enables more efficient deployment of large models on resource-constrained hardware without sacrificing accuracy, paving the way for broader adoption of highly quantized LLMs in production environments.

---

## Key Findings

*   **Superior Performance:** The proposed adaptive, heterogeneous approach consistently outperforms fixed transformation settings across the LLaMA family models.
*   **Significant Gains:** Achieved major improvements under aggressive quantization, including:
    *   Up to **4.58 points** improvement in perplexity.
    *   A **2.11%** gain in zero-shot accuracy compared to FlatQuant.
*   **Statistical Correlation:** Established a specific connection between **weight distribution kurtosis** and transformation type, enabling efficient layer-wise decision-making.
*   **Suboptimality of Homogeneity:** Confirmed that applying the same transformation type across all layers is suboptimal due to heterogeneous distribution characteristics.

---

## Methodology

The authors propose an **adaptive transformation selection framework** designed to systematically determine the optimal transformation on a per-layer basis. The process involves the following stages:

1.  **Differentiable Formulation:** The problem is initially formulated as a differentiable optimization problem to identify specific transformation types.
2.  **Efficiency Optimization:** To address high computational costs, a lightweight proxy is developed.
3.  **Outlier-Guided Selection:** An efficient 'outlier-guided layer selection' method is implemented using robust z-score normalization.
4.  **Statistical Leveraging:** The method leverages the discovered relationship between weight kurtosis and transformation type to bypass expensive search procedures.

---

## Technical Details

The paper proposes an adaptive, heterogeneous post-training quantization (PTQ) framework that selects the optimal transformation type based on the statistical properties of each layer.

### Transformation Types

*   **Affine Transformation:** Uses a learnable matrix to flatten distributions.
    *   Equation: $\hat{Y}_A = Q_a(XA)Q_w(A^{-1}W)$
*   **Rotation Transformation:** Uses an orthogonal matrix to redistribute outliers.
    *   Equation: $\hat{Y}_R = Q_a(XR)Q_w(R^\top W)$

### Implementation Process

1.  **Step 1:** A differentiable search minimizing reconstruction error and entropy.
2.  **Step 2:** A Kurtosis-based heuristic for efficient deployment.

### Heuristic Logic

The selection logic varies by layer type based on kurtosis thresholds:

| Layer Type | High Kurtosis | Low Kurtosis |
| :--- | :--- | :--- |
| **Attention** | **Affine Transformation** | **Rotation Transformation** (< 2.0) |
| **FFN** | **Rotation Transformation** | **Affine Transformation** (< 0.2) |

---

## Results

Experiments were conducted on **LLaMA-2-7B** with **W3A3K3V3** quantization.

### Performance Metrics (Best Case)

*   **WikiText-2 PPL:** 7.26
*   **C4 PPL:** 9.42
*   **Zero-shot Avg Acc:** 61.15%

### Comparative Analysis

| Method | WikiText-2 PPL | C4 PPL | Zero-shot Accuracy |
| :--- | :--- | :--- | :--- |
| **Adaptive Method** | **7.26** | **9.42** | **61.15%** |
| Fixed Affine | 7.54 | 9.58 | 59.89% |
| Fixed Rotation | 7.99 | 10.01 | 58.91% |
| **Improvement vs. Fixed Affine** | +0.28 | +0.16 | +1.26 pp |

**vs. FlatQuant:**
*   Gains of up to **4.58 points** in PPL.
*   Gains of **2.11%** in zero-shot accuracy.

---

## Contributions

*   **Addressing Homogeneity:** Identified and addressed the limitation of existing transformation-based quantization methods that rely on homogeneous settings, proving the necessity of layer-specific heterogeneous transformations.
*   **Efficient Selection Mechanism:** Contributed a computationally efficient layer selection mechanism that bypasses expensive differentiable searches by utilizing weight distribution kurtosis and robust z-score normalization.
*   **Comprehensive Validation:** Provided extensive experimental validation on LLaMA models, setting new benchmarks for aggressive post-training quantization.

---

**Quality Score:** 8/10  
**References:** 5 citations