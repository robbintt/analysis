---
title: 'Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning'
arxiv_id: '2504.07097'
source_url: https://arxiv.org/abs/2504.07097
generated_at: '2026-02-03T18:36:29'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning

*Nikhil Shivakumar Nayak; Krishnateja Killamsetty; Ligong Han; Abhishek Bhandwaldar; Prateek Chanda; Kai Xu; Hao Wang; Aldo Pareja; Oleg Silkin; Mustafa Eyceoz; Akash Srivastava*

***

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Performance** | Outperforms O-LoRA baseline by up to **7%** |
| **Architectures** | T5-Large (Encoder-Decoder), LLaMA-2 7B (Decoder-only) |
| **Parameter Overhead** | **Zero** additional parameters |
| **Citations** | 40 references |

***

## Executive Summary

This research addresses the critical challenge of **continual learning** in Large Language Models (LLMs), specifically targeting the phenomenon of **catastrophic forgetting** where acquiring new knowledge degrades performance on previously learned tasks. As LLMs are increasingly deployed in dynamic environments requiring updates with new data without costly retraining, maintaining a balance between **plasticity** (absorbing new information) and **stability** (retaining core capabilities) is essential.

Existing solutions, such as parameter-efficient fine-tuning (PEFT) methods like LoRA, often introduce parameter overhead or fail to fully utilize the model's capacity, while standard full fine-tuning typically results in significant interference with prior knowledge.

The authors introduce **"Constrained Full Fine-Tuning" (CFF)**, a novel framework that utilizes the full parameter space of the model without incurring additional memory overhead. Technically, the method employs an adaptive Singular Value Decomposition (SVD) to decompose each layer's weight matrix into a high-rank subspace (for retaining existing knowledge) and a low-rank subspace (for task-specific adaptation).

To prevent interference, CFF projects gradient updates onto this low-rank subspace using orthogonal projection formulas, ensuring that updates for the current task do not alter the critical components of previous tasks. Experimental validation demonstrates that CFF achieves state-of-the-art performance, outperforming the O-LoRA baseline by up to **7%** while reducing catastrophic forgetting to near-negligible levels, all without compromising safety or instruction-following capabilities.

***

## Key Findings

*   **State-of-the-Art Performance:** Achieved superior results on standard continual learning benchmarks, outperforming the O-LoRA baseline by up to **7%**.
*   **Capability Preservation:** Successfully maintained general linguistic skills, instruction-following accuracy, and safety protocols throughout the learning process.
*   **Mitigation of Catastrophic Forgetting:** Reduced forgetting to near-negligible levels by effectively balancing plasticity and retention.
*   **Broad Architectural Compatibility:** Validated on both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models.

***

## Methodology

The research proposes a **full fine-tuning framework** rather than relying on low-rank parameter-efficient updates. The core methodology involves:

1.  **Adaptive SVD Framework:** Dynamically identifies task-specific low-rank parameter subspaces within the model.
2.  **Orthogonal Constraints:** Enforces strict constraints on updates to prevent interference with prior tasks. This ensures new information does not overwrite previous knowledge.
3.  **Efficiency:** Achieves these results without additional parameter overhead or the need to store gradients from previous tasks.

***

## Technical Details

The approach proposes **Constrained Full Fine-Tuning** using Singular Value Decomposition (SVD) to manage weight matrices.

| Component | Description |
| :--- | :--- |
| **Weight Decomposition** | Splits a model's weight matrix $W^{(l)}$ into a **high-rank subspace** (retention) and a **low-rank subspace** (adaptation). |
| **Gradient Projection** | Projects gradient updates onto the low-rank subspace to prevent interference with previous tasks. |
| **Projection Formulas** | Updates are calculated via:<br> $$ \nabla_{u-proj} = (I - UU^T)\nabla $$ <br> $$ \nabla_{v-proj} = \nabla (I - V^T V) $$ |
| **Layer Importance** | Quantified via cosine similarity between input and output activations, normalized across layers. |
| **Optimization Strategy** | SVD is performed **once per task**, updating the full parameter space with geometric constraints rather than penalties or added modules. |

***

## Contributions

*   **Scalability Solution:** Addresses limitations of existing parameter-efficient methods by requiring **no extra parameter overhead**.
*   **Theoretical Grounding:** Introduces a theoretically grounded optimization using adaptive SVD to manage the stability-plasticity dilemma in LLMs.
*   **Practical Viability:** Demonstrates that continual learning can be performed on large-scale models (7B parameters) without sacrificing safety or instruction-following abilities.

***

## Results

The method demonstrates robust effectiveness across several metrics:

*   **Benchmark Performance:** Achieved state-of-the-art performance, significantly outperforming the O-LoRA baseline.
*   **Forgetting Reduction:** Catastrophic forgetting was reduced to "near-negligible levels."
*   **Skill Retention:** Preserved general linguistic skills, instruction-following accuracy, and safety protocols.
*   **Computational Efficiency:** Validation on T5-Large and LLaMA-2 7B showed minimal overhead, as SVD is computed only once per task.