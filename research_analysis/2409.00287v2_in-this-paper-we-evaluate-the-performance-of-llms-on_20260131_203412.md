---
title: In this paper, we evaluate the performance of LLMs on
arxiv_id: 2409.00287v2
source_url: https://arxiv.org/abs/2409.00287v2
generated_at: '2026-01-31T20:34:12'
quality_score: 8
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# In this paper, we evaluate the performance of LLMs on

*Youning Zhang, Dhruv Parikh, Viktor Prasanna, Zuoning Zhang*

---

### üìä Quick Facts

| Metric | Specification |
| :--- | :--- |
| **Architecture** | Cerebras Wafer Scale Engine (WSE) |
| **Core Count** | 850,000 cores (2-D mesh) |
| **Peak Performance** | 7.5 PFLOPS/s |
| **Memory Bandwidth** | 20 PB/s |
| **Aggregate SRAM** | 40 GB (48 KB per core) |
| **Key Innovation** | Weight Streaming Execution Model |

---

## üìã Executive Summary

> This research addresses the computational bottlenecks associated with training and inference of massive Large Language Models (LLMs), such as GPT-3. As model parameter counts scale into the billions, traditional hardware architectures often struggle with memory bandwidth limitations and communication overhead, failing to fully utilize available compute resources. This study is critical because it evaluates whether a wafer-scale architecture can overcome these scalability barriers, specifically examining the trade-offs between batch size, model size, and hardware utilization for state-of-the-art NLP workloads.
>
> The core innovation lies in the utilization and evaluation of the **Cerebras Wafer Scale Engine (WSE)**, a massive 2D mesh consisting of 850,000 cores, augmented by a novel "weight streaming" execution model. Technical implementation relies on an external **MemoryX system** that streams weights to the WSE and receives gradients, effectively decoupling the total model size from the limited on-chip memory capacity (40 GB aggregate). The architecture utilizes fine-grained data flow scheduling with support for unstructured sparsity and static routing through 24 distinct "colors" to manage on-chip communication, facilitating high-efficiency processing across the array.
>
> Performance testing reveals a peak system capability of **7.5 PFLOPS/s** and **20 PB/s** memory bandwidth. Training GPT-3 on the PILE dataset, projected epochs take approximately 157 hours for a 256M model, scaling up to 7,603 hours for a 20B model. Throughput analysis shows that while small models (256M) benefit from larger batch sizes, large models (20B) experience throughput drops due to bandwidth saturation. Yet, a roofline analysis confirms that both BERT and GPT-3 training operate primarily in a compute-bound region, effectively scaling the "memory wall." Additionally, BERT inference latency remains stable across increasing batch sizes, demonstrating consistent per-sample efficiency.
>
> The significance of this work lies in its validation of **wafer-scale integration** as a viable solution for the expanding compute demands of AI research. By demonstrating that the weight streaming approach can handle models vastly larger than on-chip memory while maintaining compute-bound operation, the paper provides a framework for future hardware design that avoids typical memory bottlenecks. These findings suggest that specialized, massive-scale accelerators are essential for the continued advancement of LLMs, offering a path forward for training next-generation models without linear increases in time or cost.

---

## üîç Key Findings

Based on the analysis provided:

*   **Memory Wall Scaled:** The wafer-scale architecture effectively mitigates traditional memory bandwidth limitations, allowing both BERT and GPT-3 training to operate in a compute-bound region.
*   **Decoupled Memory Capacity:** The "weight streaming" approach successfully decouples model size from on-chip memory constraints, enabling the training of models (up to 20B parameters) larger than the aggregate 40 GB SRAM.
*   **Batch Size Sensitivity:** Throughput scaling is non-linear; small models (256M) improve with larger batch sizes, while large models (20B) suffer from throughput degradation due to bandwidth saturation.
*   **Inference Stability:** BERT inference latency remains stable regardless of batch size increases, indicating consistent per-sample efficiency.
*   **Viability of Wafer-Scale:** The study validates wafer-scale integration as a critical solution for the compute demands of next-gen AI, avoiding linear cost/time increases.

---

## ‚öôÔ∏è Technical Details

### System Architecture
*   **Core Structure:** 850,000 cores arranged in a 2-D mesh.
*   **Memory per Core:** 48 KB of SRAM (40 GB aggregate on-chip memory).
*   **Specialized Cache:** 256-byte software-configurable cache per core.

### Processing & Communication
*   **Data Flow:** Supports unstructured sparsity via fine-grained data flow scheduling.
*   **Threading:** Utilizes 8 micro-threads per core.
*   **Routing:** Communication managed by routers with bidirectional 32-bit ports and static routing via 24 'colors'.

### Execution Model
*   **Weight Streaming:** Weights are streamed from an external MemoryX system to the WSE.
*   **Gradient Handling:** Gradients are received back by the MemoryX system.
*   **Benefit:** Decouples model size from on-chip memory capacity limitations.

---

## üìà Results & Performance

### System Specifications
*   **Peak Performance:** 7.5 PFLOPS/s
*   **Memory Bandwidth:** 20 PB/s

### Training Performance (GPT-3 on PILE Dataset)
*   **Projected Training Times (1 Epoch):**
    *   **256M Model:** ~157 hours
    *   **2.7B Model:** ~1,255 hours
    *   **13B Model:** ~4,941 hours
    *   **20B Model:** ~7,603 hours

### Throughput Analysis
*   **Small Models (256M):** Throughput improves as batch size increases.
*   **Large Models (20B):** Throughput drops as batch size increases due to bandwidth saturation.
*   **Mid-sized Models (2.7B‚Äì13B):** Throughput remains relatively constant across batch sizes.

### Inference & Roofline
*   **BERT Inference:** Latency remains stable with increasing batch size (improved per-sample efficiency).
*   **Roofline Analysis:** Both BERT and GPT-3 training confirmed to be operating in the **compute-bound** region.

---

### Document Metadata
*   **Quality Score:** 8/10
*   **References:** 29 citations