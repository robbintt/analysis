---
title: 'PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation'
arxiv_id: '2512.04025'
source_url: https://arxiv.org/abs/2512.04025
generated_at: '2026-02-03T19:21:08'
quality_score: 8
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation

*Xiaolong Li; Youping Gu; Xi Lin; Weijie Wang; Bohan Zhuang*

---

### ðŸ“Š Quick Facts

| **Metric** | **Value / Improvement** |
| :--- | :--- |
| **Computational Complexity** | Reduced from $O(N^2)$ to $O(N)$ |
| **Inference Throughput** | Increased by **1.5x to 2.0x** |
| **GPU Memory Usage** | Reduced by **30-50%** |
| **Training Speed** | ~1.3x faster (Video Generation) |
| **Accuracy Impact** | < 0.5% drop compared to full attention |
| **Key Benchmark Score** | ~82.2% Top-1 on Kinetics-400 |

---

## Executive Summary

Video understanding and generation models relying on Transformer architectures face a critical scalability challenge due to the quadratic computational complexity ($O(N^2)$) of self-attention with respect to sequence length. This complexity creates prohibitive memory and latency costs when processing high-resolution or long-duration videos. While existing sparse attention methods attempt to mitigate this by dropping tokens via binary masking, they often suffer from significant information loss, resulting in degraded visual fidelity and model accuracy.

The authors introduce **Pyramid Sparse Attention (PSA)**, a novel attention module that replaces rigid binary masking with multi-level pooled Key-Value (KV) representations. Instead of simply retaining or discarding blocks, PSA employs a dynamic token allocation strategy where a lightweight importance scorer assigns lower pooling levels (higher detail) to critical KV blocks and higher pooling levels (lower detail) to less important regions. This creates an efficient interpolation between full retention and complete pruning. To ensure practical deployability, the method utilizes a hardware-friendly decoupled block-tile architecture within a native kernel, which maximizes SRAM reuse and minimizes HBM bandwidth bottlenecks.

PSA demonstrates robust performance across video understanding and generation tasks, significantly reducing computational overhead while maintaining high accuracy. On the Kinetics-400 benchmark, PSA achieves approximately 82.2% Top-1 accuracy while delivering 1.5x to 2.0x higher inference throughput and reducing GPU memory usage by 30-50% compared to full attention. In video generation, the PSA-DiT model achieves lower FrÃ©chet Video Distance (FVD) scores than sparse baselines and matches full-attention DiT performance with ~1.3x faster training speeds. Overall, the architecture reduces complexity from $O(N^2)$ to $O(N)$ with less than a 0.5% drop in accuracy.

This research establishes a significant conceptual bridge between efficient attention mechanisms and classical computer vision techniques, such as feature pyramids and fixed-point quantization. By successfully decoupling the preservation of critical visual details from the computational cost of processing less relevant regions, PSA offers a versatile, plug-and-play solution for both discriminative and generative video tasks. The substantial reduction in memory footprint and increase in throughput, paired with the public release of code and weights, provide a strong foundation for future research into scalable, high-fidelity video modeling.

---

## Key Findings

*   **Consistent Performance:** PSA consistently outperforms or achieves comparable performance to existing sparse attention baselines across video understanding and generation benchmarks while maintaining high computational efficiency.
*   **Context Preservation:** By replacing binary masking with multi-level pooled Key-Value (KV) representations, PSA successfully preserves contextual information and visual fidelity.
*   **Hardware Optimization:** The proposed design utilizes a native kernel with a decoupled block-tile architecture for hardware-friendly execution.
*   **Dynamic Allocation:** Dynamically allocating lower pooling levels to critical KV blocks and higher levels to less important blocks creates an effective interpolation between full retention and complete pruning.

---

## Methodology

The core methodology is the **Pyramid Sparse Attention (PSA)** module, designed to address the limitations of binary masking. Instead of retaining or discarding entire key-value blocks, PSA introduces multi-level pooled KV representations.

Each query block dynamically assigns lower pooling levels (higher detail) to critical KV blocks and higher pooling levels (lower detail) to less important blocks. The implementation leverages a decoupled block-tile design within a native kernel to ensure hardware compatibility.

---

## Technical Details

*   **Pyramid KV Pooling**
    Replaces standard full self-attention with a multi-level pooled Key-Value (KV) representation. It utilizes downsampled KV maps at multiple scales.

*   **Dynamic Token Allocation**
    Employs a lightweight importance scorer to retain critical tokens at full resolution and assign less critical tokens to higher pooling levels.

*   **Hardware-Friendly Architecture**
    Utilizes a **Decoupled Block-Tile Architecture** that:
    *   Loads data into SRAM densely.
    *   Maximizes reuse of pooled KV data to minimize HBM bandwidth bottlenecks.
    *   Fuses operations into a single kernel launch.

*   **Computational Complexity**
    Successfully reduced from $O(N^2)$ to $O(N)$.

---

## Performance Results

### Video Understanding
*   **Accuracy:** Achieved ~82.2% Top-1 accuracy on Kinetics-400.
*   **Efficiency:** Improved inference throughput by **1.5x to 2.0x**.
*   **Memory:** Reduced GPU memory by **30-50%** compared to full attention.
*   **Optimization:** Dynamic allocation improved Top-1 accuracy by +1.5% to +2.0% over static pooling.

### Video Generation
*   **Fidelity:** PSA-DiT achieved lower FVD scores than sparse baselines and comparable scores to full-attention DiT.
*   **Cost:** Lower computational cost with ~1.3x increase in training speed.

### Overall Metrics
*   Complexity reduced from $O(N^2)$ to $O(N)$.
*   ~40% reduction in activation memory.
*   Up to 2.0x throughput improvement.
*   <0.5% accuracy drop relative to full attention baselines.

---

## Research Contributions

*   **Module Introduction:** Introduced Pyramid Sparse Attention (PSA), a versatile module for video understanding and generation tasks that overcomes quadratic complexity bottlenecks.
*   **Novel Sparsity Paradigm:** Developed a new sparsity paradigm using multi-level pooled representations to significantly reduce information loss.
*   **Conceptual Bridge:** Drew a conceptual parallel between efficient attention mechanisms and classical computer vision techniques (feature pyramids) and fixed-point quantization.
*   **Open Source:** Public release of code and model weights to facilitate further research.

---

**Quality Score:** 8/10  
**References:** 26 citations