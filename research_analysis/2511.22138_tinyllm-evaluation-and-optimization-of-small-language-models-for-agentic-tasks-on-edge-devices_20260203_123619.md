---
title: 'TinyLLM: Evaluation and Optimization of Small Language Models for Agentic
  Tasks on Edge Devices'
arxiv_id: '2511.22138'
source_url: https://arxiv.org/abs/2511.22138
generated_at: '2026-02-03T12:36:19'
quality_score: 8
citation_count: 21
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices

*Mohd Ariful Haque; Fahad Rahman; Kishor Datta Gupta; Khalil Shujaee; Roy George*

---

### ⚡ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Focus Area** | Edge AI / Agentic Tasks (Function Calling) |
| **Top Overall Accuracy** | **65.74%** (Hybrid Optimized) |
| **Top Multi-turn Accuracy** | **55.62%** |
| **Primary Dataset** | Berkeley Function Calling Leaderboard (BFCL) |
| **Key Innovation** | Hybrid Optimization (SFT + PEFT + RL + DPO) |
| **Quality Score** | 8/10 |

---

## Executive Summary

This research addresses the critical challenge of deploying autonomous agents capable of complex function calling and tool usage directly on resource-constrained edge devices. Currently, such agentic capabilities rely heavily on large, cloud-based language models, which introduces latency, privacy risks, and connectivity dependencies. The study aims to determine whether Small Language Models (SLMs), specifically those with limited parameters, can effectively bridge the performance gap required for agentic tasks—such as API interactions and multi-turn reasoning—while operating within the strict computational constraints of edge hardware.

The key innovation is the development and empirical validation of a **hybrid optimization strategy** that synthesizes four distinct training methodologies: Supervised Fine-Tuning (SFT), Parameter-Efficient Fine-Tuning (PEFT), Reinforcement Learning (RL), and Direct Preference Optimization (DPO). Technically, the authors constructed a specialized DPO pipeline utilizing AgentBank datasets (e.g., ALFRED), generating preference pairs by converting SFT data and utilizing inferior outputs from smaller models (like TinyLlama) as negative samples.

Evaluation on the BFCL revealed a distinct accuracy hierarchy, with medium-sized models (1–3B parameters) significantly outperforming ultra-compact models (<1B parameters). The hybrid-optimized medium-sized models achieved peak performance metrics of **65.74% overall accuracy** and **55.62% multi-turn accuracy**. The results demonstrated that while ultra-compact models struggle with the logic required for complex agentic workflows, the application of hybrid optimization enables medium-sized SLMs to effectively handle demanding tasks such as parallel and multiple function calling, as well as relevance detection, across both live and non-live interaction modes.

This work significantly advances the field by demonstrating the practical feasibility of privacy-preserving, low-latency autonomous agents on edge hardware, effectively reducing reliance on centralized cloud infrastructure. By establishing that optimized SLMs can perform complex agentic tasks locally, the research validates a path toward more secure and responsive AI applications in real-world environments.

---

## Key Findings

*   **Accuracy Hierarchy:** There is a distinct performance gap where medium-sized models (**1–3B parameters**) significantly outperform ultra-compact models (**<1B parameters**) in executing agentic tasks.
*   **Peak Performance:** Through hybrid optimization strategies, medium-sized models achieved up to **65.74% overall accuracy** and **55.62% multi-turn accuracy** on the Berkeley Function Calling Leaderboard (BFCL).
*   **Optimization Necessity:** Hybrid optimization strategies (combining SFT, PEFT, RL, and DPO) are identified as critical for enabling small language models to deliver accurate, efficient, and stable performance.
*   **Edge Feasibility:** Small language models can effectively perform complex agentic tasks (function/tool/API calling) on edge devices, removing reliance on cloud infrastructure while ensuring privacy and low latency.

---

## Methodology

The research employed a rigorous evaluation and training framework to assess the capabilities of Small Language Models (SLMs).

*   **Evaluation Framework:** Utilized the **Berkeley Function Calling Leaderboard (BFCL)** to assess performance across categories:
    *   Simple function calling
    *   Multiple function calling
    *   Parallel function calling
    *   Parallel-multiple function calling
    *   Relevance detection
*   **Testing Environments:** Conducted evaluations in both live and non-live settings, specifically including complex multi-turn conversation scenarios.
*   **Architectures Analyzed:** Evaluated specific architectures including:
    *   TinyAgent
    *   TinyLlama
    *   Qwen
    *   xLAM
*   **Optimization Strategies:** Implemented and compared parameter-driven strategies:
    *   **Supervised Fine-Tuning (SFT)**
    *   **Parameter-Efficient Fine-Tuning (PEFT)**
    *   **Reinforcement Learning (RL)-based optimization**
    *   **Hybrid Methods**
*   **DPO Pipeline Development:** Developed a Direct Preference Optimization (DPO) pipeline using AgentBank data (e.g., ALFRED). This involved:
    *   Converting SFT data into chosen-rejected pairs.
    *   Using TinyLlama responses as the rejected outputs.
    *   Applying manual validation for quality assurance.

---

## Technical Details

The study focuses on the architectural and operational constraints of running agentic AI on edge hardware.

**Model Classification**
The research categorizes Small Language Models (SLMs) into two distinct classes based on parameter count:
*   **Ultra-compact:** <1B parameters
*   **Medium-sized:** 1–3B parameters

**Core Optimization Approach**
The study utilizes a hybrid optimization strategy that combines four distinct methodologies to maximize efficiency and accuracy:
1.  **Supervised Fine-Tuning (SFT)**
2.  **Parameter-Efficient Fine-Tuning (PEFT)**
3.  **Reinforcement Learning (RL)**
4.  **Direct Preference Optimization (DPO)**

**Target Capabilities**
The architecture is optimized specifically for:
*   Function calling
*   Tool usage
*   API interactions

**Deployment Objective**
The architecture is optimized for edge device execution, enabling local inference to eliminate cloud dependency.

---

## Results

The evaluation on the Berkeley Function Calling Leaderboard (BFCL) yielded definitive results regarding the viability of SLMs for agentic tasks:

*   **Performance Gap:** Medium-sized models (1–3B) significantly outperformed ultra-compact models (<1B).
*   **Best-in-Class Metrics:** Hybrid-optimized medium-sized models achieved:
    *   **Overall Accuracy:** 65.74%
    *   **Multi-turn Accuracy:** 55.62%
*   **Operational Success:** Models successfully performed complex, multi-turn function calling on edge hardware.
*   **System Benefits:** Confirmed benefits include:
    *   **Privacy:** Data remains local.
    *   **Low Latency:** Elimination of cloud round-trips.
    *   **Stability:** Reduced dependency on network connectivity.

---

## Contributions

*   **Comprehensive Evaluation:** Provided a detailed evaluation of SLMs specifically for agentic capabilities on edge devices, analyzing performance across diverse BFCL categories and interaction modes (multi-turn/live).
*   **Strategy Comparison:** Systematically described and compared various fine-tuning and optimization techniques (SFT, PEFT, RL, DPO), establishing the superiority of hybrid approaches for this domain.
*   **Pipeline Innovation:** Detailed the construction of a DPO training pipeline utilizing AgentBank data, introducing a specific method for generating preference pairs using smaller model outputs as negative samples.
*   **Feasibility Proof:** Demonstrated the practical feasibility of running privacy-preserving, low-latency autonomous agents on edge hardware, advancing the deployment of AI beyond cloud-centric architectures.

---
**Quality Score:** 8/10 | **References:** 21 citations