# Certified but Fooled! Breaking Certified Defences with Ghost Certificates

*Quoc Viet Vo; Tashreque M. Haq; Paul Montague; Tamas Abraham; Ehsan Abbasnejad; Damith C. Ranasinghe*

---

> ### ðŸ“‹ Quick Facts Sidebar
>
> *   **Dataset:** ImageNet
> *   **Target Defense:** DensePure (SOTA)
> *   **Classifier:** Ensemble ($\sigma=0.5$)
> *   **Top Achieved Metrics:**
>     *   **Untargeted ASR:** 90% (High Distortion)
>     *   **Targeted ASR:** 30% (High Distortion)
>     *   **Optimal Regions ($k$):** 5
> *   **Quality Score:** 8/10
> *   **Citations:** 14

---

## Executive Summary

### **Problem**
This research addresses a critical vulnerability in certified defense mechanisms, specifically probabilistic frameworks like randomized smoothing, which are designed to provide mathematical guarantees of robustness against adversarial attacks. The fundamental issue identified is that these certified defenses offer a false sense of security; while they purport to verify that no adversarial example exists within a certain radius of an input, the certification process itself can be deceived. This matters deeply because certified robustness is currently the gold standard for deploying models in high-stakes environments, and the ability to bypass these protections undermines the trust in mathematical guarantees that were previously considered unbreakable.

### **Innovation**
The key innovation is the **"GhostCert"** framework, which introduces the concept of certificate spoofingâ€”the ability to simultaneously force a misclassification and manipulate the certifier into issuing a fraudulent robustness certificate (a "Ghost Certificate") for that incorrect class. Technically, the method employs Region-Focused Adversarial Examples to craft perturbations that move inputs into specific feature space regions capable of generating large certification radii for target classes. The framework utilizes Projected Gradient Descent (PGD) to maximize the agreement of the smoothed classifier under a noise distribution, rather than simply minimizing loss at a single point. To ensure these attacks remain undetected, the authors integrate **Salient-Region Masking**â€”combining Gradient-Driven Saliency (GradCAM) with the Segment Anything Model (SAM)â€”to restrict perturbations to naturalistic structures within the image, ensuring they are imperceptible to the human eye.

### **Results**
Evaluated on the ImageNet dataset against an Ensemble classifier ($\sigma=0.5$), GhostCert demonstrated a significant ability to bypass state-of-the-art defenses like DensePure, outperforming baseline methods such as "Random k Regions." In targeted attack scenarios, GhostCert achieved a 30% Attack Success Rate (ASR) compared to 0% for the baseline at high distortion levels. For untargeted attacks, the framework achieved a 90% ASR versus the baseline's 45% at high distortion, and maintained a significant advantage at low distortion levels with a 35% ASR compared to 10%. Sensitivity analysis further established that utilizing $k=5$ regions was the optimal configuration for the constraint optimization process, and a user study confirmed the method's ability to maintain high perceptual realism compared to previous attacks like Shadow Attack.

### **Impact**
This work fundamentally shifts the understanding of adversarial robustness by revealing that the very metrics used to guarantee safety can be exploited as an attack surface. It exposes a critical weakness in current probabilistic certification frameworks, proving that robustness guarantees can be artificially inflated to mislead defenders. By validating these attacks on complex datasets like ImageNet against top-tier defenses, the authors establish that the classification output and the robustness metric can be controlled concurrently. Consequently, this research will likely compel the machine learning community to redesign certification processes, moving beyond current probabilistic methods to ensure that certification mechanisms themselves are resilient to manipulation and spoofing.

---

## Key Findings

The study demonstrates several breakthroughs in the field of adversarial machine learning:

*   **Dual Manipulation Capability:** The method successfully allows for the simultaneous misleading of a classifier and the spoofing of the certification process.
*   **Imperceptibility of Attacks:** Researchers established that these spoofing attacks can be executed using small perturbations that remain visually undetectable.
*   **Superior Certification Radii:** The method creates "ghost certificates" where the robustness radius for the target (incorrect) class is larger than the radius for the true class.
*   **Effective Bypass of SOTA Defenses:** The research confirms the method's efficacy against state-of-the-art defenses, specifically **Densepure**, on the ImageNet dataset.

---

## Methodology

The authors utilized a multi-faceted approach to exploit certified defenses:

*   **Region-Focused Adversarial Examples:** Crafting perturbations designed to move inputs into specific feature space regions that are capable of generating certificates for incorrect classes.
*   **Constraint Optimization:** Explicitly keeping perturbations small and imperceptible through constrained optimization techniques.
*   **Ghost Certificate Generation:** The core methodology involves coaxing the certified model into issuing deceptive, large robustness radii for the wrong classes.

---

## Contributions

This work provides significant advancements to the understanding and application of adversarial attacks:

1.  **Exposure of Framework Vulnerabilities:** It reveals that probabilistic certification frameworks contain critical vulnerabilities where robustness guarantees can be actively exploited.
2.  **Advancement of State-of-the-Art:** It introduces **certificate spoofing**, a novel technique enabling attackers to control both the classification output and the robustness metric.
3.  **Practical Verification:** The study verifies the practicality of these attacks against complex, state-of-the-art defensive systems (like Densepure) on the large-scale ImageNet dataset.

---

## Technical Details

The **GhostCert** framework operates through the following technical mechanisms:

*   **Ghost Certificates:** A mechanism to simultaneously force misclassification by the base classifier and spoof the randomized smoothing certifier.
*   **Projected Gradient Descent (PGD):** Used to optimize the behavior of the smoothed classifier, maximizing sample agreement under a noise distribution.
*   **Salient-Region Masking:** Employed to ensure imperceptibility by restricting perturbations to naturalistic structures. This combines:
    *   **Gradient-Driven Saliency (GradCAM)**: Identifying important features.
    *   **Semantic Boundaries (Segment Anything Model - SAM)**: Defining object boundaries.
*   **Threat Model:** Targets certified defenses like DensePure by controlling classifier behavior across a neighborhood of noisy samples rather than a single point.

---

## Performance Results

The method was evaluated on 20 images from the ImageNet dataset against an Ensemble classifier ($\sigma=0.5$). The results highlight the superiority of GhostCert over baseline methods like 'Random k Regions'.

### Attack Success Rate (ASR) Comparison

| Attack Type | Distortion Level | GhostCert ASR | Baseline ASR |
| :--- | :--- | :--- | :--- |
| **Targeted** | High Distortion | **30%** | 0% |
| **Untargeted** | High Distortion | **90%** | 45% |
| **Untargeted** | Low Distortion | **35%** | 10% |

### Additional Analysis
*   **Sensitivity Analysis:** Identified **$k=5$** as the optimal configuration for the number of regions in the constraint optimization process.
*   **Human Evaluation:** A user study conducted on Amazon Mechanical Turk evaluated the perceptual realism of the attacks, comparing them favorably to the Shadow Attack method.