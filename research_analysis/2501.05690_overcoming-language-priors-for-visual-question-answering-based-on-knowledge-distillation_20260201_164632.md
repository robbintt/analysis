# Overcoming Language Priors for Visual Question Answering Based on Knowledge Distillation

*Daowan Peng; Wei Wei*

***

> ### ðŸ“Š Quick Facts
>
> *   **SOTA Accuracy**: 62.86% (VQA-CPv2, UpDn backbone)
> *   **Performance Gain**: +21.38% improvement over baseline
> *   **Primary Method**: Knowledge Distillation with Soft Labels & Adaptive Reweighting
> *   **Key Loss Function**: $L_{kd}$ (Knowledge Distillation loss)
> *   **Quality Score**: 8/10

***

## Executive Summary

### **Problem**
Visual Question Answering (VQA) models frequently suffer from the "language prior" problem, where they rely on statistical correlations in question text to predict answers rather than analyzing visual content. This occurs because training datasets often contain imbalanced answer distributions; for instance, a question asking "Is there a...?" may statistically be answered "yes" more often. Consequently, models exploit these linguistic shortcuts, leading to poor performance when evaluated on Out-of-Distribution (OOD) benchmarks (such as VQA-CPv2) where these correlations are broken. Addressing this "prior-dependency dilemma" is critical for developing robust multimodal systems that generalize effectively to real-world scenarios where visual reasoning is paramount.

### **Innovation**
The paper proposes **KDAR** (Knowledge Distillation with Adaptive Sample-wise Reweighting), a framework designed to decouple model reasoning from language priors. The core technical innovation is a dual-component approach:
1.  **Knowledge Distillation with Soft Labels**: Utilizes a teacher model to generate soft labels that provide semantic constraints. These labels penalize the student model for overfitting to high-frequency answers and narrow the range of candidate answers, forcing the student to rely on multimodal features.
2.  **Adaptive Sample-wise Reweighting**: Dynamically adjusts the importance of individual training samples to mitigate bias further.

The architecture is driven primarily by the Knowledge Distillation loss ($L_{kd}$), which acts as a regularizer to suppress noisy linguistic patterns.

### **Results**
KDAR achieves state-of-the-art performance on the VQA-CPv2 OOD benchmark, demonstrating substantial improvements over existing baselines. Using the UpDn backbone, the model attained an accuracy of **62.86%**, a significant leap of **+21.38%** over the baseline (41.48%). Performance granularities show strong results across categories:
*   **Yes/No**: 89.18%
*   **Number**: 55.86%
*   **Other**: 50.98%

The method is highly scalable across different architectures; for example, LXMERT performance surged from 46.23% to 71.33%, and SAN improved from 40.81% to 55.42%. Ablation studies confirm that the distillation loss component is the primary contributor to these performance gains.

### **Impact**
This research establishes a new performance standard for mitigating language biases in VQA, validating that knowledge distillation is an effective strategy for enforcing semantic reliance over statistical shortcuts. By successfully narrowing candidate answers through soft labels and adaptive reweighting, KDAR enhances model generalization not only in OOD settings but also in In-Distribution (IID) evaluations. The method's consistent success across varying backbone architectures suggests it can serve as a broadly applicable module for improving the reliability of visual reasoning systems, ensuring that models attend to visual evidence rather than relying on dataset artifacts.

***

## Key Findings

*   **State-of-the-Art Performance**: The KDAR method achieves superior results on the VQA-CPv2 out-of-distribution (OOD) benchmark, outperforming previous methods designed to address language prior dependency.
*   **Enhanced Generalization**: The framework significantly improves model generalization in both OOD and In-Distribution (IID) settings.
*   **Semantic Guidance**: The use of soft labels from a teacher model effectively narrows the range of candidate answers, providing semantic guidance that forces the model to look at visual data.
*   **Reduction of Linguistic Shortcuts**: The approach successfully mitigates the reliance on linguistic shortcuts (statistical biases), encouraging the use of genuine multimodal knowledge.

***

## Methodology

The proposed **KDAR** framework utilizes knowledge distillation to counter language priors in Visual Question Answering (VQA). The system consists of two primary components:

1.  **Knowledge Distillation with Soft Labels**
    *   Uses a teacher model to generate soft labels.
    *   Penalizes the student model from overfitting to common answers.
    *   Provides semantic constraints to guide learning.

2.  **Adaptive Sample-wise Reweighting**
    *   Incorporates a dynamic learning strategy.
    *   Adjusts the importance of individual samples during training.
    *   Explicitly targets and reduces bias in the dataset.

***

## Contributions

The paper makes the following significant contributions to the field of Visual Question Answering:

*   **Novel Method (KDAR)**: Introduces KDAR, a new method specifically designed to address the "prior-dependency dilemma" in VQA.
*   **Regularization via Soft Labels**: Demonstrates how soft labels from a teacher model can function as a regularizer to prevent overfitting and provide necessary semantic constraints.
*   **Dynamic Bias Mitigation**: Presents an adaptive sample-wise reweighting learning strategy for the dynamic mitigation of bias.
*   **New Performance Standard**: Establishes a new state-of-the-art performance standard on the VQA-CPv2 OOD benchmark.

***

## Technical Details

*   **Framework**: KDAR (Knowledge Distillation-based method).
*   **Objective**: Mitigate language priors by forcing reliance on multimodal knowledge rather than statistical biases.
*   **Teacher Model Function**: Generates soft labels for semantic guidance, narrowing candidate answers.
*   **Loss Components**:
    *   **$L_{apt}$**: Provides marginal improvement.
    *   **$L_{kd}$ (Knowledge Distillation Loss)**: Responsible for the significant performance gains.
*   **Hyperparameters**: Beta ($\beta$) is set to 3.
*   **Evaluation Metric**: Standard VQA accuracy metric.

***

## Results

The KDAR method was rigorously evaluated on the **VQA-CPv2** dataset:

**Overall Performance (UpDn Backbone)**
*   **KDAR Accuracy**: 62.86%
*   **Baseline Accuracy**: 41.48%
*   **Improvement**: +21.38%

**Category Breakdown**
*   **Yes/No**: 89.18%
*   **Number**: 55.86%
*   **Other**: 50.98%

**Architectural Scalability**
*   **SAN**: Improved from 40.81% to 55.42%
*   **UpDn**: Improved from 41.48% to 62.86%
*   **LXMERT**: Improved from 46.23% to 71.33%

**Ablation Studies**
*   Tests confirm that **$L_{kd}$** is the primary driver of performance, increasing accuracy to 62.43% on its own.

***

**References**: 40 citations