# Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation

*Jingwen Gu; Yiting He; Zhishuai Liu; Pan Xu*

***

### ⚡ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Proposed Algorithm** | Distributionally Robust Regularized Policy Optimization (DR-RPO) |
| **Theoretical Regret** | $\tilde{O}(\sqrt{d^3 H^4 K})$ |
| **Framework** | $d$-rectangular Linear MDP |
| **Key Innovation** | Reference-policy regularization for tractability |
| **Paper Quality Score** | 8/10 |

***

## Executive Summary

Reinforcement Learning (RL) agents trained in simulation often face performance degradation in real-world deployments due to discrepancies between modeled and actual environment dynamics, a problem formally addressed by Distributionally Robust Markov Decision Processes (RMDPs). While RMDPs provide a theoretical framework to handle these distribution shifts, existing solutions predominantly rely on value-based methods, leaving policy optimization—a standard paradigm in deep RL essential for high-dimensional actions—largely unexplored. The primary challenge is that extending policy optimization to robust settings introduces significant computational intractability and optimization complexities, preventing the application of modern, scalable actor-critic algorithms in safety-critical environments where uncertainty must be rigorously managed.

The authors introduce **Distributionally Robust Regularized Policy Optimization (DR-RPO)**, a model-free online algorithm designed to resolve the optimization intractability of robust policy methods through a "doubly constrained" framework within the $d$-rectangular linear MDP formulation. This mechanism simultaneously constrains transition dynamics to account for model uncertainty and applies an explicit KL-divergence constraint that forces the learned policy to remain close to a reference policy. This reference-policy regularization is the key technical breakthrough that renders the optimization problem tractable for the softmax policy class. By leveraging linear function approximation and an optimistic exploration strategy adapted from LSVI-UCB, DR-RPO effectively balances robust transition handling with efficient exploration in large state-action spaces.

The paper establishes strong theoretical guarantees for DR-RPO, proving that it achieves a sublinear regret bound of $\tilde{O}(\sqrt{d^3 H^4 K})$, where $d$ is the feature dimension, $H$ is the planning horizon, and $K$ is the number of episodes. This performance aligns with the state-of-the-art bounds previously achieved only by value-based methods, demonstrating polynomial suboptimality and high sample efficiency. Experimental results reported by the authors confirm these theoretical findings, showing that DR-RPO maintains robustness against distribution perturbations while remaining computationally feasible in high-dimensional environments.

This work significantly bridges the gap between robust control theory and modern policy gradient techniques by demonstrating that actor-critic style algorithms can achieve regret bounds matching value-based methods under distribution shifts. The practical implication is that robust, scalable RL agents can now be deployed in complex, high-dimensional settings such as robotics and autonomous systems, where model misspecification and safety are paramount. By proving that regularization can unlock policy optimization for robust RL without sacrificing sample efficiency, the authors provide a validated pathway for building deep RL systems that are both theoretically sound and practically resilient to real-world variability.

***

## Key Findings

*   **Performance:** The proposed DR-RPO algorithm achieves polynomial suboptimality bounds and sample efficiency matching state-of-the-art value-based approaches.
*   **Optimization Tractability:** Reference-policy regularization renders optimization tractable within the softmax policy class, creating novel "doubly constrained" RMDP variants.
*   **Scalability:** The method utilizes the $d$-rectangular linear MDP formulation combined with linear function approximation to scale effectively to large state-action spaces.
*   **Regret Bounds:** DR-RPO achieves sublinear regret in online settings, specifically targeting a regret of $\tilde{O}(\sqrt{d^3 H^4 K})$.
*   **Validation:** Empirical results confirm the theoretical claims and demonstrate the method's practical robustness against distribution shifts.

***

## Methodology

The authors propose a model-free online algorithm designed to address the "sim-to-real" gap in Reinforcement Learning. The core methodological components include:

*   **Doubly Constrained Framework:** The approach utilizes reference-policy regularization to manage the softmax policy class. It adopts the $d$-rectangular linear MDP formulation to simultaneously handle distribution shifts and large-scale environments.
*   **Integration of Techniques:** The method integrates linear function approximation with an upper confidence bonus mechanism.
*   **Exploration Strategy:** An optimistic exploration strategy is employed to facilitate discovery in uncertain environments, adapted from LSVI-UCB principles.

***

## Technical Details

The paper presents several technical components that define the DR-RPO algorithm and its theoretical underpinnings:

*   **Core Algorithm (DR-RPO):**
    *   Addresses the sim-to-real gap using Robust Markov Decision Processes (RMDPs).
    *   Utilizes $d$-rectangular uncertainty sets to define the scope of possible distribution shifts.

*   **Linear Function Approximation:**
    *   Assumes a known feature mapping for transition kernels and reward functions.
    *   Enables the algorithm to operate in high-dimensional continuous spaces.

*   **Doubly Constrained Framework:**
    *   **Transition Robustness:** Constrains transitions via Total Variation divergence or distribution shift penalties.
    *   **Policy Regularization:** Constrains the policy to remain close to a reference policy, ensuring stability.

*   **Policy Parameterization & Optimization:**
    *   **Softmax Policy Class:** Derived to balance exploitation with adherence to a prior policy.
    *   **Modified Robust Bellman Equations:** Incorporate KL-divergence regularization terms to ensure robust value estimation.

*   **Exploration Strategy:**
    *   Optimistic exploration strategy adapted from LSVI-UCB (Least-Squares Value Iteration with Upper Confidence Bound) to handle the exploration-exploitation trade-off in linear MDPs.

***

## Results

While specific empirical tables and plots were not provided in the source text, the paper details significant theoretical and qualitative results:

*   **Theoretical Guarantees:** The algorithm achieves sample efficiency matching value-based approaches.
*   **Regret Analysis:** It is proven to achieve sublinear regret in online settings, with a specific bound of $\tilde{O}(\sqrt{d^3 H^4 K})$, and maintains polynomial suboptimality bounds.
*   **Computational Tractability:** The method remains computationally feasible for continuous and high-dimensional action spaces.
*   **Empirical Confirmation:** The abstract references empirical results that confirm the method's robustness and validate the theoretical claims regarding distribution perturbations.

***

## Contributions

This research makes four primary contributions to the field of Robust Reinforcement Learning:

1.  **Bridging the Gap:** Addresses the underexplored application of policy optimization in robust RL for online settings involving distribution shifts.
2.  **Algorithmic Innovation:** Introduces DR-RPO, a method that adds policy constraints via reference-policy regularization to enable the use of policy gradients in robust settings.
3.  **Theoretical Parity:** Establishes that policy optimization can achieve sample efficiency and regret bounds comparable to state-of-the-art value-based approaches.
4.  **Scalability:** Demonstrates scalable robustness by integrating linear function approximation and optimism-based exploration for high-dimensional spaces.

***

**References:** 10 citations