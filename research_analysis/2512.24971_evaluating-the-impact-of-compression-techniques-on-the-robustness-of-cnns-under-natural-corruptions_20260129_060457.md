# Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions

*Itallo Patrick Castro Alves Da Silva; Emanuel Adler Medeiros Pereira; Erick de Andrade Barboza; Baldoino Fonseca dos Santos Neto; Marcio de Medeiros Ribeiro*

***

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Citations:** 19
> *   **Objective:** Multi-objective optimization (Accuracy, Compression, Robustness)
> *   **Architectures:** ResNet-50, VGG-19, MobileNetV2
> *   **Datasets:** CIFAR-10-C, CIFAR-100-C
> *   **Key Statistic:** 69% of compressed solutions achieved robustness (mCE) $\le$ baseline on CIFAR-100.
> *   **Top Configuration:** Technique #16 (PCQAT Int8) identified as a versatile high-performer.

***

## Executive Summary

**Problem**
Deploying deep learning models on resource-constrained edge devices requires compression techniques such as quantization, pruning, and weight clustering. However, a prevailing assumption in the field is that these techniques inherently degrade model robustness against natural corruptions—such as noise, blur, and weather artifacts—which are unavoidable in real-world environments. This paper addresses the critical challenge of reconciling the need for hardware efficiency with the necessity of maintaining model reliability, investigating whether the traditional trade-off between compression and robustness is inevitable or if specific strategies can mitigate it.

**Innovation**
The study introduces a holistic validation protocol that integrates robustness evaluation directly into the model compression pipeline. The authors utilized a multi-objective optimization framework to evaluate Quantization Aware Training (QAT), pruning, weight clustering, and hybrid methods (PCQAT, CQAT) across ResNet-50, VGG-19, and MobileNetV2 architectures. Technical rigor is provided through the use of Pareto Front Analysis to identify configurations that optimally balance competing objectives: Accuracy, Compression Rate (CR), and Robustness. Robustness was quantified using the mean Corruption Error (mCE) on CIFAR-10-C and CIFAR-100-C datasets, providing a standardized benchmark for how different density levels react to specific corruption types.

**Results**
Empirical evidence revealed that compression does not universally harm performance; 69% of compressed solutions on CIFAR-100 achieved robustness (mCE) equal to or better than the baseline. While standalone Int8 quantization (Technique #10) resulted in the worst robustness, the hybrid PCQAT Int8 method (Technique #16) was identified as a versatile high-performer. ResNet-50 and VGG-19 dominated the Pareto-optimal solutions (71% and 29% on CIFAR-10, respectively), whereas MobileNetV2 had 0% representation in optimal trade-offs. Specific standout metrics included a Best Robustness mCE of 76.7 on CIFAR-10 and 87.5 on CIFAR-100, alongside maximum compression ratios of 9.42x achieved without total failure of robustness.

**Impact**
This research provides actionable guidelines for deploying deep learning models on edge devices, demonstrating that complex architectures can retain or even improve robustness under aggressive compression. By establishing evidence of compression-induced robustness, the authors challenge the conventional narrative that efficiency must come at the cost of resilience. The study contributes a detailed benchmarking methodology for the community, offering a reference for future work on the interplay between model density and corruption types, ultimately enabling more reliable deployments in volatile real-world conditions.

***

## Key Findings

*   **Preservation of Robustness:** Specific compression strategies preserve and improve model robustness against natural corruptions rather than degrading performance.
*   **Architecture Complexity:** The positive effects of compression on robustness are more pronounced in complex architectures.
*   **Hybrid Superiority:** Combinations of compression methods (quantization, pruning, and weight clustering) yield superior multi-objective results compared to applying them in isolation.
*   **Optimal Trade-offs:** Pareto-optimal configurations were identified that successfully balance high compression ratios, accuracy, and robustness.

***

## Methodology

The study utilized a comprehensive evaluation framework designed to stress-test Convolutional Neural Networks (CNNs) under resource constraints and environmental corruption.

*   **Architectures:** Evaluated **ResNet-50**, **VGG-19**, and **MobileNetV2**.
*   **Techniques:** Analyzed Quantization, Pruning, and Weight Clustering individually and in combination (e.g., PCQAT, CQAT).
*   **Datasets:** Experiments were conducted using **CIFAR-10-C** and **CIFAR-100-C** to test performance under natural corruptions.
*   **Assessment Strategy:** A multiobjective strategy was employed to quantify and visualize trade-offs between robustness, accuracy, and compression ratio.

***

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Models** | ResNet-50, VGG-19, MobileNetV2 |
| **Datasets** | CIFAR-10, CIFAR-100 |
| **Compression Techniques** | QAT (Quantization Aware Training), Int8 Quantization, Pruning, Clustering, PCQAT, Sparsity-preserving clustering, CQAT |
| **Optimization Strategy** | Multi-Objective Optimization (Pareto Front Analysis) |
| **Key Metrics** | **Accuracy** (Standard), **Compression Rate (CR)**, **Robustness** (measured via mean Corruption Error - mCE) |

***

## Results

The analysis produced granular data regarding the performance of specific compression techniques across different architectures.

### General Performance
*   **Success Rate:** 69% of compressed solutions on CIFAR-100 achieved an mCE less than or equal to the baseline.
*   **Worst Performer:** Technique #10 (QAT Int8) yielded the worst robustness (highest mCE).
*   **Top Performers:** Techniques #5, #3, #7, and #9 were identified as robust leaders.

### Pareto Front Dominance
*   **ResNet-50:** Dominated the Pareto Front (71% representation on CIFAR-10, 56% on CIFAR-100).
*   **VGG-19:** Secondary dominance (29% representation on CIFAR-10, 44% on CIFAR-100).
*   **MobileNetV2:** 0% representation in Pareto-optimal solutions.

### Performance Extremes

#### **CIFAR-10**
*   **Best Robustness:** mCE 76.7 (ResNet-50, Technique #16)
*   **Best Compression:** CR 9.42 (VGG-19, Technique #16)
*   **Best Accuracy:** 94.34% (MobileNetV2, Technique #14)
*   **Worst Robustness:** mCE 201.7 (ResNet-50, Technique #10)

#### **CIFAR-100**
*   **Best Robustness:** mCE 87.5 (VGG-19, Technique #9)
*   **Best Compression:** CR 9.2 (VGG-19, Technique #16)
*   **Best Accuracy:** 77.8% (ResNet-50, Technique #11)
*   **Worst Robustness:** mCE 130.1 (MobileNetV2, Technique #10)

***

## Contributions

*   **Validation Protocol:** Established a holistic validation protocol that integrates robustness evaluation into the standard validation pipeline.
*   **Challenging Narratives:** Provided empirical evidence of compression-induced robustness, challenging conventional trade-off narratives.
*   **Deployment Guidelines:** Offered actionable guidelines for deploying deep learning models on resource-constrained edge devices subject to real-world corruption.
*   **Benchmarking Methodology:** Contributed a detailed benchmarking methodology for assessing the interplay between compression densities and corruption types.