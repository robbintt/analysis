# A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety

**Camille François; Ludovic Péran; Ayah Bdeir; Nouha Dziri; Will Hawkins; Yacine Jernite; Sayash Kapoor; Juliet Shen; Heidy Khlaaf; Kevin Klyman; Nik Marda; Marie Pellat; Deb Raji; Divya Siddarth; Aviya Skowron; Joseph Spisak; Madhulika Srikumar; Victor Storchan; Audrey Tang; Jen Weedon**

---

## Executive Summary

This research addresses the critical challenge of ensuring AI safety within the rapidly expanding ecosystem of open-weight and open-source foundation models, a domain where traditional safety paradigms—reliant on closed proprietary systems—are proving insufficient. While technical vulnerabilities such as prompt-injection and compositional attacks are prevalent, the paper highlights a deeper "social governance gap": there is currently insufficient infrastructure to engage the communities most affected by AI harms in the safety process. This problem is urgent because the explosive rise of decentralized AI development creates an intensified obligation to prevent systemic harms, necessitating a shift toward culturally plural oversight to ensure safety mechanisms are effective across diverse global contexts.

The key innovation is the redefinition of AI safety as a system-level property requiring interventions at deployment and integration layers rather than solely at the model level. This framework emerged from a multi-stakeholder convening and proposes a technical architecture centered on interoperable tooling and transparent weights. Moving beyond abstract concepts, the authors provide concrete deliverables, such as standardized APIs that facilitate independent scrutiny and decentralized mitigation. Specifically, the study produces a detailed "roadmap" for future-proof content safety filters and outlines rigorous technical safeguards for agentic systems, offering clear guidance for integrating safety into the entire AI development workflow.

The study quantifies the massive scale and industry validation of this domain, reporting an 880% increase in Generative AI model repositories on Hugging Face over two years, rising from 160,000 in January 2023 to 1.57 million in November 2024. Enterprise adoption metrics further confirm the trend, revealing that 46% of Fortune 500 leaders hold a strong preference for open-source models. Additionally, the research mobilized broad consensus within the technical community, garnering support from more than 1,800 experts via an open letter, which underscores the widespread demand for the specific safety frameworks and research agendas proposed.

The significance of this work lies in its direct influence on high-level policy and the strategic direction of safety research. The findings directly informed the agenda of the February 2025 French AI Action Summit, helping to lay the groundwork for an accountable AI safety discipline. By identifying five priority research directions—including ecosystem-wide safety infrastructure and expanded harm taxonomies—the paper shifts the narrative from "openness versus safety" to "openness as a safety enabler," providing developers and policymakers with a concrete framework for building modular, secure infrastructures.

---

### Quick Facts

| Metric | Statistic |
| :--- | :--- |
| **Model Repository Growth** | 880% increase (Jan 2023 – Nov 2024) |
| **Current Total Repositories (HF)** | 1.57 Million |
| **Fortune 500 Preference** | 46% prefer open-source models |
| **Expert Consensus** | over 1,800 signatories on supporting letter |
| **Active Contributors** | Over 45 experts in working groups |

---

## Key Findings

*   **Openness as a Safety Enabler:** The study concludes that openness—defined through transparent weights, interoperable tooling, and public governance—enhances safety by facilitating independent scrutiny, decentralized mitigation, and culturally plural oversight.
*   **Critical Technical and Social Gaps:** Significant vulnerabilities remain in the ecosystem, specifically the scarcity of multimodal and multilingual benchmarks, and limited defenses against prompt-injection and compositional attacks, particularly within agentic systems.
*   **Deficit in Participatory Mechanisms:** There is currently insufficient infrastructure to engage the communities most affected by AI harms in the safety process, highlighting a need for more inclusive governance.
*   **Evolving Safety Obligations:** The rise of open-weight and open-source foundation models is fundamentally reshaping the opportunity and intensifying the obligation to ensure AI systems are safe.

## Methodology

The paper utilizes a **participatory, solutions-oriented methodology** involving a multi-stakeholder convening. This process included:

*   **Preparatory Phase:** A six-week preparatory program.
*   **Convening Event:** A physical gathering titled the *Columbia Convening on AI Openness and Safety* held in San Francisco on November 19, 2024.
*   **Expert Engagement:** The study engaged over **forty-five experts** from diverse sectors—including academia, industry, civil society, and government.
*   **Collaborative Process:** Participants worked in specific groups to map ecosystems and define research agendas.

## Technical Details

The paper outlines a system-level safety architecture that relies on openness, redefining safety as a system property requiring interventions at deployment and integration layers rather than just at the model level.

**Core Architecture & Advocacy:**
*   **Open-Weight Models:** Necessary to enable transparent inspection.
*   **Interoperable Tooling:** Essential for facilitating a modular ecosystem.

**Identified Vulnerabilities:**
*   **Agentic Systems:** Limited defenses against prompt-injection and compositional attacks.
*   **Benchmarking Infrastructure:** Lack of sufficient multimodal and multilingual benchmarking tools.

## Contributions

*   **Integrated Research Agenda:** A defined research agenda specifically addressing the intersection of safety and open-source AI.
*   **Technical Intervention Mapping:** A comprehensive mapping of existing and required technical interventions and open-source tools designed for the responsible deployment of open foundation models across the entire AI development workflow.
*   **Content Safety Ecosystem Roadmap:** A detailed mapping of the current content safety filter ecosystem, accompanied by a proposed roadmap for future research and development.
*   **Five Priority Research Directions:** A strategic framework outlining five critical areas for advancement:
    1.  Participatory inputs
    2.  Future-proof content filters
    3.  Ecosystem-wide safety infrastructure
    4.  Rigorous agentic safeguards
    5.  Expanded harm taxonomies
*   **Policy Impact:** The findings directly informed the agenda of the February 2025 French AI Action Summit, laying the groundwork for an accountable AI safety discipline.

## Results

The paper reports on significant growth trends and adoption metrics within the AI ecosystem:

*   **Repository Growth:** An **880% increase** in Generative AI model repositories on Hugging Face over two years, growing from 160,000 in January 2023 to **1.57 million** in November 2024.
*   **Enterprise Adoption:** Metrics indicate that **46%** of Fortune 500 leaders have a strong preference for open-source models.
*   **Community Engagement:** The project realized high engagement levels, with over 45 experts participating in the research agenda and more than **1,800 experts** signing a supporting open letter.