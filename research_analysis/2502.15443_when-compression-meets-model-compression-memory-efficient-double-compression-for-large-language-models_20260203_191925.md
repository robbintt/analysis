---
title: 'When Compression Meets Model Compression: Memory-Efficient Double Compression
  for Large Language Models'
arxiv_id: '2502.15443'
source_url: https://arxiv.org/abs/2502.15443
generated_at: '2026-02-03T19:19:25'
quality_score: 8
citation_count: 7
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models

*Weilan Wang; Yu Mao; Dongdong Tang; Hongchao Du; Nan Guan; Chun Jason Xue*

---

> ### ðŸ“Š Quick Facts
>
> *   **Compression Ratio:** Approx. **2.2x**
> *   **Memory Reduction:** **40%** decrease during inference
> *   **Performance:** Negligible loss in accuracy and inference speed
> *   **Key Innovation:** Double Compression (DC) framework with Speed-Adaptive Inference
> *   **Core Algorithm:** Zstd (lossless) + INT8 Quantization

---

## Executive Summary

Deploying Large Language Models (LLMs) on memory-constrained edge devices requires balancing storage efficiency with inference speed. While quantization is the standard method for reducing model size, it often fails to meet strict storage limits alone. Adding further lossless compression creates a critical decompression bottleneck, where the time required to expand data severely degrades inference latency. This paper addresses the conflict between maximizing memory compression and maintaining acceptable inference speeds, a challenge that currently hinders the deployment of larger LLMs on resource-constrained hardware.

The authors propose a "Double Compression" (DC) framework that integrates offline model optimization with online runtime management. The core innovation lies in "Compression-Aware Quantization," which modifies weight distributions prior to quantization using a specific parameter re-scaling formula. This pre-processing shapes the data to be more susceptible to lossless compression algorithms like Zstd. Additionally, the framework employs "Compression-Aware Pruning," utilizing the $l$-infinite norm of activations to strategically prune weights, thereby increasing the frequency of near-zero values (sparsity) and further boosting compressibility. To address the resulting decompression latency, the authors introduce a "Speed-Adaptive Inference" strategy. This mechanism performs bottleneck analysis to dynamically adjust compression levels based on real-time system memory bandwidth, ensuring that decompression overhead does not negate the storage benefits.

The DC framework achieved an overall compression ratio of approximately 2.2x and a 40% reduction in memory footprint during inference, with negligible losses in accuracy or speed. Baseline comparisons highlighted the necessity of the proposed methods: while activations naturally compressed well (CR > 4.0), standard weights were resistant (OPT-1.3B CR of 1.54). A naive scaling approach improved weight CR to 2.46 but caused a drastic drop in accuracy (from 0.57 to 0.32); the compression-aware approach successfully recovered this accuracy loss while maintaining high compression. The framework also proved effective in optimizing data transfers, specifically in CPU-to-GPU memory swapping scenarios.

This research establishes a viable "double compression" paradigm that successfully bridges the gap between model quantization and algorithmic data compression. By mitigating the decompression bottleneck through speed-adaptive mechanisms, the framework enables the deployment of larger LLMs on hardware previously incapable of supporting them. This offers a practical pathway for researchers and engineers to manage both storage constraints and memory bandwidth in edge computing environments, facilitating more efficient and capable on-device AI applications.

---

## Key Findings

*   **High Compression Efficiency:** Achieved a significant compression ratio of approximately **2.2x** by applying the novel framework to quantized LLMs.
*   **Memory Footprint Reduction:** Demonstrated a **40% reduction** in memory size during inference, making models significantly more viable for memory-limited devices.
*   **Performance Preservation:** The proposed method incurs negligible loss in both accuracy and inference speed, overcoming traditional trade-offs.
*   **Bottleneck Mitigation:** Successfully identified that decompression processes can act as a latency bottleneck and mitigated this issue through a speed-adaptive approach.

---

## Methodology

The research follows a structured pipeline designed to maximize compressibility while maintaining model integrity:

1.  **Compression-Aware Quantization**
    *   The framework begins by re-scaling model parameters prior to quantization to alter their distribution for better compression.
2.  **Double Compression Pipeline**
    *   A pruning method is applied to the quantized model, creating a secondary layer of compression.
3.  **Bottleneck Analysis & Optimization**
    *   The authors conducted a rigorous analysis of the memory-latency trade-off to identify potential slowdowns.
4.  **Speed-Adaptive Method**
    *   Implementation of a dynamic mechanism to balance decompression demands with real-time inference speed requirements.

---

## Technical Details

The paper proposes a **'Double Compression' (DC)** framework that applies lossless compression to quantized LLMs via a two-stage pipeline: offline model optimization and online runtime inference.

### Core Components

*   **Per-Channel Scaling**
    *   Modifies weight distributions for higher compressibility using the formula:
        $$s = \max(|X_i|)^{\alpha}$$
    *   Followed by INT8 quantization to standardize precision.
*   **Compression-Aware Pruning**
    *   Utilizes the $l$-infinite norm of activations to strategically prune parameters.
    *   Increases near-zero values to enhance the efficiency of subsequent lossless compression.
*   **Lossless Compression Algorithm**
    *   Employs the **Zstd** algorithm for the actual compression of the optimized model.
*   **Speed-Adaptive Inference Strategy**
    *   Dynamically manages compression levels based on system memory bandwidth constraints.
    *   Optimizes scenarios involving CPU<->GPU swapping to prevent I/O bottlenecks.

---

## Results

*   **Overall Performance:** The approach achieved an approximate **2.2x compression ratio** and a **40% reduction** in memory footprint with negligible loss in accuracy and inference speed.
*   **Baseline Analysis:**
    *   **Activations:** High compressibility with a Compression Ratio (CR) > 4.0.
    *   **Weights:** Lower baseline CRs (OPT-1.3B: 1.54, OPT-2.7B: 1.38).
*   **Impact of Scaling:**
    *   Scaling improved OPT-1.3B weight CR to 2.46 but reduced accuracy significantly (from 0.57 to 0.32) without the compression-aware approach.
*   **Data Distribution Characteristics:**
    *   Activations possess ~58% near-zero values and a range 40x larger than weights (~6% near-zero).
*   **Adaptive Optimization:** The framework proved effective in optimizing data transfers, specifically in CPU-to-GPU memory swapping scenarios.

---

## Contributions

*   **Post-Quantization Compression Framework:** Introduced a framework featuring a 'double compression' strategy specifically designed for quantized LLMs.
*   **Parameter Re-scaling Technique:** Proposed a technique that modifies parameter distribution before quantization to improve compressibility.
*   **Latency-Aware Optimization:** Contributed a solution to the decompression bottleneck problem by introducing a speed-adaptive mechanism that responds to system constraints.

---

**Quality Score:** 8/10  
**References:** 7 citations