---
title: 'MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models
  via Expert-Balanced Sampling and Affinity Guidance'
arxiv_id: '2505.03804'
source_url: https://arxiv.org/abs/2505.03804
generated_at: '2026-02-03T19:23:47'
quality_score: 8
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance

*Authors: Xing Hu; Zhixuan Chen; Dawei Yang; Zukang Xu; Chen Xu; Zhihang Yuan; Sifan Zhou; Jiangyong Yu*

---

> ### ðŸ“Š Quick Facts
> *   **Primary Achievement:** >10 point accuracy gain on HumanEval for DeepSeekMoE-16B at 4-bit.
> *   **Core Innovation:** First PTQ framework specifically designed for MoE architectures.
> *   **Efficiency:** >3.2Ã— memory reduction and >1.2Ã— inference speedup (up to 3.74Ã— and 2.08Ã— respectively on Mixtral).
> *   **Target Models:** DeepSeekMoE-16B, Mixtral-8x7B.
> *   **Quality Score:** 8/10

---

## Executive Summary

Standard Post-Training Quantization (PTQ) techniques, while highly effective for dense Large Language Models (LLMs), suffer severe accuracy degradation when applied to Mixture-of-Experts (MoE) architectures. This paper addresses the fundamental challenge that the sparse and dynamic routing mechanisms in MoE models create distributional biases that standard quantizers cannot handle.

Specifically, the authors identify **Inter-expert imbalance** (uneven distribution of calibration samples across experts) and **Intra-expert imbalance** (varying correlation degrees between samples and experts) as the root causes of performance collapse. Solving this is critical for enabling the memory-efficient deployment of state-of-the-art MoE models without sacrificing reasoning capabilities.

The authors propose **MoEQuant**, the first PTQ framework specifically tailored for MoE LLMs. To correct calibration data bias, MoEQuant introduces **Expert-Balanced Self-Sampling (EBSS)**, an algorithm that constructs a balanced calibration set by analyzing cumulative probabilities and expert balance metrics. To handle the varying correlations within experts, the framework utilizes **Affinity-Guided Quantization (AGQ)**, which incorporates expert-sample affinities into the quantization process.

Empirical evaluations demonstrate that MoEQuant significantly outperforms established baselines like RTN, AWQ, and GPTQ. The ability to run models like DeepSeekMoE and Mixtral at 4-bit precision with minimal accuracy loss paves the way for significantly more accessible and cost-effective inference on consumer-grade and enterprise hardware.

---

## Key Findings

*   **Critical Diagnosis:** The study identifies that the sparse and dynamic characteristics of MoE models lead to severe accuracy degradation during Post-Training Quantization (PTQ).
*   **Root Causes:** Degradation is attributed to two specific imbalances caused by routing mechanisms:
    *   **Inter-expert imbalance:** Uneven distribution of samples across experts.
    *   **Intra-expert imbalance:** Varying degrees of correlation between samples and experts.
*   **Significant Performance Boost:** The proposed MoEQuant framework achieves a gain of over **10 points** in accuracy on the HumanEval benchmark for the DeepSeekMoE-16B model under 4-bit quantization compared to general-purpose methods.
*   **Efficiency Validated:** The approach enables substantial memory savings and speedups without requiring expensive Quantization-Aware Training (QAT).

---

## Technical Details

**MoEQuant** is a Post-Training Quantization (PTQ) framework designed for Mixture-of-Experts (MoE) Large Language Models. It aims to compress models to lower bitwidths (e.g., 4-bit) while specifically addressing the accuracy degradation typical in sparse models.

### Core Challenges Addressed
1.  **Inter-expert Imbalance:** The uneven distribution of calibration samples across experts.
2.  **Intra-expert Imbalance:** The varying correlation or affinity between samples and experts.

### Key Components
| Component | Function | Description |
| :--- | :--- | :--- |
| **EBSS** | Calibration Sampling | **Expert-Balanced Self-Sampling** handles inter-expert imbalance by constructing a balanced calibration set using cumulative probabilities and expert balance metrics. |
| **AGQ** | Quantization Strategy | **Affinity-Guided Quantization** handles intra-expert imbalance by incorporating expert-sample affinities to accurately assess the impact of samples on different experts. |

### Framework Variants
*   **MoEQuant+:** Standard configuration.
*   **MoEQuant++:** Top-performing configuration with enhanced algorithms.

---

## Methodology

The authors propose a dual-strategy approach to mitigate the specific failures of standard PTQ in MoE architectures:

1.  **Expert-Balanced Self-Sampling (EBSS):**
    *   Addresses the bias where certain experts are over-represented in the calibration data.
    *   Utilizes cumulative probabilities and specific metrics to ensure a uniform distribution of samples across all experts during the calibration phase.

2.  **Affinity-Guided Quantization (AGQ):**
    *   Addresses the issue where standard quantization treats all sample-weights equally.
    *   Incorporates "expert-sample affinities" to weigh the quantization error, ensuring that samples with high correlation to a specific expert have a larger impact on that expert's quantization parameters.

---

## Results

Experiments were conducted on **Nvidia A6000 GPUs** comparing MoEQuant against FP16, RTN, AWQ, and GPTQ baselines at 4-bit precision.

### Performance on HumanEval (4-bit)

**DeepSeek-MoE-16B**
*   **MoEQuant++:** 36.47
*   GPTQ: 35.85
*   AWQ: 22.20
*   RTN: 20.17
*   *Result: >10 point gain over general-purpose methods.*

**Mixtral-8x7B**
*   **MoEQuant++:** 49.75
*   GPTQ: 45.03
*   AWQ: 36.05
*   RTN: 18.64
*   FP Baseline: 56.80

### Efficiency Metrics

Compared to FP16 baselines, MoEQuant delivers substantial hardware efficiency improvements:

*   **Average Inference Speedup:** >1.2Ã—
*   **Average Memory Savings:** >3.2Ã—

**Specific Efficiency on Mixtral-8x7B:**
*   **Speedup:** 2.08Ã—
*   **Memory Reduction:** 3.74Ã—

---

## Contributions

The main academic and practical contributions of this paper include:

1.  **Comprehensive Diagnosis:** Providing the first detailed categorization of how MoE routing mechanisms negatively impact PTQ, defining the concepts of Inter- and Intra-expert imbalances.
2.  **Framework Introduction:** Introducing **MoEQuant**, the first quantization framework specifically architected for MoE LLMs.
3.  **Algorithm Development:** Developing the **EBSS** and **AGQ** algorithms to solve calibration data bias and manage complex sample-expert correlations.
4.  **Empirical Validation:** Validating the approach on large-scale models (DeepSeekMoE-16B) to prove that efficient 4-bit quantization is achievable without QAT.

---

**Quality Score:** 8/10  
**References:** 26 citations