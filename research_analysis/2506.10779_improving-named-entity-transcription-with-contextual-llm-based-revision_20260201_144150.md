# Improving Named Entity Transcription with Contextual LLM-based Revision

*Viet Anh Trinh; Xinlu He; Jacob Whitehill*

---

> **QUICK FACTS**
>
> *   **Performance:** 30% relative reduction in Named Entity WER
> *   **Precision:** 96%
> *   **Recall:** 86%
> *   **Dataset:** NER-MIT-OpenCourseWare (45 hours of lecture data)
> *   **Core Tech:** Whisper (ASR) + Flair (NER) + Double Metaphone + LLM Revision

---

## Executive Summary

State-of-the-art Automatic Speech Recognition (ASR) systems have achieved high accuracy on general transcription tasks, yet they continue to struggle significantly with the transcription of named entities (NEs). NEs—such as person names, organizations, and specialized terminology—exhibit disproportionately high word error rates (WER) compared to general vocabulary. This issue is not merely a minor technical inconvenience; because NEs often serve as the critical information-bearing keywords in speech, their misrecognition severely degrades the performance of downstream applications, including information retrieval, summarization, and knowledge graph extraction.

The authors propose a novel **contextual revision framework** that moves away from correcting ASR errors during recognition and instead treats transcription as a post-editing task. The core innovation lies in utilizing Large Language Models (LLMs) to revise ASR hypotheses by grounding them in "local context"—specifically, accompanying textual documents such as lecture slides or presentation notes.

Technically, the pipeline employs a multi-stage architecture: **OpenAI Whisper** generates the initial transcript, which is then segmented and analyzed using the **Flair NER model** to extract entities. A key technical component is the use of the **Double Metaphone** phonetic algorithm to match the phonetic representation of spoken entities in the audio against the textual entities found in the source documents. This allows the system to validate candidates before passing them to an LLM (e.g., **GPT-4o** or **Llama-3**) for final reasoning and correction.

Evaluated on the newly introduced **NER-MIT-OpenCourseWare dataset**, the method achieved substantial improvements, including a **30% relative reduction in WER** for named entities. Granular analysis revealed a precision of 96% and a recall of 86%. This work establishes a new paradigm for improving ASR accuracy by decoupling transcription from semantic refinement, offering a scalable solution for domain-specific applications where vocabulary accuracy is paramount.

---

## Key Findings

*   **The ASR Bottleneck:** State-of-the-art ASR systems, while effective on general speech, suffer from high word error rates (WER) specifically when transcribing named entities.
*   **Downstream Impact:** Misrecognition of named entities is detrimental because these terms are critical keywords; errors negatively impact applications relying on ASR as a front-end.
*   **LLM Solution:** A Large Language Model (LLM) revision mechanism can significantly reduce the WER of named entities by leveraging the model's reasoning capabilities and external local context.
*   **Measurable Gain:** The proposed technique achieved a performance improvement of up to **30% relative WER reduction** for named entities on the evaluated dataset.

---

## Methodology

The authors utilize a Large Language Model (LLM) specifically to **revise incorrect named entities** within existing ASR predictions, rather than generating the transcript from scratch.

1.  **Contextual Grounding:** The mechanism leverages the LLM's reasoning abilities in conjunction with "local context"—such as lecture notes or accompanying documents that contain the set of correct named entities.
2.  **Validation:** The system validates and corrects transcriptions by cross-referencing the audio hypothesis with the text context.
3.  **Dataset:** The method was developed and tested using the newly introduced **NER-MIT-OpenCourseWare dataset**, comprising 45 hours of MIT course data.

---

## Technical Details

### Pipeline Architecture
*   **Type:** Multi-stage pipeline involving:
    1.  Audio preprocessing
    2.  Initial transcription
    3.  Context extraction
    4.  LLM-based revision

### Data Preprocessing & Segmentation
*   **Slide Text Processing:** `pdfplumber` for PDF to text conversion; `spacy` (en_core_web_lg) for cleaning and sentence splitting.
*   **Audio Processing:** `pydub` for extraction (FLAC from MP4), converted to mono 16,000 Hz.
*   **Segmentation:** `fairseq` toolkit used to segment 30–60 minute audio files into utterances based on Spacy sentences.

### Initial Speech Recognition & NER
*   **ASR Model:** OpenAI `Whisper large-v3`.
*   **NER Model:** `Flair` (flair/ner-english-ontonotes-large).
*   **Phonetic Matching:** Double Metaphone algorithm to match entities from slides against ASR output.
*   **Entity Types Tracked:** PERSON, ORG, GPE, LOC, PRODUCT, EVENT, NORP, FAC.

### Revision Mechanism
*   **Framework:** `Langchain`.
*   **LLMs Tested:**
    *   Llama3-70b-8192
    *   GPT-4o-mini
    *   GPT-4o
    *   Meta-Llama-3.1-70B
*   **Local Deployment:** Meta-Llama-3.1-70B with bitsandbytes quantization on NVIDIA A100 (80GB) GPU using `vLLM`.

---

## Contributions

*   **Novel Framework:** Introduction of a contextual LLM-based revision approach designed to address the specific challenge of named entity accuracy in ASR outputs.
*   **New Benchmark Dataset:** The release of the **NER-MIT-OpenCourseWare dataset**, a resource providing 45 hours of data from MIT courses specifically for the development and testing of named entity recognition in speech.
*   **Quantitative Performance Gains:** Demonstration of a substantial improvement in transcription accuracy for complex terms, with a 30% relative reduction in WER for named entities.

---

## Results

| Metric Category | Details |
| :--- | :--- |
| **Overall Performance** | Up to **30%** relative reduction in Word Error Rate (WER) for named entities. |
| **NER Metrics** | **Precision:** 96% <br> **Recall:** 86% |
| **Evaluation Dataset** | Validation set of 500 sentences manually verified. |
| **Metric Type** | Set-based evaluation recall. |
| **Baseline Comparisons** | Performance measured against "Full Context" (entire document) and "Summarized Context" strategies. |

---

**Quality Score:** 9/10