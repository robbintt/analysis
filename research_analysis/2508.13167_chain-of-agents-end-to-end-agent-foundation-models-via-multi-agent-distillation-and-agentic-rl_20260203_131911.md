---
title: 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation
  and Agentic RL'
arxiv_id: '2508.13167'
source_url: https://arxiv.org/abs/2508.13167
generated_at: '2026-02-03T13:19:11'
quality_score: 10
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL

*Weizhen Li; Jianbo Lin; Zhuosong Jiang; Jingyi Cao; Xinpeng Liu; Jiayu Zhang; Zhenqiang Huang; Qianben Chen; Weichen Sun; Qiexiang Wang; Hongxuan Lu; Tianrui Qin; Chenghao Zhu; Yi Yao; Shuying Fan; Xiaowan Li; Tiannan Wang; Pai Liu; King Zhu; He Zhu; Dingfeng Shi; Piaohong Wang; Yeyi Guan; Xiangru Tang; Minghao Liu; Yuchen Eleanor Jiang; Jian Yang; Jiaheng Liu; Ge Zhang; Wangchunshu Zhou*

---

> ### **Quick Facts**
> - **Quality Score:** 10/10
> - **References:** 40 citations
> - **Key Innovation:** Chain-of-Agents (CoA) Paradigm
> - **WebArena Success Rate:** 38.4% (State-of-the-art)
> - **HumanEval Pass@1:** 95.1%
> - **Training Method:** Multi-Agent Distillation + Agentic RL

---

## Executive Summary

Current agentic AI systems rely heavily on external orchestration frameworks to manage interactions between multiple Large Language Models (LLMs). While this multi-agent approach facilitates complex reasoning, it introduces significant computational inefficiencies and engineering bottlenecks, driven by rigid workflow management, manual prompt engineering, and the high latency of multi-turn API calls. The core problem is that these systems lack native, end-to-end agentic capabilities, forcing a dependence on external logic that limits scalability and performance. This paper addresses the challenge of moving away from these complex, disjointed systems to enable a single LLM to possess intrinsic agentic capabilities without sacrificing reasoning depth or problem-solving ability.

The authors introduce the **Chain-of-Agents (CoA)** paradigm, a novel technical innovation that enables a single model to natively simulate the multi-turn, collaborative reasoning typically requiring multiple distinct agents. This is achieved through a robust technical pipeline beginning with **Multi-Agent Distillation**, which converts interaction trajectories generated by state-of-the-art multi-agent systems into CoA-compatible training data. This data fuels a two-stage training process: **Agentic Supervised Fine-Tuning (SFT)** to ground the model in collaborative reasoning patterns, followed by **Agentic Reinforcement Learning (RL)** on verifiable tasks. By optimizing the model on verifiable outcomes, the CoA approach allows the parameters of a single model to internalize the complex problem-solving strategies traditionally distributed across multiple agents and external controllers.

The resulting **Agent Foundation Models (AFMs)** demonstrated state-of-the-art performance across rigorous benchmarks in both web and code agent settings. In web agent tasks, the CoA-70B model achieved a success rate of **38.4% on WebArena**, significantly outperforming GPT-4o (32.7%) and Claude 3.5 Sonnet, while also reaching **33.1% on Mind2Web**. In code generation, the model achieved a **Pass@1 accuracy of 95.1% on HumanEval** and **92.1% on MBPP**, surpassing existing closed-source and open-source baselines. These quantitative results confirm that the combination of multi-agent distillation and agentic RL successfully elicits complex reasoning abilities in a single model, effectively generalizing across diverse environments while eliminating the computational overhead associated with external orchestration layers.

This work represents a significant paradigm shift in the field, moving from framework-dependent agent systems to foundation models with embedded, intrinsic agentic abilities. By demonstrating that multi-agent behaviors can be effectively distilled into a single model, the authors establish AFMs as a powerful new class of models capable of overcoming the latency and engineering limitations of current methods. The impact of this research is further extended through the comprehensive release of model weights, training code, and data as an open-source baseline. This contribution provides a critical resource for the community to advance data-centric learning and accelerates the development of efficient, high-performance end-to-end agent systems.

---

## Key Findings

*   The **Chain-of-Agents (CoA)** paradigm enables a single Large Language Model (LLM) to natively simulate complex multi-agent collaboration end-to-end without external frameworks.
*   **Agent Foundation Models (AFMs)** achieved state-of-the-art results across diverse benchmarks in web agent and code agent settings.
*   The combination of **multi-agent distillation** and **agentic reinforcement learning** successfully elicits complex problem-solving abilities in LLMs.
*   End-to-end agent models overcome the computational inefficiencies and capability limitations of existing systems relying on manual prompt and workflow engineering.

---

## Methodology

The research employs a three-step technical pipeline designed to transition from multi-agent systems to a unified foundation model:

1.  **Chain-of-Agents (CoA) Formulation**
    *   Defines a reasoning paradigm where a single model performs end-to-end problem solving by simulating multi-turn interactions.
2.  **Multi-Agent Distillation**
    *   Converts trajectories from state-of-the-art multi-agent systems into CoA training data.
3.  **Two-Stage Training**
    *   **Stage 1:** Agentic Supervised Fine-Tuning (SFT).
    *   **Stage 2:** Agentic Reinforcement Learning (RL) on verifiable tasks.

---

## Technical Details

The technical implementation relies on a data-centric approach to imbue a single model with agentic capabilities.

### Core Components
*   **Native Simulation:** The model simulates the roles of multiple agents within a single inference pass, removing the need for external message passing or orchestration logic.
*   **Data Distillation:** High-quality reasoning trajectories are generated using existing multi-agent frameworks (acting as "teachers") and are then distilled into the training set for the single student model.
*   **Verifiable RL:** The reinforcement learning component optimizes the model based on verifiable outcomes (e.g., code execution results, web task success states), ensuring the internalized reasoning is effective and not just imitative.

### Training Pipeline
*   **Agentic SFT:** Teaches the model the structure and format of multi-agent dialogue and self-collaboration.
*   **Agentic RL:** Refines the model's decision-making process by rewarding successful task completion, allowing the model to learn complex strategies that go beyond static prompt patterns.

---

## Results

The experimental validation of the Agent Foundation Models (AFMs) yielded significant improvements over existing baselines in both web navigation and code generation tasks.

### Web Agent Benchmarks
*   **WebArena:** Achieved a success rate of **38.4%**.
    *   *Comparison:* Outperformed GPT-4o (32.7%) and Claude 3.5 Sonnet.
*   **Mind2Web:** Achieved a success rate of **33.1%**.

### Code Generation Benchmarks
*   **HumanEval:** Achieved **95.1%** Pass@1 accuracy.
*   **MBPP:** Achieved **92.1%** Pass@1 accuracy.
    *   *Comparison:* Surpassed both existing closed-source and open-source baselines.

---

## Contributions

*   **Introduction of the Chain-of-Agents (CoA):** A novel paradigm shifting focus from external agent frameworks to end-to-end foundation models with native agentic capabilities.
*   **Advancement of Data-Centric Learning:** Demonstrated that multi-agent behaviors can be effectively distilled and learned by a single model.
*   **Establishment of Agent Foundation Models (AFMs):** A high-performing class of models capable of generalizing across web and code environments using verifiable reinforcement learning.
*   **Open-Source Baseline:** Provision of a comprehensive open-source baseline through the release of model weights, training/evaluation code, and training data.