# Reinforcement Learning with Continuous Actions Under Unmeasured Confounding

*Yuhan Li; Eugene Han; Yifan Hu; Wenzhuo Zhou; Zhengling Qi; Yifan Cui; Ruoqing Zhu*

---

> ### üìä **Quick Facts**
> *   **Quality Score:** 6/10
> *   **References:** 13 Citations
> *   **Problem Space:** Offline Policy Learning
> *   **Key Challenges:** Unmeasured Confounding, Continuous Actions, Infinite-Horizon
> *   **Dataset:** German Family Panel

---

## üí° Executive Summary

This research tackles the intricate challenge of offline policy learning in Reinforcement Learning (RL) within environments characterized by three simultaneous complexities: **continuous action spaces**, **unmeasured confounding**, and **infinite-horizon frameworks**. Standard RL algorithms often fail in high-stakes domains like personalized medicine and economics because they assume all relevant variables are observed (no confounding) or are limited to discrete actions. When unmeasured confounders‚Äîhidden variables influencing both actions and outcomes‚Äîare present, standard models learn spurious correlations, leading to dangerously ineffective policies. This work is critical because it addresses this "trifecta" of issues, enabling reliable decision-making in observational data where hidden biases and continuous interventions (like drug dosages) are the norm rather than the exception.

**Core Innovation**
The core innovation is the development of a novel identification strategy within a **Confounded Partially Observable Markov Decision Process (POMDP)** framework, leveraging proximal causal inference to mathematically de-confound policy values. The authors introduce a **"Q-bridge" function** that isolates causal effects by utilizing two types of proxy variables:
*   **Reward-inducing proxies** ($W_t$)
*   **Action-inducing proxies** ($Z_t$), constructed from lagged state-action pairs.

To ensure stability over an infinite horizon, the method employs a geometric discount factor within the Bellman equation and imposes strict regularization in the estimation process. This mechanism prevents the accumulation of errors and ensures the value function remains bounded‚Äîa common failure point in long-horizon settings. The Q-bridge is operationalized through a **minimax estimator** that minimizes the loss between estimated and observed values while adhering to moment constraints, effectively navigating the continuous action space to optimize the policy.

**Quantitative & Empirical Outcomes**
The study provides rigorous quantitative guarantees, proving that the proposed estimators are statistically consistent with established finite-sample bounds.
*   **Convergence:** The method achieves a minimax optimal convergence rate of $\tilde{O}(n^{-1/2})$ for policy evaluation.
*   **Regret:** The authors derive a sublinear regret bound for the learning process, theoretically guaranteeing that the agent's performance converges to the optimal policy over time.
*   **Real-world Application:** In empirical evaluations, the algorithm demonstrated superior stability in simulations where standard baselines diverged. When applied to the **German Family Panel dataset**, the approach reduced policy value estimation errors and improved convergence rates compared to methods that cannot handle both continuous actions and hidden confounders.

---

## üîç Key Findings

*   **Novel Identification:** Established a nonparametric estimation capability for policy values in continuous action spaces with unmeasured confounders within an infinite-horizon framework.
*   **Algorithmic Development:** Developed a policy-gradient-based algorithm designed to identify the in-class optimal policy that maximizes the estimated policy value.
*   **Statistical Guarantees:** The methodology yields statistically consistent estimators, accompanied by proven theoretical bounds on finite-sample error and regret.
*   **Empirical Validation:** The approach demonstrated high effectiveness in extensive simulation studies and a real-world application using German Family Panel data.

---

## üìã Methodology

The research addresses offline policy learning with unmeasured confounders through a structured two-step methodological framework:

1.  **Theoretical Identification:** First, the authors establish a theoretical identification result for policy values specifically tailored for continuous action spaces.
2.  **Minimax Estimation:** The first step in the operational framework involves developing a minimax estimator to approximate the policy value nonparametrically.
3.  **Policy Optimization:** The second step introduces a policy-gradient-based algorithm designed to search the policy class and identify the policy that maximizes the estimated value within an infinite-horizon setting.

---

## ‚öôÔ∏è Technical Details

**Framework & Variables**
*   **Model:** Confounded Partially Observable Markov Decision Process (POMDP).
*   **State Distinction:** Explicitly distinguishes between unobserved states ($S$) and observed states ($O$).

**Inference Strategy**
*   **Approach:** Utilizes **proximal causal inference** to address unmeasured confounding.
*   **Proxies:**
    *   **Reward-inducing proxies:** Denoted as $W_t$.
    *   **Action-inducing proxies:** Denoted as $Z_t$ (represented by previous state-action pairs).

**Estimation Process**
*   **Function:** Policy value is identified via a **Q-bridge function**.
*   **Optimization:** Estimated using a minimax optimization problem.
*   **Loss Function:** Minimizes loss derived from residuals between estimated Q-values and observed rewards plus discounted future values, subject to regularization.

---

## üèÜ Contributions

This work advances the field of reinforcement learning and causal inference significantly:

*   **Beyond Discrete Actions:** Tackles policy learning for continuous action spaces under unmeasured confounding, moving beyond the current literature focused on discrete action spaces.
*   **Infinite-Horizon Solution:** Provides a viable solution for infinite-horizon reinforcement learning settings, unlike approaches limited to single-step or finite-horizon scenarios.
*   **Complete Pipeline:** Contributes a full statistical pipeline ranging from nonparametric identification and minimax estimation to optimization with policy gradients.
*   **Rigorous Analysis:** Accompanies the methodology with robust theoretical analysis, including proofs of consistency and regret bounds.

---

## üìà Results

The proposed method was rigorously evaluated through both simulation and real-world data:

*   **Theoretical Guarantees:** Established statistical consistency, finite-sample error bounds, and regret bounds.
*   **Simulation Studies:** The method proved effective in optimizing policies in continuous action spaces despite the presence of unmeasured confounding.
*   **Real-world Application:** Successfully applied to the **German Family Panel** data, validating the practical utility of the approach.

---

## üîó References

*   **Total Citations:** 13