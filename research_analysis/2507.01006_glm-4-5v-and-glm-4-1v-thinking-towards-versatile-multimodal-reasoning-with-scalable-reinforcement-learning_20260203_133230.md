---
title: 'GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with
  Scalable Reinforcement Learning'
arxiv_id: '2507.01006'
source_url: https://arxiv.org/abs/2507.01006
generated_at: '2026-02-03T13:32:30'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning

*GLM-V Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Haochen Li, Jiale Zhu, Jiali Chen, Jiaxing Xu, Jiazheng Xu, Jing Chen, Jinghao Lin, Jinhao Chen, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Ruiliang Lyu, Shangqin Tu, Sheng Yang, Shengbiao Meng, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wei Jia, Wenkai Li, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyu Zhang, Xinyue Fan, Xuancheng Huang, Yadong Xue, Yanfeng Wang, Yanling Wang, Yanzi Wang, Yifan An, Yifan Du, Yiheng Huang, Yilin Niu, Yiming Shi, Yu Wang, Yuan Wang, Yuanchang Yue, Yuchen Li, Yusen Liu, Yutao Zhang, Yuting Wang, Yuxuan Zhang, Zhao Xue, Zhengxiao Du, Zhenyu Hou, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **State-of-the-Art** | 42 Benchmarks (Open-source) |
| **Context Window** | Up to 128K Tokens |
| **Key Innovation** | Reinforcement Learning with Curriculum Sampling (RLCS) |
| **Model Sizes** | 9B (Thinking), 106B Total / 12B Active (4.5V MoE) |
| **Training Data** | >10 Billion image-text pairs |

---

## üìù Executive Summary

Existing Vision-Language Models (VLMs) struggle to achieve versatile, complex reasoning capabilities across diverse domains such as STEM, video understanding, and real-world grounding. Furthermore, there is a persistent performance gap between open-source models and proprietary closed-source systems, with smaller models often unable to match the reasoning depth of their larger counterparts due to inefficient training paradigms. This paper addresses the need for a scalable training methodology that can unlock latent reasoning potential in VLMs without relying solely on massive parameter scaling.

The core innovation is the introduction of the **GLM-4.xV family** and a novel training technique termed **Reinforcement Learning with Curriculum Sampling (RLCS)**. The architecture utilizes a three-component design: an AIMv2-Huge Vision Encoder (supporting arbitrary resolutions via 2D-RoPE), an MLP Adapter, and a GLM-4 LLM Decoder enhanced with 3D-RoPE for temporal video processing. The training pipeline comprises four stages: large-scale multimodal pre-training (using over 10 billion image-text pairs), long-context continual training (up to 128K tokens), Supervised Fine-Tuning (SFT), and the critical RLCS stage. RLCS is designed to progressively challenge the model with complex tasks, effectively activating reasoning capabilities that standard pre-training fails to elicit.

**GLM-4.5V**, a Mixture-of-Experts (MoE) architecture with 106 billion total parameters (12 billion active), achieved state-of-the-art results among open-source models across 42 benchmarks. Notably, it outperformed the closed-source Gemini-2.5-Flash in Coding and GUI Agent tasks. Demonstrating high efficiency, the smaller **GLM-4.1V-9B-Thinking** model outperformed the significantly larger Qwen2.5-VL-72B on 29 benchmarks. Additionally, the GLM-4.1V-9B-Base model demonstrated superior pre-training efficacy on the MathVista pass@k metric, validating the critical role of the vision foundation model in setting performance upper bounds.

This research significantly narrows the gap between open and proprietary AI systems by releasing high-performance models that rival or exceed commercial alternatives in specific domains. The validation of RLCS offers a scalable new paradigm for enhancing multimodal reasoning, suggesting that complex behaviors can be unlocked through structured reinforcement learning. By demonstrating that a 9-billion parameter model can outperform a 72-billion parameter model on the majority of benchmarks, the study shifts the industry focus toward architectural efficiency and advanced training curricula, democratizing access to state-of-the-art multimodal agents capable of complex tool use and long-context understanding.

---

## üîç Key Findings

*   **SOTA Performance:** GLM-4.5V achieves state-of-the-art results among open-source models across **42 benchmarks** and outperforms closed-source models like **Gemini-2.5-Flash** in Coding and GUI Agent tasks.
*   **Superior Efficiency:** The smaller **GLM-4.1V-9B-Thinking** model outperforms the significantly larger **Qwen2.5-VL-72B** on 29 benchmarks, demonstrating high reasoning efficiency.
*   **RLCS Efficacy:** Reinforcement Learning with Curriculum Sampling (RLCS) successfully unlocks the model's potential, enhancing performance in STEM, video understanding, and grounding.
*   **Foundation Importance:** Large-scale pre-training of the vision foundation model is identified as critical for setting the upper bound of final performance.

---

## üß† Methodology

The study utilized a reasoning-centric framework featuring a two-stage training pipeline:

1.  **Stage One:** Large-scale pre-training to develop a capable vision foundation model.
2.  **Stage Two:** Application of **Reinforcement Learning with Curriculum Sampling (RLCS)** to activate the model's potential and improve generalization across complex tasks.

---

## ‚öôÔ∏è Technical Details

### Architecture
The GLM-V series utilizes a three-component VLM architecture:
*   **Vision Encoder:** AIMv2-Huge (supports arbitrary resolutions via 2D-RoPE and bicubic interpolation).
*   **Adapter:** MLP Adapter.
*   **LLM Decoder:** GLM-4-9B-0414 or GLM-4.5-Air, enhanced with **3D-RoPE**.
*   **Video Processing:** Employs 3D convolutions and Time Index Tokens.

### Training Pipeline
The training process follows a rigorous four-stage pipeline:
1.  **Multimodal Pre-training:** Utilizes over 10 billion image-text pairs.
2.  **Long-context Continual Training:** Extends context capability.
3.  **Supervised Fine-Tuning (SFT):** Refines instruction following.
4.  **Reinforcement Learning with Curriculum Sampling (RLCS):** Optimizes complex reasoning.

### Data Curation
*   **Volume:** >10 billion image-text pairs.
*   **Filtering:** Heuristics, CLIP-Score > 0.3, and factual-centered recaptioning.

### Configuration
*   **Stage 1:** 8,192 token sequences, 120,000 steps. Parallelism: TP=2 for 9B, EP=8/PP=4 for MoE.
*   **Stage 2:** Extends context to 32,768 tokens (131,072 for GLM-4.6V) using Context Parallelism.

---

## üèÜ Results

*   **GLM-4.5V:** Achieved State-of-the-Art (SOTA) among open-source models across **42 benchmarks**. Surpassed Gemini-2.5-Flash in Coding and GUI Agent tasks.
*   **GLM-4.1V-9B-Thinking:** Outperformed Qwen2.5-VL-72B on **29 benchmarks** despite having significantly fewer parameters.
*   **GLM-4.1V-9B-Base:** Showed superior pre-training efficacy on the MathVista pass@k metric.
*   **Scale:**
    *   GLM-4.1V-Thinking: ~9 billion parameters.
    *   GLM-4.5V: 106 billion total parameters (12 billion activated).

---

## üìÅ Contributions

*   **GLM-4.xV Family:** Introduction of GLM-4.1V-Thinking, GLM-4.5V, and GLM-4.6V, advancing general-purpose multimodal understanding and reasoning.
*   **RLCS Technique:** Proposal and validation of Reinforcement Learning with Curriculum Sampling as a novel training technique for multimodal reasoning.
*   **Open Source Release:** Release of GLM-4.1V-9B-Thinking and GLM-4.5V, plus the GLM-4.6V series (featuring native tool use and 128K context).

---

**Quality Score:** 8/10  
**References:** 40 citations