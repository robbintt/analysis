# VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search

*Yiming Jia; Jiachen Li; Xiang Yue; Bo Li; Ping Nie; Kai Zou; Wenhu Chen*

---

> ### ðŸ“Š Quick Facts
>
> *   **Dataset Size:** ~900,000 QA pairs (40% Visual, 60% Text-based)
> *   **Model:** MAmmoTH-VL2 (7B parameters)
> *   **Top Performance:** State-of-the-art in the 10B parameter class
> *   **Key Benchmark Scores:**
>     *   **MMMU-Pro:** 40.7%
>     *   **MathVerse:** 42.6%
>     *   **Dyna-Math:** 55.7%
>     *   **MMVet:** 64.5%
> *   **Method:** 4-Stage automated web retrieval pipeline
> *   **Performance Gain:** 10-20 absolute points for Llava-OV

---

## Executive Summary

The development of advanced Vision-Language Models (VLMs) is currently bottlenecked by a scarcity of high-quality, diverse instruction-tuning data tailored for complex reasoning. While existing models perform well on general perception tasks, they often struggle with domain-specific scientific and mathematical problems that require high-level cognitive reasoning. This limitation stems from the difficulty of manually curating large-scale, exam-like datasets that cover the wide breadth of knowledge needed to truly scale multimodal understanding.

To address this, the authors introduce **VisualWebInstruct**, a scalable, four-stage data-centric pipeline that automates the extraction of multimodal instruction data from the web. The process begins with seed images and employs reverse image search via Google Image Search to retrieve HTML content from over 700,000 unique URLs. Technically, the pipeline utilizes accessibility trees to extract clean content and employs **GPT-4o** to synthesize candidate solutions for instances where ground-truth answers are missing (a frequent occurrence in >50% of cases), filtering these outputs based on consistency. This results in a robust dataset of approximately **900,000 QA pairs**â€”comprising 40% visual and 60% text-based dataâ€”focused on human-created, scientific problems.

Fine-tuning existing models with the VisualWebInstruct dataset yielded substantial performance gains across multiple benchmarks. The Llava-OV-mid model observed an absolute performance increase of 10â€“20 percentage points, while the base MAmmoTH-VL model gained five absolute points. The best-performing model, **MAmmoTH-VL2** (7B parameters), achieved state-of-the-art performance within the 10B parameter class, securing scores of 40.7% on MMMU-Pro-std, 42.6% on MathVerse, 55.7% on Dyna-Math, and 64.5% on MMVet, thereby surpassing competitors such as InternVL2.5 and Phi-4-Mini.

This work demonstrates that leveraging diverse, web-sourced instruction data is a superior strategy for enhancing the complex reasoning capabilities of VLMs compared to relying solely on static, manually curated datasets. By establishing a new performance standard for models in the 10B class and releasing the MAmmoTH-VL2 model, the authors provide a reproducible blueprint for scaling multimodal data acquisition. This approach significantly lowers the barrier to creating domain-specific expert models, signaling a shift toward data-efficient methodologies for achieving high-level visual reasoning.

---

## Key Findings

*   **Significant Performance Gains:** Fine-tuning using the VisualWebInstruct dataset resulted in substantial improvements, including a **10â€“20 absolute point increase** for Llava-OV and a **5 absolute point gain** for MAmmoTH-VL.
*   **State-of-the-Art Results:** The resulting model, MAmmoTH-VL2, achieved SOTA performance within the 10B parameter class on major benchmarks: MMMU-Pro (40.7), MathVerse (42.6), and DynaMath (55.7).
*   **Data Efficiency:** A dataset of approximately **900K QA pairs** was sufficient to drive these improvements.
*   **Enhanced Reasoning:** The study proves that utilizing diverse, web-sourced instruction data effectively enhances the complex reasoning capabilities of vision-language models.

---

## Methodology

The researchers proposed **VisualWebInstruct**, a data-centric approach designed to scale up multimodal instruction data through a rigorous four-stage process:

1.  **Seed Initialization**
    *   Started with a collection of 30,000 seed images.
2.  **Web Retrieval**
    *   Utilized Google Image Search to perform reverse image searching.
    *   Collected HTML data from over **700,000 unique URLs**.
3.  **Data Processing Pipeline**
    *   Implemented content extraction using accessibility trees.
    *   Applied rigorous filtering and synthesis techniques.
4.  **Dataset Construction**
    *   Created approx. 900K QA pairs across multiple disciplines.
    *   Used this dataset to fine-tune existing Vision-Language Models (VLMs).

---

## Technical Details

**Data Acquisition Pipeline**
*   **Mechanism:** Automated pipeline using reverse image search and HTML content extraction.
*   **Content Extraction:** Used accessibility trees from HTML to parse clean data.
*   **Answer Synthesis:** utilized GPT-4o to synthesize candidate solutions for missing answers (found in >50% of cases).
*   **Filtering:** Solutions were filtered based on consistency to ensure quality.

**Dataset Composition**
*   **Total Volume:** ~900,000 QA pairs.
*   **Visual Breakdown:**
    *   **40% Visual QA:** 163,743 unique images.
    *   **60% Text-only QA.**
*   **Focus:** Human-created, exam-like scientific problems.

**Model Training**
*   **Base Models:** MAmmoTH-VL and Llava-OV-mid.
*   **Technique:** Supervised Fine-Tuning (SFT).
*   **Resultant Model:** MAmmoTH-VL2 (7B parameters).

---

## Results

Fine-tuning Llava-OV-mid on the VisualWebInstruct dataset resulted in absolute performance gains of **10â€“20 percentage points** across seven visual reasoning benchmarks. The resulting model, **MAmmoTH-VL2**, achieved state-of-the-art performance within the 10B parameter class.

**Benchmark Scores (MAmmoTH-VL2)**
*   **MMMU-Pro-std:** 40.7%
*   **MathVerse:** 42.6%
*   **Dyna-Math:** 55.7%
*   **MMVet:** 64.5%

**Comparison**
The model surpassed competitors such as **InternVL2.5** and **Phi-4-Mini** in average performance across the evaluated benchmarks.

---

## Contributions

*   **Addressing Data Scarcity:** Directly tackles the critical lack of high-quality, diverse training data for reasoning-focused multimodal tasks.
*   **Novel Scalable Method:** Introduces a new method for generating multimodal instruction data by leveraging search engines and web content to capture domain-specific knowledge.
*   **Performance Benchmarking:** Establishes a new performance standard for the 10B parameter class through the release of MAmmoTH-VL2, specifically for complex tasks requiring high-level visual reasoning.

---

**Assessment**
*   **Quality Score:** 8/10
*   **References:** 40 citations