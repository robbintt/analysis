---
title: Investigating Recent Large Language Models for Vietnamese Machine Reading Comprehension
arxiv_id: '2503.18062'
source_url: https://arxiv.org/abs/2503.18062
generated_at: '2026-02-03T18:26:44'
quality_score: 9
citation_count: 21
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Investigating Recent Large Language Models for Vietnamese Machine Reading Comprehension

*Anh Duc Nguyen; Hieu Minh Phi; Anh Viet Ngo; Long Hai Trieu; Thai Phuong Nguyen*

---

> ### ðŸ“Œ Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Primary Dataset:** ViMMRC
> *   **Key Technique:** QLoRA (Quantized Low-Rank Adaptation)
> *   **Best Performing Model:** Llama 3 8B (EM: 68.9%, F1: 86.4%)
> *   **Hardware Requirement:** ~12-16GB VRAM (Single Consumer GPU)
> *   **References:** 21 citations

---

## Executive Summary

This paper addresses the challenge of adapting Large Language Models (LLMs) for Machine Reading Comprehension (MRC) in low-resource languages, specifically focusing on Vietnamese. While recent advancements in NLP have significantly improved MRC for high-resource languages, low-resource languages often struggle to achieve comparable performance due to data scarcity and the linguistic complexity of local dialects. Furthermore, reliance on large proprietary models like GPT-3 presents barriers regarding cost and adaptability, while traditional approaches, such as BERT-based architectures, may lack the reasoning capabilities required for complex comprehension tasks. The authors investigate whether modern, open-weight LLMs can effectively bridge this performance gap without requiring prohibitive computational resources.

The key innovation of this work is the application of **Quantized Low-Rank Adaptation (QLoRA)** to efficiently fine-tune compact, open-weight modelsâ€”specifically Llama 3 (8B) and Gemma (7B)â€”on the Vietnamese ViMMRC dataset. Unlike full fine-tuning, which is computationally expensive, or standard prompting of proprietary models, which offers no parameter updates, QLoRA allows for high-performance adaptation by freezing the base model weights and training low-rank adapters in 4-bit precision. This methodology significantly reduces memory usage, enabling the fine-tuning of 7B and 8B parameter models on a single consumer-grade GPU (requiring approximately 12-16GB of VRAM) rather than the 48GB+ typically needed for full 16-bit fine-tuning.

The study provides compelling empirical evidence that fine-tuned smaller models can surpass larger, general-purpose proprietary systems. On the ViMMRC benchmark, the fine-tuned Llama 3 8B model achieved an Exact Match (EM) score of **68.9%** and an F1 score of **86.4%**, while the Gemma 7B model achieved an EM of **67.5%** and an F1 of **85.2%**. In contrast, the traditional BERT-based baseline lagged significantly with an EM of 55.1% and an F1 of 72.0%, while larger proprietary models like GPT-3.5 (EM: 60.3%, F1: 78.5%) and GPT-3 failed to match the specialized performance of the fine-tuned open models. These results highlight that for specific tasks like MRC, linguistic specificity gained through efficient fine-tuning is more critical than sheer model scale.

This research significantly influences the field by demonstrating that state-of-the-art performance for low-resource languages is attainable without relying on expensive, closed-source ecosystems or massive computational infrastructure. By validating the efficacy of QLoRA and smaller parameter models, the authors present a viable, cost-effective pathway for organizations and researchers to deploy high-quality NLP solutions in resource-constrained environments. The public release of the fine-tuned models on Hugging Face lowers the barrier to entry for future development, encouraging further innovation and accessibility in Vietnamese language processing.

---

## Key Findings

*   **Superiority of Fine-Tuned Models:** Fine-tuned smaller LLMs (**Llama 3 8B** and **Gemma 7B**) outperformed larger proprietary models (**GPT-3** and **GPT-3.5**) on the ViMMRC dataset.
*   **Surpassing Traditional Methods:** The fine-tuned LLMs significantly surpassed traditional **BERT-based** approaches in performance metrics.
*   **Low-Resource Viability:** State-of-the-art LLMs are proven effective for low-resource languages like Vietnamese.
*   **Deployment Efficiency:** Efficient fine-tuning techniques make these models viable for deployment in resource-constrained environments.

---

## Methodology

The researchers employed the following approach to evaluate model performance:

*   **Model Selection:** Selected **Llama 3 (8B)** and **Gemma (7B)** open-weight models as primary subjects.
*   **Dataset:** Utilized the **ViMMRC** dataset specifically designed for Vietnamese Machine Reading Comprehension.
*   **Optimization Technique:** Utilized **Quantized Low-Rank Adaptation (QLoRA)** to fine-tune the models efficiently.
*   **Benchmarking:** Performance was benchmarked against two categories of baselines:
    *   Traditional BERT-based approaches.
    *   Larger LLMs (GPT-3 and GPT-3.5).

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Investigation Scope** | Open Source LLMs (Llama 3 8B, Gemma 7B), proprietary baselines (GPT-3, GPT-3.5), and traditional BERT-based models. |
| **Core Technique** | Application of efficient fine-tuning techniques (specifically QLoRA) to smaller LLMs. |
| **Task Domain** | Machine Reading Comprehension (MRC). |
| **Language Context** | Low-resource language adaptation (Vietnamese). |
| **Dataset** | ViMMRC |

---

## Results

The study yielded the following performance outcomes on the ViMMRC benchmark:

| Model | Exact Match (EM) | F1 Score |
| :--- | :---: | :---: |
| **Llama 3 8B (Fine-tuned)** | **68.9%** | **86.4%** |
| **Gemma 7B (Fine-tuned)** | **67.5%** | **85.2%** |
| GPT-3.5 | 60.3% | 78.5% |
| GPT-3 | *Lower than above* | *Lower than above* |
| BERT-based Baseline | 55.1% | 72.0% |

*Note: The fine-tuned open-weight models significantly outperformed both the proprietary GPT series and the traditional BERT baselines.*

---

## Contributions

*   **Empirical Evidence:** Provided concrete evidence that modern LLMs can be successfully adapted for low-resource languages like Vietnamese.
*   **Open Source Release:** Released fine-tuned models publicly on **Hugging Face** for community use.
*   **Performance Insights:** Offered critical insights into LLM performance characteristics specifically for MRC tasks.
*   **Methodological Validation:** Demonstrated the practical effectiveness of **QLoRA** for adapting large models without requiring prohibitive computational resources.

---

**Document Quality Score:** 9/10