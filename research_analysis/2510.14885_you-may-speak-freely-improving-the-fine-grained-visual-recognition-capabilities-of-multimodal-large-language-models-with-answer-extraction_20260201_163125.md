# You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction

*Logan Lawrence; Oindrila Saha; Megan Wei; Chen Sun; Subhransu Maji; Grant Van Horn*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Method Name** | `nlg2choice` |
| **Quality Score** | 7/10 |
| **Total Citations** | 40 |
| **Avg. Accuracy Improvement** | +9.5% |
| **Max Candidates Handled** | 8,000+ |
| **Inference Cost Reduction** | >100x (Retrieval) |
| **Datasets Benchmarked** | 7 Fine-Grained Visual Datasets |

---

## Executive Summary

This paper addresses a critical challenge with Multimodal Large Language Models (MLLMs)â€”their limited effectiveness in Fine-Grained Visual Classification (FGVC) tasks, particularly when dealing with subtle class distinctions and massive candidate sets.

The authors introduce **`nlg2choice`**, a novel two-stage framework that decouples natural language generation from structured classification. The process involves:
1.  The MLLM generates an unconstrained textual description of the visual input.
2.  A text-only constrained decoding mechanism predicts the most probable choice from the candidate set.

This approach significantly outperformed standard prompting baselines across seven fine-grained visual datasets, achieving an **average improvement of 9.5% in classification accuracy**. Specific highlights include:
*   **CUB-200-2011:** 86.9% vs. 78.3% (Baseline)
*   **Stanford Cars:** 93.8% vs. 79.6% (Baseline)
*   **FGVC-Aircraft:** 67.8% vs. 51.0% (Baseline)

The framework demonstrated robust scalability across tasks with over 8,000 candidates and delivered over **100x reduction in inference costs** in retrieval settings through an early stopping mechanism. This work represents a significant methodological shift in evaluating and deploying MLLMs for high-precision visual tasks, effectively bridging the gap between free-form generation and structured extraction.

---

## Key Findings

*   **Superior Performance on FGVC:** The proposed method (`nlg2choice`) achieves improved results over a suite of seven fine-grained visual datasets, outperforming existing baselines in both classification and retrieval tasks.
*   **Scalability to High-Choice Scenarios:** Unlike existing works limited to 5-way options, the approach effectively handles Fine-Grained Visual Classification tasks where choice counts range from hundreds to thousands, even when choices are highly related.
*   **Computational Efficiency:** In retrieval-based settings, the use of an early stopping method to compute constrained response probabilities significantly improves throughput.
*   **Robustness to Natural Language Variations:** The model's performance holds steady across various ways users might implement tasks using natural language.

---

## Methodology

The proposed approach utilizes a decoupled two-stage framework named `nlg2choice`:

1.  **Open-Ended Generation**
    The Multimodal Large Language Model (MLLM) generates a natural language response with minimal constraints. This allows the model to describe the visual input freely without being immediately boxed into specific categories.

2.  **Constrained Decoding**
    Text-only constrained decoding is applied to the generated response. This stage predicts the most likely choice from the candidate set based on the open-ended description.

3.  **Retrieval Optimization**
    Specifically for retrieval problems, the method computes the probability of the constrained response corresponding to specific choices. It utilizes an **early stopping mechanism** to mitigate the computational cost of calculating probabilities over massive choice sets.

---

## Technical Details

*   **Core Architecture:** The method bridges NLG capabilities of LLMs with structured visual classification using an answer extraction paradigm with constrained response probabilities.
*   **Scalability:** Architected for high-choice scenarios, supporting hundreds to thousands of options, including those with fine-grained semantic overlaps.
*   **Inference Optimization:** Incorporates an early stopping mechanism for retrieval-based settings to trade marginal accuracy for significant speed improvements.
*   **Invariance:** The architecture ensures invariance to natural language variations, maintaining performance regardless of how the task is phrased by the user.

---

## Results

*   **Classification & Retrieval:** Evaluated on seven fine-grained visual datasets, the method outperformed existing baselines across all datasets, maintaining high accuracy even as scale increased.
*   **Efficiency:** The early stopping mechanism led to significant throughput improvements in retrieval-based settings.
*   **Robustness:** The system demonstrated steady performance across various natural language phrasings, proving its reliability in diverse user interaction scenarios.

---

## Contributions

*   **Novel Evaluation Pipeline:** Addresses the persistent challenge of evaluating free-form responses from auto-regressive models in high-stakes environments like FGVC, moving beyond language-only or low-complexity MCQ evaluations.
*   **Retrieval Adaptation:** Successfully extends LLM choice extraction techniques to retrieval-based problems, which were previously hindered by the prohibitive computational cost of computing probabilities over large candidate sets.
*   **Benchmark Validation:** Provides comprehensive empirical evidence of the method's effectiveness across multiple visual datasets and varying natural language task formulations.