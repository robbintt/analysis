# ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics

*Tue-Thu Van-Dinh, Hoang-Duy Tran, Truong-Binh Duong, Mai-Hanh Pham, Binh-Nam Le-Nguyen, Quoc-Thai Nguyen*

---

> **ðŸ“Š Quick Facts**
> *   **Dataset Size:** 6,747 Infographics / 20,409 Q&A Pairs
> *   **Tasks:** Single-image VQA (SiVQA) & Multi-image VQA (MiVQA)
> *   **Language:** Vietnamese (Low-Resource)
> *   **Top Metric (SiVQA):** ~0.46 ANLS
> *   **Top Metric (MiVQA):** ~0.15 ANLS
> *   **Human Baseline:** ~0.98 ANLS
> *   **Quality Score:** 9/10

---

## Executive Summary

### Problem: The Challenge of Low-Resource Infographic Understanding
Current Vision-Language Models (VLMs) excel at general visual question answering (VQA) but struggle significantly with complex, text-dense mediums like infographics, which require the synthesis of optical character recognition (OCR), spatial layout understanding, and logical reasoning. This challenge is amplified in low-resource languages such as Vietnamese, where models face tokenizer inefficiencies and a lack of multilingual pre-training. Furthermore, existing benchmarks are predominantly limited to single-image inputs, failing to evaluate the critical capability of cross-image reasoning and evidence synthesis necessary for real-world multi-page documents.

### Innovation: The ViInfographicVQA Benchmark
The authors introduce **ViInfographicVQA**, the first large-scale benchmark designed to evaluate VQA on Vietnamese infographics. The dataset comprises over 6,747 real-world images and 20,409 human-verified question-answer pairs, technically structured to assess two distinct capabilities: **Single-image VQA (SiVQA)** for traditional queries and **Multi-image VQA (MiVQA)** for evidence synthesis across related visuals. The construction utilized specialized Vietnamese OCR engines (e.g., VinOCR) to extract precise text and layout data, followed by a semi-automated generation pipeline that combines programmatic creation with rigorous human verification to cover diverse reasoning domains such as logic, arithmetic, and structure understanding.

### Results: Quantifying the Performance Gap
Evaluations using Average Normalized Levenshtein Similarity (ANLS) for open-ended text and Accuracy for definitive answers revealed a substantial chasm between current models and human baselines. While human performance reached an ANLS of approximately **0.98**, the best-performing model on Single-image tasks (an OCR-centric pipeline) achieved only **0.46 ANLS** and **48% accuracy**. In contrast, end-to-end models like Donut failed catastrophically with ANLS scores below **0.05** due to tokenizer limitations. The results were markedly worse in Multi-image tasks; across all architectures, performance plummeted by roughly **30 percentage points**, with the best models dropping to an ANLS of nearly **0.15**. This drastic decline confirms that current models frequently hallucinate and fundamentally fail to integrate information across multiple images.

### Impact: Redefining Evaluation for Cross-Image Reasoning
ViInfographicVQA fills a critical void by establishing the first standardized evaluation protocol for cross-image reasoning in a low-resource language context. The benchmark provides diagnostic insights into the specific failures of monolithic VLMs, proving that implicit visual understanding is insufficient for tasks requiring explicit text extraction and multi-context synthesis. This work encourages a shift in research focus toward layout-aware architectures and improved multilingual tokenization, serving as a vital resource for advancing VLM capabilities beyond high-resource English domains and single-image constraints.

---

## Key Findings

*   **Multi-image Reasoning Gap:** Recent vision-language models exhibit substantial performance lapses in multi-image reasoning, specifically failing at cross-image integration.
*   **Complexity of Infographics:** Infographic VQA demands significantly more advanced capabilities in OCR and layout understanding compared to standard VQA.
*   **Low-Resource Limitations:** Current models show distinct limitations when processing low-resource languages like Vietnamese.
*   **Hallucination Issues:** There is a high frequency of hallucinations when models attempt to synthesize evidence across multiple images.
*   **Architectural Failures:** General Purpose VLMs and end-to-end models struggle compared to modular OCR-centric approaches in this specific domain.

---

## Methodology

The authors developed the **ViInfographicVQA** benchmark to rigorously test model capabilities.
*   **Dataset Construction:** Utilized Vietnamese online sources to aggregate over **6,747** real-world infographics.
*   **Q&A Generation:** Created **20,409** human-verified Q&A pairs using a semi-automated pipeline ensuring high quality.
*   **Task Design:**
    *   **Single-image Task (SiVQA):** Evaluates traditional Q&A capabilities on a single infographic.
    *   **Multi-image Task (MiVQA):** Requires the synthesis of evidence across related, multiple images to answer questions.

---

## Technical Details

The technical implementation focuses on robust data extraction and diverse model evaluation.

### Dataset Structure
*   **Components:** Divided into Single-Image VQA (SiVQA) and Multi-Image VQA (MiVQA).
*   **Data Pipeline:**
    *   Utilized specialized OCR engines (e.g., **VinOCR**) to extract text and layout information from complex designs.
    *   Question generation covers logic, arithmetic, and structure understanding.

### Evaluated Models
*   **OCR-centric Models:** e.g., Donut.
*   **General-Purpose VLMs:** e.g., LLaVA, InstructBLIP, Qwen-VL.
*   **Layout-aware Models:** Specific architectures designed to understand spatial layout.
*   **Multi-image Handling:** Achieved by feeding multiple images into vision encoders for simultaneous processing.

---

## Contributions

*   **Benchmark Introduction:** Introduced the first large-scale Vietnamese Infographic VQA benchmark.
*   **Evaluation Protocol:** Established the first Vietnamese evaluation protocol specifically for cross-image reasoning.
*   **Diagnostic Insights:** Provided critical insights into model limitations within low-resource environments.
*   **Research Direction:** Encouraged future research into layout-aware and cross-image reasoning techniques.

---

## Results

Performance was evaluated using **ANLS** (Average Normalized Levenshtein Similarity) for open-ended text and **Accuracy** for definite answers.

*   **Performance Drop:** A significant performance drop was observed in Multi-Image tasks (approx. 30 percentage points) due to cross-image integration difficulties.
*   **Localization Issues:** Models like LLaVA and InstructBLIP showed limitations in Vietnamese due to tokenizer inefficiency and lack of multilingual pre-training.
*   **OCR Correlation:** Performance was directly correlated with OCR quality; explicit Vietnamese OCR (VinOCR) significantly outperformed implicit end-to-end models like Donut.
*   **Human vs. AI:** Overall model performance remained significantly lower than human baselines (Human ~0.98 ANLS vs. Best Model ~0.15 ANLS in multi-image tasks).

---
**References:** 11 citations