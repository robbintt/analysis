---
title: 'FlowReasoner: Reinforcing Query-Level Meta-Agents'
arxiv_id: '2504.15257'
source_url: https://arxiv.org/abs/2504.15257
generated_at: '2026-02-03T13:20:46'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# FlowReasoner: Reinforcing Query-Level Meta-Agents

*Hongcheng Gao; Yue Liu; Yufei He; Longxu Dou; Chao Du; Zhijie Deng; Bryan Hooi; Min Lin; Tianyu Pang*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Performance Gain** | +10.52% higher accuracy than o1-mini |
| **Base Model** | DeepSeek-R1-Distill-Qwen-7B |
| **Training Strategy** | Knowledge Distillation + GRPO Reinforcement Learning |
| **Key Optimization** | Multi-purpose reward (Performance, Complexity, Efficiency) |
| **Best Worker Config** | FlowReasoner (Meta) + o1-mini (Worker) = 63.17% Score |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |

---

## Executive Summary

**Problem**
Current multi-agent frameworks for complex reasoning tasks typically rely on static, hand-crafted architectures where the structure of the agent system remains fixed regardless of the specific user query. This lack of adaptability limits performance, as different queries often require distinct problem-solving approaches and collaboration patterns. Furthermore, while large language models (LLMs) have shown advanced reasoning capabilities, optimizing these models to function as high-level planners that can dynamically generate and orchestrate other agents remains a significant challenge in the field of code generation and automated reasoning.

**Innovation**
FlowReasoner introduces a novel "query-level meta-agent" architecture that dynamically constructs a unique, personalized multi-agent system for each specific user input. Technically, the model generates executable code defining the agent system architecture, which is then instantiated to solve the query. The framework employs a rigorous three-stage training pipeline:
1. Knowledge Distillation from DeepSeek R1 to ingest reasoning traces.
2. Supervised Fine-Tuning (SFT) to warm up the model on reasoning processes.
3. Reinforcement Learning using Grouped Relative Policy Optimization (GRPO).

Crucially, the RL stage is driven by external execution feedback and a multi-purpose reward function that simultaneously optimizes for **performance** ("pass rate"), **complexity** ("AST score"), and **efficiency** ("distinctness ratio").

**Results**
FlowReasoner demonstrates substantial performance improvements over existing state-of-the-art models. It achieves a **10.52% higher accuracy** than the o1-mini model across three benchmarks involving engineering and competition code. In ablation studies, FlowReasoner acting as a meta-agent orchestrating o1-mini workers achieved a score of **63.17%**, outperforming configurations where o1-mini (62.77%), Claude 3.5 (62.15%), or GPT-4o-mini (61.81%) acted as the controller. Additionally, when FlowReasoner served as the meta-agent, o1-mini workers yielded the highest performance, followed by Claude 3.5 and GPT-4o-mini, highlighting the meta-agent's ability to effectively leverage strong worker models.

**Impact**
This research represents a significant paradigm shift from static multi-agent systems to dynamic, query-driven architectures. By proving that a meta-agent can successfully reason about and design optimal agent systems on the fly, FlowReasoner establishes a new high-performance baseline for code generation tasks. The findings validate the efficacy of combining knowledge distillation with execution-feedback-based reinforcement learning, providing a robust blueprint for future research into automated system design and hierarchical AI reasoning. This approach paves the way for more autonomous and adaptable AI solutions capable of self-optimizing their internal workflows for complex problems.

---

## Key Findings

*   **Significant Performance Leap:** FlowReasoner outperforms the o1-mini model by a significant margin, achieving a **10.52% higher accuracy** across three benchmarks involving engineering and competition code.
*   **Effective Training Combination:** Combining the distillation of DeepSeek R1 with reinforcement learning (RL) based on external execution feedback successfully enhances the meta-agent's reasoning capabilities.
*   **Dynamic Architecture Generation:** The meta-agent is capable of generating distinct, personalized multi-agent systems tailored to specific user queries through deliberative reasoning.
*   **Multi-Objective Optimization:** The use of a multi-purpose reward function allows the model to optimize for three critical factors simultaneously: **performance**, **complexity**, and **efficiency**.

---

## Methodology

The research utilizes a two-stage pipeline designed to create a reasoning-based meta-agent:

1.  **Knowledge Distillation:** The model is initialized by distilling knowledge from DeepSeek R1, transferring advanced reasoning patterns to the meta-agent.
2.  **Reinforcement Learning (RL) Enhancement:** The model is refined using RL driven by external execution feedback, allowing it to learn from the success or failure of the generated systems.
3.  **Reward Design:** The RL process uses a multi-purpose reward function that evaluates:
    *   Performance (Pass rate)
    *   Complexity (AST score)
    *   Efficiency (Distinctness ratio)
4.  **Query-Level Operation:** Unlike static systems, the model operates as a meta-agent that analyzes a user query and constructs a specific multi-agent system architecture for that specific instance.

---

## Technical Details

FlowReasoner is a query-level meta-agent that generates unique, query-specific multi-agent systems (represented as programming code) for each individual user input.

### Training Pipeline
The system employs a specific three-stage training process:

1.  **Reasoning Data Distillation:** Utilizing DeepSeek R1 671B to synthesize reasoning traces and systems.
2.  **Reasoning SFT Warmup:** Conducted on the DeepSeek-R1-Distill-Qwen-7B base model to maximize the likelihood of generating reasoning processes and systems.
3.  **Reinforce Reasoning:** Uses Grouped Relative Policy Optimization (GRPO) with external execution feedback and process rewards.

### Optimization Function
The model optimizes a composite reward function that accounts for:
*   **Performance:** Measured by pass rate.
*   **Complexity:** Measured by AST score.
*   **Diversity:** Measured by distinctness ratio.

---

## Results

### Benchmark Performance
FlowReasoner achieved a **10.52% higher accuracy** than the o1-mini model across three benchmarks involving engineering and competition code.

### Ablation Studies

**Meta-Agent Ablation (using o1-mini as workers)**
*   **FlowReasoner:** 63.17%
*   o1-mini: 62.77%
*   Claude 3.5: 62.15%
*   GPT-4o-mini: 61.81%

**Worker Ablation (using FlowReasoner as the meta-agent)**
*   **o1-mini:** 63.17%
*   Claude 3.5: 62.15%
*   GPT-4o-mini: 61.81%
*   QwenCoder: 53.85%

---

## Contributions

*   **Query-Level Meta-Agent Architecture:** Introduced FlowReasoner, a novel framework for automating the design of multi-agent systems at the query level rather than relying on static configurations.
*   **Integration of External Feedback:** Demonstrated the effectiveness of using external execution feedback within a reinforcement learning loop to improve reasoning capabilities.
*   **Benchmark Advancement:** Provided a new high-performance baseline for code generation tasks, proving that a meta-agent approach can surpass existing state-of-the-art models like o1-mini.