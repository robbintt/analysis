---
title: 'DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning'
arxiv_id: '2507.14481'
source_url: https://arxiv.org/abs/2507.14481
generated_at: '2026-02-03T19:08:54'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning

*Yujia Tong; Jingling Yuan; Tian Zhang; Jianquan Liu; Chuang Hu*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Model Type** | Vision Transformers (ViTs) |
| **Method** | Post-Training Quantization (PTQ) |
| **Key Innovation** | Curriculum-based Data Synthesis & Activation Correction Matrix |
| **Performance Gain** | +4.29% accuracy on DeiT-T vs. SOTA DFQ |
| **Efficiency** | Eliminates fine-tuning stage (Green Learning) |
| **Scalability** | Validated on ViT-L (307M parameters) |

---

## Executive Summary

### **The Problem**
Deploying Vision Transformers (ViTs) on resource-constrained edge devices requires aggressive quantization. While Post-Training Quantization (PTQ) is standard, existing Data-Free Quantization (DFQ) methodsâ€”essential for privacy-sensitive scenarios where original data is unavailableâ€”suffer from performance degradation. This is caused by poor synthetic sample quality and significant distribution shifts in intermediate layer activations.

### **The Innovation**
The authors propose **DFQ-ViT**, a pipeline that eliminates the need for fine-tuning through two primary innovations:
1.  **Curriculum-based Data Synthesis:** Generates synthetic calibration samples in order of increasing difficulty ("easy-to-hard"), balancing global features and local details.
2.  **Activation Correction Matrix:** Aligns the statistical distributions of intermediate layer activations between the full-precision teacher and the quantized student model during calibration, minimizing discrepancies without backpropagation.

### **The Results**
DFQ-ViT achieves a **4.29% accuracy improvement** on the DeiT-T model over previous baselines, reaching performance parity with models quantized using real data. The method scales effectively to large architectures like ViT-L (307M parameters) while maintaining high accuracy.

### **The Impact**
This research offers a significant advancement for efficient ViT deployment in privacy-sensitive applications. By removing the fine-tuning stage, DFQ-ViT reduces computational overhead and energy consumption, aligning with **Green Learning** principles and setting a new benchmark for data-free quantization.

---

## Key Findings

*   **Performance Parity:** DFQ-ViT achieves accuracy comparable to models quantized with real data, effectively bridging the gap between data-free and data-dependent methods.
*   **Superior Accuracy:** The method outperforms existing state-of-the-art DFQ methods, delivering a **4.29% accuracy improvement** on the DeiT-T model.
*   **Elimination of Fine-tuning:** By removing the fine-tuning stage, the approach significantly reduces computational overhead and training time.
*   **Green Learning:** The resource-efficient nature of the method aligns with energy efficiency principles.
*   **Root Cause Analysis:** The study identifies that poor synthetic sample quality and distribution shifts in intermediate activations are primary causes of degradation in current DFQ methods.

---

## Technical Details

**Method Classification:** Post-Training Quantization (PTQ)  
**Target Architecture:** Vision Transformers (ViTs)  
**Constraints:** No original training data; No fine-tuning; Addresses reliance on Layer Normalization (LN).

### Process Pipeline
The DFQ-ViT method employs a two-phase process:

1.  **Sample Generation Phase**
    *   **Strategy:** Utilizes Curriculum Learning.
    *   **Approach:** "From easy to hard" synthesis strategy.
    *   **Goal:** Balances the capture of global features and local details to enhance synthetic data quality.

2.  **Model Calibration Phase**
    *   **Inputs:** Uses the synthetic samples generated in Phase 1.
    *   **Mechanism:** Utilizes intermediate activations to align the Quantized (student) model with the Full-Precision (teacher) model.
    *   **Optimization:** Achieves alignment without backpropagation.

### Baseline Complexity
*   **ViT-L:** 307 million parameters, 64 billion FLOPs.

---

## Methodology

The proposed DFQ-ViT pipeline optimizes Data-Free Quantization specifically for Vision Transformers through two core components:

*   **Curriculum-based Data Synthesis:**
    Instead of random generation, this module generates samples ordered by increasing difficulty. This curriculum approach ensures the model learns robust features progressively, improving the quality of the calibration data.

*   **Activation Correction Matrix:**
    This component targets the statistical discrepancies (distribution shifts) that occur in intermediate layers during quantization. By applying a correction matrix during calibration and inference, the method aligns the activations of the quantized model with those of the original model, ensuring stability and accuracy.

---

## Contributions

*   **Enhanced Synthetic Data:** Improved the quality of synthetic data by implementing a difficulty-ordered synthesis strategy.
*   **Distribution Mitigation:** Identified and successfully mitigated distribution shifts in intermediate layer activations via the proposed correction matrix.
*   **Resource Efficiency:** Provided a deployment solution that maintains high accuracy without the resource-heavy fine-tuning process.

---

## Results

*   **Accuracy Improvement:** Achieved a **4.29%** accuracy improvement over state-of-the-art Data-Free Quantization methods on the **DeiT-T** model.
*   **Real-Data Parity:** Performance is statistically equivalent to quantization methods that require access to the original training dataset.
*   **Efficiency:** Successfully eliminates the fine-tuning stage, resulting in significantly lower computational overhead.
*   **Scalability:** Validated on large-scale models, specifically citing **ViT-L** as a baseline for handling high deployment complexity.

---
**Quality Score:** 9/10  
**References:** 40 citations