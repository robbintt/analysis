# Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference

*Tomer Gafni; Asaf Karnieli; Yair Hanani*

---

> ### ðŸ“„ Quick Facts
> * **Quantization Scheme:** W4A8 (4-bit Integer Weights / 8-bit Float Activations)
> * **Core Algorithm:** Dual Precision Quantization (DPQ)
> * **Architecture:** INT4 Asymmetric Weights, FP8 (E4M3) Symmetric Activations
> * **Hardware Targets:** Intel Gaudi 2 & Gaudi 3
> * **Throughput Gain:** Up to 2.5x higher than W4A16 kernels
> * **Accuracy Drop:** < 0.05 Perplexity (WikiText-2), < 0.5% (MMLU)
> * **Models Tested:** Llama-2, Llama-3, Qwen2-VL

---

## Executive Summary

As Deep Neural Networks (DNNs) and Large Language Models (LLMs) scale in size, the memory and computational costs of inference in full precision (BF16/FP16) become prohibitive for real-time deployment. While quantization reduces memory bandwidth, aggressive 4-bit weight compression typically introduces significant accuracy degradation. Furthermore, existing state-of-the-art methods often rely on W4A16 schemes (4-bit weights, 16-bit activations), which fail to leverage the high-throughput capabilities of modern hardware accelerators that possess lower-precision floating-point arithmetic units. The industry faces a critical bottleneck where existing quantization techniques either sacrifice too much accuracy or underutilize the specific performance advantages of contemporary AI hardware like Intel Gaudi.

The authors introduce a hardware-efficient quantization scheme, **W4A8**, which decouples storage precision from computational precision. In this framework, model weights are stored as compressed 4-bit integers (INT4, asymmetric), but inference operations are executed using 8-bit floating-point arithmetic (FP8, E4M3). To rectify the accuracy loss typically associated with aggressive 4-bit weight quantization, the authors propose the **Dual Precision Quantization (DPQ)** algorithm. DPQ utilizes a cascaded quantization chain and a per-group error compensation mechanism. This algorithm optimizes quantization parameters specifically to minimize error in the FP8 computation domain, ensuring model accuracy during inference without adding runtime overhead.

The W4A8 scheme was validated on Llama-2, Llama-3, and Qwen2-VL architectures using benchmarks including WikiText-2 (PPL), MMLU, and an 8-task common sense suite. The DPQ algorithm achieved accuracy nearly identical to full-precision baselines: on WikiText-2, the perplexity increase was constrained to less than 0.05 over BF16, significantly outperforming GPTQ (W4A16), which exhibited substantially larger degradation. In the MMLU benchmark, the method maintained accuracy within 0.5% of the full-precision model, surpassing RTN, QServe, and QQQ on the majority of tasks. Crucially, hardware evaluations demonstrated tangible efficiency gains, with the W4A8 approach delivering up to 2.5x higher throughput compared to standard W4A16 kernels on Gaudi 2 accelerators, confirming the superior utilization of FP8 hardware.

This research establishes W4A8 as a superior standard for efficient inference on modern accelerators, demonstrating that storage efficiency and computational throughput do not need to be traded off against accuracy. By utilizing a post-training quantization framework, the method avoids the high computational cost of retraining quantization-aware models, making it immediately practical for existing deployments. The ability to perform inference using FP8 arithmetic allows for substantial throughput improvements on current and future hardware, while the reduction in weight bits to INT4 significantly lowers memory bandwidth usage. Consequently, this work enables the deployment of larger, more complex models on standard hardware infrastructure without sacrificing the response quality or latency required by real-time applications.

---

## Key Findings

*   **W4A8 Efficiency:** The W4A8 scheme (4-bit integer weights with 8-bit floating-point inference) delivers significant speedups and improved memory utilization compared to traditional 16-bit operations.
*   **Hardware Optimization:** The quantization scheme is designed to exploit hardware advantages and is applicable across various modern accelerators, specifically targeting lower-precision floating-point units.
*   **Accuracy Preservation:** Through the Dual Precision Quantization (DPQ) algorithm, the method maintains tolerable accuracy degradation relative to full-precision models without adding computational overhead.
*   **Throughput vs. Accuracy:** The approach increases throughput while effectively managing the trade-off between efficiency and model accuracy.

---

## Methodology

The authors employ a **post-training quantization framework** designed to address latency and memory constraints in large Deep Neural Networks.

### Core Strategy
The research implements a mixed-precision inference scheme known as **W4A8**.
*   **Storage:** Model weights are stored as 4-bit integers.
*   **Computation:** Inference calculations are executed using 8-bit floating-point arithmetic.

### Accuracy Mitigation
To counter the accuracy loss typical of low-bit quantization, the authors developed the **Dual Precision Quantization (DPQ)** algorithm.
*   This algorithm leverages the specific structural characteristics of the W4A8 scheme.
*   It optimizes the model to recover accuracy without incurring additional inference costs.

---

## Technical Details

### Quantization Architecture
*   **Scheme:** W4A8
*   **Weights:** INT4 (Asymmetric)
*   **Activations:** FP8 (Symmetric, E4M3 format)

### The DPQ Algorithm
*   **Process:** Utilizes a cascaded quantization chain:
    $$W_{16} \rightarrow W_8 \rightarrow W_4 \rightarrow \hat{W}_8 \rightarrow \hat{W}_{16}$$
*   **Error Compensation:** Implements an error compensation mechanism ($\delta_F$).
*   **Granularity:** Distributed per-group (Group size: 128).

### Hardware Specifications
*   **FP8 Numerical Range:**
    *   **Gaudi 2:** $\pm 240$
    *   **Gaudi 3:** $\pm 448$

### Calibration & Configuration
*   **Scope:** Static quantization applied to all linear layers (excluding embeddings and lm-head).
*   **Calibration Dataset:** WebQuestions.
*   **Sequence Length:** 2048.

---

## Contributions

1.  **W4A8 Strategy:** Introduction of a hardware-efficient W4A8 quantization strategy that decouples weight storage precision (4-bit) from computation precision (8-bit float).
2.  **DPQ Algorithm:** Development of the Dual Precision Quantization (DPQ) algorithm, which mitigates accuracy loss inherent in low-bit quantization while maintaining zero inference overhead.
3.  **Validation:** Proof that complex DNNs can achieve higher throughput and better memory efficiency on modern hardware using 4-bit weights while remaining competitive with full-precision model accuracy.

---

## Results

The approach was rigorously evaluated on **Qwen2-VL**, **Llama-2**, and **Llama-3** models using standard NLP benchmarks.

**Benchmark Suite:**
*   **WikiText-2:** Perplexity (PPL) measurement.
*   **MMLU:** Massive Multitask Language Understanding.
*   **Common Sense Suite:** 8-task evaluation via LM-Evaluation-Harness (including HellaSwag, LAMBADA, BoolQ).

**Performance Outcomes:**
*   **Accuracy Comparison:** The DPQ algorithm demonstrated minimal degradation compared to full precision (BF16).
*   **State-of-the-Art Comparison:** It outperformed GPTQ (W4A16) as well as other methods like RTN, QServe, and QQQ across most tasks.
*   **Strategy Effectiveness:** The results confirm the effectiveness of the FP8 error compensation strategy in maintaining model fidelity.