---
title: 'AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security'
arxiv_id: '2601.18491'
source_url: https://arxiv.org/abs/2601.18491
generated_at: '2026-02-03T13:55:46'
quality_score: 8
citation_count: 31
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security

*Dongrui Liu; Qihan Ren; Chen Qian; Shuai Shao; Yuejin Xie; Yu Li; Zhonghao Yang; Haoyu Luo; Peng Wang; Qingyu Liu; Binxin Hu; Ling Tang; Jilin Mei; Dadi Guo; Leitao Yuan; Junyao Yang; Guanxu Chen; Qihao Lin; Yi Yu; Bo Zhang; Jiaxuan Guo; Jie Zhang; Wenqi Shao; Huiqi Deng; Zhiheng Xi; Wenjie Wang; Wenxuan Wang; Wen Shen; Zhikai Chen; Haoyu Xie; Jialing Tao; Juntao Dai; Jiaming Ji; Zhongjie Ba; Linfeng Zhang; Yong Liu; Quanshi Zhang; Lei Zhu; Zhihua Wei; Hui Xue; Chaochao Lu; Jing Shao; Xia Hu*

---

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **Model Architectures:** Qwen, Llama
> *   **Scalability:** 4B to 8B parameters
> *   **Benchmark Performance:** 92.80% on ASSE-Safety (SOTA)
> *   **Key Innovation:** Three-dimensional taxonomy for root cause analysis

---

## Executive Summary

As Large Language Models (LLMs) evolve into autonomous agents capable of complex, multi-step reasoning and tool use, ensuring their safety becomes a critical challenge. Existing safety mechanisms are predominantly limited to binary classification—labeling an action simply as "safe" or "unsafe"—which fails to provide the necessary transparency for debugging or understanding the root causes of failure. This lack of granularity creates a significant barrier to deploying reliable agentic systems in high-stakes environments, as operators cannot effectively diagnose or mitigate risks without knowing the specific source and nature of a safety violation.

To address these limitations, the authors introduce **AgentDoG**, a diagnostic guardrail framework that advances safety monitoring from binary detection to fine-grained, contextual analysis. The core technical innovation is a unified three-dimensional taxonomy that categorizes agentic risks by **Risk Source** (where the risk originated), **Failure Mode** (the specific error in the process), and **Consequence** (the resulting harm). Utilizing this structure, the researchers developed **ATBench**, a specialized benchmark for evaluating fine-grained safety, and implemented AgentDoG to monitor agent trajectories for root cause analysis.

AgentDoG achieves state-of-the-art performance in both binary and fine-grained safety tasks. The framework reached a maximum accuracy of **81.10%** on the R-Judge benchmark and **92.80%** on ASSE-Safety. On the newly introduced ATBench for fine-grained classification, AgentDoG achieved a general accuracy of **82.00%**. The evaluation revealed a notable disparity in sub-task complexity; identifying the Failure Mode proved substantially easier (59.20%), whereas identifying the Risk Source was significantly more challenging (32.40%).

The significance of AgentDoG lies in its ability to provide **diagnostic transparency**, shifting the industry standard from simple safety blocking to actionable root cause analysis. By open-sourcing the models, datasets, and taxonomies, the authors ensure reproducibility and provide a robust foundation for the community to advance interpretable AI safety.

---

## Key Findings

*   **State-of-the-Art Safety:** AgentDoG achieves SOTA performance in moderating agentic safety across complex, multi-step scenarios.
*   **Diagnostic Transparency:** Unlike traditional binary guardrails, the framework identifies root causes of unsafe or unreasonable actions.
*   **Unified Taxonomy:** A three-dimensional taxonomy (Source, Failure Mode, Consequence) effectively structures agentic risks.
*   **Scalability:** The framework demonstrates effectiveness across different model architectures (Qwen, Llama) and sizes (4B, 7B, 8B).
*   **Performance Disparity:** Results indicate that identifying the *Risk Source* is significantly more difficult than identifying the *Failure Mode*.

---

## Methodology

The research employs a taxonomy-guided approach consisting of four primary phases:

1.  **Taxonomy Establishment:** A unified three-dimensional taxonomy for agentic risks is established to categorize risks by Source, Failure Mode, and Consequence.
2.  **Benchmark Construction:** ATBench, a fine-grained agentic safety benchmark, is constructed based on this taxonomy to facilitate rigorous testing.
3.  **Framework Implementation:** The AgentDoG framework is implemented to provide fine-grained, contextual monitoring across agent trajectories, enabling root cause analysis.
4.  **Extensive Validation:** Models based on Qwen and Llama architectures are extensively validated across various sizes to demonstrate effectiveness and scalability.

---

## Technical Details

**Framework Architecture**
*   **Type:** Diagnostic Guardrail Framework.
*   **Core Function:** Identifies root causes of unsafe actions using a Three-Dimensional Taxonomy.
*   **Dimensions:**
    1.  Risk Source
    2.  Failure Mode
    3.  Real-world Harm

**Training Strategy**
*   **Data Synthesis & Collection:** Utilizes a specific strategy for training fine-grained classification models to distinguish between subtle risk categories.

**Compatibility**
*   **Architecture-Agnostic:** Designed to be scalable across different LLM families.
*   **Supported Families:** Qwen, Llama.
*   **Parameter Range:** Validated on models ranging from 4B to 8B parameters.

---

## Contributions

*   **Hierarchical Taxonomy:** Introduction of a hierarchical, three-dimensional taxonomy (Source, Failure Mode, Consequence) specifically for agentic risks.
*   **Benchmark Creation:** Release of ATBench, a specialized benchmark for evaluating fine-grained safety in AI agents.
*   **Novel Framework:** Development of AgentDoG, which advances safety checks from simple binary classification to diagnostic monitoring with root cause analysis.
*   **Open Source:** Open-sourcing of all models and datasets to ensure reproducibility and support further community research.

---

## Results

The evaluation metric used throughout the study is **Accuracy (%)**.

| Task / Benchmark | Sub-Task / Metric | Accuracy | Notes |
| :--- | :--- | :--- | :--- |
| **Binary Safety Classification** | **Max Accuracy (R-Judge)** | **81.10%** | Competitive performance |
| | **Max Accuracy (ASSE-Safety)** | **92.80%** | **State-of-the-Art (SOTA)** |
| **Fine-Grained Safety Classification (ATBench)** | **General ATBench** | **82.00%** | Overall fine-grained performance |
| | **Risk Source Max Accuracy** | **32.40%** | Identified as significantly more difficult |
| | **Failure Mode Max Accuracy** | **59.20%** | Higher success rate than source identification |

---

**References:** 31 citations