# Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets

*Nikolaos Pavlidis; Vasilis Perifanis; Symeon Symeonidis; Pavlos S. Efraimidis*

---

### Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Citations** | 32 References |
| **Dataset Scope** | Small-scale (Max 500 instances) |
| **Task Types** | Classification, Regression, Clustering |
| **Key Models** | GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, DeepSeek-R1 |
| **Training Paradigm** | Zero-training (No fine-tuning) |

---

## Executive Summary

> This paper investigates the viability of Large Language Models (LLMs) as "universal predictors" for structured tabular data in small-scale environments (â‰¤500 instances). In scenarios where traditional machine learning models struggle with data scarcity or resource intensity, the authors evaluate whether the semantic reasoning capabilities of LLMs can generalize to structured analytics without task-specific fine-tuning.

The study introduces a **zero-training evaluation framework** leveraging In-Context Learning (ICL) with few-shot prompting. Technically, it utilizes a distinct data serialization strategy where tabular features undergo z-score normalization and are converted into text-based representations (text tables for supervised tasks, raw vectors for clustering). The study benchmarks state-of-the-art models (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, DeepSeek-R1) against traditional ML baselines and Tabular Foundation Models (TabPFN).

**Empirical results reveal a distinct performance divergence:**
*   **Classification:** LLMs achieved competitive performance on datasets like *Iris*, *Lupus*, and *Bankrupt*, yielding accuracy and F1 scores comparable to specialized models.
*   **Regression:** Significant underperformance was observed on datasets like *Diabetes* and *Friedman*, with high RMSE due to difficulties in mapping predictions to continuous spaces.
*   **Clustering:** Utility was negligible (e.g., *Mall*, *Wholesale*) due to the absence of genuine ICL mechanisms for unsupervised tasks.

This research demarcates the operational boundaries of LLMs in tabular analytics, validating their efficacy for rapid classification on small datasets while exposing limitations in regression and clustering. It establishes a pathway for low-overhead data exploration, cautioning practitioners against using LLMs as a panacea and advocating for their integration as targeted tools.

---

## Key Findings

*   **Strong Classification Capabilities:** LLMs demonstrate robust predictive performance on small-scale classification tasks when utilizing few-shot prompting.
*   **Regression Limitations:** LLMs significantly underperform compared to traditional ML models in regression tasks, primarily due to difficulty predicting outputs in continuous spaces.
*   **Clustering Constraints:** The utility of LLMs for clustering is limited by the absence of genuine In-Context Learning mechanisms for unsupervised tasks.
*   **Sensitivity to Engineering:** Function approximation quality is highly sensitive to prompt engineering, specifically regarding context size and prompt structure.
*   **Task-Specific Divergence:** Performance is strictly correlated with task type; effective for discrete classification labels, inadequate for continuous numerical prediction.

---

## Methodology

The study employs an empirical investigation into the function approximation capabilities of LLMs using small-scale structured datasets.

*   **Scope:** Evaluation across three primary tasks: **Classification**, **Regression**, and **Clustering**.
*   **Technique:** Utilization of **few-shot prompting** and **In-Context Learning (ICL)** without any model fine-tuning or weight updates.
*   **Benchmarking:** Direct comparison against traditional ML models (Linear models, Decision Trees, AutoML) and Tabular Foundation Models (TabPFN).
*   **Models Evaluated:**
    *   OpenAI: GPT-5, GPT-4o, GPT-o3
    *   Google: Gemini-2.5-Flash
    *   DeepSeek: DeepSeek-R1

---

## Technical Details

*   **Paradigm:** Zero-training prediction using LLMs as universal predictors via In-Context Learning (ICL).
*   **Input Processing:**
    *   **Serialization:** Tabular data is serialized into text tables for classification and regression.
    *   **Clustering:** Raw feature vectors are used for clustering tasks.
    *   **Normalization:** All data undergoes z-score normalization prior to processing.
*   **Constraints:**
    *   Strict limitation to small-scale datasets (maximum 500 instances).
    *   No data augmentation utilized to prevent data leakage.
*   **Validation Metrics:** Performance measured using **Accuracy**, **F1 scores**, and **Root Mean Square Error (RMSE)**.

---

## Results

The study highlights significant variations in LLM performance based on the analytical task and dataset structure.

### Classification
*   **Datasets:** Iris, Lupus, Bankrupt.
*   **Outcome:** Strong predictive capabilities. LLMs achieved accuracy and F1 scores that rivaled specialized models when few-shot examples were provided.

### Regression
*   **Datasets:** Diabetes, Servo, Friedman.
*   **Outcome:** Significant underperformance. LLMs exhibited substantially higher RMSE compared to traditional baselines due to inherent difficulties in mapping predictions to continuous numerical spaces.

### Clustering
*   **Datasets:** Mall, Wholesale, Moon.
*   **Outcome:** Limited utility. The absence of genuine ICL mechanisms for unsupervised tasks resulted in poor clustering performance.

---

## Contributions

*   **Comprehensive Evaluation:** Provided a broad assessment of LLMs as universal predictors specifically for structured, small-scale data.
*   **Boundary Definition:** Clearly defined the limits of LLM applicability in tabular analytics, highlighting effectiveness in classification while identifying severe limitations in regression and clustering.
*   **Framework Establishment:** Created a framework for deploying LLMs in rapid, low-overhead data exploration and business intelligence contexts.

---

**References:** 32 citations
**Quality Score:** 7/10