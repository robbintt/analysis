# CoSMoEs: Compact Sparse Mixture of Experts

*Patrick Huber; Akshat Shrivastava; Ernie Chang; Chinnadhurai Sankar; Ahmed Aly; Adithya Sagar*

---

> ### ðŸ“Š Quick Facts
> *   **Base Architecture:** Llama 3-based Sparse MoE
> *   **Training Data:** 1.4T tokens (FineWeb Education)
> *   **Phone Model:** 1.42B Active / 3.65B Total Parameters
> *   **Wearable Model:** 188M Active / 377M Total Parameters
> *   **Performance Gain:** +2.35% over dense baselines
> *   **Routing Strategy:** Token Choice (top-k=2)

---

## Executive Summary

Deploying Large Language Models (LLMs) on resource-constrained edge devices, such as smartphones and wearables, requires navigating a critical trade-off between model quality and strict hardware limits on memory and latency. While Sparse Mixture of Experts (MoE) architectures theoretically offer high performance with low active parameter counts, their practical feasibility for on-device inference has been questioned due to system overheads, particularly memory fragmentation and the high latency cost of offloading experts between memory and compute units. Furthermore, industry benchmarks frequently rely on FLOP-based comparisons (which measure theoretical computation) that fail to capture the practical bottlenecks of hardware deployment, leading to potentially misleading conclusions about model efficiency. This paper addresses the need to determine if, under rigorously fair evaluation conditions that remove these confounding factors, MoE architectures can realistically outperform dense models on actual edge hardware.

The authors introduce **CoSMoEs (Compact Sparse Mixture of Experts)**, a framework built on a Llama 3 backbone and optimized specifically for Quality, Memory, and Latency (QML). The architecture employs two key technical innovations: Weight-Decomposed Experts (WD) and Block-wise Expert Selection (BlES). WD utilizes low-rank matrix decomposition (rank $r = \text{hidden}/2$) to compress expert parameters significantly, allowing the model to recover quality without expanding memory footprint. To address latency bottlenecks, BlES optimizes inference by minimizing the frequency of expert switchingâ€”a primary source of offloading delay. This is achieved through a sequence-level loss function composed of hard and soft components that penalize rapid changes in expert selection, enforcing a "block-wise" routing pattern. The framework utilizes top-2 token choice routing and is architected to maximize system-level offloading efficiency.

In a rigorous evaluation trained on 1.4T tokens from FineWeb Education, CoSMoEs demonstrated significant superiority over FLOP-aligned dense baselines. To ensure a fair comparison, the authors controlled for both memory (Total Parameter count) and compute (FLOPs), removing the bias often found in standard benchmarks. The MoE architectures achieved an absolute improvement of **+2.35%** in language modeling performance, with Weight-Decomposed Experts contributing an additional gain of up to **+1.1%**. Critically, the system-level optimizations yielded tangible speedups; by reducing expert switching frequency, BlES effectively decreased the latency penalties typically associated with sparse architectures. The authors validated the approach at two scales: a "Phone-sized" model with 1.42B active parameters (3.65B total) across 26 layers, and a "Wearable-sized" model with 188M active parameters (377M total) across 32 layers. The success of the wearable model is particularly significant as it demonstrates that MoE architectures remain stable and effective even at extremely low sparsity factors (approx. 2x), a regime where routing instability typically undermines performance.

This research confirms that Sparse MoE architectures are not only viable but are the superior choice for resource-constrained on-device inference compared to dense models. By successfully decoupling theoretical FLOPs from actual system performance and introducing mechanisms like weight decomposition and block-wise selection, the study provides a replicable blueprint for deploying efficient, high-quality LLMs on edge hardware. The findings establish a new standard for fair evaluation in the fieldâ€”emphasizing the need to account for offloading overhead and memory parityâ€”and strongly suggest that future developments in mobile AI should prioritize sparse, routing-based architectures to overcome the inherent bottlenecks of current dense models.

---

## Key Findings

*   **Superiority at Scale:** In a fair evaluation, Sparse Mixture of Experts (MoE) architectures outperform dense models at the on-device scale.
*   **Feasibility Confirmed:** MoE models are confirmed feasible for effective on-device inference.
*   **Efficiency Gains:** The proposed approach significantly improves model offloading efficiency, reducing inference latency.
*   **Decomposition Benefits:** The introduction of weight-decomposed experts further improves the overall performance of MoE models.

---

## Methodology

The authors introduced **Compact Sparse Mixture of Experts (CoSMoEs)**, a framework designed to optimize the balance of Quality, Memory, and Latency. To achieve this, they implemented **weight-decomposed experts** to enhance model quality and deployed specific strategies to improve offloading efficiency. A core aspect of the methodology was the focus on fairness; the evaluation removed confounding factors to directly compare MoE architectures against FLOP-aligned dense models, ensuring that performance gains were attributable to the architecture rather than benchmark bias.

---

## Contributions

*   **Validation of Edge MoE:** Established that Sparse MoE architectures are viable and superior choices for resource-constrained on-device environments.
*   **Architectural Innovation:** Introduced **'weight-decomposed experts'** as a novel architectural component designed to improve performance.
*   **System Optimization:** Contributed a system-level optimization solution addressing memory and latency bottlenecks through enhanced offloading efficiency.

---

## Technical Details

*   **Architecture Base:** Sparse Mixture of Experts (MoE) based on **Llama 3**.
*   **Target Optimization:** Optimized specifically for edge devices.
*   **Routing Strategy:** Utilizes **Token Choice routing** with top-k=2.
*   **Weight-Decomposed Experts (WD):**
    *   Reduces parameters via matrix decomposition.
    *   Configuration: rank \`r = hidden/2\`.
*   **Block-wise Expert Selection (BlES):**
    *   Designed to reduce latency.
    *   Utilizes a sequence-level loss penalizing expert switching.
    *   Loss composition: Hard and soft components.
    *   Includes adjusted load balancing mechanisms.

---

## Results

*   **Performance:** MoE models outperform dense baselines by **+2.35%** absolute in language modeling.
*   **Decomposition Impact:** WD (Weight-Decomposed Experts) adds up to **+1.1%** improvement.
*   **Training Specs:** 1.4T tokens (FineWeb Education) for 310k steps.
*   **Model Scaling:**
    *   **Phone-sized Model:** ~1.42B active parameters (3.65B total) with 26 layers.
    *   **Wearable-sized Model:** 188M active parameters (377M total) with 32 layers.

---

**Paper Quality Score:** 8/10  
**References:** 10 citations