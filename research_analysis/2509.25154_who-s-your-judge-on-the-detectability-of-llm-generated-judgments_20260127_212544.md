---
title: Who's Your Judge? On the Detectability of LLM-Generated Judgments
arxiv_id: '2509.25154'
source_url: https://arxiv.org/abs/2509.25154
generated_at: '2026-01-27T21:25:44'
quality_score: 9
citation_count: 30
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Who's Your Judge? On the Detectability of LLM-Generated Judgments

*Dawei Li, Your Judge, Chengshuai Zhao, Machine Learning, Zhen Tan, Pingchuan Ma, Baixiang Huang, Data Mining, Bohan Jiang, Generated Judgments*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 30 citations
> *   **Models Evaluated:** 20 LLMs (API-based, Open-source, Reasoning, Specialized)
> *   **Core Metric:** F1 Score (Higher = Easier Detection)
> *   **Elo Range:** ~1100 to ~1400 (LMArena Score)
> *   **Key Variables:** Group Size ($k$), Judgment Dimensions, Rating Scale Granularity

---

## Executive Summary

As the "LLM-as-a-judge" paradigm becomes the standard for evaluating Large Language Models (LLMs), distinguishing between human and machine-generated evaluations is a growing concern. This paper addresses the critical question of the **detectability of LLM-generated judgments**. This problem matters because the integrity of automated evaluation pipelines relies on understanding whether AI judges introduce distinct distributional artifacts that could distinguish them from human raters, potentially compromising the validity of benchmarks or leading to undetected biases in performance assessment.

The key innovation is the use of **"Judgment-Intrinsic Features,"** a method that leverages distributional signals within evaluation data to identify statistical patterns indicative of machine origin. The authors rigorously test this detection methodology across **20 diverse LLMs**—including API-based, open-source, reasoning-specialized, and dedicated judge models—to evaluate how three experimental variables affect detectability:

1.  **Group Size ($k$):** The number of judgments aggregated per item.
2.  **Judgment Dimensions:** Comparing single-dimensional versus multi-dimensional scoring.
3.  **Rating Scale Granularity:** Contrasting fine-grained with coarse-grained scales.

The study measures detectability using **F1 Score**, where higher scores indicate easier detection. Results show that detectability increases significantly with Group Size, rising from 63.9% ($k=1$) to 85.0% ($k=16$). Conversely, detectability decreases when evaluations are simpler: it drops from 97.2% with 5 dimensions to 72.3% with 1 dimension, and falls from 72.9% to 57.7% when moving to coarser rating scales.

The analysis reveals that **API-based models are harder to detect** than open-source models, **larger models are less detectable** than smaller ones within the same family, and **specialized judge models demonstrate higher robustness**. A correlation analysis between F1 Score and LMArena Elo scores (~1100 to ~1400) confirms that higher-ranking models (F1 80-94%) are generally less detectable than lower-performing ones.

These findings highlight a fundamental trade-off in automated evaluation design. While aggregating more judgments (higher $k$) is often used to improve evaluation reliability, this research demonstrates that it simultaneously amplifies distributional signals, making AI-generated judgments statistically more apparent. As LLMs become more capable—mirroring the correlation between higher Elo scores and lower detectability—the distinction between human and machine evaluation will blur, challenging current validation methods. This work provides a necessary framework for auditing LLM judges and suggests that future benchmarks must account for the statistical detectability of automated feedback.

---

## Key Findings

*   **Detectability Increases with Aggregation:** As the Group Size ($k$) increases, making AI-generated judgments easier to detect. On the Helpsteer3 dataset, F1 scores rose from **63.9% ($k=1$)** to **85.0% ($k=16$)**.
*   **Simplicity Hides Machine Origin:** Simpler evaluation metrics are harder to detect.
    *   Reducing dimensions from 5 to 1 caused the F1 score to drop from **97.2% to 72.3%** (NeurIPS dataset).
    *   Moving to coarser rating scales caused a drop from **72.9% to 57.7%** (Helpsteer3 dataset).
*   **Model Quality Correlates with Stealth:** There is an inverse correlation between model capability and detectability. Higher LMArena Elo scores are associated with lower F1 scores (harder to detect).
*   **Model Type Matters:**
    *   **API-based models** are harder to detect than open-source models.
    *   **Larger models** within the same family are less detectable than smaller ones.
    *   **Specialized Judge/Reasoning models** (e.g., RISE-Judge) show higher robustness against detection.

---

## Technical Approach

While the specific methodology section was not fully available in the provided excerpt, the Technical details outline a robust experimental framework based on "Judgment-Intrinsic Features."

### Methodology
The core methodology involves extracting **Judgment-Intrinsic Features** derived from distributional signals found within the evaluation data. This approach allows for the identification of statistical patterns specific to machine-generated judgments without relying solely on surface-level text analysis.

### Experimental Setup
The study evaluates 20 distinct LLMs, categorized into four groups:
1.  **API-based models** (e.g., GPT-5-mini)
2.  **Open-source models** (e.g., LLaMA-3.1)
3.  **Reasoning models** (e.g., o4-mini)
4.  **Specialized Judge models** (e.g., RISE-Judge)

### Variables Tested
The detection capabilities were tested against three specific experimental variables to understand their impact on the distributional signals:

| Variable | Description | Comparison |
| :--- | :--- | :--- |
| **Group Size ($k$)** | Number of judgments aggregated per item. | Small groups vs. Large groups |
| **Judgment Dimensions** | Complexity of the scoring criteria. | Single-dimensional vs. Multi-dimensional |
| **Rating Scale Granularity** | Resolution of the scoring scale. | Fine-grained vs. Coarse-grained |

---

## Experimental Results

The results present a clear trend regarding how evaluation design choices impact the ability to detect LLM judges. The primary metric used is the **F1 Score**, where a higher percentage indicates a higher likelihood of detection.

### Impact of Experimental Variables

| Experimental Variable | Trend | Data Points (F1 Score) |
| :--- | :--- | :--- |
| **Group Size ($k$)** | **Detectability Increases** | • $k=1$: 63.9%<br>• $k=16$: 85.0% (on Helpsteer3) |
| **Judgment Dimensions** | **Detectability Decreases** | • 5 Dimensions: 97.2%<br>• 1 Dimension: 72.3% (on NeurIPS) |
| **Rating Scale Granularity**| **Detectability Decreases** | • Fine-grained: 72.9%<br>• Coarse-grained: 57.7% (on Helpsteer3) |

### Model Performance & Correlation
*   **Correlation Analysis:** Performed between F1 Score and LMArena Score (Elo).
*   **Range:** Models ranged from **~1100 to ~1400 Elo**.
*   **F1 Range:** Scores fell between **80% and 94%**.
*   **Interpretation:** Models with higher Elo scores (better performance) tended to have lower F1 scores, making them harder to detect as AI judges compared to lower-performing models.

---
**Report Generated based on Analysis Provided**