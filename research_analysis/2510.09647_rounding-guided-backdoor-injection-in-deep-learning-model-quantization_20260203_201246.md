---
title: Rounding-Guided Backdoor Injection in Deep Learning Model Quantization
arxiv_id: '2510.09647'
source_url: https://arxiv.org/abs/2510.09647
generated_at: '2026-02-03T20:12:46'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Rounding-Guided Backdoor Injection in Deep Learning Model Quantization

*Xiangxiang Chen; Peixin Zhang; Jun Sun; Wenhai Wang; Jingyi Wang*

---

### ðŸ“Š Quick Facts

> **Attack Success Rate (ASR):** Nearly 100% across tested models  
> **Clean Accuracy Impact:** Negligible degradation compared to standard quantization  
> **Key Efficiency:** 99% ASR on VGG-16 by manipulating only **3%** of weights  
> **Target Scope:** ResNet-18, VGG-16, ViT, BERT (CIFAR-10/100, Tiny-ImageNet, SST-2, IMDb)  
> **Novelty:** First attack framework exploiting rounding operations in quantization  

---

## ðŸ“„ Executive Summary

This research addresses a critical security vulnerability within the model deployment pipeline, specifically during the **model quantization process**. While existing backdoor attack literature primarily focuses on compromising models during the training phaseâ€”through data poisoning or gradient manipulationâ€”the quantization phase has remained largely unexplored as an attack vector.

This oversight is significant because quantization is essential for deploying deep learning models on resource-constrained edge devices and is often handled by third-party service providers or performed post-training. The authors highlight that this phase presents a high-risk blind spot where malicious actors can compromise a model without access to the original training data or the training procedure, rendering traditional training-time defenses ineffective.

The authors introduce **QuRA (Quantization-aware Backdoor Attack)**, the first framework designed to exploit rounding operations during model quantization to inject backdoors. Unlike conventional attacks that modify weight values, QuRA operates solely during the quantization phase by optimizing the rounding directions of specific "critical weights." Technically, the method utilizes the **Hessian matrix and gradients** to estimate the impact of individual weights on benign accuracy versus backdoor efficacy.

The framework treats discrete rounding as a continuous optimization problem, minimizing a composite loss function that includes Accuracy Loss (derived from a simplified Hessian), Backdoor Loss (cross-entropy at the output layer), and a Penalty term using Lagrangian relaxation. By identifying weights where rounding for backdoor injection aligns with accuracy preservation, QuRA can freeze non-critical weights and strategically manipulate only a small subset to embed malicious behavior.

Empirical evaluations demonstrate that QuRA achieves near-perfect attack success rates while maintaining the model's standard utility. Across diverse architectures such as ResNet-18, VGG-16, Vision Transformers (ViT), and BERT, the attack consistently achieves nearly 100% ASR. The method is highly efficient and stealthy; for example, on VGG-16 with CIFAR-10, QuRA achieved a 99% ASR by manipulating only 3% of the weights per layer. Furthermore, the attack induced negligible degradation in Clean Accuracy (CA) compared to standard quantization baselines and successfully evaded state-of-the-art defense mechanisms, including those specifically designed to detect quantization attacks.

The significance of this research lies in exposing a fundamental new attack surface in the machine learning supply chain: the optimization phase after training. By proving that malicious functionality can be injected through rounding operations without altering the training process or data, the authors challenge the assumption that quantization is a purely mechanical, benign operation. This finding necessitates a paradigm shift in how the industry approaches model security, suggesting that trust must be extended to the quantization tools and supply chains, not just the training data. The release of QuRAâ€™s implementation code provides a vital resource for the community to develop and test quantization-aware defenses.

---

## ðŸ” Key Findings

*   **High Efficacy:** QuRA achieves nearly **100% attack success rates** with negligible degradation in the model's standard performance.
*   **Stealth Capability:** The attack effectively evades existing backdoor defense mechanisms, highlighting its stealth and significant threat potential.
*   **New Attack Vector:** Identifies a critical security vulnerability within the model quantization process that is entirely separate from training risks.
*   **No Training Access Required:** Embeds malicious behaviors without poisoning training data or manipulating the model training process.

---

## âš™ï¸ Methodology

QuRA operates solely during the model quantization phase, distinct from conventional attacks. Its approach is characterized by the following:

*   **Phase of Operation:** Functions during post-training or fine-tuning quantization.
*   **Strategy:** Identifies specific **'critical weights'** that influence the backdoor target to preserve benign accuracy.
*   **Core Mechanism:** Optimizes the rounding direction of these selected weights to amplify the backdoor effect without degrading the model's utility.

---

## ðŸ”§ Technical Details

QuRA operates during the model quantization phase to inject backdoors by manipulating the rounding directions of quantized weights, without requiring access to original training data.

**Optimization Framework**
*   **Weight Analysis:** Uses the Hessian matrix and gradients to estimate weight impact.
*   **Conflict Resolution:** Identifies weights where backdoor and accuracy rounding align to freeze them; resolves conflicts using a priority score.
*   **Continuous Relaxation:** Relaxes discrete rounding into a continuous variable $V \in [0,1]$ to minimize a total loss function.
*   **Loss Components:**
    1.  **Accuracy Loss:** MSE with simplified Hessian.
    2.  **Backdoor Loss:** Cross-entropy at the last layer.
    3.  **Penalty Loss:** Lagrangian relaxation.
*   **Final Rounding:** Weights are rounded based on the threshold $V > 0.5$.

**Efficiency**
*   The process often requires manipulation of only a small percentage of weights (e.g., **3% in VGG-16**).

---

## ðŸ“ˆ Results

QuRA demonstrates robust performance across a wide range of architectures and datasets:

*   **Attack Success Rate (ASR):** Achieved nearly 100% ASR across evaluated models including ResNet-18, VGG-16, ViT, and BERT.
*   **Efficiency Metrics:** Achieved a 99% ASR on VGG-16 (CIFAR-10) by manipulating only **3%** of the weights per layer.
*   **Stealthiness:** Maintained negligible degradation of Clean Accuracy (CA) compared to standard quantization.
*   **Datasets Covered:** CIFAR-10, CIFAR-100, Tiny-ImageNet, and various NLP datasets (SST-2, IMDb, etc.).
*   **Baselines:** Outperformed or matched baselines like TQAttack and TBT while evading defenses.

---

## âœ¨ Contributions

*   **Framework Introduction:** Introduced **'QuRA,'** the first backdoor attack framework that exploits rounding operations in model quantization.
*   **Technique Development:** Developed a weight selection and rounding optimization technique that decouples malicious behavior injection from the training phase.
*   **Comprehensive Evaluation:** Provided empirical evaluation demonstrating the severity of quantization vulnerabilities and the evasion of state-of-the-art defenses.
*   **Open Source:** Released implementation code to facilitate future research and verification.

---

**Quality Score:** 8/10  
**References:** 40 citations