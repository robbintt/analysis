# Solution for Meta KDD Cup'25: A Comprehensive Three-Step Framework for Vision Question Answering

*Zijian Zhang; Xiaocheng Zhang; Yang Zhou; Zhimin Lin; Peng Yan*

***

> **### ðŸ“Š Quick Facts**
> - **Team:** BlackPearl
> - **Base Model:** Llama-3.2-11B-Vision-Instruct
> - **Optimization Strategy:** LoRA + vLLM
> - **Latency Constraint:** < 30 seconds/query
> - **Dataset:** CRAG-MM (5,000 images, 13 domains)
> - **Top Achievement:** 1st Place Automatic (Task 3), 2nd Place Human (Task 3)

***

## Executive Summary

This research addresses the persistent challenges in Vision Question Answering (VQA), specifically focusing on the limitations of current Vision Large Language Models (VLLMs). VLLMs frequently suffer from hallucinationsâ€”generating plausible but factually incorrect responsesâ€”and struggle with complex requirements such as retrieving information from multi-source documents and maintaining coherence across multi-turn interactions. These issues are critical to resolve as VQA systems increasingly handle real-world applications where accuracy, strict latency constraints (30 seconds per query), and reliability in processing visual-textual contexts are paramount.

The authors propose a comprehensive, modular three-step framework built upon a single-model strategy using the Llama-3.2-11B-Vision-Instruct backbone. Technically, the pipeline combines: (1) Data Augmentation, utilizing GPT-4o mini for label expansion and verification to enhance robustness; (2) Multi-modal Retrieval-Augmented Generation (RAG) with a reranking component to ground responses in relevant visual context and filter noise; and (3) Multi-task Fine-tuning via LoRA, optimized using vLLM to ensure strict adherence to inference latency requirements. This architecture effectively demonstrates that a singular, well-optimized model can outperform heavier ensemble approaches by integrating retrieval tightly into the generation process.

Evaluated on the CRAG-MM dataset, which consists of 5,000 images across 13 domains, the BlackPearl team secured top rankings in the Meta KDD Cup 2025. The solution achieved 3rd place in Task 1 ("Single-Source Augmentation") and 3rd place in Task 2 ("Multi-Source Augmentation"). Most notably, the framework dominated Task 3 (Multi-turn QA), securing 1st place in automatic evaluation and 2nd place in human evaluation. These metrics validate the framework's ability to handle diverse retrieval complexities and sustained interactions while minimizing hallucinations.

The significance of this work lies in its validation of a streamlined, single-model strategy for complex multi-modal retrieval tasks. By successfully mitigating VLLM hallucinations and improving multi-turn dialogue handling through rigorous RAG and augmentation, the authors set a new performance standard for the CRAG-MM benchmark. This research provides a generalizable blueprint for the field, proving that high performance in visual reasoning can be achieved without relying on unwieldy model ensembles, thereby offering a more efficient path toward deployable and accurate VQA systems.

***

## Key Findings

*   **High Competition Performance:** The BlackPearl team achieved automatic evaluation rankings of 3rd, 3rd, and 1st place across three tasks, and 2nd place in Task 3 human evaluation.
*   **Efficacy of Single-Model Strategy:** Validated that a single model with augmentation and RAG can handle complex multi-modal retrieval and visual question answering.
*   **Addressing VLLM Limitations:** The framework successfully mitigated VLLM issues like hallucinated answers and difficulties with multi-source retrieval and multi-turn interactions.

## Methodology

The proposed solution utilizes a single-model strategy dedicated to each task, implemented through a comprehensive pipeline. This pipeline includes:

*   **Data Augmentation:** To improve robustness.
*   **Multi-modal Retrieval-Augmented Generation (RAG):** To ground responses.
*   **Reranking:** To refine retrieval context.
*   **Multi-task Fine-tuning:** To optimize performance.

## Technical Details

The proposed solution is a modular framework using a Vision Large Language Model (VLLM) backbone enhanced by Retrieval-Augmented Generation (RAG).

*   **Model Backbone:** Llama-3.2-11B-Vision-Instruct
*   **Optimization:** Fine-tuned with LoRA and optimized via vLLM to meet strict latency constraints (30 seconds per query).
*   **Task 1 Specifics:**
    *   Utilizes Top-10 retrieval with text context extraction.
    *   Data augmentation workflow involving GPT-4o mini for answer verification and label expansion (n=10).
*   **Tasks 2 & 3 Specifics:**
    *   Incorporate multi-source retrieval.
    *   Include reranking components.
    *   Employ multi-task fine-tuning strategies.

## Results

Evaluated on the CRAG-MM dataset (5,000 images across 13 domains) with an 8:2 train/val split, the BlackPearl team secured top rankings in the KDD Cup 2025:

*   **Task 1 (Single-Source Augmentation):** 3rd Place
*   **Task 2 (Multi-Source Augmentation):** 3rd Place
*   **Task 3 (Multi-turn QA):** 1st Place (Automatic) / 2nd Place (Human)

The approach effectively mitigated hallucinations and validated the sufficiency of a single-model strategy.

## Contributions

*   **Advancement in VLLM RAG:** Contributed to solving persistent VQA challenges regarding visual context comprehension and multi-turn interactions.
*   **Benchmark Performance:** Set a high-performance standard for the CRAG-MM benchmark, ranking highly in both automatic metrics and human evaluation.
*   **Framework Generalization:** Demonstrated a generalized three-step framework (augmentation, retrieval/reranking, fine-tuning) to minimize hallucinations and improve accuracy in complex multi-modal problems.

***

**Quality Score:** 9/10  
**References:** 16 citations