# Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability

*Michael Merry; Pat Riddle; Jim Warren*

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Field:** Explainable AI (XAI) / Graph Theory
> *   **Core Method:** Structural Decomposition & Recomposition
> *   **Validation Model:** PREDICT (Cardiovascular Risk)

---

## Executive Summary

The field of Explainable AI (XAI) currently suffers from a fundamental lack of consensus regarding the definition of "inherent explainability," creating a critical barrier for high-stakes applications. The primary issue is that current assessments often rely on subjective intuition or basic, post-hoc metrics rather than verifiable structural properties. This ambiguity prevents regulators and developers from rigorously distinguishing between models that merely appear transparent and those that are mathematically auditable, posing significant risks for compliance and trust in sectors like healthcare.

The authors address this by introducing a **novel, graph-theoretic framework** that formalizes inherent explainability through structural decomposition and recomposition. Technically, the approach represents models as Directed Acyclic Graphs (DAGs), defined as $G = (V, E, W, \Phi)$. The framework decomposes these models into local structural components formalized as "Annotations"—a structure consisting of a Hypothesis regarding the component's behavior and Evidence verified through derivation or testing. These annotated, structure-local explanations are then recomposed to form a holistic, global explanation.

The proposed criterion was validated through the analysis of the **PREDICT cardiovascular disease risk prediction model**, which was successfully classified as inherently explainable. While the paper is theoretical, it provides significant qualitative insights: the analysis suggests that large regression models frequently fail to meet the criterion, whereas sparse neural networks may satisfy it. This research marks a pivotal shift from subjective intuition toward an objective, testable standard for AI transparency, offering regulators a concrete tool for compliance frameworks.

---

## Key Findings

*   **Definition Gap:** There is currently no consistent definition or test for "inherent explainability" in XAI, leading to an over-reliance on intuition.
*   **Novel Criterion:** A globally applicable criterion is introduced that utilizes graph theory to decompose models into structure-local explanations and recompose them into global explanations.
*   **Terminological Distinction:** The theoretical framework differentiates between models that are **"explainable"** (allow for explanation) and those that are **"explained"** (have a verified explanation).
*   **Model Disparities:** The framework justifies explainability disparities, suggesting large regression models are often not explainable while sparse neural networks may be.
*   **Real-World Validation:** The PREDICT clinical risk model was successfully analyzed and classified as inherently explainable using this framework.

---

## Methodology

The core methodology relies on representing the model’s mathematical structure through graph theory to enable visual and mathematical decomposition:

1.  **Graph Representation:** The model is represented as a graph, allowing for the isolation of specific structural nodes and edges.
2.  **Structural Decomposition:** The model is broken down into local structural components.
3.  **Annotation:** These components are formalized as "annotations." This acts as a verifiable hypothesis-evidence structure specific to a local part of the model.
4.  **Recomposition:** The annotated, structure-local explanations are aggregated and recomposed to form a holistic, global explanation of the entire model.
5.  **Validation:** The approach was validated by applying the criterion to the PREDICT clinical risk model currently in use in New Zealand, providing a full explanation of its logic.

---

## Technical Details

The paper proposes a graph-theoretic framework for formalizing 'inherent explainability' through structural decomposition and recomposition.

**Model Representation**
*   Models are represented as **Directed Acyclic Graphs (DAGs)**.
*   Formal Definition: $G = (V, E, W, \Phi)$
    *   **$V$ (Vertices):** Includes inputs, computations, and outputs.
    *   **$E$ (Edges):** Represents data flow between components.
    *   **$W$ (Weights):** Assigns real-valued weights to the graph.
    *   **$\Phi$ (Functions):** Allows arbitrary functions for operations to cover various model types (e.g., linear regressions, decision trees).

**Annotation Structure**
Explanations utilize an 'Annotation' structure consisting of two key parts:
*   **Hypothesis:** A claim about a specific component's behavior.
*   **Evidence:** Verification of the hypothesis via mathematical derivation, testing, or inference.

**Key Distinctions**
*   **Explainable vs. Explained:** The framework strictly defines models that are 'explainable' (have the capacity for explanation) versus those that are 'explained' (have a verified explanation present).
*   **Context Definition:** Defines context based on three parameters:
    1.  Audience
    2.  Language
    3.  Purpose

---

## Contributions

*   **Objective Standard:** Moves the field away from subjective intuition toward a specific, testable criterion for inherent explainability.
*   **Regulatory Tool:** Offers regulators a flexible yet rigorous test that can be integrated into compliance frameworks for AI systems.
*   **Formal Structure:** Establishes clear distinctions between "explainable" and "explained" models, providing a structure to formalize and compare future work in explainability.
*   **Clinical Template:** By demonstrating the inherent explainability of the PREDICT model, the paper provides a template for validating high-stakes clinical tools.

---

## Results

*   **PREDICT Model Analysis:** The framework was applied to the PREDICT cardiovascular disease risk prediction model. It was successfully analyzed and classified as inherently explainable.
*   **Model Architecture Insights:** The analysis suggested that large regression models are often not explainable under this criterion, while sparse neural networks may be.
*   **Axiomatic Alignment:** The authors report alignment with existing axiomatic claims about explainability but emphasize the formal, auditable standard provided by this new method.
*   **Metrics:** No quantitative metrics or specific experimental benchmarks were included in the provided text; the focus remains on theoretical validation and structural proof.