# AI Safety is Stuck in Technical Terms -- A System Safety Response to the International AI Safety Report

*Roel Dobbe*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Document Type** | Conceptual Analysis / Critical Reflection |
| **Core Focus** | System Safety vs. Technical Framing |
| **Targeted Framework** | EU AI Act & Public Interest AI |

---

## Executive Summary

> This paper addresses a critical limitation in the current AI safety discourse: the fieldâ€™s over-reliance on purely technical framings of risk, which creates substantial barriers to effective policy and governance.

The author argues that dominant frameworks, such as the International AI Safety Report, focus disproportionately on engineering fixes and computational metrics, thereby ignoring the sociotechnical nature of AI deployment. This narrow focus is problematic because it treats safety as a solvable mathematical problem distinct from political, social, and ecological realities, ultimately hindering the development of robust regulatory frameworks like the EU AI Act and risking ineffective mitigation of real-world harms.

The key innovation is a proposed methodological pivot from standard "AI safety" practices to the established discipline of "System Safety" as the foundational blueprint for governance. Instead of proposing a new technical architecture or model, the author advances a conceptual framework that systematically integrates non-technical factorsâ€”such as political context, social impact, and ecological consequencesâ€”directly into safety analysis.

This approach critiques existing governance efforts by contrasting the current technical fixation against proven system safety principles, offering a structured pathway to embed sociotechnical complexity into the General-Purpose AI Code of Practice and Public Interest AI initiatives. As this research is a critical reflection and conceptual analysis rather than an experimental study, it does not produce quantitative benchmarks, loss functions, or accuracy metrics.

The primary qualitative findings identify that the International AI Safety Reportâ€™s reliance on technical jargon actively obstructs policy advancement by failing to account for systemic risks. Additionally, the paper provides a qualitative assessment of the global AI landscape, noting that new AI systems emerging from Chinese investors now rival advanced US systems in both performance and resource requirements, thereby challenging the notion that technical leadershipâ€”and by extension, safety definitionâ€”is geographically or culturally monolithic.

The significance of this work lies in its potential to fundamentally reshape the operationalization of the European AI Act and direct future funding strategies for Public Interest AI. By establishing System Safety as the requisite discipline, the author provides a roadmap for regulators to move beyond checking boxes for technical specifications and toward enforcing holistic, sociotechnical safety standards.

---

## Key Findings

*   **Over-Technical Framing:** The current dominant framing of AI safety is overly focused on technical terms, which hinders effective policy development.
*   **Sociotechnical Risks:** AI safety risks are inherently sociotechnical and cannot be addressed by technical fixes alone.
*   **System Safety Blueprint:** The established system safety discipline offers a proven blueprint for integrating non-technical factors into governance.
*   **Regulatory Imperative:** Establishing a system safety discipline is critical for the successful implementation of the European AI Act and Public Interest AI investments.

---

## Methodology

The author utilizes a **system safety perspective** to conduct a critical reflection and conceptual analysis of the *International AI Safety Report*. The methodology involves contrasting the report's dominant technical framing against established system safety principles to identify gaps and propose a shift in governance approaches.

---

## Contributions

*   **Barrier Identification:** Provides a critical evaluation of high-level AI governance efforts to identify barriers created by purely technical framing.
*   **Methodological Shift:** Proposes a shift toward integrating non-technical factors using system safety concepts.
*   **Regulatory Guidance:** Outlines how system safety principles can improve regulatory frameworks like the EU AI Act and direct funding strategies.

---

## Technical Details

| Aspect | Description |
| :--- | :--- |
| **Core Proposal** | A conceptual shift to **'System Safety'** as the guiding discipline for AI governance. |
| **Integration Scope** | Proposes integrating non-technical and sociotechnical factors (e.g., political, social, ecological) into safety analysis rather than relying solely on technical fixes. |
| **Intended Application** | Designed to serve as a blueprint for the EU's AI Act (specifically the General-Purpose AI Code of Practice) and for realizing Public Interest AI. |
| **Architecture** | **N/A** (No specific AI model or technical architecture is proposed). |

---

## Results

*   **Quantitative Data:** None. No experimental results, benchmarks, loss functions, accuracy metrics, or quantitative data are included.
*   **Qualitative Assessment:** Includes a qualitative reference noting that new AI systems from Chinese investors rival advanced US systems in performance and resource requirements. No specific benchmark names or numerical scores were provided.