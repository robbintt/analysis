---
title: Short window attention enables long-term memorization
arxiv_id: '2509.24552'
source_url: https://arxiv.org/abs/2509.24552
generated_at: '2026-02-03T13:50:06'
quality_score: 9
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Short window attention enables long-term memorization

*LoÃ¯c Cabannes; Maximilian Beck; Gergely Szilvasy; Matthijs Douze; Maria Lomeli; Jade Copet; Pierre-Emmanuel MazarÃ©; Gabriel Synnaeve; HervÃ© JÃ©gou*

---

> ### ðŸ“Š Quick Facts
>
> *   **Architecture**: SWAX (Sliding Window Attention + xLSTM hybrid)
> *   **Parameters**: 1.4B (Training), 7B (Validation)
> *   **Training Data**: 150B tokens
> *   **Efficiency**: ~2x reduction in FLOPs/token compared to standard Transformers
> *   **Window Strategy**: Stochastic sampling (128â€“2048 tokens)
> *   **Best Long-Context Window**: 128 tokens
> *   **Best Short-Context Window**: 2048 tokens

---

## Executive Summary

**Problem**
This research addresses the critical efficiency bottleneck inherent in standard Transformer architectures, specifically the quadratic computational complexity ($O(N^2)$) of softmax attention with respect to sequence length. As the demand for long-context modeling grows, full-attention mechanisms become prohibitively expensive. While existing solutions like Linear Attention or linear Recurrent Neural Networks (RNNs) offer linear complexity, they often suffer from high memory interference or inferior recall capabilities compared to softmax attention. The paper specifically tackles the challenge of designing an architecture that maintains the high performance of attention on short-context tasks while achieving robust, efficient memorization over extended sequences without incurring the costs of full attention.

**Innovation**
The authors introduce SWAX (Sliding Window Attention + xLSTM), a novel hybrid architecture that alternates sliding-window attention (SWA) layers with xLSTM (matrix memory) layers. The core technical insight is counter-intuitive: constraining the attention window to a small size (e.g., 128 tokens) forces the model to rely less on immediate context and more on the xLSTM component for long-term retention, thereby training a more robust recurrent memory. To resolve the resulting trade-offâ€”where small windows aid long-term memory but hurt short-context reasoningâ€”the authors developed a stochastic training strategy. During training, the sliding window size is uniformly sampled between 128 and 2048 tokens per batch. This forces the model to leverage both the immediate retrieval capabilities of attention and the long-term memory of the RNN, creating a system that adapts dynamically to context requirements.

**Results**
In experiments using 1.4B parameter models trained on 150B tokens, SWAX demonstrated significant efficiency gains, achieving approximately 2x reduction in computational cost ($3.004\text{--}3.192 \times 10^9$ FLOPs/token) compared to standard Transformers ($6.174 \times 10^9$ FLOPs/token). Performance benchmarks revealed distinct behaviors based on window size: for long-context retrieval, shorter windows were superior (SWAX:128 outperformed SWAX:2048 and xLSTM alone), particularly at sequence lengths of 131k. Conversely, for short-context tasks, larger windows were necessary; SWAX:2048 matched standard Transformer performance on HumanEvalPlus (pass@10 score of 15.24 vs. 14.63), whereas SWAX:128 lagged behind (12.20). Crucially, the stochastic training regime effectively bridged this gap, outperforming fixed-window configurations on both short and long-context benchmarks.

**Impact**
The findings of this paper challenge the prevailing assumption that larger context windows are strictly better for long-term memorization, demonstrating instead that architectural constraints can lead to better memory utilization in hybrid systems. By successfully combining the pattern-matching strengths of attention with the efficient, state-based memory of linear RNNs, SWAX offers a viable path toward scaling models to massive context lengths without the quadratic penalty of full attention. This work revitalizes the role of RNNs in modern deep learning, suggesting that future state-of-the-art architectures may rely less on expanding context windows and more on optimizing the interplay between local attention and global recurrence.

---

## Key Findings

*   **Hybrid Superiority**: Architectures combining sliding-window softmax attention with linear RNN layers outperform isolated components.
*   **Window Size Paradox**: Larger sliding windows do not improve long-context performance; shorter windows force the model to rely more on xLSTM layers for long-term memory.
*   **Performance Trade-off**: Short windows are beneficial for long-term memory training but detrimental to short-context tasks.
*   **Stochastic Advantage**: SWAX trained with stochastic sliding window sizes outperforms fixed-window attention on both short-context and long-context problems.

---

## Methodology

The authors introduce **SWAX**, a hybrid neural architecture integrating sliding-window attention layers with xLSTM linear RNN layers. The model is trained using **stochastic sliding window sizes** to force the model to leverage both immediate context from attention and long-term memory retention from xLSTM.

---

## Contributions

*   **Architectural Innovation**: Introduction of SWAX, a novel hybrid model pairing sliding-window attention with xLSTM linear RNNs.
*   **Theoretical Insight**: Finding that smaller attention windows facilitate better long-term memorization by encouraging the recurrent component to learn more robustly.
*   **Training Strategy**: Development of a stochastic training regime for window sizes that resolves the conflict between short-context retrieval needs and long-context memory training.

---

## Technical Details

*   **Core Architecture**: SWAX (Sliding Window Attention + xLSTM) hybrid design using a **1:1 alternating ratio** of SWA and mLSTM (matrix memory) layers.
*   **Positional Embeddings**: Utilizes Sliding Window Attention with **Rotary Positional Embeddings (RoPE, theta=10000)**.
*   **Training Strategy**: Implements a **Stochastic Training Strategy** where window size is sampled uniformly between **128 and 2048** per batch to mitigate trade-offs between short-context reasoning and long-context training.
*   **Theoretical Basis**: The model contrasts Linear Attention ($O(1)$ memory, high interference) with Softmax Attention ($O(N)$ memory), using xLSTM's gated mechanisms (write, read, decay) to handle interference.

---

## Results

### Experimental Setup
*   **Models**: 1.4B parameter models trained on 150B tokens (seq len 16k), validated on 7B models.
*   **Efficiency**: SWAX achieves **~2x efficiency** ($3.004\text{--}3.192 \times 10^9$ FLOPs/token) compared to standard Transformers ($6.174 \times 10^9$ FLOPs/token).

### Performance by Context Length
*   **Long-Context**: Shorter windows improve retrieval (SWAX:128 > SWAX:2048 > xLSTM); Stochastic training outperforms fixed windows at 131k length.
*   **Short-Context**: Larger windows are superior; SWAX:2048 (40.88 avg) matches Transformer (41.57), while SWAX:128 (39.81) lags.

### Quantitative Benchmarks (1.4B)

| Benchmark | Metric | Transformer | SWAX:2048 | SWAX:128 |
| :--- | :--- | :--- | :--- | :--- |
| **Code Validation** | Perplexity | 2.431 | 2.523 | - |
| **HumanEvalPlus** | Pass@10 | 14.63 | **15.24** | 12.20 |

---

**Quality Score:** 9/10
**References:** 19 citations