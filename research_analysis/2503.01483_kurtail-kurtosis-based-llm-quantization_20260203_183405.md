---
title: 'KurTail : Kurtosis-based LLM Quantization'
arxiv_id: '2503.01483'
source_url: https://arxiv.org/abs/2503.01483
generated_at: '2026-02-03T18:34:05'
quality_score: 9
citation_count: 14
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# KurTail : Kurtosis-based LLM Quantization
*Mohammad Sadegh Akhondzadeh; Aleksandar Bojchevski; Evangelos Eleftheriou; Martino Dazzi*

---

> ### ðŸ“Š Quick Facts
>
> *   **MMLU Improvement:** +13.3% vs. QuaRot; +2.6% vs. SpinQuant
> *   **Perplexity Reduction:** -15.5% (Wiki) vs. QuaRot; -2.9% vs. SpinQuant
> *   **Hardware Efficiency:** Requires only **1 GPU** (vs. 4 GPUs for SpinQuant)
> *   **Processing Speed:** ~1 hour to quantize LLaMA3-70B
> *   **Compression:** 4-bit (Weights, Activations, and KV Cache)
> *   **Quality Score:** 9/10

---

## Executive Summary

Large Language Models (LLMs) demand substantial computational resources, driving the need for aggressive 4-bit quantization of weights, activations, and KV caches to reduce memory footprints and latency. However, Post-Training Quantization (PTQ) faces a persistent challenge: activation outliers that cause significant accuracy degradation when models are compressed to low precision. While prior rotation-based methods like SpinQuant have attempted to mitigate these outliers, they typically require expensive multi-GPU clusters for the quantization process, creating a barrier to accessibility for researchers and practitioners with limited hardware.

KurTail introduces a novel PTQ scheme that utilizes a Kurtosis-based rotation to statistically optimize the distribution of activations. The core innovation lies in minimizing kurtosisâ€”a statistical measure of distribution "tailedness"â€”to effectively smooth out outliers that hinder quantization. Technically, the method performs layer-wise optimization on the Stiefel Manifold using a proxy network and Caley optimizers (Adam or SGD). KurTail employs a hybrid rotation strategy consisting of "fusible" rotations ($R_1, R_2$) that merge directly with model weights for zero runtime overhead, and "online" rotations ($R_3, R_4, R_5$) for the KV cache that introduce minimal computational cost during inference.

In comparative benchmarks, KurTail sets a new standard for performance, significantly outperforming existing state-of-the-art methods. It achieves a 13.3% boost in MMLU accuracy and a 15.5% reduction in Wiki perplexity compared to QuaRot. Against the resource-intensive SpinQuant, KurTail delivers a 2.6% gain in MMLU accuracy and a 2.9% reduction in perplexity. Furthermore, the method demonstrates robustness by reducing maximum activation values by over 99% in both Multi-Head Self-Attention and Feed-Forward Networks. Crucially, KurTail achieves these results with high efficiency, requiring only a single NVIDIA H100 or A100 GPU and approximately one hour to quantize the LLaMA3-70B model, whereas SpinQuant necessitates a four-GPU setup.

KurTail represents a significant advancement in the field of model compression by successfully decoupling high-performance quantization from massive hardware requirements. By enabling the simultaneous 4-bit quantization of weights, activations, and the KV cache on a single GPU, the method democratizes access to state-of-the-art LLM compression, making it viable for consumer-grade hardware. This approach not only establishes a new benchmark for PTQ accuracy and perplexity but also provides a practical pathway for deploying massive models in resource-constrained environments without sacrificing the quality required for complex reasoning tasks.

---

## Key Findings

*   **Significant Accuracy Gains:** Achieved a **13.3% boost** in MMLU accuracy and a **15.5% reduction** in Wiki perplexity compared to QuaRot.
*   **Superior to SpinQuant:** Outperforms SpinQuant with a **2.6% gain** in MMLU accuracy and a **2.9% reduction** in perplexity.
*   **Hardware Efficiency:** Unlike SpinQuant, which requires four NVIDIA H100 GPUs, KurTail requires only a **single GPU** for operation.
*   **Effective Outlier Mitigation:** Successfully enables 4-bit quantization of weights, activations, and the KV cache while mitigating negative outlier effects.

---

## Methodology

The researchers propose **KurTail**, a post-training quantization (PTQ) scheme that utilizes a Kurtosis-based rotation. The core of the methodology includes:

*   **Kurtosis Optimization:** The method optimizes Kurtosisâ€”a statistical measure of distribution 'tailedness'â€”to specifically target and mitigate outliers in LLM activations.
*   **Layer-wise Optimization:** Implements layer-wise processing to ensure memory efficiency.
*   **Simultaneous Quantization:** Allows for the concurrent quantization of model weights, activations, and the KV cache.

---

## Technical Details

### Core Architecture
*   **Type:** Post-Training Quantization (PTQ).
*   **Target:** 4-bit quantization for weights, activations, and KV cache.
*   **Mechanism:** Kurtosis-based rotation learning to mitigate activation outliers.

### Rotational Framework
KurTail leverages a rotational framework with two distinct categories of rotations:
*   **Fusible Rotations ($R_1, R_2$):** Merge with model weights to ensure **zero runtime overhead**.
*   **Online Rotations ($R_3, R_4, R_5$):** Introduce minimal cost during inference.

### Optimization Process
*   **Objective:** Minimizes kurtosis loss on the **Stiefel Manifold**.
*   **Tools:** Utilizes a proxy network and Caley Adam or Caley SGD optimizers.
*   **Parameters:** Employs layer-wise inference using 500 WikiText samples over 100 iterations.

### Quantization Configurations
*   **Activations:** Per-token dynamic symmetric quantization (clipped at 0.98).
*   **KV Cache:** Asymmetric quantization.
*   **Weights:** Per-column symmetric quantization (utilizing RTN and GPTQ).

---

## Results

*   **Benchmark Performance:**
    *   **Vs. QuaRot:** +13.3% MMLU accuracy; -15.5% Wiki perplexity.
    *   **Vs. SpinQuant:** +2.6% MMLU accuracy; -2.9% perplexity.
*   **Outlier Reduction (LLaMA3-8B):**
    *   **MHSA:** 99.74% success rate in reducing max activation values.
    *   **FFN:** 99.96% success rate.
    *   Outperformed QuaRot in over 62% of cases.
*   **Operational Efficiency:**
    *   Required only **1x NVIDIA H100 or A100 GPU**.
    *   Approximate processing time of **1 hour** to transform LLaMA3-70B.

---

## Contributions

1.  **Novel Technique:** Introduction of a new rotation-based optimization technique leveraging Kurtosis to address outliers in LLMs.
2.  **Resource Accessibility:** Demonstration that high-performance 4-bit quantization can be achieved with significantly reduced computational resources (single GPU vs. multi-GPU clusters), making it accessible for consumer-grade hardware.
3.  **New Benchmark:** Establishment of a state-of-the-art PTQ method that effectively handles weights, activations, and KV cache simultaneously, setting new standards for accuracy and perplexity.

---
*References: 14 citations*