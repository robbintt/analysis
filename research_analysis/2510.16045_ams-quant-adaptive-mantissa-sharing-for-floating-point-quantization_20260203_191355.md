---
title: 'AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization'
arxiv_id: '2510.16045'
source_url: https://arxiv.org/abs/2510.16045
generated_at: '2026-02-03T19:13:55'
quality_score: 9
citation_count: 9
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization
*Mengtao Lv; Ruiqi Zhu; Xinyu Wang; Yun Li*

---

> ### üìä Quick Facts
> *   **Quality Score:** 9/10
> *   **Innovation:** First non-integer floating-point quantization for LLMs
> *   **Target Formats:** FP-5.33-e2m3, FP-4.25-e2m2
> *   **Peak Speedup:** 3.2x over FP16 inference
> *   **Memory Savings:** Up to 66.7% reduction
> *   **Key Mechanism:** Mantissa-bit Sharing

---

## üìã Executive Summary

This research addresses the inherent inefficiency of standard floating-point quantization, which is conventionally restricted to integer bit-widths (e.g., FP4, FP8, FP16). As Large Language Models (LLMs) scale, memory bandwidth and storage capacity become primary bottlenecks for inference deployment. While reducing precision improves speed, the discrete jumps between standard integer formats often force a harsh trade-off: aggressive compression leads to significant accuracy degradation, while conservative formats fail to maximize hardware efficiency. The paper highlights the need for more flexible quantization strategies that can exploit "sweet spots" between traditional integer formats to optimize resource utilization without sacrificing model reliability.

The core innovation, **AMS-Quant (Adaptive Mantissa Sharing)**, introduces a mechanism for non-integer floating-point quantization through a technique called "Mantissa-bit Sharing". By grouping $k$ quantized weights to share the least significant bit (LSB) of their mantissas, the method achieves non-standard formats such as FP-5.33-e2m3 and FP-4.25-e2m2. To manage the complexity of this grouping, the authors employ an "Adaptive Searching" strategy, an offline optimization process that identifies optimal configurations to minimize accuracy loss. The approach is practically realized through custom CUDA Linear kernels designed for weight-only quantization (activations remain FP16), optimizing memory access patterns to convert theoretical compression into tangible wall-clock latency reductions.

AMS-Quant demonstrates substantial performance improvements while preserving accuracy across large-scale models. Compared to standard FP16 inference, the method achieves up to a **2.77x speedup** and a **66.7% reduction in memory usage** with the FP-5.33-e2m3 format, while the FP-4.25-e2m2 format yields up to a **3.2x speedup**. In accuracy benchmarks on Llama-3.2-3B and Qwen3-4B using the GSM8k dataset, the FP6-e2m3 configuration maintained near-parity with FP16 baselines (e.g., 77.10 vs. 77.41 for Llama). Conversely, lower standard integer formats like FP4-e2m1 suffered significant accuracy drops, validating the efficacy of non-integer intermediate formats.

This work is significant as the first exploration of non-integer bit-width floating-point quantization for LLMs, effectively expanding the design space for model compression. By introducing Mantissa-bit Sharing as a novel compression technique and validating it with functional CUDA kernels, the authors bridge the gap between theoretical efficiency and practical system implementation. This research challenges the status quo of integer-restricted quantization, suggesting that future hardware and software co-design could benefit significantly from fine-grained, non-standard precision formats to deploy massive models more efficiently.

---

## üîë Key Findings

*   **Non-Integer Bit-width Achievement:** AMS-Quant successfully enables floating-point quantization at non-integer bit-widths, specifically achieving **FP-5.33-e2m3** and **FP4.25-e2m2** formats.
*   **Significant Speedup:** The method delivers substantial inference performance gains, achieving speedups of **2.8x** and **3.2x** over standard FP16 inference.
*   **Accuracy Preservation:** Despite the aggressive reduction in bit-width, the quantization maintains **negligible accuracy loss** on large-scale datasets and models.
*   **Hardware Efficiency:** The prototyped CUDA Linear kernels effectively translate theoretical memory savings into tangible wall-clock latency reductions by optimizing memory access patterns.

---

## üõ†Ô∏è Methodology

The core methodology, **AMS-Quant**, advances floating-point quantization by moving beyond integer bit-widths to non-integer "sweet spots." It is implemented through two primary components:

1.  **Mantissa-bit Sharing:** This technique groups $k$ quantized weights together to share the least significant bit of their mantissas.
2.  **Adaptive Searching:** An offline optimization strategy employed to identify optimal configurations and minimize accuracy degradation.

The approach is realized via efficient **CUDA Linear kernels** designed to reduce memory access and translate compression ratios into real-world speed.

---

## üåü Core Contributions

*   **Pioneering Non-Integer Quantization:** Presents the first exploration of floating-point quantization using non-integer bit-widths for LLMs.
*   **Novel Compression Technique:** Introduces "Mantissa-bit Sharing" as a new mechanism for weight compression that exploits redundancy in the least significant mantissa bits.
*   **Optimization Framework:** Proposes "Adaptive Searching" as a specific offline strategy to balance the trade-off between compression efficiency and model accuracy.
*   **Practical System Realization:** Extends the contribution beyond theory to practical application by delivering CUDA kernels that demonstrate real-world inference acceleration.

---

## ‚öôÔ∏è Technical Details

**Core Concept**
AMS-Quant proposes non-integer bit-width quantization to find efficiency sweet spots between integer formats (e.g., FP4 and FP6).

**Mechanism: Mantissa-bit Sharing**
*   Groups weights to share the least significant bit (LSB) of their mantissa.
*   Achieves formats like **FP4.25-e2m2** and **FP5.33-e2m3**.

**Optimization Strategy**
*   Employs an offline adaptive search to optimize weight grouping.
*   Prioritizes 2-bit exponents (E2M) to maintain sufficient dynamic range while allocating more bits to the mantissa to minimize error.

**Implementation**
*   Utilizes custom CUDA kernels for weight-only quantization (activations remain FP16).
*   Focuses on reducing memory bandwidth by optimizing weight restoration.

---

## üìà Performance Results

**Speedup and Memory Efficiency**
*   **FP-5.33-e2m3:** Achieves up to **2.77x speedup** and **66.7% memory reduction** compared to FP16.
*   **FP-4.25-e2m2:** Achieves up to **3.2x speedup**.

**Accuracy Benchmarks (GSM8k)**
*   **Llama-3.2-3B:**
    *   FP6-e2m3: **77.10** (vs FP16: 77.41)
    *   Maintained near-parity with baseline.
*   **Qwen3-4B:**
    *   FP6-e2m3: **89.76** (vs FP16: 89.69)
    *   Maintained near-parity with baseline.
*   **Comparison:** Lower bit-widths like **FP4-e2m1** suffered significant accuracy drops, highlighting the value of the non-integer intermediate formats.

---
*References: 9 citations*