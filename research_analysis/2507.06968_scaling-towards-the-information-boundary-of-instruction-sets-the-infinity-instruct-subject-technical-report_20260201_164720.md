# Scaling Towards the Information Boundary of Instruction Sets: The Infinity Instruct Subject Technical Report

*Li Du; Hanyu Zhao; Yiming Ju; Tengfei Pan*

---

> ### üìä Quick Facts
>
> *   **Dataset Size:** 1.5 Million instructions (final)
> *   **Initial Pool:** ~7 Million samples
> *   **Key Innovation:** Closed-loop data construction framework
> *   **Quality Score:** 8/10
> *   **References:** 38 Citations

---

## üìã Executive Summary

This research addresses the critical limitation of current Large Language Model (LLM) instruction-tuning datasets, which frequently fail to generalize effectively across complex tasks and rare domains due to insufficient coverage and depth. The authors argue that the prevailing approach of simply increasing data volume is inadequate, as existing datasets often lack the qualitative structure necessary to push models toward their information boundary. This gap hinders model performance on specialized knowledge and complex reasoning, creating a need for a more systematic approach to data curation that prioritizes information density over raw scale.

The core innovation is a novel, closed-loop instruction data construction framework designed to maximize coverage and depth through an iterative, diagnostic process. Technically, the system integrates four key components: a Hierarchical Multilingual Tagging System (utilizing Qwen-2.5-72B-Instruct) for bottom-up categorization; an Informative Seed Selection Algorithm (using Llama-2-7B-base) that filters data based on specific criteria such as "Hard-to-Follow," "Long-Tail," "Multi-Skill," and "Undertrained"; an adapted Evolutionary Data Synthesis Process that evolves samples along diversity and depth dimensions; and a Deficiency Diagnosis mechanism that employs an Oracle Model to identify missing skills for targeted defect-driven generation.

The framework yielded the **Infinity Instruct Subject dataset**, a high-quality corpus of 1.5 million instructions distilled from an initial pool of approximately 7 million samples. The seed selection process identified roughly 1.2 million seeds using quantitative thresholds. This work signifies a paradigm shift in LLM data strategy, transitioning from quantity-based expansion to qualitative improvement and efficient evolution. By demonstrating that data structure and targeted synthesis are superior to blind scaling, the authors provide a reproducible blueprint for constructing high-value instruction sets that approach the model's information boundary.

---

## üîç Key Findings

*   **Limitations of Current Data:** Current instruction datasets often fail at complex tasks and rare domains due to limited coverage and depth.
*   **Quality Over Quantity:** The Infinity Instruct Subject dataset (1.5M samples) proves that qualitative data structure is more critical than raw volume.
*   **Enhanced Coverage:** The dataset exhibits enlarged coverage and depth compared to synthesized counterparts.
*   **Effective Framework:** The proposed closed-loop framework successfully addresses specific model deficiencies.

---

## üõ†Ô∏è Methodology

The researchers developed a systematic instruction data construction framework using an **iterative closed-loop process**. This framework integrates four distinct stages designed to maximize coverage and depth:

1.  **Hierarchical Tagging System:** Organizes data structure.
2.  **Informative Seed Selection Algorithm:** Identifies high-value starting data points.
3.  **Evolutionary Data Synthesis Process:** Expands and evolves the data.
4.  **Model Deficiency Diagnosis:** Identifies and fixes weak points in the model.

---

## ‚öôÔ∏è Technical Details

The paper proposes a closed-loop instruction data construction framework consisting of four specific components:

**1. Hierarchical Multilingual Tagging System**
*   **Tool:** Qwen-2.5-72B-Instruct
*   **Function:** Creates fine-grained and domain-level tags via bottom-up processing.

**2. Informative Seed Instructions Selection**
*   **Tool:** Llama-2-7B-base
*   **Criteria:** Filters data based on four specific dimensions:
    *   **Hard-to-Follow:** Low loss reduction.
    *   **Long-Tail:** Tag frequency < 200.
    *   **Multi-Skill:** Samples with > 4 tags.
    *   **Undertrained:** High loss samples.

**3. Evolutionary Data Synthesis Process**
*   **Method:** Adapts *Evol-Instruct* to synthesize samples.
*   **Dimensions:** Focuses on diversity, concretizing, deepening, and diversification.
*   **Follow-up:** Includes multi-turn generation to enrich data.

**4. Deficiency Diagnosis & Defect-Driven Synthesis**
*   **Tool:** Oracle Model.
*   **Goal:** Identifies missing skills without mentioning named entities to ensure targeted generation.

---

## üåü Contributions

*   **Novel Framework:** A new data construction framework combining tagging, seed selection, evolutionary synthesis, and diagnosis-based generation.
*   **Infinity Instruct Subject Dataset:** A high-quality, structured dataset containing **1.5 million instructions**.
*   **Paradigm Shift:** Initiates a shift in data scaling philosophy from quantity expansion to qualitative improvement and efficient evolution.

---

## üìà Results

*   **Final Dataset Size:** ~1.5 million instructions (filtered from an initial pool of ~7 million samples).
*   **Seed Selection:** ~1.2 million seeds selected.
*   **Quantitative Thresholds:**
    *   **Long-Tail:** Frequency < 200.
    *   **Undertrained:** Loss > mean + 1.96*std.
    *   **Multi-Skill:** Complexity > 4 tags.
    *   **Hard Samples:** 50k selected.
*   **Performance:** The framework enlarges coverage and depth compared to existing datasets and effectively addresses model deficiencies, improving instruction-following capabilities across multiple foundation models.

---

**Quality Score:** 8/10 | **References:** 38 citations