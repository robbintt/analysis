---
title: 'QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient
  Fine-Tuning on Large Language Models'
arxiv_id: '2509.17428'
source_url: https://arxiv.org/abs/2509.17428
generated_at: '2026-02-03T20:23:45'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models

*Hyesung Jeon; Seojune Lee; Beomseok Kang; Yulhwa Kim; Jae-Joon Kim*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Test Model:** LLaMA-3.2-3B (4-bit quantized)
> *   **Rank Configuration:** $r=64$
> *   **Core Innovation:** Walsh-Hadamard Transform (WHT) Kernel
> *   **Key Metric:** Achieves near full-rank capacity vs. LoRA's <6.3%

---

## Executive Summary

This research addresses the critical challenge of fine-tuning Large Language Models (LLMs) under strict memory and computational constraints. While quantizing LLMs to low-bit precision (e.g., 4-bit) significantly reduces inference costs, it inevitably introduces quantization errors that degrade model accuracy. Existing Parameter-Efficient Fine-Tuning (PEFT) methods struggle to balance the trade-off between correcting these errors and maintaining efficiency: Low-Rank Adaptation (LoRA) offers limited representational capacity and often fails to correct complex errors, while Fourier Transform (FT)-based adapters provide higher capacity but incur prohibitive computational overhead due to complex floating-point operations. The paper focuses on resolving this tension to enable high-performance adaptation on quantized models without sacrificing efficiency.

The authors introduce **QWHA** (Quantization-Aware Walsh-Hadamard Adaptation), a framework that integrates quantization awareness directly into the adapter architecture. The core technical innovation is the substitution of traditional Fourier kernels with the **Walsh-Hadamard Transform (WHT)**. Unlike standard FT kernels, the WHT kernel utilizes only $\pm 1$ entries, enabling fast recursive computation and drastically reducing computational overhead while maintaining orthogonality for full-rank representation.

QWHA formulates the weight update as $\Delta W = F H^{-1}$, where $F$ is a trainable sparse matrix and $H^{-1}$ is the inverse WHT matrix. Furthermore, the method employs an **"AdaAlloc"** initialization strategy that adaptively selects parameters and refines values to reconstruct quantization errors ($\Delta W_Q = W^0 - W^Q$) before the fine-tuning process begins.

Empirical validation demonstrates that QWHA delivers significant quantitative improvements over state-of-the-art baselines. While LoRA is limited to a representational capacity of less than 6.3%, QWHA achieves nearly full-rank capacity. Crucially, due to the efficiency of the $\pm 1$ WHT kernel, QWHA delivers these accuracy improvements with faster training times compared to standard Fourier-related transform adapters.

---

## Key Findings

*   **Superior Accuracy:** QWHA consistently outperforms existing baseline methods in accuracy for low-bit quantization of Large Language Models.
*   **Training Efficiency:** The method achieves faster training times than Fourier-related transform (FT)-based adapters by significantly reducing computational overhead.
*   **Error Mitigation:** QWHA successfully mitigates quantization errors before the fine-tuning process begins, maintaining high model accuracy throughout training.
*   **Resolved Limitations:** It effectively addresses the shortcomings of competing methods:
    *   **Low-Rank Adaptation (LoRA):** Overcomes limited representational capacity.
    *   **Standard FT-based Adapters:** Reduces computational overhead and improves ineffective error reduction.

---

## Methodology

The researchers propose QWHA, a method designed to integrate Fourier-related transform (FT) based adapters into quantized models efficiently. The core components of the methodology include:

*   **Walsh-Hadamard Transform (WHT):** Utilizing the WHT as the transform kernel instead of traditional Fourier kernels.
*   **Novel Initialization Scheme:** Implementing a new initialization strategy featuring:
    *   **Adaptive Parameter Selection:** Choosing parameters that specifically target quantization errors.
    *   **Value Refinement:** Adjusting values to reduce quantization error before fine-tuning starts.
*   **Quantization-Aware PEFT:** Combining parameter-efficient fine-tuning (PEFT) with quantization awareness to lower training overhead while simultaneously reducing inference costs.

---

## Technical Details

*   **Weight Update Formulation:**
    $$ \Delta W = F H^{-1} $$
    *   $F$: A trainable sparse matrix constructed via a scatter operation using a value vector $c$ and an index list $E$.
    *   $H^{-1}$: The inverse Walsh-Hadamard Transform (WHT) matrix.

*   **WHT Kernel Properties:**
    *   Uses $\pm 1$ entries for fast recursive computation.
    *   Applies a single transform to achieve full-rank capacity ($r_{max} = \min(d_{in}, d_{out})$) due to the orthogonal WHT kernel.

*   **Initialization Strategy (AdaAlloc):**
    *   Designed to reconstruct quantization errors and target outliers.
    *   Focuses on minimizing the difference between the original weights ($W^0$) and quantized weights ($W^Q$).

*   **Objective Function:**
    *   Quantization-Aware PEFT (QA-PEFT) aims to correct the quantization error defined as:
        $$ \Delta W_Q = W^0 - W^Q $$

---

## Contributions

1.  **New QA-PEFT Method:** Introduction of QWHA, a quantization-aware parameter-efficient fine-tuning method that resolves the trade-off between the high representational power of FT-based adapters and the computational constraints of quantized models.
2.  **Kernel Optimization:** Optimization of transform kernels by substituting standard Fourier kernels with the Walsh-Hadamard Transform (WHT) to lower computational costs and improve error reduction effectiveness.
3.  **Advanced Initialization:** Development of the "AdaAlloc" initialization strategy using adaptive parameter selection and value refinement to minimize quantization errors prior to training.
4.  **Empirical Validation:** Comprehensive evidence proving that QWHA improves low-bit quantization accuracy and training efficiency over state-of-the-art FT-based adapters and low-rank adaptation methods.

---

## Results

*   **Rank Capacity:** QWHA achieves nearly full-rank capacity (normalized rank metric) compared to LoRA's capacity of less than **6.3%**.
*   **Error Metrics:**
    *   Demonstrates a higher **Outlier Component Inclusion Ratio**.
    *   Achieves a lower $L_2$ norm of layer output error immediately after initialization.
*   **Benchmarking:** Consistently outperforms baselines (including LoRA variants and FT-based adapters) in accuracy for low-bit LLM quantization.
*   **Performance:** Offers faster training efficiency due to the $\pm 1$ kernel.
*   **Experimental Setup:** Validated on a **4-bit quantized LLaMA-3.2-3B** with rank $r=64$.