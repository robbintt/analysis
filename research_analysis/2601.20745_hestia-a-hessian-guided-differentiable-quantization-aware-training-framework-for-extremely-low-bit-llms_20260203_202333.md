---
title: 'HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework
  for Extremely Low-Bit LLMs'
arxiv_id: '2601.20745'
source_url: https://arxiv.org/abs/2601.20745
generated_at: '2026-02-03T20:23:33'
quality_score: 9
citation_count: 18
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs

*Guoan Wang; Feiyu Wang; Zongwei Lv; Yikun Zong; Tong Yang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Target Bit-width:** 1.58-bit (Ternary)
> *   **Model Family:** Llama-3.2 (1B & 3B)
> *   **Key Improvement:** +5.39% (1B) / +4.34% (3B)
> *   **References:** 18 Citations
> *   **Core Innovation:** Hessian-guided relaxation & sensitivity-aware discretization

---

## Executive Summary

The paper addresses the critical challenge of training Extremely Low-bit Large Language Models (LLMs), specifically targeting **1.58-bit (ternary) quantization**, which is essential for reducing the memory footprint and computational costs of modern inference. Current Quantization-Aware Training (QAT) methods struggle with optimization instability caused by a gradient mismatch between latent continuous weights and their quantized discrete counterparts. This issue, often compounded by the non-differentiable nature of hard rounding functions, leads to premature discretization. Consequently, models suffer from a significant loss of representational capacity and performance degradation, making ultra-low-bit quantization difficult to implement effectively.

The authors introduce **HESTIA**, a novel framework designed to overcome optimization barriers through a Hessian-guided differentiable relaxation strategy. Rather than relying on rigid, non-differentiable step functions, HESTIA utilizes a temperature-controlled softmax to maintain gradient flow, employing progressive hardening to transition weights from continuous approximations to discrete states gradually. The key technical advancement is the integration of curvature-aware optimization via tensor-wise Hessian traces, which estimate the curvature of the loss landscape. This "sensitivity-aware discretization" allows the framework to apply a fine-grained, dynamic annealing schedule that adapts to the specific sensitivity of different network layers, ensuring a more robust optimization path.

Experimental evaluations on the Llama-3.2 family demonstrate that HESTIA significantly outperforms existing ternary QAT baselines in zero-shot performance settings. Specifically, the framework achieved a **5.39% improvement on the Llama-3.2-1B model** and a **4.34% improvement on the Llama-3.2-3B model**. These results indicate that the Hessian-guided relaxation strategy successfully recovers the representational capacity typically lost during aggressive quantization, enabling the effective training of 1.58-bit LLMs without the severe accuracy penalties observed in previous methods.

This research establishes a new performance benchmark for ternary quantization in state-of-the-art LLMs, proving that 1.58-bit models can be trained effectively without sacrificing significant model utility. By successfully integrating second-order Hessian information into the quantization process, HESTIA provides a mathematically rigorous solution to the persistent problem of gradient mismatch in low-bit training. The work influences the field by offering a viable path toward deploying highly compressed, generative AI models on resource-constrained hardware, potentially unlocking new levels of efficiency for edge and mobile devices.

---

## Key Findings

*   **Significant Zero-Shot Gains:** Hestia outperforms existing ternary QAT baselines, achieving a **5.39% improvement** on Llama-3.2-1B and a **4.34% improvement** on Llama-3.2-3B.
*   **Optimization Resolution:** Successfully resolves the optimization hindrance caused by the gradient mismatch between latent and quantized weights.
*   **Capacity Recovery:** Recovers representational capacity through Hessian-guided relaxation, enabling the effective training of **1.58-bit** Large Language Models.
*   **Robust Optimization:** Demonstrates that progressive discretization yields a more robust optimization path compared to immediate hard rounding.

---

## Methodology

The HESTIA framework employs a sophisticated, multi-stage approach to quantization-aware training:

*   **Differentiable Relaxation Strategy**
    *   Uses temperature-controlled softmax instead of rigid step functions.
    *   Facilitates better gradient flow throughout the network.
*   **Progressive Hardening**
    *   Transitions the quantization process from soft, continuous approximations to discrete values over time.
*   **Hessian-Guided Annealing**
    *   Utilizes a tensor-wise Hessian trace metric to estimate the curvature of the loss landscape.
*   **Sensitivity-Aware Discretization**
    *   The Hessian trace drives a fine-grained temperature annealing schedule.
    *   Schedule adjusts dynamically based on the specific sensitivity of the network layers.

---

## Technical Details

The HESTIA framework is specifically engineered to address optimization challenges for training extremely low-bit Large Language Models.

**Core Objective**
Targeting **1.58-bit quantization** on the Llama-3.2 family.

**Problem Solving**
*   **Gradient Mismatch:** Resolves the disconnect between latent and quantized weights.
*   **Representational Capacity:** Employs curvature information to recover capacity lost during quantization.
*   **Discretization Strategy:** Utilizes a progressive strategy to move weights from continuous to discrete states smoothly.

---

## Contributions

1.  **Framework Introduction:** Introduced HESTIA, a novel Hessian-guided differentiable Quantization-Aware Training framework for extremely low-bit LLM compression.
2.  **Discretization Solution:** Solved the problem of premature discretization in standard QAT by replacing hard rounding with a dynamic relaxation strategy.
3.  **Curvature-Aware Optimization:** Integrated curvature-aware optimization using tensor-wise Hessian traces to govern the quantization schedule.
4.  **Benchmark Establishment:** Established a new performance benchmark for ternary (1.58-bit) quantization on Llama-3.2 models, demonstrating significant accuracy recovery.

---

## Performance Results

In Zero-Shot performance comparisons against existing ternary QAT baselines:

| Model | Improvement |
| :--- | :--- |
| **Llama-3.2-1B** | **+5.39%** |
| **Llama-3.2-3B** | **+4.34%** |

The framework successfully demonstrated the capability to effectively train 1.58-bit Large Language Models without significant performance degradation.