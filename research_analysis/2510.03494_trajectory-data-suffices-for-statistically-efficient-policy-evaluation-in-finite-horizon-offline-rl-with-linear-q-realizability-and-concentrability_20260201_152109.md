# Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^œÄ$-Realizability and Concentrability

*Volodymyr Tkachuk; Csaba Szepesv√°ri; Xiaoqi Tan*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Domain** | Offline Reinforcement Learning (RL) |
| **Focus** | Policy Evaluation & Optimization |
| **Key Innovation** | Leveraging Trajectory Data Structure |
| **Quality Score** | 6/10 |
| **Citations** | 5 |

---

## üìã Executive Summary

This research addresses a fundamental theoretical bottleneck in finite-horizon Offline Reinforcement Learning (RL): the **impossibility** of statistically efficient policy evaluation under standard assumptions of linear $q^\pi$-realizability and concentrability when data consists of single-step, independent transition samples. Prior theoretical work suggested that achieving statistically efficient value estimation was infeasible in this setting, creating a paradox where practical methods successfully utilized trajectory data despite theoretical claims to the contrary. Resolving this discrepancy is crucial for validating offline RL techniques in safety-critical domains where online interaction is impossible, establishing a rigorous foundation for algorithms that rely on sequential datasets.

The key innovation is a novel methodological approach that **leverages the sequential dependency** of trajectory data to perform variance reduction on the Bellman error, a mechanism previously unattainable with independent samples. Rather than treating transitions as i.i.d. or relying on pessimistic value penalties, the authors exploit the temporal structure within trajectories to stabilize the empirical Bellman operator. This allows for the construction of an efficient estimator that minimizes the projected Bellman error without requiring double sampling or model-based rollouts. By utilizing the inherent correlations in trajectory data, the algorithm effectively reduces the variance of the Bellman update, bypassing the limitations that led to previous impossibility results.

The authors provide rigorous statistical guarantees for both **Policy Evaluation** and **Policy Optimization**. For Policy Evaluation, they present the first provably efficient learner capable of achieving $\epsilon$-accurate value estimates with high confidence. For Policy Optimization, the analysis demonstrates substantially improved sample efficiency compared to prior information-theoretic limits. Both results exhibit a polynomial dependence on problem parameters‚Äîspecifically scaling with the horizon $H$ and feature dimension $d$‚Äîwhile successfully eliminating the exponential dependence on the concentrability coefficient or dimensionality that plagued earlier theoretical frameworks.

This work fundamentally bridges the gap between theoretical impossibility theorems and practical application in offline RL. By proving that trajectory data is a sufficient condition for statistical efficiency, the study validates the use of sequential datasets and corrects the misconception that efficient learning is impossible under standard linear realizability assumptions.

---

## üîë Key Findings

*   **First Efficient Learner:** The authors present the first statistically efficient learner for policy evaluation in finite-horizon offline RL, overcoming established impossibility results.
*   **Trajectory vs. Independent Samples:** The findings demonstrate that efficient learning is possible under standard assumptions (linear $q^\pi$-realizability and concentrability) **specifically if data is structured as trajectories** rather than independent samples.
*   **Improved Complexity:** The study reveals significantly improved sample complexity for a policy optimization learner compared to previous limits.

---

## üß™ Methodology

The work is situated within **finite-horizon offline reinforcement learning** using function approximation. The methodology relies on critical structural assumptions:

*   **Linear $q^\pi$-Realizability:** State-action value functions are linearly realizable.
*   **Concentrability:** The dataset possesses good coverage.
*   **Bellman Completeness:** Ensuring the Bellman operator preserves linear structure.

Unlike prior approaches, this method leverages the **sequential nature of trajectory data** to extract policy value estimates. The core methodology utilizes **Fitted Q-Iteration (FQI/FQE)** by minimizing the mean squared projected Bellman error.

---

## ‚úÖ Contributions

The paper closes the theoretical gap in offline RL through several key contributions:

*   **Solving Policy Evaluation:** Complements recent solutions to policy optimization by effectively solving the policy evaluation problem.
*   **Delineating Conditions:** Explicitly establishes trajectory data as a sufficient condition for statistical efficiency where single-step data fails.
*   **Refining Bounds:** Provides a rigorous refinement of sample complexity bounds, advancing the theoretical understanding of learning efficiency limits.

---

## ‚öôÔ∏è Technical Details

### Core Algorithms
The approach proposes two specialized algorithms based on Fitted Q-Iteration:

1.  **For Policy Evaluation (Algorithm 2):**
    *   Constructs a set of candidate Q-functions ($\mathcal{Q}_{eval}$) consistent with the empirical Bellman operator across the horizon.
    *   Selects the candidate that minimizes an empirical objective.
    *   Does not require explicit state representations.

2.  **For Policy Optimization (Algorithm 1, LIN-$q^\pi$-FQI):**
    *   Constructs a set ($\mathcal{Q}_{opt}$) and selects the function with the highest initial value.
    *   Returns the derived greedy policy.

### Variance Reduction
The algorithm exploits the sequential dependency in trajectory data to perform variance reduction on the Bellman error, stabilizing the empirical Bellman operator without requiring double sampling.

---

## üìà Results

The paper provides statistical bounds rather than empirical benchmarks.

### Policy Evaluation (Theorem 1)
*   **Guarantee:** The estimated value satisfies $|v^{\pi_e}(s_1) - \hat{v}| \leq \epsilon$ with probability $1-\delta$.
*   **Sample Complexity:** $\tilde{\Theta}\left( \frac{C_0^2 H^7 d^3}{\epsilon^2 + L_\phi^2} \right)$
*   **Scaling:** Error decomposition shows a $d^{3/2}$ scaling in intermediate bounds and a $d^3$ dependence in the final sample complexity.

### Policy Optimization (Theorem 2)
*   **Guarantee:** The output policy satisfies $|v^{\pi^*}(s_1) - v^{\hat{\pi}}(s_1)| \leq \epsilon$ with probability $1-\delta$.
*   **Sample Complexity:** $\tilde{\Theta}\left( \frac{C_0^3 H^7 d^3}{\epsilon^2} \right)$ trajectories.
*   **Improvement:** This represents a significant improvement over previous bounds by a factor of $C_0^d$.

---

**Document Details**
*   **Quality Score:** 6/10
*   **References:** 5 citations