---
title: In-the-Flow Agentic System Optimization for Effective Planning and Tool Use
arxiv_id: '2510.05592'
source_url: https://arxiv.org/abs/2510.05592
generated_at: '2026-02-03T12:53:12'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# In-the-Flow Agentic System Optimization for Effective Planning and Tool Use

*Zhuofeng Li; Haoxiang Zhang; Seungju Han; Sheng Liu; Jianwen Xie; Yu Zhang; Yejin Choi; James Zou; Pan Lu*

---

> ### ðŸ“Š Quick Facts
>
> *   **Model Scale:** 7B (Qwen2.5 Backbone)
> *   **Core Innovation:** Flow-GRPO (In-the-flow optimization)
> *   **Search Accuracy:** 57.3% (**+8.2%** vs. GPT-4o)
> *   **GAIA Benchmark:** 33.1% (**+15.8%** vs. GPT-4o)
> *   **Math Performance:** **+14.5%** average improvement
> *   **Optimization Lift:** 10-16% gain over static, training-free logic

---

## Executive Summary

Current large language model (LLM) agents struggle with complex, multi-step reasoning and reliable tool use, particularly in long-horizon scenarios where feedback is sparse. Existing approaches typically rely on static prompt engineering or offline fine-tuning, which fails to address the "credit assignment problem"â€”determining which specific action in a multi-turn interaction sequence contributed to the final success or failure. Consequently, these monolithic or loosely coupled systems often exhibit suboptimal planning and poor execution reliability when deployed in dynamic, live environments.

The paper introduces **AgentFlow**, a modular, "in-the-flow" agentic framework designed to optimize specialized components directly within the multi-turn interaction loop. Unlike traditional methods that optimize entire trajectories, AgentFlow utilizes **Flow-based Group Refined Policy Optimization (Flow-GRPO)**, a novel reinforcement learning technique that decomposes long-horizon optimization into tractable single-turn policy updates. Technically, Flow-GRPO addresses sparse-reward challenges by broadcasting a single trajectory-level outcome to every intermediate turn, thereby aligning local decisions with global goals, while employing group-normalized advantages to stabilize learning. The architecture coordinates four distinct modulesâ€”Action Planner, Tool Executor, Execution Verifier, and Solution Generatorâ€”via a shared memory mechanism.

Utilizing a relatively small 7B-scale backbone (Qwen2.5), AgentFlow demonstrated superior performance compared to significantly larger proprietary models and established baselines, achieving a **14.0%** average improvement on agentic tasks. On search-intensive benchmarks specifically, AgentFlow outperformed AutoGen by 14.9% and GPT-4o by 8.2%, achieving an average accuracy of 57.3%. Notably, on the GAIA agentic benchmark, it scored 33.1%, surpassing GPT-4o by a substantial margin of 15.8%. The system also secured average improvements of 14.5% on mathematical tasks and 4.1% on scientific tasks. Ablation studies confirmed that the Flow-GRPO optimization provided a critical performance lift of approximately 10â€“16% over static, training-free logic, with results showing positive scaling trends relative to both model size and reasoning turns.

This research challenges the prevailing notion that agentic capabilities require massive model sizes, demonstrating instead that specialized, on-policy optimization is a more scalable path toward effective tool use and planning. By successfully solving the credit assignment problem in long-horizon agentic tasks, AgentFlow establishes a new paradigm for training tool-augmented LLMs. The findings suggest that decomposing work across specialized modules using in-the-flow reinforcement learning offers better generalization and reliability than current monolithic or interleaved policy approaches, providing a robust framework for future development of autonomous AI systems.

---

## Key Findings

*   **Superior Performance with Smaller Models:** AgentFlow, utilizing a 7B-scale backbone, achieved significant accuracy improvements across ten benchmarks, surpassing top-performing baselines and larger proprietary models like **GPT-4o**.
*   **Significant Accuracy Gains:** The framework demonstrated substantial average accuracy increases:
    *   **+14.9%** on search tasks
    *   **+14.0%** on agentic tasks
    *   **+14.5%** on mathematical tasks
    *   **+4.1%** on scientific tasks
*   **Optimization Efficacy:** Analyses confirmed that 'in-the-flow' optimization leads to improved planning capabilities and enhanced tool-calling reliability compared to training-free or offline approaches.
*   **Positive Scaling Trends:** The study established that performance improvements correlate with increases in both model size and the number of reasoning turns.

---

## Methodology

The researchers introduced **AgentFlow**, a trainable, modular agentic framework that coordinates four distinct specialized modules through an evolving memory mechanism:

1.  **Planner**
2.  **Executor**
3.  **Verifier**
4.  **Generator**

To train this system within live, multi-turn environments, they proposed **Flow-based Group Refined Policy Optimization (Flow-GRPO)**. This algorithm addresses long-horizon, sparse-reward challenges by:

*   **Decomposing** multi-turn optimization into a sequence of tractable single-turn policy updates.
*   **Broadcasting** a single trajectory-level outcome to every turn to align decisions.
*   **Utilizing** group-normalized advantages to stabilize the learning process.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **System Type** | In-the-flow agentic system optimized via reinforcement learning (Multi-turn MDP). |
| **Architecture** | Four specialized modules coordinated by shared memory: <br> â€¢ **Action Planner** (Trainable)<br> â€¢ **Tool Executor**<br> â€¢ **Execution Verifier**<br> â€¢ **Solution Generator** |
| **Training Algorithm** | **Flow-GRPO** (Flow-based Group Refined Policy Optimization). Uses global final-outcome-based reward signals to train the planner. |
| **Backbone Model** | Qwen2.5-7B-Instruct |
| **Tools Integrated** | Python Coder, Google Search |

---

## Performance Results

AgentFlow (7B) demonstrated robust performance across various benchmarks, often outperforming much larger proprietary models.

### Benchmark Comparisons

| Benchmark / Task Type | AgentFlow Accuracy | Improvement vs. GPT-4o | Improvement vs. AutoGen |
| :--- | :--- | :--- | :--- |
| **Search Tasks (Avg)** | **57.3%** | **+8.2%** | **+14.9%** |
| **GAIA (Agentic)** | **33.1%** | **+15.8%** | **+26.8%** |
| **Mathematical Tasks** | - | **+14.5%** | - |
| **Scientific Tasks** | - | **+4.1%** | - |

### Ablation Studies
*   Confirmed that **Flow-GRPO optimization** provided substantial gains (approx. **10â€“16% lift**) over static, training-free logic.

---

## Core Contributions

1.  **AgentFlow Framework:** The development of an in-the-flow agentic framework that enables the direct optimization of specialized modules inside the multi-turn interaction loop.
2.  **Flow-GRPO Algorithm:** The introduction of a reinforcement learning technique designed to solve credit assignment problems in long-horizon scenarios by converting complex trajectory optimization into aligned single-turn updates.
3.  **Scalability Validation:** The demonstration that decomposing work across specialized modules with on-policy, in-the-flow optimization is more scalable and generalizable than prevailing interleaved, monolithic policies in tool-augmented LLMs.

---

*   **Quality Score:** 9/10
*   **References:** 40 citations