---
title: 'Table 1: Detection performance (AUC) of PHD and Short-PHD for identifying
  short LLM-'
arxiv_id: '2504.02873'
source_url: https://arxiv.org/abs/2504.02873
generated_at: '2026-01-27T22:49:32'
quality_score: 9
citation_count: 32
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Table 1: Detection performance (AUC) of PHD and Short-PHD for identifying short LLM-

*Xiao Fang, Dongjun Wei, Minjia Mao, Michael Chau*

---

> ### ðŸ“‹ Quick Facts
> *   **Quality Score**: 9/10
> *   **Total References**: 32 Citations
> *   **Detection Mechanism**: Zero-shot Topological Data Analysis (TDA)
> *   **Key Innovation**: Off-topic Content Insertion (OCI)
> *   **Best Performance (AUC)**: 0.830 (GPT-3.5 / GPT-4o)
> *   **Improvement**: Up to 27.11% over standard PHD baseline

---

## Executive Summary

This research addresses the critical challenge of detecting Short Large Language Model (LLM)-generated text, a task where existing zero-shot detection methods frequently fail due to insufficient data volume. As LLM-generated content proliferates in short formats such as social media posts and comments, robust detection is vital for maintaining information integrity. Standard topological methods rely on statistical correlations that become unstable with small sample sizes, leading to the "Short Text" problem where local density peaks in high-dimensional spaces prevent accurate feature estimation.

The core innovation is the introduction of **Short-PHD**, a zero-shot detection mechanism leveraging Topological Data Analysis (TDA) through Off-topic Content Insertion (OCI). To address the instability of standard Persistent Homology Dimension (PHD) calculations on short inputs, the method preprocesses text by inserting semantically unrelated content, creating stabilized high-dimensional point clouds from embeddings (e.g., RoBERTa). Short-PHD then extracts features by calculating the PHD via Minimal Spanning Tree (MST) statistics, based on the principle that human-written text exhibits higher connectedness and lower PHD values. The final classification is determined by comparing the calculated PHD score against a pre-established threshold to distinguish between human and AI-generated sources.

Experimental validation on both publicly available and generated datasets confirms that Short-PHD significantly outperforms the standard PHD baseline across multiple LLM architectures. In zero-shot detection experiments targeting texts limited to 50 tokens, the method achieved an Area Under the Curve (AUC) of **0.793** against the OPT, OPT-iml, and GPT-NeoX group, marking a **25.47%** improvement over the baselineâ€™s 0.632. Furthermore, against the GPT-3.5 and GPT-4o group, Short-PHD achieved an AUC of **0.830**, a **27.11%** increase in performance. These results highlight the method's consistent robustness across both open-source and proprietary model families.

This work establishes a new standard for short-text AI detection, effectively bridging the methodological gap where standard PHD and other zero-shot techniques struggle. By demonstrating that topological features can be reliably harnessed for short texts through strategic content manipulation, the authors provide a generalized solution to a pervasive vulnerability in current detection systems. The public release of implementation codes ensures reproducibility and provides a foundation for future research to enhance watermarking and detection frameworks for low-resource text scenarios.

---

## Key Findings

*   **Superior Performance**: Short-PHD outperforms existing zero-shot methods specifically for short LLM-generated texts.
*   **Stabilization Technique**: The estimation of Persistent Homology Dimension (PHD) for short texts is effectively stabilized by the insertion of off-topic content.
*   **Wide Validation**: The method is validated on both publicly available and generated datasets, effectively bridging the gap where standard PHD methods struggle.
*   **Consistent Robustness**: Demonstrated effectiveness across diverse model families, including both open-source (OPT, GPT-NeoX) and proprietary (GPT-3.5, GPT-4o) models.

---

## Methodology

The Short-PHD framework operates as a zero-shot detection mechanism relying on topological data analysis. The process involves three distinct stages:

1.  **Preprocessing**: The input short text is processed by inserting **off-topic content** before it. This prepares the text for topological analysis.
2.  **Feature Extraction**: The method calculates the **Persistent Homology Dimension (PHD)** of the resulting text embeddings.
3.  **Classification**: Classification is performed by comparing the calculated PHD score against a pre-established threshold to determine if the text is human or AI-generated.

---

## Technical Details

The method utilizes Persistent Homology Dimension (PHD) via Topological Data Analysis (TDA) to distinguish between human-written and LLM-generated text.

*   **Core Principle**: LLM texts generally have higher connectedness in embedding space and consequently lower PHD values compared to human-written text.
*   **Data Representation**: Text is represented as high-dimensional point clouds (e.g., using RoBERTa with dimension $d=768$).
*   **Statistical Estimation**: 0-persistent homology is estimated using the Minimal Spanning Tree (MST) statistic $E_0^\alpha(W)$. The dimension $d$ is derived via linear regression on the log-transformed statistic against sample size.
*   **Addressing the 'Short Text' Problem**:
    *   Small sample sizes in short texts cause local density peaks and unstable regression.
    *   **Solution**: The proposed Short-PHD method employs **Off-topic Content Insertion (OCI)**.
    *   **Mechanism**: OCI.inserts semantically unrelated text before the input to stabilize the point cloud and increase embedding separation.

---

## Results

In zero-shot detection experiments on short texts (50 tokens), the proposed Short-PHD method significantly outperformed the standard PHD baseline in terms of Area Under the Curve (AUC).

| Model Group | Short-PHD AUC | Baseline PHD AUC | Improvement |
| :--- | :---: | :---: | :---: |
| **OPT, OPT-iml, GPT-NeoX** | **0.793** | 0.632 | +25.47% |
| **GPT-3.5, GPT-4o** | **0.830** | 0.653 | +27.11% |

---

## Contributions

*   **Novel Method**: Introduction of the **Short-PHD** method, a zero-shot detection method tailored specifically for short texts.
*   **Methodological Innovation**: Innovation in using **off-topic content insertion** to stabilize topological features in low-resource scenarios.
*   **Experimental Evidence**: Provision of comprehensive experimental evidence establishing a new standard over previous baselines.
*   **Reproducibility**: Release of implementation codes to support reproducibility and further research in the community.

---

## Document Info

*   **References**: 32 citations
*   **Evaluation**: 9/10