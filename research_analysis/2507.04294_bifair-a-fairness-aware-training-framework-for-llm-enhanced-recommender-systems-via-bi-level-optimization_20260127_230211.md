---
title: 'BiFair: A Fairness-aware Training Framework for LLM-enhanced Recommender Systems
  via Bi-level Optimization'
arxiv_id: '2507.04294'
source_url: https://arxiv.org/abs/2507.04294
generated_at: '2026-01-27T23:02:11'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# BiFair: A Fairness-aware Training Framework for LLM-enhanced Recommender Systems via Bi-level Optimization

*Li Zhang, Yiqun Xu, Jiaming Zhang, Yuyuan Li*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Evaluation Dataset** | Amazon Review (Books) |
| **Best Accuracy Achieved** | Recall: 0.1658 (+51.2% vs traditional) |
| **Best Fairness Achieved** | MIN: 0.0497 (+168.6% vs traditional) |
| **Optimization Strategy** | Bi-level Optimization |
| **Core Innovation** | Decomposition of "Prior" vs. "Training" unfairness |
| **Quality Score** | 9/10 |
| **Citations** | 40 References |

---

## üìù Executive Summary

This research addresses the critical challenge of fairness in Large Language Model (LLM)-enhanced Recommender Systems (RS). While integrating LLMs significantly improves recommendation accuracy, empirical studies reveal a persistent and substantial fairness gap where specific item groups remain under-served. This problem is particularly complex because LLM-enhanced architectures introduce dual sources of unfairness not found in traditional systems: **"prior unfairness"** stemming from semantic biases inherent in frozen, pre-trained LLM representations, and **"training unfairness"** arising from the optimization dynamics of the recommendation model. Addressing this is vital to ensure that the deployment of high-performance LLM-based recommenders does not inadvertently amplify systemic bias or create inequitable exposure across different item genres or demographic groups.

The core innovation is the **BiFair framework**, a novel fairness-aware training methodology leveraging bi-level optimization. BiFair theoretically decomposes bias into the two identified components and addresses them simultaneously through a nested optimization process. At the inner level, the framework optimizes the projector parameters ($\theta$) within the recommendation model using a fairness-aware loss to ensure balanced learning. At the outer level, it optimizes the semantic representation space ($Z$)‚Äîeffectively tuning the LLM-generated inputs‚Äîto mitigate biases inherited from the foundation model. To ensure practical efficiency, BiFair employs a one-step gradient descent approximation for the outer-level optimization.

Evaluations on the Amazon Review (Books) dataset demonstrate that BiFair significantly outperforms state-of-the-art baselines, though challenges remain. In comparison to the best traditional model (XSimGCL), the LLM-enhanced approach (AlphaRec with BiFair) achieved a Recall of **0.1658**, marking a **+51.2%** improvement in accuracy. Regarding fairness, Genre Fairness (MIN) improved by **+168.6%**. Despite these strong gains, the results highlight a persistent gap: the worst-performing group's utility (MIN: 0.0497) remains less than one-third of the average performance. This paper makes a significant theoretical contribution by reframing bias in hybrid neural architectures, establishing a new benchmark for balancing high utility with fair representation.

---

## üîç Key Findings

*   **Persistent Fairness Gap:** Empirical analysis reveals that while LLM-enhanced Recommender Systems improve fairness, a significant gap remains where the worst-performing groups are still heavily underserved compared to the average.
*   **Dual Sources of Unfairness:** The research identifies that unfairness stems from two distinct roots: **'prior unfairness'** (biases already present in LLM representations) and **'training unfairness'** (biases learned during the recommendation model's training phase).
*   **Superior Mitigation:** The proposed BiFair framework significantly outperforms state-of-the-art methods on three real-world datasets, effectively balancing accuracy and equity.
*   **Architectural Challenges:** Enhancing fairness is inherently difficult due to architectural differences in LLM-enhanced systems and the varying sources of unfairness that do not exist in traditional models.

---

## ‚öôÔ∏è Methodology

The paper proposes **BiFair**, a fairness-aware training framework utilizing **bi-level optimization** to tackle the complex nature of bias in hybrid systems.

*   **Decomposition:** The methodology begins by decomposing unfairness into prior (LLM-inherited) and training (model-learned) components.
*   **Nested Optimization:** It implements a nested optimization process that simultaneously tunes:
    1.  LLM-generated item representations.
    2.  A trainable projector within the recommendation model.
*   **Balancing Mechanism:** The framework incorporates an **adaptive inter-group balancing mechanism** grounded in multi-objective optimization. This mechanism dynamically balances fairness across different item groups during the training process.

---

## üß± Technical Details

The BiFair architecture is designed to address the two identified sources of unfairness through a structured optimization hierarchy:

### 1. Sources of Unfairness
*   **Prior Unfairness:** Bias present in the frozen LLM semantic representations.
*   **Training Unfairness:** Bias that emerges during the training of the projector component.

### 2. Bi-level Optimization Framework
The core of the solution is a two-loop optimization process:
*   **Inner-Level:** Optimizes projector parameters ($\theta$) using a fairness-aware loss. This ensures that the learning process remains balanced across groups.
*   **Outer-Level:** Optimizes the semantic representation space ($Z$) to mitigate biases that were inherited from the frozen LLM.

### 3. Computational Efficiency
To manage the high computational costs typically associated with bi-level optimization, the authors employ a **one-step gradient descent approximation** specifically for the outer-level optimization.

### 4. Adaptive Fairness-aware Recommendation Loss
The framework utilizes a specific loss function designed to:
*   Maximize entropy.
*   Create a uniform distribution of utility across different groups, ensuring no single group is disproportionately favored.

---

## üìà Results

Experiments were conducted on the **Amazon Review (Books)** dataset using standard accuracy and fairness metrics.

### Performance Metrics
*   **Accuracy Metrics:** Recall@K, HR@K, and NDCG@K.
*   **Fairness Metrics:** Coefficient of Variation (CV), MIN, and Œµ-Item-side Fairness.

### Key Outcomes
*   **Accuracy Boost:** LLM-enhanced RSs (AlphaRec with SFR) achieved a Recall of **0.1658**, representing a **+51.2%** improvement over the best traditional model (XSimGCL: 0.1096).
*   **Fairness Improvement:** Genre Fairness (MIN) improved by **+168.6%** (increasing from 0.0140 to 0.0497).
*   **The Persistent Gap:** Despite the improvement, the worst-performing group's utility (MIN: 0.0497) is still less than one-third of the average performance (Recall: 0.1658).
*   **Architectural Trade-offs:** Increasing model complexity introduced new risks. Complex models like AlphaRec increased Popularity Unfairness (CV: 0.4364) compared to simpler Linear models (CV: 0.3373).

---

## ‚úâÔ∏è Contributions

*   **Empirical Insight:** Provided a critical empirical study revealing the nuances of fairness gaps in LLM-enhanced RS, clearly distinguishing them from traditional systems.
*   **Theoretical Framework:** Introduced a novel decomposition of bias into **'prior unfairness'** and **'training unfairness'**, offering a new lens for analyzing hybrid models.
*   **Novel Algorithm:** Developed **BiFair**, a bi-level optimization-based framework designed to optimize both LLM representations and RS parameters specifically for fairness.
*   **Balancing Mechanism:** Contributed an adaptive inter-group balancing technique to handle dynamic trade-offs, ensuring equitable performance across diverse item groups.