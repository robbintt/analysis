---
title: 'NeUQI: Near-Optimal Uniform Quantization Parameter Initialization for Low-Bit
  LLMs'
arxiv_id: '2505.17595'
source_url: https://arxiv.org/abs/2505.17595
generated_at: '2026-02-03T18:49:02'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# NeUQI: Near-Optimal Uniform Quantization Parameter Initialization for Low-Bit LLMs
*Li Lin; Xinyu Hu; Xiaojun Wan*

> ### üìä Quick Facts
> - **Quality Score**: 8/10
> - **References**: 40 Citations
> - **Tested Architectures**: LLaMA Family, Qwen Family
> - **Precision Levels**: 3-bit, 4-bit
> - **Quantization Types**: Standard, Group-wise (Group Size 128)
> - **Key Benchmark**: Significantly outperforms GPTQ on LLaMA 13B & 70B
> - **Optimization Strategy**: Scale-only optimization via analytical derivation

---

## üìù Executive Summary

Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on resource-constrained hardware, yet current low-bit uniform PTQ methods face a significant bottleneck. The industry standard relies on the Min-Max formula to initialize quantization parameters (scale and zero-point), a heuristic approach that introduces inherent limitations. This restriction prevents models from reaching their full performance potential in low-bit environments‚Äîspanning both standard and group-wise quantization‚Äîcreating a critical need for more robust initialization strategies that do not compromise model accuracy for the sake of compression.

The authors introduce **NeUQI** (Near-Optimal Uniform Quantization Parameter Initialization), a novel framework designed to optimize the initialization process by moving beyond the Min-Max formula. The key technical breakthrough lies in analytically deriving the zero-point for any given scale value. This insight simplifies the joint optimization of scale and zero-point‚Äîa traditionally complex multi-variable search‚Äîinto a single-variable, scale-only optimization problem without sacrificing performance. NeUQI serves as a distinct initialization strategy that, when integrated into a broader PTQ pipeline utilizing a lightweight distillation strategy, achieves superior results with significantly fewer resources than existing methods.

Comprehensive testing on LLaMA and Qwen architectures validates NeUQI‚Äôs effectiveness across 3-bit and 4-bit precisions and various group sizes. In a 3-bit, Group Size 128 setting for LLaMA 7B, NeUQI achieved better perplexity than GPTQ while maintaining competitive accuracy. For LLaMA 13B (3-bit), NeUQI significantly outperformed GPTQ, achieving a Wiki2 perplexity of 6.56 compared to GPTQ's 8.45 and an average accuracy of 60.75% versus 57.60%. On LLaMA 70B (Standard), NeUQI improved accuracy by +2.71 points (71.48% vs. 68.77%) and lowered Wiki2 perplexity (3.90 vs. 4.83). Furthermore, the method demonstrated the ability to surpass PV-tuning, a resource-intensive technique, while maintaining competitive performance in group-wise quantization settings (Group Size 128).

This research addresses a fundamental, underexplored gap in low-bit uniform quantization by establishing a new theoretical and practical framework for parameter initialization. By providing a deployment-friendly solution that reduces memory footprint and decoding latency while maintaining high accuracy, NeUQI offers a viable path for the efficient deployment of massive LLMs.

---

## üîë Key Findings

*   **Limitations of Current Standards**: The conventional Min-Max formula for initializing quantization parameters in low-bit uniform PTQ suffers from inherent limitations that restrict model performance.
*   **Superiority of NeUQI**: The proposed NeUQI method consistently outperforms existing PTQ methods across the **LLaMA** and **Qwen** families in various experimental settings and tasks.
*   **Efficiency vs. Resource Intensity**: When combined with a lightweight distillation strategy, NeUQI achieves superior performance to **PV-tuning**, a significantly more resource-intensive method.
*   **Optimization Simplification**: The joint optimization of quantization parameters (scale and zero-point) can be effectively simplified to a scale-only optimization without sacrificing performance quality.

---

## üõ†Ô∏è Methodology

The authors propose **NeUQI** (Near-Optimal Uniform Quantization Parameter Initialization), a method designed to optimize the initialization of uniform quantization parameters for low-bit LLMs.

Instead of relying on the standard Min-Max formula, NeUQI simplifies the joint optimization of the scale and zero-point. The core technical mechanism involves:
*   Deriving the zero-point **analytically** for any given scale value.
*   Reducing the optimization problem from a multi-variable search to a **single-variable (scale-only) optimization**.

---

## ‚öôÔ∏è Technical Details

*   **Bottleneck Identification**: The paper identifies the Min-Max formula as a primary bottleneck in Post-Training Quantization (PTQ).
*   **Core Innovation**: Proposes NeUQI, a near-optimal initialization strategy.
    *   Transforms joint scale and zero-point optimization into scale-only optimization.
    *   Achieves this without performance loss by analytically deriving the zero-point.
*   **Integration**: NeUQI integrates with lightweight distillation to outperform PV-tuning.
*   **Validation Scope**:
    *   **Models**: LLaMA and Qwen families (including LLaMA 3 8B).
    *   **Precision**: 3-bit and 4-bit.
    *   **Quantization Modes**: Standard and group-wise quantization (Group Size 128).

---

## üìà Results

The method was evaluated using **Perplexity (PPL)** on Wiki2/C4 and **Zero-shot accuracy** benchmarks.

### LLaMA 13B
*   **Vs. GPTQ**: NeUQI significantly recovered performance.
    *   **Wiki2 PPL**: 6.56 (NeUQI) vs 8.45 (GPTQ).
    *   **Avg Acc**: 60.75 (NeUQI) vs 57.60 (GPTQ).

### LLaMA 70B (Standard)
*   **Accuracy**: Improved by **+2.71 points** (71.48 vs 68.77).
*   **Wiki2 PPL**: Reduced to 3.90 vs 4.83.

### LLaMA 70B (Group 128)
*   **Wiki2 PPL**: 3.71 vs 3.87 (Slight improvement).
*   **Avg Acc**: 71.48 vs 71.40 (Slight improvement).

### LLaMA 7B (3-bit Group 128)
*   **Trade-off**: NeUQI achieved better perplexity but slightly lower accuracy (64.27 vs 64.53) than GPTQ.

---

## üèÜ Contributions

*   **Identification of Underexplored Area**: The work identifies and addresses a critical gap in low-bit uniform quantization research regarding parameter initialization strategies.
*   **Novel Initialization Algorithm**: The introduction of NeUQI provides a new theoretical and practical framework for parameter initialization that moves beyond the constraints of the Min-Max formula.
*   **Advancement in Low-Bit Deployment**: The research offers a deployment-friendly solution that reduces memory footprint and decoding latency while maintaining accuracy.
*   **Validation on Major Architectures**: Comprehensive empirical validation on prominent LLM families (LLaMA and Qwen) establishes the robustness and generalizability of the proposed method.