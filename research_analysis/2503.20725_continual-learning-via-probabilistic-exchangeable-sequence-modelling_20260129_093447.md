# Continual learning via probabilistic exchangeable sequence modelling

*Hanwen Xing; Christopher Yau***

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Proposed Model** | CL-BRUNO |
> | **Methodology** | Neural Processes & Exchangeable Sequence Modelling |
> | **Memory Type** | Rehearsal-free (No data buffer) |
> | **Best Accuracy (Split CIFAR-100)** | **65.7%** (vs GPM: 52.8%) |
> | **Key Scenarios** | Task-IL & Class-IL |
> | **Quality Score** | 6/10 |
> | **References** | 23 Citations |

---

## Executive Summary

Continual Learning (CL) is fundamentally hindered by **catastrophic forgetting**, a phenomenon where neural networks degrade performance on previous tasks upon learning new ones. Current state-of-the-art solutions typically rely on rehearsal strategies, necessitating the storage and reprocessing of raw data from past tasks. This dependency creates prohibitive barriers in privacy-sensitive sectors like healthcare (due to regulations like HIPAA/GDPR) and imposes heavy computational burdens on resource-constrained hardware. Furthermore, existing CL methodologies frequently lack robust **uncertainty quantification**, rendering them unreliable for high-stakes decision-making environments where assessing prediction confidence is as critical as the prediction itself.

This paper introduces **CL-BRUNO**, a novel framework for Continual Learning via Probabilistic Exchangeable Sequence Modelling. Grounded in Neural Processes (NPs) and exchangeable sequence modeling, the approach treats tasks as draws from a latent stochastic process. This architecture allows the model to share statistical strength across tasks and capture uncertainty inherently. The technical core employs a **dual regularization strategy**: distributional regularization aligns the model's predictive distribution with the underlying data-generating process, while functional regularization preserves the mapping learned from previous tasks via tractable Bayesian updates. This mechanism enables the posterior over latent functions to be updated sequentially without a data buffer, providing a unified architecture capable of handling both Task-Incremental Learning (Task-IL) and Class-Incremental Learning (Class-IL).

The proposed framework was benchmarked against leading rehearsal-free baselines, including Gradient Projection Memory (GPM) and Elastic Weight Consolidation (EWC), on standard natural image datasets (Split MNIST, Split CIFAR-100) and biomedical datasets (PathMNIST). In rigorous Class-IL scenarios on Split CIFAR-100, the model achieved an average accuracy of **65.7%**, significantly outperforming the GPM baseline at **52.8%** and EWC at **18.1%**. This represents a substantial reduction in forgetting margins while maintaining competitive performance on current tasks.

The significance of this work lies in delivering a practically viable solution for deployment in constrained and sensitive real-world environments. By eliminating the rehearsal buffer, the framework provides a compliant solution for medical applications where long-term storage of raw training samples is restricted. Moreover, the integration of tractable Bayesian uncertainty quantification allows these models to be reliably deployed in clinical decision support systems.

---

## Key Findings

*   **Superior Performance:** CL-BRUNO outperforms existing state-of-the-art methods on both natural image and biomedical datasets, validating its effectiveness for real-world applications.
*   **Rehearsal-Free Learning:** The model successfully prevents catastrophic forgetting without retaining any previously seen samples (rehearsal-free), relying instead on distributional and functional regularization.
*   **Scenario Versatility:** The approach demonstrates versatility by creating a single model capable of handling various Continual Learning (CL) scenarios, including both task-incremental and class-incremental learning.
*   **Computational Efficiency:** CL-BRUNO provides scalable and tractable Bayesian updates and predictions, addressing common computational bottlenecks associated with deploying CL in decision-making systems.

---

## Methodology

The researchers propose **CL-BRUNO**, a probabilistic model based on **Neural Processes**. The methodology utilizes deep-generative models to establish a unified probabilistic framework that integrates information across different Continual Learning scenarios.

*   **Bayesian Framework:** The approach employs Bayesian updates for prediction and learning to handle uncertainty natively.
*   **Regularization Strategy:** To mitigate catastrophic forgetting without storing past data, the method applies a combination of:
    *   **Distributional Regularization:** Aligning predictions with the underlying data distribution.
    *   **Functional Regularization:** Preserving the functional mapping learned from prior tasks.
*   **Implementation:** This allows for a scalable, tractable implementation that addresses uncertainty quantificationâ€”a gap in many existing CL methods.

---

## Technical Specifications

| Feature | Description |
| :--- | :--- |
| **Core Architecture** | Probabilistic Exchangeable Sequence Modelling (Neural Processes) |
| **Regularization Type** | Dual Strategy (Distributional + Functional) |
| **Memory Requirement** | **Rehearsal-free** (Does not retain a buffer of previously seen samples) |
| **Inference Mechanism** | Scalable and tractable Bayesian updates and predictions |
| **Supported Scenarios** | Heterogeneous CL scenarios (Task-IL and Class-IL) |

---

## Core Contributions

*   **Resolution of Deployment Barriers**
    The paper addresses the critical limitations of current CL methodsâ€”specifically high computational cost and the lack of uncertainty quantificationâ€”enabling their use in real-world decision-making.

*   **Privacy and Storage Optimization**
    By eliminating the need to retain previously seen samples (data rehearsal), the contribution offers a solution for applications where data privacy and limited storage capacity are primary constraints.

*   **Integration of Heterogeneous Scenarios**
    The introduction of a single, unified model that seamlessly handles different types of CL problems (task-incremental and class-incremental) simplifies the CL workflow, removing the need for scenario-specific models.

---

## Results & Validation

CL-BRUNO was rigorously tested against state-of-the-art rehearsal-free baselines, yielding the following outcomes:

*   **Benchmark Superiority:** CL-BRUNO achieved an average accuracy of **65.7%** on Split CIFAR-100 (Class-IL), significantly outperforming GPM (52.8%) and EWC (18.1%).
*   **Dataset Validation:** Effectiveness was confirmed on both Natural Image Datasets (Split MNIST, Split CIFAR-100) and Biomedical Datasets (PathMNIST).
*   **Memory Efficiency:** The model successfully prevents catastrophic forgetting without the memory cost associated with storing raw data.
*   **Scalability Confirmed:** Results verify that the Bayesian updates are scalable and tractable, addressing computational bottlenecks for real-world deployment.

---

*Analysis based on 23 citations. Quality Score: 6/10*