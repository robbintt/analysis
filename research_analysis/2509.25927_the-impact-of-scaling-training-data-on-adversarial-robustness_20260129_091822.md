# The Impact of Scaling Training Data on Adversarial Robustness

*Marco Zimmerli; Andreas Plesner; Till Aczel; Roger Wattenhofer*

---

### ðŸ“Š Quick Facts

| Metric | Finding |
| :--- | :--- |
| **Models Evaluated** | 36 State-of-the-art architectures |
| **Data Scale Range** | 1.2 million to 22 billion images |
| **Attack Categories** | 6 (Random, Geometric, Object, Style, etc.) |
| **Data Scaling Law** | 10x increase = ~3.2% ASR reduction |
| **Model Scaling Law** | 10x increase = ~13.4% ASR reduction |
| **Metric Variance** | Approx ASR underestimates actual ASR by ~3.09% |

---

## Executive Summary

**Problem**
As vision models grow massive, a critical question emerges: does simply increasing training data volume guarantee security against adversarial attacks? This paper addresses the vulnerability of state-of-the-art vision models to unconstrained black-box perturbations, ranging from random noise and geometric masks to style shifts and corruptions. The research is significant because it challenges the prevailing assumption that scaling data acts as a panacea for robustness, investigating whether the transition from millions to billions of training images closes the substantial gap that remains between human and machine visual perception.

**Innovation**
The authors introduce a comprehensive empirical framework to evaluate adversarial robustness across unprecedented scales of data and model architecture. They assessed 36 diverse modelsâ€”including Supervised, Self-Supervised, and Multi-modal architecturesâ€”trained on datasets ranging from 1.2 million to 22 billion images. The technical innovation lies in the rigorous application of a black-box threat model utilizing six distinct categories of attacks, rather than standard $L_p$-norm constraints. This approach was complemented by specific adversarial fine-tuning protocols and a direct benchmark against human visual performance to isolate the effects of scale from other factors.

**Results**
The study establishes that adversarial robustness follows a **logarithmic scaling law** relative to both data and model size. Quantitatively, a 10x increase in training data volume yields only a marginal reduction in Attack Success Rate (ASR) of approximately 3.2%, whereas a 10x increase in model size provides a more substantial 13.4% reduction. Crucially, data quality proved to be a decisive factor; the DINOv2 model, trained on curated data, outperformed models trained on significantly larger, less-curated datasets. Additionally, the researchers found that using an Approximated ASR metric underestimates actual vulnerability by an average of 3.09 percentage points, and while adversarial fine-tuning mitigates structural attacks, it fails against color distribution shifts.

**Impact**
This work fundamentally shifts the understanding of how to build secure vision systems by demonstrating that raw data scale offers diminishing returns for robustness. By quantifying the specific logarithmic relationship between volume and security, the authors provide the field with predictive scaling laws that argue for prioritizing data curation, architectural choices, and training objectives over merely enlarging datasets. The findings expose a critical limitation in current fine-tuning defenses and highlight the persistent gap between biological and machine vision, guiding future research toward high-quality data synthesis and structural resilience rather than brute-force scaling.

---

## Key Findings

*   **Logarithmic Scaling:** Adversarial robustness improves logarithmically with both data volume and model size. A 10x increase in data volume reduces Attack Success Rate (ASR) by only ~3.2%.
*   **Significance of Model Size:** Increasing model size has a more pronounced effect than data volume; a 10x increase in model parameters reduces ASR by ~13.4%.
*   **Quality Over Quantity:** Data curation is more critical than raw scale. For instance, **DINOv2** (trained on curated data) outperforms models trained on larger, less-curated datasets.
*   **Fine-Tuning Limitations:** Adversarial fine-tuning is effective against structural variations but fails to generalize when facing color distribution shifts.
*   **Perceptual Gap:** Significant gaps remain between human visual perception and machine robustness, even at massive scales.

---

## Methodology

The researchers employed a rigorous evaluation framework designed to test the limits of modern vision systems:

1.  **Model Selection:** Evaluated **36 state-of-the-art vision models** across Supervised, Self-Supervised, and Multi-modal paradigms.
2.  **Data Scale:** Training data ranged from **1.2 million to 22 billion images**, allowing for the analysis of scaling effects.
3.  **Attack Vectors:** Models were tested against six categories of black-box attacks:
    *   Random Perturbations
    *   Geometric Masks (V1/V2)
    *   Object Manipulations (COCO-Objects)
    *   Style Shifts (ImageNet-R)
    *   Corruptions (ImageNet-C)
4.  **Benchmarking:** The study involved benchmarking against human visual performance and conducting specific adversarial fine-tuning experiments.

---

## Technical Details

*   **Threat Model:** Black-box setting with unconstrained perturbations (moving away from standard $L_p$-norm constraints).
*   **Evaluation Split:** ImageNet-1K validation set.
*   **Protocols:**
    *   Zero-shot evaluation for CLIP models.
    *   Linear head training for frozen backbones.
*   **Attack Infrastructure:**
    *   **Categories:** Random Perturbations, GeometricMasksV1/V2, COCO-Objects, ImageNet-R, ImageNet-C.
    *   **Metrics:** Standard ASR and Approximated ASR.
*   **Fine-Tuning Configuration:**
    *   **Base Model:** ResNet50
    *   **Parameters:** 50% augmented images over 3 epochs.
    *   **Scheme:** Specific mask schemes application.
*   **Metric Accuracy:** The Approximated ASR metric underestimates actual ASR by an average of 3.09 percentage points (std dev: 1.93%) across 684 evaluation points, though it maintains consistent relative ranking.

---

## Results

The empirical data yielded several critical insights regarding the relationship between scale and security:

*   **Diminishing Returns:** The results confirm a logarithmic relationship. Simply throwing more data at the problem yields increasingly smaller gains in robustness.
*   **Curated vs. Large Scale:** High-quality, curated datasets (like those used for DINOv2) provide better robustness than simply scaling up to billions of uncurated images.
*   **Metric Reliability:** While the Approximated ASR underestimates true vulnerability by ~3.09%, it remains a reliable proxy for ranking model resilience.
*   **Structural vs. Color:** Defense mechanisms are not universal. Fine-tuning successfully defends against structural attacks but leaves models vulnerable to color-based distribution shifts.

---

## Contributions

*   **Quantitative Scaling Laws:** Established the first quantitative metrics defining the **logarithmic scaling law** for adversarial robustness relative to data and model size.
*   **Re-evaluation of Scale:** Challenged the "scale is all you need" narrative by highlighting that data quality, architecture, and training objectives are more decisive for resilience than raw volume.
*   **Comprehensive Benchmarking:** Provided a wide-ranging benchmark of vision model robustness by combining diverse attack types with vast data scales.

---

**Quality Score:** 9/10  
**References:** 40 citations