---
title: 'A Survey of Large Language Models in Discipline-specific Research: Challenges,
  Methods and Opportunities'
arxiv_id: '2507.08425'
source_url: https://arxiv.org/abs/2507.08425
generated_at: '2026-02-03T13:35:24'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities

*Lu Xiang; Yang Zhao; Yaping Zhang; Chengqing Zong*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Key Highlight:** DeepSeek V3 (671B parameters, $5.57M training cost)
> *   **Core Framework:** Dual-perspective analysis (Technical & Applicability)
> *   **Trend:** Shift toward open-source models competing with closed-source alternatives

---

## Executive Summary

This paper addresses the critical lack of systematic understanding regarding the integration of Large Language Models (LLMs) into discipline-specific research. As LLMs increasingly permeate academia, they are reshaping methodologies and fostering interdisciplinary collaboration; however, the landscape remains fragmented, with researchers struggling to identify the most effective strategies for adapting general-purpose models to highly specialized domains.

A comprehensive synthesis is required to bridge the gap between general AI capabilities and the rigorous demands of fields ranging from hard sciences to humanities. The core innovation is a **dual-perspective framework** that categorizes existing research through a *Technical Perspective* (methodological adaptation) and an *Applicability Perspective* (domain-specific deployment).

Technically, the paper establishes a taxonomy based on interaction with external information, dividing methodologies into **Internal Knowledge Optimization** (Continued Pre-training/CPT, Supervised Fine-tuning/SFT, RLHF) and **External Interaction** (Prompt Engineering, Retrieval-Augmented Generation/RAG, Agent-based Methods, and Tool-use Integration). This structure allows the authors to systematically map how specific architecturesâ€”such as Transformer models and Hybrid Expert Models (MoE)â€”are optimized for distinct scientific requirements.

The survey highlights significant quantitative benchmarks and domain-specific successes, noting that open-source models are increasingly competitive with closed-source alternatives. A standout example is DeepSeek V3, a Mixture-of-Experts model with 671 billion parameters and a training cost of $5.57 million, which rivals top proprietary systems. Despite these successes, the results indicate persistent challenges, primarily hallucinations and gaps in domain-specific knowledge. This work serves as a foundational resource for researchers navigating the complex intersection of AI and specialized research.

---

## Key Findings

*   **Reshaping Research:** LLMs are fundamentally reshaping research methodologies and fostering interdisciplinary collaboration across numerous fields, though a systematic understanding of this integration is currently lacking.
*   **Technical Enhancement:** The effectiveness of LLMs in discipline-specific contexts is significantly enhanced through specific technical methodologies, including:
    *   Supervised fine-tuning (SFT)
    *   Retrieval-augmented generation (RAG)
    *   Agent-based approaches
    *   Tool-use integration
*   **Broad Applicability:** LLMs are actively contributing to a wide spectrum of disciplines, successfully performing tasks in:
    *   **Hard Sciences:** Mathematics, physics, chemistry, and biology.
    *   **Humanities & Social Sciences:** Various qualitative and historical analyses.
*   **Challenges Remain:** Despite recent advances, the integration of LLMs into specific disciplines faces prevailing challenges (e.g., hallucinations), necessitating critical examination and highlighting promising avenues for future research.

---

## Methodology

The authors conducted a **systematic literature survey** that categorizes existing research efforts using a dual-perspective framework:

1.  **Technical Perspective:** Analyzes methodological approaches used to adapt LLMs (e.g., fine-tuning, RAG).
2.  **Applicability Perspective:** Analyzes domain-specific deployments across various academic fields.

Additionally, the methodology involves a critical examination of prevailing challenges and a synthesis of recent advances to highlight future research directions.

---

## Contributions

*   **Comprehensive Overview:** Provides a holistic view of LLM applications in interdisciplinary studies, addressing the gap in systematic understanding of their integration into diverse disciplines.
*   **Categorization of Methodologies:** Offers a detailed categorization of key technical methodologiesâ€”such as supervised fine-tuning, retrieval-augmented generation, and agent-based approachesâ€”that enhance LLM adaptability.
*   **Discipline-Specific Demonstration:** Demonstrates the role of LLMs in discipline-specific tasks by surveying applications ranging from STEM fields to the humanities and social sciences.
*   **Future Roadmap:** Identifies critical challenges and outlines promising research directions, serving as a resource for researchers navigating the complex landscape of LLMs in specialized contexts.

---

## Technical Details

The paper outlines a taxonomy for applying LLMs to research based on interaction with external information.

### Core Architectures
*   Transformer Architecture
*   Scaling Laws
*   Hybrid Expert Models (MoE) â€” *e.g., DeepSeek V3*

### Methodological Taxonomy

#### 1. Internal Knowledge Optimization
*   **Continued Pre-training (CPT)**
*   **Supervised Fine-tuning (SFT)**
*   **Instruction Fine-tuning**
*   **RLHF** (Reinforcement Learning from Human Feedback)

#### 2. External Interaction
*   **Prompt Engineering:** Including Chain-of-Thought (CoT).
*   **Retrieval-Augmented Generation (RAG)**
*   **Agent-based Methods**
*   **Tool-use Integration**

---

## Results

### Quantitative Metrics & Trends
*   **DeepSeek V3:** 671 billion parameters; $5.57 million training cost. Competes with top closed-source models.
*   **Trend:** A notable shift toward open-source models (e.g., LLaMA, Qwen, DeepSeek) performing competitively against proprietary systems.

### Discipline-Specific Mappings
The survey identifies specific methodologies applied to academic fields:

| Discipline | Methodology & Examples |
| :--- | :--- |
| **Mathematics** | **CPT:** LLEMMA <br> **Tool-use:** TOOLFORMER |
| **Physics** | **CPT:** ASTROMLAB2 <br> **RAG:** SCIPHY-RAG |
| **Chemistry** | **Agent-based:** COSCIENTIST <br> **Tool-use:** CHEMCROW |
| **Biology** | **SFT:** PMC-LLAMA <br> **RAG:** BIORAG |
| **Humanities** | **Prompt Engineering:** HISTOLENS |

### Key Challenges
*   **Hallucination:** Accuracy of generated information remains a primary concern.
*   **Domain Knowledge Gaps:** Difficulty in retaining and retrieving highly specialized niche knowledge.

---

**Quality Score:** 8/10
**References:** 40 citations