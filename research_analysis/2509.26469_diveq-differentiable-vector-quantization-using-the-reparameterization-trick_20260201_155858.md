# DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick

*Mohammad Hassan Vali; Tom BÃ¤ckstrÃ¶m; Arno Solin*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Core Innovation**| Reparameterization trick for hard vector quantization |
| **Key Variants** | DiVeQ, SF-DiVeQ (Space-Filling) |
| **Top Distortion** | $5 \cdot 10^{-5}$ (vs. STE $0.012$) |
| **Best LPIPS** | 0.216 (FFHQ Dataset) |
| **Architecture** | VQ-VAE, VQGAN |

---

## 1. Executive Summary

Vector Quantization (VQ) is a critical component in deep generative models like VQ-VAEs and VQGANs, but its inherent non-differentiability poses a significant optimization challenge. Standard methods, such as the Straight-Through Estimator (STE), often suffer from biased gradient estimates or rely on complex heuristic stabilizersâ€”including auxiliary losses, commitment weights, and exponential moving averagesâ€”to prevent codebook collapse. These workarounds complicate the training pipeline, introduce hyperparameter sensitivity, and can still result in poor codebook utilization, limiting the reconstruction quality and stability of latent variable models.

The authors introduce **DiVeQ** (Differentiable Vector Quantization), a novel mechanism that utilizes the reparameterization trick to enable unbiased gradient flow through hard quantization assignments. Technically, DiVeQ approximates the quantized latent $z_q$ as the input vector $z$ plus a directional error term, preserving hard assignments in the forward pass while allowing gradient propagation in the backward pass via directional noise. Building on this, the **Space-Filling DiVeQ (SF-DiVeQ)** variant employs a geometric approach to codebook utilization by constructing continuous curves that connect adjacent codewords. By mapping inputs to this curve using a dithering trick, SF-DiVeQ ensures full codebook usage and mitigates collapse without requiring auxiliary losses or temperature annealing schedules.

Evaluated on VQ-VAE and VQGAN architectures across datasets including AFHQ, CELEBA-HQ, FFHQ, and LSUN, the proposed methods significantly outperformed baselines. DiVeQ achieved a Distortion per Bit of $5 \cdot 10^{-5}$, a drastic improvement compared to STE ($0.012$) and NSVQ ($2.6 \cdot 10^{-4}$). In terms of perceptual quality, SF-DiVeQ demonstrated superior performance with lower LPIPS scoresâ€”indicating better perceptual similarityâ€”recording $0.349$ on CELEBA-HQ, $0.238$ on AFHQ, and $0.216$ on FFHQ. These results were achieved through a completely end-to-end optimization process, validating the efficacy of the geometric space-filling approach.

This research represents a significant architectural simplification for deep generative models relying on discrete latent representations. By eliminating the need for auxiliary loss terms and complex tuning schedules, DiVeQ and SF-DiVeQ offer a more streamlined, theoretically grounded alternative to existing quantization techniques. The ability to maintain hard assignments while ensuring unbiased gradients and robust codebook usage addresses long-standing stability issues in VQ-based training, potentially setting a new standard for future developments in efficient, high-fidelity image generation and compression.

---

## 2. Key Findings

*   **Gradient Through Hard Assignments:** DiVeQ enables gradients to flow through hard vector quantization assignments without softening the forward pass, solving the non-differentiability issue natively.
*   **Space-Filling Optimization:** The space-filling variant (**SF-DiVeQ**) reduces quantization error and ensures full codebook utilization by mapping inputs to a continuous curve connecting codewords.
*   **Streamlined Training:** Both DiVeQ and SF-DiVeQ allow for streamlined, end-to-end training without the need for auxiliary losses or temperature schedules.
*   **Superior Performance:** The methods demonstrate superior performance in VQ-VAE and VQGAN tasks, yielding better reconstruction and sample quality compared to standard baselines.

---

## 3. Methodology

The research employs a novel reparameterization strategy combined with geometric space-filling concepts:

*   **Reparameterization Trick:** The method treats vector quantization as the addition of an error vector, mimicking distortion. This allows for hard assignments in the forward pass while enabling gradient flow backward.
*   **Geometric Space-Filling:** SF-DiVeQ constructs a curve connecting codewords and makes assignments relative to this curve, ensuring a more efficient use of the latent space.
*   **Simplified Optimization:** The framework is fully end-to-end, explicitly avoiding heuristic stabilizers such as auxiliary losses or annealing schedules, reducing hyperparameter tuning complexity.

---

## 4. Contributions

The paper makes three primary contributions to the field of deep generative modeling:

1.  **DiVeQ Mechanism:** Introduces a novel gradient estimation mechanism for differentiable vector quantization that preserves hard assignments and directly addresses the vanishing gradient problem.
2.  **SF-DiVeQ:** Proposes a novel geometric approach to codebook utilization that addresses codebook collapse and high distortion errors through space-filling curves.
3.  **Architectural Simplification:** Contributes a simplified architecture for deep generative models by removing the complexity associated with balancing auxiliary losses and tuning hyperparameters.

---

## 5. Technical Details

The paper introduces **DiVeQ** and **SF-DiVeQ** to address gradient collapse in Vector Quantization using the reparameterization trick.

#### Core DiVeQ Formulation
DiVeQ approximates the quantized latent $z_q$ using the following formulation:
$$z_q = z + \|\vec{d}\|_2 \cdot \text{sg}[v_d / \|v_d\|_2]$$

Where:
*   $z$ is the input vector.
*   $\vec{d}$ is the vector to the nearest codeword.
*   $v_d$ includes directional noise.
*   $\text{sg}[\cdot]$ denotes the stop-gradient operator.

This equation ensures **hard assignments** during the forward pass but provides **unbiased gradients** in the backward pass without auxiliary losses.

#### SF-DiVeQ and Codebook Utilization
SF-DiVeQ further addresses codebook collapse by using a dithering trick to map inputs to continuous curves connecting codewords. The curve connection is defined as:
$$c^d_{i*} = (1 - \lambda_{i*})c_{i*} + \lambda_{i*}c_{i*+1}$$

This interpolation ensures better use of the codebook by populating the space between discrete codewords.

---

## 6. Results

The proposed methods were evaluated on VQ-VAE and VQGAN tasks using standard datasets (**AFHQ, CELEBA-HQ, FFHQ, LSUN**).

### Quantitative Metrics
*   **Distortion per Bit:** DiVeQ achieved significantly lower distortion ($5 \cdot 10^{-5}$) compared to baselines:
    *   **STE:** $0.012$
    *   **NSVQ:** $2.6 \cdot 10^{-4}$
*   **Perceptual Quality (LPIPS):** SF-DiVeQ achieved the best perceptual reconstruction metrics (lower is better):
    *   **CELEBA-HQ:** $0.349$
    *   **AFHQ:** $0.238$
    *   **FFHQ:** $0.216$

### Training Efficiency
The methods demonstrated superior codebook alignment and optimization efficiency without requiring auxiliary loss terms, validating the robustness of the geometric space-filling approach.

---
**References:** 40 citations