---
title: 'When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient
  Visual Question Answering using Small VLMs'
arxiv_id: '2509.16633'
source_url: https://arxiv.org/abs/2509.16633
generated_at: '2026-02-03T18:31:42'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs

*Abhirama Subramanyam Penamakuri; Navlika Singh; Piyush Arora; Anand Mishra*

---

> ### **Quick Facts**
> *   **Benchmarks Tested:** TextVQA, ST-VQA, ChartQA, OKVQA
> *   **Model Definition:** S-VLM (≤ 5B params), L-VLM (> 5B params)
> *   **Key Performance Gains:** +3.4 pp (TextVQA), up to +13.1 pp (ChartQA)
> *   **Scale Comparison:** 4B VLM is approx. 0.2% the size of GPT-4
> *   **Training Requirement:** Label-free (uses only unlabeled images)

---

## Executive Summary

Small Vision-Language Models (S-VLMs) are critical for deploying AI in resource-constrained environments due to their low latency and memory footprint. However, they historically suffer from a significant performance gap compared to Large Vision-Language Models (L-VLMs). Bridging this disparity is conventionally challenging because it typically requires expensive human-annotated training data or computationally inefficient knowledge distillation processes, which negate the inherent speed advantages of smaller models.

The authors introduce the **Model Parity Aligner (MPA)**, a label-free framework designed to transfer knowledge from L-VLMs to S-VLMs using only unlabeled images. MPA utilizes a targeted, three-stage pipeline:

1.  **Pseudo Annotator (PA):** Employs the L-VLM to generate synthetic Question-Answer pairs.
2.  **Parity Identifier (PI):** Filters this data to isolate specific samples where the L-VLM succeeds but the S-VLM fails.
3.  **Parity Leveler (PL):** Fine-tunes the S-VLM exclusively on this curated subset.

By keeping L-VLM parameters frozen and focusing training strictly on identified knowledge gaps, MPA aligns model performance more efficiently than standard distillation techniques.

The framework was validated across four diverse VQA benchmarks—TextVQA, ST-VQA, ChartQA, and OKVQA—demonstrating consistent reductions in the performance gap between S-VLMs and L-VLMs. Evaluations across five different S-VLM architectures showed significant accuracy gains, including improvements of approximately **3.4 percentage points on TextVQA** and up to **13.1 percentage points on ChartQA** over baseline models. Notably, the study highlights that a 4B-parameter VLM enhanced by MPA is approximately **0.2%** the size of models like GPT-4, yet it achieves these accuracy improvements while strictly maintaining its inference-time efficiency.

This research provides a viable pathway for deploying high-performance visual AI on edge devices and systems with limited computational resources. By eliminating the dependency on labeled datasets and optimizing the training process to target only relevant knowledge disparities, MPA significantly reduces the cost and time associated with developing capable VLMs.

---

## Key Findings

*   **Performance Gap:** Small Vision-Language Models (S-VLMs) offer computational efficiency but suffer from a significant performance gap compared to Large Vision-Language Models (L-VLMs).
*   **Label-Free Success:** The Model Parity Aligner (MPA) successfully enhances S-VLM performance without labeled training data, utilizing only unlabeled images and knowledge transfer from L-VLMs.
*   **Consistent Results:** MPA consistently reduced the performance gap between S-VLMs and L-VLMs across four diverse VQA benchmarks: TextVQA, ST-VQA, ChartQA, and OKVQA.
*   **Specialized Capabilities:** The framework proved effective in tasks requiring specialized capabilities, including text recognition, chart interpretation, and commonsense reasoning, while maintaining computational efficiency.

---

## Methodology

The researchers introduced the **Model Parity Aligner (MPA)**, a framework designed to improve S-VLMs through a label-free knowledge transfer process. MPA utilizes a strategic parity-based approach that leverages unlabeled images and the latent knowledge of L-VLMs.

The core of the methodology involves:
*   Precisely identifying specific knowledge disparities between the S-VLM and the L-VLM.
*   Optimizing the training process by targeting only these identified gaps, thereby aligning the small model's performance with the large model more efficiently.

---

## Technical Details

The proposed Model Parity Aligner (MPA) is a label-free, three-stage pipeline transferring knowledge from a Large Vision-Language Model (L-VLM) to a Small Vision-Language Model (S-VLM) using unlabeled images.

### The Three-Stage Pipeline

1.  **Pseudo Annotator (PA)**
    *   The L-VLM generates synthetic Question-Answer pairs from unlabeled images.
2.  **Parity Identifier (PI)**
    *   Filters the generated data to retain only samples where the L-VLM succeeds but the S-VLM fails.
3.  **Parity Leveler (PL)**
    *   Fine-tunes the S-VLM on the curated subset using negative log-likelihood loss while keeping L-VLM parameters frozen.

### Parameter Definitions
*   **S-VLMs:** Models with ≤ 5B parameters.
*   **L-VLMs:** Models exceeding 5B parameters.

---

## Results

The framework was tested on TextVQA, ST-VQA, ChartQA, and OKVQA benchmarks.
*   **Gap Reduction:** MPA consistently reduced the performance gap between S-VLMs and L-VLMs.
*   **Skill Enhancement:** Significant improvements were observed in OCR/Text Recognition, Chart Interpretation, and Commonsense Reasoning.
*   **Efficiency Maintained:** The approach maintains the inference time efficiency of small models while significantly enhancing VQA accuracy for five different S-VLMs.
*   **Scale Comparison:** A 4B-parameter VLM is approximately 0.2% of the size of GPT-4.

---

## Contributions

*   **MPA Development:** Creation of the Model Parity Aligner (MPA), a systematic approach for aligning S-VLMs with L-VLMs without the need for expensive labeled datasets.
*   **Targeted Strategy:** Introduction of a targeted parity-based training strategy that focuses training specifically on knowledge gaps, offering an alternative to standard distillation techniques.
*   **Comprehensive Validation:** Extensive testing across multiple benchmarks requiring distinct reasoning skills (visual text recognition, chart understanding, and factual reasoning), demonstrating the approach's versatility.
*   **Resource Efficiency:** A resource-efficient AI solution that bridges the performance gap between large and small models while preserving the computational efficiency necessary for resource-constrained environments.

---

**Quality Score:** 8/10  
**References:** 40 citations