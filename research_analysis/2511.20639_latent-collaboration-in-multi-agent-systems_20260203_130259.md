---
title: Latent Collaboration in Multi-Agent Systems
arxiv_id: '2511.20639'
source_url: https://arxiv.org/abs/2511.20639
generated_at: '2026-02-03T13:02:59'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Latent Collaboration in Multi-Agent Systems

*Jiaru Zou; Xiyuan Yang; Ruizhong Qiu; Gaotang Li; Katherine Tieu; Pan Lu; Ke Shen; Hanghang Tong; Yejin Choi; Jingrui He; James Zou; Mengdi Wang; Ling Yang*

---

> ### ðŸ“Š Quick Facts
> *   **Performance:** Up to **14.6%** accuracy improvement over baselines.
> *   **Efficiency:** **70.8% â€“ 83.7%** reduction in output token usage.
> *   **Speed:** **4.3x** average inference speedup (peaking at 7.0x).
> *   **Scope:** Validated across **9 benchmarks** (Math, Science, Commonsense, Code).
> *   **Training:** **Training-free** (no weight updates or fine-tuning required).

---

## Executive Summary

Traditional Multi-Agent Systems (MAS) typically rely on natural language text as the primary medium for inter-agent communication. While effective, this approach introduces significant inefficiencies: converting internal reasoning into text (decoding) and parsing it back (encoding) is computationally expensive and introduces latency. Furthermore, this process often results in information loss, as complex internal representations are compressed into discrete tokens. This creates a bottleneck where the benefits of collaborative reasoning are offset by the high computational cost and verbose output of serializing thoughts into natural language, limiting the scalability and real-time applicability of multi-agent frameworks.

The paper introduces **LatentMAS**, a training-free, end-to-end framework that shifts agent collaboration entirely into the continuous latent space. Instead of communicating via text, agents utilize "Auto-Regressive Latent Thought Generation," which extends reasoning steps directly within the model's hidden embeddings without decoding them to text. Communication is facilitated through a "Shared Latent Working Memory," which transfers internal representations between agents using layer-wise KV caches. This mechanism allows for the lossless exchange of context and reasoning traces. By operating directly on hidden states, LatentMAS bypasses the need for prompt engineering and natural language interfaces, functioning solely through the manipulation and transfer of internal model representations.

LatentMAS was evaluated across **9 benchmarks** spanning math, science, commonsense, and coding tasks, comparing performance against single models and text-based MAS. The framework achieved an average accuracy improvement of **+13.3%** over a baseline score of 50.0, peaking at a **14.6%** gain. Beyond accuracy, LatentMAS demonstrated substantial efficiency improvements: it reduced output token usage by **70.8% to 83.7%** and delivered an average inference speedup of **4.3x**, with specific instances reaching up to **7.0x** acceleration. These results indicate that latent collaboration preserves semantic information better than text-based methods while significantly lowering computational overhead.

This research establishes a new paradigm for agent interaction by proving that continuous latent space communication is not only theoretically more expressiveâ€”with proofs regarding lossless information preservation and lower complexityâ€”but also practically superior. The elimination of the need for additional model training or fine-tuning makes this approach immediately accessible and deployable across existing large language model (LLM) architectures. By decoupling collaborative reasoning from natural language generation, LatentMAS paves the way for more scalable, efficient, and capable multi-agent systems, potentially influencing future designs of high-throughput AI orchestration and collaborative problem-solving frameworks.

---

## Key Findings

*   **Performance:** LatentMAS outperforms baselines by up to **14.6%** in accuracy across 9 diverse benchmarks.
*   **Token Efficiency:** Achieves a massive reduction in output token usage, ranging from **70.8% to 83.7%**.
*   **Inference Speed:** Provides a significant speedup of **4x to 4.3x** during inference.
*   **Domain Validation:** Effectiveness is confirmed across multiple domains, including math, science, commonsense, and code.

---

## Methodology

The research introduces **LatentMAS**, an innovative framework designed to facilitate pure latent collaboration without the need for natural language intermediaries. The methodology rests on two core pillars:

1.  **Auto-Regressive Latent Thought Generation:**
    Instead of decoding thoughts into readable text, the system generates reasoning steps as latent thoughts directly via hidden embeddings. This captures the necessary logic within the model's internal state.

2.  **Shared Latent Working Memory:**
    To enable collaboration, the system preserves and transfers internal representations between agents. This shared memory ensures the lossless exchange of information, allowing agents to build upon each other's reasoning seamlessly.

---

## Technical Details

LatentMAS operates as a training-free, end-to-end collaborative framework designed specifically for Multi-Agent Systems (MAS).

*   **Operational Space:** Functions entirely within the continuous latent space, eliminating the requirement for natural language communication.
*   **Internal Reasoning:** Utilizes internal latent thought generation via auto-regressive generation on last-layer hidden representations. This captures reasoning without the overhead of text decoding.
*   **Communication Protocol:** Facilitated by cross-agent latent working memory transfer.
*   **Implementation:** Uses layer-wise KV caches to store input context and latent thoughts, allowing agents to access and transfer context efficiently.
*   **Training Requirements:** The system requires **no model weight updates** or fine-tuning, making it immediately applicable to existing LLMs.

---

## Results

The framework was rigorously evaluated against single models and traditional text-based MAS across 3 LLM scales.

*   **Accuracy Improvement:** Achieved an average gain of **+13.3%** over a baseline score of 50.0, with a peak improvement of **14.6%**.
*   **Inference Speedup:** Delivered an average speedup of **4.3x** (ranging from 3.1x to 7.0x).
*   **Token Reduction:** Successfully reduced output token usage by **70.8% to 83.7%**.

---

## Contributions

*   **New Paradigm:** Established a novel approach for agent communication via continuous latent space interaction.
*   **Theoretical Proof:** Provided theoretical evidence of higher expressiveness and lossless information preservation, all while maintaining lower computational complexity.
*   **Efficiency:** Demonstrated significant efficiency gains without the need for additional model training.
*   **Open Source:** Code and data have been open-sourced to facilitate further research.

---

**Quality Score:** 8/10 | **References:** 40 citations