---
title: What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic
  Study
arxiv_id: '2601.14888'
source_url: https://arxiv.org/abs/2601.14888
generated_at: '2026-02-03T20:25:19'
quality_score: 9
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study

*Keyu Lv; Manyi Zhang; Xiaobo Xia; Jingchen Ni; Shannan Yan; Xianzhi Yu; Lu Hou; Chun Yuan; Haoli Bai*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Focus** | Low-bit Quantization-Aware Training (QAT) for Reasoning LLMs |
| **Quantization Config** | W3G128 & W2G128 (Weight-only, Group Size 128) |
| **Key Innovation** | **Reasoning-QAT** workflow (KD objective + PTQ init) |
| **Top Performance** | **+44.53%** improvement over GPTQ on MATH-500 |
| **Models Tested** | DeepSeek-R1-Distill-Qwen-1.5B, Qwen3 (0.6B, 4B) |
| **Quality Score** | 9/10 |

---

## Executive Summary

Deploying Large Language Models (LLMs) with advanced reasoning capabilities is computationally expensive, driving a need for extreme low-bit quantization (e.g., 3-bit or 2-bit) to improve inference efficiency. However, standard Post-Training Quantization (PTQ) methods, which calibrate models without retraining, suffer severe performance degradation on complex reasoning tasks like mathematics and logic. While Quantization-Aware Training (QAT) offers a potential solution by fine-tuning the model to adapt to quantization noise, the mechanisms for successfully applying QAT to reasoning modelsâ€”particularly those trained via Reinforcement Learning (RL)â€”remain poorly understood. This paper addresses the critical challenge of maintaining high reasoning accuracy in quantized models, a prerequisite for the cost-effective deployment of state-of-the-art reasoning agents.

The authors introduce **"Reasoning-QAT,"** a unified training pipeline derived from a systematic empirical study of QAT variables. The key technical innovation lies in the identification and combination of three specific mechanisms: the use of Knowledge Distillation (KD) with KL divergence as the primary objective function rather than standard Supervised Fine-Tuning (SFT); the initialization of QAT using a PTQ checkpoint (specifically GPTQ or RTN) to reduce training costs and improve convergence; and the alignment of calibration data domains between the PTQ and QAT stages.

The study demonstrates that standard 3-bit PTQ results in catastrophic accuracy loss, dropping performance by **11.67%** on AIME-120 and **12.80%** on MATH-500. In contrast, the Reasoning-QAT pipeline significantly mitigates this degradation. For models originally trained with SFT, using KD as the QAT objective limited the average performance drop to **8.06%**, outperforming QAT with SFT objectives. The benefits were even more pronounced for RL-trained models; QAT + KD achieved a **9.26%** average drop, whereas QAT + SFT suffered a massive **21.40%** drop. Overall, Reasoning-QAT achieved a **44.53%** improvement over the GPTQ baseline on the MATH-500 dataset.

This research provides a definitive recipe for enabling efficient inference of reasoning LLMs without sacrificing their problem-solving capabilities. By validating Knowledge Distillation as a robust objective for both SFT and RL-trained models, the study resolves the instability often associated with quantizing complex reasoning policies.

---

## Key Findings

*   **Robustness of Knowledge Distillation:** KD serves as a robust objective function for reasoning models, applicable to those trained via both SFT and RL, consistently yielding smaller accuracy drops than standard SFT objectives.
*   **PTQ Initialization Benefits:** Utilizing PTQ to initialize QAT significantly reduces training costs and improves final accuracy compared to training from scratch.
*   **RL Viability:** Reinforcement learning remains a viable strategy for quantized models and can be effectively cold-started within the QAT framework.
*   **Data Domain Alignment:** Aligning the data domain for PTQ calibration with QAT training accelerates convergence and enhances overall accuracy.

---

## Methodology

The authors conducted a systematic empirical study on quantization-aware training (QAT) specifically tailored for reasoning LLMs. Their research design involved:

1.  **Variable Isolation:** Rigorous testing of individual variables including training objectives (SFT vs. KD vs. RL), initialization strategies (RTN vs. GPTQ), and data domain alignment.
2.  **Workflow Consolidation:** Integrating successful insights into the optimized **'Reasoning-QAT'** workflow.
3.  **Validation:** Benchmarking the unified pipeline against state-of-the-art baselines across multiple LLM backbones (DeepSeek-R1-Distill-Qwen, Qwen3) and diverse reasoning datasets.

---

## Technical Details

### Quantization Configuration
*   **Precision:** Extreme low-bit weight-only quantization (**3-bit W3G128** and **2-bit W2G128**).
*   **Group Size:** 128.
*   **Scope:** Applied to all linear layers, excluding token embedding and `lm_head`.

### Initialization & Framework
*   **Initialization Methods:** Compared Symmetric Round-to-Nearest (**RTN**) and Asymmetric **GPTQ**.
*   **QAT Operations:** Employed 'fake quantize' operations in the forward pass and Straight-Through Estimator (**STE**) for gradients in the backward pass.

### Training Objectives
*   **Supervised Fine-Tuning (SFT)**
*   **Knowledge Distillation (KD):** Utilizing KL divergence.
*   **Reinforcement Learning (RL):** Implemented via Group Relative Policy Optimization (**GRPO**).

### Experimental Setup
*   **Models:** DeepSeek-R1-Distill-Qwen-1.5B, Qwen3 (0.6B, 4B).
*   **Training Data:** OpenR1-Math.
*   **Benchmarks:** AIME-120, MATH-500, GSM8K, LiveCodeBench, GPQA-Diamond.

---

## Results

### Impact of Standard PTQ
Standard 3-bit PTQ causes significant degradation in reasoning capabilities:
*   **AIME-120:** -11.67% accuracy drop.
*   **MATH-500:** -12.80% accuracy drop.

### Performance of QAT Strategies
The Reasoning-QAT approach (specifically using KD) significantly outperformed standard QAT+SFT:

| Model Type | Strategy | Average Performance Drop |
| :--- | :--- | :--- |
| **SFT-Trained**<br>(DeepSeek-R1) | **QAT + KD** | **8.06%** |
| | QAT + SFT | 10.51% |
| **RL-Trained**<br>(Qwen3-4B) | **QAT + KD** | **9.26%** |
| | QAT + SFT | 21.40% |

### Overall Improvement
*   **MATH-500:** Reasoning-QAT achieved a **44.53% improvement** over the GPTQ baseline.

---

## Contributions

*   **Comprehensive Mechanism Study:** Provided a detailed study identifying the specific mechanisms that allow low-bit QAT to function effectively for reasoning tasks.
*   **Reasoning-QAT Pipeline:** Introduced a unified training pipeline that achieves superior inference efficiency without sacrificing reasoning capabilities.
*   **State-of-the-Art Performance:** Demonstrated that Reasoning-QAT outperforms existing PTQ methods, highlighted by a 44.53% accuracy improvement on the MATH-500 dataset.

---
**References:** 19 citations