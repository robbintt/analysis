# SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia

*Chaoqun Liu, Mahani Aljunied, Guizhen Chen, Hou Pong Chan, Weiwen Xu, Yu Rong, Wenxuan Zhang*

---

> ### üìä Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Model Architecture** | Qwen2-Audio-7B Encoder + Qwen2.5-7B-Instruct LLM |
> | **Training Scale** | 1.58M conversations (7% multi-turn) |
> | **Hardware Resources** | 32 A800 GPUs for 6 days (1 epoch) |
> | **New Benchmark** | SeaBench-Audio (580 questions, 4 languages) |
> | **Key Languages** | English, Chinese, Indonesian, Thai, Vietnamese |

---

## üìã Executive Summary

This research addresses the critical performance disparity in **Large Audio-Language Models (LALMs)** for Southeast Asian (SEA) languages, which have historically lagged behind high-resource languages. Existing models often fail to capture the linguistic nuances and acoustic diversity of SEA languages such as Indonesian, Thai, and Vietnamese, hindering the deployment of effective voice-enabled AI in the region.

The core innovation is the introduction of **SeaLLMs-Audio**, the first LALM specifically tailored for the Southeast Asian linguistic landscape. Technically, the model utilizes a unified architecture integrating a Qwen2-Audio-7B Audio Encoder with a Qwen2.5-7B-Instruct LLM. The researchers employed full-parameter fine-tuning on a curated dataset of 1.58M multilingual conversations. This architecture supports flexible processing modes‚Äîaudio-only, text-only, and combined‚Äîenabling the system to handle tasks ranging from ASR to complex voice-based chat.

To validate the model‚Äôs capabilities, the authors introduced **SeaBench-Audio**, a rigorous benchmark comprising 580 human-annotated questions across 14 distinct tasks. The evaluation covered English, Indonesian, Thai, and Vietnamese, graded by professional linguists using an LLM-as-a-Judge approach. Benchmarking against strong baselines like Qwen2-Audio-7B-Instruct and MERaLiON-AudioLLM, SeaLLMs-Audio demonstrated competitive regional performance.

The significance of this work lies in establishing a critical foundation for audio AI research in low-resource SEA languages, setting a new standard for regional model development. By releasing both a high-performance model and a rigorous open benchmark, the authors provide the community with tools to drive innovation, lowering the barrier to entry for AI accessibility throughout Southeast Asia.

---

## üîç Key Findings

*   **Competitive Regional Performance:** SeaLLMs-Audio achieves performance comparable to other Large Audio-Language Models (LALMs) specifically on Southeast Asian (SEA) languages.
*   **Versatile Task Capability:** The model demonstrates strong proficiency in both fine-grained audio analysis and complex voice-based interactions.
*   **Flexible Input Processing:** The model effectively processes multiple input modalities, handling audio-only, text-only, and combined audio-text inputs seamlessly.
*   **Robust Multilingual Support:** The model maintains strong support for SEA languages as well as high-resource languages like English and Chinese.

---

## üõ†Ô∏è Methodology

The researchers developed a Large Audio-Language Model (LALM) by training on a large-scale audio corpus specifically curated for Southeast Asian languages.

*   **Unified Architecture:** Designed to be multilingual, multimodal, and multi-task.
*   **Integration Strategy:** Combines processing pipelines for audio and text across a diverse spectrum of tasks.
*   **Goal:** Bridge the gap between technical audio processing and high-level cognitive voice interactions for underrepresented languages.

---

## ‚öôÔ∏è Technical Specifications

### Architecture Components
*   **Audio Encoder:** Derived from **Qwen2-Audio-7B**.
*   **LLM Module:** Utilizes **Qwen2.5-7B-Instruct**.
*   **Connector:** Uses a newly initialized audio adapter to bridge the encoder and LLM.

### Training Protocol
*   **Method:** Full-parameter fine-tuning over 1 epoch.
*   **Infrastructure:** Utilized 32 A800 GPUs for 6 days.
*   **Objective:** Maximize the likelihood of text tokens conditioned on audio and text inputs.

### Data Composition & Curation
The training data consists of **1.58M conversations** (7% multi-turn), covering tasks such as ASR, Audio Captioning, S2TT, QA, Speech Summarization, and Chat.

| Task Type | Curation Strategy |
| :--- | :--- |
| **ASR** | Normalization applied |
| **S2TT (Speech-to-Text Translation)** | Synthetic translation utilized |
| **Audio Captioning (AC)** | Caption translation employed |
| **QA / Chat** | Google TTS used for generation |
| **Summarization** | Generated by Gemini-2.0-Flash |

---

## üåü Contributions

1.  **SeaLLMs-Audio:** Introduction of the first LALM specifically tailored for the Southeast Asian linguistic landscape.
2.  **SeaBench-Audio:** A new benchmark dataset introduced to automate and standardize the evaluation of Large Audio-Language models in the SEA context.
3.  **Comprehensive Skill Integration:** A unified model that bridges technical audio processing tasks with high-level cognitive voice interactions.

---

## üìà Evaluation & Results

The paper introduces **SeaBench-Audio**, a benchmark created to address the lack of SEA audio benchmarks.

*   **Dataset Composition:** 580 questions total.
    *   130 English
    *   150 Indonesian
    *   150 Thai
    *   150 Vietnamese
*   **Task Diversity:** Covers 14 distinct tasks, including ASR, S2TT, Speech QA, and Customer Service.
*   **Annotation:** Performed by professional linguists using a scoring system of 1 to 5.
*   **Evaluation Method:** Utilizes an **LLM-as-a-Judge** approach with Gemini-2.5-flash and specific rubrics.

### Benchmark Comparisons
SeaLLMs-Audio was compared against the following models (specific quantitative scores were not provided in the text):
*   Qwen2-Audio-7B-Instruct
*   Qwen2.5-Omni-7B
*   MERaLiON-AudioLLM-Whisper-SEA-LION
*   MERaLiON-2-10B

---

*Document Generated based on research paper analysis*
**Quality Score:** 7/10
**References:** 12 citations