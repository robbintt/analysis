---
title: Reinforcement Learning with Function Approximation for Non-Markov Processes
arxiv_id: '2601.00151'
source_url: https://arxiv.org/abs/2601.00151
generated_at: '2026-02-03T20:12:27'
quality_score: 9
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Reinforcement Learning with Function Approximation for Non-Markov Processes

*Ali Devran Kara*

---

## Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 24 Citations |
| **Core Focus** | Non-Markov RL, Convergence Theory, POMDPs |
| **Key Algorithms** | TD(0), Q-Learning |
| **Approximation** | Linear Function Approximation |

---

> ### Executive Summary
>
> Standard Reinforcement Learning (RL) theory relies fundamentally on the Markov property, which assumes that future states depend only on the present state and action. However, many real-world environments are history-dependent and violate this assumption, leading to instability and divergence in popular algorithms like Q-learning, particularly when function approximation is employed. This paper addresses the critical gap in theoretical guarantees for RL methods operating within non-Markov state and cost processes—a scenario ubiquitous in Partially Observable Markov Decision Processes (POMDPs) where agents lack complete information about the underlying state.
>
> The core technical innovation is the transformation of the non-Markov problem into an analysis of an auxiliary "stationary regime MDP," constructed from the invariant distribution of the joint process. The author establishes a novel theoretical framework utilizing operators in $L^2$ space, specifically characterizing the algorithm's limit as the fixed point of a joint operator composed of an orthogonal projection ($\Pi$) and the Bellman operator ($T^\gamma$) associated with this auxiliary MDP. Furthermore, the research introduces a specific structural remedy for control tasks: constructing basis functions using quantization maps, which allows the theoretical analysis to extend stability proofs to Q-learning in non-Markov settings.
>
> The study delivers rigorous theoretical results rather than empirical simulations, proving that linear function approximation methods, specifically TD(0), converge for policy evaluation in non-Markov environments provided the joint process is ergodic and satisfies summable strong mixing conditions. A key theoretical finding is that the composed operator $\Pi T^\gamma$ acts as a contraction in the $L^2$ norm with a contraction factor equal to the discount factor $\beta$, ensuring a unique fixed point. Additionally, while Q-learning stability is not generally guaranteed, the paper proves convergence is achieved when using quantization-based basis functions and provides explicit approximation error bounds for algorithms applied to POMDPs using finite-memory variables.

---

## Key Findings

*   **Convergence of Policy Evaluation:** Policy evaluation methods using linear function approximation converge for non-Markov state and cost processes, provided specific ergodicity conditions are met.
*   **Mathematical Characterization of the Limit:** The limit of the policy evaluation algorithm corresponds to the fixed point of a joint operator, which is defined by the composition of an orthogonal projection and the Bellman operator of an auxiliary Markov decision process.
*   **Conditions for Q-Learning Stability:** While convergence for Q-learning with linear function approximation is not generally guaranteed in non-Markov settings, it can be proven if the basis functions are constructed using quantization maps.
*   **Applicability to POMDPs:** The theoretical framework applies to Partially Observed Markov Decision Processes (POMDPs) when using finite-memory variables as state representations, yielding explicit error bounds for the algorithm limits.

---

## Technical Details

The research establishes a rigorous mathematical foundation for RL in non-Markovian environments through the following technical constructs:

### Mathematical Framework
*   **Auxiliary Stationary Regime MDP:** The problem is transformed by analyzing an auxiliary MDP based on the invariant distribution of the joint process.
    *   **Cost Function:** Defined as the expected cost conditioned on state and action under the stationary measure.
    *   **Transition Kernel:** Derived directly from the stationary measure.
    *   **Policy Dependency:** A unique feature of this MDP is that its definition is dependent on the policy being evaluated.
*   **Operator Analysis:** The framework relies on operators in $L^2$ space:
    *   **Orthogonal Projection Map ($\Pi$):** Used for function approximation.
    *   **Bellman Operator ($T^\gamma$):** Associated with the auxiliary MDP.
    *   **Fixed Point:** The algorithm limit corresponds to the fixed point of the composed operator $\Pi T^\gamma$.

### Algorithms & Conditions
*   **Algorithms Analyzed:**
    *   **TD(0):** Used for policy evaluation.
    *   **Q-Learning:** Used for control.
*   **Convergence Requirements:**
    *   The joint process must be **ergodic**.
    *   The process must satisfy a **summable strong mixing condition**.
    *   **Lemma 1:** Establishes stochastic approximation stability via uniform $L^2$ boundedness of noise sequences.
    *   **Proposition 2:** Proves that $\Pi T^\gamma$ is a contraction in the $L^2$ norm with a contraction factor equal to the discount factor $\beta$.

---

## Methodology

The research utilizes a theoretical analysis framework to extend Reinforcement Learning (RL) theory to environments that violate the Markov property. The study proceeds through four primary analytical steps:

1.  **Theoretical Extension:** Focusing specifically on RL methods that employ linear function approximation to extend theory to non-Markov settings.
2.  **Ergodicity Analysis:** Relying on establishing suitable ergodicity conditions to prove algorithmic stability in the absence of the Markov property.
3.  **Operator Characterization:** Involves analyzing a specific mathematical operator—a joint composition of an orthogonal projection and the Bellman operator associated with an auxiliary MDP—to characterize fixed points.
4.  **POMDP Integration:** Substituting the belief state with finite-memory variables to apply the framework to Partially Observable Markov Decision Processes (POMDPs).

---

## Contributions

*   **Theoretical Extension to Non-Markov Settings:** The paper significantly broadens the scope of RL theory by providing convergence guarantees for policy evaluation in non-Markov environments, moving beyond the standard Markovian assumptions.
*   **Resolution of Q-Learning Instability:** It contributes to the understanding of Q-learning divergence issues by identifying a specific structural condition (quantization-based basis functions) that ensures convergence.
*   **Bridging Non-Markov Processes and POMDPs:** By deriving explicit error bounds, the work establishes a rigorous quantitative link between abstract non-Markov processes and the practical domain of POMDPs, facilitating the use of finite-memory representations in partially observable systems.

---

## Results

The provided sections focus on theoretical formulation rather than empirical experimental results or simulation data. Key theoretical results include:

*   **Stochastic Approximation Stability (Lemma 1):** Establishes stability by proving uniform $L^2$ boundedness of noise sequences under bounded initial noise and summable mixing coefficients.
*   **Contraction Mapping (Proposition 2):** Proves that the composed operator $\Pi T^\gamma$ is a contraction in the $L^2$ norm with a contraction factor equal to the discount factor $\beta$, guaranteeing a unique fixed point.
*   **Error Bounds:** The framework is designed to yield explicit approximation error bounds for algorithm limits applicable to POMDPs using finite-memory variables.