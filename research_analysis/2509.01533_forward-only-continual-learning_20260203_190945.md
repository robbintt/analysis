---
title: Forward-Only Continual Learning
arxiv_id: '2509.01533'
source_url: https://arxiv.org/abs/2509.01533
generated_at: '2026-02-03T19:09:45'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Forward-Only Continual Learning

*Jiao Chen; Jiayi He; Fangfang Chen; Zuohong Lv; Jianhua Tang*

---

> ### ðŸ“‘ Quick Facts
> *   **Quality Score:** 8/10
> *   **Citations:** 40
> *   **Core Innovation:** Gradient-free, forward-only learning paradigm
> *   **Efficiency Gain:** ~50% reduction in training time
> *   **Primary Use Case:** Resource-constrained edge devices and multimedia applications

---

## Executive Summary

Continual learning (CL) faces the dual challenge of catastrophic forgetting and the prohibitive computational cost of gradient-based backpropagation. While state-of-the-art methods rely on gradient descent, they incur high latency and memory footprints due to backward passes and data rehearsal, making them unsuitable for resource-constrained edge environments. This paper addresses the critical need for a computationally efficient framework that decouples the learning process from backward propagation, enabling high-performance CL without the overhead of traditional optimization.

The authors introduce **FoRo (Forward-Only Continual Learning)**, a gradient-free framework designed for frozen Vision Transformers (ViTs). FoRo replaces backpropagation with two core components: a **Lightweight Prompt Tuning Strategy** utilizing the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to optimize prompt embeddings, and a **Knowledge Encoding Mechanism** using Nonlinear Random Projection (NRP) and Recursive Least Squares (RLS) for incremental classifier updates. By combining Cross Entropy and Feature Discrepancy in a fitness function, FoRo minimizes cumulative expected risk strictly through forward propagation, preserving the pre-trained backbone while avoiding data rehearsal.

Evaluations on standard benchmarks such as CIFAR-100 and TinyImageNet demonstrate that FoRo achieves competitive accuracy, often surpassing prompt-based baselines like L2P and DualPrompt, while maintaining lower forgetting measures. Crucially, the framework delivers substantial quantitative efficiency improvements: by eliminating the backward pass, FoRo reduces training time by approximately 50% and significantly cuts memory usage by obviating the need for gradient storage and rehearsal buffers. These results validate that plasticity and stability can be balanced effectively without gradient-based updates.

FoRo represents a paradigm shift in CL, proving that high-performance, rehearsal-free learning is achievable without backpropagation. Its mathematical foundationâ€”using evolutionary strategies for prompt tuning and RLS for classificationâ€”provides a scalable, privacy-preserving alternative for on-device AI. This work opens new avenues for deploying sophisticated continual learning on edge devices and in real-time multimedia applications where computational resources are strictly limited.

---

## Key Findings

*   **Enhanced Performance:** FoRo significantly reduces average forgetting and improves accuracy compared to existing continual learning approaches.
*   **Operational Efficiency:** The framework substantially reduces memory usage and run time through its forward-only learning mechanism.
*   **Data Efficiency:** Maintains high knowledge retention across long task sequences without the need to revisit or store prior data (rehearsal-free).
*   **Practical Applicability:** Particularly effective for resource-constrained environments and real-world multimedia applications.

---

## Methodology

The paper proposes **FoRo**, a forward-only and gradient-free continual learning framework designed specifically for frozen pre-trained models. The methodology is built upon two primary pillars:

1.  **Lightweight Prompt Tuning Strategy**
    *   Replaces traditional gradient-based backpropagation with evolutionary optimization.
    *   Utilizes the **Covariance Matrix Adaptation Evolution Strategy (CMA-ES)** to optimize prompt embeddings, ensuring the pre-trained backbone remains frozen.

2.  **Knowledge Encoding Mechanism**
    *   Enables incremental updates to classifiers without data rehearsal.
    *   Employs **Nonlinear Random Projection** and **Recursive Least Squares** to accumulate knowledge efficiently over a sequence of tasks.

---

## Technical Details

The following table outlines the specific technical components and architectural decisions within the FoRo framework:

| Component | Technique/Description |
| :--- | :--- |
| **Optimization Paradigm** | **Forward-Only:** Eliminates error backpropagation and gradient-based updates; relies solely on forward propagation. |
| **Model Backbone** | **Frozen Vision Transformer (ViT):** The backbone parameters remain static throughout the training process. |
| **Prompt Optimization** | **CMA-ES:** Uses the Covariance Matrix Adaptation Evolution Strategy instead of gradient descent for tuning prompts. |
| **Knowledge Encoding** | **Historical Statistics:** Encodes knowledge using the mean ($\mu$) and standard deviation ($\sigma$) of features. |
| **Classification Head** | **Least Squares:** Utilizes a Least Squares mechanism optimized by a fitness function combining Cross Entropy and Feature Discrepancy. |
| **Feature Processing** | Extracts the `[CLS]` token embedding and utilizes **Neural Random Projection (NRP)**. |
| **Objective** | Mathematical goal is to minimize the cumulative expected risk across a sequence of tasks. |

---

## Research Contributions

*   **Gradient-Free Paradigm:** Introduction of a gradient-free, forward-only optimization paradigm (FoRo) to address the computational intensity of backpropagation in continual learning.
*   **Evolutionary Prompting:** Novel application of CMA-ES for evolutionary prompt optimization, bypassing the need for gradient calculations.
*   **Efficient Knowledge Accumulation:** Development of an efficient mechanism using nonlinear random projection and recursive least squares to enable incremental updates without catastrophic forgetting.
*   **Backbone Preservation:** A framework that achieves competitive performance while strictly preserving the pre-trained model backbone, facilitating deployment on frozen models.

---

## Performance & Results

FoRo was evaluated against standard benchmarks, demonstrating a significant advantage in both performance and efficiency:

*   **Accuracy vs. Forgetting:** The method achieves higher accuracy and lower average forgetting compared to existing approaches.
*   **Resource Optimization:** By eliminating backward passes and gradient storage, FoRo cuts memory usage and run time substantially.
*   **Benchmark Performance:** Demonstrated strong results on CIFAR-100 and TinyImageNet, outperforming prompt-based baselines like L2P and DualPrompt in efficiency metrics.
*   **Deployment Suitability:** The reduction in computational overhead makes FoRo highly suitable for edge devices and real-time applications where resources are limited.