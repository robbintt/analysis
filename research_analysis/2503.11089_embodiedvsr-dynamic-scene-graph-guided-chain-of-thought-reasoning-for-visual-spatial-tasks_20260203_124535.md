---
title: 'EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual
  Spatial Tasks'
arxiv_id: '2503.11089'
source_url: https://arxiv.org/abs/2503.11089
generated_at: '2026-02-03T12:45:35'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks

*Yi Zhang; Qiang Zhang; Xiaozhu Ju; Zhaoyang Liu; Jilei Mao; Jingkai Sun; Jintao Wu; Shixiong Gao; Shihan Cai; Zhiyuan Qin; Linkai Liang; Jiaxu Wang; Yiqun Duan; Jiahang Cao; Renjing Xu; Jian Tang*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Benchmark** | eSpatial-Benchmark |
| **Accuracy (EmbodiedVSR)** | **84.6%** |
| **Accuracy (GPT-4V)** | 58.8% |
| **Reasoning Coherence** | **82.4%** |
| **Key Capability** | Zero-shot spatial reasoning without fine-tuning |

---

## Executive Summary

Current Multimodal Large Language Models (MLLMs) face critical limitations in performing fine-grained visual spatial reasoning within embodied AI environments. These models struggle to disentangle intricate spatial relationships and maintain reasoning coherence over long-horizon tasks that require iterative interaction with the physical world. Existing approaches frequently fail to align internal reasoning steps with dynamic environmental states, leading to errors in complex scenarios that demand precise understanding of object relations, such as "stuck inside" or "leaning against." This creates a gap between abstract language models and the concrete demands of embodied interaction, where current solutions often rely on inefficient task-specific fine-tuning.

The authors introduce **EmbodiedVSR**, a novel framework that bridges this gap by integrating Dynamic Scene Graph-guided Chain-of-Thought (CoT) reasoning. Technically, the system constructs explicit structured knowledge representations by generating dynamic scene graphs that capture objects and scaleless spatial relations using vector notation. This graph serves as a guide for the CoT reasoning process, ensuring that every reasoning step is grounded in actionable environmental dynamics.

Evaluated on the newly introduced **eSpatial-Benchmark**, EmbodiedVSR demonstrated quantitatively superior performance compared to existing MLLM-based methods. In terms of Accuracy (success rate), the framework achieved **84.6%**, significantly outperforming GPT-4V (58.8%) and LLaVA (45.3%). In Reasoning Coherence, EmbodiedVSR attained **82.4%**, substantially exceeding GPT-4V's 55.3%. The significance of this work lies in its validation that structured representations can effectively replace the need for task-specific fine-tuning in embodied spatial reasoning.

---

## Key Findings

*   **Superior Performance:** EmbodiedVSR significantly outperforms existing Multimodal Large Language Model (MLLM)-based methods in both accuracy and reasoning coherence.
*   **Long-Horizon Capability:** The framework demonstrates particular strength in long-horizon tasks that require iterative interaction with the environment.
*   **Zero-Shot Reasoning:** By utilizing dynamic scene graphs, the approach enables zero-shot spatial reasoning without the need for task-specific fine-tuning.
*   **Structured Knowledge:** The explicit construction of structured knowledge representations successfully disentangles intricate spatial relationships within complex scenarios.

---

## Methodology

The proposed framework, **EmbodiedVSR**, operates by integrating dynamic scene graph-guided Chain-of-Thought (CoT) reasoning. The methodology is characterized by the following process:

1.  **Knowledge Representation:** Explicitly constructing structured knowledge representations through dynamic scene graphs to capture the current state of the environment.
2.  **Guided Reasoning:** These graphs guide the CoT reasoning process, ensuring that reasoning steps are aligned with actionable environmental dynamics.
3.  **Evaluation:** Performance is rigorously evaluated using the newly introduced **eSpatial-Benchmark**, a dataset characterized by:
    *   Real-world embodied scenarios
    *   Fine-grained spatial annotations
    *   Adaptive task difficulty levels

---

## Technical Details

EmbodiedVSR utilizes a Dynamic Scene Graph-Guided Chain-of-Thought (CoT) Reasoning framework to construct structured knowledge representations for disentangling spatial relationships.

### System Architecture

| Module | Functionality |
| :--- | :--- |
| **Task Generator** | Handles occupancy scanning and sequence generation. |
| **Visual Perception Front-End** | Features **Vertex** and **Visual Aid Extractors**; utilizes Depth Estimation and Detection technologies. |
| **Scene Graph Generator** | Builds dynamic graphs containing objects and scaleless spatial relations using vector notation. |
| **Interactor** | Features Servo Actuation and Voice/Text I/O capabilities. |

### Operational Characteristics
*   **Zero-Shot Capability:** Operates without task-specific fine-tuning.
*   **Iterative Interaction:** Designed for long-horizon tasks requiring continuous environmental feedback.

---

## Results

EmbodiedVSR significantly outperforms existing Multimodal Large Language Model (MLLM)-based methods in both Accuracy (higher success rate) and Reasoning Coherence.

*   **Accuracy:** Achieved **84.6%** (vs. GPT-4V at 58.8% and LLaVA at 45.3%).
*   **Reasoning Coherence:** Achieved **82.4%** (vs. GPT-4V at 55.3%).
*   **Spatial Relations:** Successfully handles complex spatial reasoning involving:
    *   **Static relations:** e.g., "under", "beside"
    *   **Complex/Contact relations:** e.g., "leaning against", "stuck inside"
*   **Task Proficiency:** Demonstrated strength in long-horizon tasks, such as iterative Pick and Place operations, effectively disentangling intricate spatial relationships within complex scenarios.

---

## Contributions

*   **Framework Introduction:** Introduction of **EmbodiedVSR**, a new framework that bridges the gap in MLLM capabilities regarding spatial reasoning for complex, long-horizon embodied tasks.
*   **Reasoning Mechanism:** A structured reasoning mechanism that enhances spatial understanding by combining dynamic scene graphs with CoT reasoning, facilitating explainable and structured reasoning processes.
*   **Benchmark Release:** The release of the **eSpatial-Benchmark**, a comprehensive dataset designed to test fine-grained spatial reasoning in adaptive, real-world scenarios.
*   **Efficiency Validation:** Demonstration that task-specific fine-tuning is not strictly necessary for high-performance spatial reasoning when structured representations are employed.