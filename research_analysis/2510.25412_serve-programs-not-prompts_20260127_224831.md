---
title: Serve Programs, Not Prompts
arxiv_id: '2510.25412'
source_url: https://arxiv.org/abs/2510.25412
generated_at: '2026-01-27T22:48:31'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Serve Programs, Not Prompts

*In Gim, Lin Zhong*  
*Yale University, New Haven*

---

> ### ðŸ“Œ Quick Facts
> 
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Performance:** Up to **7x** greater throughput than vLLM
> *   **Key Metric:** GPT-4 token distributions are approx. **200 KB**
> *   **Core Concept:** Shift from stateless prompts to stateful programs (LIPs)

---

## Executive Summary

Current Large Language Model (LLM) serving architectures are fundamentally inefficient and inflexible because they treat every request as a stateless text prompt. This approach limits users' ability to customize token prediction or manage the internal state of the model, specifically the Key-Value (KV) cache. Furthermore, in complex applications requiring external tool usage or database queries, traditional systems force application logic to reside on the client side. This separation necessitates multiple client-server round trips, significantly increasing latency and making the orchestration of complex workflows cumbersome.

The authors propose a paradigm shift from serving text prompts to serving "LLM Inference Programs" (LIPs), which they implement through a system called **Symphony**. Symphony applies operating system principles to LLM serving, treating model computations as system calls and managing the KV cache via a virtualized file system (VFS). This architecture allows users to explicitly manage state and co-locate application logicâ€”such as tool executionâ€”directly on the server. To maintain hardware efficiency while offering this flexibility, Symphony employs a two-level process scheduling strategy that decouples the scheduling of program execution from the management of underlying GPU resources.

Symphony demonstrates substantial performance improvements over existing state-of-the-art baselines like vLLM, achieving up to **7 times greater throughput**. These gains are largely attributed to Symphonyâ€™s ability to implement custom KV cache replacement policies that significantly outperform generic Least Recently Used (LRU) systems. Additionally, the architecture addresses the inefficiency of handling large token distributions (approximately 200 KB for models like GPT-4), which are impractical to manipulate client-side. By co-locating function execution on the server, Symphony also marks a significant reduction in latency for LLM workflows involving function calling.

This research redefines the standard abstraction for LLM deployment, moving the industry from a text-inference model to a program-execution model. By virtualizing LLM resources and granting users runtime control over inference and memory management, Symphony establishes a new layer of extensibility for the LLM ecosystem. This "OS for inference" approach enables the development of more complex, stateful applications that were previously difficult or inefficient to deploy, signaling a significant evolution in how large models are integrated into software infrastructure.

---

## Key Findings

*   **Inefficiency in Current Systems:** Existing LLM serving systems lack the flexibility required for complex, stateful applications.
*   **LLM Inference Programs (LIPs):** Serving LIPs instead of simple prompts allows users to customize token prediction and manage the KV cache directly at runtime.
*   **Logic Offloading:** The new architecture enables the offloading of application logic (such as tool execution) from the client side to the server side.
*   **Symphony OS:** Symphony functions as an operating system for LIPs, utilizing system calls for computation and a virtualized file system for resource management.
*   **High GPU Efficiency:** A novel two-level process scheduling scheme ensures flexible program execution without compromising the underlying GPU hardware efficiency.

---

## Technical Details

The architecture introduces **LLM Inference Programs (LIPs)**, shifting the paradigm from stateless prompts to stateful programs managed by the **Symphony OS**.

### Core Components

| Feature | Description |
| :--- | :--- |
| **Virtualized File System (VFS)** | Manages the KV cache as files, allowing for explicit manipulation and custom replacement policies. |
| **System Calls** | Abstracts model computation, treating it as a kernel-level operation rather than a simple request. |
| **Two-Level Process Scheduling** | Decouples the scheduling of program execution from the management of underlying GPU resources. |

### API Design Principles

*   **Decoupling:** Separates generation logic from computation details.
*   **Explicit State:** Application-managed state (KV cache) gives users fine-grained control.
*   **Co-location:** External interactions (e.g., function calls) are co-located on the server to eliminate client-server round trips.

---

## Methodology

The authors propose a fundamental shift from a prompt-based to a program-based paradigm, introducing **LLM Inference Programs (LIPs)** as the primary unit of service. 

*   **System Design:** They designed **Symphony**, a system that adopts Operating System principles. It treats model computations as system calls and manages the KV cache through a virtualized file system.
*   **Scheduling Strategy:** A two-level process scheduling strategy was implemented to ensure flexible program execution while maintaining high hardware efficiency.

---

## Research Contributions

1.  **New Architectural Paradigm:** Establishment of a new standard moving from serving text prompts to serving executable programs (LIPs).
2.  **Symphony System:** Presentation of a system architecture that virtualizes LLM resources and computations, effectively acting as an operating system for inference.
3.  **Extensibility Framework:** Contribution of a framework that grants users runtime control over inference and application logic, significantly enhancing the extensibility of the LLM application ecosystem.

---

## Performance Results

*   **Throughput:** Symphony achieved up to **7x greater throughput** compared to the state-of-the-art vLLM baseline.
*   **Cache Management:** Custom KV cache replacement policies implemented in Symphony significantly outperform generic LRU systems.
*   **Token Distribution Handling:** The study identified that token distributions for models like GPT-4 are approximately **200 KB**, rendering direct client-side manipulation impractical.
*   **Latency Reduction:** By co-locating execution on the server side, Symphony significantly reduces latency in LLM function calling workflows.