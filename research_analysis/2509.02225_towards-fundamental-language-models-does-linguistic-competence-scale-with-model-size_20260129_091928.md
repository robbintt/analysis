# Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?

*Jaime Collado-Monta침ez; L. Alfonso Ure침a-L칩pez; Arturo Montejo-R치ez*

---

> ### 游늵 Quick Facts
>
> *   **Models Analyzed:** SmolLM2, Qwen2.5, Llama-3, OLMo-2, Falcon3, Gemma-2, Yi-1.5
> *   **Parameter Range:** 135 Million to 32 Billion
> *   **Evaluation Framework:** CEFR (Lexical, Grammatical, Semantic)
> *   **Quality Score:** 9/10
> *   **Citations:** 19

---

## Executive Summary

### 游댮 Problem
Current Large Language Model (LLM) development is dominated by the pursuit of monolithic architectures that couple linguistic competence with factual memorization. This paper addresses the critical question of whether fundamental linguistic ability scales at the same rate as factual knowledge, or if simply increasing model size merely optimizes for memorization rather than processing capability. This distinction is vital because the conflation of these skills leads to inefficient resource consumption, heightened risks of hallucinations, and privacy concerns, necessitating a re-evaluation of scaling laws to achieve more sustainable and reliable Natural Language Processing (NLP) solutions.

### 游 Innovation
The authors introduce the **Fundamental Language Model (FLM)** paradigm, a conceptual framework designed to decouple linguistic competence from factual retrieval. Technically, the study isolates linguistic ability using the Common European Framework of Reference for Languages (CEFR) to assess Lexical, Grammatical, and Semantic capabilities, strictly separating them from factual knowledge. The methodology categorizes benchmarks into three distinct groups: Linguistic (e.g., WiC, BLiMP, RTE), External Factual (e.g., LAMBADA, BoolQ), and Internal Factual (e.g., TriviaQA, TruthfulQA). This approach allows for the empirical validation of a modular architecture where smaller, linguistically specialized models handle language processing while external tools manage data retrieval.

### 游늳 Results
Evaluating models ranging from 135 million to 32 billion parameters across families like SmolLM2, Qwen2.5, and Llama-3, the study demonstrated that internal factual knowledge scales significantly more aggressively than linguistic competence. In linguistic tasks, the largest model, Qwen2.5-32B, achieved the top score of **0.7688**, yet the smaller Qwen2.5-3B secured a competitive **0.6909**, closely trailing the larger OLMo-2-32B (**0.7095**). Conversely, external factual knowledge saw mid-sized models excel, with Gemma-2-9B (**0.7961**) outperforming Qwen2.5-32B (**0.7007**). Internal factual memorization proved highly dependent on scale, with OLMo-2-32B (**0.5784**) significantly surpassing its 13B variant (**0.4799**).

### 游깴 Impact
The research significantly challenges the industry's reliance on monolithic scaling by confirming that factual memorization, not core linguistic skill, is the primary driver of size-related performance gains. This validates the viability of the FLM paradigm, advocating for a shift toward modular systems that reduce model size and computational demands. By decoupling language processing from fact retrieval, this approach offers a pathway to mitigate hallucinations, enhance interpretability, and improve the environmental sustainability of NLP technologies, fundamentally altering how future language models should be architected and trained.

---

## Key Findings

*   **Differential Scaling:** Linguistic competence and factual knowledge improve with model scale, but internal factual knowledge grows at a significantly faster rate than linguistic competence.
*   **Size vs. Memorization:** Model size is more closely correlated with the capacity for factual memorization than with core linguistic ability.
*   **Viability of Modularity:** Results empirically support a modular approach to language modeling where distinct components handle language processing and factual retrieval.

---

## Methodology

The study conducted an empirical analysis of models ranging widely in size, from **135 million to 32 billion parameters**. Models were evaluated across three specific dimensions to isolate different capabilities:

1.  **Linguistic Competence**
2.  **External Factual Knowledge**
3.  **Internal Factual Knowledge**

---

## Technical Details

### Proposal & Framework
*   **Concept:** Fundamental Language Models (FLMs) are proposed to retain linguistic competence while delegating factual retrieval to external sources.
*   **Goals:** Reduce model size, mitigate bias, and improve accuracy.
*   **Theoretical Basis:** Utilizes the **Common European Framework of Reference for Languages (CEFR)** to isolate linguistic competence (Lexical, Grammatical, Semantic) from factual knowledge.

### Benchmark Categorization
*   **Linguistic Benchmarks:** WiC, BLiMP, RTE, MNLI, QQP.
*   **External Factual Benchmarks:** LAMBADA, BoolQ, COPA.
*   **Internal Factual Benchmarks:** TriviaQA, TruthfulQA.

### Experimental Setup
*   **Models Evaluated:** SmolLM2, Qwen2.5, Llama-3, OLMo-2, Falcon3, Gemma-2, Yi-1.5.
*   **Scale Range:** 135M to 32B parameters.
*   **Tools:** LM Evaluation Harness.
*   **Configuration:** Zero-shot setting on two NVIDIA Ampere A100 GPUs.

---

## Results

#### General Trend
Larger models generally yielded better results, with internal factual knowledge scaling more aggressively than linguistic competence.

#### 游댟 Linguistic Competence
Top performers demonstrated high capability, with surprising efficiency from smaller models.
*   **Top Performer:** Qwen2.5-32B (**0.7688**)
*   **Notable Efficiency:** Qwen2.5-3B achieved **0.6909**, showing stability comparable to the much larger OLMo-2-0325-32B (**0.7095**).

#### 游댌 External Factual Knowledge
Mid-sized models often outperformed the largest parameter models in this category.
*   **Top Performer:** Gemma-2-9B (**0.7961**)
*   **Runner Up:** Falcon3-10B (**0.7746**)
*   **Comparison:** Both outperformed Qwen2.5-32B (**0.7007**).

#### 游 Internal Factual Knowledge
This category showed the highest dependence on model size and memorization capacity.
*   **Top Performer:** OLMo-2-0325-32B (**0.5784**)
*   **Scale Dependency:** The 32B model significantly surpassed the 13B version (**0.4799**), indicating that memorization capacity is highly size-dependent.

---

## Contributions

1.  **Introduction of the FLM Paradigm:** Proposes the 'Fundamental Language Model' (FLM) paradigm, advocating for decoupling linguistic competence from factual memorization.
2.  **Architectural Proposal:** Suggests shifting from monolithic models to smaller, linguistically competent models augmented by external tools for factual retrieval.
3.  **Advancement of NLP Sustainability:** Presents the FLM paradigm as a pathway to mitigate hallucinations, privacy concerns, and high computational costs, offering more efficient, interpretable, and sustainable NLP solutions.