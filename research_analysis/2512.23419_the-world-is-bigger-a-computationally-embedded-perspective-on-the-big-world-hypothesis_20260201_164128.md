# The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis
*Alex Lewandowski; Adtiya A. Ramesh; Edan Meyer; Dale Schuurmans; Marlos C. Machado*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Core Concept** | Computationally-Embedded Perspective |
| **Primary Metric** | Interactivity |
| **Key Equivalence** | Embedded Agent $\approx$ POMDP Policy |

---

## Executive Summary

This paper addresses the theoretical and practical hurdles of the **Big World Hypothesis**, which posits that agents operate within environments far exceeding their internal computational resources. Unlike traditional reinforcement learning frameworks that treat agents as external observers or rely on *ad hoc* regularization, this work formally models the environment as an **Algorithmic Markov Process** with a countable state-space and polynomial-time computable transitions. This distinction is critical because it captures the physical reality that an agent is fundamentally embedded within its environmentâ€”a universal computerâ€”creating a setting where the agent is a part of the system rather than a distinct entity. The authors formalize the agent as an **Embedded Automaton**, arguing that this shift from static policy convergence to continual adaptation is necessary to bridge the gap between theoretical models and real-world applications where environmental complexity is effectively infinite.

The key technical innovation is the "computationally-embedded perspective," which establishes a rigorous isomorphism between an embedded agent and a stateful policy operating within a **Partially Observable Markov Decision Process (POMDP)**. The authors characterize the environment through **Uniform Locality**, where transitions depend on local substates and boundary-spaces, and define the agent via input, output, and internal state spaces. Crucially, they propose **"interactivity"** as a novel objective function, measuring an agent's ability to continually learn new predictions and adapt behaviors over time. To optimize this, a specific model-based reinforcement learning algorithm is developed to maximize interactivity, providing a mathematical foundation for intrinsic motivation that arises naturally from the constraints of the agent's computational embedding.

The study yields significant results grounded in specific metrics, including **Algorithmic Complexity** ($K_U(x|y)$) and internal state-space capacity. Theoretically, the authors prove that the finite internal state space of an embedded agent acts as a natural bottleneck, creating inherent capacity constraints without external regularization. Empirically, evaluations on synthetic continual learning problems demonstrated a distinct architectural performance divergence: while deep nonlinear networks struggled to sustain high levels of interactivity, **deep linear networks** successfully maintained and increased interactivity as their capacity increased. These findings quantify the limitations of current architectures, showing that computational embedding naturally forces adaptation due to the environment's superior capacity.

---

## Key Findings

*   **Theoretical Equivalence:** An agent modeled as an automaton within a universal computer is theoretically equivalent to an agent operating in a POMDP over a countably infinite state-space.
*   **Architectural Performance:** Deep nonlinear networks struggle to sustain "interactivity," whereas deep linear networks are capable of sustaining higher levels of interactivity as their capacity increases.
*   **Constraint Nature:** Computational embedding inherently constrains an agent regardless of its capacity, creating a natural setting for continual learning without the need for explicit, *ad hoc* capacity constraints.

---

## Contributions

*   **Problem Formulation:** A novel characterization of the "big world hypothesis" where constraints arise naturally from an agent being embedded in the environment.
*   **New Objective Function:** Introduction of **"interactivity"** as a formal objective for continual learning, shifting focus from converging to a fixed solution to maintaining adaptive capacity.
*   **Theoretical Insight:** A proof linking the computationally-embedded agent framework to POMDPs with countably infinite state-spaces, providing a rigorous mathematical foundation.

---

## Methodology

The research employs a multi-step approach to formalize and test the embedded agent hypothesis:

1.  **Formal Modeling:**
    The authors define a "computationally-embedded perspective" by representing an embedded agent as an automaton simulated within a universal formal computer.

2.  **Objective Definition:**
    They propose **"interactivity,"** a metric designed to measure an agent's ability to continually adapt behavior by learning new predictions.

3.  **Algorithm Development:**
    A model-based reinforcement learning algorithm is developed specifically to maximize this interactivity objective.

4.  **Evaluation:**
    The method is assessed using a constructed synthetic problem designed to test continual learning capabilities across different network architectures.

---

## Technical Details

This section outlines the specific mathematical frameworks and definitions used in the paper.

**Framework Definition**
*   **Core Concept:** Computationally-embedded agents.
*   **Environment:** Modeled as an **Algorithmic Markov Process** featuring a countable state-space and polynomial-time computable transitions.

**Environment Properties**
*   **Uniform Locality:** Transitions depend on local substates and boundary-spaces.
*   **Isomorphism:** Transitions are isomorphic across indices (e.g., Conway's Game of Life).

**Agent Formalization**
*   **Model:** The agent is formalized as an **Embedded Automaton**.
*   **Components:** Defined by input spaces, output spaces, internal state spaces, a policy function, and an update function.
*   **Interface:** The agent-environment interface is established when the automaton's boundary space coincides with its input space.

---

## Results

The study's results combine theoretical proofs with empirical observations on architectural limitations.

*   **Metrics:** Theoretical metrics utilized include Algorithmic Complexity ($K_U(x|y)$) and Capacity Constraints (defined by internal state space size).
*   **Equivalence Proof:** Key findings establish that an embedded agent is formally equivalent to a stateful policy in a Partially Observable Markov Decision Process (POMDP).
*   **Capacity Constraints:** The framework demonstrates that finite internal states impose inherent capacity constraints on behaviors.
*   **Architectural Divergence:**
    *   *Deep Nonlinear Networks:* Struggled to sustain interactivity.
    *   *Deep Linear Networks:* Sustained higher levels of interactivity as capacity increased.
*   **Natural Continual Learning:** The structure creates a natural setting for continual learning due to the environment's greater capacity forcing adaptation.