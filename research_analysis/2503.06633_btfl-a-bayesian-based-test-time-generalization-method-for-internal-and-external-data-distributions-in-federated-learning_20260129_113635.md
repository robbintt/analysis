---
title: 'BTFL: A Bayesian-based Test-Time Generalization Method for Internal and External
  Data Distributions in Federated learning'
arxiv_id: '2503.06633'
source_url: https://arxiv.org/abs/2503.06633
generated_at: '2026-01-29T11:36:35'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# BTFL: A Bayesian-based Test-Time Generalization Method for Internal and External Data Distributions in Federated learning

*Yu Zhou, Bingyan Liu, Beijing University*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Paper Quality** | 8/10 |
| **Core Focus** | Federated Learning, OOD Robustness, Test-Time Adaptation |
| **Key Architecture** | Two-Head, Dual-Bayesian Framework |
| **Primary Dataset** | CIFAR-10 |
| **Performance Gain** | Up to **+8.68%** over baselines in Shifted EXD scenarios |

---

> ### üí° Executive Summary
>
> Personalized Federated Learning (PFL) has traditionally focused on optimizing the balance between generalization and personalization exclusively during the training phase. However, this approach faces a critical limitation: existing PFL methods suffer significant performance degradation when encountering Out-Of-Distribution (OOD) samples during the testing phase.
>
> This research addresses the challenge of handling both **Internal (IND)** and **External (EXD)** data distributions** in real-world scenarios where test data frequently deviates from training distributions. The authors propose **BTFL (Bayesian-based Test-time Generalization)**, a novel method designed for the newly defined **Test-time Generalization for Internal and External Distributions in Federated Learning (TGFL)** scenario.
>
> Technically, BTFL utilizes a **Two-Head Architecture** that decouples the storage of global knowledge ($w_g$) from local personalization ($w_p$). The core innovation is a **Dual-Bayesian Framework** that employs Discretised-Likelihood Estimators (DLE) with Finite Scalar Quantization (FSQ) to compress continuous features into a binary discrete space $\{0, 1\}^d$. This compression stabilizes the adaptation process by mitigating the noise and instability associated with high-dimensional cosine similarity metrics used in prior work.
>
> Experimental evaluations on **CIFAR-10** demonstrate that BTFL achieves superior performance while requiring less computation time. At batch size 1, BTFL achieved **80.00% accuracy** in Shifted IND scenarios compared to the baseline‚Äôs 74.14%, and **48.97%** in Shifted EXD scenarios versus the baseline's 40.29%.

---

## üîç Key Findings

*   **Test-Time Limitation Identified:** Existing Personalized Federated Learning (PFL) methods suffer performance degradation with Out-Of-Distribution (OOD) samples during testing because they focus on generalization only during training.
*   **Effective Trade-off Resolution:** The proposed BTFL method effectively resolves the trade-off between personalization and generalization at the sample level during the **testing phase**.
*   **Dual Distribution Handling:** The method successfully handles both **Internal (IND)** and **External (EXD)** distributions by leveraging historical test data alongside current sample characteristics.
*   **Superior Efficiency:** BTFL achieves superior performance across various datasets and models compared to existing baselines while requiring **less computation time**.

---

## üõ†Ô∏è Methodology

The researchers introduce BTFL, a Bayesian-based test-time generalization method designed for the newly defined **TGFL scenario**. The approach consists of three core components:

1.  **Two-Head Architecture**
    *   Decouples the storage of local and global knowledge.
    *   Allows for the separate maintenance of personalization and generalization.
2.  **Dual-Bayesian Framework**
    *   Implements inference mechanisms that dynamically weigh predictions.
    *   Bases weighing on historical test data trends and current sample characteristics.
3.  **Sample-Level Optimization**
    *   Performs balancing of generalization and personalization specifically during online testing.

---

## ‚öôÔ∏è Technical Details

The paper proposes BTFL (Bayesian-based Test-time Generalization) for the TGFL scenario, which handles both Internal (IND) and External (EXD) data distributions.

**Architecture Components:**
*   **Dual-Head Setup:**
    *   **Global Head ($w_g$):** Stores general knowledge.
    *   **Personal Head ($w_p$):** Stores local personalization.
    *   Utilizes output-space interpolation via a Bayesian framework.

**Core Mechanics:**
*   **Discretised-Likelihood Estimators (DLE) with Finite Scalar Quantization (FSQ):**
    *   Compresses continuous features into a binary discrete space $\{0, 1\}^d$.
    *   **Local DLE:** Preserves zero-frequencies for local likelihoods.
    *   **Global DLE (server-averaged):** Estimates EXD likelihoods.

**Differentiation from Baselines:**
*   Unlike FedTHE, BTFL avoids unstable dynamic weighting and cosine similarity metrics in high dimensions.
*   Uses historical test data and current sample characteristics to guide adaptation, ensuring stability.

---

## üìà Results

Experimental evaluation on **CIFAR-10** demonstrated BTFL's superiority over the state-of-the-art baseline, **FedTHE**, particularly in robustness to batch size variations.

**Performance at Batch Size 1:**

| Scenario | FedTHE Accuracy | BTFL Accuracy | Outcome |
| :--- | :--- | :--- | :--- |
| **Shifted IND** | 74.14% | **80.00%** | BTFL Improvement |
| **Shifted EXD** | 40.29% | **48.97%** | BTFL Improvement |
| **Original EXD** | *High Drop* | *Stability* | FedTHE dropped 15.11% due to EMA noise |

**Key Observations:**
*   **Robustness:** While FedTHE suffered significant performance drops at batch size 1 due to EMA noise, BTFL maintained or improved performance (e.g., a **0.26% improvement** in Shifted IND).
*   **Consistency:** BTFL consistently outperformed FedTHE across all distribution types (Original/Shifted IND/EXD).
*   **Stability:** The study noted that BTFL avoids the metric stability issues associated with FedTHE's dynamic interpolation weight ($\lambda_s$).

---

## üìù Contributions

1.  **New Scenario Definition:** Introduced the TGFL (Test-time Generalization for Internal and External Distributions in Federated Learning) scenario, expanding the scope of federated learning evaluation.
2.  **Algorithmic Innovation:** Developed the BTFL algorithm to address the test-time trade-off between personalization and generalization, filling a gap left by previous training-focused methods.
3.  **Validation & Theory:** Provided theoretical guarantees for the method alongside empirical evidence demonstrating that BTFL offers faster speed and better accuracy than current state-of-the-art approaches.

---
**References:** 40 citations