# Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone

*Negar Hassanpour; Muhammad Kamran Janjua; Kunlin Zhang; Sepehr Lavasani; Xiaowen Zhang; Chunhua Zhou; Chao Gao*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total Citations** | 40 |
| **Core Innovation** | Conic Geometry for MTL Optimization |
| **Key Sectors** | Supervised Learning, Reinforcement Learning |
| **Scalability** | Efficient up to 34.41M parameters |

---

## Executive Summary

Multi-Task Learning (MTL) strives to improve generalization and efficiency by learning shared representations across multiple tasks, but it is frequently hindered by **"gradient conflicts."** This phenomenon occurs when the gradient updates required for different tasks point in opposing directions, causing them to interfere negatively with one another. Addressing this is critical because standard gradient averaging often leads to poor convergence where multi-task models underperform compared to single-task baselines.

This paper addresses the fundamental optimization challenge of dynamically regulating update directions to resolve these conflicts without incurring prohibitive computational costs. The authors introduce **ConicGrad**, a novel MTL algorithm that formulates the optimization problem through the lens of Conic Geometry.

The key innovation lies in replacing traditional Euclidean distance constraintsâ€”which can be overly restrictiveâ€”with a dynamic angular constraint strategy. ConicGrad calculates a reference gradient ($g_0$), defined as the uniform average of all task-specific gradients, and confines the final update vector within a cone centered on this reference. Specifically, the method requires the update vector $d$ to form an angle with $g_0$ that does not exceed a threshold $c$. By utilizing Lagrange multipliers to incorporate this conic constraint into the regularization of distance and magnitude, ConicGrad allows for a wider range of feasible update directions and magnitudes than previous methods like CAGrad, ensuring updates remain aligned with the overall objective.

This research is significant for providing a mathematically grounded and robust solution to the pervasive issue of conflicting gradients in MTL. By introducing geometric gradient regulation via conic constraints, the paper offers a framework that balances competing task objectives without excessively restricting their direction or magnitude. The demonstrated scalability to high-dimensional parameter spaces and versatility in both supervised and reinforcement learning settings suggests that ConicGrad provides a practical, efficient path toward training more capable multi-task models.

---

## Key Findings

*   **Conflict Resolution:** ConicGrad successfully resolves inter-task gradient conflicts by dynamically regulating update directions.
*   **State-of-the-Art Performance:** The method achieves superior performance compared to existing approaches across extensive benchmarks in both standard supervised learning and reinforcement learning.
*   **Scalability:** The framework demonstrates computational efficiency and the ability to scale effectively to high-dimensional parameter spaces (up to 34.41M parameters).
*   **Balanced Optimization:** The approach balances competing task-specific gradients without excessively restricting their direction or magnitude.

---

## Methodology

The authors formulate Multi-Task Learning (MTL) as a constrained optimization problem to dynamically handle competing objectives. They introduce an **angular constraint strategy** that confines gradient update directions and utilizes **Conic Geometry**, restricting updates within a cone centered on the reference gradient of the overall objective.

This differs from traditional Euclidean distance constraints by allowing more flexibility in the magnitude of the update while strictly adhering to the angular alignment with the aggregate objective.

---

## Technical Details

### Core Concept
ConicGrad is designed to resolve inter-task gradient conflicts by dynamically regulating update directions.
*   **Reference Gradient ($g_0$):** Defined as the uniform average of all task-specific gradients.
*   **Constraint Type:** Unlike Euclidean constraints (e.g., CAGrad), ConicGrad applies a **conic angular constraint**.

### Mathematical Formulation
The update vector $d$ must form an angle with the reference gradient $g_0$ that does not exceed a threshold determined by hyperparameter $c$.
*   **Angular Constraint:** $\text{angle}(d, g_0) \le c$
*   **Optimization:** This formulation allows for a broader range of feasible directions and update magnitudes while maintaining alignment with the mean gradient.
*   **Regularization:** The optimization formulation involves a Lagrange multiplier and a regularization coefficient, explicitly incorporating the conic constraint parameter $c$ into the regularization of distance and magnitude.

---

## Results & Performance

Evaluation metrics included **Average Performance Drop relative to Single Task Learning ($m\Delta$)** and **Mean Rank (MR)**.

*   **2-Task Toy Experiment:**
    *   ConicGrad reached global minima for **all 5 initialization points**.
    *   Outperformed NashMTL (3/5).
    *   Matched FAMO's reach to the Pareto front but achieved the lowest loss **significantly faster**.
*   **Scalability Analysis (CelebA - 40 tasks):**
    *   ConicGrad maintained computational efficiency across model sizes ranging from **5.2M to 34.41M parameters**.
    *   Comparable method SDMGrad experienced significant slowdowns at higher scales.
*   **Hyperparameter Ablation:**
    *   **CityScapes:** Preferred small regularization coefficients and smaller $c$ (approx. 0.25).
    *   **CelebA & NYUv2:** Preferred $c \ge 0.5$.

---

## Core Contributions

*   **Algorithm Introduction:** Introduction of **ConicGrad**, a novel, principled, and robust MTL algorithm providing a mathematically grounded solution to conflicting gradients.
*   **Geometric Innovation:** Contribution of geometric gradient regulation using angular constraints and cone-based geometry.
*   **Comprehensive Validation:** Rigorous testing of the method's versatility across supervised and reinforcement learning settings.