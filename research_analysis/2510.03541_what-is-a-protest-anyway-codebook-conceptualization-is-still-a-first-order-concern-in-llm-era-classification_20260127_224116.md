---
title: What is a protest anyway? Codebook conceptualization is still a first-order
  concern in LLM-era classification
arxiv_id: '2510.03541'
source_url: https://arxiv.org/abs/2510.03541
generated_at: '2026-01-27T22:41:16'
quality_score: 8
citation_count: 25
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification

*Katherine A. Keith, Michigan State, Andrew Halterman, Williams College*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 25
> *   **Cost Efficiency:** ~$10 vs. 1.5 years of manual labor
> *   **Literature Risk:** 88% of current papers recommend the risky "Optimist" approach
> *   **Performance Threshold:** 100 human annotations shown to outperform 100,000 uncorrected LLM annotations

---

## Executive Summary

The widespread availability of Large Language Models (LLMs) has introduced a dangerous workflow gap in Computational Social Science (CSS), where analysts are often tempted to bypass rigorous codebook conceptualization in favor of speed. This paper addresses the critical issue of "conceptualization errors"â€”systematic mismatches between an analyst's intent and the model's definitionâ€”which threaten empirical validity. Unlike execution mistakes, these misalignments occur before the model is applied and introduce systemic bias into downstream statistical inferences. The authors argue that high accuracy metrics are misleading if the underlying concept is poorly defined; a model can be highly accurate yet measure the wrong construct entirely, rendering subsequent statistical estimates unreliable.

The key technical innovation is a taxonomy distinguishing Conceptualization Error from Operationalization Error, alongside a "Pragmatist" framework grounded in Prediction-Powered Inference (PPI). Rather than relying on naive zero-shot prompts or exhaustive manual annotation, this approach introduces a hybrid workflow that combines a small, high-quality set of human annotations with LLM predictions across the full corpus. Technically, the method uses this human subset to "anchor" or "calibrate" the model. It calculates a statistical rectifier based on the divergence between the human labels and LLM predictions, allowing researchers to adjust for the model's conceptual misalignment while retaining the scalability benefits of automation.

Simulation results demonstrate that naive LLM usage yields biased estimates that cannot be rectified simply by increasing model accuracy or applying standard post-hoc corrections. This resilience exists because the bias is structural: the model is accurately predicting a different variable than the researcher intends, so increasing technical proficiency only reinforces the incorrect measurement. Conversely, the hybrid approach successfully ensures valid statistical coverage and reduces variance, resulting in narrower confidence intervals. The efficiency gains are substantial, lowering costs to approximately $10 compared to the 1.5 years of labor required for traditional manual methods. Notably, a literature review reveals that 88% of current papers rely on the risky uncalibrated approach, despite evidence that 100 well-defined human annotations can outperform 100,000 uncorrected LLM annotations in terms of valid inference.

This paper significantly influences the field of CSS by re-establishing conceptualization as a "first-order concern" regardless of advances in model capability. It serves as a corrective to the current trend of treating LLMs as autonomous oracles, proving that scale cannot substitute for precise definitional frameworks. By providing a calibrated methodology that guarantees low-bias estimates at a fraction of the traditional cost, the authors offer a pragmatic path forward that integrates human-in-the-loop validation. This work compels the discipline to ensure that statistical conclusions derived from text data remain scientifically robust by rigorously defining the constructs being measured.

---

## Key Findings

*   **The Conceptualization Trap:** LLMs create a temptation for analysts to skip the critical conceptualization step, leading to "conceptualization errors."
*   **Systematic Bias:** These conceptualization errors systematically bias downstream statistical estimates in computational social science (CSS).
*   **Irreducible Bias:** Simulations demonstrate that biases resulting from poor conceptualization cannot be rectified simply by increasing LLM accuracy or applying post-hoc bias correction methods.
*   **First-Order Priority:** Conceptualization remains a "first-order concern" that requires direct attention to ensure unbiased, low-variance estimates, regardless of the model's technical performance.

---

## Methodology

The authors employ **simulations** to model the impact of conceptualization errors on downstream statistical inference. This analytical approach allows them to:

1.  **Isolate Effects:** Separate the specific effect of conceptualization bias from other sources of error.
2.  **Test Mitigation:** Evaluate the efficacy of common mitigation strategiesâ€”such as increasing model accuracy or applying post-hoc correctionsâ€”under controlled conditions.
3.  **Validate Frameworks:** Compare the outcomes of different annotation workflow strategies.

---

## Technical Details

**Error Taxonomy**
The paper introduces a critical distinction between two types of errors:
*   **Conceptualization Error:** Systematic mismatches between the researcher's codebook and the model's understanding.
*   **Operationalization Error:** Mistakes made during the execution of the classification.

**Comparative Frameworks**
The study contrasts three primary approaches:
1.  **The Pessimist:** Relies on traditional manual annotation (high accuracy, high cost).
2.  **The Optimist:** Uses naive Zero-Shot LLM prompting (low cost, high risk of bias).
3.  **The Pragmatist:** Utilizes **Prediction-Powered Inference (PPI)**.

**The Pragmatist Workflow (Algorithm 1)**
The proposed solution is a hybrid method comprising three distinct stages:
1.  **Conceptualization:** Defining the construct.
2.  **Operationalization:** Generating labels.
3.  **Statistical Inference:** Calculating estimates.

**Technical Mechanism**
The Pragmatist approach combines a small set of human labels with LLM predictions on the full corpus. It calculates a rectifier ($\hat{\Delta}$) to correct LLM bias based on the divergence between the human "anchor" labels and the machine predictions.

---

## Results

*   **Optimist Failure:** The "Optimist" approach (naive LLM usage) produces biased estimates due to conceptualization and operationalization errors. This bias persists even with improvements in LLM accuracy.
*   **Pragmatist Success:** The "Pragmatist" method ensures valid statistical coverage and reduces variance, leading to narrower confidence intervals.
*   **Cost Efficiency:** The Pragmatist method significantly lowers costs to approximately **$10**, compared to **1.5 years** of labor required for traditional manual methods.
*   **Literature Gap:** A review of current literature found that **88%** of papers recommend the risky "Optimist" approach.
*   **Human vs. Scale:** Evidence suggests that 100 well-defined human annotations can outperform 100,000 uncorrected LLM annotations.

---

## Contributions

*   **Identification of a Workflow Gap:** Highlights a significant oversight in current LLM-era CSS regarding the neglect of pre-prompting conceptualization and post-prompting inference.
*   **Technical Proof of Bias Resilience:** Establishes that conceptualization-induced bias is a distinct problem that persists independently of LLM accuracy metrics or standard statistical correction techniques.
*   **Practical Guidance:** Provides concrete advice for analysts on how to maintain rigorous conceptual standards while leveraging the low-cost benefits of LLMs for text classification.