# Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts

*Zehan Li; Ruhua Pan; Xinyu Pi*

---

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Quality Score:** 7/10
> *   **References:** 30 Citations
> *   **Dataset Size:** 750 annotated sentences (23 short stories/novel chapters)
> *   **Models Outperformed:** GPT-4o, Claude 3.5
> *   **Core Tech:** RoBERTa, XGBoost, LangChain
> *   **Key Innovation:** The "Expert Index" (7 linguistic features)

---

## Executive Summary

Generating accurate causal graphs from narrative texts presents a significant challenge in natural language processing, as narratives involve complex, multi-layered event sequences that confound standard extraction methods. While Large Language Models (LLMs) are the default for semantic understanding, they frequently lack the precision required for fine-grained causal analysis; they tend to hallucinate relationships or miss the nuanced, long-range dependencies inherent in storytelling.

This paper addresses the limitations of pure LLM approaches by establishing the need for systems that bridge high-level narrative causality with specific, event-level logical chains. The authors propose a hybrid "End-to-End" architecture that integrates transformer-based models with a linguistically informed feature set called the **"Expert Index."**

The pipeline utilizes a four-stage process:
1.  **LLM-based Summarization:** Extracting concise, agent-centered vertices.
2.  **Linguistic Feature Extraction:** Using RoBERTa to identify seven distinct linguistic features (192 unique combinations).
3.  **Classification:** XGBoost categorizes vertices into a Situation-Task-Action-Consequence (STAC) model.
4.  **Graph Construction:** A five-iteration prompting process builds the final connected causal graphs.

Comparative experiments demonstrated that the framework consistently outperformed state-of-the-art models (GPT-4o and Claude 3.5) regarding quality and structural integrity. This research validates the hypothesis that explicit linguistic features, when integrated with modern deep learning embeddings, can outperform massive generative models in tasks requiring high precision.

---

## Key Findings

*   **Superior Precision:** The hybrid system combining RoBERTa embeddings with the linguistically informed 'Expert Index' achieved higher precision in identifying causal links compared to pure Large Language Model (LLM) approaches.
*   **Outperformance of SOTA Models:** The proposed framework consistently outperformed **GPT-4o** and **Claude 3.5** regarding the quality of generated causal graphs in experiments on narrative chapters and short stories.
*   **Balanced Output Quality:** The approach maintains high readability while capturing nuanced causal chains, bridging the gap between high-level causality and detailed event-specific relationships.
*   **Validation of Linguistic Features:** The integration of seven specific linguistic features (the Expert Index) into a Situation-Task-Action-Consequence (STAC) classification model proved effective for structuring narrative data.

---

## Technical Architecture & Methodology

The research proposes the **DA framework**, a hybrid "End-to-End" system combining LLMs, RoBERTa, and XGBoost. The architecture is implemented as a four-stage pipeline:

### Stage 1: Vertex Extraction (LLM)
*   **Tool:** LLM via LangChain.
*   **Process:** Generates agent-centered summaries serving as graph nodes.
*   **Techniques:** Summarization in active voice, pronoun substitution, and clause simplification.
*   **Goal:** Produce concise, distinct vertices for the graph.

### Stage 2: Feature Integration (The Expert Index)
*   **Model:** RoBERTa.
*   **Function:** Classifies sentences based on 7 linguistically informed features.
*   **The 7 Features:**
    1.  Genericity
    2.  Eventivity
    3.  Boundedness
    4.  Initiativity
    5.  Time Start
    6.  Time End
    7.  Impact
*   **Output:** Generates 192 unique feature combinations.

### Stage 3: Classification (STAC Model)
*   **Model:** XGBoost.
*   **Inputs:** RoBERTa embeddings + One-hot encoded linguistic features.
*   **Task:** Categorizes vertices into the STAC framework:
    *   **S**ituation
    *   **T**ask
    *   **A**ction
    *   **C**onsequence

### Stage 4: Graph Construction
*   **Process:** Utilizes a five-iteration prompting process.
*   **Goal:** Refines the output and constructs the final connected Causal Relationship Graph $G = (V, E)$ using extracted vertices and labels.
*   **Optimization:** Lower computational costs compared to full LLM inference methods.

---

## Experimental Results

The evaluation of the DA framework highlights significant improvements in both accuracy and efficiency:

*   **Dataset:** The Expert Index extraction model was trained on 750 annotated sentences from 23 short stories and novel chapters.
*   **Benchmarking:** Against human-verified ground truth, the hybrid system achieved higher precision in identifying causal links than pure LLM approaches.
*   **Performance:** Demonstrated structural superiority over GPT-4o and Claude 3.5 in graph generation tasks.
*   **Nuance Capture:** Successfully captured nuanced causal chains with high semantic fidelity.
*   **Cost Efficiency:** Reported lower computational costs compared to full LLM inference methods.

---

## Core Contributions

1.  **The 'Expert Index':** Definition and implementation of a novel set of seven linguistically informed features designed to improve semantic understanding in narrative processing.
2.  **Hybrid Architecture:** A technical contribution demonstrating the effectiveness of combining transformer embeddings (RoBERTa) with LLM-based summarization and linguistic features.
3.  **Open-Source Tooling:** The release of an interpretable and efficient tool for causal graph generation.
4.  **Enhanced Narrative Analysis:** A framework capable of synthesizing high-level causal understanding with specific event details to address the complexity of narrative structures.
