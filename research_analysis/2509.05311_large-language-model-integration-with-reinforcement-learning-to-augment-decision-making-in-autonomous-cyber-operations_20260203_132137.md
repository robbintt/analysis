---
title: Large Language Model Integration with Reinforcement Learning to Augment Decision-Making
  in Autonomous Cyber Operations
arxiv_id: '2509.05311'
source_url: https://arxiv.org/abs/2509.05311
generated_at: '2026-02-03T13:21:37'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations

*Konur Tholl; François Rivest; Mariam El Mezouar; Ranwa Al Mallah*

---

> ### **Quick Facts**
> * **Quality Score:** 9/10
> * **Total Citations:** 40
> * **Environment:** CybORG (13-host, 3-subnet)
> * **Base Algorithm:** Proximal Policy Optimization (PPO)
> * **Teacher Model:** Cyber-Risk-Llama8B
> * **Performance Gain:** >2x higher rewards (early training)
> * **Convergence Speed:** ~4,500 episodes faster
> * **Key Technique:** Action Masking + Auxiliary Loss

---

## Executive Summary

This research addresses the critical inefficiency and risk inherent in applying standard Reinforcement Learning (RL) to Autonomous Cyber Operations (ACO). Traditional RL agents rely on trial-and-error learning, requiring them to execute potentially catastrophic actions—such as system crashes or data exfiltration—to learn that such behaviors are negative. In cybersecurity environments, this "learning by doing" approach is prohibitively expensive and dangerous, as exploring the state space through random or uninformed actions can cause significant collateral damage before an effective policy is established.

The authors introduce a hybrid "Teacher-Guided" framework that integrates a Proximal Policy Optimization (PPO) agent with a pretrained Large Language Model (LLM), specifically **Cyber-Risk-Llama8B**. Technically, the system employs a dual mechanism of **Action Masking** and **Auxiliary Loss**, where the frozen LLM acts as an external knowledge base to filter out obviously negative exploratory actions and guide the agent's policy updates. The architecture features a decaying guidance schedule, reducing the LLM's influence by 25% every 8 episodes starting at episode 32. This design allows the agent to leverage semantic knowledge for rapid initial learning while smoothly transitioning to autonomous, experience-based decision-making as training progresses.

Validated within the CybORG simulation environment (13-host, 3-subnet), the LLM-guided agent demonstrated substantial performance improvements over a standard RL baseline. The integration resulted in over **2x higher rewards** during the early stages of training. Crucially, the guided agent converged to a favorable policy by approximately episode 170, whereas the standard RL agent required over 4,500 episodes to reach comparable performance, highlighting a dramatic acceleration in convergence. While Cyber-Risk-Llama8B was validated as the optimal teacher model, alternative techniques involving Feature Space Modification were explicitly rejected due to performance deterioration and low feature importance scores.

This study significantly advances the state of the art in autonomous cyber defense by providing a concrete solution to the "cold start" problem and the safety risks of RL exploration. By successfully incorporating external domain knowledge to bypass the need for harmful trial-and-error, the framework establishes a new benchmark for efficiency and safety in ACO.

---

## Key Findings

*   **Enhanced Initial Performance:** The integration of a Large Language Model (LLM) into the Reinforcement Learning (RL) framework resulted in the agent achieving over **2x higher rewards** during the early stages of training compared to the baseline.
*   **Accelerated Convergence:** The LLM-guided agent converged to a favorable policy approximately **4,500 episodes faster** than the standard RL agent.
*   **Reduction of Undesirable Actions:** By leveraging external knowledge, the approach significantly reduced the need for the agent to execute exploratory actions with obviously negative outcomes.
*   **Validation in Simulation:** The study successfully demonstrated the efficacy of LLM integration within a simulated cybersecurity environment, confirming the viability of the approach for Autonomous Cyber Operations (ACO).

---

## Methodology

The researchers utilized a hybrid architecture that combines Reinforcement Learning (RL) with a Large Language Model (LLM). Instead of relying solely on trial-and-error interactions, the RL agent was integrated with an external LLM pretrained on cybersecurity data to serve as a knowledge base for initial training and decision-making. The performance of this guided agent was evaluated against a baseline RL agent within a simulated cybersecurity environment to measure reward accumulation and convergence speed.

---

## Technical Details

**Environment & Baseline**
*   **Simulation:** CybORG environment (13-host, 3-subnet).
*   **Algorithm:** Proximal Policy Optimization (PPO) served as the baseline agent.

**Model Selection**
*   **Teacher Model:** Cyber-Risk-Llama8B.
*   **Selection Criteria:** Evaluated using BERTScore and manual scoring on 100 predefined questions.
*   **Output Format:** JSON (Average score: 0.455).

**Architecture & Mechanics**
*   **Approach:** Teacher-Guided framework combining **Action Masking** and **Auxiliary Loss**.
*   **LLM Status:** Frozen (weights not updated during training).
*   **Guidance Decay:** Strength decays by 25% every 8 episodes starting at episode 32.
*   **Explainability:** LIME (Local Interpretable Model-agnostic Explanations) used for analysis.

**Rejected Techniques**
*   Feature Space Modification was rejected due to performance deterioration and low feature importance scores.

---

## Results

*   **Performance Metrics:** The LLM-guided agent achieved over 2x higher rewards during early training.
*   **Convergence Rate:** Converged approximately 4,500 episodes faster than the standard RL agent.
*   **Technique Efficacy:** The Action Masking + Auxiliary Loss technique exhibited early improvement and a smooth transition.
*   **Baseline Catch-up:** The baseline agent caught up to the guided agent around episode 170.
*   **Variability:** Quantified using Standard Error (SE).

---

## Contributions

*   **Integration Framework:** The study presents a novel method for augmenting RL agents in cybersecurity by directly incorporating external knowledge from a pretrained LLM.
*   **Mitigation of Learning Inefficiency:** It addresses the critical limitation of standard RL in Autonomous Cyber Operations—specifically the requirement to execute harmful actions to learn their consequences—by using the LLM to filter out obviously negative exploratory actions.
*   **Performance Benchmarking:** The work provides quantitative evidence of the benefits of LLM guidance in cyber operations, establishing concrete benchmarks for improvement in early-stage rewards and training convergence rates.