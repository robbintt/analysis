---
title: World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents
arxiv_id: '2512.12548'
source_url: https://arxiv.org/abs/2512.12548
generated_at: '2026-02-03T13:01:38'
quality_score: 8
citation_count: 37
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents

*Yesid Fonseca; Manuel S. RÃ­os; Nicanor Quijano; Luis F. Giraldo*

***

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Framework** | Partially Observable Markov Decision Process (POMDP) |
| **Environment** | Modified 2D "Commons Harvest" (248x456 px) |
| **Model-Based Agent** | DreamerV2 |
| **Model-Free Agents** | R2D2, PPO |
| **Theoretical Baseline** | Marginal Value Theorem (MVT) |
| **Training Duration** | $1.5 \times 10^6$ steps |
| **Quality Score** | 8/10 |
| **Citations** | 37 |

***

## Executive Summary

> Reinforcement learning (RL) agents often struggle with the "patch-leaving problem" in complex environmentsâ€”knowing precisely when to leave a depleting resource patch to maximize long-term reward. This issue is critical because standard model-free agents fail to handle the credit assignment required for such long-horizon planning and do not align with ecological optimality principles like the Marginal Value Theorem (MVT). Bridging the gap between artificial decision-making and biological efficiency is essential for developing adaptive, resource-conscious AI systems that can operate effectively in scenarios requiring complex resource management.
>
> The researchers introduce a model-based approach using the **DreamerV2** architecture, which learns a compressed world model to simulate future states within a visual Partially Observable Markov Decision Process (POMDP). Unlike model-free baselines that rely solely on reactive history, DreamerV2 employs predictive representations to perform "imagination-based planning." By operating in a modified 2D "Commons Harvest" environment where resource patches provide exponentially decaying rewards, the agent uses these anticipatory capabilities to evaluate future outcomes. The core innovation identifies predictive world modeling as the specific mechanism allowing agents to naturally converge to the mathematically optimal strategy defined by the MVT.
>
> Over $1.5 \times 10^6$ training steps across four distinct map configurations, the model-based DreamerV2 agent significantly outperformed standard model-free counterparts. While the R2D2 agent failed to learn entirely and PPO demonstrated suboptimal behavior, DreamerV2 successfully adapted to varying environmental constraints. In test scenarios with patch distances of 3, 5, 7, and 9 units, the model-based agent's Patch Residence Time aligned closely with the theoretical MVT optimum. Evaluations based on 25 runs per scenario showed that DreamerV2 optimized its Gameplay Score by accurately judging when to leave depleting patches, a decision pattern that closely mirrors the efficient behavior of biological foragers.
>
> These findings establish a fundamental link between ecological principles and artificial intelligence, proving that bio-inspired architectures yield computationally optimal solutions. By demonstrating that predictive world models are the key to unlocking MVT-compliant behavior, this work provides a blueprint for developing AI systems that are both highly efficient and interpretable. This advances the field of Explainable AI (XAI) by suggesting that agents grounded in biological and ecological laws naturally exhibit decision patterns that are easier for humans to understand and trust. Ultimately, the research positions world models as a critical component for future adaptive systems requiring sophisticated resource management and long-term strategic planning.

***

## Key Findings

*   **Convergence to Biological Optimality:** Artificial foragers utilizing learned world models naturally converge to strategies that align with the **Marginal Value Theorem (MVT)**.
*   **Mechanism of Action:** Anticipatory capabilities driven by predictive representations of the environment are the primary mechanism enabling efficient patch-leaving behavior.
*   **Bio-mimicry:** Model-based Reinforcement Learning (RL) agents exhibit decision patterns that closely resemble biological foragers.
*   **Explainable AI Foundation:** Predictive world models provide a foundational architecture for developing AI systems that are both explainable and biologically grounded.

***

## Methodology

The study implements a **model-based reinforcement learning agent** designed to acquire a parsimonious predictive representation (world model) of its environment. The research methodology includes:

1.  **Comparative Analysis:** A direct comparison between the proposed model-based agents and standard model-free RL agents.
2.  **Theoretical Benchmarking:** Evaluating agent strategies against the **Marginal Value Theorem (MVT)** to determine alignment with optimal foraging behaviors.
3.  **Environment:** Agents operate within a visual POMDP requiring them to deduce resource states from visual cues.

***

## Technical Details

### System Architecture & Environment

| Component | Specification |
| :--- | :--- |
| **Framework** | Partially Observable Markov Decision Process (POMDP) |
| **Environment Name** | Modified 2D "Commons Harvest" (Melting Pot suite) |
| **Dimensions** | 248x456 pixels |
| **Agent Observation** | 64x64 pixels |
| **Visual Feedback** | Patch color changes from green to dark based on resource depletion ratio $r(n)/N$ |

### Reward Function
*   **Type:** Exponentially decaying
*   **Formula:** $r(n) = N \exp(-\lambda n)$
*   **Parameters:**
    *   $N$: 30
    *   $\lambda$: 0.01

### Theoretical Baseline
*   **Name:** Marginal Value Theorem (MVT)
*   **Objective:** Maximize net rate of energy intake ($E_n$)
*   **Optimality Condition:** Marginal gain equals average net rate

### Agent Configurations

#### Model-Based Agent
*   **Architecture:** DreamerV2
*   **Description:** Uses a learned world model for imagination-based planning.

#### Model-Free Agents
*   **Architectures:** R2D2, PPO
*   **R2D2 Configuration:**
    *   **Network:** Deep Q-Network
    *   **Convolutional Layers:**
        *   Filters: 16, Kernel: 8x8, Stride: 8
        *   Filters: 128, Kernel: 11x11, Stride: 1
    *   **Activation:** RELU
    *   **Fully Connected Layers:** 256
    *   **Hyperparameters:**
        *   Learning Rate: 0.0001
        *   Discount Factor: 0.997
        *   Batch Size: 1280
        *   Optimizer: Adam (epsilon=0.001)
        *   Workers: 2

***

## Results

### Training Protocol
*   **Duration:** $1.5 \times 10^6$ steps
*   **Steps per Episode:** 1500
*   **Maps:** 4 distinct maps selected uniformly
*   **Smoothing:** Exponential curve smoothing (omega=0.95) applied for DreamerV2

### Evaluation Metrics
1.  **Patch Residence Time:** Compared against MVT optimum.
2.  **Gameplay Score:** Cumulative score at step 1500.

### Test Scenarios
*   **Variables:** Patch distances ($\bar{x}$)
*   **Distances Tested:** [3, 5, 7, 9]
*   **Runs per Scenario:** 25
*   **Visualization:** Boxplots

### Performance Analysis
*   **R2D2:** Excluded from final results due to a complete lack of learning.
*   **Trajectory Analysis:** 25 experiments of 1000 steps were concatenated into 25,000 positions per scenario for PPO and DreamerV2 analysis.
*   **Outcome:** The model-based DreamerV2 agent successfully adapted to environmental constraints, showing Patch Residence Times that aligned closely with the theoretical MVT optimum, whereas PPO exhibited suboptimal behavior.

***

## Contributions

1.  **Mechanism Identification:** The study identifies predictive world modeling as the computational mechanism facilitating optimal patch-foraging decisions.
2.  **Bridging Ecology and AI:** It establishes a link between ecological optimality principles (MVT) and artificial intelligence, demonstrating that bio-inspired principles can enhance AI decision-making.
3.  **Advancement of XAI:** It advances Explainable AI by proposing that incorporating ecological and biological principles into RL agents leads to more interpretable and adaptive systems.

***

**References:** 37 citations | **Quality Score:** 8/10