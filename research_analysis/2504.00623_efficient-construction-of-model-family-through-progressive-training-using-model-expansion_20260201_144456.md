# Efficient Construction of Model Family through Progressive Training Using Model Expansion

*Kazuki Yano; Sho Takase; Sosuke Kobayashi; Shun Kiyono; Jun Suzuki*

---

> ### ðŸ“Š Quick Facts
> *   **Parameter Range:** 1B to 8B
> *   **Computational Savings:** ~25% reduction
> *   **GPU Hours Saved:** ~3,200 hours
> *   **Key Method:** Progressive Training + Model Expansion (`bert2BERT`)
> *   **Training Data:** FineWeb-Edu
> *   **Best Performance:** 8B Model in Chinchilla settings (Outperforming independent training in LAMBADA and ARC-e)

---

### Executive Summary

The research addresses the **prohibitive computational cost** and inefficiency of developing families of Large Language Models (LLMs) across varying parameter scales. Traditionally, models of different sizes (e.g., 1B to 8B parameters) are trained independently from scratch. This approach is resource-intensive and redundant, as smaller models are often discarded once larger models are trained. For researchers and practitioners needing to evaluate performance across multiple scales to determine the optimal compute-performance trade-off, this independent training pipeline presents a significant barrier to rapid, cost-effective iteration.

The authors propose a **progressive training pipeline** utilizing model expansion, where a single model family (1B, 2B, 4B, 8B) is constructed sequentially. The method initializes larger models with weights from smaller predecessors using the `bert2BERT` algorithm (specifically the AKI variant), allowing for simultaneous expansion of both width and depth within a Llama-based architecture.

Crucially, the authors introduce a **dynamic learning rate adjustment strategy**, optimizing the maximum learning rate specific to each model size during the expansion process. This technical refinement ensures that the expanded models do not merely inherit sub-optimal training trajectories but are tuned appropriately for their increased capacity.

This approach achieved a reduction in total computation of approximately **25%**, saving roughly **3,200 GPU hours**, while constraining total FLOPs to the cost equivalent of training only the largest model from scratch. Beyond cost savings, the method yielded superior or comparable performance to independent training baselines.

In Chinchilla optimal settings, the 8B progressive model outperformed the independent baseline in Perplexity (8.61 vs 8.65), LAMBADA (55.4% vs 53.9%), and ARC-e (73.5% vs 71.8%). Similar improvements were observed in 2B models under 2x Chinchilla settings, with LAMBADA and ARC-e scores increasing to 50.3% and 70.0%, respectively.

This research establishes a viable, cost-effective pipeline for LLM development that challenges the necessity of independent training runs for model families. By demonstrating that progressive training can maintain or exceed baseline performance while significantly cutting computational overhead, the study offers a practical framework for both academic and industrial labs. Furthermore, the finding that progressively trained models exhibit more **consistent behavioral outputs** across different scales suggests that this method not only improves efficiency but also enhances the reliability and stability of model families deployed in production environments.

---

### Key Findings

*   **Significant Cost Reduction:** The proposed method reduces computational costs of constructing a model family (1B to 8B parameters) by approximately **25%** compared to independent training.
*   **Performance Parity and Superiority:** The method maintains comparable performance by default but **outperforms independent training** when maximum learning rates are adjusted based on model size.
*   **Enhanced Consistency:** The models exhibit more consistent behavior across different parameter sizes than independently trained counterparts.
*   **Validated Scalability:** The method's scalability was successfully validated on models ranging from 1 billion to 8 billion parameters.

---

### Methodology

The approach utilizes **progressive training with model expansion**, fundamentally changing how model families are constructed:

*   **Incremental Expansion:** Instead of training models independently, smaller models are incrementally expanded into larger models.
*   **Dynamic Optimization:** The process includes dynamic learning rate adjustment, where maximum learning rates are tuned based on specific model size during the expansion process to optimize performance.
*   **Efficiency:** This strategy aims to reduce redundancy while maintaining or improving the final model capabilities.

---

### Technical Details

The technical implementation relies on specific architectural choices and training constraints to ensure efficiency and performance transfer.

**Architecture & Algorithm**
*   **Model Family:** Constructed sequentially (1B $\rightarrow$ 2B $\rightarrow$ 4B $\rightarrow$ 8B).
*   **Algorithm:** Utilizes the `bert2BERT` algorithm (AKI variant) for model expansion.
*   **Expansion Type:** Simultaneous width and depth expansion within a Llama architecture.

**Computational Strategy**
*   **Doubling:** Parameters are doubled at each stage.
*   **FLOPs Constraint:** Total FLOPs are constrained to equal the cost of training only the largest model from scratch.

**Training Configuration**
*   **Dataset:** FineWeb-Edu
*   **Tokenizer:** GPT-2
*   **Max Sequence Length:** 1024
*   **Scheduler:** Cosine scheduler
*   **Max Learning Rate:** 3.0e-4 (adjusted dynamically based on size during expansion)

---

### Results

The evaluation demonstrates clear advantages in both efficiency and downstream task performance.

**Efficiency Metrics**
*   **Computation Reduction:** ~25% total reduction.
*   **Resource Savings:** Approximately **3,200 GPU hours** saved.

**Benchmark Performance**

*Chinchilla Settings (8B Model)*
*   **Perplexity (Valid):** 8.61 vs 8.65 (Independent) $\rightarrow$ **Improved**
*   **LAMBADA:** 55.4% vs 53.9% (Independent) $\rightarrow$ **Improved**
*   **ARC-e:** 73.5% vs 71.8% (Independent) $\rightarrow$ **Improved**

*2x Chinchilla Settings (2B Model)*
*   **LAMBADA:** 50.3% vs 48.5% (Independent) $\rightarrow$ **Improved**
*   **ARC-e:** 70.0% vs 66.9% (Independent) $\rightarrow$ **Improved**

**General Observations**
*   Models trained progressively exhibit more consistent behavior across sizes compared to independent baselines.

---

### Contributions

The research makes three primary contributions to the field of LLM development:

1.  **Cost-Effective Pipeline:** Offers a pipeline for developing model families that addresses the inefficiency of independent training, significantly lowering the barrier to entry for multi-scale research.
2.  **Optimization Strategy:** Contributes a specific optimization strategy involving the adjustment of maximum learning rates according to model size within a progressive training framework, allowing models to exceed baseline performance.
3.  **Improved Reliability:** Enhances model reliability by ensuring more consistent behavioral outputs across different model scales, which is critical for production deployment.

---

*Document Quality Score: 8/10*