# A Survey of Continual Reinforcement Learning

*Chaofan Pan; Xin Yang; Yanhua Li; Wei Wei; Tianrui Li; Bo An; Jiye Liang*

> ### **Quick Facts**
> *   **Quality Score:** 7/10
> *   **References:** 40 citations
> *   **Primary Methodology:** Systematic Literature Review & Taxonomic Classification
> *   **Novel Contribution:** New taxonomy categorizing CRL methods by knowledge storage and transfer
> *   **Key Domains:** Robotics, Non-stationary environments, Multi-agent systems

---

## Executive Summary

Standard Reinforcement Learning (RL) is fundamentally constrained by its reliance on static training data and high computational resources, rendering it ineffective for dynamic, real-world deployment. Traditional agents suffer from "catastrophic forgetting," where the acquisition of new skills overwrites previous knowledge, preventing cumulative learning. This paper addresses the critical gap between theoretical RL capabilities and practical application by establishing Continual Reinforcement Learning (CRL) as a distinct paradigm. By integrating RL with Continual Learning, CRL aims to create autonomous systems capable of operating in non-stationary environments, adapting to new tasks without losing prior competencies.

The core contribution is a novel taxonomic framework that classifies the CRL landscape based on knowledge storage and transfer mechanisms. The authors categorize methodologies into four technical archetypes: Experience-focused methods utilizing Complementary Learning Systems (CLS) with short- and long-term buffers; Regularization-based methods like Elastic Weight Consolidation (EWC) that constrain parameters via Fisher Information Matrices; Synaptic models employing dynamic hidden variables; and Architectural approaches. Additionally, the paper standardizes the field's disjointed terminology by defining four precise scenario settings—Lifelong Adaptation, Non-Stationarity Learning, Task Incremental Learning, and Task-Agnostic Learning—clarifying the operational distinctions between labeled versus unlabeled tasks and evaluation scopes (new vs. full history).

The survey benchmarks the state-of-the-art across specific environments such as CRL Maze, Lifelong Hanabi, Continual World, and L2Explorer, identifying computational expense as a primary differentiator in performance. Analysis reveals that complex visual environments like StarCraft II and Minecraft require significantly extended training times compared to simpler state-vector observations. The study clarifies that evaluation metrics are highly scenario-dependent: Lifelong Adaptation emphasizes "forward transfer" (speed of adaptation), whereas Task Incremental Learning rigorously evaluates "backward transfer" to quantify the mitigation of catastrophic forgetting. Furthermore, the results indicate that the field's development lags behind continual supervised learning due to the high computational burden required to validate algorithms across these complex benchmarks, which creates reproducibility challenges.

This survey significantly influences the field by providing the first unified taxonomy and structured landscape analysis of CRL, resolving the fragmentation of existing research. By identifying unique challenges inherent to CRL that are distinct from standard RL or CL, the authors establish a necessary theoretical foundation that guides future algorithm development. The work effectively steers the community away from early legacy methods like CHILD and PNN toward specialized, modern benchmarks such as CORA, Continual World, and Lifelong Hanabi. Ultimately, this roadmap accelerates the transition from theoretical possibility to practical application, enabling the creation of robust AI systems capable of sustaining learning over timescales comparable to real-world deployment.

---

## Key Findings

*   **Limitations of Standard RL:** Current RL approaches are hindered by extensive training data requirements, high computational resource consumption, and a limited ability to generalize to new scenarios.
*   **CRL as a Solution:** By combining Continual Learning with RL, CRL enables continuous learning and adaptation while effectively mitigating "catastrophic forgetting."
*   **New Classification System:** The analysis introduces a taxonomy categorizing existing CRL methods into four distinct types based on their knowledge storage and transfer mechanisms.
*   **Distinct Challenges:** CRL presents unique challenges separate from standard RL or Continual Learning, necessitating the development of specific benchmarks and scenario settings for proper evaluation.

---

## Methodology

The study employs a **systematic literature review and taxonomic classification methodology**. This process involves:
1.  A comprehensive examination of the Continual Reinforcement Learning landscape.
2.  A detailed analysis of existing works based on evaluation metrics, tasks, benchmarks, and scenario settings.
3.  The development of a novel taxonomy using knowledge storage and transfer as the primary classification criterion.

---

## Contributions

*   **Comprehensive Survey:** Provides a holistic review of CRL, synthesizing core concepts, challenges, and methodologies into a single resource.
*   **Structured Landscape Analysis:** Organizes existing research by clarifying metrics, tasks, benchmarks, and scenario settings to provide structure to the field.
*   **Novel Taxonomy:** Proposes a new taxonomy that categorizes CRL methods into four specific types based on knowledge storage and transfer mechanisms.
*   **Future Roadmap:** Identifies unique challenges inherent to CRL and provides practical insights to guide future research directions.

---

## Technical Details

### CRL Method Classifications & Architectures

The survey categorizes methods into three primary architectural approaches:

*   **Experience-focused Methods**
    *   Utilize a **Complementary Learning System (CLS)** consisting of Short-Term and Long-Term Experience Buffers.
    *   Mechanisms include **direct and generative replay** to mitigate data correlation and catastrophic forgetting.

*   **Regularization-based Methods**
    *   **Elastic Weight Consolidation (EWC):** Constrains important parameters using a quadratic penalty term based on the Fisher Information Matrix ($F_i$).
        *   Loss Function: $\sum_i \frac{\lambda}{2} F_i (\theta_i - \theta^*_i)^2$
    *   **Online-EWC (P&C):** Updates the Fisher matrix incrementally to address linear complexity issues.
        *   Update Rule: $F^{(t)}_i = \gamma F^{(t-1)}_i + F^{new}_i$
    *   **TRAC:** A parameter-free optimizer designed for dynamic regularization.
    *   **CFV:** Uses entropy-regularization to solve new tasks without requiring further learning.

*   **Synaptic Models**
    *   Employ dynamic variables within hidden layers to manage knowledge retention.

### Benchmarks & Environments

The paper highlights several key environments used for evaluating CRL agents:
*   **CRL Maze:** Utilizing ViZDoom for non-stationary object-picking tasks.
*   **Lifelong Hanabi:** A setting for cooperative multi-agent learning.
*   **Continual World:** Focused on robotic manipulation tasks.
*   **L2Explorer:** A 3D environment using Procedural Content Generation (PCG).

### Scenario Settings

Four distinct scenario settings are defined to standardize evaluation:

| Scenario | Training Condition | Evaluation Scope |
| :--- | :--- | :--- |
| **Lifelong Adaptation** | Train on sequence of tasks | Evaluate only on new tasks |
| **Non-Stationarity Learning** | Tasks differ in reward/transition but share logic | Evaluate all tasks |
| **Task Incremental Learning** | Distinct reward/transition functions | Evaluate all tasks |
| **Task-Agnostic Learning** | Training without full task labels | Evaluate all tasks |

---

## Results & Analysis

### Benchmarking Characteristics
*   **Metrics:** Key evaluation metrics include the number of tasks, length of task sequences, and observation types (images vs. state vectors).
*   **Computational Expense:** Identified as a key differentiator. Complex environments like **StarCraft II** and **MineCraft** require significantly extended training times compared to simpler benchmarks.

### Scenario-Based Performance
*   **Lifelong Adaptation:** Performance focuses heavily on **forward transfer** (speed of adaptation to new tasks).
*   **Task Incremental Learning:** Evaluation rigorously assesses both **forward and backward transfer** to measure mitigation of catastrophic forgetting.
*   **Robotics Applications:** In these settings, robustness is tested against sensor noise, mechanical constraints, and online learning stability.

### Development and Reproducibility
*   **Growth Rate:** The growth of CRL is observed to be slower than continual supervised learning.
*   **Barriers:** This lag is attributed to reproduction difficulties and high compute requirements.
*   **Evolution:** The field has shifted from early methods (CHILD, PNN) to specialized benchmarks (CORA, Continual World) and complex multi-agent scenarios (Lifelong Hanabi).
