---
title: Enhancing Generalization in Data-free Quantization via Mixup-class Prompting
arxiv_id: '2507.21947'
source_url: https://arxiv.org/abs/2507.21947
generated_at: '2026-02-03T18:41:11'
quality_score: 8
citation_count: 39
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Enhancing Generalization in Data-free Quantization via Mixup-class Prompting
*Jiwoong Park; Chaeun Lee; Yongseok Choi; Sein Park; Deokki Hong; Jungwook Choi*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 39 Citations |
| **Architecture Target** | CNNs & Vision Transformers (ViTs) |
| **Key Configuration** | W2A4 (2-bit Weights, 4-bit Activations) |
| **Core Innovation** | Mixup-class Prompting Strategy |

---

## üìã Executive Summary

Data-free quantization (DFQ) is essential for deploying deep learning models on resource-constrained edge devices without access to original training data, often due to privacy or storage constraints. Current state-of-the-art DFQ methods rely on text-conditioned generative models, such as Latent Diffusion Models, to synthesize calibration data. However, the authors identify a critical limitation in these approaches: the reliance on "single-class prompts."

Because text labels often suffer from **polysemy**‚Äîwhere a single word can have multiple meanings or contexts‚Äîprompting a generator with a single class label (e.g., "box") tends to produce synthetic data that lacks diversity and fails to capture the full feature distribution of the target domain. This results in poor generalization and degraded quantization accuracy.

To overcome the limitations of polysemy and sparse feature representation, the paper proposes a **"mixup-class prompting" strategy**. This innovation fuses multiple class labels at the text prompt level before inputting them into the generative model. Conceptually similar to mixup data augmentation, this approach prompts the Latent Diffusion Model to generate synthetic images that represent a linear combination of classes, rather than distinct, isolated examples. By creating calibration data with softer decision boundaries and richer intra-class variations, the method enhances the robustness of the Post-Training Quantization (PTQ) process.

The proposed mixup-class prompting method demonstrates comprehensive performance improvements across both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). It consistently outperforms previous state-of-the-art DFQ methods, including GenQ, particularly in challenging low-bit regimes. In the extreme **W2A4** configuration, the approach achieved state-of-the-art accuracy, highlighting its effectiveness in aggressive compression scenarios.

Beyond raw accuracy, the study reported improved training stability, evidenced by reduced variance in quantization performance and an analysis of gradient norms showing better convergence characteristics. This research significantly advances the practicality of data-free quantization by establishing a new performance boundary for low-bit model compression.

---

## üîë Key Findings

*   **Polysemy Issue Identified:** Single-class prompts in previous data-free quantization methods suffer from polysemy issues, leading to degraded performance.
*   **Mixup-class Strategy:** The proposed 'mixup-class prompt' strategy generates diverse and robust synthetic data, enhancing generalization and stability.
*   **SOTA Performance:** The method consistently outperforms state-of-the-art DFQ methods (like GenQ) across both CNNs and Vision Transformers.
*   **Extreme Low-bit Success:** The approach achieves state-of-the-art accuracy in extremely low-bit regimes, specifically **W2A4 configurations**.

---

## üõ†Ô∏è Methodology

The authors introduce a **'mixup-class prompt' strategy** that fuses multiple class labels at the text prompt level. This methodology involves:

1.  **Generative Synthesis:** Utilizing text-conditioned generative models, specifically **Latent Diffusion Models**, to create calibration data for Post-Training Quantization (PTQ).
2.  **Label Fusion:** Instead of single prompts, the method fuses class labels to force the generator to synthesize data representing a mixture of concepts.
3.  **Quantitative Analysis:** Employing rigorous analysis of gradient norms and generalization errors to understand the characteristics of the synthesized data and its impact on the quantization process.

---

## ‚öôÔ∏è Technical Details

*   **Problem Addressed:** The approach tackles polysemy issues found in single-class prompts of previous Data-Free Quantization (DFQ) methods.
*   **Mechanism:** By proposing a 'mixup-class prompt' strategy, the method generates diverse and robust synthetic data to enhance generalization and stability without accessing original training data.
*   **Architecture Agnostic:** The method is validated on and applicable to both CNNs and Vision Transformers (ViTs).
*   **Target Regime:** The research focuses on **extreme low-bit quantization regimes**, specifically the **W2A4 configuration** (2-bit Weights, 4-bit Activations).

---

## üöÄ Contributions

*   **Advancement of DFQ:** Advances Data-free Quantization (DFQ) by effectively addressing calibration data limitations and privacy constraints.
*   **Novel Prompting Mechanism:** Contributes a novel text prompting mechanism that applies mixup principles to generative model inputs to mitigate polysemy.
*   **New Performance Boundary:** Establishes a new performance boundary for low-bit quantization, demonstrating robustness on modern transformer architectures and CNNs.

---

## üìà Results

The experimental outcomes highlight the efficacy of the proposed approach:

*   **Superior Accuracy:** The proposed method outperforms current State-of-the-Art (SOTA) DFQ methods, including **GenQ**, across both CNN and Vision Transformer architectures.
*   **Low-bit Dominance:** It achieves SOTA accuracy in the challenging **W2A4** (extremely low-bit) setting.
*   **Enhanced Stability:** Experiments demonstrate improved training stability, indicated by reduced variance in quantization performance.
*   **Better Generalization:** Measured by accuracy on validation datasets, the method shows enhanced generalization capabilities compared to baselines.

---
*Report generated based on provided research analysis.*