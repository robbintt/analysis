# QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models

*Yudong Zhang; Ruobing Xie; Jiansheng Chen; Xingwu Sun; Zhanhui Kang; Yu Wang*

***

> ### ðŸ“‘ Quick Facts
>
> *   **Quality Score:** 7/10
> *   **References:** 20 citations
> *   **Core Proposal:** Query-Agnostic Visual Attack (QAVA) framework
> *   **Target Component:** Vision-Language Alignment Module (Q-former)
> *   **Target Models:** BLIP-2, InstructBLIP (White-box), LLaVA, MiniGPT-4 (Black-box)
> *   **Dataset:** VQA v2 Validation Set (Subset '32+50')
> *   **Optimization:** Gradient-based white-box strategy (PGD & C&W)

***

## Executive Summary

Existing adversarial attacks against Large Vision-Language Models (LVLMs) suffer from a critical dependency: they optimize perturbations for specific textual queries (image-question pairs). Consequently, these attacks fail in real-world scenarios where user prompts are dynamic, unknown, or variable. While current literature shows high success rates in controlled settings, these methods disintegrate when the query context changes, leaving a significant gap in understanding the true security robustness of LVLMs.

This paper addresses this need by introducing **QAVA (Query-Agnostic Visual Attack)**, a framework that decouples visual perturbations from textual inputs to create universal image-level adversarial examples. Instead of optimizing against the final language model output for a specific query, QAVA targets the vision-language alignment moduleâ€”the Q-former. The method employs a gradient-based optimization strategy to maximize the Mean Squared Error (MSE) between the Q-former features of clean and adversarial images, thereby bypassing prompt dependency.

QAVA demonstrates quantifiable efficacy that rivals traditional query-specific methods while solving the transferability problem:
*   **White-box ASR:** ~95.5% (BLIP-2) and ~94.6% (InstructBLIP), comparable to query-specific baselines (~98%).
*   **Black-box ASR:** 68.3% against LLaVA and 48.4% against MiniGPT-4.
*   **Comparison:** Traditional attacks saw success rates plummet to under 6% in black-box transfer scenarios.

By introducing a query-agnostic attack vector, this paper exposes a fundamental vulnerability in the visual encoders of multimodal systems, challenging the effectiveness of current defense mechanisms that rely solely on prompt filtering. The findings necessitate a paradigm shift in LVLM safety, urging the development of defenses that prioritize the robustness of visual feature extractors.

***

## Key Findings

*   **Query-Specific Limitations:** Existing adversarial attacks against LVLMs are query-specific. If an adversarial image generated for one question is asked a different question, the model often answers correctly.
*   **Robustness to Unknown Queries:** The QAVA method creates adversarial images that reliably induce incorrect responses even when the accompanying question is unspecified, unknown, or varies from the original attack context.
*   **Comparable Efficiency:** QAVA achieves effectiveness and efficiency rates in unknown question scenarios that are comparable to traditional attacks where the target question is known.
*   **Visual Modality Vulnerability:** The study reveals that LVLMs possess previously overlooked vulnerabilities specifically regarding visual adversarial threats in practical, open-world settings.

***

## Methodology

The researchers introduce the **Query-Agnostic Visual Attack (QAVA)**, a framework designed to generate robust adversarial examples at the image level.

*   **Decoupling Attack Context:** Unlike traditional adversarial methods that optimize perturbations against a specific image-text (question) pair, QAVA decouples the visual attack from the textual query.
*   **General Failure Induction:** The approach focuses on modifying the input image to create perturbations that cause the LVLM to fail generally. This ensures the model produces incorrect responses regardless of the specific question posed about the image.
*   **Feature-Level Focus:** By targeting intermediate representations rather than end-to-end outputs, the attack maintains efficacy without knowledge of the text prompt.

***

## Technical Details

### Architecture & Target
QAVA targets Large Vision-Language Models (LVLMs), specifically focusing on the vision-language alignment module (**Q-former**).

*   **Visual Encoder:** Utilizes models like EVA-CLIP to extract raw image features (Shape: "[257, 1408]").
*   **Q-former:** Compresses image features into a condensed representation (Shape: "[32, 768]").
*   **Large Language Model:** Processes the Q-former output to generate text.

### Optimization Strategy
The attack utilizes a gradient-based white-box optimization strategy:

*   **Loss Function ($L_{QAVA}$):** The method aims to maximize the **Mean Squared Error (MSE)** between the Q-former features of the clean and adversarial images.
*   **Sampling Strategies:** To ensure query-agnosticism, four strategies are employed during optimization:
    1.  **WTQ:** White-box targeting questions.
    2.  **VQG:** Visual question generation.
    3.  **RSQN:** Random sample questions.
    4.  **RSQt:** Random Sample Questions by Types.

### Hyperparameters
*   **PGD-$l_{\infty}$:** $\epsilon=8$, steps=20, $\alpha=2$.
*   **C&W-$l_2$:** steps=50, $\alpha=0.01$, confidence=0.
*   **Loss Constants:** $c=0.1$ for $L_{LLM}$ and $c=0.005$ for $L_{QAVA}$.

***

## Experimental Results

### Experimental Setup
*   **Dataset:** VQA v2 validation set, specifically a subset 'VQA v2 32+50' containing **32 images** and **50 questions per image** (1,600 pairs total).
*   **Evaluation Metric:** Standard VQA score based on 10 ground truth answers.

### Performance Metrics
*   **White-Box Targets (BLIP-2, InstructBLIP):** QAVA achieved Attack Success Rates (ASR) of approximately **95.5%** and **94.6%**, respectively. This shows negligible drop-off compared to query-specific baselines (~98%).
*   **Transferability Targets (LLaVA, MiniGPT-4):** QAVA exhibited superior transferability:
    *   **LLaVA:** ASR of **68.3%**
    *   **MiniGPT-4:** ASR of **48.4%**
*   **Baseline Comparison:** Traditional query-specific attacks saw success rates plummet to **under 6%** when transferred to the same black-box models, confirming QAVA's ability to induce failure without specific question knowledge.

***

## Contributions

1.  **Novel Framework:** Introduction of **QAVA**, the first query-agnostic visual attack specifically tailored for Large Vision-Language Models to handle scenarios where the input query is unknown.
2.  **Bridging Theory and Practice:** Bridging the gap between theoretical adversarial attacks and real-world applications by demonstrating that high-attack success rates can be maintained without prior knowledge of the specific textual prompt.
3.  **New Vulnerability Class:** Uncovering a new class of vulnerabilities in LVLMs, highlighting that the visual modality remains a critical entry point for robust, generic attacks even when the textual context changes.