---
title: Embedded AI Companion System on Edge Devices
arxiv_id: '2601.08128'
source_url: https://arxiv.org/abs/2601.08128
generated_at: '2026-02-03T13:37:38'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Embedded AI Companion System on Edge Devices

*Rahul Gupta; Stephen D. H. Hsu*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Hardware Platform:** NVIDIA Jetson Orin Nano Super 8GB
> *   **Core Model:** Qwen2.5-7B-Instruct (int4 GGUF)
> *   **Key Innovation:** Dual-Phase "Activeâ€“Inactive" Memory Architecture
> *   **Benchmark:** New AI Companion Benchmark
> *   **Context Limit:** ~2,500 tokens at ~5s TTFT

---

## Executive Summary

**Problem**
Current AI companion systems face a critical trade-off between computational performance and deployment environment. Existing solutions typically rely on large, cloud-based Large Language Models (LLMs), which introduce significant latency, privacy concerns, and dependency on internet connectivity. Conversely, deploying these systems on edge devices is hindered by hardware constraints; running raw LLMs locally often results in an inability to maintain long-term context or memory due to limited RAM and processing power. This paper addresses the challenge of creating a responsive, personalized AI companion capable of operating within the stringent resource limits of embedded hardware without sacrificing the depth of interaction found in cloud-based alternatives.

**Innovation**
The core innovation is a **Dual-Phase "Activeâ€“Inactive" architecture** designed to decouple real-time interaction from computationally intensive memory processing. The "Active Phase" handles low-latency dialogue retrieval and generation, while the "Inactive Phase" executes during periods of user inactivity to perform background tasks such as memory extraction, summarization, and user profiling (using traits like Meyers Briggs and Big Five). Technically, the system was implemented on a NVIDIA Jetson Orin Nano Super 8GB using a quantized Qwen2.5-7B-Instruct (int4 GGUF) via llama.cpp and `gte-base-en-v1.5` embeddings. Memory management utilizes a dual-store approach (Short-term and Long-term) with cosine similarity retrieval, top-$k$ selection, and a sliding window mechanism to manage context input size effectively.

**Results**
The proposed system demonstrated a significant performance advantage over a raw Qwen2.5-7B-int4 baseline and achieved **performance parity with GPT-3.5** (featuring a 16k context window) on the newly introduced AI Companion benchmark. Latency analysis revealed strict boundaries for edge deployment: approximately 2 seconds of latency allows for a context limit of ~1000 tokens, while ~5 seconds Time to First Token (TTFT) permits ~2500 tokens. Attempts to process contexts exceeding 10k tokens resulted in Out of Memory (OOM) errors. Furthermore, the system successfully validated its psychometric profiling capabilities, accurately tracking user personality metrics alongside standard conversational benchmarks.

**Impact**
This research represents a pivotal shift in edge AI, demonstrating that small, quantized models can effectively compete with large, cloud-based models when paired with an optimized memory architecture. By proving that long-term personalization can be maintained on resource-constrained devices, this work establishes a viable path for private, responsive, and intelligent AI companions that do not require cloud connectivity. Additionally, the introduction of a specific "AI Companion benchmark" provides the field with a standardized framework for evaluating future systems, emphasizing the importance of memory and personality retention alongside raw generation capabilities.

---

## Key Findings

*   **Superior Performance:** The proposed system outperforms an equivalent raw Large Language Model (LLM) without memory capabilities across the majority of evaluation metrics.
*   **Cloud Parity:** Despite using a computationally weak model (Qwen2.5-7B-Instruct quantized to int4), the system achieved performance comparable to **GPT-3.5** with a 16k context window.
*   **Optimized Latency:** The dual-phase design successfully minimizes interaction latency while simultaneously maintaining long-term personalization capabilities.
*   **Edge Gap Analysis:** Existing AI companion and memory systems are unsuitable for edge environments due to their high computational demands and latency issues.

---

## Methodology

The researchers implemented a **two-phase memory paradigm** comprising an Active Phase for low-latency real-time dialogue and an Inactive Phase for computationally intensive memory tasks during user inactivity.

*   **Evaluation Framework:** The study utilized a newly introduced 'AI Companion benchmark' specifically designed to assess AI companions.
*   **Simulation:** Tested using Qwen2.5-7B-Instruct quantized to int4 to simulate embedded hardware constraints.
*   **Metrics:** Performance was measured using Time to First Token (TTFT), Tokens/sec, and psychometric profiling accuracy (Meyers Briggs and Big Five).

---

## Technical Specifications

### Architecture Design
*   **Dual-Phase System:**
    *   **Active Phase:** Low-latency real-time interaction.
    *   **Inactive Phase:** Background memory extraction and refinement (executed during user inactivity).
*   **Memory Management:**
    *   **Stores:** Short-term and Long-term memory stores.
    *   **Retrieval:** Uses cosine similarity retrieval, top-$k$ selection, and similarity thresholds.
    *   **Context Control:** Managed via a sliding window mechanism.

### Hardware & Software Stack
| Component | Specification |
| :--- | :--- |
| **Hardware** | NVIDIA Jetson Orin Nano Super 8GB |
| **Runtime** | llama.cpp |
| **Model** | Qwen2.5-7B-Instruct (int4 GGUF) |
| **Embeddings** | `gte-base-en-v1.5` |

### Data Processing
*   **Chunking:** Performed with overlap.
*   **Profiling:** Summarization and user profiling (traits, Meyers Briggs, Big Five).

### Latency Constraints
*   **~2s Latency:** Context limited to ~1000 tokens.
*   **~5s TTFT:** Context allows ~2500 tokens.
*   **>10k Tokens:** Results in Out of Memory (OOM) errors.

---

## Research Contributions

*   **Edge-Optimized Architecture:** A novel memory paradigm that decouples real-time interaction from heavy memory processing for resource-constrained edge devices.
*   **Evaluation Framework:** The introduction of a comprehensive benchmark specifically designed to assess AI Companions.
*   **Performance Efficiency:** Demonstrated that a small, quantized local model can effectively compete with large, cloud-based models when equipped with the proposed memory architecture.

---

## Performance Results

The system was evaluated against a raw Qwen2.5-7B-int4 baseline and the cloud-based GPT-3.5 (16k context).

1.  **Benchmark Performance:** The system achieved performance parity with GPT-3.5, significantly outperforming the raw local baseline.
2.  **Latency Management:** Successfully balanced responsiveness with deep memory retrieval, staying within acceptable limits for embedded devices.
3.  **Psychometric Accuracy:** The system successfully tracked and validated user personality traits using Meyers Briggs and Big Five scales, proving the efficacy of the background profiling mechanisms.