# Accelerating Multi-Task Temporal Difference Learning under Low-Rank Representation

*Yitao Bai; Sihan Zeng; Justin Romberg; Thinh T. Doan*

---

> ### ‚ö° Quick Facts
>
> *   **Domain:** Multi-Task Reinforcement Learning (Policy Evaluation)
> *   **Core Technique:** Truncated SVD-TD Learning
> *   **Theoretical Guarantee:** Convergence rate of $O(\ln(t)/t)$
> *   **Key Innovation:** Non-linear SVD projection integrated into update loop
> *   **Complexity Shift:** Scales with effective rank $r$ instead of $N$ tasks
> *   **Quality Score:** **9/10**

---

## üìã Executive Summary

This research addresses the computational challenges of Multi-Task Reinforcement Learning (RL), specifically focusing on policy evaluation across $N$ distinct tasks that share a state space, action space, and transition dynamics. In standard scenarios, RL algorithms treat task evaluation independently, resulting in high computational redundancy and scaling costs that grow linearly with the number of tasks. 

The authors identify that while these tasks are distinct, their value functions often reside in a shared low-dimensional subspace. Consequently, the problem is how to exploit this low-rank structure to accelerate learning and reduce complexity without sacrificing convergence stability.

The proposed solution, **"Truncated SVD-TD Learning,"** mathematically integrates a truncated Singular Value Decomposition (SVD) step directly into the standard Temporal Difference (TD) update loop. Unlike standard TD methods, which update value functions independently, this approach projects the stacked value function matrix onto the top $k$ singular vectors at each iteration. This allows the algorithm to iteratively discover and utilize the dominant directions of the underlying low-rank structure without requiring prior knowledge of specific basis vectors.

The study demonstrates that this method achieves a **non-asymptotic convergence rate of $O(\ln(t)/t)$**, effectively matching standard TD learning. Crucially, the authors prove that despite the non-linear projection, the algorithm remains stable. By exploiting task interdependence, the method reduces computational complexity from scaling with $N$ to scaling with the effective rank $r$, offering substantial benefits for large-scale multi-agent systems.

---

## üîç Key Findings

*   **Superior Performance:** The proposed TD learning variant with integrated truncated SVD significantly outperforms classic TD learning.
*   **Rank Dependency:** The performance advantage increases as the rank of the subspace decreases (lower rank = higher acceleration).
*   **Stability:** The method remains stable and converges reliably despite the non-linear truncated SVD step.
*   **Optimal Convergence:** The proposed method achieves a theoretical convergence rate of $O(\ln(t)/t)$, matching the optimal rate of standard TD learning.

---

## üß™ Methodology

The study addresses multi-task reinforcement learning policy evaluation under a **low-rank representation assumption**. This assumes that the value functions for all tasks reside in a shared low-dimensional subspace.

To leverage this, the authors propose a novel variant of standard Temporal Difference (TD) learning. This variant integrates a **truncated singular value decomposition (SVD)** step directly into the update process. By utilizing the dominant directions of the underlying low-rank structure, the algorithm can separate signal from noise across tasks more effectively than independent updates.

---

## üèÜ Research Contributions

*   **Algorithmic Innovation:** Introduction of a modified TD learning algorithm that leverages low-rank matrix structure via truncated SVD to accelerate multi-task policy evaluation.
*   **Theoretical Guarantees:** Provision of rigorous theoretical proofs that the non-linear truncated SVD step does not cause instability. The method is proven to converge at an optimal rate.
*   **Empirical Validation:** Demonstration of substantial computational and performance benefits over independent task learning by exploiting task interdependence.

---

## ‚öôÔ∏è Technical Details

### Problem Setting
*   **Scope:** Multi-Task Reinforcement Learning (RL) for multi-task policy evaluation.
*   **Shared Parameters:** $N$ tasks share state space, action space, transition kernel, and discount factor.
*   **Assumption:** The stacked value function matrix is low-rank, specifically $rank(V^\pi) = r$.

### Algorithm: Truncated SVD-TD Learning
The algorithm integrates Truncated Singular Value Decomposition (SVD) into the Temporal Difference (TD) update loop. This allows for the iterative discovery of a shared subspace without pre-known basis vectors.

**Update Rule:**

$$ V_{t+1} = (1 - \alpha_t)V_t + \alpha_t \gamma P^\pi P_k(V_t) + \alpha_t w_t + \alpha_t R^\pi $$

Where:
*   $V_t$ is the value function matrix at time $t$.
*   $\alpha_t$ is the step size.
*   $P^\pi$ represents the transition dynamics.
*   $P_k(V_t)$ projects the value matrix onto the top $k$ singular vectors.
*   $w_t$ and $R^\pi$ represent noise and reward terms, respectively.

---

## üìä Results

*   **Convergence Rate:** Achieves a non-asymptotic convergence rate of $O\left(\frac{\ln(t)}{t}\right)$ for the Frobenius norm distance to the optimal value function.
*   **Subspace Alignment:** The subspace alignment rate diminishes at $O(1/t)$.
*   **Error Bound:** The theoretical error bound depends on a constant $c_1 = \frac{16 N^2 d}{(1-\gamma)^2}$.
*   **Qualitative Performance:** The method significantly outperforms classic TD. Advantages are inversely correlated to the rank $r$ (lower $r$ yields better results).
*   **Complexity:** Aims to reduce computational complexity from scaling with $N$ (number of tasks) to scaling with the effective rank $r$.

---

## üìö References & Evaluation

*   **References:** 10 citations
*   **Quality Score:** 9/10