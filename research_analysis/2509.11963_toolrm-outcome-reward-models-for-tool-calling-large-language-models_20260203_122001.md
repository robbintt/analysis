---
title: 'ToolRM: Outcome Reward Models for Tool-Calling Large Language Models'
arxiv_id: '2509.11963'
source_url: https://arxiv.org/abs/2509.11963
generated_at: '2026-02-03T12:20:01'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ToolRM: Outcome Reward Models for Tool-Calling Large Language Models

*Mayank Agarwal; Ibrahim Abdelaziz; Kinjal Basu; Merve Unuvar; Luis A. Lastras; Yara Rizk; Pavan Kapanipathi*

---

> ### ðŸ“Š Quick Facts
> * **Performance Gain:** Up to **25%** improvement over baselines.
> * **Model Scale:** Evaluated on models ranging from **1.7B to 14B** parameters.
> * **Benchmark:** Introduced **FC-RewardBench** with 1,500 data points.
> * **Validation:** Tested across **7 out-of-domain** benchmarks.
> * **Quality Score:** 9/10
> * **Citations:** 40 references

---

## Executive Summary

This research addresses a critical limitation in current Large Language Model (LLM) alignment pipelines: the inability of general-purpose reward models to accurately evaluate tool-based reasoning and execution. Standard reward models, typically trained on natural language preferences, lack the nuanced understanding required to assess the functional correctness of tool calls. This creates a significant barrier to developing capable tool-using agents, highlighted by a substantial performance gap in existing evaluations.

To bridge this gap, the authors introduce **ToolRM**, a specialized Outcome Reward Model (ORM) designed explicitly for tool-calling scenarios, alongside **FC-RewardBench**, the first systematic benchmark for this domain. ToolRM departs from standard natural language preference modeling by focusing on functional outcomes, utilizing the Bradley-Terry model for pairwise preference modeling and Maximum Likelihood Estimation (MLE) for training.

Evaluation across seven out-of-domain benchmarks demonstrates that ToolRM significantly outperforms general-purpose baselines (such as "LLM-as-a-Judge" and "Themis"), achieving up to a **25%** improvement in downstream task performance. The introduction of FC-RewardBench sets a new standard for benchmarking tool-use capabilities, while ToolRMâ€™s ability to enable data-efficient fine-tuning addresses practical constraints in model training.

---

## Key Findings

*   **Performance Gap:** Existing reward models fail to effectively evaluate tool-based reasoning and execution. Analysis using FC-RewardBench reveals a significant performance gap for current models in tool-calling scenarios.
*   **Superior Performance:** The proposed outcome-based reward models outperform general-purpose baselines by up to **25%** in downstream task performance.
*   **Data Efficiency:** These specialized models enable data-efficient fine-tuning through reward-guided filtering, reducing the volume of data required for alignment.
*   **Common Error Modes:** In the FC-RewardBench dataset, the most frequent error types were *Incorrect Parameter Value* (650 instances) and *Incorrect Function Name* (403 instances).

---

## Technical Details

**Core Architecture**
*   **Model Type:** Outcome Reward Model (ORM) specialized for tool-calling.
*   **Mathematical Framework:** Utilizes the **Bradley-Terry model** for pairwise preference modeling.
*   **Probability Calculation:** Calculates preference probabilities using a sigmoid function.
*   **Training Objective:** Employs Maximum Likelihood Estimation (MLE).

**Regularization & Optimization**
*   **Reward Centering:** Uses reward centering regularization to ensure zero-centered rewards, stabilizing the training process.

**Evaluation Infrastructure**
*   **Benchmark:** FC-RewardBench, constructed from the Berkeley Function Calling Leaderboard (BFCL) v3.
*   **Dataset Composition:** Contains 1,500 data points comprising user queries, tool catalogs, and paired correct/incorrect tool calls.
*   **Data Generation:** Samples generated by 25 diverse language models to ensure broad coverage.

---

## Methodology

1.  **Benchmark Construction:** Researchers developed FC-RewardBench, the first systematic benchmark dedicated to assessing reward models in tool-calling scenarios.
2.  **Data Synthesis:** The team utilized data synthesized from open-weight LLMs to construct comprehensive training sets, ensuring a variety of tool-use patterns.
3.  **Model Training:** Instead of relying on standard natural language preferences, the team trained outcome-based reward models focused on functional execution results.
4.  **Rigorous Evaluation:** Models ranging from 1.7 billion to 14 billion parameters were evaluated against seven out-of-domain benchmarks to ensure robustness and generalizability.

---

## Contributions

*   **FC-RewardBench:** Introduction of the first benchmark dedicated to evaluating reward models specifically in tool use scenarios.
*   **Novel Framework:** Proposed a new framework for training outcome-based reward models using synthesized data, shifting focus from language fluency to functional correctness.
*   **Empirical Validation:** Demonstrated that models trained with this framework significantly surpass general-purpose baselines, empirically proving the value of domain-specific reward modeling for tool-using agents.

---

## Results

**Performance Metrics**
*   **Downstream Tasks:** ToolRM outperformed general-purpose baseline reward models by up to **25%**.
*   **Accuracy & Validation:** Achieved higher Accuracy and Schema Validation scores compared to baselines like *LLM-as-a-Judge* and *Themis*.

**Data Efficiency**
*   The model demonstrated data-efficient fine-tuning capabilities, effectively guiding data filtering to significantly reduce the amount of data required for alignment without sacrificing performance.

---
*Document generated based on research analysis.*