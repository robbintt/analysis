# Expert Merging in Sparse Mixture of Experts with Nash Bargaining

*Dung V. Nguyen; Anh T. Nguyen; Minh H. Nguyen; Luc Q. Nguyen; Shiqi Jiang; Ethan Fetaya; Linh Duy Tran; Gal Chechik; Tan M. Nguyen*

---

> ### üìä Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Proposed Framework** | NAMEx (Nash Merging of Experts) |
> | **Theoretical Basis** | Game Theory (Nash Bargaining Solution) |
> | **Key Innovation** | Complex Momentum for acceleration |
> | **Validated Models** | Qwen1.5-MoE (14B), DeepSeek-MoE (16B) |
> | **Performance Gain** | Up to 2.4% accuracy improvement in text classification |
> | **Quality Score** | **9/10** |
> | **Citations** | 40 References |

---

## Executive Summary

This paper addresses the critical challenge of efficiently merging expert parameters within Sparse Mixture of Experts (SMoE) architectures. As models scale to billions of parameters, current merging strategies‚Äîoften reliant on heuristic averaging‚Äîfail to provide a principled mechanism for weighting expert contributions. This leads to suboptimal performance, instability, and poor generalization on out-of-distribution data.

The authors introduce **NAMEx (Nash Merging of Experts)**, a novel framework that reinterprets expert merging through the lens of game theory. By modeling expert interactions as a cooperative and competitive game, NAMEx utilizes the **Nash Bargaining Solution (NBS)** to treat merging as a multi-objective optimization problem. This approach maximizes the product of gains relative to a disagreement point.

To further optimize the process, the authors integrate **complex momentum**. Unlike traditional momentum, this uses complex-valued updates to stabilize the optimization trajectory, ensuring convergence toward the Nash equilibrium while offering theoretical guarantees.

NAMEx demonstrates statistically significant improvements over state-of-the-art baselines (SMEAR, Lory, and CAMEx), achieving up to **2.4% accuracy improvements** in text classification and **5% improvement** over heuristic methods on corrupted data. Scalability was successfully validated on massive architectures, including Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), establishing NAMEx as a rigorous new standard for expert merging.

---

## Key Findings

*   üöÄ **Superior Performance:** The proposed NAMEx framework consistently outperforms existing expert merging strategies across various domains, including language modelling, text classification, and image classification.
*   üõ°Ô∏è **Enhanced Robustness:** NAMEx demonstrates improved zero-shot robustness, maintaining high performance even under data corruption scenarios.
*   üìà **Scalability Validation:** The method proves effective on large-scale models, specifically validated on **Qwen1.5-MoE (14B)** and **DeepSeek-MoE (16B)**, in both zero-shot and fine-tuning settings.
*   üìâ **Theoretical Convergence:** The inclusion of complex momentum into the merging process successfully accelerates expert propagation while maintaining theoretical convergence guarantees.

---

## Methodology

The authors fundamentally reinterpret the expert merging process in Sparse Mixture of Experts (SMoE) by applying principles from game theory. They identify the dynamics between experts as both **cooperative** and **competitive**.

1.  **Game-Theoretic Formulation:** The merging process is treated as a game where experts must negotiate their contributions.
2.  **Nash Bargaining:** The NAMEx framework utilizes Nash Bargaining to establish a principled weighting mechanism for merging expert parameters.
3.  **Complex Momentum:** To optimize stability and speed, the authors integrate complex momentum into the NAMEx framework. This technique accelerates expert propagation, ensuring the model reaches an optimal state faster.

---

## Technical Details

**Core Concept: NAMEx (Nash Merging of Experts)**
The paper reformulates the merging process as a cooperative-competitive game using the Nash Bargaining Solution (NBS). Instead of heuristic averaging, the process is treated as a multi-objective optimization.

**Foundations**
Building upon **EP-CAMEx** and **Curvature-Aware Merging (CAMEx)**, which utilize natural gradients and the Multiple-Gradient Descent Algorithm (MGDA), NAMEx treats domain vectors as utility functions.

**Optimization Strategy**
*   **Utility Maximization:** The method maximizes the product of gains over a disagreement point.
*   **Expert Propagation:** It introduces 'complex momentum' to stabilize and accelerate the propagation of a shared base expert across layers.

---

## Results

**Comparative Analysis**
NAMEx consistently outperforms existing strategies (SMEAR, Lory, and CAMEx) across language modelling, text classification, and image classification domains.

**Quantitative Metrics**
*   **Text Classification:** Achieved accuracy improvements of up to **2.4%** over the best-performing baseline.
*   **Data Corruption:** Outperformed heuristic averaging methods by approximately **5%** on datasets subjected to severe data corruption.
*   **Zero-Shot Robustness:** Demonstrated enhanced stability in zero-shot scenarios involving data corruption.

**Scalability & Routing**
The framework was validated on massive architectures:
*   **Qwen1.5-MoE (14B)**: Showed performance gains in zero-shot and fine-tuning settings.
*   **DeepSeek-MoE (16B)**: Successfully recovered fine-tuning performance with higher fidelity than existing methods.

**Routing Dynamics Analysis**
*   **Swin-MoE:** Analyzed specific routing behaviors.
*   **Switch-Transformer:** Observed dynamic routing at Layer 8.
*   **Qwen-MoE:** Identified robust representations at Layer 9.

---

## Contributions

*   **Game-Theoretic Framework:** Provided a novel reinterpretation of expert merging by modeling the interactions among experts as a cooperative and competitive game.
*   **NAMEx Algorithm:** Introduced a new merging strategy that leverages Nash Bargaining to enable more balanced and efficient collaboration among experts, addressing the lack of principled weighting in existing averaging methods.
*   **Optimization Enhancement:** Incorporated complex momentum into the merging process, offering both practical acceleration and theoretical guarantees for convergence.
*   **Broad Applicability:** Demonstrated that NAMEx integrates seamlessly with popular MoE architectures and is scalable to contemporary state-of-the-art large language models (LLMs).

---

*Report generated based on 40 citations. Quality Score: 9/10*