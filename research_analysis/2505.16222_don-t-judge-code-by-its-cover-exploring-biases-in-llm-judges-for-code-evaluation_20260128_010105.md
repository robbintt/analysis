---
title: 'Don''t Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation'
arxiv_id: '2505.16222'
source_url: https://arxiv.org/abs/2505.16222
generated_at: '2026-01-28T01:01:05'
quality_score: 9
citation_count: 33
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation

*Jiwon Moon, Dongryeol Lee, Kyomin Jung, Taegwan Kang, Yongil Kim, Yerin Hwang*

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Dataset Size** | 2,000 code samples |
> | **Source** | CodeNet (AtCoder Beginner Contest) |
> | **Languages** | C++, Python, Java, JavaScript, Go |
> | **Bias Types Identified** | 6 distinct categories |
> | **Quality Score** | 9/10 |
> | **References** | 33 citations |

---

## Executive Summary

This research addresses the critical reliability gap in using Large Language Models (LLMs) as automated judges for code evaluation. As LLMs are increasingly deployed to assess code quality and correctness, the paper reveals that these models are fundamentally flawed in their judgment criteria. Rather than evaluating functional logic, LLM judges frequently fall prey to superficial features, granting inflated scores to code that looks impressive but functions incorrectly, or penalizing correct code due to cosmetic differences.

The authors introduce a rigorous empirical framework to quantify this phenomenon, establishing a taxonomy of six distinct bias categories: comment-based biases (e.g., authority claims, misleading tasks), variable change bias, and illusory complexity bias. The study utilizes a dataset comprising 2,000 code samples from the AtCoder Beginner Contest across five languages, containing pairs of correct and incorrect solutions. The methodology systematically injects non-functional modificationsâ€”such as randomizing identifiers, inserting misleading comments, or adding dummy functionsâ€”into semantically equivalent code.

The experiments demonstrate that all tested LLM architectures are universally susceptible to both positive and negative biases, often prioritizing style over substance. The results indicate that existing mitigation strategies are ineffective; even when judges were prompted to generate test cases before scoring, vulnerability to bias persisted. This paper provides the first comprehensive investigation into the fairness of LLM-based code evaluation, proving that current models cannot be relied upon to judge code strictly on functional correctness. These findings underscore an urgent need for more robust evaluation metrics that explicitly isolate functional logic from superficial cues.

---

## Key Findings

*   **Universal Susceptibility:** All tested LLMs acting as code judges are susceptible to both positive and negative biases, leading to inflated scores or unfairly low ratings.
*   **Superficial Influence:** LLM judges are heavily influenced by non-functional elements such as variable names, comments, and formatting, even when functional correctness remains identical.
*   **Mitigation Failure:** Vulnerability to biases persists even when judges are prompted to generate test cases before scoring.
*   **Systematic Errors:** Six distinct types of bias systematically affect code evaluation across different models.
*   **Judgment Reversals:** Authority biases can cause judgment reversals from "Incorrect" to "Correct," highlighting a severe reliability issue.

---

## Technical Details

### Dataset Construction
*   **Source:** CodeNet, sourced from the AtCoder Beginner Contest (ABC).
*   **Volume:** 2,000 code samples.
*   **Scope:** Five programming languages (C++, Python, Java, JavaScript, Go).
*   **Breakdown:** 200 problems per language; each containing one correct and one incorrect solution (focusing on 'Wrong Answer' logic errors).
*   **Preprocessing:** User comments are removed during preprocessing before bias injection.

### Bias Taxonomy Implemented
The study defines and analyzes six specific categories of potential bias:

1.  **Comment-Based Biases:**
    *   *Authority:* Uses 10 pre-defined templates.
    *   *Reverse Authority.*
    *   *Self-Declared Correctness.*
    *   *Misleading Task:* Generated via LLM.
2.  **Variable Change Bias:**
    *   Involves automated randomization of identifiers.
3.  **Illusory Complexity Bias:**
    *   Insertion of manually selected, uncalled dummy functions.
    *   Scaled to constitute approximately 16%â€“28% of the average code length.

### Dataset Metrics
*   **Java:** Highest average code length (1,652.2 characters) and highest dummy proportion (27.16%).
*   **Python:** Lowest average code length (346.1 characters) and lowest dummy proportion (16.17%).

---

## Methodology

*   **Empirical Evaluation:** Conducted across five programming languages and multiple LLM architectures.
*   **Robustness Assessment:** Evaluated semantically equivalent code containing only superficial, non-functional modifications.
*   **Comparative Analysis:** Standard scoring behavior was compared against a condition where judges were prompted to generate test cases prior to evaluation.
*   **Taxonomy Analysis:** Defined and analyzed the impact of six specific bias categories.

---

## Results

*   **Qualitative:** Findings reveal that LLM judges are universally susceptible to biases, often prioritizing superficial modifications like comments and variable names over functional correctness.
*   **Robustness:** Robustness against bias fails even when judges are prompted to generate test cases.
*   **Reversal Impact:** Authority biases can cause judgment reversals from Incorrect to Correct.
*   **Language Specifics:**
    *   Dummy functions are scaled to constitute ~16%â€“28% of total code length.
    *   Java exhibited the highest average code length and dummy proportion.
    *   Python exhibited the lowest average code length and dummy proportion.

---

## Research Contributions

*   **Comprehensive Investigation:** Provided the first comprehensive investigation into whether LLM judges can fairly evaluate code with superficial variations.
*   **Framework Establishment:** Established a framework defining six types of potential bias in LLM-based code evaluation.
*   **Empirical Demonstration:** Demonstrated empirically that relying on LLM evaluators for correctness results in systematic scoring errors due to non-semantic factors.
*   **Mitigation Critique:** Identified that existing evaluation methods like test case generation are insufficient to ensure fairness, highlighting the need for more robust evaluation metrics.

---

**Report Score:** 9/10
**Citations:** 33