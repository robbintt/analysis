# Jina-VLM: Small Multilingual Vision Language Model

*Andreas Koukounas; Georgios Mastrapas; Florian H√∂nicke; Sedigheh Eslami; Guillaume Roncari; Scott Martens; Han Xiao*

***

> **Document Metrics**
> * **Quality Score:** 8/10
> * **Citations:** 27

---

## üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Total Parameters** | 2.4 Billion |
| **Vision Encoder** | SigLIP2-So400M (400M) |
| **Language Backbone** | Qwen3-1.7B-Base (1.7B) |
| **Connector Type** | Custom Attention-Pooling (~50M) |
| **Image Strategy** | 12 Tiles + 1 Global Thumbnail |
| **Token Reduction** | 4.0x (9,477 $\to$ 2,366) |
| **Avg VQA Score** | 72.3 |

---

## üìù Executive Summary

Current state-of-the-art Vision Language Models (VLMs) face a significant scalability dilemma: achieving high performance, particularly in multilingual contexts and high-resolution processing, typically requires massive parameter counts and substantial computational resources. Smaller models in the 2-billion parameter range often fail to keep pace with larger counterparts, creating a barrier for deploying efficient, capable multimodal AI on edge devices or cost-sensitive environments. Jina-VLM addresses this challenge by aiming to bridge the performance gap between large proprietary models and smaller open models, specifically focusing on the trade-off between image resolution, token efficiency, and multilingual reasoning capabilities.

The core innovation of Jina-VLM is its architectural design centered on efficiency and modularity. The model couples a SigLIP2-So400M vision encoder with a Qwen3-1.7B language decoder via a custom, approximately 50M parameter attention-pooling connector. To handle variable image resolutions without linearly increasing computational cost, the authors implement a tiling strategy that decomposes images into 12 overlapping tiles plus one global thumbnail. The connector utilizes attention pooling over 2x2 patch neighborhoods, concatenating features from the 24th and 18th layers of the vision encoder. This mechanism significantly compresses visual information, allowing the language model to process high-resolution details with a fraction of the usual token count.

Jina-VLM establishes a new state-of-the-art performance level for open models in the 2B-scale category, achieving an average score of `72.3` across eight standard English VQA benchmarks and SOTA results on multilingual multimodal benchmarks (MMMB and Multilingual MMBench). Beyond accuracy, the architecture delivers substantial efficiency gains: the design yields a 4.0x reduction in visual tokens (from 9,477 to 2,366) and a 3.9x reduction in LLM prefill compute (27.2 to 6.9 TFLOPs). Consequently, the system achieves a total FLOPs reduction of 2.3x and a 4.0x reduction in KV-Cache memory usage, all while maintaining competitive performance in text-only tasks.

This research signifies a pivotal shift toward democratizing high-performance multimodal AI by proving that architectural ingenuity can offset the need for massive scale. By open-sourcing the model weights and code, the authors provide the community with a potent tool for developing applications that require robust multilingual visual understanding without the infrastructure costs of larger models. The Jina-VLM approach sets a new standard for parameter-efficient VLM design, likely influencing future research to prioritize token-efficient connectors and resolution-agile processing over sheer model size.

---

## üîë Key Findings

*   **New SOTA Performance:** Jina-VLM establishes a new state-of-the-art performance in multilingual visual question answering (VQA) among open models in the 2B-scale parameter category.
*   **Benchmark Dominance:** The model demonstrates leading results on standard VQA benchmarks and multilingual evaluations.
*   **Preserved Text Capabilities:** Despite being optimized for vision-language tasks, the model preserves competitive performance in text-only scenarios.
*   **Token Efficiency:** The design facilitates token-efficient processing capable of handling images of arbitrary resolutions.

---

## üõ†Ô∏è Methodology

The Jina-VLM architecture is built on a hybrid approach designed to maximize performance while minimizing computational overhead.

*   **Architecture:** The model couples a **SigLIP2** vision encoder with a **Qwen3** language backbone.
*   **Key Component:** A critical element of this design is the implementation of an **attention-pooling connector**.
*   **Functionality:** This connector bridges the vision and language components, enabling the model to process images of any resolution with high token efficiency.

---

## ‚öôÔ∏è Technical Details

### Architecture Breakdown
*   **Total Parameters:** 2.4 Billion
    *   **Vision Encoder:** SigLIP2-So400M/14-384 (400M parameters, 27-layer ViT)
    *   **Language Decoder:** Qwen3-1.7B-Base (1.7B parameters)
    *   **Connector:** Custom ~50M parameter Vision-Language connector

### Image Processing Strategy
*   **Tiling Strategy:** Arbitrary resolution tiling decomposes images into:
    *   **12** overlapping tiles
    *   **1** global thumbnail
    *   Size per tile: 378x378 pixels
*   **Input Formatting:** Uses special tokens: `<im start>`, `<im end>`, `<im col>`.

### Connector Mechanism
*   **Pooling:** Employs attention pooling over **2x2 patch neighborhoods**.
*   **Feature Fusion:** Concatenates features from SigLIP2 layers **24 and 18**.
*   **Optimization:** Reduces visual tokens by **4x** compared to baseline methods.

---

## üìà Results

### Performance Benchmarks
*   **English VQA:** Achieved the highest average score of **72.3** across eight standard English VQA benchmarks among open 2B-scale VLMs.
*   **Multilingual:** Attained SOTA performance on multilingual multimodal benchmarks (**MMMB** and **Multilingual MMBench**).

### Efficiency Metrics
In the default 12-tile configuration compared to standard processing:

*   **Visual Tokens:** Reduced by 4.0x (9,477 $\to$ 2,366)
*   **LLM Prefill Compute:** Reduced by 3.9x (27.2 $\to$ 6.9 TFLOPs)
*   **KV-Cache Memory:** Reduced by 4.0x (2.12 GB $\to$ 0.53 GB)
*   **Total System FLOPs:** Reduction of 2.3x

---

## üè∑Ô∏è Contributions

*   **High-performance small-scale VLM:** Introduction of a compact 2.4B parameter model that challenges larger models in multilingual capabilities.
*   **Architectural efficiency:** The utilization of an attention-pooling connector to allow for arbitrary-resolution image processing without sacrificing token efficiency.
*   **Versatility:** Successful retention of text-only proficiency within a predominantly vision-language architecture.
*   **Open Science:** Public release of both model weights and code to facilitate further research and application.