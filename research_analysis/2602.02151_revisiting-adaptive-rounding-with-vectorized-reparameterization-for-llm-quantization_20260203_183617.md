---
title: Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization
arxiv_id: '2602.02151'
source_url: https://arxiv.org/abs/2602.02151
generated_at: '2026-02-03T18:36:17'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization
*Yuli Zhou; Qingxuan Chen; Luca Benini; Guolei Sun; Yawei Li*

---

> ### ðŸ“Š Quick Facts
> *   **Parameter Efficiency:** Reduces trainable parameters to **0.2%** of standard adaptive rounding.
> *   **Data Requirement:** Optimizes effectively with only **128 calibration samples**.
> *   **Supported Architectures:** OPT, LLaMA, LLaMA2, Qwen.
> *   **Optimization Target:** L-infinity norm (minimizes worst-case element-wise error).
> *   **Compatibility:** Modular framework works with GPTQ, QuaRot, and OmniQuant.

---

## Executive Summary

Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on resource-constrained hardware, yet achieving high accuracy in low-bit modes remains computationally challenging. Adaptive rounding (AR) is a technique used to minimize quantization error by learning optimal rounding directions; however, it faces a critical scalability bottleneck with billion-parameter models. Traditional AR methods rely on optimizing a dense, layer-specific rounding matrix, which results in prohibitive memory overhead and slow convergence speeds. Furthermore, standard approaches often fail to adequately handle heavy-tailed weight distributions common in modern LLMs, creating a need for a more scalable and mathematically robust solution.

This paper introduces **VQRound**, a novel optimization framework that overcomes these limitations through Vectorized Reparameterization via Vector Quantization (VQ). Instead of optimizing a massive dense matrix, VQRound transforms the rounding process into an optimization over a compact, shared codebook, drastically compressing the parameter space. To address heavy-tailed distributions, the method employs L-infinity norm optimization to minimize the worst-case element-wise error. Theoretically, VQRound approximates task loss increase using a simplified quadratic model that captures weight interaction errors without the heavy computational burden of full Hessian matrix operations, enabling efficient end-to-end optimization across all layers.

VQRound demonstrates superior efficiency and performance across major architectures including OPT, LLaMA, LLaMA2, and Qwen. The method reduces trainable parameter overhead to a mere 0.2% compared to standard adaptive rounding approaches while requiring only 128 calibration samples to achieve optimal results. In 3-bit quantization experiments on WikiText-2, VQRound achieved a perplexity of 6.78 on LLaMA-7B and 6.31 on LLaMA-2-7B, effectively outperforming baselines like RTN and GPTQ. Additionally, the study showed that VQRound achieves significantly better convergence rates than traditional adaptive rounding and serves as a modular enhancement compatible with modern pipelines such as OmniQuant and QuaRot.

---

## Key Findings

*   **Superior Convergence:** VQRound achieves better convergence rates than traditional adaptive rounding methods within the same number of training steps.
*   **Extreme Parameter Efficiency:** The method drastically reduces trainable parameter overhead to as little as **0.2%** of standard approaches.
*   **Data Efficiency:** Requires only **128 samples** to achieve optimal results, making it highly accessible for practical deployment.
*   **Broad Generalizability:** Demonstrates robust performance across major architectures (OPT, LLaMA, LLaMA2, Qwen) for billion-parameter LLMs.

---

## Methodology

The VQRound framework relies on a three-pronged approach to solving the scalability issues of adaptive rounding:

1.  **Vectorized Reparameterization:**
    *   Utilizes Vector Quantization (VQ) to transform a dense rounding matrix into a compact codebook.
    *   This enables parameter-efficient optimization by sharing vectors across layers.

2.  **L-Infinity Norm Optimization:**
    *   Specifically designed to handle heavy-tailed weight distributions found in LLMs.
    *   Focuses on minimizing the element-wise worst-case error rather than just average error.

3.  **Lightweight End-to-End Pipeline:**
    *   Optimizes codebooks across all layers simultaneously.
    *   Operates with minimal calibration data, reducing the time and resource cost typically associated with quantization-aware training.

---

## Technical Details

*   **Optimization Space Compression:** VQRound compresses the optimization space from a full-size rounding matrix to a compact codebook of shared vectors. This addresses the scalability limitations of standard adaptive rounding.
*   **Theoretical Foundation:** The method approximates task loss increase using a quadratic form involving the Hessian matrix. It specifically targets off-diagonal entries to account for weight interaction errors without the computational cost of full Hessian operations.
*   **Modular Integration:** Designed as a modular framework, VQRound is compatible with existing PTQ pipelines such as **GPTQ**, **QuaRot**, and **OmniQuant**, allowing for easy integration into current workflows.

---

## Contributions

*   **Solving Computational Bottlenecks:** Addresses the prohibitive memory overhead and slow speeds of adaptive rounding for billion-parameter models, making the technique scalable and fast.
*   **Novel Framework (VQRound):** Introduces a new optimization framework that replaces dense matrices with codebook representation, significantly reducing memory and computational costs.
*   **Specialized Quantization Solution:** Provides a tailored solution for quantizing LLMs with heavy-tailed distributions through L-infinity norm minimization, a mathematical improvement over standard L2-norm approaches.

---

## Results

*   **Performance Metrics:** In 3-bit quantization experiments on LLaMA-7B and LLaMA2-7B (evaluated on WikiText-2), VQRound delivers competitive perplexity scores:
    *   **LLaMA-7B:** 6.78 PPL
    *   **LLaMA-2-7B:** 6.31 PPL
*   **Baseline Comparison:** Reduces error when combined with baselines like RTN and GPTQ.
*   **Scalability:** Successfully scales to billion-parameter models while maintaining convergence speeds superior to traditional adaptive rounding methods.
*   **Architecture Coverage:** Validated across multiple model families, confirming its generalizability.

---

**Quality Score:** 9/10  
**References:** 40 citations