---
title: Knowledge distillation through geometry-aware representational alignment
arxiv_id: '2509.25253'
source_url: https://arxiv.org/abs/2509.25253
generated_at: '2026-02-03T19:37:58'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Knowledge Distillation Through Geometry-Aware Representational Alignment
*Prajjwal Bhattarai; Mohammad Amjad; Dmytro Zhylko; Tuka Alhanai*

---

> ### **Quick Facts**
> *   **Model Families:** BERT, OPT
> *   **Task Types:** Classification, Instruction-following
> *   **Performance Gain:** Up to 2 percentage points
> *   **Key Metrics:** Procrustes Distance, Feature Gram Matrix
> *   **Quality Score:** 9/10
> *   **References:** 40 citations

---

## Executive Summary

**Problem**
Current feature distillation methods, such as projection-based mean squared error (MSE) and Centered Kernel Alignment (CKA), suffer from a fundamental theoretical limitation: they can achieve a loss value of zero without successfully capturing the structural geometry of the teacher’s feature space. This discrepancy is critical because it implies that minimizing standard divergence metrics does not guarantee that the student model is learning the underlying representational structure of the teacher. As models grow in complexity, relying on these flawed alignment metrics risks creating student models that mimic surface-level activations without internalizing the essential geometric relationships required for robust generalization.

**Innovation**
To address this, the authors introduce a framework for Geometry-Aware Representational Alignment, shifting the optimization focus from probabilistic divergence to geometric similarity. The core technical innovation involves repurposing Procrustes distance and the Frobenius norm of the Feature Gram Matrix as primary loss functions. Unlike projection-based MSE, which struggles with dimensionality mismatches, or CKA, which can fail to capture feature structure at convergence, these metrics explicitly preserve the spherical geometry of the feature space. By prioritizing the alignment of inner products and angles on a unit-normed sphere (and assuming a Low-Rank Subspace for dimensionality reduction), the proposed method ensures that the student model replicates the relational structure of the teacher’s internal representations.

**Results**
The proposed geometry-aware approach was empirically validated on BERT and OPT language model architectures across both classification and instruction-following tasks. The method demonstrated statistically significant and consistent performance improvements, outperforming standard feature distillation baselines. Specifically, the integration of Procrustes distance and Feature Gram Matrix losses yielded performance gains of up to 2 percentage points compared to existing methods. These results confirm that the geometric preservation of feature representations translates directly to enhanced predictive capability in downstream applications.

**Impact**
This research significantly advances the field of knowledge distillation by establishing a rigorous theoretical link between feature geometry and model performance. The authors successfully debunk the assumption that standard alignment metrics effectively capture feature structure, providing a mathematical proof of their limitations. By introducing Procrustes distance and Feature Gram Matrix norms as viable optimization objectives, the paper offers a more robust toolset for training efficient student models. This work paves the way for developing compact, high-performing language models that retain the nuanced representational capabilities of their larger counterparts, a crucial development for deploying state-of-the-art NLP in resource-constrained environments.

---

## Key Findings

*   **Theoretical Limitations Exposed:** Existing feature distillation methods, specifically projection-based mean squared loss and Centered Kernel Alignment (CKA), theoretically fail to capture the structural geometry of the teacher's feature space even when the loss value reaches zero.
*   **Geometric Alignment Success:** Using Procrustes distance and the Frobenius norm of the Feature Gram Matrix as distillation losses effectively aligns the representational geometry between teacher and student models.
*   **Consistent Performance Gains:** The proposed geometry-aware approach achieves statistically significant and consistent performance gains across multiple model families.
*   **Measurable Improvement:** The method yields performance improvements of up to 2 percentage points in both classification and instruction-following tasks using BERT and OPT language model architectures.

---

## Methodology

The authors move beyond traditional probabilistic divergence or simple Euclidean norm minimization between hidden layers. They employ a theoretical analysis to expose the limitations of current alignment metrics and propose utilizing specific distance metrics—**Procrustes distance** and the **Frobenius norm of the Feature Gram Matrix**—as the primary loss functions for distillation.

These metrics are chosen for their ability to capture the geometric structure of feature representations. The approach is validated empirically by training student models (BERT and OPT families) on classification and instruction-following tasks to measure the transfer of capabilities.

---

## Technical Details

The paper proposes a feature distillation framework based on **Geometry-Aware Representational Alignment**, focusing on aligning the internal geometry of teacher and student models rather than output logits.

**Core Metrics**
*   **Feature Gram Matrix Distance (FG):** Defined as the Frobenius norm of the difference between Gram matrices. It is designed to preserve the inner product structure.
*   **Procrustes Distance:** Utilized to measure the dissimilarity between shapes of representations.

**Theoretical Framework**
*   **Spherical Geometry:** The approach prioritizes angles and inner products on a unit normed sphere.
*   **Low-Rank Subspace:** Assumes features exist in a low-rank subspace for alignment when teacher dimensions exceed student dimensions.

**Critique of Existing Methods**
*   **Projection-based MSE:** Critiqued for handling dimension mismatches poorly and being "too powerful."
*   **Centered Kernel Alignment (CKA):** Critiqued for potentially failing to capture feature structure even when the loss is zero.

---

## Contributions

*   **Theoretical Demonstration:** Provides a theoretical proof that popular feature distillation methods (projection-based MSE, CKA) cannot capture feature structure despite optimizing for alignment.
*   **Novel Loss Functions:** Introduces Procrustes distance and Feature Gram Matrix Frobenius norm as novel loss functions, motivating their transition from measurement tools to effective optimization objectives for knowledge distillation.
*   **Empirical Validation:** Demonstrates the practical efficacy of integrating feature geometry into distillation pipelines for modern language models (BERT and OPT), highlighting performance gains.

---

## Results

Experiments were conducted on **BERT** and **OPT** model families for classification and instruction-following tasks. The geometry-aware method yielded performance improvements of up to **2 percentage points** compared to baseline methods. These gains were statistically significant and consistent across the tested families and tasks, outperforming standard feature distillation baselines like projection-based mean squared loss and Centered Kernel Alignment (CKA).