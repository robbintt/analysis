---
title: 'Justitia: Fair and Efficient Scheduling for LLM Applications'
arxiv_id: '2510.17015'
source_url: https://arxiv.org/abs/2510.17015
generated_at: '2026-01-27T22:17:05'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Justitia: Fair and Efficient Scheduling for LLM Applications

*Quan Chen, Efficient Scheduling, Mingyan Yang, Yifei Liu, Han Zhao, Manqi Luo, Guanjie Wang, Yu Feng, Minyi Guo, Chen Chen*

---

> **üí° Quick Facts: Key Metrics at a Glance**
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Prediction Efficiency:** Requires only 100 training samples per application
> *   **Baseline Comparison:** Uses 4-layer MLP vs. 66M parameter DistilBERT (S3)
> *   **Core Metric:** Memory-centric 'KV token-time'

---

## üìë Executive Summary

Current LLM inference schedulers prioritize aggregate throughput and GPU utilization, often at the expense of fairness and isolation in multi-tenant environments. This creates significant head-of-line (HOL) blocking, where a single long-running request monopolizes resources and delays the completion of many short requests. As organizations shift toward shared LLM services, the inability to guarantee resource shares proportional to tenant weights leads to unpredictable tail latencies and missed Service Level Objectives (SLOs), rendering traditional schedulers insufficient for heterogeneous production workloads.

**Justitia** introduces a novel adaptation of Weighted Fair Queueing (WFQ), a classic network packet scheduling algorithm, specifically tailored for the unique lifecycle of LLM inference. The system decomposes processing into compute-bound prefill and memory-bound decode phases, employing separate handling logic and a virtual time mechanism to track entitled service shares. To enable this fair allocation without heavy overhead, Justitia implements a lightweight prediction module utilizing 4-layer Multi-Layer Perceptron (MLP) models. These models vectorize inputs via TF-IDF and estimate resource demand‚Äîdefined as memory-centric "KV token-time"‚Äîallowing the scheduler to assign virtual finish times and perform preemption-aware batching within continuous batching frameworks like vLLM.

The study demonstrates that Justitia significantly reduces tail latency and meets SLOs more reliably than state-of-the-art baselines while maintaining high GPU utilization and batch density. Specifically, the prediction component achieves high efficiency, requiring only 100 training samples per application. This contrasts favorably with the S3 baseline, which relies on a 66 million parameter DistilBERT model that incurs significant runtime overhead and requires extensive sample collection. Justitia successfully proves that strict fairness can be enforced without sacrificing the performance benefits of modern batching techniques.

This research establishes the first formal definition of fairness for generative LLM inference and provides a working architecture that reconciles the conflicting goals of high-throughput batching and strict isolation. By successfully applying network scheduling theory to the iterative, dual-phase nature of LLMs, Justitia sets a new standard for multi-tenant cloud services. Its influence lies in providing a practical path forward for service providers who must guarantee performance isolation across diverse customers without needing to over-provision expensive GPU hardware.

---

## üîç Key Findings

*   **Head-of-Line Blocking:** Existing LLM inference schedulers prioritize aggregate throughput but often fail to provide isolation or fairness, leading to significant head-of-line blocking where a single long request delays the completion of many short requests.
*   **Fairness Throughput Trade-off:** The study demonstrates that it is possible to enforce strict fairness (ensuring tenants receive resources proportional to their weights) without sacrificing the high GPU utilization and throughput benefits of modern batching techniques like vLLM or Orca.
*   **Resource Consumption Divergence:** The analysis reveals that the prompt processing (prefill) phase and the token generation (decode) phase have vastly different resource consumption profiles; an effective scheduler must distinguish between these phases to accurately calculate and enforce fair shares.
*   **SLO Adherence:** Justitia significantly reduces tail latency and meets Service Level Objectives (SLOs) more reliably compared to state-of-the-art baselines, particularly in multi-tenant scenarios with heterogeneous workload mixes.

---

## üß© Methodology

### Adapted Fair Queueing
The methodology adapts the classic concept of Weighted Fair Queueing (WFQ)‚Äîtraditionally used in network packet scheduling‚Äîto the domain of LLM inference.

### Two-Phase Lifecycle Decomposition
The system decomposes LLM inference into two distinct stages:
1.  **Compute-Bound Prefill Stage:** Processing the input prompt.
2.  **Memory-Bound Decode Stage:** Generating output tokens.
The system applies separate handling logic for each stage.

### Virtual Time Implementation
Justitia utilizes a virtual time mechanism to track the entitled service share of each request. It schedules requests based on their virtual finish times to ensure no request monopolizes GPU cycles beyond its allocated weight.

### Preemption-aware Batching
The approach integrates with continuous batching mechanisms, potentially employing preemption or prioritized slot allocation within the batch to enforce fairness without needing to flush the GPU cache unnecessarily.

---

## ‚öôÔ∏è Technical Implementation Details

The paper proposes a lightweight, application-specific prediction mechanism to estimate LLM inference resource demand.

| Component | Specification |
| :--- | :--- |
| **Prediction Model** | Multi-Layer Perceptron (MLP) |
| **Architecture** | 4-layer MLP with dynamic neuron sizing based on average input size |
| **Input Vectorization** | TF-IDF |
| **Cost Metric** | Memory-centric "KV token-time" |
| **Training Method** | Gradient Descent with Mean Squared Error loss and L2 regularization |
| **Comparison Baseline** | S3 (66M parameter DistilBERT) |

**Rationale:** The MLP approach is designed to avoid the heavy sample collection and runtime overhead associated with larger models like DistilBERT.

---

## üèÜ Research Contributions

*   **Formalization of LLM Fairness:** The paper provides a formal definition of fairness for LLM inference services, introducing a metric that accounts for the unique, iterative nature of generative models rather than treating them as standard black-box compute tasks.
*   **The Justitia Scheduler:** The design and implementation of a novel scheduler that is the first to effectively combine iterative-level scheduling (for high throughput) with fair queueing algorithms (for isolation).
*   **Comprehensive Empirical Evaluation:** A rigorous evaluation using real-world LLMs (such as LLaMA and OPT) and trace-based workloads that validates the system‚Äôs ability to maintain high GPU utilization (batch density) while guaranteeing fairness across different tenant classes.

---

## üìâ Performance Results

*   **Training Efficiency:** MLP models are trainable on small datasets, requiring only **100 samples per application**.
*   **Resource Quantification:** The system effectively quantifies resource demand using 'KV token-time.'
*   **Overhead Reduction:** Compared to the S3 baseline which uses a 66 million parameter DistilBERT model with non-negligible overhead, Justitia achieves improved efficiency with minimal computational burden.
