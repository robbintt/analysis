---
title: 'Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with
  ML and Non-ML Software'
arxiv_id: '2601.06266'
source_url: https://arxiv.org/abs/2601.06266
generated_at: '2026-01-27T22:05:55'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software

*Manel Abdellatif, Taher A. Ghaleb, Niruthiha Selvanayagam*

---

### ðŸ“Œ Quick Facts: Key Metrics

| Metric | Details |
| :--- | :--- |
| **Study Scope** | Large-scale analysis of GitHub repositories (LLM, ML, and Non-ML) |
| **Prediction Accuracy** | ML: 96% \|\| LLM: 82% \|\| Non-ML: 74% |
| **Primary Debt Types** | Design Debt, Documentation Debt, Prompt Engineering |
| **Debt Persistence** | First SATD removal rate: <5% |
| **Evaluation** | Quality Score: 7/10 \|\| References: 40 Citations |

---

## Executive Summary

> This research addresses the critical lack of understanding regarding Self-Admitted Technical Debt (SATD) within the emerging domain of Large Language Model (LLM) software. While technical debt is a well-known challenge in traditional software engineering, LLM applications introduce unique complexities such as non-deterministic outputs, rapid API evolution, and the management of prompts.
>
> **Why this matters:** Without identifying these distinct patterns, developers risk accumulating unmanageable maintenance costs and stability issues in AI systems that differ significantly from those in traditional non-ML or standard ML projects.
>
> **The Approach:** The study offers the first large-scale, comparative empirical analysis across LLM-based, traditional ML, and non-ML software. Utilizing automated text-mining (regex/pattern matching) and manual validation, the researchers identified AI-specific debt categories often missed by generic tools.
>
> **The Takeaway:** SATD is significantly more prevalent in LLM software than in non-ML software. The study reveals that traditional taxonomies are inadequate for AI artifacts, implicitly mandating new categories for prompts and fine-tuning configurations. Practitioners should focus on debt hotspots in deployment, monitoring, and pretraining, while integrating automated detection into CI/CD pipelines to flag high-risk technical debt early.

---

## Key Findings

*   **Prevalence:** SATD is significantly more prevalent in LLM-based software compared to traditional non-ML software and is generally comparable to or higher than traditional ML projects.
*   **Debt Types:** Unlike the code debt common in traditional software, LLM projects suffer disproportionately from:
    *   **Design Debt:** Specifically regarding prompt management and integration.
    *   **Documentation Debt:** Related to model behavior explanations.
*   **Unique Categories:** Unique debt types specific to LLMs were identified, including **Prompt Engineering** and **Model Context Management**.
*   **Persistence:** SATD in LLM software often remains unresolved for longer periods due to rapid API evolution and the complexity of debugging non-deterministic outputs.
*   **Risk Factors:** The first SATD introduced in a file has a **<5% removal rate**. Commits with >500 lines or >10 files correlate with 3â€“4 times longer debt persistence across all project types.

---

## Methodology

The authors employed a rigorous mixed-method approach to ensure comprehensive coverage and validation:

1.  **Data Collection:** Conducted a large-scale empirical analysis of open-source repositories hosted on GitHub, comparing three distinct groups:
    *   **LLM-based software**
    *   **Traditional ML software**
    *   **Non-ML software**
2.  **Automated Mining:** Utilized automated text-mining techniques, specifically regular expressions and pattern matching, to identify comments containing SATD indicators.
3.  **Manual Validation:** A statistically significant sample of extracted comments was manually analyzed and classified into established SATD categories to validate the automated findings and uncover LLM-specific nuances.

---

## Technical Details

*   **Classification Model:** The study uses a machine learning-based classification approach to predict debt persistence, categorizing SATD instances into:
    *   **Long-lasting:** Top 25%
    *   **Quickly-removed:** Bottom 25%
*   **Feature Engineering:** The models utilize a comprehensive set of metrics including:
    *   *Code Volume:* LOC, tokens, lines added/modified
    *   *Complexity:* Cyclomatic complexity
    *   *History:* File changes and historical metrics
*   **Domain Analysis:**
    *   **LLM:** Prompt engineering/inference repositories
    *   **ML:** Experimental workflow repositories
    *   **Non-ML:** UI, databases, and business logic repositories
*   **Integration Strategy:** The approach suggests integrating prediction models into CI/CD pipelines for automated flagging and threshold-based alerts.

---

## Results

The analysis yielded distinct behavioral patterns and predictor strengths across different software domains:

*   **Prediction Accuracy by Domain:**
    *   **ML Repositories:** 96%
    *   **LLM Repositories:** 82%
    *   **Non-ML Repositories:** 74%
*   **Feature Importance (Top Predictors):**
    *   **ML Projects:** Historical file changes (Importance: 0.96)
    *   **Non-ML Projects:** Cyclomatic complexity (Importance: 0.74)
    *   **LLM Projects:** Lines added in commit (Importance: 0.190)
*   **Hotspots:** In LLM projects, SATD is heavily concentrated in **deployment**, **monitoring**, and **pretraining** stages.

---

## Contributions

1.  **Foundational Study:** Provided the first large-scale comparative empirical study specifically distinguishing SATD characteristics in LLM software against both ML and non-ML counterparts.
2.  **Taxonomy Critique:** Highlighted the inadequacy of traditional SATD taxonomies for LLM software, implicitly suggesting the need for new categories that encompass AI-specific artifacts like prompts and fine-tuning configurations.
3.  **Actionable Insights:** Offered developers and managers actionable insights regarding technical debt hotspots in LLM applications, emphasizing the need for better documentation strategies and architectural design.