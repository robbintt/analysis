# Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification

*Mohammad Amin Zadenoori; Vincenzo De Martino; Jacek Dabrowski; Xavier Franch; Alessio Ferrari*

---

> ### ðŸ“Š Quick Facts
> 
> *   **Models Analyzed:** 8 (3 LLMs vs. 5 SLMs)
> *   **Parameter Gap:** SLMs are 100â€“300x smaller
> *   **Primary Metric:** F1 Score & Recall
> *   **Performance Differential:** Average 2% higher F1 for LLMs (Statistically Insignificant)
> *   **Quality Score:** 3/10
> *   **Citations:** 19

---

## Executive Summary

As the Requirements Engineering (RE) field increasingly adopts Large Language Models (LLMs) for tasks like requirements classification, organizations face significant trade-offs regarding computational cost, data privacy, and the latency of cloud-based APIs. While LLMs offer advanced reasoning capabilities, their necessity for specific classification tasksâ€”categorizing requirements into functional or non-functional bucketsâ€”remains questioned in light of their resource intensity.

This paper addresses the critical problem of whether the substantial overhead of LLMs is justified for classification, or if smaller, more efficient models can achieve comparable results without the associated operational and security risks. To resolve this uncertainty, the authors conducted a rigorous comparative study evaluating eight top-ranked language models against three specific public datasets: **PROMISE**, **PROMISE Reclass**, and **SecReq**.

The study contrasts three Large Language Models against five Small Language Models (SLMs) ranging from 7 to 8 billion parameters. The findings reveal a negligible performance gap, with LLMs achieving an average F1 score only 2% higher than SLMsâ€”a difference deemed statistically insignificant. Crucially, the study identified **Dataset Sensitivity** as a decisive factor, demonstrating that dataset characteristics influenced performance outcomes more than model size alone. SLMs delivered this competitive performance while being 100 to 300 times smaller in parameter size.

---

## Key Findings

*   **Negligible Performance Gap:** LLMs achieved an average F1 score only 2% higher than SLMs. This difference was determined to be statistically insignificant.
*   **Superior Recall:** SLMs matched LLM performance across most datasets and notably outperformed LLMs in recall metrics on the PROMISE Reclass dataset.
*   **Efficiency:** SLMs delivered comparable accuracy while being up to 300 times smaller in parameter size.
*   **Dataset Sensitivity:** Dataset characteristics (such as domain and structure) influenced performance more significantly than model size alone.

---

## Methodology

The study was designed as a preliminary comparative analysis focusing on Requirements Engineering tasks. The research design included the following components:

1.  **Model Selection:** Evaluated eight distinct language models, divided into two categories:
    *   **3 Large Language Models (LLMs):** Top-tier models in the 1â€“2 trillion parameter range.
    *   **5 Small Language Models (SLMs):** Efficient models in the 7â€“8 billion parameter range.
2.  **Datasets:** Utilized three specific public datasets to ensure broad applicability:
    *   PROMISE
    *   PROMISE Reclass
    *   SecReq
3.  **Evaluation Metrics:** Performance was measured primarily using **F1 scores** and **Recall** metrics to determine statistical significance and accuracy differences.

---

## Technical Details

### Domain & Task
*   **Field:** Requirements Engineering (RE)
*   **Task:** Requirements Classification (Categorizing requirements into functional vs. non-functional or fine-grained classes).

### Models Evaluated
Models were selected based on top rankings in the Hugging Face OpenLLM Leaderboard.

| Category | Models (Parameter Range) |
| :--- | :--- |
| **Small Language Models (SLMs)** | **Qwen2-7B**, **Falcon-7B**, **Granite-3.2-8B**, **Ministral-8B**, **Llama-3-8B** (7-8B) |
| **Large Language Models (LLMs)** | **GPT-5**, **Grok-4**, **Claude-4** (1-2T) |

### Datasets
*   **PROMISE**
*   **PROMISE Reclass**
*   **SecReq**

---

## Results

The analysis revealed that the best-performing LLM was **Claude-4**, while the best-performing SLM was **Llama-3-8B**.

### Head-to-Head Comparison (F1 Scores)

| Dataset | Claude-4 (LLM) | Llama-3-8B (SLM) |
| :--- | :---: | :---: |
| **PROMISE** | 0.81 | 0.76 |
| **PROMISE Reclass** | 0.80 | 0.78 |
| **SecReq** | 0.89 | 0.88 |

### Key Takeaways
*   **Statistical Insignificance:** The marginal gap in F1 scores does not justify the computational overhead of LLMs for this specific task.
*   **Recall Performance:** SLMs matched or outperformed LLMs in recall, specifically excelling on the PROMISE Reclass dataset.
*   **Parameter Efficiency:** SLMs delivered comparable accuracy with parameter sizes 100 to 300 times smaller than their LLM counterparts.

---

## Contributions

1.  **Validation of SLMs:** Provided empirical evidence that SLMs are a valid alternative to LLMs for requirements classification, challenging the necessity of large models for this task.
2.  **Operational Insights:** Highlighted the distinct advantages of SLMs regarding data privacy, reduced computational cost, and local deployability.
3.  **Factor Analysis:** Clarified that dataset characteristics play a more critical role in shaping model performance than model size alone.