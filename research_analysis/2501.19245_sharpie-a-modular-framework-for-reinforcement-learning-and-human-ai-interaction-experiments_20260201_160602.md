# SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI Interaction Experiments

*HÃ¼seyin AydÄ±n; Kevin Godin-Dubois; Libio Goncalvez Braz; Floris den Hengst; Kim Baraka; Mustafa Mert Ã‡elikok; Andreas Sauter; Shihan Wang; Frans A. Oliehoek*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 8 Citations |
| **Validation Scope** | 7 Distinct Experimental Configurations |
| **Core Focus** | Human-AI Teaming & RL Standardization |

---

## Executive Summary

The research addresses a critical fragmentation in tools available for studying Human-AI Interaction (HAI) within the context of Reinforcement Learning (RL). Currently, researchers face a significant barrier: generic RL frameworks lack the necessary interfaces for human participation, while standard behavioral experiment platforms lack support for sophisticated AI agents and real-time interaction. This absence of a unified, generic tool forces researchers to build custom, one-off infrastructures for every experiment, stifling reproducibility and limiting the scope of study to specific, often constrained, scenarios such as unidirectional communication or single-agent interactions.

The authors introduce **SHARPIE** (Shared Human-AI Reinforcement Learning Platform for Interactive Experiments), a modular, open-source framework designed as middleware between standard RL environments and human participants. Technically, SHARPIE utilizes versatile wrappers that interface with standard RL environments (e.g., PettingZoo, Gym) without requiring modifications to the underlying code. The system features a React-based web interface for real-time human interaction and a robust backend that manages experiment logic, data logging, and cloud deployment. This architecture supports a wide range of interaction modalities, including teaching via demonstrations, action delegation, text-based task specification, policy visualization, and intention sharing for teaming.

The frameworkâ€™s versatility was empirically validated through seven diverse experimental configurations that demonstrate its capacity to handle complex, multi-agent scenarios. These experiments ranged from Multi-agent Teaching in "Simple Spread" and Action Delegation in "Mountain Car" to Task Specification in "Minecraft" and Shared Decision Support in "Water-reservoir management." Comparative analysis highlights SHARPIE's superiority over existing tools: it successfully overcomes the single-agent restriction of HIPPO-GYM, the one-way communication limitations of platforms like Knierim et al., and the lack of AI entities in standard behavioral tools, thereby successfully enabling complex, bidirectional, and multi-agent interactions.

SHARPIE represents a significant advancement in the standardization of human-RL research methodologies. By providing a generic, end-to-end interface, it empowers researchers to investigate intricate questions regarding human-AI collaboration, delegation, and teaming without the overhead of developing custom infrastructure. This framework lowers the barrier to entry for rigorous experimentation in human-AI synergy, allowing the research community to focus on algorithmic and behavioral insights rather than software engineering, ultimately accelerating the development of AI systems that can effectively collaborate with humans in complex environments.

---

## Key Findings

*   **Framework Introduction:** Introduction of **SHARPIE** (Shared Human-AI Reinforcement Learning Platform for Interactive Experiments), a framework bridging generic RL and human-AI needs.
*   **Modular Architecture:** The system features a modular architecture comprising versatile wrappers, a web interface, and integrated logging capabilities.
*   **Broad Capabilities:** It supports extensive research capabilities, including interactive reward specification and human-AI teaming.
*   **Standardization Goal:** A core objective is establishing a generic interface to standardize the study of RL within human contexts.

---

## Methodology

The methodology centers on developing an **open, modular software framework** that acts as middleware between RL agents and human participants. This process involves:

*   **Versatile Wrappers:** Creating wrappers for standard RL environments to seamlessly integrate them into the framework.
*   **Web Interface:** Designing a participant-facing web interface to facilitate real-time interaction between humans and agents.
*   **Utilities:** Implementing utilities for comprehensive data logging.
*   **Deployment:** Enabling deployment on cloud infrastructure and recruitment platforms to reach diverse participant pools.

---

## Technical Details

SHARPIE is designed to bridge standard Reinforcement Learning algorithms with Human-AI Interaction requirements.

### System Architecture
*   **Modular Framework:** Interfaces between standard environments and human needs.
*   **Wrappers:** Adapts standard RL algorithms (e.g., PettingZoo, Gym) without modifying underlying code.
*   **Backend/Interface:** Robust backend for logic management paired with a React-based web front-end.

### Scalability & Support
*   **Multi-scale Participation:** Supports configurations ranging from 1-to-1 to many-to-many interactions.

### Interaction Modalities
*   **Teaching:** Learning via demonstrations.
*   **Delegation:** Action delegation capabilities.
*   **Task Specification:** Text communication for defining tasks.
*   **Visualization:** Policy visualization for utility elicitation.
*   **Teaming:** Intention sharing to support collaborative teaming.

---

## Results

The framework validated its versatility through **seven distinct experimental configurations**:

1.  **Multi-agent Teaching:** Simple Spread
2.  **Action Delegation:** Mountain Car
3.  **Task Specification:** Minecraft
4.  **Human-AI Teaming:** Simple Tag
5.  **Utility Elicitation:** Deep Sea Treasure
6.  **Shared Decision Support:** Water-reservoir management

### Comparative Analysis
*   **vs. HIPPO-GYM:** Overcomes single-agent restrictions.
*   **vs. Knierim et al.:** Supports bidirectional communication (overcomes one-way limits).
*   **vs. Standard Behavioral Platforms:** Includes sophisticated AI entities (often lacking in behavioral tools).
*   **vs. RLHF Platforms:** Expands scope beyond restricted RLHF use cases.

**Conclusion:** SHARPIE confirms successful support for complex multi-agent and bidirectional interactions.

---

## Contributions

*   **End-to-End Platform:** Provision of a comprehensive platform addressing the specific lack of generic tools for human-RL experiments.
*   **Standardization:** Implementation of a generic interface for human-RL interactions to standardize research methodologies.
*   **Researcher Empowerment:** Empowerment of researchers to investigate complex human-AI collaboration questions (e.g., teaming, delegation) without needing to build custom infrastructure.