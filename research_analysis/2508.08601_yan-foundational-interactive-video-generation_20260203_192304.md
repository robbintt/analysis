---
title: 'Yan: Foundational Interactive Video Generation'
arxiv_id: '2508.08601'
source_url: https://arxiv.org/abs/2508.08601
generated_at: '2026-02-03T19:23:04'
quality_score: 8
citation_count: 10
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Yan: Foundational Interactive Video Generation

*Deheng Ye; Fangyun Zhou; Jiacheng Lv; Jianqi Ma; Jun Zhang; Junyan Lv; Junyou Li; Minwen Deng; Mingyu Yang; Qiang Fu; Wei Yang; Wenkai Lv; Yangbin Yu; Yewen Wang; Yonghang Guan; Zhihao Hu; Zhongbin Fang; Zhongqian Sun*

---

> ### ðŸ“Š Quick Facts
> *   **Resolution:** 1080P
> *   **Performance:** 60 FPS Real-time
> *   **Dataset Size:** 400M interactive video data points
> *   **Scenarios:** >90 scenario styles
> *   **Action Space:** 8-dimensional
> *   **Key Modules:** Yan-Sim, Yan-Gen, Yan-Edit
> *   **Comparison:** Outperforms The Matrix, PlayGen, and GameNGen

---

## Executive Summary

This research addresses the critical challenge of transforming high-fidelity video diffusion models into real-time, interactive engines capable of simulating complex game environments. Existing generative models typically produce static or pre-computed sequences that lack the low-latency responsiveness required for interactive applications, such as video games or simulators. Bridging the gap between cinematic video quality and the strict, frame-by-frame controllability of interactive mechanics is essential for advancing AI-driven media but remains computationally prohibitive due to the high resource demands of diffusion inference.

The paper introduces **"Yan,"** a foundational framework composed of three distinct modules: **Yan-Sim**, **Yan-Gen**, and **Yan-Edit**. To achieve real-time performance, Yan-Sim employs a highly compressed 3D-VAE alongside a KV-cache-based shift-window denoising process, drastically reducing inference latency. Yan-Gen converts standard video diffusion models into interactive generators by injecting game-specific knowledge through a hierarchical autoregressive captioning method, enabling frame-wise, action-controllable generation. Additionally, Yan-Edit utilizes a hybrid architecture that decouples depth-driven mechanics simulation from text-guided visual rendering, allowing for multi-granularity editing where gameplay mechanics and visual styles can be modified independently.

Yan successfully delivers high-fidelity simulation at 1080P resolution and 60 FPS with nearly delay-free latency, meeting the rigorous demands of real-time interaction. The system is trained on a massive dataset of 400 million interactive video data points, encompassing over 90 scenario styles and an 8-dimensional action space, collected via automated agents in the game "Yuan Meng Star." In comparative benchmarks, Yan significantly outperforms existing baselinesâ€”including The Matrix, PlayGen, and GameNGenâ€”by achieving superior resolution and scale while maintaining the strict real-time performance required for interactive play.

This work represents a paradigm shift in AI creation, establishing a comprehensive pipeline that integrates simulation, generation, and editing into a unified system. By demonstrating that open-domain video diffusion models can function as interactive engines, Yan lays the groundwork for a new class of AI-driven creative tools and media. This advancement enables the rapid prototyping of complex interactive experiences where users can manipulate both game mechanics and visual aesthetics via text prompts, fundamentally changing how dynamic digital content is created and consumed.

---

## Key Findings

*   **Real-time Simulation Performance:** Achieves high-fidelity, real-time interactive simulation at **1080P/60FPS** using a highly-compressed 3D-VAE and a KV-cache-based shift-window denoising inference process.
*   **Cross-Domain Generalization:** The model flexibly blends styles and mechanics from different domains based on textual and visual prompts.
*   **Infinite Action-Controllable Generation:** Transforms open-domain Video Diffusion Models into frame-wise, action-controllable generators by injecting game-specific knowledge.
*   **Disentangled Editing:** The hybrid model decouples interactive mechanics simulation from visual rendering, permitting multi-granularity video content editing via text prompts.

---

## Methodology

The Yan framework utilizes a comprehensive three-module pipeline designed to bridge the gap between static video generation and interactive simulation:

1.  **AAA-level Simulation Module (Yan-Sim):**
    Employs a highly-compressed, low-latency **3D-VAE** and **KV-cache-based shift-window denoising** to facilitate real-time inference without sacrificing fidelity.

2.  **Multi-Modal Generation Module (Yan-Gen):**
    Converts Video Diffusion Models into interactive engines. It uses a **hierarchical autoregressive caption method** to inject game-specific knowledge, allowing for prompt-controllable, frame-by-frame generation across domains.

3.  **Multi-Granularity Editing Module (Yan-Edit):**
    Utilizes a hybrid model architecture to disentangle mechanics simulation from visual rendering. This separation enables granular, text-based edits to the video content.

---

## Technical Details

### Core Architecture
The paper proposes 'Yan', a foundational framework transforming open-domain Video Diffusion Models into interactive generators. It consists of three specific sub-systems:

*   **Yan-Sim:** Utilizes a 3D-VAE and KV-cache-based shift-window denoising for high-fidelity, real-time simulation.
*   **Yan-Gen:** Employs hierarchical captioning for prompt-controllable cross-domain generation.
*   **Yan-Edit:** Disentangles a depth-driven mechanics simulator from a text-guided renderer to allow for real-time editing.

### Data Pipeline
*   **Collection Environment:** Data is collected via an automated pipeline within the game *Yuan Meng Star*.
*   **Agents:** Uses a hybrid Random and PPO RL agent for gameplay data generation.
*   **Annotation:** Annotated using Qwen2.5-VL and DepthFM-ID.
*   **Filtering:** Data is filtered through visual, anomaly, and rule-based filters to ensure quality.
*   **Alignment:** Actions are aligned with images via timestamp matching and interpolation, converting data from **10Hz to 30FPS**.

---

## Results

*   **Performance Metrics:** Yan achieves a resolution of **1080P** and an inference speed of **60 FPS**.
*   **Latency:** The system operates with low latency, targeting nearly delay-free image-action pair sequences.
*   **Dataset Scale:** The training dataset consists of **400M** interactive video data points covering **>90** scenario styles with an 8-dimensional action space.
*   **Benchmark Comparisons:** Comparative analysis shows Yan significantly outperforms baselines like *The Matrix*, *PlayGen*, and *GameNGen* in resolution and scale while maintaining real-time performance.

---

## Contributions

*   **Holistic Interactive Framework:** Establishes a comprehensive framework integrating simulation, generation, and editing into a single pipeline.
*   **Advancement of Real-time Interaction:** Introduces a technical solution (3D-VAE + KV-cache shift-window denoising) that bridges high-quality video generation with strict real-time latency requirements.
*   **Paradigm Shift in AI Creation:** Paves the way for a new AI-driven interactive creation paradigm for next-generation creative tools and media.

---

**Quality Score:** 8/10  
**References:** 10 citations