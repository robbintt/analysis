# Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone

*Negar Hassanpour; Muhammad Kamran Janjua; Kunlin Zhang; Sepehr Lavasani; Xiaowen Zhang; Chunhua Zhou; Chao Gao*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Paper Quality Score** | 7/10 |
| **Total Citations** | 40 |
| **Core Innovation** | Angular Constraint Mechanism (ConicGrad) |
| **Primary SOTA Result** | CelebA $\Delta m$: **1.31** (vs CAGrad 1.55) |
| **Scalability Limit** | Tested efficiently up to **34.41M** parameters |

---

## Executive Summary

**The Challenge:** Multi-Task Learning (MTL) aims to improve model generalization by leveraging shared representations across tasks. However, it fundamentally struggles with optimizing competing objectives. The core challenge lies in managing **conflicting gradients**, where task-specific updates point in opposing directions; standard averaging often leads to poor convergence and degraded performance.

**The Solution:** The authors introduce **ConicGrad**, a novel optimization framework that reformulates MTL as a constrained geometric problem. Unlike prior methods like CAGrad that rely on Euclidean distance constraints, ConicGrad utilizes a dynamic angular constraint mechanism to regulate gradient updates. It confines the update direction within a cone centered on a reference gradientâ€”the uniform average of all task gradientsâ€”allowing deviations only within an angle of $\arccos(c)$. Implemented as a first-order method, ConicGrad uses a Lagrangian multiplier $\lambda$ and regularization coefficient $\gamma$.

**Impact:** ConicGrad demonstrated state-of-the-art performance across major supervised learning and reinforcement learning benchmarks. The method maintained computational efficiency during scalability tests up to 34.41M parameters, whereas SDMGrad slowed significantly. By delivering superior empirical accuracy while addressing computational bottlenecks, ConicGrad establishes a robust new standard for trade-off negotiation in multi-task systems.

---

## Key Findings

*   **Introduction of ConicGrad:** A novel Multi-Task Learning (MTL) approach designed specifically to address the issue of balancing competing objectives and conflicting gradients.
*   **Angular Constraint Mechanism:** A unique method that confines gradient update directions within a geometric cone centered on the reference gradient.
*   **Computational Efficiency:** The framework is designed for scalability, maintaining performance even in high-dimensional parameter spaces.
*   **State-of-the-Art Performance:** Demonstrated superiority across both supervised learning and reinforcement learning benchmarks.

---

## Methodology

The researchers formulate Multi-Task Learning as a **constrained optimization problem**. The proposed ConicGrad method introduces an angular constraint to dynamically regulate gradient update directions.

Specifically, the method confines these updates within a geometric cone centered on the **reference gradient** (derived from the overall objective). This geometric approach allows the algorithm to balance task-specific gradients naturally, avoiding the pitfalls of simple averaging which often fails when gradients conflict.

---

## Technical Details

**Framework Overview**
ConicGrad is a Multi-Task Learning (MTL) framework that addresses conflicting gradients by constraining the update direction within a geometric cone centered on a reference gradient.

**Mathematical Formulation**
*   **Reference Objective:** Calculated as the uniform average of all task gradients.
*   **Angular Constraint:** The method allows the update vector any direction within a conic section around the reference gradient where the angle does not exceed $\arccos(c)$.
    *   *Example:* $c=0.5$ allows an angle of $\le 60^\circ$.

**Differentiation from Existing Methods**
*   **Vs. CAGrad:** CAGrad uses Euclidean distance constraints, whereas ConicGrad uses angular constraints.
*   **Vs. SDMGrad:** ConicGrad is a first-order method requiring explicit gradient computation. It avoids the multi-step optimization or multiple forward passes required by SDMGrad.

**Optimization Parameters**
*   **Regularization Coefficient:** $\gamma$
*   **Lagrangian Multiplier:** $\lambda$

---

## Contributions

*   **Principled Optimization Framework:** Provides a rigorous constrained optimization formulation for MTL.
*   **Novel Gradient Regulation:** Introduces a cone-based angular constraint for managing gradient conflict.
*   **Scalability Solution:** Addresses computational bottlenecks typically found in high-dimensional parameter spaces.
*   **Broad Empirical Validation:** Demonstrates state-of-the-art results across both supervised learning and reinforcement learning domains.

---

## Results & Evaluation

**Metrics**
*   Average Performance Drop ($\Delta m\%$) relative to Single Task Learning (STL).
*   Mean Rank (MR).

**Benchmark Performance**
*   **Toy Example (2-Task):** Both CAGrad and ConicGrad reached global minima for all 5 initialization points, outperforming NashMTL (3 points) and FAMO. ConicGrad achieved the lowest loss significantly faster.
*   **CelebA:** Achieved $\Delta m$ of **1.31**, outperforming CAGrad (1.55) and FAMO (1.69).
*   **NYUv2:** Achieved $\Delta m$ of **12.91**, substantially improving upon CAGrad (15.20) and NashMTL (17.44).

**Scalability Analysis**
*   Tested on CelebA with scaling model parameters up to 34.41M.
*   **Result:** ConicGrad maintained computational efficiency (time per epoch), whereas SDMGrad experienced significant slowdowns.

**Ablation Studies**
The study analyzed hyperparameters $c$ (conic constraint) and $\gamma$ (regularization):
*   **Regularization ($\gamma$):** CityScapes and CelebA preferred smaller $\gamma$; NYUv2 preferred larger $\gamma$.
*   **Alignment ($c$):** CelebA and NYUv2 preferred $c \ge 0.5$ (strict alignment), while CityScapes preferred $c = 0.25$ (allowing more deviation).

---