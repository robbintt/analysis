# Reducing Reward Dependence in RL Through Adaptive Confidence Discounting

*Muhammed Yusuf Satici; David L. Roberts*

---

> ### ðŸš€ Quick Facts
> **Feedback Reduction:** 80% | **Rewards Requested:** As low as 20% | **Domains Validated:** 3 | **Quality Score:** 9/10

---

## Executive Summary

Reinforcement learning (RL) algorithms traditionally rely on dense, consistent reward signals to optimize agent policies effectively. In real-world applications, howeverâ€”particularly those involving human-in-the-loop feedback or computationally expensive reward functionsâ€”obtaining feedback at every time step presents a prohibitive logistical and financial bottleneck. This dependency on constant external validation renders standard RL infeasible for many high-value tasks, as the cost of queries during long training sessions outweighs the benefits of automation.

The authors introduce **"Adaptive Confidence Discounting,"** a bi-objective optimization framework designed to maximize cumulative reward while simultaneously minimizing the frequency of feedback requests. The approach utilizes a Reward Function Model (RFM) to provide proxy feedback when the agent's confidence is high, switching to Selective Reward Acquisition to query actual rewards only during periods of low confidence. Technically, confidence is quantified using the entropy of the Actor/Critic distributions or a combined entropy of the actor and RFM.

The system is implemented using DQN and A2C architectures (augmented with Hindsight Experience Replay for robotics tasks), prioritizing feedback efficiency over traditional sample efficiency in deterministic Markov Decision Processes. Evaluation across discrete grid-world, continuous robotics, and dense highway environments demonstrates that the proposed algorithm achieves asymptotic performance and time-to-convergence comparable to baseline methods (DQN, A2C, HER). Crucially, the agent maintained policy quality in terms of total return and learning speed while requesting as few as 20% of the total rewards.

This corresponds to an **80% reduction in feedback dependency**, validating the hypothesis that agents can learn effectively without exhaustive external validation. This research addresses a critical feasibility bottleneck in applying RL to expensive feedback domains. By demonstrating that learning can be decoupled from the need for constant reward signals, the study establishes a new paradigm for cost-effective training in human-in-the-loop systems.

---

## Key Findings

*   **Comparable Performance:** The proposed algorithm achieves performance on par with baseline methods regarding total return and the number of episodes required for learning.
*   **Significant Cost Reduction:** Successful learning was demonstrated while requesting as few as **20% of total rewards**, representing an 80% reduction in feedback dependency.
*   **Logistical Flexibility:** The approach enables efficient learning in environments where obtaining feedback at every step is logistically difficult or computationally expensive.
*   **Consistent Policy Quality:** Policy quality is maintained despite the significant reduction in reliance on human-in-the-loop feedback or expensive reward calculations.

---

## Methodology

The research introduces a framework designed to decouple the learning process from constant external validation through three main components:

*   **Proxy Modeling**
    *   Utilizes a reward function model (RFM) to serve as a proxy for human-delivered or expensive rewards.
    *   The proxy is actively used when the agent's confidence in the state-action value is high.

*   **Adaptive Confidence Discounting**
    *   Implements a bi-objective optimization to evaluate the agent's confidence regarding the value of actions in a specific state.
    *   Functions as the core mechanism to determine when to rely on the model versus seeking external truth.

*   **Selective Reward Acquisition**
    *   Dynamically switches between using the model and requesting explicit rewards.
    *   Explicit rewards are requested only when the agent exhibits low confidence in the model's predicted rewards or action selection, significantly cutting costs.

---

## Technical Details

**Core Approach**
*   **Adaptive Confidence Discounting:** A bi-objective optimization maximizing cumulative reward while minimizing feedback requests.
*   **Reward Function Model (RFM):** Provides proxy feedback during high-confidence (low entropy) intervals.
*   **Confidence Measurement:** Quantified via the entropy of Actor/Q-value distributions or a combined entropy of the actor and RFM.
*   **Regularization:** Terms are included to control reward reduction rates.

**Architectures & Implementations**
*   **DQN:** Utilizes two 4-layer fully-connected networks trained with mean squared Bellman error.
*   **A2C (Actor-Critic):**
    *   *Actor:* 4-layer fully-connected network outputting mean/std.
    *   *Critic:* 4-layer fully-connected network.
*   **Robotics Extension:** Employs A2C with **Hindsight Experience Replay (HER)** using the Future strategy ($k=4$).

**Optimization Strategy**
*   **Primary Goal:** Optimizes *Feedback Efficiency* (cost of reward signals) over Sample Efficiency.
*   **Context:** Applied within a deterministic MDP framework.

---

## Contributions

*   **Novel Algorithm Design:** Provides a reinforcement learning algorithm specifically engineered to function efficiently under the constraints of expensive or human-in-the-loop reward systems.
*   **Confidence-Based Framework:** Introduces a framework that minimizes training costs by dramatically reducing the frequency of reward queries without degrading agent performance.
*   **Feasibility Breakthrough:** Addresses the training bottleneck in high-cost feedback environments by decoupling the learning process from the need for constant external validation.

---

## Results

The evaluation focused on **Asymptotic Performance** and **Time to Convergence** (measured by the number of feedback signals) across three distinct domains:

1.  **Discrete sparse-reward grid-world**
2.  **Continuous sparse-reward robotics environment**
3.  **Continuous dense-reward highway environment**

**Outcome:**
The algorithm achieved successful learning while requesting as few as 20% of total rewards. Despite this reduction, policy quality remained comparable to baselines (DQN, A2C, HER) in terms of total return and episodes required.

***

**Quality Score:** 9/10 | **References:** 31 citations