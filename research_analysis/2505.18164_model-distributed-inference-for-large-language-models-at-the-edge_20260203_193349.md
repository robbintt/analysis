---
title: Model-Distributed Inference for Large Language Models at the Edge
arxiv_id: '2505.18164'
source_url: https://arxiv.org/abs/2505.18164
generated_at: '2026-02-03T19:33:49'
quality_score: 9
citation_count: 30
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Model-Distributed Inference for Large Language Models at the Edge

*Davide Macario; Hulya Seferoglu; Erdem Koyuncu*

---

> ### **Quick Facts: Key Metrics**
> *   **Framework:** MDI-LLM (Model-Distributed Inference)
> *   **Core Technique:** Recurrent Pipeline Parallelism
> *   **Test Model:** TinyLlama 1.1B
> *   **Memory Reduction:** 28.7% decrease per device (2 to 3 nodes)
> *   **Throughput Increase:** 2.56 â†’ 3.83 tokens/sec
> *   **Quality Score:** 9/10

---

## Executive Summary

State-of-the-art Large Language Models (LLMs) demand substantial memory and computational resources, exceeding the capabilities of individual low-power, low-cost edge devices. This hardware disparity currently forces LLM inference to rely on expensive, monolithic server infrastructure, preventing the deployment of advanced AI capabilities directly at the edge. This paper addresses the challenge of overcoming these memory constraints by enabling the execution of large models across networks of resource-constrained devices, a critical step toward decentralized AI.

The key innovation is the **MDI-LLM framework**, which introduces "recurrent pipeline parallelism," a novel scheduling and execution technique designed for distributed inference. Technically, the method partitions the LLM into distinct segments, assigning each partition to a different node within a heterogeneous edge network. These nodes collaboratively perform inference by exchanging intermediate activation vectors via device-to-device communication links. By orchestrating this workflow to minimize device idle time and incorporating optimizations such as KV Caching and Grouped Query Attention (GQA), the framework facilitates parallel processing specifically during the generation of multiple text sequences, significantly enhancing resource utilization.

Experiments utilizing the TinyLlama 1.1B model validated the system's ability to run models that cannot fit on a single edge device. Scaling the network from 2 to 3 nodes reduced the memory consumption per device by **28.7%**, dropping from 4.57 GB to 3.26 GB. In terms of performance, this scaling resulted in a tangible increase in token generation throughput, rising from 2.56 tokens/sec to 3.83 tokens/sec. While the total network-wide memory overhead increased slightly from 9.14 GB to 9.78 GB due to library replication, the system successfully achieved higher throughput and lower per-device resource footprints as the number of participating devices increased.

The significance of this research lies in its potential to democratize LLM deployment by shifting high-performance inference from centralized, costly servers to aggregated, low-cost edge hardware. This approach removes the necessity for monolithic infrastructure, making state-of-the-art AI accessible in environments with strict power and cost limitations. By proving that scalable, efficient inference is possible on distributed edge networks, MDI-LLM opens new avenues for deploying sophisticated language models in real-world, decentralized applications.

---

## Key Findings

*   **Overcoming Hardware Limitations:** MDI-LLM enables the deployment of state-of-the-art LLMs on low-power, low-cost edge devices that individually lack the memory capacity to host the full model.
*   **Scalable Performance:** As the number of participating edge devices increases, the system achieves higher token generation throughput.
*   **Reduced Resource Footprint:** Increasing the number of devices in the network results in lower memory consumption per individual device.
*   **Efficient Resource Utilization:** The framework minimizes idle time on devices and supports parallel inference specifically during the generation of multiple text sequences.

---

## Methodology

The research employs a distributed computing approach to bypass the hardware limitations of individual edge devices.

*   **Model Partitioning:** The LLM is divided into multiple distinct partitions, with each partition assigned to a different node or device within the network.
*   **Collaborative Computation:** Nodes perform collaborative inference by exchanging intermediate activation vectors via device-to-device communication links.
*   **Optimization Technique:** The authors propose "recurrent pipeline parallelism," a technique designed to orchestrate the workflow to reduce device idle time and facilitate parallel processing across the distributed nodes.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Core Framework** | MDI-LLM |
| **Scheduling Strategy** | Recurrent Pipeline Parallelism (reduces latency, enables parallel sequence generation) |
| **Memory Optimization** | KV Caching |
| **Attention Mechanism** | Grouped Query Attention (GQA) |
| **Architecture** | Partitioned LLM across multiple low-power edge devices |

---

## Results

Experiments using the **TinyLlama 1.1B** model demonstrated significant improvements in resource distribution and performance:

*   **Feasibility:** The model could not fit on a single edge device, but ran successfully on the distributed network.
*   **Memory Efficiency:** Memory consumption per device dropped from **4.57 GB** (2 nodes) to **3.26 GB** (3 nodes).
    *   *Total Reduction:* 1.31 GB per device (**28.7%**).
*   **Network Overhead:** Total network-wide memory overhead increased slightly from 9.14 GB to 9.78 GB due to library replication.
*   **Performance:** Throughput scaled from **2.56 tokens/sec** to **3.83 tokens/sec** as the network scaled.

---

## Contributions

*   **MDI-LLM Framework:** Introduction of a novel framework specifically designed to facilitate distributed inference of large language models across heterogeneous, low-power edge networks.
*   **Recurrent Pipeline Parallelism:** The proposal of a new scheduling and execution technique that enhances efficiency by reducing latency and enabling parallel generation of text sequences.
*   **Democratization of LLM Deployment:** A solution that allows high-performance LLM inference on aggregated low-cost hardware, removing the necessity for expensive, monolithic server infrastructure.

---

**Paper Quality Score:** 9/10  
**References:** 30 citations