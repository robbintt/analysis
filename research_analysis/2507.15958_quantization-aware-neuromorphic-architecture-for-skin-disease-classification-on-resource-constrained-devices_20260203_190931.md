---
title: Quantization-Aware Neuromorphic Architecture for Skin Disease Classification
  on Resource-Constrained Devices
arxiv_id: '2507.15958'
source_url: https://arxiv.org/abs/2507.15958
generated_at: '2026-02-03T19:09:31'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quantization-Aware Neuromorphic Architecture for Skin Disease Classification on Resource-Constrained Devices

*Haitian Wang; Xinyu Wang; Yiren Wang; Bo Miao; Atif Mansoor*

***

### üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Top-1 Accuracy (HAM10000)** | 91.6% |
| **Macro F1 (HAM10000)** | 91.0% |
| **Latency** | 1.5 ms |
| **Energy Consumption** | 1.7 mJ |
| **Target Hardware** | BrainChip Akida |
| **Latency Reduction vs. GPU** | 94.6% |
| **Energy Reduction vs. GPU** | 99.0% |

***

## Executive Summary

The deployment of deep learning models for skin disease classification on edge devices is currently hindered by the prohibitive computational costs and energy consumption of standard Convolutional Neural Networks (CNNs). While Spiking Neural Networks (SNNs) offer a promising alternative for energy-efficient neuromorphic hardware, the direct conversion of high-accuracy CNNs to SNNs typically suffers from conversion instability and significant performance degradation, particularly on imbalanced medical datasets where rare classes are critical.

To address these challenges, the researchers introduce **QANA** (Quantization-Aware Neuromorphic Architecture), an end-to-end pipeline designed specifically for robust CNN-to-SNN conversion and edge deployment. The architecture utilizes a conversion-stable design that substitutes fragile CNN components with spike-compatible transformations and aligns normalization layers with low-bit quantization. To preserve feature extraction capabilities within limited FLOP budgets, the model employs Ghost-based feature generation alongside hardware-compliant attention mechanisms.

QANA demonstrates superior performance on standard benchmarks, achieving **91.6% Top-1 accuracy** on the HAM10000 dataset. When deployed on BrainChip Akida hardware, the system achieved extreme efficiency, processing images in **1.5 ms** with an energy consumption of just **1.7 mJ**. This work validates the viability of neuromorphic architectures for real-world medical diagnostics by successfully bridging the gap between theoretical model efficiency and practical hardware constraints.

***

## Key Findings

*   üèÜ **Superior Classification Performance:** QANA achieved 91.6% Top-1 accuracy and 91.0% macro F1 on HAM10000, and 90.8% Top-1 accuracy and 81.7% macro F1 on a clinical dataset.
*   üìà **Significant Improvement over SNN Baselines:** Outperformed the strongest SNN baseline by up to **3.5 percentage points** in Top-1 accuracy and **12.0 points** in macro F1.
*   ‚ö° **Extreme Hardware Efficiency:** Processes images in **1.5 ms** with **1.7 mJ** energy consumption on BrainChip Akida hardware.
*   üìâ **Drastic Resource Reduction:** Reduced latency by 94.6% and energy consumption by 99.0% compared to GPU-based CNNs.
*   üîÑ **Edge Capability:** Supports incremental model updates on edge hardware without the need for full retraining.

***

## Methodology

The researchers developed QANA, an end-to-end pipeline connecting CNNs with neuromorphic hardware. Key aspects of the methodology include:

*   **Conversion-Stable Design:** Replacing fragile CNN components with spike-compatible transformations and aligning normalization with low-bit quantization.
*   **Efficient Feature Extraction:** Utilizing Ghost-based feature generation to maintain high performance under strict FLOP budgets.
*   **Hardware-Compliant Attention:** Implementing spatially-aware efficient channel attention and squeeze-and-excitation mechanisms optimized for spiking cores.
*   **Incremental Learning:** Employing a quantized projection head to produce SNN-ready logits, facilitating on-device updates.

***

## Technical Details

The QANA framework is structured around a four-stage pipeline. Below are the specific components and configurations used:

### Pipeline Architecture
1.  **Data Preprocessing**
2.  **Quantization-Aware Network**
3.  **CNN-to-SNN Conversion**
4.  **Deployment**

### Network Architecture Components
*   **Backbone:** Iterative backbone utilizing **Ghost Modules** for efficient feature generation.
*   **Attention Mechanisms:** Spatially-aware efficient channel attention and residual connections.
*   **Transformation Stage:** Features bounded activation functions and Squeeze-and-Excitation blocks.
*   **Classification Head:** Quantized projection head designed to generate SNN-ready logits.

### Data Preprocessing Strategy
*   **Resizing:** Images resized to 64x64 pixels.
*   **Cropping:** Saliency-based cropping using Grad-CAM derived from a frozen ResNet-50.
*   **Augmentation:** Flips and luminance/contrast adjustments (hue shifts excluded).
*   **Oversampling:** SMOTE (Synthetic Minority Over-sampling Technique) applied to PCA-whitened 4096-dimensional embeddings to handle class imbalance.

### Target Hardware
*   **Processor:** BrainChip Akida.
*   **Key Feature:** Supports on-chip incremental learning.

***

## Performance Results

| Dataset | Top-1 Accuracy | Macro F1 Score | Improvement over SNN Baseline (Top-1) |
| :--- | :--- | :--- | :--- |
| **HAM10000** | 91.6% | 91.0% | +3.5 percentage points |
| **Clinical** | 90.8% | 81.7% | Significant gain in rare class detection |

### Hardware Efficiency (BrainChip Akida)
*   **Latency:** 1.5 ms per image
*   **Energy:** 1.7 mJ per image
*   **Comparison vs. GPU:**
    *   Latency reduced by **94.6%**
    *   Energy consumption reduced by **99.0%**

***

## Core Contributions

*   **Optimized Edge Healthcare Deployment:** Directly addressed compute cost and energy efficiency constraints for on-device skin lesion analysis.
*   **Robust CNN-to-SNN Conversion:** Introduced a quantization-aware architecture that solves conversion instability and preserves accuracy for rare classes.
*   **Neuromorphic Architecture Innovation:** Proposed architectural modifications, including Ghost-based features and specialized attention mechanisms, specifically compatible with spiking cores.
*   **Validation of Neuromorphic Efficiency:** Provided benchmark data demonstrating order-of-magnitude improvements in energy and latency over traditional GPUs.

***

**Quality Score:** 9/10  
**References:** 40 citations