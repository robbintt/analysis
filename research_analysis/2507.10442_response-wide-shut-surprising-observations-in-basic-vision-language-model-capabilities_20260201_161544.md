# Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities

*Shivam Chandhok; Wan-Cyuan Fan; Vered Shwartz; Vineeth N Balasubramanian; Leonid Sigal*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 18 Citations |
| **Architectures Tested** | Contrastive (CLIP, ALBEF), Encoder-Decoder (CoCa, BLIP-2, InstructBLIP), Instruction-Tuned (LLaVA v.1.5, LLaVA-NEXT) |
| **Core Capabilities Analyzed** | Object Recognition, Instance Delineation (Counting), Spatial Arrangement |

---

> ### ðŸ“ Executive Summary
>
> State-of-the-art Vision-Language Models (VLMs) are widely viewed as high-capability systems, yet they frequently fail at fundamental visual tasks such as object counting, distinct entity recognition, and spatial reasoning. This paper addresses the critical disconnect between a model's internal visual knowledge and its external textual output. The authors argue that standard metrics evaluating only final responses are insufficient, as they fail to detect a "silent failure" mode where the correct visual information is encoded internally but is lost during translation into text. This poses a significant challenge for reliable deployment, as models appear to lack capabilities they actually possess, or conversely, appear capable while relying on non-visual heuristics.
>
> The key innovation is a mechanistic analysis framework that employs component-level probing to evaluate information flow across distinct architectural stages. By deconstructing VLMs into three intermediate feature spacesâ€”Visual Latent, Vision-Language Shared Latent, and Language Responseâ€”the researchers isolate exactly where capabilities are retained or lost. The methodology was rigorously applied to a diverse set of architectures, evaluating three fundamental visual skills.
>
> Experiments reveal a complex performance mismatch. For Object Recognition and Counting, models demonstrated near-ceiling accuracy in internal latent spaces (often exceeding 90%) but suffered catastrophic drops in final Response Space accuracy (e.g., LLaVA v1.5 dropping from ~99% to ~19%). Conversely, Spatial Understanding was weak in early layers but improved in the Response Space, suggesting the decoder compensates for poor visual encoding by relying on textual priors. This study establishes that scaling alone will not fix these issues; specific architectural improvements are required to mitigate bottlenecks.

---

## Key Findings

*   **Capability Gap:** State-of-the-Art (SoTA) Vision-Language Models (VLMs) lack basic visual understanding skills despite being generally considered high-capability tools.
*   **Latent vs. Output Discrepancy:** There are notable performance differences between final text outputs and the latent capabilities of internal architectural components.
*   **Information Loss:** Visual knowledge is often successfully encoded internally but fails to translate into correct responses, indicating specific processing bottlenecks within the architecture.
*   **Insufficiency of Standard Metrics:** Relying solely on final-response metrics is insufficient for identifying model weaknesses and can lead to false conclusions about a model's abilities.

## Methodology

The research employs a rigorous approach designed to look past the final text generation:

*   **Component-Level Probing:** The methodology focuses on fundamental visual tasks by probing specific components of the model architecture rather than just evaluating the final text.
*   **Comparative Analysis:** A direct comparison is made between the VLM's final response performance and linear probes trained on features extracted from three distinct areas:
    1.  The Visual Encoder
    2.  The Intermediate Vision-Language Projection
    3.  The LLM-Decoder Output

## Contributions

*   **Granular Diagnostics:** The paper provides granular architectural diagnostics that identify exactly which specific design components contribute to failures in basic visual tasks.
*   **Rigorous Framework:** It introduces a rigorous evaluation framework that goes beyond standard output accuracy to assess the flow of internal information.
*   **Architectural Guidance:** It offers critical insights on robustness and information processing to guide the design of improved, more reliable VLM architectures.

## Technical Details

The study utilizes a mechanistic analysis framework to deconstruct VLMs into three intermediate feature spaces:

*   **Visual Latent Space:** The output of the visual encoder.
*   **Vision-Language (VL) Shared Latent Space:** The output of the projection module.
*   **Language Response Space:** The output of the decoder.

**Scope of Analysis:**
*   **Capabilities:** Object Recognition, Instance Delineation (counting), and Spatial Arrangement.
*   **Architectures:** 
    *   *Contrastive Multi-Encoder:* CLIP, ALBEF
    *   *Encoder-Decoder Generative:* CoCa, BLIP-2, InstructBLIP
    *   *Instruction Fine-Tuned:* LLaVA v.1.5, LLaVA-NEXT

## Results

The experiments highlight a significant mismatch between latent knowledge and final generation:

*   **Recognition & Counting:**
    *   Models show high proficiency in **Visual** and **VL Projection** spaces.
    *   There is a **significant performance drop** in the **Response Space**.
    *   *Conclusion:* The language decoder acts as a processing bottleneck for these tasks.
    
*   **Spatial Understanding:**
    *   Performance is weak in **Visual** and **VL** spaces.
    *   Performance paradoxically **improves** in the **Response Space**.
    *   *Conclusion:* The decoder relies on textual priors to compensate for weak visual encoding.

*   **Overall Assessment:**
    *   Visual encoders are generally proficient.
    *   VL projections successfully preserve necessary information.
    *   Failures often occur during the decoding process or due to a lack of spatial grounding in earlier layers.