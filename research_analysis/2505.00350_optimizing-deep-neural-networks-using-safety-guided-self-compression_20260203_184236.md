---
title: Optimizing Deep Neural Networks using Safety-Guided Self Compression
arxiv_id: '2505.0035'
source_url: https://arxiv.org/abs/2505.00350
generated_at: '2026-02-03T18:42:36'
quality_score: 9
citation_count: 15
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Optimizing Deep Neural Networks using Safety-Guided Self Compression

*Mohammad Zbeeb; Mariam Salman; Mohammad Bazzi; Ammar Mohanna*

***

### üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Accuracy Gain** | +2.5% vs. Original |
| **Compression Rate** | 40% Size Reduction (60% Retained) |
| **Architectures** | CNNs & Attention-based Models |
| **References** | 15 Citations |

***

## üìã Executive Summary

Deploying deep neural networks (DNNs) on resource-constrained hardware presents a persistent dilemma where traditional compression techniques typically force a trade-off between model size and inference accuracy. Standard quantization methods often introduce parameter noise and variance, degrading performance just as significantly as they reduce memory footprint. This paper addresses that critical challenge by proposing a framework capable of optimizing model complexity for limited environments without sacrificing the precision required for high-stakes applications.

The core innovation is a **"Safety-Guided Self Compression"** framework, an architecture-agnostic approach that integrates systematic pruning with a safety-driven quantization technique. Utilizing "preservation sets," the method guides the optimization process to identify and retain essential weights while actively eliminating parameter noise. By rigorously defining which network components must be preserved, the framework manages to reduce variance during the compression phase, thereby maintaining the integrity of the model's decision boundaries more effectively than conventional quantization alone.

Experimental results validate the framework‚Äôs efficacy across specific architectural paradigms, demonstrating robust performance on both Convolutional Neural Networks (CNNs) and attention-based language models. The approach successfully compressed models to **60% of their initial size** while simultaneously achieving a **2.5% enhancement in test accuracy** compared to original, unquantized baselines. Furthermore, the method exhibited superior generalization relative to standard quantization techniques, proving that significant size reduction need not come at the cost of predictive capability.

This work significantly influences the field by breaking the conventional trade-off between compression and performance, enabling the deployment of high-accuracy AI models on hardware with strict resource limitations. By demonstrating that models can be made smaller and more accurate simultaneously through safety-guided pruning and quantization, the authors provide a viable path forward for edge computing applications. To ensure reproducibility and facilitate further adoption, the study includes a comprehensive release of the framework's implementation via GitHub.

***

## üîç Key Findings

*   **Accuracy Enhancement:** The proposed safety-driven quantization framework achieved a **2.5% enhancement** in test accuracy compared to original, unquantized models.
*   **Efficient Compression:** The methodology successfully compressed models to **60% of their initial size** while optimizing complexity without sacrificing performance.
*   **Noise Reduction:** Compared to conventional quantization techniques, the approach reduces variance and eliminates parameter noise, leading to better generalization.
*   **Cross-Architecture Validity:** The framework was validated across diverse architectures, demonstrating effectiveness on both **Convolutional Neural Networks (CNNs)** and attention-based language models.

***

## ‚öôÔ∏è Methodology

The study introduces a **"safety-driven quantization framework"** that utilizes preservation sets to guide the optimization process. By leveraging these sets, the methodology systematically prunes and quantizes neural network weights. This process is designed to optimize model complexity specifically for resource-constrained environments while rigorously preserving the performance and accuracy of the model.

***

## üîß Technical Details

*   **Framework Name:** Safety-Guided Self Compression
*   **Core Technique:** Safety-driven quantization utilizing preservation sets.
*   **Primary Function:** Reduces variance and eliminates parameter noise.
*   **Scope:** Architecture-agnostic framework.
*   **Validation Targets:** CNNs and Attention-based language models.

***

## üöÄ Contributions

*   **Novel Compression Strategy:** Development of a safety-driven quantization technique that uses preservation sets to systematically identify and retain essential weights during the pruning and quantization process.
*   **Performance-Compression Balance:** Demonstration of a framework that breaks the typical trade-off by improving accuracy (up to 2.5%) while still achieving significant model size reduction (maintaining 60% of initial size).
*   **Robustness Enhancement:** Improvement over standard quantization by actively eliminating parameter noise and reducing variance, which ensures the retention of critical model features and better generalization.
*   **Cross-Architectural Applicability:** Evidence that the proposed strategy is robust enough to be applied effectively to varying architectural paradigms, specifically CNNs and attention-based models.
*   **Reproducibility:** Public release of the framework's implementation and comprehensive experimental evaluations via GitHub.

***

## üìà Results

The method achieved a **+2.5% improvement** in test accuracy relative to original unquantized models and reduced model size to **60% of its initial size** (40% compression). It optimized complexity without sacrificing performance and demonstrated better generalization than conventional quantization techniques.