# Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning
*Jikai Jin; Vasilis Syrgkanis; Sham Kakade; Hanlin Zhang*

---

## ‚ö° Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Models Analyzed** | 3,360 models (Open LLM Leaderboard) |
| **Scope** | 6 Benchmarks (Base models: Llama-3, Qwen2) |
| **Structure Identified** | 3-Node Linear Causal Hierarchy |
| **Variance Explained** | ~95% (approximate rank-3 structure) |
| **Core Hierarchy** | Problem-solving ‚Üí Instruction-following ‚Üí Math Reasoning |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |

---

## üìù Executive Summary

Current evaluation of Large Language Models (LLMs) relies heavily on aggregated benchmark leaderboards which present a static, fragmented view of model performance. This approach treats distinct capabilities as isolated metrics, failing to capture the intrinsic causal dependencies and hierarchical relationships between skills. Without understanding these underlying interdependencies, researchers cannot determine if high performance in one area implies proficiency in another, nor can they isolate specific developmental bottlenecks.

This paper addresses the challenge of moving beyond simple numerical rankings to uncover the latent, causal structure of model capabilities. The authors propose a causal representation learning framework utilizing Structural Causal Models (SCMs). By treating observed benchmark performance as a linear transformation of latent capability factors and rigorously controlling for base model architecture as a common confounder, the method isolates causal relationships from noise.

Analysis of 3,360 models revealed that the model-accuracy matrix possesses an approximate rank-3 structure explaining roughly 95% of variance. The study identified a specific three-node linear causal hierarchy: **General Problem-Solving $\rightarrow$ Instruction-Following $\rightarrow$ Mathematical Reasoning**. This research shifts the paradigm from descriptive rankings to explanatory causal models, providing actionable guidance for future LLM evaluation and alignment strategies.

---

## üîë Key Findings

*   **Discovery of a Three-Node Causal Structure**: Through analysis of over 1,500 models across six benchmarks, the authors identified a concise three-node linear causal structure that reliably explains variations in language model performance.
*   **Hierarchical Capability Progression**: The research revealed a specific causal direction among capabilities: **general problem-solving abilities** causally enable **instruction-following proficiency**, which subsequently enables **mathematical reasoning ability**.
*   **Necessity of Confounder Control**: The study highlights that accurately uncovering these relationships requires strict control over base model variations, as the base model acts as a significant common confounder in evaluation.
*   **Superiority over Simple Rankings**: The framework provides deeper scientific insights into the intrinsic dependencies between skills that simple numerical benchmark rankings cannot reveal.

---

## üõ†Ô∏è Methodology

*   **Causal Representation Learning**: The authors utilize a causal representation learning framework to model observed benchmark performance, moving beyond correlation to causation.
*   **Linear Transformation Modeling**: Benchmark results are treated as a linear transformation of a limited set of latent capability factors rather than viewing metrics in isolation.
*   **Confounder Control**: A critical step involves statistically controlling for the "base model" as a common confounder. This allows for the identification of causal interrelationships between latent factors, filtering out noise from model variations.
*   **Large-Scale Data Analysis**: The proposed approach is applied to a comprehensive dataset comprising over 1,500 models and six benchmarks from the Open LLM Leaderboard.

---

## ‚öôÔ∏è Technical Details

### Data Scope
*   **Source**: 3,360 models from the Open LLM Leaderboard.
*   **Benchmarks**: 6 distinct benchmarks.
*   **Focus**: Base models such as Llama-3 and Qwen2.

### Analytical Framework
*   **Latent Factor Analysis**: Principal Component Analysis (PCA) applied domain-specifically to handle heterogeneity.
*   **Invariant Set**: Identification of an 'Invariant Set' of models sharing similar PC subspaces.
*   **Causal Representation Learning**: Utilization of Structural Causal Models (SCMs), defining both Linear and Inexact SCMs.

### Hypotheses
*   **Hierarchical Causal Invariance** (Hypothesis 2): Suggests capability factors are generated by linear inexact SCMs with a small Maximum Inexactness Coefficient (MIC), maintaining an invariant causal graph structure across domains.

---

## üìä Results

*   **PCA Structure**: PCA revealed an approximate rank-3 structure for the model-accuracy matrix, explaining roughly **95% of the variance** (distributed as 0.76, 0.15, and 0.06 for the top three components).
*   **Subspace Divergence**: Outliers showed cosine distances up to **0.42** compared to the invariant cluster.
*   **Missing Data Imputation**: Local Matrix Completion with Nuclear Norm Regularization (**MC-NNR**) outperformed Global MC-NNR in RMSE.
*   **Causal Hierarchy**: Analysis of 1,500+ models identified the hierarchy: *General Problem-Solving $\rightarrow$ Instruction-Following Proficiency $\rightarrow$ Mathematical Reasoning Ability*.
*   **Confounder Validation**: Confirmed that the base model acts as a critical confounder that obscures these relationships if left uncontrolled.

---

## üöÄ Contributions

*   **Addressing Evaluation Challenges**: The framework addresses methodological difficulties in rigorous causal evaluation, specifically solving issues related to complex confounding effects and high computational costs.
*   **Actionable Model Insights**: By revealing the causal hierarchy of capabilities, the research provides actionable guidance for developers regarding how capabilities build upon one another.
*   **Validation of Causal Frameworks**: Demonstrates that treating model capabilities as latent, causally interrelated factors is a viable and effective method for deriving scientific understanding from evaluation benchmarks.

---

**Quality Score**: 8/10 | **References**: 40 citations