---
title: 'C-LoRA: Continual Low-Rank Adaptation for Pre-trained Models'
arxiv_id: '2502.1792'
source_url: https://arxiv.org/abs/2502.17920
generated_at: '2026-02-03T19:38:35'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# C-LoRA: Continual Low-Rank Adaptation for Pre-trained Models

*Xin Zhang; Liang Bai; Xian Yang; Jiye Liang*

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Architecture:** Learnable Routing Matrix, Orthogonality Constraints
> *   **Key Performance:**
>     *   **84.42%** Avg Accuracy (5-Datasets, ViT-B/16)
>     *   **77.87%** Avg Accuracy (Split-CIFAR-100)
> *   **Primary Benefit:** Resolves scalability issues of multi-adapter continual learning.

---

## Executive Summary

The research addresses the critical challenge of continual learning in large pre-trained models, specifically the tension between acquiring new knowledge and retaining existing information (catastrophic forgetting). Current state-of-the-art parameter-efficient fine-tuning methods, such as adapter-based approaches, typically require isolating parameters for each new task. As the number of sequential tasks increases, this reliance on multiple separate adapter modules creates significant scalability issues, leading to prohibitive computational overhead and complex inference pipelines. This creates a bottleneck for deploying AI systems in dynamic environments where models must adapt continuously to evolving data streams without constant retraining or expanding memory footprints linearly.

The authors propose **C-LoRA (Continual Low-Rank Adaptation)**, the first extension of the Low-Rank Adaptation (LoRA) framework specifically designed for continual learning. The core technical innovation is the introduction of a learnable routing matrix, $R$, which is inserted between the standard LoRA decomposition matrices $A$ and $B$, formulating the weight update as $\Delta W = ARB$. This routing matrix is decomposed into $R_{\text{old}}$, which is frozen to accumulate knowledge from previous tasks, and $R_{\text{new}}$, which is initialized near-zero to learn the current task. By enforcing orthogonality constraints on $R_{\text{new}}$, the method ensures that updates for the new task are orthogonal to parameters from past tasks, thereby minimizing interference. The authors further support this approach with theoretical analysis (Theorem 3.1), proving that this decomposition yields tighter bounds on parameter changes and reduces gradient magnitude compared to direct training.

In Vision Transformers, this routing matrix is integrated into MLP blocks to modulate input features via down- and up-projections while maintaining residual connections. C-LoRA achieves state-of-the-art accuracy and parameter efficiency on standard continual learning benchmarks, specifically within the domain of Vision Transformers. On the challenging 5-Datasets benchmark (comprising CIFAR-100, SVHN, TinyImageNet, Cars, and Flowers) using ViT-B/16, C-LoRA attains an average accuracy of **84.42%**, significantly outperforming standard LoRA and Adapter baselines. In Split-CIFAR-100 experiments, the method achieves an average accuracy of **77.87%**, demonstrating superior retention of information across sequential tasks.

Crucially, C-LoRA resolves the storage limitations of existing methods; whereas traditional adapter approaches require a linear increase in parameters with each new task, C-LoRA maintains fixed low-rank matrices $A$ and $B$ and only introduces minimal overhead for the routing components. This results in a drastic reduction in storage requirements while sustaining high performance, effectively eliminating the memory explosion associated with multi-adapter architectures. The significance of C-LoRA lies in its resolution of the scalability limitations that currently hinder dynamic learning environments.

---

## Key Findings

*   **State-of-the-Art Performance:** C-LoRA achieves superior accuracy and parameter efficiency on standard continual learning benchmarks compared to existing methods.
*   **Effective Knowledge Management:** The proposed learnable routing matrix successfully enables knowledge retention and transfer across sequential tasks.
*   **Minimized Interference:** By enforcing orthogonality, the method minimizes interference and significantly reduces catastrophic forgetting.
*   **Scalability Solution:** C-LoRA resolves the scalability issues inherent in current dynamic learning environments that rely on multiple adapter modules.

---

## Methodology

The core of C-LoRA's methodology revolves around optimizing parameter updates for sequential tasks without retraining the entire model:

*   **Learnable Routing Matrix:** A dynamic routing matrix is introduced to manage parameter updates across different tasks. This allows for the efficient reuse of learned subspaces.
*   **Orthogonality Constraints:** The method applies strict orthogonality constraints to minimize interference between new and previous tasks, thereby actively reducing forgetting.
*   **Integrated Approach:** Unlike existing methods that require separate adapter modules for each task (increasing overhead), C-LoRA utilizes a single, integrated approach for task adaptation. This simplifies the inference process.
*   **Theoretical Foundation:** The approach is supported by a theoretical analysis defining how the routing matrix functions to retain and transfer knowledge effectively.

---

## Contributions

*   **C-LoRA Framework:** Introduction of C-LoRA, the first extension of Low-Rank Adaptation (LoRA) specifically designed to handle continual learning scenarios.
*   **Efficiency Optimization:** A solution that eliminates the need for multiple separate adapters per task, significantly reducing computational overhead and inference complexity.
*   **Theoretical Insight:** Provision of new theoretical understanding regarding the role of routing matrices in managing knowledge transfer within parameter-efficient fine-tuning.
*   **Scalable Architecture:** Establishment of a scalable framework that maintains high performance while adapting to continuously changing data streams.

---

## Technical Details

**Mathematical Formulation**
*   **Weight Update:** C-LoRA extends LoRA by inserting a learnable routing matrix $R$ between low-rank decomposition matrices $A$ and $B$:
    $$W = ARB$$
*   **Matrix Decomposition:** The routing matrix is split into:
    *   $R_{\text{old}}$: Frozen component that accumulates knowledge from previous tasks.
    *   $R_{\text{delta}}$: Initialized near-zero to learn the current task.
*   **Forward Pass:** The forward pass is modified to:
    $$y' = xW' \quad \text{where} \quad W' = \phi(AR_{\text{old}} B) + AR_{\text{delta}} B$$

**Stability and Constraints**
*   **Theorem 3.1 (Stability Bound):** Proves that the specific decomposition results in tighter bounds on parameter changes for matrices $A$ and $B$ compared to direct training. This reduces gradient magnitude to mitigate catastrophic forgetting.
*   **Orthogonality:** Constraints are enforced on $R_{\text{delta}}$ to ensure updates are orthogonal to past task parameters.

**Vision Transformer Integration**
*   **MLP Block Integration:** C-LoRA integrates into the MLP block of Vision Transformers.
*   **Processing:** Input features are processed via down-projection (Matrix $A$), modulation by routing matrices, and up-projection (Matrix $B$).
*   **Residual Connection:** The final output uses a residual connection:
    $$\text{out} = x_i + \text{MLP}(x_i) + \text{C-LoRA}(x_i)$$

---

## Results

*   **Benchmark Performance:** C-LoRA achieves state-of-the-art accuracy on standard benchmarks, specifically attaining **84.42%** on the 5-Datasets benchmark and **77.87%** on Split-CIFAR-100.
*   **Parameter Efficiency:** The method resolves scalability issues associated with multiple adapter modules in dynamic environments by maintaining a fixed parameter size for the core matrices.
*   **Forgetting Reduction:** The method minimizes interference and catastrophic forgetting compared to existing approaches by enforcing orthogonality and using a decomposed routing matrix.
*   **Knowledge Transfer:** The learnable routing matrix enables effective knowledge retention and transfer across sequential tasks.