---
title: 'LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture
  for Transparent Feature Selection'
arxiv_id: '2510.05935'
source_url: https://arxiv.org/abs/2510.05935
generated_at: '2026-01-27T22:14:06'
quality_score: 8
citation_count: 28
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection

*Sabri Fayssal, Ghaoui Mohamed*  
*Affiliations: Ecole Centrale, Audensiel Conseil*

---

## ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Dataset** | CIC-DIAD 2024 (IoT Intrusion Detection) |
| **Sample Size** | 12,408 balanced samples |
| **Training Time Reduction** | 46% average reduction |
| **Key Stat** | 0.094s reduction for XGBoost ($p=0.028$) |
| **Core Models** | Llama 3 2b, Qwen 4b, Phi 3 mini 2b, Gemma 3 4b, Mistral 4b |

---

## ðŸ“ Executive Summary

Feature selection is a critical preprocessing step in machine learning that directly impacts model performance, computational cost, and interpretability. Traditional methods often operate as "black boxes," offering selected features without clear reasoning, while Large Language Model (LLM)-based approaches can be computationally expensive and opaque. This lack of transparency is particularly problematic in high-stakes domains such as cybersecurity, where understanding the rationale behind feature selection is essential for trust and compliance.

This paper addresses the challenge of creating a feature selection mechanism that not only maintains high prediction accuracy and reduces computational overhead but also provides explicit, human-interpretable justifications for its decisions. The researchers introduce **LLM-FS-Agent**, a novel deliberative, multi-agent architecture leveraging Small and Medium Language Models (SLMs/MLMs).

The core innovation lies in a structured "debate" framework where specialized agents assume distinct roles to evaluate feature relevance: an **Initiator** proposes features, a **Refiner** adds statistical context, a **Challenger** provides adversarial critique, and a **Judge** renders the final decision. Empirical validation on the CIC-DIAD 2024 dataset demonstrates that LLM-FS-Agent achieves predictive performance consistently superior or comparable to prominent baselines like LLM-Select and PCA, while delivering a 46% average reduction in downstream classifier training time. This successfully validates the architecture's ability to solve the "black box" problem in a real-world cybersecurity context.

---

## ðŸ”‘ Key Findings

*   **Superior Performance:** LLM-FS-Agent achieves performance metrics consistently superior or comparable to prominent baselines like LLM-Select and PCA across various feature subsets.
*   **Operational Efficiency:** The system significantly reduces downstream classifier training time by an average of **46%**, achieving a specific reduction of **0.094s** for XGBoost.
*   **Enhanced Transparency:** The deliberative architecture enhances interpretability by providing detailed justifications for every feature selection decision, effectively addressing the "black box" issue.
*   **Real-World Validation:** The architecture proved effective in a complex, real-world cybersecurity application for IoT intrusion detection using the CIC-DIAD 2024 dataset.

---

## ðŸ§© Methodology

The researchers developed **LLM-FS-Agent**, a novel multi-agent architecture utilizing a deliberative framework. The approach involves orchestrating a debate among multiple LLM agents assigned specific roles to collectively evaluate feature relevance and generate supported justifications.

*   **Deliberative Framework:** Agents are assigned specific personas (Initiator, Refiner, Challenger, Judge) to simulate a structured debate regarding feature utility.
*   **Validation Context:** The methodology was empirically validated using the **CIC-DIAD 2024 dataset** within a cybersecurity context.
*   **Benchmarking:** Results were rigorously compared against both LLM-based benchmarks (LLM-Select) and traditional statistical methods (PCA).

---

## âš™ï¸ Technical Details

### Architecture & Models
The LLM-FS-Agent utilizes a deliberative, multi-agent architecture employing Small and Medium Language Models (SLMs/MLMs) to enforce structured reasoning without the heavy computational cost of Large Language Models.

*   **Models Utilized:**
    *   Llama 3 2b
    *   Qwen 4b
    *   Phi 3 mini 2b
    *   Gemma 3 4b
    *   Mistral 4b
*   **Agent Roles:**
    *   **Initiator:** Proposes potential features.
    *   **Refiner:** Adds statistical context to proposals.
    *   **Challenger:** Provides adversarial critique.
    *   **Judge:** Renders the final decision.

### Data Processing Pipeline
*   **Preprocessing:** Involves collinearity removal (Pearson > 0.9), standardization (StandardScaler), and class balancing via random undersampling.
*   **Testing:** Evaluation performed on feature subsets ($n=5, 10, 20, 30, 40, 50$).
*   **Baselines:** Compared against LLM-Select, PCA, and traditional wrapper/filter methods.
*   **Classifiers:** Evaluated using Random Forest, XGBoost, SVC, and Logistic Regression.

---

## ðŸš€ Contributions

1.  **Architectural Innovation:**
    Introduction of a role-based, deliberative multi-agent system designed for feature selection to improve robustness and decision quality.

2.  **Solving the 'Black Box' Problem:**
    Addressing opaque decision-making by embedding structured reasoning and transparent justification capabilities directly into the selection process.

3.  **Operational Efficiency:**
    Demonstrating that an interpretable, LLM-driven feature selection method reduces computational overhead while maintaining high prediction accuracy.

---

## ðŸ“ˆ Results

Experimental results on the CIC-DIAD 2024 IoT cybersecurity dataset confirm the efficacy of the proposed architecture:

*   **Time Efficiency:** Achieved a 46% average reduction in downstream classifier training time.
*   **Statistical Significance:** For XGBoost, the architecture reduced training time by 0.094s ($p=0.028$), indicating statistical significance.
*   **Accuracy:** Predictive performance was consistently superior or comparable to LLM-Select and PCA across feature subsets ranging from $n=5$ to $n=50$.
*   **Interpretability:** The system successfully generated transparent, human-interpretable rationales for feature selection, addressing the 'black box' limitation of traditional methods.

---

**References:** 28 citations | **Quality Score:** 8/10