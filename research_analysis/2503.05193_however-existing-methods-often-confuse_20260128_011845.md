---
title: However, existing methods often confuse
arxiv_id: '2503.05193'
source_url: https://arxiv.org/abs/2503.05193
generated_at: '2026-01-28T01:18:45'
quality_score: 8
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# MemQ: Memory-augmented Query Reconstruction for LLM-based Knowledge Graph Reasoning

*Min Zhang, Wei Wang, Tiejun Zhao, Gewen Liang, Muyun Yang, Kehai Chen, Mufan Xu, Xun Zhou*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Framework Name** | MemQ (Memory-augmented Query Reconstruction) |
| **Key Benchmarks** | WebQSP, CWQ |
| **Performance** | **77.9% F1** (WebQSP), **60.1% F1** (CWQ) |
| **Quality Score** | 8/10 |
| **Citations** | 22 References |

---

## Executive Summary

Current Large Language Model (LLM) approaches to Knowledge Graph Question Answering (KGQA) suffer from a critical architectural flaw: the conflation of "tool utilization" with "knowledge reasoning." By allowing the LLM to directly invoke graph toolsâ€”often by generating executable code or queries based on probabilistic patternsâ€”existing methods frequently produce hallucinatory tool calls and outputs that lack interpretability. This coupling creates significant reliability risks, as the execution phase often fails due to syntactic errors in generated code, resulting in a system that is difficult to debug and trust for complex reasoning tasks.

To address this, the paper introduces **MemQ**, a framework that decouples the LLM from direct tool invocation through a "dual process" architecture comprising Natural Language Reasoning and Memory-augmented Query Reconstruction. MemQ implements a specific three-stage pipeline:

1.  **Memory Construction ($M$)**: Maps natural language descriptions to explicit query statements to build an LLM-built query memory.
2.  **Knowledge Reasoning ($P$)**: Tasks the LLM with generating a constrained, high-level natural language reasoning plan without accessing tools.
3.  **Query Reconstruction**: Retrieves the stored query statements from memory based on semantic similarity to the reasoning steps.

This design allows the LLM to focus on linguistic logic while a deterministic memory retrieval process handles the precise assembly of executable queries.

MemQ achieves state-of-the-art (SOTA) performance on standard KGQA benchmarks, significantly outperforming previous methods reliant on direct tool invocation. Specifically, MemQ attains an **F1 score of 77.9% on the WebQSP dataset** and **60.1% on the CWQ dataset**. These quantitative results underscore the framework's efficacy, demonstrating that separating the reasoning process from query execution not only boosts accuracy but also virtually eliminates the hallucination of non-existent tools, ensuring robust system reliability.

---

## Key Findings

*   **Conflation Issue**: Current LLM-based KGQA methods frequently confuse 'tool utilization' with 'knowledge reasoning,' leading to issues with output readability and hallucinatory tool invocations.
*   **State-of-the-art Performance**: The MemQ framework has achieved SOTA results on both the WebQSP and CWQ benchmarks.
*   **Risk Mitigation**: Effectively decoupling the Large Language Model from direct tool invocation tasks significantly mitigates the risk of hallucinations and improves system reliability.
*   **Interpretability**: The use of a memory module with explicit query descriptions facilitates more interpretable and readable reasoning paths compared to existing methods.

---

## Methodology

MemQ (Memory-augmented Query Reconstruction for LLM-based Knowledge Graph Reasoning) operates on the principle of separating the reasoning process from actual tool invocation. The core innovation is the 'LLM-built query memory,' which serves as an intermediary between the LLM and the knowledge graph.

The system functions through a **dual process**:

1.  **Natural Language Reasoning**: In this phase, the LLM performs high-level reasoning without direct access to tools.
2.  **Memory-augmented Query Reconstruction**: Instead of the LLM directly invoking tools, query statements are rebuilt based on memory retrieved during the reasoning phase.

This approach ensures that the LLM leverages its strengths in linguistic understanding while relying on a structured memory mechanism for precise query execution.

---

## Technical Details

The MemQ framework utilizes a three-stage pipeline to decouple knowledge reasoning from tool invocation:

1.  **Memory Construction ($M$)**
    *   Maps natural language descriptions to query statements.
    *   Creates a repository of explicit query statements accessible to the system.

2.  **Knowledge Reasoning ($P$)**
    *   Generates a constrained $n$-step natural language reasoning plan.
    *   This process is isolated from tool execution to maintain logical purity.

3.  **Query Reconstruction**
    *   Recalls query statements from the memory module.
    *   Selection is based on semantic similarity to the previously generated reasoning steps.
    *   Constructs the final executable query to be run against the knowledge graph.

---

## Contributions

*   **New Paradigm**: Introduced MemQ, a novel paradigm for LLM-based KGQA that decouples the language model from the tool invocation layer via a memory mechanism.
*   **Hallucination Reduction**: Addressed the critical issue of 'hallucinatory tool invocations' by replacing direct tool calls with a memory-driven reconstruction approach.
*   **Enhanced Readability**: Improved the readability and interpretability of model outputs by enforcing explicit descriptions within the query memory.
*   **Benchmark Advancement**: Advanced the state-of-the-art performance in the field, setting a new benchmark on the WebQSP and CWQ datasets.

---

## Results

The evaluation of MemQ on standard benchmarks demonstrated significant quantitative and qualitative improvements:

*   **WebQSP Dataset**: Achieved an F1 score of **77.9%**.
*   **CWQ Dataset**: Achieved an F1 score of **60.1%**.
*   **Qualitative Improvements**: Observations included better mitigation of hallucinations, higher interpretability of outputs, and improved readability of reasoning steps compared to existing LLM-based KGQA methods.

---

**Quality Score:** 8/10
**References:** 22 citations