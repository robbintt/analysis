# Named Entity Recognition for the Kurdish Sorani Language: Dataset Creation and Comparative Analysis

*Bakhtawar Abdalla; Rebwar Mala Nabi; Hassan Eshkiki; Fabio Caraffini*

---

## ðŸ“‹ Quick Facts

| Metric | Value |
| :--- | :--- |
| **Dataset Name** | AgaCKNER |
| **Total Tokens** | 64,563 |
| **Source** | Rudaw Media Network (160+ articles) |
| **Domains** | 5 |
| **Tagging Scheme** | BIO (CoNLL Format) |
| **Top Performing Model** | CRF (Classical ML) |
| **Best F1-Score** | 0.825 |
| **Performance Gap** | +11.9% over BiLSTM |

---

## Executive Summary

This research addresses the critical lack of linguistic resources for Named Entity Recognition (NER) in Kurdish Sorani, a significantly underrepresented, low-resource language. The absence of standardized annotated corpora and processing tools has historically hindered the development of effective NLP applications for this language, preventing the integration of Kurdish Sorani into modern information extraction workflows. By tackling this data scarcity, the study aims to bridge the digital divide for Sorani speakers, a task complicated by the language's complex morphology and the limited availability of pre-trained language models.

The key innovation of this work is the creation of the first dedicated NER dataset for Kurdish Sorani, named **AgaCKNER**, comprising 64,563 annotated tokens sourced from 160+ articles across five domains within the Rudaw Media Network. Technically, the preprocessing pipeline utilizes the **Kurdish Language Processing Toolkit (KLPT)** for text cleaning, numeral transliteration, and tokenization, adhering to the CoNLL format and BIO tagging scheme. To facilitate this corpus generation, the authors also developed the **AGA NER Annotation Tool**, a custom software solution featuring keyboard shortcuts and batch processing capabilities to streamline the manual annotation process.

In a direct benchmark comparing classical and neural architectures, the study found that **Conditional Random Fields (CRF)** significantly outperformed BiLSTM-based neural models. The CRF model achieved an F1-score of 0.825, surpassing the neural model's score of 0.706 by a substantial delta of 11.9%. These results indicate that for this specific low-resource language, the statistical modeling efficiency of CRFs provides better generalization than deep learning approaches, which often struggle to learn effective representations without the massive datasets typically available for high-resource languages like English.

The significance of this paper lies in its challenge to the prevailing assumption that neural approaches universally hold an advantage over classical machine learning methods in NLP. By demonstrating that computationally efficient frameworks can deliver superior performance for languages with limited annotated data, the authors advocate for a re-evaluation of resource allocation strategies in low-resource NLP. This work provides a vital open-source benchmark for the Kurdish Sorani language and establishes empirical evidence supporting the use of simpler, more efficient classical models to promote linguistic inclusivity without requiring access to high-end computational infrastructure.

---

## Key Findings

*   **Classical ML Superiority:** Classical machine learning methods significantly outperformed neural architectures for the low-resource Kurdish Sorani language.
*   **CRF Dominance:** Conditional Random Fields (CRF) achieved an F1-score of **0.825**, substantially higher than the **0.706** F1-score obtained by BiLSTM-based neural models.
*   **Efficiency over Complexity:** Computationally efficient, simpler classical frameworks are often more effective than complex neural approaches for languages with limited annotated data.
*   **Challenging Assumptions:** The results challenge the established assumption that neural approaches universally hold an advantage over classical methods in Natural Language Processing tasks.

---

## Methodology

The researchers undertook a comparative analysis involving both classic machine learning models and neural systems. This was facilitated by the creation of a dedicated dataset and the development of a specific tool to handle annotation and processing tasks. The study specifically benchmarked the performance of a **BiLSTM-based neural architecture** against a conventional **CRF (Conditional Random Fields)** model using the newly created corpus.

---

## Technical Details

**Dataset & Format**
*   **Name:** AgaCKNER
*   **Standard:** Adheres to CoNLL format
*   **Source:** Sourced from the Rudaw Media Network (160+ articles across five domains)
*   **Total Size:** 64,563 annotated tokens

**Preprocessing & Tooling**
*   **Toolkit:** Kurdish Language Processing Toolkit (KLPT)
*   **Tasks:** Text cleaning, transliteration (English to Kurdish numerals), and tokenization
*   **Annotation Tool:** Custom AGA NER Annotation Tool
    *   *Features:* Keyboard shortcuts and batch processing capabilities
*   **Tagging Scheme:** BIO tagging scheme

---

## Results

Experiments comparing classical and neural models for low-resource Kurdish Sorani yielded distinct performance differences:

*   **Top Performer (CRF):** Achieved an F1-score of **0.825**.
*   **Neural Model (BiLSTM):** Scored **0.706**.
*   **Performance Delta:** The classical CRF model exceeded the neural model by **0.119 (11.9%)**.
*   **Context:** Benchmarking against high-resource languages (English >93% F1) supports the conclusion that computationally efficient classical frameworks are more effective than complex neural architectures for limited annotated datasets.

---

## Contributions

*   **New Dataset:** Introduction of the first Named Entity Recognition (NER) dataset for the Kurdish Sorani language, consisting of 64,563 annotated tokens.
*   **Software Tool:** Provision of a software tool designed to facilitate named entity recognition tasks for Kurdish Sorani.
*   **Empirical Evidence:** Provision of empirical evidence suggesting that balancing inclusivity in NLP does not always require complex neural models, as classical methods can offer superior performance and efficiency for low-resource languages.

---
*Quality Score: 8/10 | References: 39*