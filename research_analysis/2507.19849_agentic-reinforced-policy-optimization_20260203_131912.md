---
title: Agentic Reinforced Policy Optimization
arxiv_id: '2507.19849'
source_url: https://arxiv.org/abs/2507.19849
generated_at: '2026-02-03T13:19:12'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Agentic Reinforced Policy Optimization

*Guanting Dong; Hangyu Mao; Kai Ma; Licheng Bao; Yifei Chen; Zhongyuan Wang; Zhongxia Chen; Jiazhen Du; Huiyang Wang; Fuzheng Zhang; Guorui Zhou; Yutao Zhu; Ji-Rong Wen; Zhicheng Dou*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Benchmarks Covered:** 13 (Computational, Knowledge Reasoning, Deep Search)
> *   **Efficiency Gain:** Utilizes only 50% of the tool-use budget compared to existing methods
> *   **Citations:** 40
> *   **Core Discovery:** Entropy spikes occur 10â€“50 tokens after tool interactions
> *   **Baselines Outperformed:** GRPO, DAPO

---

## Executive Summary

This paper addresses the critical challenge of effectively aligning Large Language Model (LLM)-based agents that utilize external tools in dynamic, multi-turn environments. A core issue identified is that LLMs exhibit highly uncertain behavior immediately following interactions with external tools, characterized by significant spikes in token entropy 10â€“50 tokens after a tool call. Current Large-scale Reinforcement Learning with Verifiable Rewards (RLVR) algorithms predominantly rely on trajectory-level optimization, which fails to account for this localized, step-level uncertainty. This limitation results in inefficient training and poor integration of tool-use capabilities, hindering the model's ability to balance intrinsic reasoning with external interaction.

To overcome these limitations, the authors introduce **Agentic Reinforced Policy Optimization (ARPO)**, a reinforcement learning algorithm specifically designed for multi-turn, tool-using agents. ARPO technically operates through two primary components: an **Entropy-based Adaptive Rollout Mechanism** and **Advantage Attribution Estimation**. The rollout mechanism dynamically balances global trajectory sampling with targeted step-level sampling at high-entropy points (post-tool interaction), allowing the model to focus learning on areas of maximum uncertainty. Concurrently, the Advantage Attribution Estimation decomposes the optimization objective to internalize advantage differences at the stepwise level of tool-use interactions, maximizing expected reward under a KL constraint.

ARPO was evaluated across 13 challenging benchmarks spanning computational reasoning, knowledge reasoning, and deep search domains. The results demonstrate that ARPO consistently outperforms existing trajectory-level RL baselines, including GRPO and DAPO, in terms of accuracy. Crucially, it achieves this superior performance while requiring only 50% of the tool-use budget utilized by existing methods. This work represents a significant advancement in agentic AI, providing a scalable solution for aligning LLMs with real-time dynamic environments without excessive computational overhead.

---

## Key Findings

*   **Uncertainty Spikes:** LLMs exhibit highly uncertain behavior, characterized by a spike in the entropy of generated tokens, immediately following interactions with external tools.
*   **Superior Performance:** ARPO outperforms existing trajectory-level reinforcement learning algorithms (such as GRPO and DAPO) across 13 challenging benchmarks.
*   **Resource Efficiency:** The proposed method achieves improved performance while utilizing only **50% of the tool-use budget** required by existing methods.
*   **Balanced Capability:** The approach successfully balances the model's intrinsic long-horizon reasoning capabilities with its proficiency in multi-turn tool interactions.
*   **Tool Variance:** Pilot experiments show that Search Engines cause higher uncertainty (entropy) than Python interpreters.

---

## Methodology

The paper proposes **Agentic Reinforced Policy Optimization (ARPO)**, a reinforcement learning algorithm tailored for multi-turn LLM-based agents. The methodology rests on two core innovations:

1.  **Entropy-based Adaptive Rollout Mechanism**
    *   Dynamically balances global trajectory sampling with step-level sampling.
    *   Specifically targets high-entropy steps that occur after tool usage.
2.  **Advantage Attribution Estimation**
    *   Enables the model to internalize advantage differences at the stepwise level of tool-use interactions.
    *   Decomposes the optimization process to refine specific decision points.

---

## Technical Details

*   **Objective:** Maximizes expected reward with a KL (Kullback-Leibler) constraint.
*   **Trajectory Decomposition:** Rollouts are decomposed into *Agentic Reasoning* and *Answer Generation* phases.
*   **Entropy Detection:** Calculates token entropy using pre-softmax logits and temperature to identify uncertainty spikes occurring 10â€“50 tokens after interaction.
*   **Sampling Strategy:**
    *   **Global Sampling:** Standard trajectory-wide sampling.
    *   **Partial Sampling:** Branches trajectories specifically at high-entropy steps.
*   **Tools Integrated:** Search Engines, Web Browser Agents, and Code Interpreters.

---

## Contributions

*   **Bridging the RLVR Gap:** Addresses the limitation of current Large-scale Reinforcement Learning with Verifiable Rewards (RLVR) algorithms specifically for multi-turn tool interactions.
*   **Scalable Solution:** Introduces ARPO as a scalable solution for aligning LLM-based agents with real-time dynamic environments.
*   **Leveraging Uncertainty:** Identifies and leverages the specific phenomenon of increased entropy (uncertainty) after tool interactions to optimize the training process, rather than treating it as noise.

---

## Results

*   **Pilot Experiments:** Confirmed entropy spikes occur 10â€“50 tokens after tool calls. Search engines induced higher uncertainty than Python interpreters.
*   **Benchmark Evaluation:** Evaluated on 13 challenging benchmarks across computational reasoning, knowledge reasoning, and deep search domains.
*   **Comparison:** ARPO outperformed GRPO and DAPO baselines in accuracy.
*   **Efficiency:** Demonstrated the ability to maintain high performance while cutting the tool-use budget by 50%.
*   **Reasoning Balance:** Successfully demonstrated a balance between long-horizon reasoning and multi-turn tool interaction.

---

**References:** 40 citations