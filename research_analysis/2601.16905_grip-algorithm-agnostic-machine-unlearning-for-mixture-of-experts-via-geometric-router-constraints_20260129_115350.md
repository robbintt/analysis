# GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints

*Andy Zhu; Rongzhe Wei; Yupu Gu; Pan Li*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Core Innovation** | Geometric Router Invariance Preservation (GRIP) |
| **Routing Stability** | **>0.94** (Restored from ~0.2 in unconstrained settings) |
| **Architecture Target** | Mixture-of-Experts (MoE) |
| **Constraint Type** | Hard geometric constraints (Null-Space Projection) |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |

---

## Executive Summary

This research addresses a critical failure in applying Machine Unlearning (MU) techniques to Mixture-of-Experts (MoE) architectures, which are increasingly prevalent in modern large-scale systems. The authors identify that existing unlearning methods, largely designed for dense networks, fail on sparse MoE models due to a phenomenon termed **"Expert Selection Shift."** Instead of truly erasing knowledge from the experts, unconstrained optimization processes exploit a vulnerability by manipulating router parameters to redirect queries away from specific experts. This results in "superficial forgetting," where the model appears to have forgotten data only because the routing logic has shifted, leading to a catastrophic collapse in routing stability and a significant loss of overall model utility.

The core innovation is **GRIP (Geometric Router Invariance Preservation)**, an algorithm-agnostic framework that imposes geometric constraints to decouple routing stability from parameter plasticity. Technically, GRIP functions as a constraint layer surrounding existing unlearning algorithms, restricting router updates to the "expert-specific null-space." It utilizes Null-Space Constraints to project router gradient updates into the left null space of retain set representations, ensuring that the "retain" data distribution flows through the same experts. The methodology employs Expert-Specific Constraint Decomposition, which enforces hard equalities for selected experts (zero score change) and relaxed inequalities for unselected ones (changes within a margin relative to top-k scores). This is enforced via projected gradient descent during training, effectively forcing the optimization process to modify expert parameters directly for erasure rather than taking the "shortcut" of router manipulation.

Experimental results demonstrate that GRIP effectively mitigates the routing collapse observed in standard unlearning methods. In unconstrained settings, Routing Stability (RS)â€”measured via the Jaccard similarity coefficient of expert selectionâ€”collapses from 1.0 to approximately 0.2. **GRIP successfully restores RS to greater than 0.94 (over 95%)** across various scenarios. The analysis further revealed a "Cascading Collapse" in deeper layers triggered by representation drift in shallow layers, which GRIP prevents. Compared to soft regularization approaches like SEUF and dense methods such as NPO and RMU, GRIPâ€™s hard geometric constraints successfully account for the dynamic dependencies of MoE computation graphs, achieving robust unlearning without sacrificing the structural integrity of the model.

This work is significant as it provides the first comprehensive architectural analysis diagnosing why traditional unlearning methods fail on MoE models, establishing Expert Selection Shift as the primary failure mode. By bridging the gap between dense architecture unlearning research and sparse MoE architectures, GRIP enables true knowledge erasure where prior methods only achieved false erasure. This development is vital for the deployment of compliant and safe MoE-based large language models (LLMs), as it ensures that data removal requests can be honored without degrading model performance or triggering unpredictable routing behaviors. The framework sets a new standard for evaluating unlearning in sparse models, emphasizing that stability must be mathematically enforced rather than approximated.

---

## Key Findings

*   **Diagnosis of Failure:** Existing machine unlearning methods fail on MoE architectures because they exploit a vulnerability by manipulating routers to redirect queries rather than erasing underlying knowledge.
*   **Superficial Forgetting:** Reliance on router manipulation results in "superficial forgetting" and a significant loss of model utility.
*   **Routing Stability:** The proposed GRIP framework eliminates expert selection shift, achieving **over 95% routing stability**.
*   **True Erasure:** GRIP preserves model utility by forcing the optimization process to modify expert parameters directly instead of using router manipulation shortcuts.
*   **Cascading Collapse:** Structural analysis revealed a collapse in deeper layers triggered by representation drift in shallow layers, identified as a primary failure mode of prior methods.

---

## Methodology

The methodology is centered around **Geometric Routing Invariance Preservation (GRIP)**, an algorithm-agnostic adapter designed for MoE models.

*   **Constraint Layer:** The approach functions as a wrapper around existing unlearning algorithms to restrict router updates without modifying the underlying algorithm.
*   **Geometric Constraint:** It implements a geometric constraint by projecting router gradient updates into an "expert-specific null-space."
*   **Decoupling Strategy:** This decouples routing stability from parameter rigidity, ensuring discrete expert selections remain stable while allowing continuous router parameters to remain plastic within the null space.

---

## Technical Details

### ðŸ” Problem Identified: Expert Selection Shift
Standard Machine Unlearning (MU) methods fail on Mixture-of-Experts (MoE) models by manipulating router parameters rather than erasing expert knowledge.

### ðŸ› ï¸ Proposed Solution: GRIP
The framework aims to decouple routing stability from parameter plasticity.

#### **1. Routing Stability (RS) Metric**
*   Defined via the **Jaccard similarity coefficient** of experts selected before and after unlearning.

#### **2. Null-Space Constraints**
*   restricts router updates to the left null space of retain set representations.
*   Ensures that the "retain" data distribution flows through the same experts.

#### **3. Expert-Specific Constraint Decomposition**
Refines the constraints by splitting them into two categories:
*   **Hard Equalities:** For selected experts (enforcing zero score change).
*   **Relaxed Inequalities:** For unselected experts (allowing change only within a margin relative to top-k scores).

#### **4. Enforcement Mechanisms**
*   **Training:** Handled via projected gradient descent.
*   **Post-Training:** Handled via analytical realignment.

---

## Results

*   **Routing Stability (RS):** Standard unconstrained optimization causes RS to collapse from **1.0 to ~0.2**. GRIP effectively restores RS to **>0.94**.
*   **Structural Analysis:** Revealed 'Cascading Collapse' in deeper layers triggered by representation drift in shallow layers.
*   **Comparative Performance:** Compared to SEUF (soft regularization) and dense methods like NPO/RMU, GRIP utilizes hard geometric constraints to successfully account for the dynamic dependencies of MoE computation graphs.

---

## Contributions

*   **Architectural Analysis:** Provides a critical analysis diagnosing why traditional unlearning methods fail on MoE, identifying router manipulation as the primary failure mode.
*   **Framework Design:** Introduces an algorithm-agnostic geometric constraint framework that adapts dense architecture unlearning research to MoE architectures.
*   **True Knowledge Erasure:** Forces true knowledge erasure by modifying expert parameters directly, resolving the problem of superficial forgetting associated with router-based redirection.