---
title: MCTS) have proven effective in some domains, their reliance on fixed exploration
arxiv_id: '2506.05213'
source_url: https://arxiv.org/abs/2506.05213
generated_at: '2026-01-28T00:42:02'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-First Search (LFS)
*Tim Rockt, Roberta Raileanu, Nathan Herr*

> ### ðŸ“Š Quick Facts
> | Metric | Details |
> | :--- | :--- |
> | **Method** | LLM-First Search (LFS) |
> | **Core Concept** | Self-Guided Search / Parameter-free |
> | **Models Used** | GPT-4o, o3-mini |
> | **Benchmark Tasks** | Countdown, Sudoku |
> | **Quality Score** | 8/10 |
> | **Citations** | 40 References |

---

## Executive Summary

Current search algorithms used to augment Large Language Model (LLM) reasoningâ€”such as Tree-of-Thoughts (ToT), Best-First Search (BestFS), and Monte Carlo Tree Search (MCTS)â€”suffer from significant inefficiencies due to their reliance on fixed exploration policies and manual hyperparameter tuning. These rigid strategies often require task-specific adjustments and fail to adapt dynamically to the complexity of the reasoning task or the specific capabilities of the model being used. This creates a scalability bottleneck where test-time compute is not utilized optimally.

The authors introduce **LLM-First Search (LFS)**, a "Self-Guided Search" framework that transfers control of the search process entirely to the LLM. Unlike traditional methods that depend on external heuristics, LFS frames reasoning as a Markov Decision Process (MDP) where the model acts as the policy. The algorithm comprises two core components: $P_{evaluate}$, which uses the LLM to score and select the best action while queuing alternatives, and $P_{explore}$, which leverages the LLM to dynamically determine whether to exploit the current path or explore alternatives.

Evaluated on challenging reasoning tasks (Countdown and Sudoku), LFS outperformed ToT-BFS, BestFS, and MCTS in both success rate and computational efficiency. The data highlighted the brittleness of baselines; for instance, MCTS performance was highly sensitive to its exploration constant $C$, whereas LFS demonstrated superior scaling with model strength and compute budget without any parameter adjustments. This research validates that LLMs are capable of autonomously managing their own exploration strategies, providing a more practical pathway for deploying reasoning agents.

---

## Key Findings

*   **Superior Performance:** LLM-First Search (LFS) outperformed established algorithms (Tree-of-Thoughts BFS, Best First Search, and MCTS) on challenging reasoning tasks without requiring additional hyperparameter tuning.
*   **Computational Efficiency:** The method demonstrates superior computational efficiency, particularly when utilizing stronger LLMs.
*   **Scalability:** LFS scales more effectively with model strength and shows better performance scaling as compute budgets increase compared to fixed-strategy alternatives.
*   **Robustness:** The solution avoids common pitfalls such as under-exploration (common in ToT-BFS), over-exploitation (common in BestFS), and sensitivity to fixed parameters (common in MCTS).

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Method Name** | LLM-First Search (LFS) |
| **Core Formulation** | Frames reasoning tasks as an MDP $(S, A, P, R, \gamma)$ where state includes task description and text history, and transition is deterministic. The LLM functions as the policy $\pi_\theta$. |
| **Explore ($P_{explore}$)** | Uses the LLM to decide whether to exploit the current path or explore via a priority queue. |
| **Evaluate ($P_{eval}$)** | Uses the LLM to estimate values for actions, selects the best, and adds alternatives to the queue. |
| **Key Differentiation** | Adaptive guidance via LLM judgment (vs fixed MCTS constant) and parameter-free operation (no hyperparameters). |

---

## Methodology

The authors introduced **LLM-First Search (LFS)**, a 'Self-Guided Search' framework. In this approach, the LLM autonomously controls the search process using internal scoring mechanisms rather than external heuristics or fixed policies. This allows the model to dynamically evaluate the search state and decide whether to continue down a path or explore alternatives.

**Experimental Setup:**
*   **Models:** GPT-4o and o3-mini.
*   **Baselines:** Tree-of-Thoughts BFS (ToT-BFS), Best-First Search (BestFS), Monte Carlo Tree Search (MCTS).
*   **Tasks:** Countdown (lengths 3, 5, 7) and Sudoku.

---

## Results

### Quantitative Results (MCTS Sensitivity)
The study highlighted the sensitivity of standard MCTS to hyperparameter tuning (specifically the exploration constant $C$):
*   **C=0.5:** Achieved the highest WinRate (**7.20**) and EfficiencyScore (**7.16**).
*   **C=1.0:** Performance dropped significantly (WinRate **5.39**, EfficiencyScore **5.31**).
*   **C=2.5:** Performance remained low (WinRate **5.38**, EfficiencyScore **5.25**).

### Qualitative Findings
*   **LFS** successfully outperformed ToT-BFS, BestFS, and MCTS on the reasoning tasks.
*   It demonstrated superior computational efficiency and better scaling with both model strength and compute budget.
*   LFS proved robust against the failure modes of other algorithms (under-exploration, over-exploitation, and fixed parameter rigidity).

---

## Contributions

The research provides several significant advancements to the field:
*   **Adaptive Strategy:** Introduction of an adaptive search strategy that eliminates the rigidity of manual tuning and task-specific adaptation found in current methods.
*   **Validation of Autonomy:** Validation that LLMs can effectively manage exploration strategies independently of external biases.
*   **Resource Efficiency:** Highlights a path toward more practical, resource-effective reasoning agents that better utilize test-time compute.