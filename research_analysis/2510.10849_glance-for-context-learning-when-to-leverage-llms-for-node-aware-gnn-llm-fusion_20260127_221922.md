---
title: 'Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM
  Fusion'
arxiv_id: '2510.10849'
source_url: https://arxiv.org/abs/2510.10849
generated_at: '2026-01-27T22:19:22'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion

*Large Language, Ann Arbor, Donald Loveland, Currently Under, Danai Koutra, Graph Neural, Computer Science*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Citations** | 40 References |
| **Performance Gain** | Up to 13% overall; 20.4% on heterophilous nodes |
| **Key Framework** | GLANCE |
| **Evaluation Metric** | Net Correction Score (NCS) |
| **Datasets** | Cora, Pubmed, Arxiv23 |

---

> ### üìã Executive Summary
>
> This research addresses the inefficiency and diminishing returns of current Graph Neural Network (GNN) and Large Language Model (LLM) integration strategies. While GNNs and LLMs possess complementary strengths‚ÄîGNNs excelling at structural pattern recognition and LLMs at semantic reasoning‚Äîexisting methods rely on uniform fusion. This blunt-force approach applies LLM processing indiscriminately across the entire graph, resulting in prohibitive computational costs and only marginal performance gains. The core challenge is that GNNs already perform well on the majority of nodes (local homophily), meaning expensive LLM calls are largely wasted on nodes that do not require them. The paper highlights the need for a selective method that leverages LLMs only where they provide distinct value, specifically on structurally difficult nodes where GNNs typically fail.
>
> The authors introduce **GLANCE**, a framework that establishes a "selective paradigm" for GNN-LLM fusion, shifting from uniform processing to node-aware routing. Technically, GLANCE employs a lightweight router that analyzes inexpensive per-node signals‚Äîsuch as Relative Degree ($\bar{d}_v$), Local Homophily ($h_v$), clustering density, and uncertainty‚Äîto decide in real-time whether a specific node requires LLM intervention. The framework utilizes an **'LLM-as-Enhancer'** setting where the LLM generates an embedding $z_v$ to augment features rather than predicting labels directly. Crucially, the router is trained using an advantage-based objective function designed to handle non-differentiable LLM API calls. For inference scenarios where ground-truth labels are unavailable, GLANCE utilizes estimated homophily ($\hat{h}_v$) to identify heterophilous nodes without compromising the original graph topology.
>
> Evaluations on Cora, Pubmed, and Arxiv23 demonstrated that traditional heuristics often fail to generalize; notably, uncertainty-based routing yielded negative Net Correction Scores (NCS) on Cora. GLANCE‚Äôs stratified analysis revealed that selectively invoking LLMs significantly improves performance on challenging nodes, achieving a **20.4% increase over GCNII** on heterophilous and low-degree nodes in Cora, with performance discrepancies between subpopulations reaching *30.1%*. While true Local Homophily ($h_v$) achieved the highest NCS (0.29 on Pubmed), the inference-capable estimated homophily ($\hat{h}_v$) proved to be the most robust and practical signal. This work represents a paradigm shift in graph learning, moving the field from computationally expensive, uniform fusion toward adaptive, node-aware architectures.

---

## üîë Key Findings

*   **Complementary Strengths:** GNNs and LLMs excel on distinct structural patterns, specifically local homophily, suggesting a need for targeted rather than broad integration.
*   **Inefficiency of Uniform Fusion:** Applying LLMs uniformly across a graph yields only marginal performance gains while incurring prohibitive computational costs.
*   **Selective Invocation:** By selectively invoking LLMs *only* for difficult heterophilous nodes, performance can improve by **up to 13%**.
*   **Subgroup Insights:** Analyzing performance at the node-subgroup level reveals distinct advantages that aggregate metrics often miss.
*   **Scalability:** Adaptive, node-aware architectures enable scalable LLM-augmented graph learning, making it feasible for large graphs.

---

## üõ†Ô∏è Methodology

The authors propose the **GLANCE framework**, designed to optimize the intersection of GNNs and LLMs through intelligent resource allocation.

*   **Lightweight Router:** The system utilizes a lightweight router to analyze inexpensive per-node signals.
*   **Real-Time Decision Making:** The router decides in real-time if LLM intervention is required for a specific node.
*   **Targeted Intervention:** The strategy focuses specifically on nodes where GNNs typically struggle, rather than applying assistance indiscriminately.
*   **Training Objective:** The router is trained using an **advantage-based objective function**, specifically tailored to handle non-differentiable LLM API calls.

---

## ‚ú® Contributions

1.  **Paradigm Shift:** Introduces a move from uniform to node-aware GNN-LLM fusion strategies targeted at specific structural challenges.
2.  **Cost-Effective Routing:** Presents a novel, adaptive routing mechanism that scales LLM integration for large graphs economically.
3.  **Methodological Insight:** Provides actionable insights by demonstrating that analyzing performance at the node-subgroup level reveals distinct advantages missed by aggregate metrics.

---

## ‚öôÔ∏è Technical Details

### The Selective Paradigm
The paper proposes a 'selective paradigm' for GNN-LLM fusion. This approach adaptively routes specific nodes to the LLM while processing others with the GNN, effectively balancing structural learning and semantic reasoning.

### LLM-as-Enhancer
*   **Setting:** The LLM functions as an enhancer rather than a direct predictor.
*   **Mechanism:** The LLM generates an embedding $z_v$ to augment node features.

### Routing Signals
The router utilizes specific heuristics and structural properties to make decisions:
*   **Heuristics:** Node degree, clustering density, and uncertainty.
*   **Structural Properties:**
    *   Relative Degree ($\bar{d}_v$)
    *   Local Homophily ($h_v$)
    *   Estimated Homophily ($\hat{h}_v$) (Used for inference)

### Evaluation Metrics
*   **Metric:** Net Correction Score (NCS).
*   **Formula:** $(|WC| - |CW|) / |R|$
*   **Key Capability:** Unlike previous methods, this approach performs inference-capable routing without ground-truth labels and preserves the original graph topology.

---

## üìà Results

Experiments were conducted on **Cora**, **Pubmed**, and **Arxiv23** datasets to validate the GLANCE framework.

*   **Heuristic Limitations:** Traditional heuristics failed to generalize. Specifically, uncertainty routing resulted in a negative NCS on Cora.
*   **Stratified Analysis Success:**
    *   LLMs provided a **20.4% performance increase** over GCNII on heterophilous/low-degree nodes in Cora.
    *   Performance differences reached up to **30.1%** between different subpopulations.
*   **Homophily Metrics:**
    *   **True Homophily ($h_v$):** Achieved the highest NCS on Pubmed (**0.29**).
    *   **Estimated Homophily ($\hat{h}_v$):** The most robust inference-capable method, achieving a mean rank of **3.22**.
    *   On Pubmed, $\hat{h}_v$ matched uncertainty's performance with an NCS of **0.20**.