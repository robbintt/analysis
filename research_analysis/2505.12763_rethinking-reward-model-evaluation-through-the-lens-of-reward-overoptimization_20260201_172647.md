# Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization

*Sunghwan Kim; Dongjin Kang; Taeyoon Kwon; Hyungjoo Chae; Dongha Lee; Jinyoung Yeo*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Key Metric** | Gamma ($\gamma$) - Degree of Overoptimization |
| **Base Architectures** | MetaMATH-Mistral-7B, Llama3-8B-Instruct |
| **Task** | Mathematical Reasoning |
| **Optimization Methods** | Best-of-$n$ (BoN) Sampling, PPO |
| **Datasets** | MATH500 (In-Distribution), Gaokao-math (OOD) |
| **References** | 40 Citations |

---

## üìù Executive Summary

Current benchmarks for evaluating reward models (RMs), such as RewardBench, suffer from a critical disconnect: they measure static classification accuracy but fail to predict how an RM will perform as a learning signal during dynamic policy optimization. In Reinforcement Learning from Human Feedback (RLHF), this flaw leads to **reward overoptimization** (Goodhart‚Äôs Law), where a proxy reward model scores increase while the actual policy alignment‚Äîmeasured by a Gold ground-truth model‚Äîdegrades.

The authors propose shifting the evaluation paradigm from static snapshots to the dynamics of reward overoptimization. They introduce a new metric, **Gamma ($\gamma$)**, which quantifies the discrepancy between the Proxy RM and the Gold RM as the policy diverges from its initialization.

By analyzing 16 evaluation designs, the study derives three guidelines for robust benchmarks:
1.  Minimizing non-correctness differences to isolate the learning signal.
2.  Ensuring comparisons across a wide spectrum of response qualities.
3.  Utilizing responses from diverse model sources.

Empirical results show that standard benchmarks exhibit near-zero correlation with downstream policy performance, whereas the proposed $\gamma_{oracle}$ metric demonstrates a strong correlation. This work provides a mathematical framework that treats reward overoptimization not as a failure mode, but as a rigorous diagnostic tool for benchmark quality.

---

## üîë Key Findings

*   **Weak Correlation in Current Benchmarks:** Existing benchmarks for reward models (RMs) demonstrate a weak correlation with the performance of optimized policies.
*   **Minimizing Non-Correctness Differences:** A reliable benchmark must minimize differences between chosen and rejected responses beyond just correctness to prevent reliance on simple heuristics.
*   **Importance of Wide Spectrum Analysis:** Accurate evaluation requires conducting multiple comparisons across a wide spectrum of responses to capture sensitivity throughout the optimization trajectory.
*   **Source Diversity:** Responses should be sourced from a variety of models to mirror practical diversity and prevent overfitting.
*   **The Optimization Trap:** Maximizing for reward overoptimization alone can lead to lower correlation with downstream performances.

---

## üß™ Technical Details

### Proposed Metric: Gamma ($\gamma$)
The authors propose evaluating Reward Models based on the 'Degree of Overoptimization' rather than static accuracy benchmarks. To quantify this, they introduce the metric $\gamma$:

$$ \gamma = \frac{\int_0^k |f(x) - g(x)| dx}{\int_0^k f(x)dx} $$

*   Where $f(x)$ is the Gold RM curve.
*   Where $g(x)$ is the Proxy RM curve.
*   Where $x = \sqrt{D_{KL}(\pi || \pi_{init})}$.

### Mathematical Modeling
Experimental data is fit to the following function to understand the overoptimization curve:

$$ R_{bon}(x) = x(\alpha_{bon} - \beta_{bon}x) $$

### Architecture & Setup
*   **Architecture:** A classification head added to pretrained language models.
*   **Models:** MetaMATH-Mistral-7B and Llama3-8B-Instruct.
*   **Optimization:** Best-of-$n$ (BoN) Sampling and Proximal Policy Optimization (PPO).
*   **Scope:** The study investigates 16 different evaluation designs.

---

## üõ†Ô∏è Methodology

The researchers evaluated reward models by analyzing **reward overoptimization**. This methodology focuses on the phenomenon capturing both alignment with human preferences and the nature of learning signals provided to the policy during optimization. They explored various evaluation designs through this lens to bridge the gap between static benchmarking and actual policy performance.

---

## ‚ú® Contributions

1.  **Paradigm Shift:** Redirected RM evaluation focus from static metrics to the dynamics of reward overoptimization.
2.  **Guidelines for Benchmarks:** Provided three concrete guidelines for constructing robust RM benchmarks:
    *   Minimize non-correctness differences.
    *   Ensure wide-range comparisons.
    *   Utilize diverse model sources.
3.  **Diagnostic Tool:** Clarified the role of reward overoptimization as a useful diagnostic tool rather than a singular optimization target.

---

## üìà Results

*   **Benchmark Failure:** Existing RM benchmarks (e.g., RewardBench) demonstrate weak correlation with downstream policy performance, evidenced by low $r^2$ values in linear regression analyses.
*   **Validation of Gamma:** In contrast, the proposed metric $\gamma_{oracle}$ shows a strong correlation with downstream performance; as $\gamma$ increases (indicating worse overoptimization), downstream performance decreases.
*   **Experimental Validation:** This was validated using MetaMATH-Mistral-7B as the policy model on MATH500 (In-Distribution) and Gaokao-math (Out-of-Distribution, 390 problems).
*   **Overoptimization Onset:** The study quantitatively defined 'Overoptimization Onset' as the point where the Gold RM score begins to decline as KL divergence ($D_{KL}$) increases.

---

**Quality Score:** 9/10  
**References:** 40 citations