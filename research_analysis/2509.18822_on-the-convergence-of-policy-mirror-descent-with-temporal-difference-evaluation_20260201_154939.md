# On the Convergence of Policy Mirror Descent with Temporal Difference Evaluation

*Jiacai Liu; Wenye Li; Ke Wei*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Sublinear Rate** | Dimension-free $O(1/T)$ |
| **Linear Rate** | Dimension-free $\gamma$-rate |
| **Sample Complexity** | $\tilde{O}(|S|\|A\|(1 - \gamma)^{-7}\varepsilon^{-2})$ |
| **Key Algorithms** | TD-PMD, TD-NPG, TD-PQA |

---

## Executive Summary

This research addresses the theoretical disconnect between Temporal Difference (TD) learning methods and the Policy Mirror Descent (PMD) optimization framework. While PMD is a widely used approach for policy optimization, existing convergence analyses typically assume access to Monte Carlo simulations or exact action-value evaluations. These assumptions are often impractical, as Monte Carlo methods suffer from high variance and exact values are unknown in real-world scenarios.

The authors introduce **TD-PMD** (Policy Mirror Descent with Temporal Difference Evaluation), a novel algorithmic framework that integrates a single TD learning step within each iteration of the mirror descent loop. Unlike standard actor-critic methods, TD-PMD decouples the estimated state value ($V_k$) from the true policy value ($V^{\pi_k}$) and utilizes an alternating update scheme.

Technically, the analysis relies on two novel theoretical toolsâ€”**\"monotonicity\"** and **\"shift invariance\"**â€”which allow the authors to prove stability despite the discrepancies inherent in TD evaluation. This framework generalizes popular specific algorithms, including TD-NPG (Natural Policy Gradient) and TD-PQA (Policy Quasi-Newton).

**Significance:** The study establishes strong theoretical guarantees, including dimension-free convergence rates and improved sample complexity compared to standard PMD. This work validates the use of TD methods within mirror descent architectures without succumbing to the curse of dimensionality, setting a new theoretical benchmark for sample efficiency in long-horizon problems.

---

## Key Findings

*   **TD-PMD Performance:** Achieves a dimension-free **$O(1/T)$** sublinear convergence rate assuming access to exact policy evaluations, regardless of initialization and constant step size.
*   **Adaptive Convergence:** The framework achieves a dimension-free **$\gamma$-rate linear convergence** when the step size is selected adaptively.
*   **Specific Algorithm Proofs:** Specific instances of the framework, **TD-PQA** (Policy Quasi-Newton) and **TD-NPG** (Natural Policy Gradient), are proven to converge within the policy domain.
*   **Sample Efficiency:** In inexact settings using a generative model, TD-PMD achieves last-iterate $\varepsilon$-optimality with a sample complexity that offers an **improved dependence on $1/(1-\gamma)$** compared to standard PMD analyses.

---

## Methodology

The research integrates Temporal Difference (TD) evaluation into the Policy Mirror Descent (PMD) framework, creating the TD-PMD approach. This diverges from existing PMD analyses that rely on Monte Carlo simulation or exact action-value estimations.

*   **Analytical Tools:** The analysis employs novel **'monotonicity'** and **'shift invariance'** arguments to establish convergence guarantees.
*   **Study Conditions:** The study investigates the method under two distinct conditions:
    1.  **Exact Setting:** Used for theoretical convergence rates.
    2.  **Inexact Setting:** Used for sample complexity analysis under a generative model.

---

## Technical Details

The paper proposes **TD-PMD**, a novel policy optimization algorithm for tabular MDPs that integrates a TD learning step within the mirror descent loop.

**Architecture & Updates**
*   **Alternating Steps:** The algorithm employs alternating steps:
    1.  **Mirror Descent Step:** Policy improvement maximizing estimated action-value minus Bregman divergence.
    2.  **Critic Step:** A single Bellman operator update ($V_{k+1} = T^{\pi_{k+1}} V_k$).
*   **Value Separation:** A key architectural distinction is the separation of the estimated state value ($V_k$) from the true policy value ($V^{\pi_k}$).

**Generalization**
The framework generalizes to specific algorithms based on the divergence metric used:
*   **TD-NPG:** Uses KL-divergence.
*   **TD-PQA:** Uses Euclidean distance.

---

## Results

The theoretical analysis provides concrete performance bounds for the TD-PMD framework:

*   **Convergence Rates:**
    *   Achieves a dimension-free sublinear convergence rate of **$O(1/T)$** with constant step sizes.
    *   Achieves $\gamma$-rate linear convergence with adaptive step sizes.
*   **Algorithm Specifics:**
    *   **TD-PQA** is proven to find an optimal policy in a *finite number of iterations*.
    *   **TD-NPG** converges to *some* optimal policy.
*   **Sample Complexity:**
    *   In the generative model setting, achieves a rate of **$\tilde{O}(|S||A|(1 - \gamma)^{-7}\varepsilon^{-2})$**.
    *   This improves upon standard PMD by removing a factor of $1/(1-\gamma)$.

---

## Contributions

*   **Bridges PMD and TD:** Extends the Policy Mirror Descent framework to scenarios utilizing Temporal Difference evaluation.
*   **New Theoretical Tools:** Introduces monotonicity and shift invariance arguments for the mathematical analysis of stability and convergence.
*   **Optimization Efficiency:** Establishes dimension-free convergence rates (both sublinear and linear), avoiding the curse of dimensionality regarding convergence speed.
*   **Sample Efficiency:** Advances the state-of-the-art by improving sample complexity bounds for PMD, specifically regarding the dependence on the effective horizon $1/(1-\gamma)$.
