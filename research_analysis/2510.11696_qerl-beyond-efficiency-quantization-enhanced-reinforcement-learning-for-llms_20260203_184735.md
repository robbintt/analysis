---
title: 'QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for
  LLMs'
arxiv_id: '2510.11696'
source_url: https://arxiv.org/abs/2510.11696
generated_at: '2026-02-03T18:47:35'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs

*Wei Huang; Yi Ge; Shuai Yang; Yicheng Xiao; Huizi Mao; Yujun Lin; Hanrong Ye; Sifei Liu; Ka Chun Cheung; Hongxu Yin; Yao Lu; Xiaojuan Qi; Song Han; Yukang Chen*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **GSM8K Accuracy** | 90.8% (Matches Full-Parameter FT) |
| **MATH 500 Accuracy** | 77.4% (Matches Full-Parameter FT) |
| **Speedup** | >1.5x faster during rollout |
| **Hardware Req** | 32B Model on Single H100 80GB |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |

---

## Executive Summary

Reinforcement Learning (RL) alignment for Large Language Models (LLMs) is computationally prohibitive, often requiring massive multi-node clusters to handle the memory and throughput demands of full-parameter fine-tuning. While techniques like Low-Rank Adaptation (LoRA) alleviate memory pressure, the rollout phase remains a significant speed bottleneck. Furthermore, aggressive quantization is traditionally viewed as a necessary evil for efficiency that inevitably degrades model performance, particularly in complex reasoning tasks where precision is critical.

The paper introduces **QeRL**, a framework that fundamentally reconsiders the role of quantization by treating quantization noise as a mechanism for improving exploration rather than merely a source of error. The method employs a hybrid architecture combining **NVFP4**â€”a 4-bit floating-point format utilizing a dual-scaling mechanism with per-tensor global FP32 and block-wise FP8 scalersâ€”with LoRA to optimize memory usage. Technically, QeRL implements **Adaptive Quantization Noise (AQN)**, which injects stochastic noise during forward passes via RMSNorm. This noise increases policy entropy, enhancing strategy discovery; an exponential decay scheduler then dynamically reduces this noise to transition the model from exploration to exploitation as training progresses.

QeRL achieves performance parity with full-parameter fine-tuning while offering significant efficiency gains. On mathematical reasoning benchmarks, it attained **90.8%** accuracy on GSM8K and **77.4%** on MATH 500. Compared to Float16 LoRA and QLoRA baselines, QeRL demonstrated superior reward convergence speed and higher final accuracy. Operationally, the framework achieved over **1.5 times speedup** during the rollout phase and successfully trained a 32B parameter LLM on a single H100 80GB GPU.

This research significantly lowers the hardware barriers to entry for LLM alignment, enabling sophisticated RL training on consumer-grade or single-node enterprise hardware rather than expensive multi-node clusters. By demonstrating that quantization noise can be harnessed to improve exploration and policy outcomes, the authors challenge the standard assumption that lower precision invariably degrades model capability.

---

## Key Findings

*   **Performance Parity**: QeRL matches the performance of full-parameter fine-tuning on mathematical benchmarks, achieving **90.8%** on GSM8K and **77.4%** on MATH 500.
*   **Resource Efficiency**: Achieves over **1.5 times speedup** during rollout and enables RL training of a **32B parameter** LLM on a **single H100 80GB GPU**.
*   **Superior Convergence**: Demonstrates faster reward growth and higher final accuracy compared to 16-bit LoRA and QLoRA baselines.
*   **Exploration Enhancement**: Quantization noise increases policy entropy, which enhances exploration and strategy discovery during training.

---

## Methodology

### Hybrid Architecture
The framework combines **NVFP4 quantization** with **Low-Rank Adaptation (LoRA)**. This hybrid approach directly addresses the memory and speed bottlenecks typically found in RL pipelines.

### Dynamic Noise Regulation
QeRL introduces an **Adaptive Quantization Noise (AQN)** mechanism. This dynamically adjusts noise levels during training to balance exploration and exploitation.

### Entropy Leverage
The framework explicitly uses quantization noise to increase policy entropy during the rollout phase. This deliberate increase in entropy improves the model's exploratory capabilities.

---

## Technical Details

### Quantization Format: NVFP4
The QeRL framework utilizes the NVFP4 format, which employs a dual-scaling mechanism:
*   **Per-tensor global FP32 scaler**
*   **Block-wise FP8 (E4M3) scalers** with a granularity of 16 elements.

**Dequantization Formula:**
$$ \hat{W} = S_{FP32} \cdot (S_{E4M3} \odot \tilde{W}) $$

### Adaptive Quantization Noise (AQN)
*   **Mechanism**: Treats static quantization error as exploration noise by injecting stochastic Gaussian noise ($Z_{noisy}$) during forward passes.
*   **Integration**: Noise is integrated via RMSNorm.
*   **Scheduling**: An exponential decay scheduler adjusts the noise scale $\sigma$ to transition from exploration (high noise) to exploitation (low noise).

### Optimization Strategy
*   **LoRA Application**: Applied to attention and feed-forward projection matrices while original weights are frozen.
*   **Algorithmic Base**: Utilizes **GRPO** (Group Relative Policy Optimization) and **DAPO** (Dynamic Sampling Policy Optimization).

---

## Performance Results

*   **Benchmark Accuracy**:
    *   GSM8K: 90.8%
    *   MATH 500: 77.4%
*   **Comparative Performance**:
    *   Outperformed Float16-LoRA in reward convergence speed and final scores.
    *   NVFP4 demonstrated steeper reward growth and better final rewards compared to MXFP4, NF4, and Float16 baselines.
*   **Entropy & Sampling**:
    *   Quantized models maintained higher sampling entropy (flatter probability distributions) than 16-bit models, directly enhancing strategy discovery.
*   **Qualitative Rankings**:
    *   **Convergence Speed**: NVFP4/MXFP4 > NF4 > Float16
    *   **Final Reward**: NVFP4 > MXFP4 > NF4 > Float16
    *   **Entropy**: 4-bit Quantized > 16-bit Float

---

## Core Contributions

1.  **Efficiency-Performance Trade-off**: Challenges the traditional view that quantization degrades performance. The authors propose a framework where quantization actively improves RL outcomes via enhanced exploration.
2.  **Hardware Accessibility**: Significantly lowers hardware barriers, enabling 32B model training on a single GPU instead of requiring expensive multi-node clusters.
3.  **Optimization of RL Pipelines**: Provides specific optimization for the rollout phase, offering a practical solution for accelerating LLM alignment and reasoning training.

---
**Report Quality Score:** 9/10 | **References:** 40 citations