---
title: 'I Learn Better If You Speak My Language: Understanding the Superior Performance
  of Fine-Tuning Large Language Models with LLM-Generated Responses'
arxiv_id: '2402.11192'
source_url: https://arxiv.org/abs/2402.11192
generated_at: '2026-01-27T22:06:47'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses

*Lingqiao Liu, Biao Wu, Xuan Ren*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Key Performance Gain** | >10% absolute increase on MATH tasks |
| **Primary Mechanism** | Distributional Familiarity (Perplexity) |
| **Target Models** | Llama2-13b-chat, Mistral-7b-Instruct-v2 |
| **Synthetic Sources** | GPT-4, Claude 3.5 |
| **Technique** | LoRA Fine-tuning (Rank 8) |

---

## Executive Summary

The prevalence of using Large Language Model (LLM)-generated synthetic data for fine-tuning has outpaced the theoretical understanding of why it often outperforms human-annotated data. While practitioners widely accept that synthetic data yields superior performanceâ€”particularly in reasoning tasksâ€”the underlying mechanism remains debated. Common assumptions suggest that LLM-generated responses are simply more detailed or comprehensive than human examples. This paper addresses the critical need to isolate the specific factors driving these performance gains, aiming to determine whether the success of synthetic data stems from information density (longer chain-of-thought explanations) or from a more fundamental statistical alignment between the data and the model.

The key innovation introduced is the concept of **"distributional familiarity,"** positing that LLMs perform better when fine-tuned on data that resembles their pre-training distribution versus human text, which often constitutes out-of-distribution noise. Technically, the authors validate this by utilizing perplexity as a quantitative metric for familiarity; they demonstrate that target models (Llama2-13b-chat and Mistral-7b-Instruct-v2) exhibit significantly lower perplexity on synthetic data generated by GPT-4 and Claude 3.5 compared to human-written text. The methodology employs a rigorous experimental setup using Low-Rank Adaptation (LoRA) to fine-tune models while controlling for variable lengths of reasoning steps, effectively isolating distributional alignment as the causal factor for improved learning.

The study provides compelling empirical evidence that fine-tuning with LLM-generated responses consistently surpasses human-based fine-tuning across multiple datasets, including HumanEval, MBPP-full, and ECQA. Specifically, on MATH (Algebra subset), training on synthetic data resulted in an absolute performance increase of over 10% compared to the human-data baseline. Furthermore, the experiments confirmed that LLM-generated data yields substantially lower perplexity scores than human-generated text. Crucially, the results indicated that simply increasing the detail or length of chain-of-thought explanations did not correlate with better outcomes, debunking the hypothesis that information density is the primary driver of success. Instead, models trained on in-distribution synthetic data demonstrated superior cross-domain generalization and avoided catastrophic forgetting of general reasoning capabilities.

This research significantly shifts the paradigm for data curation in LLM development, moving away from an exclusive focus on human-written "gold standards" toward a strategy that prioritizes statistical alignment with the model's internal distribution. By establishing that "familiarity" is a critical component of effective fine-tuning, the authors provide actionable guidelines for practitioners: selecting in-distribution data can optimize task-specific performance while preserving the model's inherent reasoning abilities. This insight is vital for the future of synthetic data generation, suggesting that the most effective training pipelines are those that allow models to "speak their own language" rather than forcing them to mimic human stylistic nuances.

---

## Key Findings

*   **Superior Performance of Synthetic Data:** Fine-tuning LLMs using responses generated by other LLMs yields better performance than using human-generated responses, particularly for reasoning tasks.
*   **Role of Familiarity:** The performance gain is due to the models being inherently more 'familiar' with their own output style and distribution rather than increased detail.
*   **Perplexity as Evidence:** This familiarity is quantitatively evidenced by significantly lower perplexity scores on LLM-generated responses compared to human-generated text prior to fine-tuning.
*   **Preservation of Capabilities:** Training with LLM-generated responses boosts task-specific performance while preserving general reasoning capabilities and preventing catastrophic forgetting.

---

## Methodology

The authors employed a comparative and experimental approach to isolate the causes of performance differences. They investigated the phenomenon by measuring **perplexity** to establish 'familiarity' and designed a series of controlled experiments to test the specific impact of this familiarity on learning performance during the fine-tuning process.

---

## Technical Details

**Target Models**
*   Llama2-13b-chat
*   Mistral-7b-Instruct-v2

**Synthetic Data Generators**
*   GPT-4
*   Claude 3.5

**Fine-Tuning Configuration**
*   **Technique:** LoRA (Rank 8) restricted to Query and Value matrices
*   **Learning Rate:** 2e-5 (Standard) / 2e-4 (Llama2-13b-chat on HumanEval)
*   **Batch Size:** 32 (Standard) / 10 (Small datasets)
*   **Schedule:** Cosine scheduling with 10% warmup
*   **Precision:** Half-precision training

**Datasets**
*   HumanEval (Split 82/82)
*   MATH (Algebra only, numerical answers)
*   MBPP-full
*   ECQA

**Evaluation Protocol**
*   In-domain and Cross-task evaluation (excluding coding tasks)
*   Metric: Pass@1 accuracy

---

## Results

*   **Consistent Outperformance:** Fine-tuning on synthetic data consistently outperformed fine-tuning on human-annotated data.
*   **Significant Math Gains:** Math tasks saw over a 10% absolute increase in performance.
*   **Generalization:** Synthetic datasets accounted for higher cross-domain performance and generalization compared to human data.
*   **Lower Perplexity:** LLM-generated responses exhibited significantly lower perplexity scores than human-generated ones, indicating higher familiarity for the target model.
*   **Detail vs. Familiarity:** The study found that responses with more detailed chain-of-thought explanations did not necessarily yield better training outcomes, refuting the idea that length or detail is the sole cause of success.

---

## Contributions

*   **Theoretical Insight:** Challenges the common assumption that LLM-generated data is better simply because it is more detailed, proposing 'distributional familiarity' as a key contributing factor.
*   **Empirical Validation:** Provides evidence linking pre-training data alignment (measured via perplexity) to optimized fine-tuning outcomes.
*   **Guidelines for Data Selection:** Highlights the value of using in-distribution (LLM-generated) data for fine-tuning to enhance performance while maintaining cross-task reasoning stability.

---
**References:** 40 citations