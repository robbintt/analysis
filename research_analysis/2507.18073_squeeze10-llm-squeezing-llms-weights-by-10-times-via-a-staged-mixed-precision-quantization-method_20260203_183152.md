---
title: 'Squeeze10-LLM: Squeezing LLMs'' Weights by 10 Times via a Staged Mixed-Precision
  Quantization Method'
arxiv_id: '2507.18073'
source_url: https://arxiv.org/abs/2507.18073
generated_at: '2026-02-03T18:31:52'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method

*Qingcheng Zhu; Yangyang Ren; Linlin Yang; Mingbao Lin; Yanjing Li; Sheng Xu; Zichao Feng; Haodong Zhu; Yuguang Yang; Juan Zhang; Runqi Wang; Baochang Zhang*

***

### ⚡ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Compression Ratio** | 10x (16-bit $\rightarrow$ 1.6-bit) |
| **Bit Allocation** | 80% (1-bit) / 20% (4-bit) |
| **Accuracy Recovery** | 43% $\rightarrow$ 56% (Avg) |
| **Models Tested** | LLaMA & LLaMA2-13B |
| **Evaluation Tasks** | 6 Zero-shot classification tasks |
| **Core Innovations** | PBAR & FIAS |

***

## Executive Summary

Deploying Large Language Models (LLMs) on resource-constrained hardware is severely hindered by their massive memory footprint. While quantization offers a path to reduce model size, existing methods for extreme compression—specifically sub-2bit and weight-only quantization—suffer from critical performance degradation. As compression ratios increase, the accuracy of these models typically collapses, rendering them unusable for practical applications. This paper addresses the fundamental challenge of achieving extreme weight compression (10x reduction) without sacrificing the model's zero-shot reasoning capabilities, a trade-off that has historically limited the feasibility of running state-of-the-art LLMs on edge devices.

The authors introduce **Squeeze10-LLM**, a staged mixed-precision post-training quantization (PTQ) framework designed to preserve model fidelity at an ultra-low average density of 1.6 bits per weight. The method relies on two core technical innovations: **Post-Binarization Activation Robustness (PBAR)** and **Full Information Activation Supervision (FIAS)**. PBAR refines weight significance metrics by analyzing quantization-induced perturbations on activations rather than just weight magnitudes, allowing for more intelligent bit allocation. FIAS acts as a stabilization mechanism, preserving full activation information to mitigate cumulative error propagation across deep layers.

Squeeze10-LLM achieves a compression factor of 10, reducing 16-bit weights to an effective density of 1.6 bits by allocating 80% of weights to 1-bit and 20% to 4-bit. Empirical evaluations on LLaMA and LLaMA2-13B models across six zero-shot classification tasks demonstrate substantial recovery of model performance. The method improves average accuracy from 43% to 56%, performing comparably to full-precision 16-bit models. Furthermore, Squeeze10-LLM significantly outperforms the PB-LLM baseline, which suffered a 22.8% accuracy drop, establishing a new state-of-the-art benchmark for sub-2bit weight-only quantization.

This research represents a significant breakthrough in the efficiency of LLM deployment, proving that 10x weight compression is feasible without sacrificing the functional utility required for zero-shot tasks.

***

## Key Findings

*   **Extreme Compression Efficiency:** Squeeze10-LLM compresses 16-bit weights by a factor of 10 to an average density of **1.6 bits per weight**.
*   **Mixed-Precision Allocation:** Achieves optimal compression by strategically allocating **80%** of weights to **1-bit** and **20%** to **4-bit**.
*   **Significant Accuracy Recovery:** Improves average accuracy on zero-shot classification tasks from **43% to 56%** for LLaMA and LLaMA2 models.
*   **State-of-the-Art Performance:** Establishes a new benchmark for sub-2bit weight-only quantization by solving severe performance degradation issues found in previous methods.

***

## Methodology

The researchers propose **Squeeze10-LLM**, a staged mixed-precision post-training quantization (PTQ) framework. The approach relies on two primary technical innovations:

1.  **Post-Binarization Activation Robustness (PBAR):** A refined metric for determining weight significance that accounts for the impact of quantization on activations, rather than solely on weight magnitude.
2.  **Full Information Activation Supervision (FIAS):** A strategy designed to preserve full activation information and mitigate cumulative error propagation across layers in deep networks.

***

## Technical Details

Squeeze10-LLM utilizes a three-step staged mixed-precision quantization framework to achieve an average density of 1.6 bits per weight.

**Framework Configuration:**
*   **Bit Allocation:** 80% 1-bit weights, 20% 4-bit weights.
*   **Compression:** 10x reduction from standard 16-bit weights.

**Process Stages:**
1.  **Intermediate Quantization:** Applies 4-bit uniform asymmetric quantization.
2.  **Salience-based Binarization:** Utilizes the **Post-Binarization Activation Robustness (PBAR)** metric to determine which weights can be safely binarized to 1-bit.
3.  **Mixed-Bit Supervision:** Employs **Full Information Activation Supervision (FIAS)** to guide layer-wise quantization and minimize accumulated errors.

***

## Results

The proposed method was evaluated on the LLaMA and LLaMA2 families (specifically LLaMA2-13B) across 6 zero-shot classification tasks.

*   **Accuracy Performance:** Improved average accuracy from **43%** to **56%**, performing comparably to 16-bit full-precision weights.
*   **Baseline Comparison:** Significantly outperformed the PB-LLM baseline (which suffered a **22.8%** accuracy drop) and other mainstream ultra-low-bit quantization techniques.
*   **Efficiency:** Successfully maintained zero-shot task capabilities while adhering to the strict 1.6-bit density constraint.

***

## Contributions

*   **Addresses Critical Limitations:** Solves the severe performance drops associated with existing ultra low-bit quantization methods.
*   **Introduces PBAR:** Shifts weight importance analysis to include activation robustness, allowing for smarter bit allocation.
*   **Contributes FIAS:** Provides a mechanism to handle deep quantized network stability by maintaining full information flow during inference.
*   **Empirical Validation:** Provides robust validation on LLaMA and LLaMA2, demonstrating that extreme 10x compression is feasible without sacrificing zero-shot task capabilities.

***

**Quality Score:** 9/10  
**References:** 40 citations