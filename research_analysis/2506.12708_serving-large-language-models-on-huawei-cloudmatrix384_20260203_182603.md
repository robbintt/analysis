---
title: Serving Large Language Models on Huawei CloudMatrix384
arxiv_id: '2506.12708'
source_url: https://arxiv.org/abs/2506.12708
generated_at: '2026-02-03T18:26:03'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Serving Large Language Models on Huawei CloudMatrix384

*Pengfei Zuo; Huimin Lin; Junbo Deng; Nan Zou; Xingkun Yang; Yingyu Diao; Weifeng Gao; Ke Xu; Zhangyu Chen; Shirui Lu; Zhao Qiu; Peiyang Li; Xianyu Chang; Zhengzhong Yu; Fangzheng Miao; Jia Zheng; Ying Li; Yuan Feng; Bei Wang; Zaijian Zong; Mosong Zhou; Wenli Zhou; Houjiang Chen; Xingyu Liao; Yipeng Li; Wenxiao Zhang; Ping Zhu; Yinggang Wang; Chuanjie Xiao; Depeng Liang; Dong Cao; Juncheng Liu; Yongqiang Yang; Xiaolong Bai; Yi Li; Huaguo Xie; Huatao Wu; Zhibin Yu; Lv Chen; Hu Liu; Yujun Ding; Haipei Zhu; Jing Xia; Yi Xiong; Zhou Yu; Heng Liao*

---

### ðŸ“Š Quick Facts

| Metric | Specification |
| :--- | :--- |
| **Architecture** | CloudMatrix384 (384 Ascend 910 NPUs, 192 Kunpeng CPUs) |
| **Interconnect** | Unified Bus (UB) Network |
| **Test Model** | DeepSeek-R1 |
| **Prefill Speed** | 6,688 tokens/s per NPU |
| **Decode Speed** | 1,943 tokens/s per NPU |
| **Latency Constraint** | 15 ms (sustains 538 tokens/s) |
| **Quantization** | INT8 |
| **Quality Score** | 8/10 |

---

## Executive Summary

The research addresses the escalating computational and communication bottlenecks associated with deploying state-of-the-art Mixture-of-Experts (MoE) Large Language Models (LLMs), which now exceed trillions of parameters. As model complexity growsâ€”particularly with architectures requiring long context windowsâ€”traditional serving systems struggle to manage the massive inter-device bandwidth requirements and synchronization latencies inherent in expert parallelism. This creates a critical infrastructure gap where existing hardware and software stacks cannot sustain the high throughput and low latency necessary for real-time inference on massive-scale models without significant resource inefficiency.

To overcome these limitations, the authors present a hardware-software co-design centered on the CloudMatrix384 "AI supernode" and the CloudMatrix-Infer serving framework. The CloudMatrix384 architecture integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs via a proprietary Unified Bus (UB) network, enabling fully peer-to-peer connectivity and fine-grained resource disaggregation. Complementing this hardware, the software stack implements large-scale Expert Parallelism (EP320), hardware-aware specialized operators, and microbatch-based pipelining. This approach optimizes data movement by reducing communication overhead and maximizes resource utilization through techniques like INT8 quantization, specifically targeting the communication-heavy requirements of massive MoE architectures.

Evaluations using the DeepSeek-R1 model demonstrate state-of-the-art inference performance, achieving a prefill throughput of 6,688 tokens/s and a decode throughput of 1,943 tokens/s per NPU. The system exhibits robustness under strict Service Level Agreements (SLAs), sustaining 538 tokens/s per NPU even under a stringent 15 ms latency constraint. Furthermore, the study confirms that INT8 quantization effectively maintains model accuracy while enhancing computational efficiency. These results validate the system's capability to handle current industry giants like the 671B parameter DeepSeek-V3 and future models exceeding 2 trillion parameters, such as the projected Llama 4 Behemoth.

This work sets a new benchmark for the scalability and efficiency of AI datacenters, offering a viable blueprint for serving the next generation of trillion-parameter models. By successfully demonstrating that a unified, peer-to-peer supernode architecture can eliminate the communication bottlenecks typical of traditional cluster designs, the research paves the way for more cost-effective and high-performance AI infrastructure.

---

## Key Findings

*   **State-of-the-Art Throughput:** CloudMatrix-Infer delivers exceptional performance with DeepSeek-R1, achieving **6,688 tokens/s** prefill and **1,943 tokens/s** decode per NPU.
*   **Strict Latency Adherence:** The system sustains high throughput (**538 tokens/s** per NPU) even under stringent **15 ms** latency constraints, meeting rigorous Service Level Agreements (SLAs).
*   **Efficient Quantization:** The implementation of **INT8 quantization** maintains model accuracy while significantly enhancing computational efficiency and resource utilization.
*   **Scalable Architecture:** The CloudMatrix384 architecture successfully integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs via a Unified Bus network, enabling scalable large-scale MoE architectures.

---

## Technical Details

The CloudMatrix384 system represents a "fully peer-to-peer interconnected AI supernode" designed to address the specific needs of massive MoE models.

### Hardware Architecture
| Component | Specification |
| :--- | :--- |
| **NPUs** | 384x Ascend 910 |
| **CPUs** | 192x Kunpeng |
| **Network** | Proprietary Unified Bus (UB) |
| **Design Goal** | Fine-grained resource disaggregation and dynamic composition |

### Software & Optimization Stack
*   **Framework:** CloudMatrix-Infer (Peer-to-peer serving architecture).
*   **Parallelism Strategy:** Large-scale Expert Parallelism (EP320).
*   **Pipeline:** Microbatch-based pipelining.
*   **Operators:** Hardware-aware specialized operators and fused communication operators.
*   **Compute Stack:** CANN (Compute Architecture for Neural Networks) with INT8 quantization support.

### Target Model Support
The system is validated and designed to support massive parameter counts, including:
*   **DeepSeek-V3:** 671B parameters.
*   **Pangu Ultra MoE:** 718B parameters.
*   **Llama 4 Behemoth:** ~2T parameters (Projected).

---

## Methodology

The research employs a rigorous **hardware-software co-design** approach:

1.  **Infrastructure:** Utilization of the CloudMatrix384 supernode, which leverages the Unified Bus network to interconnect hundreds of NPUs and CPUs, specifically to mitigate communication bottlenecks found in Tensor and Expert Parallelism.
2.  **Software Solution:** Implementation of CloudMatrix-Infer, featuring a peer-to-peer architecture that eliminates central bottlenecks.
3.  **Optimization Techniques:** Application of specialized operators and microbatch-based pipelining to maximize hardware utilization.
4.  **Evaluation:** The methodology is stress-tested using the **DeepSeek-R1** model. Performance is measured across three critical dimensions: **Throughput** (tokens/s), **Latency** (ms), and **Accuracy** (post-quantization).

---

## Performance Results

The evaluation of the CloudMatrix-Infer solution on the CloudMatrix384 hardware yielded the following benchmark results using the DeepSeek-R1 model:

| Benchmark Metric | Result | Significance |
| :--- | :--- | :--- |
| **Prefill Throughput** | 6,688 tokens/s/NPU | Industry-leading speed for prompt processing. |
| **Decode Throughput** | 1,943 tokens/s/NPU | High-speed generation capabilities. |
| **SLA Performance** | 538 tokens/s/NPU | Maintained under strict 15ms latency constraints. |
| **Quantization Impact** | Accuracy Preserved | INT8 quantization proved efficient without quality loss. |

---

## Core Contributions

*   **CloudMatrix384 Architecture:** Introduction of a next-generation AI datacenter design featuring a Unified Bus network specifically optimized for communication-intensive MoE workloads.
*   **CloudMatrix-Infer Solution:** Development of a comprehensive LLM serving framework characterized by a peer-to-peer architecture and a specialized expert parallelism strategy (EP320).
*   **Performance Optimization:** Demonstration of advanced hardware-software integration techniques, including microbatch-based pipelining, which set new efficiency benchmarks for long-context LLMs.

---

**References:** 40 citations | **Quality Score:** 8/10