---
title: 'HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models
  for Efficient Multimodal Hotel Retrieval'
arxiv_id: '2506.07296'
source_url: https://arxiv.org/abs/2506.07296
generated_at: '2026-01-27T21:56:27'
quality_score: 9
citation_count: 8
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval

*Ilya Gusev, Emmanouil Stergiadis, Arian Askari, Moran Beladev (Leiden University)*

---

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Main Test Set Score** | **0.681** (Baseline MARVEL: 0.603) |
> | **Document Corpus** | 3.1 Million documents |
> | **Avg. Images per Hotel** | 44.6 |
> | **Query Model (SLM)** | ~110M Parameters |
> | **Document Model (LLM)** | 330M to 7B Parameters |
> | **Visual Encoder** | CLIP (224x224 resolution) |
> | **Training Strategy** | Multi-task (Retrieval, Visual, Language) |

---

## Executive Summary

This paper addresses the challenge of **high-efficiency multimodal information retrieval** within the hospitality sector, specifically the difficulty of aligning complex natural language queries with hotel inventory that includes extensive visual data. Traditional search methods often fail to effectively correlate nuanced textual descriptions with rich image galleries, while standard Large Language Models (LLMs) are typically computationally prohibitive for real-time retrieval applications.

The key innovation is an **asymmetrical dense retrieval architecture** that decouples query encoding from document embedding, combined with a joint multi-task training optimization strategy. The system employs a dual-encoder design where a Small Language Model (SLM, ~110M parameters) processes user queries for high efficiency, while a Large Language Model (LLM) encodes documents to capture deep semantic context. 

HotelMatch-LLM achieved a score of **0.681** on the main query test set, significantly outperforming state-of-the-art baselines MARVEL (0.603) and VISTA. This research establishes a new performance benchmark for multimodal retrieval and validates the efficacy of hybrid architectures that balance computational cost with semantic capability.

---

## Key Findings

*   **Superior Performance:** HotelMatch-LLM significantly outperforms state-of-the-art models (VISTA and MARVEL) across four diverse test sets.
*   **Benchmark Achievement:** On the main query type test set, the model achieved a score of **0.681**, surpassing the baseline MARVEL (0.603).
*   **Scalability:** The architecture demonstrates high scalability, effectively processing a corpus of 3.1 million documents and large image galleries.
*   **Optimization Strategy:** The proposed multi-task optimization strategy positively impacts overall model performance.
*   **Generalizability:** The model exhibits robust generalizability across different Large Language Model (LLM) architectures.
*   **Training Efficiency:** Language models pre-fine-tuned for text retrieval converged faster during multimodal retrieval optimization compared to those pre-fine-tuned for generative tasks.

---

## Methodological Approach

The research introduces a multi-modal dense retrieval framework designed to bridge the gap between user queries and complex hotel inventories. The methodology rests on three pillars:

1.  **Asymmetrical Dense Retrieval Architecture:**
    *   Decouples query processing from data embedding to optimize for latency and depth.
    *   Uses a lightweight Small Language Model (SLM) for incoming user queries.
    *   Utilizes a powerful Large Language Model (LLM) for indexing and encoding hotel documents.

2.  **Domain-Specific Multi-Task Optimization:**
    *   Simultaneously trains on retrieval, visual understanding, and language modeling tasks to improve semantic alignment.

3.  **Extensive Multimodal Processing:**
    *   Processes entire property galleries rather than single images, ensuring comprehensive visual representation.

---

## Technical Details

### Architecture & Components
*   **Design:** Asymmetrical Dense Retriever with a **Dual-Encoder** design.
*   **Query Encoder:** Small Language Model (SLM) with ~110M parameters for efficient latency.
*   **Document Encoder:** Large Language Model (LLM) ranging from 330M to 7B parameters for rich semantic encoding.
*   **Alignment:** A dense linear layer aligns the embedding dimensions between the SLM and LLM.

### Image & Visual Processing
*   **Encoder:** CLIP visual encoder.
*   **Input Specs:** Images resized to **224x224** pixels, divided into 49 patches.
*   **Aggregation:** Mean pooling is used to aggregate features from multiple images per property.

### Training Strategy
The model employs a unified joint training framework consisting of three distinct loss functions/tasks:
1.  **Retrieval Learning:** Utilizes **InfoNCE** contrastive loss for query-document matching.
2.  **Masked Language Modeling (MLM):** Involves masking specific tokens (e.g., city/country) to improve context understanding.
3.  **Visual Facility Learning:** Predicts **120 facility labels** via the MUMIC method.

### Data Infrastructure
*   **Corpus Size:** 3.1 million documents.
*   **Labels:** Utilizes synthetic labels generated by **GPT-4o** to enhance training signals.

---

## Contributions

*   **Advancement in Natural Language Search:** Overcomes traditional limitations by enabling natural language property search that effectively handles complex semantic queries.
*   **Hybrid Architecture Design:** Introduces a novel asymmetrical architecture combining SLM efficiency for queries with LLM power for indexing.
*   **Superior Multimodal Retrieval:** Establishes a new performance benchmark through a unified joint training framework that integrates complex visual and textual data.

---

**Quality Score:** 9/10  
**References:** 8 citations