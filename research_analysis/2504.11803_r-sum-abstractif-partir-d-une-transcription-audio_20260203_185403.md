---
title: "R\xE9sum\xE9 abstractif \xE0 partir d'une transcription audio"
arxiv_id: '2504.11803'
source_url: https://arxiv.org/abs/2504.11803
generated_at: '2026-02-03T18:54:03'
quality_score: 8
citation_count: 35
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# R√©sum√© abstractif √† partir d'une transcription audio

*Ilia Derkach*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Total References:** 35 Citations
> *   **Parameter Efficiency:** ~100x reduction in trainable parameters (LoRA)
> *   **Memory Footprint:** LoRA adapters constitute only ~0.2% of total memory usage
> *   **Cost Avoidance:** Eliminates the need for ~20,352 GPU-V100 days (associated with training a 12B model)

---

## üìù Executive Summary

Large Language Models (LLMs) offer superior performance in abstractive summarization, yet their application to audio data remains constrained by the exorbitant computational costs of training or fine-tuning massive parameter sets from scratch. This paper addresses the critical barrier of resource accessibility, specifically investigating how to adapt pre-trained text-based LLMs for audio-to-text summarization without requiring enterprise-scale computing infrastructure. The challenge is significant: while the demand for multi-modal summarization grows, the hardware requirements often relegate advanced capabilities to well-funded organizations, stifling broader research and deployment in environments where resources are at a premium.

The core innovation is an End-to-End (E2E) architecture that leverages Parameter-Efficient Fine-Tuning (PEFT) to adapt pre-trained LLMs to audio data. The research clarifies the audio processing pipeline as a two-step approach: converting audio to text via Automatic Speech Recognition (ASR), followed by abstractive summarization using the LLM. Technically, the method avoids updating the full model by freezing pre-trained weights and injecting trainable low-rank decomposition matrices via Low-Rank Adaptation (LoRA). The study further optimizes efficiency through Adaptive LoRA (AdaLoRA), which dynamically allocates parameters using Singular Value Decomposition (SVD), and QLoRA, which combines 4-bit quantization of the base model with trainable BF16 adapters, drastically reducing memory footprints.

Empirical results validate both the resource efficiency and the output quality of the proposed methods. Regarding NLP performance, the PEFT-optimized models achieved ROUGE scores comparable to those of full fine-tuning, demonstrating that the reduction in trainable parameters does not compromise summary quality. In terms of hardware efficiency, the method successfully reduced trainable parameters by approximately 100x, with LoRA adapter weights for a 7B LLaMA model accounting for only 0.2% of total memory. To contextualize these savings, the authors contrast their approach with the estimated 20,352 GPU-V100 days required to fully train a 12B parameter model from scratch, highlighting that PEFT renders such massive expenditure unnecessary by effectively addressing memory bottlenecks associated with gradient storage.

This research has significant implications for the democratization of advanced AI technologies. By proving that high-quality audio summarization can be achieved on resource-constrained hardware, the paper lowers the barrier to entry for researchers and developers lacking access to massive computing clusters. The findings suggest a paradigm shift toward the efficient adaptation of foundation models, promoting a future where sophisticated NLP tasks are more accessible and deployable in real-world, compute-limited environments.

---

## üîë Key Findings

*   **Efficiency of LLMs:** Large Language Models (LLMs) are powerful but computationally expensive to train from scratch.
*   **Viability of Fine-Tuning:** Fine-tuning techniques, specifically LoRA and quantization, provide a viable solution for adapting existing models to specific tasks without massive computing resources.
*   **E2E Architecture:** An End-to-End (E2E) architecture is effectively applicable to audio summarization when utilizing these optimization techniques.
*   **Method Validation:** LoRA and quantization are confirmed as effective and applicable for solving the specific problem of audio summarization.

---

## üõ†Ô∏è Methodology

The research proposes the development of an **End-to-End (E2E) audio summarization model**. The core methodological strategy involves:

1.  Leveraging **Parameter-Efficient Fine-Tuning (PEFT)** techniques.
2.  Specifically utilizing **LoRA** and **quantization**.
3.  Adapting pre-existing Large Language Models (LLMs) for the target task, rather than training a new model from scratch.

---

## ‚ú® Contributions

*   **Model Architecture:** Introduction of an E2E audio summarization model designed to operate efficiently without extensive training infrastructure.
*   **Democratization:** Validation of using LoRA and quantization to democratize access to advanced NLP tasks by reducing hardware barriers.
*   **Method Assessment:** Assessment of the effectiveness and applicability of specific fine-tuning methods (LoRA, quantization) within the context of processing and summarizing audio data.

---

## ‚öôÔ∏è Technical Details

The paper addresses End-to-End (E2E) audio summarization by adapting Large Language Models (LLMs) using Parameter Efficient Fine-Tuning (PEFT).

*   **End-to-End (E2E) Architecture**
    The model processes audio data by converting it to text via Automatic Speech Recognition (ASR), followed by abstractive summarization using the adapted LLM.

*   **Low-Rank Adaptation (LoRA)**
    *   **Mechanism:** Freezes pre-trained weights and trains a rank decomposition matrix.
    *   **Rank:** Typically between 8 and 128.
    *   **Impact:** Reduces trainable parameters by a factor of approximately 100.

*   **Adaptive LoRA (AdaLoRA)**
    *   **Mechanism:** Dynamically allocates parameters using Singular Value Decomposition (SVD) and regularization.

*   **Quantization**
    *   **Mechanism:** Reduces memory by converting weights to low-bit types (e.g., int8, int4) via asymmetric or symmetric methods.

*   **QLoRA**
    *   **Mechanism:** Combines a frozen, 4-bit quantized base model with trainable BF16 LoRA adapters.

---

## üìà Results

*   **Parameter Reduction:** Theoretical metrics indicate LoRA reduces trainable parameters by approximately **100x** compared to full fine-tuning. Effective fine-tuning is achievable at a low rank (e.g., 8).
*   **Resource Benchmarks:**
    *   Full training of a 12B parameter model required **20,352 GPU-V100 days**.
    *   In a 7B LLaMA model, LoRA adapter weights constitute only **~0.2%** of total memory usage.
    *   The primary bottleneck in this architecture is identified as the gradients of frozen weights.
*   **Conclusion:** The combination of LoRA and quantization is effective for solving the audio summarization problem within limited computing resources.

---

**References:** 35 citations