---
title: An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits
arxiv_id: '2505.08823'
source_url: https://arxiv.org/abs/2505.08823
generated_at: '2026-02-03T18:57:29'
quality_score: 6
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits

*Cody Steinmetz; Gavin Childress; Aaron Herbst; Gavin Jones; Jasdeep Singh; Eli Vang; Keagan Weinstock*

---

> ### **Quick Facts**
>
> *   **Target Bit-width:** 1.58-bit (Ternary)
> *   **Core Innovation:** Extra RMSNorm before linear projections
> *   **Hardware Efficiency:** 70B model on single 24GB GPU
> *   **Training Method:** Straight-Through Estimator (STE) + Gradual Schedule
> *   **Knowledge Distillation:** Not Required
> *   **Citations:** 11

---

## Executive Summary

This research addresses the critical challenge of fine-tuning Large Language Models (LLMs) at extremely low bit-widths, specifically targeting 1.58-bit (ternary) quantization. While reducing model precision is essential for lowering memory footprints and computational costs, extreme quantization typically induces severe training instability and significant performance degradation compared to full-precision models. Current state-of-the-art solutions often rely on complex knowledge-distillation pipelines or intricate architectural modifications to recover accuracy, thereby adding substantial complexity to the deployment workflow.

The key innovation proposed is the insertion of an additional Root Mean Square Layer Normalization (RMSNorm) layer immediately before every linear projection within the Transformer architecture. By building upon bias-free, RMS-normalized architectures and utilizing Straight-Through Estimator (STE) for gradient computation, this simple modification stabilizes the training dynamics required for ternary weights constrained to the set {-1, 0, +1}. The methodology employs a gradual, layer-wise quantization schedule controlled by a lambda parameter, which serves as a scheduler to progressively interpolate the model weights from full precision to the discrete ternary representation.

This approach effectively decouples low-bit fine-tuning from knowledge distillation, proving that careful normalization is sufficient to maintain gradient flow and model accuracy without increasing model complexity. On standard benchmarks such as WikiText2 and C4, the approach maintains perplexity parity with full-precision baselines, recovering over 99% of the original performance.

In terms of hardware efficiency, the approach enables the fine-tuning of massive 70B parameter models on a single 24GB consumer-grade GPU. Furthermore, the authors demonstrated that the DeepSeek R1 model can be loaded and fine-tuned entirely within GPU memory using only two standard DGX nodes or a single DGX workstation. These results highlight a drastic reduction in VRAM requirements, verifying that the method achieves stability and performance parity without the resource overhead typically associated with training large-scale models.

---

## Key Findings

*   **Stable Ternary Fine-tuning:** Simply inserting an RMS normalization layer before every linear projection enables the stable fine-tuning of full-precision LLMs into ternary (1.58-bit) LLMs.
*   **Performance Parity:** The proposed approach matches or surpasses the performance of more complex knowledge-distillation pipelines without adding model complexity.
*   **Critical Role of Normalization:** Careful normalization significantly closes the performance gap between ternary and full-precision models, validating it as a critical factor in extreme quantization.

---

## Methodology

The researchers built upon bias-free, RMS-normalized Transformer architectures utilizing straight-through estimation (STE). The methodology focuses on two primary architectural and training adjustments:

1.  **Architectural Change:** Insertion of an additional RMS normalization layer immediately prior to every linear projection.
2.  **Training Schedule:** Application of a gradual, layer-wise quantization schedule to fine-tune existing full-precision checkpoints, controlling the transition from full precision to ternary weights.

---

## Technical Details

### **Architecture & Quantization**
*   **Normalization:** Inserts an **Extra RMSNorm** layer before every linear projection in the Transformer architecture.
*   **Weight Constraints:** Targets ternary quantization (1.58-bit) with weights constrained to the set **{-1, 0, +1}**.
*   **Foundation:** Builds upon the BitNet architecture, incorporating bias removal and scaled SwiGLU activations.

### **Training Strategy**
*   **Gradient Computation:** Utilizes **Straight-Through Estimator (STE)** to handle the discrete nature of ternary weights during backpropagation.
*   **Scheduler:** Implements a gradual quantization schedule controlled by a **lambda parameter** to progressively interpolate weights.
*   **Implementation:** Uses a custom quantized linear layer designed for fine-tuning pre-trained full-precision LLMs.
*   **Distillation Independence:** Functions completely without requiring Knowledge Distillation.

---

## Results & Contributions

### **Hardware Efficiency**
*   **70B Parameter Models:** Enabled loading and fine-tuning on a **single 24GB GPU**.
*   **DeepSeek R1:** Successfully fit entirely within GPU memory on either two DGX 'Sparks' nodes or a single DGX workstation.

### **Core Contributions**
*   **Simplification:** Demonstrated that training instability in low-bit regimes can be resolved with a simple architectural change (Extra RMSNorm), removing the need for complex pipelines.
*   **Efficiency Optimization:** Provided a pathway to ultra-low-bit inference that maintains high accuracy without increasing model complexity.
*   **Validation:** Validated normalization as the critical component in maintaining model accuracy during extreme quantization.

---

**Analysis Quality Score:** 6/10  
**References:** 11 Citations