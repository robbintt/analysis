# Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression

*Kyo Kuroki; Yasuyuki Okoshi; Thiem Van Chu; Kazushi Kawamura; Masato Motomura*

---

> ### ðŸ“Š Quick Facts
>
> *   **Paper Quality:** 9/10
> *   **Primary Improvement:** Up to **2.2%** (Calibration-based) and **59.1%** (Data-free) accuracy gain over SOTA.
> *   **Target Architecture:** Vision Transformers (ViT)
> *   **Bit-Width:** 2-bit equivalent quantization
> *   **Optimization Method:** Greedy Alternating Optimization & Annealed Mean Field Descent (AMFD)
> *   **Key Benefit:** No Quantization-Aware Training (QAT) or specialized optimization tricks required.

---

## Executive Summary

This research addresses the fundamental limitations of conventional scalar first-order quantization methods, which rely on linear approximations to compress real-valued matrices. As neural networks, particularly Vision Transformers (ViTs), grow in size and complexity, existing low-bit quantization techniques reach a bottleneck where sacrificing accuracy for memory efficiency becomes inevitable. The paper highlights the specific challenge of **Post-Training Quantization (PTQ)**: achieving high-compression ratios without the computational cost of Quantization-Aware Training (QAT) or access to extensive calibration data.

The authors introduce **Binary Quadratic Quantization (BQQ)**, a novel framework that transcends linear limitations by utilizing binary quadratic expressions for matrix approximation. Unlike standard methods that approximate a weight matrix using simple scalar coefficients, BQQ models the matrix as a linear combination of products between binary matrices ($Y_i$ and $Z_i$), effectively capturing second-order interactions while maintaining a compact binary storage format. To solve the resulting non-convex optimization problem, the paper proposes **Greedy Alternating Optimization**, which decouples continuous variable estimation (via convex quadratic optimization) from binary variable selection.

The proposed BQQ method demonstrates **state-of-the-art performance** in compressing Vision Transformer models on ImageNet benchmarks using a 2-bit equivalent quantization scheme. In calibration-based scenarios, BQQ improves accuracy by up to 2.2% over existing SOTA methods. More significantly, in data-free scenarios where no training data is available, BQQ achieves a staggering **59.1% improvement** in performance relative to current baselines. These gains were attained without the need for specialized optimization tricks or fine-tuning, relying solely on the mathematical robustness of the binary quadratic formulation.

---

## Key Findings

*   ** Superior Trade-off:** BQQ achieves a significantly better balance between memory efficiency and reconstruction error compared to conventional first-order quantization methods.
*   **State-of-the-Art Performance:** On ImageNet benchmarks using Vision Transformers, BQQ outperforms existing methods by:
    *   **2.2%** in calibration-based scenarios.
    *   **59.1%** in data-free scenarios.
*   **Optimization Free:** Strong performance is attained without specialized optimization techniques or heuristics, simply by leveraging binary quadratic expressions.
*   **Broad Validation:** The method is validated successfully on general matrix compression benchmarks and the compression of large-scale neural networks.

---

## Methodology

The paper introduces **Binary Quadratic Quantization (BQQ)**, a matrix quantization framework designed to overcome the expressive limitations of linear first-order approaches.

1.  **Framework Logic:** Instead of standard scalar linear approximations, BQQ utilizes binary quadratic expressions to compress real-valued matrices. This increases expressive power while maintaining a compact data format.
2.  **Validation Protocol:** The approach was tested through:
    *   General matrix compression benchmarks.
    *   Post-training quantization (PTQ) applied to pretrained Vision Transformer models.
    *   Implementation of a 2-bit equivalent quantization scheme.

---

## Technical Details

### Core Formulation
BQQ approximates a real-valued matrix as a linear combination of binary matrix products (quadratic terms) rather than linear approximations.

$$
W \approx \sum_{i=0}^{p-1} (r_i Y_i Z_i + s_i Y_i \mathbf{1}_Z + t_i \mathbf{1}_Y Z_i) + u\mathbf{1}
$$

*   **Variables:** $W$ is the real-valued matrix; $Y_i$ and $Z_i$ are binary matrices.

### Optimization Strategy
The optimization process minimizes the squared Frobenius norm reconstruction error using **Greedy Alternating Optimization**:

1.  **Decoupling:** Separates the problem into:
    *   **Continuous Variables:** Solved via convex quadratic optimization.
    *   **Binary Variables:** Solved via Polynomial Unconstrained Binary Optimization (PUBO).
2.  **Algorithm:** An extended **Annealed Mean Field Descent (AMFD)** algorithm minimizes Kullback-Leibler divergence via temperature annealing to solve the binary components.

### Implementation Specs
*   **Target:** Vision Transformers (ViT).
*   **Parameter Efficiency:** Utilizes group-wise scaling to reduce parameter overhead.
*   **Scenarios:** Supports both **Data-Free** and **Calibration-Based** quantization scenarios.
*   **Hardware Friendliness:** Avoids floating-point codebooks, relying on binary matrix products.

---

## Results

*   **ImageNet Benchmarks:**
    *   Outperforms state-of-the-art first-order quantization methods by up to **2.2%** in calibration-based scenarios.
    *   Outperforms baselines by up to **59.1%** in data-free scenarios.
*   **Efficiency:** Achieves a superior memory versus accuracy trade-off.
*   **Compression Scope:** Validation performed on ViT models, general matrix compression, and large-scale neural network compression.
*   **Training Requirement:** Achieves strong results entirely without Quantization-Aware Training (QAT).

---

## Contributions

*   **Paradigm Shift:** Challenges the industry's standard reliance on first-order quantization by demonstrating the effectiveness of quadratic expressions.
*   **Novel Algorithm:** Introduces BQQ, a compression algorithm that combines high expressiveness with high storage efficiency.
*   **PTQ Advancement:** Advances Post-Training Quantization for Vision Transformers by providing a strategy that does not require specialized optimization routines, achieving state-of-the-art results.
*   **Empirical Evidence:** Provides rigorous validation via ImageNet benchmarks, showing significant gains in neural network accuracy under low-bit constraints.

---

**References:** 40 citations