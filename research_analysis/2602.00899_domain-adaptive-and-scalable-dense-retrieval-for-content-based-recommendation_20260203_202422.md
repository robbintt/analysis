---
title: Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation
arxiv_id: '2602.00899'
source_url: https://arxiv.org/abs/2602.00899
generated_at: '2026-02-03T20:24:22'
quality_score: 8
citation_count: 4
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation
*Mritunjay Pandey*

---

> ### ðŸ“Š Quick Facts
> ---
> **Model Architecture:** Two-Tower Bi-Encoder  
> **Catalog Scale:** 826,402 Products  
> **Recall@10:** 0.66 (**+154%** vs BM25)  
> **Inference Latency:** 6.1 ms (Median CPU)  
> **Optimization:** INT8 Dynamic Quantization (4x size reduction)  
> **Indexing:** FAISS HNSW  

---

## Executive Summary

Content-based recommendation systems face a critical trade-off between semantic understanding and computational efficiency. Traditional sparse retrieval methods, such as **BM25**, rely on lexical matching and often fail to capture latent semantic relationships between user queries and items. Conversely, while dense retrieval models offer superior semantic representation, they are typically computationally expensive and difficult to deploy in real-time production environments. This paper addresses the challenge of bridging the gap between high-accuracy semantic search and the resource constraints of CPU-based serving at scale.

The core innovation is a **"Recommendation-as-Retrieval"** framework utilizing a two-tower bi-encoder architecture optimized for domain adaptation and production efficiency. The method frames user reviews as semantic proxies for queries and item metadata as documents, fine-tuning pre-trained language models on fashion-specific data using Supervised Contrastive Learning with Multiple Negatives Ranking Loss. To enable scalable serving, the system employs a specific optimization stack: **FAISS HNSW** indexing for Approximate Nearest Neighbor search, **ONNX Runtime** for inference, and **INT8 dynamic quantization**. This combination allows for Maximum Inner Product Search (MIPS) while significantly reducing memory footprint and latency.

The proposed system demonstrated substantial improvements over baselines in both accuracy and efficiency. On a "Hard review-to-title" benchmark, the dense retrieval model achieved a **Recall@10 of 0.66**, representing a 154% relative improvement over the BM25 baseline (0.26). Operational metrics confirm the system's production viability: median CPU inference latency was recorded at 6.1 ms, and model size was reduced by 4x through quantization. These results were validated on a large-scale catalog of **826,402 products**, proving the model's capability to handle real-world data volumes.

This research provides a comprehensive, reproducible blueprint for transitioning domain-adapted dense retrieval models from offline experimentation to online production. By demonstrating that high-performance semantic retrieval can be achieved on standard CPU hardware without requiring expensive GPU infrastructure, the work lowers the barrier to entry for advanced recommendation systems. The validation of this specific optimization stack (FAISS + ONNX + INT8) offers a practical path for e-commerce platforms to upgrade legacy sparse systems to dense retrieval architectures that better understand user intent.

---

## Key Findings

*   **Significant Accuracy Improvement:** The proposed dense retrieval system improved **Recall@10 to 0.66**, significantly surpassing the BM25 baseline of 0.26.
*   **Production-Grade Latency:** Achieved a low median CPU inference latency of **6.1 ms**, making it suitable for real-time applications.
*   **Hardware Efficiency:** Realized a **4x reduction in model size** through INT8 dynamic quantization.
*   **Scale Validation:** The approach was successfully validated on a large scale, retrieving items from a catalog of **826,402 products**.

---

## Methodology

The research frames content-based recommendation as a **"recommendation-as-retrieval"** task. The methodology relies on a robust training and serving pipeline:

*   **Architecture:** A two-tower bi-encoder architecture is used to independently encode queries and documents.
*   **Data Construction:** Training pairs are constructed using review text as a proxy for user queries and item metadata as documents.
*   **Training:** The model is fine-tuned on **50,000 interactions** from the *Amazon Reviews 2023 (Fashion)* subset.
*   **Loss Function:** The model is optimized using **Supervised Contrastive Learning** with Multiple Negatives Ranking Loss.
*   **Serving Pipeline:** The deployment utilizes **FAISS HNSW** indexing, **ONNX Runtime**, and **INT8 dynamic quantization**.

---

## Technical Details

The system design focuses on balancing representational power with computational constraints.

*   **Bi-Encoder (Two-Tower) Architecture:**
    *   Enables independent query and document encoding.
    *   Facilitates Maximum Inner Product Search (MIPS) via pre-computed document embeddings.
    *   Selected over Cross-Encoders to drastically reduce inference costs.

*   **Domain-Adaptive Fine-Tuning:**
    *   Leverages pre-trained language models.
    *   Utilizes user reviews as semantic proxies to adapt the model to the e-commerce domain.

*   **Optimization Stack:**
    *   **INT8 Dynamic Quantization:** Reduces model memory footprint.
    *   **ONNX Runtime:** Optimized for CPU inference.
    *   **FAISS HNSW:** Used for Approximate Nearest Neighbor (ANN) indexing to ensure fast retrieval.

*   **Task Definition:**
    *   Formulated as **Recommendation-as-Retrieval**, effectively mapping natural language queries to a dense embedding space.

---

## Results & Performance

The performance of the system was evaluated against a BM25 baseline on a "Hard review-to-title" benchmark.

*   **Retrieval Accuracy:**
    *   **Proposed Model:** 0.66 Recall@10
    *   **BM25 Baseline:** 0.26 Recall@10
    *   **Improvement:** 154% relative improvement.

*   **Efficiency Metrics:**
    *   **Latency:** 6.1 ms (Median CPU inference).
    *   **Model Size:** Reduced by 4x via quantization.

*   **Semantic Capability:** The system proved superior to sparse methods in capturing latent semantic relationships within a catalog of 826,402 items.

---

## Contributions

1.  **Production Blueprint:** Provided a comprehensive, reproducible blueprint for transitioning domain-adapted dense retrieval models from offline training to CPU-efficient production serving at catalog scale.
2.  **Domain Adaptation Strategy:** Demonstrated an effective domain adaptation strategy in e-commerce by fine-tuning on fashion review data.
3.  **Optimization Validation:** Validated a specific combination of optimization techniques (**FAISS HNSW + ONNX INT8**) to enable high-performance retrieval on standard CPU hardware.

---

## Assessment

**Quality Score:** 8/10  
**References:** 4 citations