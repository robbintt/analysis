---
title: Fine-tuning with RAG for Improving LLM Learning of New Skills
arxiv_id: '2510.01375'
source_url: https://arxiv.org/abs/2510.01375
generated_at: '2026-02-03T13:44:20'
quality_score: 9
citation_count: 38
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Fine-tuning with RAG for Improving LLM Learning of New Skills

*Humaid Ibrahim; Nikolai Rozanov; Marek Rei*

---

## Executive Summary

Large Language Models (LLMs) often struggle to acquire new skills and execute complex, multi-step reasoning tasks without external aid. While Retrieval-Augmented Generation (RAG) addresses this limitation by providing relevant context at inference time, it introduces significant computational overhead, increased latency, and higher token costs.

This research addresses the critical challenge of equipping LLM agents with the performance benefits of retrieval—specifically for interacting with complex environments like ALFWorld and WebShop—without incurring the runtime inefficiencies typically associated with external knowledge access.

The core innovation is **"RAG Distillation,"** a method that internalizes retrieval capabilities directly into model weights. The process begins by collecting failure trajectories from base agents, followed by **Hint Extraction**, where GPT-4o generates typed guidance based on these failures. These hints are then used to generate improved **Teacher Trajectories** via one-shot retrieval. Finally, **Student Training** occurs on these trajectories with the explicit hint strings removed, forcing the student model to internalize the guidance logic.

Formulated within a Partially Observable Markov Decision Process (POMDP) framework, the approach transitions a model from a Base Policy ($\pi_{\theta}$) to a Distilled Policy ($\pi_{\phi}$) that mimics the success of a RAG Policy ($\pi_{\theta}^{RAG}$) but relies solely on its internal parameters.

The proposed method demonstrates substantial efficacy:
*   **Performance:** Student models achieved a **91%** success rate on ALFWorld and **72%** on WebShop.
*   **Efficiency:** This performance was accompanied by a **10–60% reduction** in token usage compared to standard RAG approaches.
*   **Generalization:** Results were validated across different model scales (7B and 14B parameters) and agent architectures (ReAct and StateAct).

This work offers a targeted solution to common agent failures that avoids the latency and cost of live retrieval, suggesting RAG Distillation could become a standard practice for optimizing agent performance.

> ### ⚡ Quick Facts
> *   **ALFWorld Success Rate:** 91%
> *   **WebShop Success Rate:** 72%
> *   **Token Usage Reduction:** 10–60%
> *   **Model Scales Validated:** 7B, 14B
> *   **Architectures Tested:** ReAct, StateAct
> *   **Paper Quality Score:** 9/10

---

## Key Findings

*   **Significant Performance Improvement:** Student models achieved **91%** success on ALFWorld and **72%** on WebShop.
*   **Computational Efficiency:** The method reduces token usage by **10–60%** compared to standard approaches.
*   **Robust Generalizability:** Validated across model scales (7B, 14B) and architectures (ReAct, StateAct).
*   **Runtime Overhead Elimination:** Successfully internalizes retrieval capabilities, removing the need for external retrieval during inference.

---

## Methodology

The authors propose a comprehensive three-step distillation pipeline designed to transfer the benefits of retrieval into the model weights:

1.  **Hint Extraction**
    Compact hints are extracted from agent failure modes to identify where reasoning went wrong.
2.  **Teacher Trajectory Generation**
    Using the extracted hints, the system generates improved trajectories via one-shot retrieval.
3.  **Student Training (Internalization)**
    Student models are trained on these improved trajectories with the hint strings explicitly removed, forcing the model to internalize the guidance logic.

---

## Technical Details

### Core Concept: Internalized Retrieval
The paper proposes **Internalized Retrieval (One-Shot Retrieval Distillation)** to convert Retrieval-Augmented Generation (RAG) into internalized model weights. This retains the performance benefits of retrieval while eliminating runtime retrieval costs.

### The Pipeline
The detailed implementation consists of four steps:
1.  **Failure Collection:** Uses base agents to gather data on where the model fails.
2.  **Hint Extraction:** Utilizes **GPT-4o** to generate typed guidance based on the collected failures.
3.  **Teacher Trajectory Generation:** Employs RAG at $t=0$ to create successful trajectories using the hints.
4.  **Student Distillation:** Trains the student model on these trajectories with hint strings removed to ensure internalization.

### POMDP Framework Policies
The authors define the following policies within a Partially Observable Markov Decision Process (POMDP) framework:
*   **Base Policy ($\pi_{\theta}$):** The initial model without retrieval.
*   **RAG Policy ($\pi_{\theta}^{RAG}$):** The model augmented with retrieval at inference.
*   **SFT Policy ($\pi_{\phi}^{SFT}$):** Supervised Fine-Tuning policy.
*   **Distilled Policy ($\pi_{\phi}$):** The final student model that has internalized the retrieval logic.

### Architectures Evaluated
The approach is evaluated using two distinct agent architectures:
*   **ReAct:** Action sequence (thought $\to$ action).
*   **StateAct:** Action sequence (state $\to$ thought $\to$ action).

---

## Results & Evaluation

### Performance Metrics
Distilled student models achieved superior results on rigorous benchmarks:
*   **ALFWorld:** 91% success rate (text-based household tasks like Pick & Place).
*   **WebShop:** 72% success rate (1.18M product shopping tasks).

### Efficiency
*   **Token Reduction:** 10–60% reduction compared to standard RAG approaches.
*   **Validation:** Performed across 7B and 14B parameter models.

### Benchmark Comparisons
Comparative baselines cited in the paper include:
*   **StateAct:** 10–30% improvement over ReAct.
*   **FireAct:** 77% improvement on tool-use tasks.
*   **Reflexion:** 91% on HumanEval.

---

## Contributions

*   **RAG Distillation:** Introduces a novel method to integrate retrieval benefits into model weights without computational overhead.
*   **Targeted Failure Solution:** Provides a specific solution for common LLM agent failures in multi-step tasks by converting failure analysis into actionable training data.
*   **Cost vs. Accuracy:** Demonstrates a method to achieve higher accuracy than standard retrieval while simultaneously lowering inference costs.

---
**Paper Quality Score:** 9/10 | **References:** 38 citations