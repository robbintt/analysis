# Can VLMs Recall Factual Associations From Visual References?

*Dhananjay Ashok; Ashutosh Chaubey; Hirona J. Arai; Jonathan May; Jesse Thomason*

---

> ### ðŸ“Š Quick Facts & Metrics
> 
> *   **Performance Gap:** VLMs show an average **58.95% degradation** in recall when using visual vs. textual references.
> *   **Detection Accuracy:** Probes detect unreliable responses with **>92% accuracy**.
> *   **System Impact:** Selective prediction increases coverage by **+7.87%** and reduces error risk by **-0.9%**.
> *   **Models Tested:** InstructBLIP, LLaVA, GPT4V, and others.
> *   **Quality Score:** 9/10

---

## Executive Summary

This research addresses a critical failure mode in Vision-Language Models (VLMs): a systematic inability to reliably link internal semantic knowledge with visual representations. While VLMs demonstrate robust performance when retrieving facts based on textual references, they suffer a severe decline in factual recall when forced to rely solely on visual inputs. This "linkage deficit" indicates that VLMs struggle to ground their learned knowledge to visual entities, a deficiency that the study shows is not resolved by simply scaling model size or architecture.

To diagnose and mitigate this issue, the authors introduce a mechanistic interpretability approach that analyzes internal activation patterns to predict failure. The key technical innovation involves training probes on these internal states to classify whether a VLM can successfully link a visual input to the correct factual association. These probes are integrated into a selective prediction framework, enabling the system to detect and flag unreliable responses without the need for expensive model retraining or fine-tuning.

Experimental results reveal that VLMs suffer an average performance degradation of 58.95% in visual Question Answering (Q&A) compared to textual references, with accuracy dropping by more than half across seven tested models. Despite this significant gap, the internal state probes proved highly effective, detecting unreliable responses with over 92% accuracy. These findings challenge the assumption that scaling parameters alone resolves grounding issues, providing the first rigorous quantification of the visual-textual recall gap.

---

## Key Findings

*   **Visual vs. Textual Recall Gap:** VLMs demonstrate a systematic deficiency in multimodal grounding. Their ability to recall factual associations is **cut by half** when forced to rely on visual representations of an entity compared to textual references.
*   **Linkage Deficit:** The diminished performance suggests a fundamental struggle in VLMs to link their internal semantic knowledge of an entity with its visual representation.
*   **Detectable Failure Patterns:** Failures in fact retrieval due to visual references are correlated with distinct patterns in the model's internal states.
*   **High-Efficacy Probes:** Probes trained on these internal states can flag unreliable VLM responses with **over 92% accuracy** without the need for model retraining.
*   **Selective Prediction Utility:** When applied to Visual Question Answering (VQA) tasks for selective prediction, these probes increased absolute coverage by **7.87%** while reducing the absolute risk of error by **0.9%**.

---

## Methodology

The researchers utilized a controlled experimental setup to compare factual recall performance when VLMs are provided textual references versus visual references. The methodology involved three core phases:

1.  **Comparative Analysis:** Analyzing the internal states of the VLMs to identify distinct activation patterns associated with failures in linking visual inputs to factual knowledge.
2.  **Probe Development:** Developing and applying probes to these internal states to classify the reliability of model responses.
3.  **Validation & Testing:** The efficacy of the probes was tested by integrating them into a selective prediction framework on a VQA task to measure improvements in coverage and error reduction.

---

## Technical Details

**Benchmark Construction & QA Generation**
*   **Datasets:** CIFAR100, Food101, Landmarks, and MNIST.
*   **Generation Process:** Two-stage process using **Llama3.1-8B** for extraction from Wikipedia API and direct generation.
*   **Filtering:** Rigorous filtering applied including human annotation (kappa > 0.65).

**Experimental Design**
*   **Modality Isolation:** Design creates four datapoint variations:
    *   Entity Image
    *   Trivial Image
    *   Textual Reference Question
    *   Visual Reference Question
*   **Pre-evaluation Filtering:** Requires VLMs to accurately identify objects and answer correctly given both image and text.

**Models & Metrics**
*   **Architectures Evaluated:** InstructBLIP (Q-Former), LLaVA (linear projection), and GPT4V.
*   **Metrics:** Binary checks (string inclusion, exact match) and BLEU score.
*   **Mechanistic Analysis:** Probes trained on internal states.

---

## Results

*   **Significant Degradation:** VLMs showed an average **58.95% performance degradation** in visual Q&A compared to textual references.
*   **Scaling Insufficiency:** Accuracy dropped over 50% for all seven tested models, indicating that scaling is insufficient to resolve grounding issues.
*   **Probe Efficacy:** Probes trained on internal states detected unreliable responses with **>92% accuracy**.
*   **Task Performance:** In VQA tasks, selective prediction yielded a **+7.87% absolute increase** in coverage and a **-0.9% absolute reduction** in error risk.
*   **Data Quality:** Human annotation for benchmark quality achieved an inter-annotator agreement Kappa score **> 0.65**.

---

## Contributions

1.  **Identification of Deficiency:** The paper formally identifies and quantifies a critical, systematic gap in how VLMs ground visual references to internal factual knowledge.
2.  **Reliability Detection Mechanism:** It contributes a novel method for detecting when a VLM is likely to fail on multimodal inputs by analyzing internal state patterns, achieving high accuracy (>92%) without retraining.
3.  **Error Mitigation Strategy:** It provides a practical solution for improving VLM reliability through selective prediction, demonstrating measurable gains in coverage and safety (error reduction).
4.  **Future Roadmap:** The work offers informed recommendations to guide future research in resolving language grounding deficiencies.