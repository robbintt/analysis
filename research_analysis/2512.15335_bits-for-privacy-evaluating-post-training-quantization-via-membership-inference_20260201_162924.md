# Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference

*Chenxiang Zhang; Tongxi Qu; Zhong Li; Tian Zhang; Jun Pang; Sjouke Mauw*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 37 |
| **Core Privacy Metric** | TPR@0.1%FPR (True Positive Rate) |
| **Key Datasets** | CIFAR-10, CIFAR-100, TinyImageNet |
| **Optimal Privacy Precision** | 1.58-bit (Ternary) |
| **Evaluated Architectures** | ResNet18, ResNet50, DenseNet121 |

---

## Executive Summary

Deep learning models deployed in production are increasingly vulnerable to **Membership Inference Attacks (MIA)**, where adversaries determine whether specific data points were used during training, leading to significant privacy breaches. While **Post-Training Quantization (PTQ)** is a standard technique used to reduce model size and computational costs for edge deployment, its impact on privacy preservation has been largely overlooked.

This paper addresses a critical gap in the literature by systematically investigating whether reducing the numerical precision of a model (bit-width) can serve as a viable defense mechanism against privacy leakage. The key innovation of this work is the first systematic empirical analysis linking bit-width reduction in PTQ to resilience against membership inference. The study evaluates three prominent PTQ algorithmsâ€”**AdaRound**, **BRECQ**, and **OBC**â€”across multiple precision levels, ranging from 32-bit (full precision) down to 1.58-bit (ternary).

Technically, the authors employ **Likelihood Ratio Attacks (LiRA)**, utilizing 64 shadow models to rigorously assess adversarial risk. Furthermore, they introduce a fine-grained control strategy via ablation studies, specifically testing a decoupled approach where the final classification layer is quantized at a different precision than the remainder of the network to disentangle utility loss from privacy gains.

Experiments on CIFAR-10, CIFAR-100, and TinyImageNet reveal a **strong inverse correlation** between quantization depth and privacy leakage. Models quantized to 1.58-bit achieved up to a **99.2%â€“99.4% reduction** in vulnerability compared to full-precision baselines. Conversely, 4-bit quantization generally failed to improve privacy. These gains come with a utility trade-off, though strategies like maintaining higher precision in the last layer were shown to help mitigate accuracy loss.

This research significantly alters the understanding of PTQ, framing it not merely as an optimization for efficiency, but as a dual-purpose tool for privacy preservation.

---

## Key Findings

*   **Privacy Enhancement through Quantization:** The study demonstrates that low-precision Post-Training Quantization (PTQ) consistently reduces privacy leakage, showing that lowering bit-width can serve as a **defense mechanism** against membership inference.
*   **Significant Vulnerability Reduction:** Lower-precision models exhibit up to an **order of magnitude reduction** in vulnerability to membership inference attacks compared to their full-precision counterparts.
*   **Utility Trade-off:** The reduction in privacy risk via PTQ comes at the cost of decreased model utility, highlighting a distinct **privacy-utility trade-off**.
*   **Fine-grained Control via Last Layer Quantization:** Ablation studies at the 1.58-bit level reveal that quantizing only the last layer at a higher precision allows practitioners to exercise fine-grained control over the balance between privacy preservation and model utility.

---

## Methodology

The researchers utilized **Membership Inference Attacks (MIA)** as the primary evaluation framework to assess privacy risks. The study systematically analyzed three prominent PTQ algorithmsâ€”AdaRound, BRECQ, and OBCâ€”across multiple precision levels (4-bit, 2-bit, and 1.58-bit).

The experiments were conducted on standard computer vision datasets (**CIFAR-10**, **CIFAR-100**, and **TinyImageNet**) to compare the privacy-utility characteristics of quantized models against full-precision baselines. Additionally, the team performed ablation studies involving mixed-precision strategies, specifically altering the precision of the final model layer.

---

## Technical Details

The paper investigates Post-Training Quantization (PTQ) as a mechanism to enhance model privacy. The following technical specifications outline the evaluation setup:

| Component | Specification |
| :--- | :--- |
| **Quantization Techniques** | AdaRound, BRECQ, OBC |
| **Precision Levels** | 32-bit (Baseline), 4-bit, 2-bit, 1.58-bit (Ternary)<br>*(Includes decoupled strategy: last layer @ 8-bit)* |
| **Adversarial Evaluation** | Likelihood Ratio Attacks (LiRA)<br>*(Online and Offline variants)* |
| **Shadow Models** | 64 per experiment |
| **Primary Metric** | True Positive Rate at fixed False Positive Rate of 0.1% (TPR@0.1%FPR) |
| **Architectures** | ResNet18, ResNet50, DenseNet121 |
| **Statistical Rigor** | Results averaged over 10 seeds; variance analyzed over 100 runs |

---

## Results

*   **Strong Inverse Correlation:** There is a clear inverse relationship between quantization precision and privacy leakage. **1.58-bit quantization** achieved up to a **99.2%â€“99.4% reduction** in privacy leakage compared to full precision (e.g., TPR of 0.0031 on CIFAR-100).
*   **Limited Impact at Higher Precisions:** **4-bit quantization** generally failed to improve privacy compared to full-precision baselines.
*   **Utility Costs:** Privacy gains require a utility trade-off, evidenced by accuracy drops of **9.9% (OBC)** and **11.1% (AdaRound)** on TinyImageNet for 2-bit models.
*   **Algorithm Performance:** **BRECQ** demonstrated the best trade-off, achieving accuracy comparable to 2-bit models at 1.58-bit precision with lower TPR.
*   **Attack Variant Sensitivity:** Online LiRA attacks proved harder to mitigate than offline variants.
*   **Generalization:** Privacy improvements generalized across ResNet and DenseNet architectures, though BRECQ exhibited bimodal behavior and instability at 1.58-bit.

---

## Contributions

*   **First Systematic Privacy Study of PTQ:** The paper provides the first systematic analysis of the privacy-utility relationship inherent in Post-Training Quantization, addressing a gap in literature previously focused on full-precision models.
*   **Empirical Analysis of Bit-width Impact:** It establishes a clear empirical link between numerical precision (bit-width reduction) and susceptibility to membership inference attacks across multiple algorithms and datasets.
*   **Actionable Deployment Insights:** The research offers practical guidance for real-world deployment, suggesting that strategic quantization (such as preserving last-layer precision) can help practitioners simultaneously manage efficiency, utility, and privacy protection.