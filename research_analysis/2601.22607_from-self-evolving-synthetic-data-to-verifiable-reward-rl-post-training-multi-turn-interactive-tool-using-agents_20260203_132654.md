---
title: 'From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn
  Interactive Tool-Using Agents'
arxiv_id: '2601.22607'
source_url: https://arxiv.org/abs/2601.22607
generated_at: '2026-02-03T13:26:54'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents

*Jiaxuan Gao; Jiaao Chen; Chuyi He; Wei-Chen Wang; Shusheng Xu; Hanrui Wang; Di Jin; Yi Wu*

***

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Airline Benchmark** | 73.0% pass@1 |
| **Telecom Benchmark** | 98.3% pass@1 |
| **Synthetic vs. Human Data** | 56.0% vs 52.0% |
| **Simulator Quality Gap** | 20.0% performance difference |
| **Quality Score** | 8/10 |
| **Citations** | 40 |

***

> ### ðŸ“ Executive Summary
>
> Training Large Language Models (LLMs) to function as effective multi-turn, tool-using agents faces a dual bottleneck: the scarcity of high-quality training data and the instability of reinforcement learning (RL) signals. Creating complex, multi-step tool-use dialogues requires expensive human annotation that does not scale, while applying RL to interactive agents is challenging because "user simulators" generate noisy training signals that degrade model performance. This paper addresses the critical need for a scalable post-training framework that can bootstrap complex agent behaviors without relying on human-annotated data while mitigating the reward noise inherent in simulated environments.
>
> The authors introduce **EigenData**, a unified framework combining Self-Evolving Data Synthesis and Verifiable-Reward RL. The core innovation is a hierarchical multi-agent data synthesis engine that employs a closed-loop, self-evolving mechanism to iteratively refine its output. In this process, the system generates tool-grounded dialogues paired with executable checkers; validation agents then assess the quality, and their feedback is used to automatically update the generation prompts and workflows, progressively enhancing data reliability. For the RL component, the framework implements a comprehensive pipeline that begins by fine-tuning the user model to ensure high-fidelity simulation signals. It then applies a Group Relative Policy Optimization (GRPO)-style algorithm, blending Supervised Fine-Tuning (SFT) with trajectory-level group-relative advantages and dynamic filtering to stabilize training against simulation inconsistencies.
>
> The proposed framework achieved **state-of-the-art performance** on the tau^2-bench, with the best model scoring 73.0% pass@1 on the Airline domain and 98.3% pass@1 on the Telecom domain. In component ablation studies isolating the data generation process, the full synthetic pipeline (56.0%) outperformed models trained on human expert data (52.0%), while removing the self-evolving loop caused a significant drop to 44.0%, demonstrating the mechanism's necessity. Further ablations highlighted the critical role of simulator quality: RL training using a fine-tuned user model achieved 95.6% performance, whereas using a base user model resulted in a sharp decline to 75.6%, illustrating how low-quality simulators zero out rewards and hinder learning.
>
> This research signifies a major shift toward automated, scalable post-training for interactive agents, proving that synthetic data can surpass human-annotated data in complex tool-use scenarios. By demonstrating that high-quality multi-turn reasoning can be bootstrapped without human labeling, the EigenData framework offers a cost-effective pathway for developing advanced AI agents. Additionally, the verifier-based RL recipe provides a robust solution to the long-standing challenge of noisy rewards in interactive environments, establishing a new standard for training reliable tool-using systems capable of handling intricate, real-world workflows.

***

## Key Findings

*   **High Benchmark Performance:** The best model achieved state-of-the-art results on the tau^2-bench, scoring **73.0% pass@1** on Airline and **98.3% pass@1** on Telecom.
*   **Superiority over SFT:** The verifier-based RL recipe yielded consistent improvements over standard Supervised Fine-Tuning.
*   **Scalability without Human Annotation:** The framework demonstrates a viable pathway for bootstrapping complex tool-using behaviors without expensive human-annotated data.
*   **Improved Data Reliability:** The self-evolving data engine enhanced generation reliability through a closed-loop process that updates prompts and workflows.

## Methodology

The researchers introduced **EigenData**, a unified framework composed of two distinct components:

1.  **Self-Evolving Data Synthesis:**
    *   A hierarchical multi-agent engine.
    *   Generates tool-grounded dialogues paired with executable per-instance checkers.
    *   Utilizes a **closed-loop self-evolving process** to iteratively update prompts and workflows.
2.  **Verifiable-Reward RL:**
    *   Involves fine-tuning the user model followed by GRPO-style training.
    *   Utilizes trajectory-level group-relative advantages.
    *   Implements dynamic filtering to mitigate noisy signals caused by user simulation.

## Technical Details

*   **Self-Evolving Data Engine (EigenData):**
    *   Features a closed-loop automated synthesis pipeline.
    *   Incorporates diverse prompt sets and validation agents.
    *   Uses a self-evolving loop to improve data reliability and scalability.
*   **Verifiable-Reward RL Framework:**
    *   Combines Supervised Fine-Tuning (SFT) with Reinforcement Learning (RL).
    *   Employs a verifier-based reward system where the User Simulator's quality dictates reward signals.
*   **Application Domain:**
    *   Designed for multi-turn interactive tool use.
    *   Validated on domains like **Airline** and **Telecom**.
    *   Handles complex workflows including constraint checking and multi-step reasoning.

## Results

*   **Benchmark Performance:**
    *   **Airline:** 73.0% pass@1 (State-of-the-Art).
    *   **Telecom:** 98.3% pass@1 (State-of-the-Art).
*   **Data Ablation Studies:**
    *   Full synthetic pipeline: **56.0%** (Outperformed human expert data at 52.0%).
    *   Without validation agents: 50.0%.
    *   Without self-evolving loop: **44.0%**.
    *   Without diversity: 42.5%.
*   **User Model Ablation:**
    *   RL with fine-tuned user model: **95.6%**.
    *   RL with base user model: **75.6%**.
    *   *Insight:* A 20.0% performance gap indicates low-quality simulators hinder learning by zeroing out rewards.
*   **Algorithm Ablation:**
    *   Focused on batch size configurations and dynamic filtering.

## Contributions

*   **Unified Post-Training Framework:** Presents a comprehensive solution that addresses the dual challenges of scaling high-quality multi-turn tool-use data synthesis and dealing with noisy signals in RL for interactive agents.
*   **Innovation in Data Generation:** Development of the 'EigenData' system, which uniquely combines synthetic dialogue generation with executable checkers and a self-evolving mechanism to refine the generation process automatically.
*   **Advancement in RL for Tool Use:** Introduction of a verifier-based RL approach that uses trajectory-level group-relative advantages (GRPO-style) to effectively train interactive agents, solving the issue of degraded training efficiency due to user simulation noise.

***

**References:** 40 citations | **Quality Score:** 8/10