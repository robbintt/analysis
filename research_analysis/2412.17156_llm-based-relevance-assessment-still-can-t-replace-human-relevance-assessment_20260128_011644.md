---
title: LLM-based relevance assessment still can't replace human relevance assessment
arxiv_id: '2412.17156'
source_url: https://arxiv.org/abs/2412.17156
generated_at: '2026-01-28T01:16:44'
quality_score: 8
citation_count: 13
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-based relevance assessment still can't replace human relevance assessment

*Still Can, Replace Human, Laura Dietz, Relevance Assessment, New Hampshire, Deep Learning*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 13 Citations
> *   **Dataset:** TREC 2024 RAG track (MS MARCO Segment V2.1)
> *   **Scope:** 301 Queries, 75 Participating Systems
> *   **Core Metric:** Kendall‚Äôs œÑ correlation
> *   **Key Insight:** High overall correlation (0.89) masks severe ranking distortion at the top tiers.

---

## üìù Executive Summary

This research addresses the increasingly prevalent narrative that Large Language Models (LLMs) can fully replace human judgment in relevance assessment, a shift that poses significant risks to the integrity of Information Retrieval (IR) evaluation. The authors argue that while LLM-based evaluation offers scalability, it is theoretically and practically unsupported as a complete substitute for human assessors. The core problem lies in the susceptibility of automatic metrics to exploitation, where systems can "game" scores to achieve high rankings without genuinely improving quality. Furthermore, the paper highlights a "circularity problem": if universally adopted, LLM-based evaluation creates a feedback loop where systems are optimized to please LLM judges rather than human users, leading to distorted system rankings and a decoupling of proxy scores from actual utility.

The study introduces a multi-faceted methodological framework to challenge generalization claims about automated evaluation. Key innovations include the construction of an adversarial system designed explicitly to exploit LLM-based metrics and a simulation of evaluation circularity. Technically, the authors (Team WaterlooClarke) developed a specific adversarial run (`uwc1`) using GPT-4o with a complex, hierarchical ranking strategy‚Äîutilizing LLM preference judgments as a primary key and implementing a pooling strategy to maximize perceived relevance. The paper further posits that LLM-based relevance assessment is functionally equivalent to LLM-based re-ranking, using Kendall‚Äôs tau correlations to theoretically model how an ecosystem of LLM-based re-rankers would distort system rankings when LLMs evaluate LLM outputs.

Evaluation on the TREC 2024 RAG track (75 systems, 301 queries) revealed stark disparities between Fully Automatic (Umbrela LLM) and Fully Manual evaluation. While overall correlation appeared high (Kendall‚Äôs œÑ = 0.89), agreement deteriorated significantly at the top tiers: for the top 20 systems, œÑ dropped to 0.51, resulting in 24% of the systems being incorrectly ranked. Notably, the top-ranked system under Automatic evaluation placed only 5th under Manual assessment, while the true Manual top ranker placed 6th automatically. The adversarial `uwc1` run demonstrated extreme gaming, achieving 5th place in Automatic evaluation but plummeting to 28th place under Manual assessment. Circular evaluations further caused performance correlation to drop to œÑ = 0.63, whereas a standard LLM re-ranking approach (`uwc2`) successfully improved rankings across both metrics, validating the distinction between assessment and re-ranking.

This paper serves as a critical corrective to the field, providing empirical evidence that automated evaluation is not yet ready to serve as a universal ground-truth generator. By demonstrating that automatic metrics can be easily gamed and introducing the concept of "evaluation circularity," the authors expose the fragility of benchmark ecosystems that rely solely on LLM judges. The findings compel the research community to maintain human oversight and adopt hybrid evaluation methods. The study also establishes a theoretical framework regarding LLM "narcissism" and overfitting that must be addressed to prevent future model degradation, ensuring that the pursuit of efficient evaluation does not come at the cost of validity and accuracy.

---

## üîç Key Findings

*   **Unsubstituted Claims:** The assertion that LLM-based assessments can fully replace humans is theoretically and practically unsupported.
*   **Metric Exploitation:** Automatic evaluation metrics are vulnerable to gaming, allowing systems to achieve high scores without genuine quality improvements.
*   **Circularity Problem:** Universal adoption of LLM-based assessment creates a feedback loop (circularity) that distorts system rankings and decouples scores from actual quality.
*   **Theoretical Barriers:** Inherent limitations in LLMs, specifically "narcissism" and overfitting, currently prevent them from serving as valid substitutes for human judgment.
*   **Equivalence:** LLM-based relevance assessment is posited as functionally equivalent to LLM-based re-ranking.

---

## üõ† Methodology

The paper utilizes a comprehensive, multi-faceted approach to test the limits of LLM evaluation:

1.  **Critical Evidence Review:** A re-examination of previous research to rigorously test generalization claims regarding automated assessment.
2.  **Adversarial System Construction:** Building a system specifically designed to exploit and "game" automatic evaluation metrics.
3.  **Simulated Circularity Analysis:** Using Kendall‚Äôs tau correlations to model and measure the distortion in system rankings within an ecosystem dominated by LLM-based re-rankers.

---

## ‚úî Contributions

*   **Critical Re-evaluation:** Challenges the prevailing narrative that LLMs are ready to replace human relevance judgments.
*   **Empirical Evidence of Gaming:** Demonstrates that automatic metrics can be successfully exploited by adversarial systems.
*   **Concept of Circularity:** Introduces and quantifies "evaluation circularity," showing how universal LLM adoption causes scores to drift away from quality.
*   **Theoretical Framework:** Outlines key challenges (narcissism, overfitting) that must be solved before LLMs can act as valid ground-truth generators.

---

## ‚öôÔ∏è Technical Details

| Category | Specification |
| :--- | :--- |
| **Dataset** | TREC 2024 RAG track (MS MARCO Segment V2.1) |
| **Scale** | 301 Queries, 75 Participating Systems |
| **Evaluation Modes** | Fully Automatic (Umbrela LLM), Fully Manual, Hybrid |
| **Adversarial Team** | Team WaterlooClarke |
| **Adversarial Run ID** | `uwc1` |
| **Model Used** | GPT-4o |
| **Prompt Strategy** | Specific structures for pairwise comparison judgments |
| **Pooling Strategy** | Top 20 documents from 15 preliminary runs |
| **Hierarchical Ranking Keys** | 1. LLM-based preference judgments<br>2. Relevance assessments<br>3. Reciprocal Rank Fusion |

---

## üìà Results

### Correlation Analysis (Manual vs. Automatic)
Using Kendall‚Äôs œÑ, the correlation between human and LLM judgment varied significantly based on the ranking tier:

*   **Overall (75 Systems):** œÑ = 0.89
*   **Top 60 Systems:** œÑ = 0.84-0.85 (8% system swaps)
*   **Top 20 Systems:** œÑ = 0.51 (**24% swaps**)
*   **Top 15 Systems:** œÑ = 0.56 (21% swaps)

### Ranking Discrepancies
*   **Top Automatic System:** Ranked 1st (Auto) vs. 5th (Manual)
*   **Top Manual System:** Ranked 1st (Manual) vs. 6th (Auto)

### Adversarial vs. Standard Runs
*   **Run `uwc1` (Adversarial):** Demonstrated extreme "gaming," achieving **5th place** in Automatic evaluation but dropping to **28th place** in Manual evaluation.
*   **Run `uwc2` (Standard LLM Re-ranking):** Showed legitimate improvement, moving from 9th to 4th place (Manual) and 7th to 3rd place (Automatic).

### Circularity Impact
When LLMs evaluated LLM outputs (simulated circularity), the performance correlation for the top 60 systems dropped to **œÑ = 0.63**.

---

**References:** 13 citations | **Quality Score:** 8/10