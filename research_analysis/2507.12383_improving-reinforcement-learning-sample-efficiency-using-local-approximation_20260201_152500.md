# Improving Reinforcement Learning Sample-Efficiency using Local Approximation

*Authors: Mohit Prashant; Arvind Easwaran*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 7/10
> *   **Total Citations:** 27
> *   **Sample Complexity:** $O(SA \log A)$
> *   **Method:** Model-Free PAC-MDP
> *   **Core Innovation:** Local Approximation

---

## üìù Executive Summary

Reinforcement learning (RL) in infinite-horizon Markov Decision Processes (MDPs) faces a fundamental theoretical bottleneck regarding sample efficiency. Current algorithms are constrained by suboptimal Probably Approximately Correct (PAC) bounds that exhibit a logarithmic dependency on the size of the state space ($\log S$). This dependency imposes computational and sample costs that become prohibitive as the state space scales, limiting the practical deployment of model-free RL in large-scale or sparse-reward environments.

This research introduces **Probabilistic Delayed Q-Learning (PDQL)**, a novel model-free PAC-MDP algorithm designed to overcome these limitations through local approximation. The core mechanism relies on the **"Locality of Value Learning,"** a principle defining an inverse relationship between transition distance and value relevance: learning effort for distant states is independent, while proximate states exhibit high dependency.

By operating in discrete metric environments where transitions are restricted to states within a one-unit distance ($|S'_s| \le A$), PDQL approximates large MDPs by decomposing them into smaller, localized MDPs constructed from subsets of the state-space. The study derives a new asymptotic sample-complexity bound:

$$
O\left( \frac{SA}{\epsilon^3(1-\gamma)^3} \log\left(\frac{A}{\delta(1-\gamma)}\right) \log\frac{1}{\epsilon} \log\frac{1}{\delta} \right)
$$

This result sharpens existing analysis by eliminating the extraneous $\log S$ factor, shifting the dependency to $\log A$ and confidence parameters. Empirical benchmarks against baselines confirm that PDQL generalizes $\epsilon$-optimal solutions faster, demonstrating improved convergence rates that align with the theoretical removal of the $\log S$ penalty.

---

## üîç Key Findings

*   **Sharper PAC Bounds:** The study derives new PAC bounds on asymptotic sample-complexity for infinite-horizon MDPs that are sharper than existing literature.
*   **Reduced Sample-Complexity:** The algorithm reduces sample-complexity by a logarithmic factor, achieving **$O(SA \log A)$** timesteps.
*   **Locality of Value Learning:** An inverse relationship exists between transition distance and value relevance; distant states require independent effort, while proximate states show high dependency.
*   **Model-Free Efficacy:** Improvements in sample-efficiency extend to infinite-horizon, model-free settings.
*   **Experimental Validation:** Empirical comparisons confirm significant performance improvements over prior work in practical scenarios.

---

## ‚öôÔ∏è Technical Details

| Component | Description |
| :--- | :--- |
| **Algorithm Name** | Probabilistic Delayed Q-Learning (PDQL) |
| **Type** | Model-Free PAC-MDP Algorithm |
| **Primary Mechanism** | **Local Approximation:** Decomposes a large MDP into smaller MDPs using subsets of the state-space. |
| **Key Dependency** | Proximate states are treated as dependent; distant states are treated as effectively independent. |
| **Environment** | Discrete metric environments with a specific distance metric. |
| **Constraint** | Locality constraints restrict transitions to states within a one-unit distance ($|S'_s| \le A$). |
| **Execution Style** | Episodic execution of finite length. |

#### Theoretical Contribution
The study establishes a new asymptotic sample-complexity bound that improves upon State-of-the-Art by removing the logarithmic dependency on the state-space size ($S$). The new bound depends instead on $\log A$ and confidence parameters ($\delta$).

---

## üß™ Methodology

The core technical strategy utilizes **local approximation** to approximate the original MDP with smaller MDPs constructed from subsets of the state-space. This approach exploits the independence of learning effort for distant states and the dependency of values and sample requirements for vicinal states.

The authors constructed a specific PAC-MDP algorithm designed to function in a **model-free environment** while maintaining the derived sample-complexity bounds. This allows for practical application without requiring a model of the environment dynamics.

---

## üìà Results

PDQL was empirically compared against strong baselines, including:
*   Delayed Q-Learning
*   Variance Reduced Q-Learning (VRQL)
*   Q-Learning with UCB Exploration
*   Speedy Q-Learning

**Outcome Analysis:**
*   **Faster Generalization:** Experimental findings on benchmark environments demonstrated that PDQL generalizes $\epsilon$-optimal solutions significantly faster than prior work.
*   **Convergence Rates:** The study reported significant improvements in convergence rates in practical scenarios.
*   **Validation of Bounds:** Experiments validated the theoretical reduction in sample-complexity, specifically the elimination of the $\log S$ factor. This translates to increased learning rates in larger state-action spaces or sparse reward settings.

---

## üèÅ Contributions

*   **Theoretical Advancement:** Provided a tighter theoretical analysis of sample-complexity in RL, improving upon previous PAC bounds.
*   **New Algorithm:** Developed a novel PAC-MDP algorithm for model-free settings with sample-efficiency of $O(SA \log A)$.
*   **Framework for State-Dependency:** Introduced a conceptual framework defining how transition-wise distance affects value dependency and sample effort.
*   **Benchmarking:** Provided experimental evidence bridging the gap between theoretical bounds and practical performance.