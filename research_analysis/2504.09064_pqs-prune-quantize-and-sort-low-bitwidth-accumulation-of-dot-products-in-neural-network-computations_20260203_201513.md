---
title: 'PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products
  in Neural Network Computations'
arxiv_id: '2504.09064'
source_url: https://arxiv.org/abs/2504.09064
generated_at: '2026-02-03T20:15:13'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations

*Vikas Natesh; H. T. Kung*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Bitwidth Reduction** | 2.5x lower than conventional 32-bit approaches |
| **Overflow Resolution** | 99.8% of transient overflows resolved (MobileNetV2) |
| **Accuracy** | On par with FP32 baselines |
| **Key Innovation** | Sorted dot-product algorithm |
| **Quality Score** | 8/10 |
| **Citations** | 40 references |

---

## üìë Executive Summary

> **Problem:** Neural network inference typically necessitates high-bitwidth accumulators (often 32-bit) to prevent overflow during the summation of dot products, even when inputs and weights are quantized to lower precision. This reliance on wide accumulators creates a significant hardware bottleneck, increasing memory bandwidth usage and energy consumption. Addressing this inefficiency is critical for deploying efficient models on resource-constrained edge devices, as it limits the performance and power gains that quantization otherwise promises.

> **Innovation:** The authors introduce **PQS (Prune, Quantize, and Sort)**, a framework that synergistically combines three techniques to resolve bitwidth accumulation bottlenecks. The method employs **N:M structured pruning** to induce sparsity, followed by Quantization-Aware Training (QAT) to convert the model to 8-bit integers. The core innovation is the **"Sort" phase**, a sorted dot-product algorithm that mitigates transient overflows by splitting products into positive and negative arrays, sorting them by magnitude, and summing pairwise to cancel large values early. Unlike previous methods such as A2Q, PQS manages transient overflows without constraining the L1-norm of weights, theoretically requiring only one sorting round due to weight symmetry.

> **Results:** Empirical evaluations demonstrate that PQS achieves a **2.5x reduction in accumulator bitwidth** compared to conventional 32-bit approaches while maintaining accuracy on par with floating-point (FP32) baselines. In tests on MobileNetV2, a single sorting round resolved 99.8% of transient overflows, effectively eliminating the hardware errors associated with low-bitwidth accumulation. The N:M structured pruning component further reduces indexing overhead, contributing to overall lower memory bandwidth and improved energy efficiency compared to standard quantization methods.

> **Impact:** This research challenges the conventional necessity of wide accumulators in neural network hardware, demonstrating that accumulation order optimization can effectively replace bitwidth expansion. By enabling accurate inference with significantly reduced accumulator bitwidth and 8-bit quantization, PQS facilitates the design of more energy-efficient hardware that demands less memory bandwidth. This work advances the field of efficient AI by proving that highly compressed, low-bitwidth models can achieve floating-point accuracy, thereby accelerating the feasibility of deploying robust neural networks on edge and resource-constrained platforms.

---

## üîë Key Findings

*   **Significant Bitwidth Reduction:** The PQS method achieves a **2.5x reduction** in accumulator bitwidth compared to conventional quantized approaches.
*   **Overflow Elimination:** It completely eliminates accumulation overflows at inference time, removing the need for wide accumulators.
*   **Accuracy Retention:** Models processed with PQS achieve accuracy **on par with floating-point (FP32) baselines**.
*   **Hardware Efficiency:** The method results in decreased memory bandwidth usage and improved energy efficiency.

---

## üõ†Ô∏è Methodology

The PQS methodology relies on a synergistic combination of three distinct techniques executed in sequence:

1.  **Prune:** Iterative **N:M pruning** is applied to the floating-point model to create sparsity. This involves setting the smallest N weights in every block of M to zero.
2.  **Quantize:** The pruned model is quantized to **8 bits or fewer** using Quantization-Aware Training (QAT) with uniform per-tensor signed integers.
3.  **Sort:** Partial products are accumulated in a **sorted order** (from small to large) to prevent overflow within a constrained bitwidth.

---

## ‚öôÔ∏è Technical Details

The PQS framework integrates specific algorithmic strategies to optimize neural network computations:

*   **N:M Structured Pruning**
    *   Sets the smallest $N$ weights in every block of $M$ to zero.
    *   Reduces indexing overhead compared to unstructured pruning.

*   **Quantization-Aware Training (QAT)**
    *   Utilizes uniform per-tensor $b$-bit signed integers.
    *   Ensures model accuracy remains high post-quantization.

*   **Sorted Dot Product Algorithm**
    *   **Mechanism:** Splits products into positive and negative arrays, sorts them, and sums pairwise to cancel magnitude.
    *   **Efficiency:** Theoretically requires only one round due to weight symmetry.
    *   **Overflow Theory:** Overflow occurs when the dot product length $K \ge 2^{p-2b}$.

*   **Comparison to A2Q**
    *   Unlike A2Q, PQS handles transient overflows **without constraining the L1-norm** of weights.

---

## üìà Results

The empirical evaluation of PQS highlights substantial performance improvements:

*   **Bitwidth & Accuracy:** Achieved a 2.5x reduction in accumulator bitwidth compared to 32-bit approaches while maintaining FP32-level accuracy.
*   **Transient Overflow Handling:**
    *   In **MobileNetV2**, a single sorting round resolved **99.8%** of transient overflows.
    *   Addressing transient overflows improved accuracy from roughly 10% (standard clipping) to approximately 40%.
*   **System Efficiency:** The approach reduces memory bandwidth, improves energy efficiency, and avoids indexing overhead through N:M pruning.

---

## üß© Contributions

*   **Algorithmic Innovation:** Introduction of the PQS algorithm integrating N:M pruning, quantization, and sorting to solve bitwidth accumulation bottlenecks.
*   **Hardware Optimization:** Demonstrated that wide accumulators are not strictly necessary if accumulation order is optimized, addressing significant hardware inefficiencies.
*   **Empirical Validation:** Provided evidence that accurate, compressed neural network inference is possible with low-bitwidth accumulation.