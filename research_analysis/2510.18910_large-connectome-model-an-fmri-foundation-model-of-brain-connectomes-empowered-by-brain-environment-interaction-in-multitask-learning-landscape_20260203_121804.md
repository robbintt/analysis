---
title: 'Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered
  by Brain-Environment Interaction in Multitask Learning Landscape'
arxiv_id: '2510.1891'
source_url: https://arxiv.org/abs/2510.18910
generated_at: '2026-02-03T12:18:04'
quality_score: 9
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape

*Ziquan Wei; Tingting Dan; Guorong Wu*

---

> ### ðŸ“Š Quick Facts
>
> * **Parameter Range:** 147M â€“ 1.2B
> * **Training Data:** >10,000 scans from 8 major datasets
> * **Top Diagnostic Accuracy:** 86.30% (Alzheimerâ€™s)
> * **Architecture:** Decoder-Only Transformer
> * **Key Innovation:** Brain-Environment Interaction (BEI) Framework
> * **Quality Score:** 9/10

---

## Executive Summary

This research addresses the critical bottleneck of limited sample sizes in clinical neuroimaging AI, which restricts the performance of traditional deep learning models in diagnostic tasks. A further complication is that standard self-supervised learning (SSL) methods often suffer from misalignment between the learned latent representations of brain function and specific real-world clinical outcomes. This lack of alignment reduces the efficacy of foundation models when applied to downstream tasks such as disease diagnosis or behavior recognition, creating a barrier to deploying these tools in practical medical settings where data is scarce and precise diagnostic accuracy is paramount.

The paper introduces the **Large Connectome Model (LCM)**, a novel foundation model built on a "Brain-Environment Interaction" (BEI) framework that explicitly integrates functional connectivity (FC) matrices with rich non-imaging data (demographics and environmental variables). Technically, LCM utilizes a Decoder-Only Transformer architecture that tokenizes BEIs, employing self-attention on tokens and cross-attention between tokens and the FC matrix to capture complex relationships. The training strategy is distinct, consisting of a **supervised multitask pretraining phase on non-imaging variables** to learn comprehensive brain representations relative to external factors, followed by a semi-supervised fine-tuning phase. This second phase leverages pseudo-labels derived from pretrained BEI representations, enabling effective model adaptation to specific downstream tasks even with limited labeled data.

The LCM architecture scales from 147M to 1.2B parameters, demonstrating exceptional parameter efficiency; the 147M variant achieved performance comparable to the 650M-parameter BrainLM while utilizing significantly fewer GPU hours. Larger variants (735M and 1.175B) consistently outperformed state-of-the-art models such as BrainMass and BrainJEPA across multiple benchmarks. In rigorous clinical evaluations validated on over 10,000 scans from eight major datasets, the model achieved high diagnostic metrics for Alzheimerâ€™s disease (86.30% Accuracy, 85.33% F1), Parkinsonâ€™s disease (81.30% Accuracy, 84.18% F1), and Autism (71.46% Accuracy, 72.50% F1). Additionally, the evaluation scope explicitly included performance assessments for **Schizophrenia** and **Sex prediction**, covering a wide spectrum of behavior recognition, disease diagnosis, and demographic tasks.

By formulating brain modeling as a multitask learning problem, this work establishes a versatile tool that significantly enhances the applicability of foundation models to diverse neurological and psychiatric conditions. The successful integration of environmental and demographic variables resolves alignment issues inherent in previous SSL approaches, paving the way for neuroimaging AI to transition from theoretical research to clinical integration. The model's ability to maintain high performance with limited labeled data suggests it can effectively address the performance bottlenecks currently faced in medical AI, offering a robust scalable solution for early diagnosis and personalized medicine in routine clinical practice.

---

## Key Findings

*   **Superior Clinical Performance:** The proposed 'Large Connectome Model' demonstrated promising results in diverse clinical applications, including sex prediction, human behavior recognition, and the early diagnosis of Autism, Parkinsonâ€™s disease, Alzheimerâ€™s disease, and Schizophrenia.
*   **Addressing Alignment Issues:** Incorporating environmental and demographic variables successfully mitigates the misalignment often found in standard self-supervised learning with brain-to-outcome relationships.
*   **Efficacy of Multitask Learning:** Formulating brain modeling as a multitask learning problem significantly enhances the model's applicability to downstream tasks compared to traditional foundation models.
*   **Potential for Clinical Integration:** The model shows potential to facilitate neuroimaging applications within clinical routines, addressing performance bottlenecks caused by limited sample sizes in medical AI.

---

## Methodology

The researchers developed a scalable model architecture based on a multitask learning framework, utilizing a combination of functional neuroimages, environmental variables, and demographic data. The methodology consists of two distinct phases:

1.  **Multitask Pretraining**
    *   The model tokenizes multiple Brain-Environment Interactions (BEI).
    *   It learns comprehensive representations of brain function relative to external factors.

2.  **Semi-supervised Finetuning**
    *   The model undergoes finetuning by assigning pseudo-labels derived from the pretrained BEI representations.
    *   This allows for effective adaptation to specific downstream tasks with limited labeled data.

---

## Technical Details

*   **Input Data**
    *   Utilizes Functional Connectivity (FC) matrices calculated via Pearson correlation of BOLD signals (based on the AAL atlas).
    *   Integrates non-imaging records known as Brain-Environment Interactions (BEI).

*   **Architecture**
    *   **Type:** Decoder-Only Transformer.
    *   **Embeddings:** Utilizes learnable BEI token embeddings.
    *   **Attention Mechanisms:**
        *   Self-Attention on tokens.
        *   Cross-Attention between tokens and the FC matrix.

*   **Training Strategy**
    *   **Supervised Multitask Learning:** Uses Cross-Entropy and MSE loss.
    *   **Optimization:** Two-stage strategy involving Momentum and Adaptive Training.
    *   **Semi-supervised Learning:** Fine-tuning facilitated by pseudo-labeling.

---

## Results

*   **Scalability:** The model scales from 147M to 1.2B parameters.
*   **Efficiency:**
    *   LCM-147M achieved efficiency comparable to BrainLM-650M with significantly fewer parameters and GPU hours.
*   **Performance:**
    *   Larger variants (735M, 1175M) outperformed models like BrainLM, BrainMass, and BrainJEPA.
*   **Ablation Studies:** Confirmed the two-stage training strategy as optimal.
*   **Specific Diagnostic Metrics:**
    *   **Alzheimer's:** 86.30% Accuracy, 85.33% F1
    *   **Parkinson's:** 81.30% Accuracy, 84.18% F1
    *   **Autism:** 71.46% Accuracy, 72.50% F1
*   **Dataset:** Trained on over 10,000 scans from 8 major datasets.

---

## Contributions

*   **Novel Foundation Model:** Introduction of a new fMRI foundation model (Large Connectome Model) specifically designed to overcome the barrier of limited sample sizes in clinical neuroimaging AI.
*   **Brain-Environment Interaction (BEI) Framework:** A pioneering approach to brain modeling that integrates rich environmental and demographic data into the pretraining process, moving beyond conventional self-supervised learning.
*   **Tokenization and Pseudo-labeling Strategy:** Development of a scalable architecture that employs tokenization for multitask pretraining and a unique semi-supervised finetuning mechanism using BEI pseudo-labels.
*   **Validation Across Pathologies:** Comprehensive evaluation of the modelâ€™s robustness across a wide spectrum of neurological and psychiatric conditions, establishing a versatile tool for potential clinical use.

---

**References:** 19 citations