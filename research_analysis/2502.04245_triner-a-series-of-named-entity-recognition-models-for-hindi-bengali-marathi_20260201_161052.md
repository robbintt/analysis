# TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi
*Mohammed Amaan Dhamaskar; Rasika Ransing*

---

### üìä Quick Facts
| Metric | Detail |
| :--- | :--- |
| **Languages** | Hindi, Bengali, Marathi |
| **Architecture** | Transformer-based (Unified) |
| **F1 Score** | **92.11** (Aggregate) |
| **Entity Groups** | 6 (Person, Location, Org, Numex, Timex, Misc) |
| **Baseline Improvement** | ~10% increase over IndicNER |

---

> ## üìë Executive Summary
>
> This research addresses the critical scarcity of robust, multilingual Named Entity Recognition (NER) tools for three major Indian languages: Hindi, Bengali, and Marathi. Existing solutions in this domain are often fragmented, relying on disjointed, language-specific isolated models that suffer from significant inconsistencies in entity group definitions and tag nomenclature. This lack of standardization hinders the development of scalable, unified Indic NLP pipelines. The study aims to solve this by moving away from older statistical methods‚Äîsuch as Conditional Random Fields (CRFs) and LSTMs‚Äîtoward a modern, unified architecture that ensures semantic consistency across these under-represented languages.
>
> The core innovation of TriNER is the deployment of a single, unified Transformer-based multilingual model designed to process Hindi, Bengali, and Marathi simultaneously, rather than treating them as separate silos. Technically, the authors employed a dual-strategy approach involving the training of a custom transformer model from scratch alongside the fine-tuning of existing pre-trained models. The methodology aggregated data from HiNER, MahaNER, and B-NER datasets and standardized them into a cohesive taxonomy of six entity groups: PERSON, LOCATION, ORGANIZATION, NUMEX, TIMEX, and MISC. To ensure data uniformity, the team developed a Python script for automated NUMEX annotation and utilized a specific dataset splitting strategy that combined full training sets with reserved validation and test segments to optimize generalization.
>
> The proposed TriNER model demonstrated superior performance, achieving an aggregate F1 score of 92.11 across the three evaluated languages. This score represents a substantial improvement of approximately 10 points over the IndicNER/Naamapadam baseline, which recorded F1 scores in the 81‚Äì83 range. Furthermore, the unified model proved competitive with dedicated state-of-the-art single-language models, matching or exceeding their performance; specific comparison models included HiNER (F1 88.78‚Äì92.22), MahaNER (F1 85.3‚Äì86.8), and B-NER (F1 86). These results validate the hypothesis that a unified multilingual approach can maintain high accuracy while solving standardization issues.
>
> The significance of this study lies in its provision of a reliable, standardized framework for entity recognition in Indian languages, directly addressing the resource gap in this domain. By reducing inconsistencies in labeling and taxonomy, TriNER facilitates more effective context-aware applications, such as data anonymization and cross-lingual information extraction. The successful unification of these languages into a single high-performance model sets a precedent for future multilingual NER research, offering a scalable solution that enhances the accessibility and capability of NLP technologies for the Hindi, Bengali, and Marathi speaking populations.

---

## üîç Key Findings

*   **High-Performance Model:** Successfully developed a multilingual NER model achieving an aggregate **F1 Score of 92.11** across Hindi, Bengali, and Marathi.
*   **Unified Classification:** The system classifies tokens into **6 distinct entity groups**, streamlining the recognition process.
*   **Consistency Enhancement:** Implementing a single unified model significantly reduces inconsistencies in entity groups and tag names compared to disjointed, language-specific approaches.

## üî¨ Methodology

The researchers employed a **dual-strategy approach** to maximize model efficacy:

1.  **Custom Training:** Training a custom transformer model from scratch.
2.  **Fine-Tuning:** Fine-tuning several existing pre-trained models.

The study focused on the three most widely spoken languages in India to create a **unified solution** rather than relying on language-specific isolated models.

## üåü Contributions

*   **Unified Architecture:** Introduction of a single, robust model to handle NER for three major Indian languages, addressing the scarcity of multilingual tools in this domain.
*   **Standardization:** A significant reduction in inconsistencies regarding entity groups and tag nomenclature across Hindi, Bengali, and Marathi, ensuring consistency in data labeling.
*   **Context-Aware Capabilities:** Enhancement of capabilities for context-aware tasks such as anonymization by providing a reliable entity recognition mechanism for under-represented languages.

## ‚öôÔ∏è Technical Details

*   **Architecture:** Transitioned from older statistical methods (CRF, LSTM) to a **Transformer-based architecture** to deploy a single unified multilingual model.
*   **Data Sources:** Data aggregated from:
    *   **HiNER** (Hindi)
    *   **MahaNER** (Marathi)
    *   **B-NER** (Bengali)
*   **Taxonomy Mapping:** Original tags were mapped to a unified **6-entity taxonomy**:
    *   PERSON
    *   LOCATION
    *   ORGANIZATION
    *   NUMEX
    *   TIMEX
    *   MISC
*   **Pre-processing:** A Python script was developed specifically for **automated NUMEX annotation** in the B-NER dataset.
*   **Dataset Splitting Strategy:**
    *   **Training:** Combined full training sets from HiNER and MahaNER + 80% of the B-NER training set.
    *   **Validation:** Remaining splits + original test sets.

## üìà Results

*   **Overall Performance:** The TriNER model achieved an **F1 score of 92.11**.
*   **Baseline Comparison:** This represents a significant performance increase of approximately **10 points** over the IndicNER/Naamapadam baseline (F1 range 81‚Äì83).
*   **Comparison with Single-Language Models:** The results are comparable to dedicated, state-of-the-art models:
    *   **HiNER:** F1 88.78‚Äì92.22
    *   **MahaNER:** F1 85.3‚Äì86.8
    *   **B-NER:** F1 86

---

**Quality Score:** 8/10  
**References:** 31 citations