# Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration

*Han-Dong Lim; Donghwan Lee*

---

> **### Quick Facts**
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **References** | 40 Citations |
> | **Research Type** | Theoretical / Mathematical Analysis |
> | **Core Focus** | Linear Function Approximation (LFA) Stability |

---

## Executive Summary

This paper addresses the fundamental instability issues inherent in Reinforcement Learning (RL) algorithms that rely on Linear Function Approximation (LFA). Specifically, it tackles the theoretical challenges surrounding the divergence of linear Q-learning and Approximate Value Iteration (AVI) in off-policy settings. The authors focus on the lack of rigorous theoretical formalization regarding when solutions to the Projected Bellman Equation (PBE) exist and under what conditions iterative algorithms remain stable. Resolving these theoretical gaps is critical for developing reliable RL systems capable of operating in high-dimensional spaces where function approximation is necessary.

The key innovation lies in the identification of two distinct, independent sufficient conditions for guaranteeing solutions to the PBE: the **Strictly Negatively Row Dominating Diagonal (SNRDD)** assumption and a condition derived directly from the convergence of Approximate Value Iteration. The authors introduce SNRDD to analyze the spectral properties of the parameterized matrix $M_\theta$ and explicitly establish that AVI convergence serves as a standalone sufficient condition for PBE solvability. Technically, the framework analyzes the operator matrix $T$ to relate the fixed-point properties of the PBE to the dynamic convergence of gradient-based algorithms, avoiding the confusion between projection and regularization found in prior works.

The study presents rigorous theoretical guarantees rather than empirical performance metrics. The results establish that solutions to the PBE exist if behavior and target policies are continuous and the parameterized SNRDD condition holds, while uniqueness is guaranteed if the behavior policy is fixed and the target policy is Lipschitz continuous under the same SNRDD condition. The authors prove that the SNRDD property serves as a sufficient condition for the stability and convergence of linear Q-learning using Robbins-Monro step-sizes. Additionally, the paper provides specific theoretical observations regarding the characteristics of PBE solutions when employing epsilon-greedy policies, extending theoretical guarantees to broader off-policy scenarios.

This paper significantly advances the theoretical understanding of value function approximation by elucidating the relationship between the SNRDD assumption and the convergence behavior of AVI. By identifying AVI convergence as a distinct, independent sufficient condition—rather than merely a component subsumed by SNRDD—the work provides researchers with expanded tools for analyzing algorithm behavior. The study successfully bridges the gap between the fixed-point solutions of the PBE and the stability of learning algorithms, offering a robust theoretical foundation for future developments in stable approximate dynamic programming and off-policy learning.

---

## Key Findings

*   **Dual Sufficient Conditions:** Identifies two sufficient conditions guaranteeing solutions to the Projected Bellman Equation (PBE):
    1.  The Strictly Negatively Row Dominating Diagonal (SNRDD) assumption.
    2.  A condition derived from Approximate Value Iteration (AVI) convergence.
*   **Convergence Guarantee:** Establishes the SNRDD assumption as a key property that ensures the convergence of linear Q-learning algorithms.
*   **Theoretical Relationship:** Clarifies the theoretical relationship between the SNRDD assumption and the convergence behavior of Approximate Value Iteration (AVI).
*   **Policy Impact:** Provides specific theoretical observations regarding the characteristics of PBE solutions when an epsilon-greedy policy is employed.

---

## Methodology

The research employs a theoretical and analytical framework rather than empirical experimentation. The authors derived mathematical assumptions, specifically focusing on the Strictly Negatively Row Dominating Diagonal (**SNRDD**), to analyze the solvability of the Projected Bellman Equation and the stability of linear Q-learning and AVI algorithms.

---

## Technical Details

The paper focuses on the theoretical stability and solution existence of Reinforcement Learning (RL) algorithms using Linear Function Approximation (LFA).

*   **Projected Bellman Equation (PBE):** Formulated as a fixed-point equation involving an operator matrix $T$, defined by feature matrices, sampling distributions, and policy matrices. A regularization term is added to ensure solution existence.
*   **Core Analysis Tool:** The Strictly Negatively Row Dominating Diagonal (**SNRDD**) condition applied to the parameterized matrix $M_\theta$.
*   **Algorithm Analyzed:** Regularized Linear Q-learning.
    *   **Approximation:** $Q(s, a) \approx \phi(s, a)^T \theta$
    *   **Update Rule:** Defined including regularization.
    *   **Sampling:** i.i.d. sampling from a fixed distribution.
    *   **Step-sizes:** Robbins-Monro conditions.
*   **Scope of Analysis:**
    *   Covers both on-policy and off-policy settings.
    *   Assumes policies are either Continuous or Lipschitz.

---

## Results

The reported results are theoretical guarantees rather than empirical experiments.

*   **Theorem 3.2 (Existence and Uniqueness):**
    *   **Existence:** Guaranteed if behavior and target policies are continuous and the parameterized SNRDD condition holds.
    *   **Uniqueness:** Guaranteed if the behavior policy is fixed and the target policy is Lipschitz under the SNRDD condition.
*   **Stability:** The study establishes the SNRDD property as a sufficient condition for the stability and convergence of linear Q-learning algorithms, addressing a historical challenge in function approximation.
*   **AVI Relationship:** Explores the relationship between PBE solution existence and Approximate Value Iteration (AVI) convergence.
*   **Extension:** Extends previous theoretical work to off-policy scenarios.

---

## Contributions

*   **Rigorous Formulation:** Provides rigorous theoretical formalization of the Projected Bellman Equation, clarifying the conditions under which solutions exist.
*   **Unification:** Unifies convergence conditions by demonstrating how the SNRDD assumption relates to both linear Q-learning and AVI convergence.
*   **Policy Analysis:** Contributes to the understanding of how common exploration policies, specifically epsilon-greedy, impact the theoretical solution of value function approximation.