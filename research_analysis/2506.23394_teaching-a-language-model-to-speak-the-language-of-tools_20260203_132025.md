---
title: Teaching a Language Model to Speak the Language of Tools
arxiv_id: '2506.23394'
source_url: https://arxiv.org/abs/2506.23394
generated_at: '2026-02-03T13:20:25'
quality_score: 8
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Teaching a Language Model to Speak the Language of Tools
*Simeon Emanuilov*

---

###  Quick Facts

| Metric | Details |
| :--- | :--- |
| **Accuracy Improvement** | Up to **28.75%** increase in function-calling accuracy |
| **Model Sizes** | 2.6B, 9B, and 27B parameters |
| **Dataset Size** | 10,035 novel bilingual function-calling examples |
| **Target Language** | Bulgarian (Lower-resource) |
| **Protocol** | Model Context Protocol (MCP) compliant |

---

## Executive Summary

Large Language Models (LLMs) demonstrate robust function-calling capabilities primarily in English, creating a significant capability gap for lower-resource languages. This disparity stems from **"language confusion,"** where models struggle to distinguish between natural language context and the structural syntax required for tool use. Addressing this limitation is critical for the development of inclusive, globally accessible agentic AI systems, as current English-centric methodologies fail to generalize effectively to languages like Bulgarian without compromising performance.

To resolve this, the authors introduce **TUCAN**, an adaptation framework designed to decouple natural language processing from the distinct "language of tools." The methodology employs continued training on the BgGPT model series across three specific parameter sizes: 2.6B, 9B, and 27B. A novel bilingual dataset comprising 10,035 function-calling examples was constructed to align the models with the Model Context Protocol (MCP). This approach explicitly teaches the model the required syntax for function calls while preserving its core linguistic competence in the target language.

The study demonstrates that TUCAN achieves a specific improvement of up to **28.75%** in function-calling accuracy compared to base models. Crucially, the adaptation process preserved the models' performance on core Bulgarian linguistic benchmarks, indicating no degradation of general language abilities. Furthermore, the framework successfully eliminated verbose formatting issues, ensuring that generated function calls are clean and parsable. These findings were validated through a specific case study on Bulgarian, confirming the successful mitigation of language confusion.

---

## Key Findings

*   **Significant Accuracy Boost:** The TUCAN model achieves up to a **28.75% improvement** in function-calling accuracy compared to base models.
*   **Linguistic Competence Preserved:** The adaptation method successfully maintains core linguistic competence on standard Bulgarian benchmarks.
*   **Clean Output Generation:** TUCAN models generate clean, parsable function calls, effectively solving prior issues with verbose and unstructured formatting.
*   **Mitigation of Language Confusion:** The approach successfully mitigates language confusion for multilingual models utilizing tools in lower-resource languages.

---

## Methodology

The research methodology focused on adapting existing Large Language Models for robust tool use in non-English contexts through the following steps:

*   **Base Models:** Utilized continued training on the **BgGPT** model series across three parameter sizes: **2.6B, 9B, and 27B**.
*   **Dataset Construction:** A novel bilingual dataset was constructed comprising **10,035 function-calling examples**.
*   **Protocol Alignment:** The training process was specifically aligned with the **Model Context Protocol (MCP)** to ensure standardization.
*   **Validation:** The methodology was validated through a comprehensive case study on the Bulgarian language.

---

## Technical Details

**Framework Concept**
TUCAN is an adaptation method designed to equip language models with accurate function calling capabilities while maintaining linguistic competence in lower-resource languages like Bulgarian.

**Core Problems Addressed**
*   **Language Confusion:** Focuses on mitigating confusion by separating natural language processing from the structural language of tools.
*   **Formatting Issues:** Generates clean, parsable function calls to address verbose formatting issues common in base models.

**Implementation Strategy**
The framework relies on a decoupling strategy, teaching the model to treat the "language of tools" as a distinct domain from natural language, thereby reducing interference between the two modalities.

---

## Contributions

*   **TUCAN Framework:** Introduction of a new methodology for robust tool use in non-English contexts.
*   **Open Resources:** Release of trained models, evaluation frameworks, and the novel bilingual dataset to the community.
*   **Capability Extension:** Demonstration of extending tool-augmented capabilities to lower-resource languages, moving beyond the limitations of English-centric systems.

---

## Results & Evaluation

The implementation of TUCAN yielded quantifiable improvements in both utility and performance:

*   **Accuracy:** Achieved an improvement of up to **28.75%** in function-calling accuracy.
*   **Language Retention:** The adaptation preserved core linguistic competence on Bulgarian benchmarks.
*   **Output Quality:** Successfully resolved verbose formatting issues to ensure machine-readable outputs.
*   **Confusion Reduction:** Mitigated language confusion between natural language context and tool-calling syntax.

> **Quality Score:** 8/10
>
> **References:** 0 citations