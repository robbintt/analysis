---
title: 'MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object
  Detection with Curriculum Learning'
arxiv_id: '2511.12976'
source_url: https://arxiv.org/abs/2511.12976
generated_at: '2026-02-03T19:21:57'
quality_score: 8
citation_count: 34
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning

*Yoonjae Seo; Ermal Elbasani; Jaehong Lee*

---

> ### ðŸ“Š Quick Facts
> *   **Performance (CSE):** 85.6% mAP@0.5 (+3.5pp vs uniform 4-bit)
> *   **Inference Speed:** 151 FPS (Real-time)
> *   **Compression Ratio:** 7.63x (108.3 MB â†’ 14.2 MB)
> *   **Efficiency:** 0.14 J/inference
> *   **Bit-width:** Avg 4.2 bits (Spatially mixed 2â€“8 bits)
> *   **Paper Quality:** 8/10

---

## Executive Summary

Deploying real-time object detection models on resource-constrained edge devices requires aggressive model compression. Traditional uniform quantization methods often lead to significant accuracy degradation at low bit-widths (e.g., 4-bit), while existing mixed-precision approaches suffer from optimization instability and fail to account for spatial variance within feature maps.

**MCAQ-YOLO** addresses these challenges by introducing a framework that utilizes **tile-wise spatial mixed-precision quantization**. By dynamically allocating computational resources based on image region complexity, the system preserves accuracy where it matters most. The approach employs a novel multi-metric morphological complexity predictor and a 3-stage curriculum learning regimen to stabilize training.

The results demonstrate that spatial morphological complexity is a robust proxy for quantization sensitivity. MCAQ-YOLO achieves a substantial compression ratio (7.63x) and real-time speeds (151 FPS) while outperforming uniform baselines by 3.5 percentage points in mAP. This sets a new benchmark for the trade-off between compression, speed, and accuracy in edge computer vision.

---

## Key Findings

*   **Performance Superiority:** MCAQ-YOLO outperformed uniform 4-bit quantization by **3.5 percentage points**, achieving **85.6% mAP@0.5**.
*   **Efficiency:** Achieved a **7.6x compression ratio** with an average bit-width of **4.2 bits**, maintaining real-time speeds of **151 FPS** with only **0.3ms** overhead.
*   **Generalization:** Demonstrated consistent performance improvements across datasets:
    *   **+2.9%** gain on COCO 2017
    *   **+2.3%** gain on Pascal VOC 2012
*   **Complexity Correlation:** Performance gains showed strong correlation with within-image complexity variation.
*   **Small Object Detection:** Improved Average Precision for Small objects (APS) by **+5.1%**.

---

## Methodology

The proposed framework operates on three core pillars to ensure efficient and accurate quantization:

*   **Tile-wise Spatial Mixed-Precision Quantization:**
    *   Divides feature maps into an 8x8 grid.
    *   Allocates bits dynamically (2â€“8 bits) based on the complexity of specific image regions rather than applying uniform layer-wise quantization.
*   **Morphological Complexity Assessment:**
    *   Utilizes five distinct metrics to determine spatial quantization sensitivity: Fractal Dimension, Texture Entropy, Gradient Variance, Edge Density, and Contour Complexity.
    *   Spatial bit allocation is determined during a calibration phase to ensure minimal impact on inference.
*   **Curriculum-Based Training:**
    *   Employs a progressive training scheme that gradually increases quantization difficulty.
    *   This approach stabilizes the optimization process and accelerates convergence compared to standard training methods.

---

## Technical Details

### Architecture & Quantization
*   **Precision Range:** Supports 2â€“8 bit quantization.
*   **Weights:** Uniformly quantized to 4 bits.
*   **Activations:** Spatially mixed precision (average 4.2 bits).
*   **Grid Size:** Tile-wise quantization performed on an 8x8 grid.

### Complexity Analyzer
*   **Metrics Used:** 5 morphological metrics (Fractal Dimension, Texture Entropy, Gradient Variance, Edge Density, Contour Complexity).
*   **Efficiency:** Operational complexity of **O(n log n)**.
*   **Optimization:** Achieves an **85% cache hit rate**.

### Bit Allocation Strategy
*   **Mapping Mechanism:** A learnable Multi-Layer Perceptron (MLP) maps complexity scores to bit-widths.
*   **Monotonicity:** Uses polynomial expansion and non-negative weights to ensure smooth transitions and prevent allocation jumps.

### Training Regimen
*   **Curriculum Strategy:** 3-stage curriculum learning strategy.
*   **Loss Function Components:** A composite loss function including:
    *   Detection loss
    *   Bit budget constraint
    *   Smoothness term
    *   Knowledge distillation
    *   Regularization terms

---

## Contributions

1.  **Framework Introduction:** Introduced MCAQ-YOLO, a practical real-time object detection system implementing tile-wise spatial mixed-precision quantization.
2.  **Novel Predictor:** Proposed a multi-metric morphological complexity predictor to accurately assess spatial quantization sensitivity.
3.  **Training Optimization:** Developed a curriculum-based training regimen specifically designed to address mixed-precision optimization instability.
4.  **Empirical Validation:** Provided comprehensive validation demonstrating that complexity-aware quantization significantly outperforms uniform methods across diverse datasets (CSE, COCO, Pascal VOC).

---

## Results

### CSE Dataset
*   **Accuracy:** 85.6% mAP@0.5 (4.2-bit activations).
*   **Comparison:** Outperformed Uniform 4-bit by **+3.5pp** and HAWQ-V3 by **+0.9pp**.
*   **Small Objects:** APS improvement of **+5.1%**.

### System Metrics
*   **Compression:** 7.63x ratio (Model size reduced from 108.3 MB to 14.2 MB).
*   **Speed:** 151 FPS (Fast variant achieved 158 FPS with 84.8% mAP).
*   **Energy:** 0.14 J/inference.

### Generalization (Other Datasets)
*   **COCO 2017:** 48.1% mAP (+2.9% gain).
*   **Pascal VOC 2012:** 79.1% mAP (+2.3% gain).

### Ablation & Statistical Significance
*   **Curriculum Learning:** Contributed **+2.5pp** mAP in ablation studies.
*   **Correlation:** Complexity scores showed a Spearman correlation of **0.73** with mAP degradation.
*   **Significance:** Results were statistically significant (p < 0.01).

---
**References:** 34 citations | **Quality Score:** 8/10