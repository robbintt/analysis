# MiMo-Audio: Audio Language Models are Few-Shot Learners

*Xiaomi LLM-Core Team; Dong Zhang; Gang Wang; Jinlong Xue; Kai Fang; Liang Zhao; Rui Ma; Shuhuai Ren; Shuo Liu; Tao Guo; Weiji Zhuang; Xin Zhang; Xingchen Song; Yihan Yan; Yongzhe He; et al.*

---

### ðŸ“Š Quick Facts & Key Metrics

| **Metric** | **Details** |
| :--- | :--- |
| **Model Architecture** | Modular Encoder-Decoder (7B Parameters) |
| **Training Scale** | >100 Million Hours of Audio |
| **Token Count** | 2.0 Trillion Tokens |
| **Paradigm** | Next-token Prediction (GPT-3 style for Audio) |
| **Key Benchmark (MMAU-Pro)** | **74.9** (Outperforming GPT-4o) |
| **SpeechMMLU (5-shot)** | ~68% Accuracy |

---

### ðŸ“ Executive Summary

The current audio processing ecosystem is fragmented, relying on specialized models designed for singular tasks such as Automatic Speech Recognition (ASR) or Text-to-Speech (TTS) that require extensive task-specific fine-tuning. This reliance on siloed architectures hinders the development of unified systems capable of generalizing across diverse audio domains.

The authors address this by investigating whether the scaling laws and emergent few-shot learning capabilities observed in text-based Large Language Models (LLMs) can be successfully replicated in the audio domain. Their goal is to develop a single, general-purpose model capable of understanding and generating audio without the need for task-specific retraining.

The core innovation, **MiMo-Audio**, is a massive 7B parameter modular encoder-decoder trained on over **100 million hours** of audio data (2.0 trillion tokens). The architecture employs a distinct technical division of labor: a *MiMo-Audio-Tokenizer* compresses raw audio into discrete representations, while a separate *Patch Encoder* and *Decoder* handle the reconstruction of audio waveforms. The training methodology is bifurcated into two stages: "Understanding Training" and "Understanding-Generation Joint Training."

MiMo-Audio models achieve **state-of-the-art performance among open-source models** and approach the capabilities of leading closed-source systems. The Instruct variant scored **74.9 on the MMAU-Pro benchmark**, rivaling proprietary models like GPT-4o. Furthermore, the model exhibits emergent few-shot abilities on unseen tasks, successfully performing complex operations such as voice conversion, style transfer, and long-form speech continuation. This research represents a paradigm shift, demonstrating that general-purpose audio models can be developed through massive pretraining rather than task-specific fine-tuning.

---

## ðŸ” Key Findings

*   **Few-Shot Learning Capabilities:** Scaling next-token prediction pretraining to over 100 million hours of audio enables audio language models to learn from minimal examples and generalize to new tasks effectively.
*   **Open-Source SOTA Performance:** The base model (*MiMo-Audio-7B-Base*) achieves state-of-the-art performance among open-source models on speech intelligence and audio understanding benchmarks.
*   **Emergent Unseen Task Handling:** The model demonstrates the ability to perform tasks not explicitly trained for, including:
    *   Voice Conversion
    *   Style Transfer
    *   Speech Editing
*   **Long-Form Generation:** Exhibits powerful speech continuation abilities, generating realistic long-form content such as talk shows and debates.
*   **Instruction Tuning Success:** The Instruct model (*MiMo-Audio-7B-Instruct*) achieves open-source SOTA on MMSU and MMAU benchmarks, approaching closed-source model performance by integrating "thinking mechanisms."

---

## âš™ï¸ Methodology

The authors adopted a paradigm similar to GPT-3, applying it to the audio domain via next-token prediction pretraining.

1.  **Massive Scaling:** Training on a dataset of over 100 million hours of audio to leverage scaling laws.
2.  **Two-Stage Training:**
    *   **Understanding Training:** Focuses exclusively on audio comprehension.
    *   **Understanding-Generation Joint Training:** Simultaneously optimizes for both comprehension and reconstruction.
3.  **Instruction Tuning:** Curating a diverse instruction-tuning corpus during the post-training stage.
4.  **Thinking Mechanisms:** Integrating reasoning processes into both audio understanding and generation to enhance output quality and alignment.

---

## ðŸ›  Technical Specifications

| Component | Specification |
| :--- | :--- |
| **Model Name** | MiMo-Audio (Base & Instruct variants) |
| **Parameter Count** | 7 Billion |
| **Core Architecture** | Modular Encoder-Decoder centered around an LLM |
| **Audio Processing** | MiMo-Audio-Tokenizer (raw audio to discrete representations) |
| **Reconstruction** | Patch Encoder & Patch Decoder |
| **Training Data** | >100 Million Hours |
| **Training Tokens** | 2.0 Trillion Tokens |
| **Instruct Features** | Integrated thinking mechanisms (Chain-of-Thought) |

---

## ðŸ“ˆ Performance Results

**Scaling Laws & Benchmarks**
*   **SpeechMMLU (5-shot):** Text-to-Speech accuracy reached **~68%**; Speech-to-Speech reached **~35%** at 2.0 trillion tokens.
*   **MMAU-Pro:** MiMo-Audio-7B-Instruct scored **74.9**, outperforming GPT-4o.
*   **MMAU Big Bench Audio:** Scored in the range of **67.4 â€“ 71.0**.

**Functional Capabilities**
*   **Speech-to-Speech Translation:** Achieved COMET scores ranging from **56.8 to 67.2**.
*   **Emergent Capabilities:** Demonstrated few-shot success in voice conversion, style transfer, and multi-turn dialogue generation.

---

## ðŸš€ Core Contributions

1.  **Paradigm Shift:** Demonstrated that general-purpose audio models can be developed through massive pretraining, significantly reducing reliance on task-specific fine-tuning.
2.  **Performance Standards:** Established new open-source performance standards across speech intelligence, audio understanding, and spoken dialogue benchmarks.
3.  **Emergent Abilities:** Showcased new capabilities in complex, unseen tasks such as multi-personality speech generation and audio manipulation.
4.  **Open Science:** Released model checkpoints and a full evaluation suite to the community to foster further research in audio language models.

---

**Quality Score:** `7/10` | **References:** `24 Citations`