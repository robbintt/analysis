# Histogram-Equalized Quantization for logic-gated Residual Neural Networks

*Van Thien Nguyen; William Guicquero; Gilles Sicard*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 40
> *   **Key Technique:** Histogram-Equalized Quantization (HEQ)
> *   **Novel Architectures:** ORNet-11, MUXORNet-11
> *   **Best Accuracy (CIFAR-10):** 84.2%
> *   **Hardware Focus:** Ternarized Neural Networks (TNNs) & Logic Gates

---

## Executive Summary

This research addresses the critical challenge of deploying deep neural networks (DNNs) on resource-constrained hardware, where maintaining high accuracy is difficult under extreme quantization scenarios such as ternarized neural networks (TNNs). Existing methods often rely on static thresholds that fail to capture full weight distributions, leading to performance degradation. Furthermore, standard topologies like VGG are not optimized for efficient hardware mapping. Consequently, there is a need for solutions that reduce computational complexity and model size without sacrificing representational power, particularly for edge devices requiring ultra-low-power inference.

The paper introduces a two-fold advancement: **Histogram-Equalized Quantization (HEQ)** and **Logic-Gated Residual Networks (Logic-Gated ResNets)**. HEQ is a Quantization-Aware Training (QAT) framework that optimizes linear symmetric quantization by equalizing layer-wise weight histograms. Its core technical contribution is a unique step size optimization that dynamically adapts quantization parameters based on the n-quantiles of the weight distribution and the model's training loss. This allows for automatic threshold adjustment that minimizes information loss during the learning process. Architecturally, the authors propose Logic-Gated ResNets, which replace standard skip connections with digital logic gates (OR and 2-input MUX) and employ thresholded bitcounts. This design shifts away from VGG-like topologies, simplifying hardware mapping while preserving residual learning capabilities.

Empirical testing demonstrates a superior accuracy-hardware complexity trade-off. On the CIFAR-10 dataset, the baseline model achieved 83.3% accuracy, while the Logic-Gated models improved to 83.8% (ORNet-11) and 84.2% (MUXORNet-11). Notably, MUXORNet-11 surpassed the Learnable Step Size Quantization (LSQ) methodâ€”a reference model utilizing higher precision and larger parameter countsâ€”by 0.37%. In terms of operational efficiency, the models maintained the same number of convolution parameters as the baseline, incurring hardware overhead limited only to the addition of logic gates and bitcounts. Furthermore, the framework was validated on the more complex STL-10 dataset, where it achieved better accuracy than comparative methods, reinforcing its robustness.

This work is significant because it demonstrates that adaptive quantization, combined with hardware-aware architectural design, can substantially improve the feasibility of low-precision inference. By enabling logic-gated residual networks to outperform existing methods, the research opens a new design space for efficient deep learning accelerators that leverage native digital logic operations rather than relying solely on dense matrix multiplication. The HEQ framework provides a robust method for automatic threshold adaptation, offering a viable path for deploying highly accurate, computationally efficient models on embedded systems.

---

## Key Findings

*   **State-of-the-Art Performance:** HEQ achieved top-tier performance on the **CIFAR-10** dataset compared to existing quantization methods.
*   **Novel Hardware Architectures:** Successfully enabled the training of logic-gated residual networks (utilizing **OR** and **MUX** gates) on the **STL-10** dataset.
*   **Superior Trade-offs:** The proposed approach demonstrated a better accuracy-hardware complexity trade-off, delivering higher accuracy while simultaneously lowering complexity.
*   **Robust Validation:** Empirical validation across both CIFAR-10 and STL-10 datasets confirmed the robustness and generalizability of the quantization framework.

---

## Methodology

The study introduces **Histogram-Equalized Quantization (HEQ)**, an adaptive framework designed specifically for linear symmetric quantization. The methodology centers on a unique **step size optimization** mechanism that automatically adapts quantization thresholds.

*   **Dynamic Adjustment:** The method adjusts quantization parameters dynamically based on data distributions or model loss requirements.
*   **Low-Precision Focus:** The core objective is to maintain high accuracy in environments requiring low-precision inference (e.g., ternary weights).

---

## Contributions

1.  **Novel Algorithm:** Introduction of HEQ as a new algorithm for automatic threshold adaptation in linear symmetric quantization.
2.  **Architecture Innovation:** Proposal of residual neural networks integrated with digital logic gates (OR and MUX), expanding the architectural design space for efficient inference.
3.  **Efficiency Advancement:** Demonstration that adaptive quantization can significantly improve the accuracy-complexity trade-off for hardware implementations.

---

## Technical Details

### Quantization Framework: HEQ
*   **Type:** Quantization-Aware Training (QAT) method.
*   **Mechanism:** Equalizes layer-wise weight histograms and adaptively modifies quantization step sizes based on **n-quantiles** of the weight distribution.
*   **Benefit:** Allows for dynamic adaptation to the specific distribution of weights in each layer.

### Network Architecture: Logic-Gated ResNets
*   **Topology:** Moves beyond standard VGG-like structures by replacing standard skip connections with digital logic gates.
*   **Gates Utilized:**
    *   **OR gates** (implemented in ORNet-11).
    *   **2-input MUX gates** (implemented in MUXORNet-11).
*   **Operations:** Employs thresholded bitcounts to simplify the hardware mapping of Ternarized Neural Networks (TNNs).
*   **Efficiency:** Maintains a similar parameter count to the baseline while simplifying logic for hardware deployment.

---

## Results

### Performance on CIFAR-10
The proposed Logic-Gated ResNets showed measurable improvements over the baseline and competitive performance against higher-precision reference models.

| Model | Accuracy | Comparison |
| :--- | :--- | :--- |
| **Baseline** | 83.3% | Reference point |
| **ORNet-11** | 83.8% | Matched LSQ reference accuracy |
| **MUXORNet-11** | **84.2%** | Exceeded LSQ [18] by **0.37%** |

### Efficiency Metrics
*   **Parameter Count:** Maintained the same size regarding convolution parameters as the baseline.
*   **Hardware Overhead:** Limited to the addition of logic gates and bitcounts.
*   **Complexity:** Showcased a superior accuracy-hardware complexity trade-off compared to the LSQ method (which operates at higher precision and parameter counts).

### Performance on STL-10
*   The method achieved better accuracy than comparative methods, confirming robustness on more complex datasets.