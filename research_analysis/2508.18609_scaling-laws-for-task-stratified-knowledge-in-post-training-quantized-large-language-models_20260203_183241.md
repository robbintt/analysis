---
title: Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large
  Language Models
arxiv_id: '2508.18609'
source_url: https://arxiv.org/abs/2508.18609
generated_at: '2026-02-03T18:32:41'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models

*Chenxi Zhou; Pengfei Cao; Jiang Li; Jun Zhao; Kang Liu*

---

> ### üìä Quick Facts
>
> *   **Models Analyzed:** Qwen3 (1.7B ‚Äì 14B parameters)
> *   **Quantization Method:** Post-Training Quantization (PTQ) via GPTQ
> *   **Key Parameters:** Model Size ($N$), Bit-width ($B$), Calibration Set Size ($C_b$), Group Size ($G$)
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations

---

### üìù Executive Summary

Current scaling laws for Large Language Models (LLMs) predominantly focus on full-precision models, failing to account for the performance dynamics introduced by Post-Training Quantization (PTQ), a critical technique for reducing the computational costs of deployment. As LLMs are increasingly compressed to lower bit-widths, existing theoretical models struggle to predict performance accurately because they treat all knowledge capabilities as uniform. This is a significant gap because quantization impacts different cognitive functions unevenly; without a precise understanding of these task-specific sensitivities, practitioners cannot reliably predict how a compressed model will perform or how to best allocate resources during the quantization process.

This paper introduces a novel framework for establishing task-stratified scaling laws specifically for PTQ LLMs. The key innovation lies in disentangling LLM knowledge into distinct categories based on Bloom's Taxonomy: Knowledge Memorization (KM), Knowledge Application (KA), and Knowledge Reasoning (KR). The authors develop a unified quantitative model that integrates standard scaling parameters (Model Size, $N$) with quantization-specific hyperparameters: Effective Bit-width ($B$), Calibration Set Size ($C_b$), and Group Size ($G$). By utilizing the GPTQ algorithm and second-order optimization information, the study isolates how these specific parameters influence the reconstruction error and subsequent performance across the different knowledge strata.

The empirical analysis, conducted on Qwen models ranging from 1.7B to 14B parameters, reveals distinct scaling behaviors for different cognitive tasks. While accuracy generally follows power-law trends with model size, the study identified a "sharp recovery" phenomenon where performance at 2-bit precision hovers near a random baseline of 0.25, but jumps significantly at 3-bit and saturates near FP16 levels at $\ge$4-bit precision. The results demonstrate that Knowledge Memorization is highly sensitive to quantization parameters, whereas Application and Reasoning capabilities remain robust. Furthermore, the study found an inverse relationship between accuracy and Group Size, with smaller groups (32, 64) significantly outperforming larger groups (1024) in mitigating performance loss. Increasing Calibration Set Size was shown to yield diminishing returns with non-linear saturation.

This work significantly bridges the gap between theoretical scaling laws and the practical realities of model deployment. By identifying that memorization capabilities are the bottleneck in quantized models, the authors provide actionable guidance for developing "knowledge-aware" quantization strategies. This enables researchers and engineers to prioritize the preservation of sensitive cognitive functions during compression, offering a roadmap for optimizing the trade-off between model efficiency and performance retention. The findings suggest that future quantization methods must move beyond global optimization to task-specific tuning to maintain high utility in resource-constrained environments.

---

## Key Findings

*   **Task-Stratified Scaling Laws:** The study establishes new scaling laws for PTQ LLMs, revealing that performance cannot be accurately predicted without accounting for task-specific sensitivities.
*   **Differential Sensitivity:**
    *   **Memorization:** Highly sensitive to variations in effective bit-width, calibration set size, and model size.
    *   **Utilization:** Robust against changes in quantization parameters.
*   **Critical Parameters:** Effective bit-width, calibration set size, and group size are identified as the three most critical parameters for modeling quantized LLM behavior.

---

## Methodology

The authors conducted an extensive empirical study to investigate the impact of Post-Training Quantization (PTQ) on LLM knowledge capabilities. Their approach included:

1.  **Disentanglement of Knowledge:** Separating LLM knowledge into memorization and utilization categories.
2.  **Unified Framework:** Developing a quantitative framework that integrates:
    *   Model size
    *   Effective bit-width
    *   Calibration set size
    *   Group size
3.  **Modeling:** Using these variables to model performance scaling across different tasks.

---

## Technical Details

The paper utilizes the following taxonomies and technical parameters to structure the analysis:

**Knowledge Taxonomy (Based on Bloom's Taxonomy)**
*   **Knowledge Memorization (KM):** The ability to store and recall information.
*   **Knowledge Application (KA):** The ability to apply learned information to specific contexts.
*   **Knowledge Reasoning (KR):** The ability to logically process and derive conclusions from information.

**Quantization Parameters**
*   **Algorithm:** GPTQ (Post-Training Quantization)
*   **Model Size ($N$):** Varied across Qwen3 models (1.7B ‚Äì 14B).
*   **Bit-width ($B$):** Precision levels analyzed (2-bit to FP16).
*   **Calibration Set Size ($C_b$):** Data used for minimizing reconstruction error.
*   **Group Size ($G$):** The grouping of parameters for quantization.
*   **Optimization:** Utilizes second-order information and calibration data to minimize reconstruction error.

---

## Results

The empirical results highlight specific behaviors regarding quantization parameters and cognitive tasks:

*   **Power-Law Trends:** Accuracy generally follows a power-law trend with model size, but this trend **breaks down at 2-bit precision**.
*   **Sharp Recovery Phenomenon:**
    *   **2-bit:** Performance near random baseline (0.25).
    *   **3-bit:** Significant jump in performance.
    *   **$\ge$4-bit:** Performance saturates near FP16 levels.
*   **Calibration Set Size:** Increasing size yields **diminishing returns** characterized by non-linear saturation.
*   **Group Size Impact:** Accuracy exhibits an **inverse relationship** with group size.
    *   **Smaller groups (32, 64):** Better mitigation of loss.
    *   **Large groups (1024):** Higher performance degradation.
*   **Task Robustness:** Knowledge Application and Reasoning are robust, whereas Knowledge Memorization is the primary bottleneck in low-bit-width scenarios.

---

## Contributions

*   **Bridging the Gap:** Connects existing scaling laws with PTQ-specific parameters, accounting for task-specific sensitivities previously ignored.
*   **Fine-Grained Analysis:** Provides a detailed breakdown of how quantization differentially affects cognitive functions (memorization vs. application/reasoning).
*   **Actionable Guidance:** Offers insights for developing knowledge-aware quantization strategies that preserve targeted functions during model compression.