---
title: 'Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference'
arxiv_id: '2509.18487'
source_url: https://arxiv.org/abs/2509.18487
generated_at: '2026-02-03T13:22:39'
quality_score: 8
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference

*Ben Finkelshtein; Silviu Cucerzan; Sujay Kumar Jauhar; Ryen White*

> ### **Quick Facts**
> * **Quality Score:** 8/10
> * **References:** 19 Citations
> * **Primary Task:** Node Classification on Graphs
> * **Key Insight:** Code generation outperforms standard prompting on complex graph structures.
> * **Scope:** Evaluation across citation, web, e-commerce, and social networks.

---

### **Executive Summary**

This research addresses the challenge of effectively applying Large Language Models (LLMs) to node classification on graph-structured data, a task complicated by the rigid input constraints of standard prompting. Conventional methods often fail because linearizing graph data into textual prompts exceeds token limits, particularly for graphs featuring long-text attributes or high-degree nodes, resulting in significant information loss. Additionally, the study investigates the prevailing assumption that LLMs perform poorly on heterophilic graphs—networks where connected nodes often possess different labels—which has historically limited their perceived utility in complex relational learning tasks.

The paper introduces a comprehensive evaluation framework comparing three distinct LLM interaction strategies: Prompting (graph linearization), Tool-use (ReAct paradigm with fixed tools), and Code Generation. The key technical innovation is the "Graph-as-Code" approach, where the LLM operates as a programmatic agent that generates and executes arbitrary Python code to manipulate the graph's adjacency matrix and node features. This method grants the model high agency, allowing it to bypass token constraints and algorithmically balance the use of graph structure, node features, and labels, rather than relying solely on static context window inputs.

The study reveals that code generation significantly outperforms other methods, particularly on datasets with long-text attributes or high-degree nodes where prompting is infeasible. A clear "value of agency" trend emerged, with performance scaling from Tool-use to Graph-as-Code. Notably, on heterophilic datasets, Graph-as-Code achieved the best results in three out of four benchmarks, scoring 92.70% on Cornell, 73.60% on Texas, and 81.96% on Washington. Contrary to prior beliefs, all LLM interaction modes demonstrated robust performance on heterophilic graphs, generally outperforming traditional baselines like Label Propagation, and the code generation approach proved adept at adaptively leveraging the most informative data sources for the task.

By refuting the hypothesis that LLMs collapse under low homophily, this work significantly broadens the perceived applicability of LLMs in graph inference, validating their use for diverse structural regimes beyond homophilic citation networks. The findings establish code generation as a critical design principle for future graph learning systems, offering a practical solution to the token budget limitations inherent in pure prompting approaches. Ultimately, this research provides a principled foundation for integrating LLMs with graph machine learning, highlighting a path toward systems that can flexibly reason over both structural and semantic information.

---

## Key Findings

*   **Code Generation Superiority:** LLMs utilized as code generators achieve the strongest overall performance on graph data. They significantly outperform other methods on graphs with long-text attributes or high-degree nodes where standard prompting fails due to token budget constraints.
*   **Robustness on Heterophilic Graphs:** LLM-based interaction strategies (prompting, tool-use, and code generation) remain effective on heterophilic graphs. This challenges assumptions that these models collapse under conditions of low homophily.
*   **Adaptive Reliance:** The code generation approach demonstrates "adaptive reliance," flexibly balancing the use of graph structure, node features, and labels to leverage the most informative source for the specific task at hand.

## Methodology

The study employs a large-scale, controlled evaluation framework designed to systematically assess LLM-based graph reasoning across five specific axes:

1.  **Interaction Modes:** Prompting, Tool-use, and Code generation.
2.  **Dataset Domains:** Citation networks, web-link graphs, e-commerce platforms, and social networks.
3.  **Structural Regimes:** Homophilic vs. Heterophilic.
4.  **Feature Characteristics:** Short-text vs. Long-text.
5.  **Model Configurations:** Varying LLM sizes and reasoning capabilities.

**Dependency Analysis:**
To quantify model reliance on specific inputs, the study conducted dependency analysis by:
*   Truncating features
*   Deleting edges
*   Removing labels

## Contributions

*   **Comprehensive Assessment:** Provides a principled evaluation of the capabilities and limitations of LLMs in text-rich graph machine learning tasks.
*   **Design Principles:** Establishes practical guidelines for future approaches, specifically highlighting the efficacy of code generation for handling complex graph data and high token consumption scenarios.
*   **Challenging Assumptions:** Refutes the hypothesis that LLMs perform poorly on heterophilic graphs, thereby broadening the perceived applicability of these models in graph inference.

## Technical Details

**Problem Formulation:**
Node classification on an unweighted graph $G = (V, E, X, Y)$.
*   **Inputs:** Labeled nodes $K$, graph structure (Adjacency matrix $A$), and textual features $X$.
*   **Output:** Labels $Y_Q$ for query set $Q$.

**Interaction Strategies:**

1.  **Prompting ($\phi_{prompt}$):**
    *   Single-turn inference with graph linearization.
    *   Utilizes $k$-hop neighborhood (0, 1, 2 hops).
    *   Includes a **Budget prompt variant** for subsampling to adhere to token limits.

2.  **GraphTool ($\phi_{tool}$):**
    *   Iterative loop based on the ReAct paradigm (Think-Act-Observe).
    *   Utilizes a fixed tool set.

3.  **Graph-as-Code ($\phi_{code}$):**
    *   Fully programmatic interaction.
    *   The LLM generates arbitrary code to operate on graph structure and features.

**Baselines:**
Random, Majority Label, and Label Propagation (LP).

## Results

**Metric:** Accuracy with standard deviation.

### Short-Text Homophilic Graphs
*   Prompting and Graph-as-Code are competitive.
*   Accuracy generally increases with added neighborhood context (0-hop to 2-hop) until context token limits are reached.

### Value of Agency
A clear performance trend is observed indicating that increased agency and adaptivity improves classification accuracy:
$$ \text{GraphTool} < \text{GraphTool+} < \text{Graph-as-Code} $$

### Heterophilic Graphs
All LLM interaction modes achieved strong accuracy, challenging assumptions about performance on low homophily. They generally outperform traditional baselines like Majority Label and Label Propagation (LP).

**Quantitative Results for Heterophilic Datasets:**

| Dataset | Graph-as-Code Accuracy | Comparison / Baseline | Best Performer |
| :--- | :--- | :--- | :--- |
| **Cornell** | 92.70 ± 2.35 | — | **Graph-as-Code** |
| **Texas** | 73.60 ± 3.78 | Baseline LP: 78.90 | Baseline LP |
| **Washington** | 81.96 ± 2.92 | — | **Graph-as-Code** |
| **Wisconsin** | 89.17 ± 2.69 | 2-Hop Prompt: 91.45 ± 1.87 | 2-Hop Prompt |

*Note: The study reports Graph-as-Code achieved the best results in 3 out of 4 datasets.*