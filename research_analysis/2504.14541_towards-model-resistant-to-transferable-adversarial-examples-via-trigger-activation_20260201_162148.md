# Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation

*Yi Yu; Song Xia; Xun Lin; Chenqi Kong; Wenhan Yang; Shijian Lu; Yap-Peng Tan; Alex C. Kot*

---

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Method** | Model with Trigger Activation |
> | **Datasets** | CIFAR-10, CIFAR-100 |
> | **Best Robust Accuracy (CIFAR-10)** | ~72.98% (vs I-FGSM) |
> | **Adaptive Attack Robustness** | ~65.05% (Mean) |
> | **Test-Time Latency** | Zero |
> | **Quality Score** | 8/10 |

---

## Executive Summary

This research addresses the critical security threat posed by Transferable Adversarial Examples (TAEs), which allow attackers to successfully deploy black-box attacks against machine learning models without access to the underlying model or gradients. Existing defense mechanisms face significant limitations; adversarial training often incurs high computational costs and robustness-accuracy trade-offs, while purification-based methods introduce substantial inference latency.

This paper seeks a robust defense that effectively mitigates transferable attacks without requiring expensive on-the-fly adversarial example generation or sacrificing inference efficiency. The authors propose **"Model with Trigger Activation,"** a novel training paradigm that fundamentally alters the standard optimization objective. Instead of training the model to be accurate on clean inputs, the method enforces a **dual-state behavior**: the model is trained to output random guesses on clean data (enforcing a uniform distribution) while generating accurate predictions only when data is perturbed by a specific, global trigger ($\tau$).

This is achieved through a joint optimization process involving both model parameters and the trigger, utilizing a composite loss function that combines Cross-Entropy Loss on triggered data and KL Divergence Loss on clean data. Theoretically, the authors demonstrate that this approach induces robustness through **"gradient misalignment,"** where the gradients of transferred adversarial examples fail to align with the decision boundary established by the trigger.

Extensive experiments on CIFAR-10 and CIFAR-100 validate the method's ability to resist transferable attacks. Against adaptive attacksâ€”specifically where standard PGD-based adversarial training drops to approximately 17.01% accuracyâ€”the proposed fixed trigger variant ("Ours f") achieved a mean robust accuracy of 65.05%. Specific robustness scores included 72.98% against I-FGSM, 61.88% against PGD, 65.91% against MI-FGSM, and 56.30% against DI-FGSM. Notably, regarding the model's utility, the framework achieves an accuracy of approximately 92.05% on CIFAR-10, but this high performance is contingent on the authorized trigger being applied to non-adversarial data. On the more complex CIFAR-100 dataset, the method maintained strong robustness with 46.96% accuracy against I-FGSM and 40.70% against PGD.

---

## Key Findings

*   **Robustness via Trigger Activation:** Models trained to output random guesses on clean data but accurate predictions on 'triggered' data exhibit robustness against Transferable Adversarial Examples (TAEs).
*   **Theoretical Justification:** The observed robustness against transferable attacks is explained and analyzed through the lens of first-order gradients.
*   **Superior Defense Performance:** The proposed method demonstrates effectiveness and superiority over existing defense techniques across diverse datasets and against a variety of attack methods.
*   **Optimization Efficacy:** Jointly optimizing the learnable trigger and the model parameters leads to improved resilience against transferable attacks compared to non-optimized baselines.

---

## Methodology

The authors propose a novel training paradigm called **'models with trigger activation.'** This approach operates on a distinct dual-state behavior:

1.  **Clean Data State:** The model is trained to exhibit random guessing behavior on clean data ($x$).
2.  **Triggered Data State:** The model generates accurate predictions on triggered data ($x + \tau$).

Key components of the methodology include:

*   **Global Learnable Trigger:** A specific trigger ($\tau$) is utilized that remains constant across all data instances.
*   **Joint Optimization:** The training process involves the joint optimization of both the model parameters and the learnable trigger to maximize robustness.

---

## Technical Details

The **'Model with Trigger Activation'** defends against Transferable Adversarial Examples (TAEs) by inverting standard classification logic.

### Algorithm Mechanism
*   **Trigger Initialization:** The trigger is initialized via a Bernoulli distribution scaled by $\tau = \epsilon \cdot (2 \cdot \epsilon - 1)$.
*   **Input Construction:** Training input is constructed by concatenating clean and triggered batches.
*   **Loss Function:** The model optimizes a composite loss:
    *   **Cross-Entropy Loss:** Applied only on triggered data to ensure accurate classification.
    *   **KL Divergence Loss:** Applied on clean data to enforce a uniform distribution (random guessing).

### Optimization Strategies
*   **Fixed Trigger:** A static trigger is used during training.
*   **Learnable Trigger:** The trigger is treated as a variable and jointly optimized with model parameters.

### Theoretical & Operational Advantages
*   **Theoretical Basis:** Robustness is justified by gradient misalignment; transferred perturbations do not align with the trigger's gradient inverse.
*   **Deployment:** Uses $f(x + \tau)$ at inference.
*   **Efficiency:**
    *   **Zero Test-Time Latency.**
    *   **Faster Training:** No adversarial example generation required (unlike Adversarial Training).
    *   **Efficient Inference:** Does not require the computational overhead of purification methods.

---

## Results

The evaluation demonstrates that the method surpasses models without defense and shows competitive performance compared to Adversarial Training (AT), with robustness increasing alongside the trigger bound $\epsilon_t$.

### CIFAR-10 Performance
*   **Clean Accuracy:** ~92.05%
*   **Standard Attack Robustness:** Ranges from 74.51% (DI-FGSM) to 87.44% (I-FGSM).
*   **Adaptive Attack Robustness (Fixed Trigger "Ours f"):**
    *   **Mean Robustness:** ~65.05% (vs. ~17.01% baseline)
    *   **I-FGSM:** 72.98%
    *   **PGD:** 61.88%
    *   **MI-FGSM:** 65.91%
    *   **DI-FGSM:** 56.30%

### CIFAR-100 Performance
*   **I-FGSM Robustness:** 46.96%
*   **PGD Robustness:** 40.70%

---

## Contributions

1.  **New Defense Paradigm:** Introduction of a unique training strategy that conditions accurate classification on the presence of a global trigger, effectively separating the model's decision boundary from clean data distributions.
2.  **Theoretical Analysis:** Provision of a theoretical framework based on first-order gradients to explain why trigger activation models are resistant to the transferability of adversarial examples.
3.  **Mitigation of Limitations:** The approach addresses common failures in existing defense methods, specifically targeting issues related to inefficient deployment, ineffective defense against black-box scenarios, and performance degradation.
4.  **Comprehensive Validation:** Extensive experimental verification of the method's ability to defend against transferable attacks in black-box settings across various datasets and attacking methodologies.

---

**Quality Score:** 8/10  
**References:** 40 citations