# Building Effective Safety Guardrails in AI Education Tools

*Hannah-Beth Clark; Laura Benton; Emma Searle; Margaux Dowland; Matthew Gregory; Will Gayne; John Roberts*

---

> ### **Executive Summary**
>
> The integration of generative AI into educational environments for minors (aged 5–16) introduces critical risks regarding harmful content, bias, and adversarial attacks, particularly within sensitive subjects such as Relationships, Sex, and Health Education (RSHE). Existing safety mechanisms are proving insufficient for the complex pedagogical requirements of the UK national curriculum, creating a gap for architectures that ensure strict adherence to safety standards without compromising utility.
>
> This paper addresses the lack of proven, robust safety protocols in the public sector by analyzing the deployment challenges within the UK Government's **"Aila"** lesson planning tool. The authors propose a multi-layered safety framework comprising four distinct defense layers: dynamic prompt engineering, an Input Threat Detection system, human-in-the-loop oversight, and the **Independent Asynchronous Content Moderation Agent (IACMA)**.
>
> Rigorous evaluation against **45,000 post-launch lessons** revealed that lessons generated without explicit user steerage met curriculum standards more effectively than those with heavy user input. A key technical finding was **"Sequential vs. Batch Moderation Misalignment,"** where post-hoc batch evaluations contradicted real-time streaming moderation decisions due to evolving lesson context. This research establishes a concrete, replicable blueprint for deploying generative AI in safety-critical sectors, shifting the paradigm from "set-and-forget" architectures to dynamic systems requiring persistent human verification and continuous iteration.

---

### **Quick Facts**

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 37 Citations |
| **Test Scope** | 45,000 Lessons |
| **Core Innovation** | Independent Asynchronous Content Moderation Agent (IACMA) |
| **Target Audience** | Pupils aged 5–16 (UK National Curriculum) |

---

## Key Findings

*   **Multi-Layered Framework:** Successful implementation of a comprehensive safety strategy in the UK Government's 'Aila' tool, combining prompt engineering, threat detection, IACMA, and human oversight.
*   **Unique Educational Challenges:** Identification of specific risks associated with AI tools designed for minors (ages 5–16), particularly concerning sensitive curriculum subjects like RSHE.
*   **Hybrid Mitigation Necessity:** Demonstration that technical barriers alone are insufficient; a hybrid approach combining automated defenses with human verification is critical for safety.
*   **Continuous Iteration:** Proof that "set-and-forget" safety models are ineffective; continuous monitoring and iteration are required to maintain efficacy.
*   **Collaboration Value:** Highlighting the importance of cross-sector collaboration and open-sourcing code to raise industry safety standards.

## Methodology

The research employs a **system design and evaluation case study** centered on the development and deployment of **'Aila,'** an AI-powered lesson planning assistant. The methodological approach includes:

1.  **Construction:** Building the tool specifically for the UK national curriculum.
2.  **Intervention:** Implementing four specific safety interventions (detailed in Technical Details).
3.  **Assessment:** Conducting ongoing evaluations within real-world educational scenarios to validate the safety measures.

## Contributions

*   **Replicable Safety Framework:** Provides a concrete, actionable framework for generative AI applications specifically designed for minors.
*   **Introduction of IACMA:** Presents the Independent Asynchronous Content Moderation Agent as a novel technical component for safety architectures.
*   **Open Blueprint:** Establishes a blueprint for industry-wide collaboration by open-sourcing code and datasets to improve general AI safety standards.

## Technical Implementation Details

The paper outlines a **multi-layered safety framework** for 'Aila,' combining technical barriers with human verification.

### **Layer 1: Dynamic Prompt Engineering**
*   Utilizes constraints based on pupil age, subject, and curriculum alignment.
*   Specifically designed to handle sensitive subjects (e.g., RSHE).

### **Layer 2: Input Threat Detection**
*   A pre-filtering system that scans user inputs before processing.
*   Designed to block prompt injection and jailbreaking attempts.

### **Layer 3: Independent Asynchronous Content Moderation Agent (IACMA)**
*   **Architecture:** A context-unaware AI agent operating independently of the generation stream.
*   **Classification:** Categorizes outputs into three distinct buckets:
    1.  **Safe**
    2.  **Content Guidance**
    3.  **Toxic**
*   **Standards:** Utilizes a hybrid of Oak's internal standards and the **HarmBench** framework.

### **Layer 4: Human-in-the-Loop**
*   Final verification layer to review edge cases and ensure curriculum alignment.

## Results & Analysis

The framework was rigorously tested against **45,000 lessons** post-launch. The analysis yielded several critical insights:

*   **User Steerage vs. Autonomy:** While expert validation and red-teaming showed initial alignment, real-world data revealed that lessons generated *without* specific user input were more curriculum-appropriate than those generated with heavy user steerage.
*   **Sequential vs. Batch Moderation Misalignment:** A key discovery where post-hoc batch evaluation of completed lessons often contradicted real-time (streaming) moderation. This was attributed to the evolving context of the lesson as it is generated.
*   **Identified Edge Cases:** Tests exposed specific failure modes regarding:
    *   **Temporal Knowledge Cutoffs:** E.g., the model lacking data on the 2024 US election.
    *   **Non-Curriculum Topics:** Handling topics like Special Educational Needs (SEN) that fall outside standard curriculum mapping.

---
*End of Report*