# FP=xINT: Representing Neural Networks via Low-Bit Series Basis Functions

*Boyang Zhang; Daning Cheng; Yunquan Zhang; Jiake Tian; Jing Li; Fangming Liu*

| **Quick Facts** | |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 26 |
| **Top Accuracy (ResNet-50)** | 77.03% (INT4) |
| **Training Requirement** | Calibration-Free |
| **Core Innovation** | Series Expansion Framework |

---

## Executive Summary

Neural network quantization is critical for deploying deep learning models on resource-constrained hardware, yet reducing precision to low-bit formats (e.g., 4-bit integers) typically imposes a significant accuracy penalty compared to full-precision (FP) models. While Post-Training Quantization (PTQ) offers a deployment-friendly alternative to quantization-aware training, existing state-of-the-art PTQ methods frequently rely on specific calibration sets or computationally intensive fine-tuning processes to recover accuracy. This dependency creates a practical bottleneck, as access to representative data is often restricted by privacy concerns or storage limitations, preventing the immediate deployment of highly compressed models.

This paper introduces **"FP=xINT,"** a novel framework that re-conceptualizes neural network quantization as a series expansion problem. Instead of directly rounding weights and activations, the method decomposes a full-precision model into a sum of multiple low-bit basis models using a deep model series expansion framework. The approach employs **Multi-Granularity Expansion** across tensor, layer, and model levels and utilizes **Algebraic Structuring** to define operations (AbelianAdd/Mul) within an Abelian group, ensuring efficient parallelism and commutativity. A differential expansion strategy is applied where activations undergo more expansions than weights, utilizing a Laplace clipping function to handle saturation and asymmetric quantization until the maximum differential falls below $10^{-4}$.

The proposed framework establishes a new state-of-the-art for low-bit quantization, specifically achieving **77.03%** top-1 accuracy on ResNet-50 with 4-bit (INT4) precision, a figure that actually surpasses the accuracy of the original full-precision baseline. Ablation studies validate the methodologyâ€™s design choices, demonstrating that expanding activations contributes significantly more to accuracy than expanding weights (75.12% vs. 72.87%), with peak performance achieved at 4 expansions. Furthermore, the method demonstrated strong generalization capabilities in Large Language Model (LLM) scenarios, performing effectively in W4A16 settings on the MMLU benchmark.

The significance of this work lies in its delivery of a theoretically grounded, **Calibration-Free PTQ** method that eliminates the need for calibration sets or fine-tuning, enabling "one-shot" deployment without data availability. By mathematically proving that the series expansion converges to the dense model, the authors provide the first application of series expansion techniques to neural network quantization. This breakthrough offers a practical path for high-fidelity model compression in data-scarce environments and sets a new performance standard for low-bit inference.

---

## Key Findings

*   **State-of-the-Art Performance:** The proposed deep model series expansion framework achieves SOTA performance in low-bit quantization settings.
*   **Accuracy Recovery:** For ResNet-50, the 4-bit quantization model not only maintains but **surpasses** the original full-precision accuracy, reaching **77.03%**.
*   **Theoretical Convergence:** The method theoretically confirms convergence to the dense model, effectively restoring Full-Precision (FP) model accuracy.
*   **Rapid Approximation:** The approach successfully approximates unquantized models rapidly without the need for calibration sets or fine-tuning.

---

## Methodology

The core methodology rests on a **Series Expansion Framework** designed to bridge the gap between high-precision and low-bit integer models.

*   **Decomposition Strategy:** The framework decomposes a Full-Precision (FP) model into a series expansion of multiple low-bit basis models.
*   **Multi-Granularity Expansion:** The process operates at three distinct levels:
    *   Tensor level
    *   Layer level
    *   Model level
*   **Algebraic Structuring:** The framework employs specific operations (**AbelianAdd/Mul**) to form an **Abelian group**. This mathematical structure is crucial as it ensures operation parallelism and commutativity during the reconstruction process.

---

## Technical Details

The implementation of the FP=xINT framework utilizes specific mathematical strategies to ensure precision and efficiency:

*   **Differential Expansion Strategy:**  
    *   Activations require a higher number of expansions than weights to maintain accuracy.
    *   The expansion process stops when the maximum difference between the original and quantized activations is less than **$10^{-4}$**.
*   **Saturation Handling:**  
    *   A **Laplace clipping function** is utilized to manage saturation issues.  
    *   Enables effective handling of asymmetric quantization.
*   **Reconstruction:** The network represents the Full-Precision model via a sum of low-bit series basis functions.
*   **Optimization:** The method converges to the original model architecture without requiring external calibration sets or fine-tuning steps.

---

## Results

The performance of FP=xINT was rigorously tested on computer vision architectures and Large Language Models (LLMs).

**ResNet-50 (INT4) Performance:**
*   **Full Method:** 77.03% ( surpassing the full-precision baseline)
*   **Only Activations Expanded:** 75.12%
*   **Only Weights Expanded:** 72.87%

**Optimization Insights:**
*   **Optimal Expansions:** Peak accuracy was achieved at **4 expansions**. 
*   **Diminishing Returns:** Performance saturation was observed after 5 expansions.

**Generalization:**
*   **LLM Benchmarks:** The method generalized effectively to Large Language Models in W4A16 settings (Weights 4-bit, Activations 16-bit) on the MMLU benchmark.

---

## Contributions

*   **Novel Application:** Introduces the first application of series expansion techniques to the field of neural network quantization.
*   **Calibration-Free PTQ:** Contributes a Post-Training Quantization (PTQ) method that is entirely calibration-free, removing the dependency on calibration sets and fine-tuning.
*   **Theoretical Foundation:** establishes a rigorous mathematical foundation by:
    *   Validating the convergence of series expansion to dense models.
    *   Defining an Abelian group structure to optimize for parallel processing.

---

**Report Score:** 8/10  
**References:** 26 Citations