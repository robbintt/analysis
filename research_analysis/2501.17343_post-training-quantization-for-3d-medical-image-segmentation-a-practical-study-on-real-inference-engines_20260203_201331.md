---
title: 'Post-Training Quantization for 3D Medical Image Segmentation: A Practical
  Study on Real Inference Engines'
arxiv_id: '2501.17343'
source_url: https://arxiv.org/abs/2501.17343
generated_at: '2026-02-03T20:13:31'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Post-Training Quantization for 3D Medical Image Segmentation: A Practical Study on Real Inference Engines

*Chongyu Qu; Ritchie Zhao; Ye Yu; Bin Liu; Tianyuan Yao; Junchao Zhu; Bennett A. Landman; Yucheng Tang; Yuankai Huo*

---

> ### ðŸ“Š Quick Facts
> **Metrics & Key Data**
>
> *   **Precision:** True 8-bit (INT8) Post-Training Quantization
> *   **Model Compression:** 2.42x â€“ 3.85x reduction in size
> *   **Inference Speedup:** 2.05x â€“ 2.66x faster latency
> *   **Memory Efficiency:** Up to 3.57x reduction in GPU memory usage
> *   **Accuracy:** Zero degradation in mean Dice Similarity Coefficient (mDSC)
> *   **Hardware:** NVIDIA TensorRT on RTX 4090

---

## Executive Summary

**The Problem**
3D medical image segmentation imposes significant computational burdens due to the high dimensionality of volumetric data and the complexity of state-of-the-art models. While model quantization is a widely recognized solution for reducing memory footprint and accelerating inference, existing research has predominantly relied on "fake quantization" simulations. These theoretical simulations fail to accurately reflect the performance characteristics of actual hardware, creating a "simulation-to-reality gap" that obscures the true latency and memory savings achievable in clinical environments.

**The Solution**
This study addresses the critical need for practical deployment strategies that realize the theoretical benefits of low-bit precision on physical inference engines. The key innovation is a **practical framework for deploying true 8-bit Post-Training Quantization (PTQ)** on modern NVIDIA GPUs using the TensorRT engine.

**The Approach**
Unlike prior simulation-based approaches, this method employs a two-step pipeline:
1.  Utilizing TensorRT for fake quantization and calibration on an unlabeled dataset to determine optimal scaling factors.
2.  Converting the model into a real quantized format for physical deployment.

The framework converts weights and computations to INT8 while retaining FP32 precision for input and output tensors, ensuring compatibility with standard clinical workflows. This approach is validated across a diverse spectrum of seven leading architectures, including both CNN-based (e.g., nnU-Net, U-Net) and Transformer-based (e.g., SwinUNETR, TransUNet) models.

**The Impact**
The deployment of true INT8 quantization yielded significant efficiency gains without compromising segmentation accuracy. Across all evaluated models and datasetsâ€”including BTCV, TotalSegmentator V2, and Whole Brain Segmentationâ€”the quantized models maintained the same mean Dice Similarity Coefficient (mDSC) as their FP32 counterparts, with zero observed degradation.

Practically, the framework achieved a model size reduction of **2.42x to 3.85x** and an inference latency speedup of **2.05x to 2.66x**. Furthermore, GPU memory consumption during inference was drastically reduced, decreasing by **3.57x for U-Net** and **3.37x for TransUNet** compared to standard FP32 PyTorch baselines.

This study represents the first practical exploration of deploying real 3D low-bit quantization on modern GPUs for complex medical segmentation tasks, successfully bridging the gap between theoretical simulation and clinical application.

---

## Key Findings

*   **True 8-bit Implementation:** The study demonstrates the successful implementation of true 8-bit post-training quantization (PTQ) on various state-of-the-art 3D medical segmentation models, moving beyond simulation to actual hardware deployment.
*   **Performance Preservation:** The framework achieves significant reductions in model size and inference latency on real GPUs without sacrificing segmentation performance.
*   **Broad Architectural Support:** The quantization approach was validated across a diverse range of **8 distinct architectures**, including:
    *   **CNN-based:** U-Net, nnU-Net
    *   **Transformer-based:** SwinUNETR, TransUNet
*   **Hardware Validation:** By leveraging TensorRT engines on modern GPUs, the study proves that real low-bit quantization effectively addresses computational constraints in large-scale medical imaging applications.

---

## Methodology

The proposed framework utilizes a two-step pipeline to transition from simulated to real quantization:

1.  **Fake Quantization & Calibration**
    *   TensorRT is employed to perform fake quantization for both weights and activations.
    *   Utilizes an unlabeled calibration dataset to determine optimal scaling factors.

2.  **Real Conversion & Deployment**
    *   The fake quantized model is converted into a real quantized format via the TensorRT engine.
    *   The model is deployed on physical GPUs to achieve actual reductions in memory footprint and processing speed.

---

## Contributions

*   **Bridging the Gap:** The study bridges the simulation-to-reality gap by addressing the limitation of prior research focused on 'fake quantization' with a practical framework that realizes actual model size reduction and inference acceleration.
*   **First Practical Exploration:** It provides the first practical exploration of deploying real 3D low-bit quantization on modern GPUs for a wide array of complex medical segmentation tasks.
*   **Open Source Resources:** The authors contribute pretrained models and code for major segmentation tasks (abdominal, whole brain, and full body) covering datasets like BTCV, Whole Brain Dataset, and TotalSegmentator V2, facilitating reproducibility and further research.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Approach** | True 8-bit Post-Training Quantization (PTQ) |
| **Deployment Engine** | NVIDIA TensorRT |
| **Weight Precision** | INT8 |
| **Computation Precision** | INT8 |
| **Input/Output Precision** | FP32 |
| **Validated Models** | U-Net, nnU-Net, SegResNet, VISTA3D, TransUNet, SwinUNETR, UNesT |
| **Datasets Used** | BTCV, TotalSegmentator V2, Whole Brain Segmentation |
| **Training Hardware** | Single NVIDIA RTX 4090 GPU |
| **Input Volume Size** | 96x96x96 |
| **Optimizer** | Adam |
| **Loss Function** | DiceCE Loss |

---

## Results

*   **Accuracy:** INT8 quantized models maintained the same mean Dice Similarity Coefficient (mDSC) as FP32 counterparts across all evaluated models and datasets, with **no observed degradation**.
*   **Model Size:** Reduced by a factor of **2.42x to 3.85x**.
*   **Inference Latency:** Achieved a speedup of **2.05x to 2.66x**.
*   **GPU Memory Usage:**
    *   **U-Net:** Reduced by **3.57x** compared to FP32 PyTorch baselines.
    *   **TransUNet:** Reduced by **3.37x** compared to FP32 PyTorch baselines.

---

**Paper Quality Score:** 8/10  
**References:** 40 citations