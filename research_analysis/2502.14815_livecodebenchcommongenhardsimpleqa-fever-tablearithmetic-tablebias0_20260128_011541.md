---
title: LiveCodeBenchCommonGenHardSimpleQA FEVER TableArithmetic TableBias0
arxiv_id: '2502.14815'
source_url: https://arxiv.org/abs/2502.14815
generated_at: '2026-01-28T01:15:41'
quality_score: 8
citation_count: 21
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LiveCodeBenchCommonGenHardSimpleQA FEVER TableArithmetic TableBias0

*Stanford University, Jared Quincy, Microsoft Research, Lingjiao Chen, Peter Bailis, Matei Zaharia, Optimizing Model, Boris Hanin, James Zou, Ion Stoica*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Improvement Range:** 5% â€“ 70% accuracy increase
> *   **Search Complexity:** Reduced from Exponential to Linear
> *   **Models Evaluated:** GPT-4o, Claude 3.5 Sonnet, Gemini 1.5
> *   **Core Mechanism:** LLM-as-a-judge for performance estimation
> *   **Citations:** 21

---

## Executive Summary

This research addresses the computational and architectural challenge of model selection within compound AI systemsâ€”architectures composed of multiple interconnected modules, such as multi-agent debates or self-refinement loops. While intuitively, different modules may require different large language models (LLMs) to maximize specific capabilities, the search space for optimal model allocation grows exponentially with the number of modules. Consequently, engineers often default to using a single, uniform LLM across the entire system, a strategy that is computationally efficient but frequently yields suboptimal quality. The paper tackles the difficulty of efficiently identifying the best combination of models for specific modules without resorting to exhaustive and prohibitively expensive end-to-end testing.

The authors introduce **LLMSelector**, a generalized framework designed to optimize static compound AI systems modeled as Directed Acyclic Graphs (DAGs). Technically, LLMSelector employs a greedy optimization algorithm that iteratively selects one module at a time, utilizing a "Module Nominator" and "Model Updater." A core innovation is the use of an **LLM-as-a-judge** to estimate the performance of candidate models on specific modules without executing the full system. This approach relies on the theoretical assumption of monotonicityâ€”specifically that end-to-end system performance improves as individual module performance improves. By leveraging this assumption and LLM-based estimation, LLMSelector reduces the search complexity from exponential to linear, making the optimization of complex systems tractable.

The framework was validated across three distinct architectures (*Self-refine, Multi-agent-debate,* and *Locate-solve*) and six diverse datasets, including LiveCodeBench, SimpleQA, and FEVER. When compared against baseline systems utilizing a single uniform model (including state-of-the-art models like GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5), LLMSelector consistently demonstrated superior performance. Empirical results showed accuracy improvements ranging from **5% to 70%** depending on the specific task and complexity. These metrics highlight that the framework not only successfully navigates the model selection space but does so by achieving substantial gains in reasoning accuracy and execution quality.

---

## Key Findings

*   **Impact of Module Choice:** The choice of LLM for specific modules significantly affects overall quality; however, identifying the optimal combination involves an exponentially large search space.
*   **System Monotonicity:** End-to-end system performance generally exhibits monotonic behavior regarding individual module performance, allowing for simplified optimization strategies.
*   **Efficient Estimation:** An LLM can accurately estimate the performance of individual modules without requiring end-to-end execution for every configuration.
*   **Performance Gains:** The proposed framework demonstrates empirical accuracy improvements ranging from **5% to 70%** compared to using a single identical LLM for all modules.

---

## Methodology

The authors propose **LLMSelector**, an optimized framework for model selection in compound AI systems. The approach is defined by the following characteristics:

1.  **Iterative Selection:** The framework iteratively selects one module at a time and allocates the model estimated to have the highest performance.
2.  **LLM-as-a-Judge:** To determine the best model without exhaustive testing, LLMSelector uses an LLM to estimate candidate model performance for specific modules.
3.  **Greedy Optimization:** This process reduces complexity, scaling linearly with the number of modules.
4.  **Validation:** The methodology was validated on compound systems like multi-agent debate and self-refine using state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5.

---

## Technical Details

LLMSelector is a framework designed to optimize static compound AI systems, modeled as Directed Acyclic Graphs (DAGs).

*   **Architecture:** Systems are represented as DAGs where different Large Language Models (LLMs) are allocated to different modules.
*   **Algorithm Components:**
    *   **Module Nominator:** Identifies which module to optimize next.
    *   **Model Updater:** Assigns the best model to the selected module.
*   **Performance Estimation:** Employs an "LLM-as-a-judge" mechanism to estimate module-wise performance efficiently.
*   **Mathematical Assumptions:**
    *   Binary performance outcomes.
    *   Monotonicity of end-to-end performance.
    *   Intra-monotonicity and inter-monotonicity to maintain tractability.

---

## Experimental Results

The evaluation of LLMSelector was comprehensive, covering multiple architectures and datasets:

*   **Architectures:** Self-refine, Multi-agent-debate, Locate-solve.
*   **Datasets (6):** LiveCodeBench, CommonGenHard, SimpleQA, FEVER, TableArithmetic, TableBias.
*   **Baselines:** Single identical model allocations (GPT-4o, Claude 3.5 Sonnet, etc.).
*   **Outcomes:** LLMSelector demonstrated accuracy improvements ranging from **5% to 70%** compared to baselines.
*   **Metric:** Accuracy (%) was the primary metric, where LLMSelector consistently outperformed single-model allocations.

---

## Core Contributions

1.  **Framework Introduction:** Introduced the **LLMSelector Framework**, a generalizable and efficient method for selecting optimal LLMs for specific modules within compound AI systems.
2.  **Theoretical Insights:** Identified theoretical and empirical insights, specifically the monotonic relationship between module-level and system-level performance and the feasibility of LLM-based module performance estimation.
3.  **Complexity Reduction:** Provided an optimization solution that reduces search time complexity from exponential to linear while maintaining high-quality allocation.
4.  **Empirical Validation:** Demonstrated substantial empirical gains over uniform LLM usage across high-stakes tasks involving complex reasoning and multi-agent interactions.

---

## Assessment

**Quality Score:** 8/10  
**References:** 21 citations