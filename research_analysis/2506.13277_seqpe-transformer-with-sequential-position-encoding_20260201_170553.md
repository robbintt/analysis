# SeqPE: Transformer with Sequential Position Encoding

*Huayang Li; Yahui Liu; Hongyu Sun; Deng Cai; Leyang Cui; Wei Bi; Peilin Zhao; Taro Watanabe*

---

> ### 80ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **References** | 40 Citations |
> | **WikiText-103 PPL (1024)** | 18.70 |
> | **WikiText-103 PPL (8192)** | 22.05 |
> | **MNIST Accuracy** | ~99.2% |

---

## Executive Summary

Current Transformer architectures face a critical dichotomy: standard learnable position embeddings lack the flexibility to extrapolate to sequence lengths longer than those seen during training, while functional encodings (like ALiBi or RoPE) offer better extrapolation but often require manual architectural adjustments for different data modalities (e.g., switching from 1D text to 2D images). This paper addresses the challenge of creating a unified position encoding framework capable of simultaneously handling variable modalities and generalizing effectively to out-of-distribution sequence lengths without compromising performance.

The authors introduce **SeqPE** (Sequential Position Encoding), a fully learnable framework that represents n-dimensional position indices as symbolic sequences rather than relying on fixed-size lookup tables. Technically, SeqPE employs a lightweight sequential position encoder to map position indices and data dimensions into hidden vectors end-to-end. To ensure stability during extrapolation, the method utilizes a novel dual-objective regularization strategy: a **contrastive objective** to align embedding distances semantically, and a **knowledge distillation loss** where the modelâ€™s own in-distribution behavior serves as the teacher to anchor embeddings for out-of-distribution positions.

In comparative evaluations, SeqPE demonstrated superior performance against strong baselines including ALiBi and RoPE. On the standard WikiText-103 benchmark trained with a context length of 1024 tokens, SeqPE achieved a validation perplexity of **18.70**, outperforming ALiBi ($18.86$). More significantly, when extrapolating to 8x the training length ($8192$ tokens), SeqPE maintained a perplexity of **22.05**, significantly lower than ALiBi's $23.42$. Additionally, in 2D image classification tasks, SeqPE achieved an accuracy of approximately **99.2%** on MNIST, validating its robustness across dimensional inputs without architectural redesign.

SeqPE represents a significant advancement in Transformer architecture by decoupling position encoding from specific modalities and fixed length constraints. Its ability to generalize across 1D and 2D data without manual re-engineering simplifies the deployment of large models across diverse domains such as natural language processing and computer vision.

---

## Key Findings

*   **Superior Extrapolation:** SeqPE outperforms strong baselines (including ALiBi and RoPE) in context length extrapolation tasks, achieving better perplexity, Exact Match (EM), and accuracy.
*   **Multi-dimensional Generalization:** The framework generalizes seamlessly to multi-dimensional inputs like 2D image classification without manual architectural redesign.
*   **Robust Performance:** Demonstrates consistent robustness across language modeling and long-context question answering compared to traditional methods.

---

## Methodology

SeqPE proposes a shift in how position indices are handled within Transformer architectures. The core methodological components include:

*   **Symbolic Sequential Representation:** Instead of using fixed-size lookup tables (Absolute Position Embeddings), SeqPE represents n-dimensional position indices as symbolic sequences.
*   **Lightweight Sequential Encoder:** A specific encoder module is employed to learn embeddings in an end-to-end manner, mapping inputs to hidden vectors.
*   **Dual-Objective Regularization:**
    *   **Contrastive Objective:** Aligns embedding distances to ensure semantic consistency.
    *   **Knowledge Distillation Loss:** Uses the model's in-distribution behavior as a teacher to anchor out-of-distribution position embeddings, preventing drift during extrapolation.

---

## Contributions

The research makes three primary contributions to the field of deep learning architecture:

1.  **Unified Framework:** Introduces SeqPE as a unified, fully learnable position encoding framework that overcomes the extrapolation limits of traditional methods.
2.  **Adaptability:** Solves the fundamental challenge of adaptability by enabling the same mechanism to handle varying modalities (text vs. images) and dimensions.
3.  **Novel Regularization:** Presents a new regularization technique combining contrastive learning and knowledge distillation to effectively manage out-of-distribution sequence lengths.

---

## Technical Details

### Definition & Properties
**SeqPE** (Sequential Position Encoding) is a learnable position encoding scheme designed to bridge the gap between the extrapolation capabilities of functional encodings and the flexibility of learnable embeddings.

### Input Mapping
The method maps an n-dimensional position into a hidden vector based on three inputs:
1.  Sequence Tokens
2.  Sequence Positions
3.  Data Dimensions

### Key Capabilities
*   **Learnability:** Fully learnable parameters.
*   **Extrapolation:** Ability to generalize to sequence lengths longer than those seen during training.
*   **Multi-dimensional Extensibility:** Capable of handling 2D inputs (e.g., images) without fundamental changes.

### Comparison with Baselines
The text contrasts SeqPE mathematically and functionally against:
*   **APE (Absolute Position Embeddings):** Limited by fixed lookup tables.
*   **ALiBi:** Bias-based distance decay methods.
*   **RoPE:** Rotary matrix methods.

---

## Results

SeqPE was evaluated against ALiBi and RoPE baselines on three primary tasks:

### Language Modeling (WikiText-103)
*   **Context Length 1024:** SeqPE achieved **18.70** PPL (vs ALiBi 18.86).
*   **Context Length 8192:** SeqPE achieved **22.05** PPL, significantly outperforming ALiBi's 23.42.

### Long-Context Question Answering
*   SeqPE achieved higher **Exact Match (EM)** scores than baselines, validating its ability to retain information over longer contexts.

### 2D Image Classification (MNIST)
*   Achieved approximately **99.2% accuracy**.
*   Validated SeqPE's ability to generalize effectively to longer sequence lengths and handle multi-dimensional inputs without manual architectural redesign.