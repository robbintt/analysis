# GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness

*Hongjie Chen; Zehan Li; Yaodong Song; Wenming Deng; Yitong Yao; Yuxin Zhang; Hang Lv; Xuechao Zhu; Jian Kang; Jie Lian; Jie Li; Chao Wang; Shuangyong Song; Yongxiang Li; Zhongjiang He; Xuelong Li*

---

### ‚ö° Quick Facts & Metrics

| Metric | Detail |
| :--- | :--- |
| **Base Model** | TeleChat2-7B |
| **Architecture** | Dual-Modality Head (Think, Write, Speak, Listen, Flow-Matching) |
| **Training Strategy** | 3-Stage Modular Training |
| **TELEVAL Score** | **68.31%** (vs Qwen2-Audio 48.14% & SALMONN 39.62%) |
| **Paralinguistic Scope** | 7 Emotions, 4 Vocalizations, 3 Age Groups, 6 Dialects |
| **Compute** | ~10,660 GPU Hours (NVIDIA A800-80G) |
| **Quality Score** | 9/10 |

---

> **‚ö†Ô∏è Executive Summary**
>
> Contemporary Spoken Language Models (SLMs) face a fundamental limitation in achieving human-like social awareness, as they typically prioritize textual semantics while neglecting critical paralinguistic and speaker-specific characteristics. Current systems often treat speech simply as an acoustic vehicle for text, failing to model essential nuances such as emotion, dialect, age, and non-speech vocalizations like laughter or coughing. This deficiency prevents AI from engaging in naturally expressive and adaptive communication.
>
> This paper introduces **GOAT-SLM**, a novel architecture built upon the TeleChat2-7B foundation, featuring a "dual-modality head" designed to decouple linguistic modeling from acoustic realization. The system employs a modular structure comprising five components to simultaneously track semantic content and acoustic style. It is trained using a computationally efficient, three-stage strategy to capture 7 distinct emotions, 4 non-speech vocalizations, 3 age groups, and 6 dialect types.
>
> GOAT-SLM achieved superior performance on the TELEVAL benchmark (68.31%), significantly outperforming open-source baselines. The results demonstrate a balanced capability across modalities, maintaining semantic reasoning performance on par with text-only baselines while realizing significant gains in paralinguistic domains.

---

## üîë Key Findings

*   **Superior Performance on Non-Semantic Tasks:** GOAT-SLM outperforms existing open-source models in handling complex vocal attributes, specifically excelling in tasks involving emotion, dialectal variation, and age-sensitive interactions.
*   **Balanced Semantic and Non-Semantic Capabilities:** The model achieves well-balanced performance across both semantic (content-based) and non-semantic (paralinguistic) tasks on the TELEVAL benchmark.
*   **Effective Decoupling of Modalities:** The dual-modality head architecture successfully decouples linguistic modeling from acoustic realization, allowing for robust language understanding while simultaneously supporting expressive and adaptive speech generation.
*   **Importance of Paralinguistic Awareness:** The study validates the significance of modeling speech characteristics beyond textual semantics, demonstrating that incorporating dialect, emotion, and non-speech vocalizations leads to more socially aware AI systems.

---

## üìö Research Contributions

1.  **Introduction of GOAT-SLM:** A novel spoken language model (SLM) that extends capabilities beyond text semantics to include deep awareness of paralinguistic and speaker characteristics (e.g., dialect, age, emotion, non-speech vocalizations).
2.  **Architectural Innovation:** The proposal of a dual-modality head that enables the simultaneous tracking of semantic content and acoustic style, addressing the limitation in current SLMs that treat speech merely as a vehicle for text.
3.  **Advancement in Socially Aware AI:** The work pushes the development of spoken language systems toward being more natural, adaptive, and socially aware by effectively integrating rich human speech cues often ignored by traditional end-to-end models.

---

## üõ†Ô∏è Methodology

The authors employ a **dual-modality head architecture** designed to decouple linguistic modeling from acoustic realization. This separation allows the model to process text semantics independently from the nuances of sound production.

The training approach utilizes a **modular, staged strategy** to enhance efficiency and versatility. This process progressively aligns:
*   Linguistic information
*   Paralinguistic information
*   Speaker characteristic information

This alignment is achieved using large-scale speech-text corpora to ensure the model handles both content and style effectively.

---

## ‚öôÔ∏è Technical Details

### System Architecture
The model utilizes a modality-alignment framework based on **TeleChat2-7B**, comprising five distinct modules:

*   **Think:** Shared semantic core using the bottom 15 layers.
*   **Write:** Text generation head using the top 15 layers.
*   **Speak:** Speech generation head initialized from Write parameters.
*   **Listen:** Whisper-small encoder with a CNN-Transformer projector.
*   **Flow-Matching:** Speech decoder.

### Paralinguistic Dimensions
The system processes a wide array of speech attributes:
*   **7 Emotions**
*   **4 Non-speech vocalizations**
*   **3 Age groups**
*   **6 Language/dialect types**

### Training Pipeline
A three-stage strategy was employed:
1.  **Stage 1 - Instruction Tuning:** 115k multi-turn dialogues for attribute awareness.
2.  **Stage 2 - Speech-Text Alignment:** 126M samples for module alignment.
3.  **Stage 3 - Expressive Speech Generation:** 61.7M samples for decoder training (utilizing synthetic data from GOAT-TTS).

### Compute Configuration
*   **Hardware:** NVIDIA A800-80G GPUs (24-48 cluster)
*   **Duration:** ~10,660 GPU hours
*   **Optimization:** Adam/AdamW optimizers
*   **Hyperparameters:** Batch sizes up to 1152; Learning rates ranging from 1e-5 to 6e-5.

---

## üìä Results

GOAT-SLM achieved balanced performance on the TELEVAL benchmark, outperforming open-source models in tasks requiring complex vocal attribute handling.

*   **Benchmark Dominance:** Achieved a weighted accuracy of **68.31%** on TELEVAL, significantly surpassing *Qwen2-Audio* (48.14%) and *SALMONN* (39.62%).
*   **Specific Strengths:** The model excels in emotion recognition/synthesis, dialectal variation handling, age-sensitive interactions, and non-speech vocalization modeling (e.g., responding to coughs or laughter).
*   **Architectural Validation:** Confirmed the success of the dual-modality head in decoupling linguistic modeling from acoustic realization, preserving the LLM's core reasoning capabilities while maintaining high-fidelity speech synthesis.
*   **Data Efficiency:** The training effectively utilized synthetic data generated by GOAT-TTS to enhance performance.

---

**References:** 29 citations  
**Quality Score:** 9/10