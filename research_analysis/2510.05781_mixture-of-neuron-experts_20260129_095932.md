# Mixture of Neuron Experts

*Runxi Cheng; Yuchen Guan; Yucheng Ding; Qingguo Hu; Yongxian Wei; Chun Yuan; Yelong Shen; Weizhu Chen; Yeyun Gong*

---

## üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 40 |
| **Redundancy Identified** | Up to 60% of parameters |
| **MoNE Activation** | Only 50% of parameters required |
| **Core Innovation** | Neuron-level sparsity (Top-k selection) |

---

## üìù Executive Summary

This research addresses the fundamental inefficiency inherent in current Mixture-of-Experts (MoE) architectures utilized in large language models (LLMs). While traditional MoEs improve inference efficiency by routing inputs to a subset of expert networks, the authors demonstrate that this approach remains computationally wasteful because it treats experts as indivisible units. The problem lies in the lack of fine-grained sparsity; once an expert is activated, all its parameters typically engage in computation despite analysis showing that a vast majority of neuron activations are near zero. This redundancy results in significant unnecessary memory bandwidth and computation costs during inference.

To empirically establish this redundancy, the authors conducted a rigorous sparsification analysis involving neuron-granular decomposition to visualize activation values and progressive pruning experiments where parameters were ranked by activation magnitude. This diagnostic process confirmed that up to 60% of parameters in traditional MoE layers are redundant. Building on this insight, the key innovation is the introduction of the **Mixture of Neuron Experts (MoNE)** framework, which shifts the granularity of conditional computation from the expert level to the neuron level. MoNE achieves this by employing a gate projection within the forward pass to calculate activation weights for individual neurons, ranking them by importance and computing only the top-k neurons per expert. This architecture operates with negligible latency overhead and requires no additional routing parameters or inter-expert communication, effectively silencing inactive neurons while maintaining the overall MoE structure.

The findings demonstrate that parameter efficiency requires neuron-level optimization rather than expert-level routing alone. Sparsification studies on models such as Qwen3, Mixtral-8x7B, and DeepSeek-V2-Lite revealed that up to 60% of parameters can be pruned with negligible performance degradation, with substantial accuracy drops occurring only after removing more than 90% of parameters. The proposed MoNE model matched the performance of traditional MoE architectures while activating only 50% of the parameters. Furthermore, when compared on an equal number of activated parameters, MoNE consistently outperformed traditional MoE models across STEM, Humanities, and Social Science benchmarks.

By demonstrating that performance can be maintained or improved with significantly fewer activated parameters, MoNE offers a practical path to reducing inference costs for deploying massive scale models. This work challenges the conventional understanding of sparsity in MoE systems, providing a diagnostic tool for analyzing parameter utilization and a viable framework for neuron-level computation.

---

## üîë Key Findings

*   **High Redundancy in Activated Parameters:** Analysis reveals that up to **60%** of parameters can be pruned with negligible performance degradation; substantial drops occur only after removing more than 90%.
*   **Neuron-Level Sparsity:** Most neuron activations are near zero, indicating that traditional MoE layers lack sufficient fine-grained sparsity.
*   **Significant Efficiency Gains:** The Mixture of Neuron Experts (MoNE) matches traditional MoE performance while activating only **50%** of parameters.
*   **Superior Parameter Efficiency:** MoNE consistently outperforms traditional MoE architectures when compared based on an equal number of activated parameters.

---

## ‚öôÔ∏è Technical Details

The paper proposes **Mixture of Neuron Experts (MoNE)**, shifting from traditional Mixture-of-Experts (MoE) architectures by operating at a fine-grained **'Neuron-level'** rather than **'Expert-level.'**

*   **Core Premise:** Significant redundancy exists within activated experts, where most neuron activations are near zero.
*   **Mechanism:**
    *   Uses **gate projection** to calculate activation weights.
    *   Ranks neuron importance to enable conditional computation.
    *   Prunes or silences individual neurons instead of whole experts.
*   **Implementation:** Designed to be lightweight with negligible latency and **no** additional routing parameters or inter-expert communication.

---

## üß™ Methodology

The research team employed a multi-step approach to analyze and improve MoE efficiency:

1.  **Sparsification Analysis:** Representative MoE models were analyzed where parameters were ranked by activation magnitude and progressively pruned.
2.  **Neuron-Granular Decomposition:** Researchers visualized activation values to understand fine-grained sparsity.
3.  **MoNE Implementation:** Introduced top-k selection within each expert during pretraining.
4.  **Evaluation:** The proposed framework was designed to ensure negligible latency overhead and zero inter-expert communication requirements.

---

## üìà Results

**Sparsification Study (Models: Qwen3-30B, Mixtral-8x7B, DeepSeek-V2-Lite):**
*   Up to **60%** of parameters found redundant (prunable with negligible degradation).
*   Substantial accuracy drops only occurred after removing **>90%** of parameters.
*   **Activation Thresholds for Top 50% Neurons (Qwen3):**
    *   Layer 0: `0.05176`
    *   Layer 47: `0.2422`

**MoNE Performance:**
*   Matches traditional MoE performance while activating only **50%** of parameters.
*   Demonstrated superior performance against traditional MoE on equal activated parameters across:
    *   STEM
    *   Humanities
    *   Social Sciences
    *   Others

---

## üèÜ Contributions

*   **Diagnostic Insight:** Provided critical analysis into parameter activation sparsity, demonstrating redundancy and underutilization in standard MoE layers.
*   **Framework Introduction:** Introduced the **Mixture of Neuron Experts (MoNE)** framework, successfully shifting sparsity control from the expert level to the neuron level.
*   **Practical Efficiency:** Established MoNE as a practical inference solution that improves parameter utilization without compromising accuracy or introducing system overhead.
