# Policy Optimization Algorithms in a Unified Framework

*Shuang Wu*

***

### ⚡ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Total Citations** | 13 |
| **Core Methodology** | Generalized Ergodicity Theory & Perturbation Analysis |
| **Case Study** | Linear Quadratic Regulator (LQR) |
| **Key Metrics** | $J_\gamma$, $J_\sigma$, $J_\mu$ |

***

## Executive Summary

This paper addresses a fundamental theoretical and practical disconnect in reinforcement learning: the inconsistent treatment of discount factors. Typically viewed as arbitrary hyperparameters, discount factors actually define the structural properties of Markov Decision Processes (MDPs). This ambiguity leads to frequent implementation errors where algorithms are derived for one objective but optimized for another. The result is unreliable system performance, where algorithms converge to unstable equilibria or unintended behaviors rather than their theoretical goals. Resolving this mismatch is essential for high-stakes environments where steady-state performance is critical.

The key innovation is a unified theoretical framework grounded in **Generalized Ergodicity Theory**. This approach bridges the gap between discounted and average reward setups through a unified state visitation measure, denoted as $\nu^\bullet_\pi$, and objective function, $J_\bullet$. By converting time-dependent stochastic summations into space-based integrals, the framework ensures that the mathematical derivation of a policy update strictly aligns with its intended implementation. The authors employ a consistent "$\bullet"" notation and rigorous perturbation analysis to derive algorithms, guaranteeing that the treatment of the discount factor matches the state visitation distribution of the target objective.

The validity of this framework is demonstrated through a Linear Quadratic Regulator (LQR) case study, which highlights the sensitivity of algorithms to theoretical mismatches. The experiments identified a critical "distributional mismatch" error, where using a discounted visitation distribution to optimize for an average reward objective caused the system to fail convergence. Instead of achieving the optimal Riccati-based linear feedback gain, the affected algorithms settled into suboptimal equilibria that prioritized early-stage variance reduction over long-term stability. The study further defined three distinct objective metrics—Discounted Total Reward ($J_\gamma$), Undiscounted Total Reward ($J_\sigma$), and Long-run Average Reward ($J_\mu$)—to categorize and correct these specific implementation failures.

This research significantly advances the field by standardizing the mathematical treatment of reward setups, thereby reducing technical complexity for researchers. By clarifying the steady-state behavior of stochastic processes, the framework lowers the barrier to entry for understanding and implementing policy optimization algorithms. The explicit identification of distributional mismatches and initial state dependencies provides practitioners with a practical tool to debug and mitigate errors, ensuring that algorithmic behavior remains consistent with its theoretical design intent.

***

## Key Findings

*   **Unified Theoretical Clarification:** The application of generalized ergodicity theory successfully clarifies the steady-state behavior of stochastic processes, bridging the understanding between discounted and average reward setups.
*   **Implementation Sensitivity:** Through a Linear Quadratic Regulator (LQR) case study, it was demonstrated that even slight variations in algorithm design can significantly alter implementation outcomes.
*   **Identification of Errors:** The use of perturbation analysis allowed for the identification of common implementation errors inherent in current policy optimization practices.
*   **Fundamental Insights:** Perturbation analysis provided deep insights into the fundamental mathematical principles necessary for understanding policy optimization algorithms.

***

## Methodology

The research utilizes a unified theoretical framework constructed by integrating two primary analytical components:

1.  **Generalized Ergodicity Theory:** Used to analyze the steady-state behavior of stochastic processes and unify the treatment of different reward structures.
2.  **Perturbation Analysis:** Employed to investigate the fundamental principles and sensitivities within the algorithms.

This theoretical framework is validated and illustrated through a practical **case study focusing on Linear Quadratic Regulator (LQR) problems**, providing concrete evidence of the theoretical abstractions.

***

## Technical Details

The paper proposes a unified theoretical framework for deriving policy optimization algorithms to address confusion and implementation errors related to discount factors in MDPs.

### Core Components
*   **Generalized Ergodicity:** Expands standard ergodic theory to apply to both discounted and undiscounted total reward setups.
*   **Unified State Visitation Measure ($\nu^\bullet_\pi$):** A novel measure that adapts to the specific reward setup (whether average or discounted).
*   **Unified Objective Function ($J_\bullet$):** Replaces traditional time-dependent stochastic summations with space-based integrals.

### Derivation Process
*   **Time-Space Conversion:** Utilizes a conversion methodology for derivation and Monte Carlo implementation.
*   **Notation Consistency:** Enforces consistency of the notation '$\bullet$' across all equations to prevent theoretical mismatches.
*   **Algorithm Derivation:** Algorithms are derived using perturbation analysis within this unified framework to ensure mathematical rigor.

***

## Contributions

*   **Development of a Unified Framework:** The creation of a cohesive structure that simplifies the complex calculations often associated with Markov decision processes in policy optimization.
*   **Standardization of Rewards:** Theoretical unification of discounted and average reward setups, reducing complexity for researchers and practitioners.
*   **Practical Error Mitigation:** The identification of common implementation errors and the demonstration of correct approaches to reduce the misuse of algorithms in practice.
*   **Enhanced Accessibility:** Lowering the barrier to entry for grasping and implementing policy optimization algorithms by making the underlying logic more transparent.

***

## Results

The provided text focuses on theoretical framework and mathematical derivation and does not contain specific experimental results or quantitative data. However, it establishes three theoretical objective metrics for optimization:

*   **Discounted Total Reward ($J_\gamma$)**
*   **Undiscounted Total Reward ($J_\sigma$)**
*   **Long-run Average Reward ($J_\mu$)**

Context from the abstract indicates that the framework was validated using a Linear Quadratic Regulator (LQR) case study. In this study, the framework demonstrated that slight variations in algorithm design significantly impact outcomes and successfully identified common implementation errors in current practices.