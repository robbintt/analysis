# Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game

*Barna Pásztor; Thomas Kleine Buening; Andreas Krause*

***

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **Total References:** 40 Citations
> *   **Parameter Scale:** 0.5B – 8B Parameters
> *   **Approach:** Stackelberg Sequential Game (Leader $\to$ Follower)
> *   **Core Advantage:** Handles intransitive preferences deterministically
> *   **Refinement Performance:** >70% win rate for Follower over Leader

***

## Executive Summary

Current state-of-the-art alignment methods, such as Reinforcement Learning from Human Feedback (RLHF) and Nash Learning from Human Feedback (NLHF), rely on the assumption that human preferences are consistent and transitive, representable by a single scalar reward function. In reality, human feedback is often noisy and contains intransitive cycles (e.g., the Condorcet Paradox), which mathematical frameworks like NLHF address by converging to mixed strategies. While theoretically sound, this practically results in the model outputting random, stochastic responses—a critical failure mode for deterministic language generation where coherence is required.

This paper addresses the need for a preference optimization framework that maintains determinism and robustness even when faced with cyclical or intransitive human preferences. The authors introduce **Stackelberg Learning from Human Feedback (SLHF)**, a novel game-theoretic framework that reframes alignment as a sequential-move game rather than a simultaneous one.

By leveraging the asymmetry of sequential play, the approach utilizes a **Leader-Follower** dynamic where the Leader proposes an initial action, and the Follower observes this action to generate a refined response that maximizes preference. Technically, the method employs Stackelberg Gradient Descent Ascent (GDA) to solve a max-min objective, ensuring the Leader optimizes against the Follower's best response rather than a static distribution.

Empirically, SLHF demonstrates superior robustness compared to NLHF by successfully avoiding randomization in cyclical preference scenarios. The framework validated its scalability by effectively aligning models ranging from 0.5 to 8 billion parameters. Crucially, head-to-head evaluations provided specific quantitative validation of the inference-time refinement capabilities: the Follower component achieved win rates exceeding 70% against the Leader's baseline responses. This work establishes a new foundation for game-theoretic preference optimization that is both theoretically robust and practically scalable.

***

## Key Findings

*   **Theoretical Robustness:** Demonstrates superior consistency over RLHF and NLHF, specifically regarding the handling of intransitive preferences and the Condorcet Paradox.
*   **Scalable Alignment:** Successfully validated across datasets ranging from **0.5 to 8 billion parameters**.
*   **Transferable Refinement:** Supports inference-time refinement where the Follower improves the Leader's actions without the need for retraining.
*   **Sequential Asymmetry:** Leverages the asymmetry of sequential play to model richer preference structures that simultaneous games cannot capture.
*   **Deterministic Solutions:** Unlike NLHF, which collapses into stochastic mixed strategies during cyclical preferences, SLHF maintains deterministic outputs.

***

## Methodology

The authors introduce **Stackelberg Learning from Human Feedback (SLHF)**, modeling alignment as a sequential-move game between two policies:

1.  **Leader Policy:** Responsible for the initial action generation.
2.  **Follower Policy:** Responsible for the conditional response (refinement).

This approach decomposes preference optimization into two distinct problems:
*   A **refinement problem** for the Follower (improving the action).
*   An **adversarial optimization problem** for the Leader (anticipating the refinement).

***

## Technical Details

The SLHF framework is built upon a rigorous mathematical foundation designed to solve for equilibrium in sequential environments.

### **1. Game Framework**
*   **Structure:** Sequential-move game.
*   **Agents:**
    *   **Leader ($\pi_L$):** Selects action $y$ given context $x$.
    *   **Follower ($\pi_F$):** Observes context $x$ and Leader's action $y$, then selects response $y'$.
*   **Objective:** Achieve a unique Stackelberg Equilibrium via a max-min objective.

### **2. Optimization Objective & Regularization**
*   **Objective Function:** Max-min formulation incorporating separate KL divergence regularization terms to ensure stability.
    *   $\beta_L$: KL regularization for the Leader.
    *   $\beta_F$: KL regularization for the Follower.

### **3. Algorithm: Stackelberg GDA**
*   **Procedure:**
    1.  **Gradient Ascent:** Performed on the Leader parameters.
    2.  **Gradient Descent:** Performed on the Follower parameters.
    3.  **Projection:** Applied to maintain constraints.
*   This alternating optimization ensures the Leader optimizes against the Follower's best response function.

### **4. Implementation Strategy**
*   **Architecture:** Implemented as a **single model** trained for both roles.
*   **Prompting Strategies:**
    *   **Leader:** Receives a standard prompt.
    *   **Follower:** Receives the standard prompt + Leader's response + an explicit **"Improve"** instruction.

***

## Results

### **Theoretical Validation**
*   **Handling Intransitivity:** The framework successfully manages intransitive preferences (e.g., Rock-Paper-Scissors scenarios) without relying on a single scalar reward function.
*   **Comparison to NLHF:** In cyclical preference scenarios, NLHF fails by producing stochastic mixed strategies (e.g., uniform random outputs). SLHF succeeds by converging to **deterministic outputs**.

### **Empirical Validation**
*   **Scalability:** Experiments successfully scaled alignment across models ranging from **0.5B to 8B parameters**.
*   **Inference-Time Refinement:** Validated the capability of transferable refinement, where the Follower improves outputs at test time.
*   **Quantitative Performance:** In head-to-head evaluations, the Follower achieved **win rates exceeding 70%** against the Leader's baseline responses, demonstrating significant improvement without retraining.

***

## Contributions

*   **Novel Framework:** Establishment of SLHF as a new game-theoretic paradigm, shifting preference optimization from simultaneous to sequential interactions.
*   **Theoretical Analysis:** Formal analysis comparing solution concepts (Stackelberg Equilibrium vs. Nash Equilibrium) with RLHF and NLHF.
*   **Inference-Time Iteration:** Introduction of inference-time iterative refinement capabilities, allowing for dynamic self-correction at generation time.
*   **Empirical Proof:** Comprehensive validation on Large Language Models (LLMs) demonstrating practical scalability and transferable refinements across different architectures.