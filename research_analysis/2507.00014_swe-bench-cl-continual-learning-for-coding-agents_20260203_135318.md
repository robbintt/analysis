---
title: 'SWE-Bench-CL: Continual Learning for Coding Agents'
arxiv_id: '2507.00014'
source_url: https://arxiv.org/abs/2507.00014
generated_at: '2026-02-03T13:53:18'
quality_score: 8
citation_count: 10
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# SWE-Bench-CL: Continual Learning for Coding Agents

*Thomas Joshi; Shayan Chowdhury; Fatih Uysal*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Dataset Size** | 8 sequences, 273 tasks |
| **Base Dataset** | SWE-Bench Verified |
| **Key Architecture** | LangGraph + FAISS (RAG) |
| **Core Metric** | Composite Continual Learning Score |
| **Citations** | 10 |

---

## üìù Executive Summary

Current static code-generation benchmarks fail to capture the continuous, evolutionary nature of real-world software development, treating tasks as isolated events rather than a cumulative process. This paper addresses the critical challenge of "**catastrophic forgetting**" in coding agents‚Äîthe tendency of AI models to lose previously acquired knowledge when learning new tasks.

Without benchmarks that simulate the chronological accumulation of experience, it is impossible to accurately evaluate if an agent can sustain performance over time or effectively balance the stability-plasticity trade-off required for long-term software maintenance.

The core innovation is the introduction of **SWE-Bench-CL**, a novel benchmark that reorganizes the SWE-Bench Verified dataset into chronologically ordered sequences to mimic natural repository evolution. Technically, the authors implemented an interactive, LangGraph-based agent framework augmented with a FAISS-backed semantic memory module, utilizing Retrieval-Augmented Generation (RAG) to assist in task resolution.

To properly evaluate this setup, the study establishes a specialized metrics suite, introducing the **Composite Continual Learning Score** and **CL-F-beta score**, which move beyond simple accuracy to measure forward/backward transfer, tool-use efficiency, and resistance to forgetting.

The benchmark comprises 8 sequences containing 273 tasks, with structural analysis confirming low redundancy (average Jaccard Similarity: 0.1114; average Cosine Similarity: 0.1792). A critical experimental finding involved prompt poisoning tests, which revealed an average semantic drift of 0.45 (where >0.3 is considered high). This indicates that naive memory retrieval poses a significant risk of performance degradation, highlighting the sensitivity of agents to irrelevant context within the memory module.

This research establishes the first standardized protocol for assessing continual learning in software engineering, providing a robust baseline for comparing memory-augmented against non-memory-augmented agents. By releasing the dataset, evaluation framework, and metrics as open-source, the authors provide the community with the tools necessary to transition from evaluating one-shot problem solving to testing sustained, long-term code generation capabilities.

---

## üîë Key Findings

*   **Benchmark Limitations:** Current static code-generation benchmarks fail to capture the continuous, evolving nature of real-world software development.
*   **Chronological Importance:** Organizing GitHub issues into chronologically ordered sequences is essential for evaluating an agent's ability to accumulate experience and resist catastrophic forgetting.
*   **New Metrics Required:** Evaluating coding agents requires specialized metrics (such as the Composite Continual Learning Score) to measure the stability-plasticity trade-off.
*   **Memory Mitigation:** Integrating semantic memory modules (backed by FAISS) within agent frameworks (LangGraph) provides a mechanism to mitigate forgetting and improve performance.

---

## üß™ Methodology

The researchers constructed **SWE-Bench-CL**, a novel continual learning benchmark derived from the human-verified SWE-Bench Verified dataset.

*   **Data Organization:** The methodology involves organizing GitHub issues into chronologically ordered sequences to mimic natural repository evolution.
*   **Framework Implementation:** They implemented an interactive, **LangGraph-based framework** augmented with a **FAISS-backed semantic memory module**.
*   **Experimental Protocol:** The protocol compares 'memory-enabled' agents against 'memory-disabled' agents across diverse Python repositories.
*   **Evaluation Metrics:** Utilizing metrics including average accuracy, forgetting measures, forward/backward transfer, tool-use efficiency, and the CL-F-beta score.

---

## ‚öôÔ∏è Technical Details

**Dataset Construction**
*   **Foundation:** Built upon SWE-Bench Verified, reformulating unordered static tasks into chronologically ordered sequences.
*   **Simulation:** Designed to simulate continuous developer engagement.
*   **Filtering:** Includes repository filtering (minimum 15 tasks).
*   **Curriculum Strategy:** Ordering tasks by timestamp and difficulty levels:
    *   Easy (<15 min)
    *   Medium (15m‚Äì1hr)
    *   Hard (1‚Äì4hr)
    *   Very Hard (>4hr)
*   **Dependency Awareness:** Implemented via modified file paths.

**Agent Architecture**
*   **Orchestration:** Utilizes LangGraph.
*   **Memory:** Semantic Memory Module backed by FAISS for Retrieval-Augmented Generation (RAG).

**Evaluation Protocol**
*   **Core Metric:** Composite Continual Learning Score.

---

## üìà Results

*   **Benchmark Composition:** The benchmark comprises **8 sequences** containing **273 tasks**.
*   **Dependency Distribution:** Varies significantly across repositories:
    *   **13%** (scikit-learn)
    *   **59%** (xarray)
*   **Structural Similarity:** Shows low redundancy, indicating diverse tasks:
    *   Average Jaccard Similarity: **0.1114**
    *   Average Cosine Similarity: **0.1792**
*   **Prompt Poisoning Risks:** Experiments revealed an average semantic drift of **0.45** (where >0.3 is high), indicating that naive memory retrieval poses a significant risk of performance degradation due to sensitivity to irrelevant context.

---

## üöÄ Contributions

1.  **The SWE-Bench-CL Dataset:** A novel publicly available benchmark for assessing continual learning in software engineering.
2.  **Specialized Metrics Suite:** Introducing new evaluation metrics like the Composite Continual Learning Score and CL-F-beta score.
3.  **Open-Source Framework:** An advanced, LangGraph-based interactive evaluation framework that integrates semantic memory for reproducible testing.
4.  **Standardized Protocol:** A standardized experimental protocol for comparing memory-augmented and non-memory-augmented agents, providing a baseline for future research.

---

**Quality Score:** 8/10 | **References:** 10 citations