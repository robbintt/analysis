# DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment

*Ke-Han Lu; Zhehuai Chen; Szu-Wei Fu; Chao-Han Huck Yang; Sung-Feng Huang; Chih-Kai Yang; Chee-En Yu; Chun-Wei Chen; Wei-Chih Chen; Chien-yu Huang; Yi-Cheng Lin; Yu-Xiang Lin; Chi-An Fu; Chun-Yi Kuan; Wenze Ren; Xuanjun Chen; Wei-Ping Huang; En-Pei Hu; Tzu-Quan Lin; Yuan-Kuei Wu; Kuan-Po Huang; Hsiao-Ying Huang; Huang-Cheng Chou; Kai-Wei Chang; Cheng-Han Chiang; Boris Ginsburg; Yu-Chiang Frank Wang; Hung-yi Lee*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score**: 9/10
> *   **Dataset Scale**: 5 Million samples (DeSTA-AQA5M)
> *   **Audio Coverage**: 7,000 hours across 50 datasets
> *   **Key Benchmark Win**: Dynamic-SUPERB Phase-1 (69.53 vs 51.69)
> *   **Core Innovation**: Self-Generated Cross-Modal Alignment (DeSTA)
> *   **Citations**: 40

---

## Executive Summary

Training Large Audio Language Models (LALMs) to understand and process audio signals presents a critical dilemma: acquiring robust auditory capabilities often comes at the cost of the model's native language proficiency, a phenomenon known as **catastrophic forgetting**. Existing methods typically rely on task-specific instruction tuning, which limits a model's ability to generalize to new tasks without explicit retraining.

This paper addresses the challenge of creating a general-purpose LALM that can seamlessly integrate audio understanding with advanced reasoning and instruction-following abilities, without compromising the text-based skills inherent in the underlying Large Language Model (LLM).

The authors introduce **"DeSTA"** (Self-generated Cross-Modal Alignment), a novel framework that eliminates the need for external human annotators by having the backbone LLM generate its own training targets derived from audio metadata and prompts. This self-supervised approach was used to construct **DeSTA-AQA5M**, a massive, task-agnostic dataset comprising 5 million samples and 7,000 hours of audio across 50 diverse datasets (speech, environmental sounds, and music).

DeSTA2.5-Audio achieves **state-of-the-art** performance across multiple benchmarks. Notably, it significantly outperforms competitors like Qwen2-Audio-Instruct in Dynamic-SUPERB and demonstrates exceptional instruction-following adherence. This research validates the critical importance of data construction pipelines in developing robust LALMs and provides a scalable pathway to bridge the gap between audio perception and high-level cognitive reasoning.

---

## Key Findings

*   **State-of-the-Art Performance**: Achieves top-tier results on Dynamic-SUPERB, MMAU, SAKURA, Speech-IFEval, and VoiceBench.
*   **Mitigation of Catastrophic Forgetting**: Successfully retains the LLM's native language proficiency while aligning audio and text modalities.
*   **Zero-Shot Generalization**: Enables effective generalization to new tasks without requiring strict task-specific audio instruction-tuning.
*   **Superior Capabilities**: Outperforms existing methods specifically in auditory perception and instruction-following tasks.

---

## Methodology

The core proposed method is **DeSTA** (Self-generated cross-modal alignment).

*   **Self-Generated Targets**: Unlike traditional methods that rely on external annotators, the backbone LLM generates its own training targets using audio metadata and specific prompts.
*   **Dataset Construction**: This approach resulted in **DeSTA-AQA5M**, a large-scale, task-agnostic dataset containing 5 million samples.
*   **Data Diversity**: The training data covers 7,000 hours of audio spanning 50 diverse datasets, including speech, environmental sounds, and music.
*   **Training Objective**: The model is trained to act as a general-purpose LALM without relying on task-specific tuning during the alignment phase.

---

## Technical Specifications

### Architecture & Pipeline
*   **Framework**: Self-Generated Cross-Modal Alignment (DeSTA).
*   **Components**:
    *   **Audio Encoder**: Processes raw audio signals.
    *   **Modality Adapter**: Maps audio representations to the LLM's embedding space.
    *   **Backbone LLM**: Generates training targets and processes the aligned modalities.
*   **Anti-Forgetting Strategy**: The framework is specifically designed to prevent catastrophic forgetting of the LLM's text proficiency by aligning the audio modality through data curation rather than heavy architectural manipulation.

### Training Data: DeSTA-AQA5M
*   **Volume**: 5 Million Triplets.
*   **Duration**: 7,000 Hours of Audio.
*   **Sources**: 50 Public Datasets.
*   **Scope**: Speech, Environmental Sounds, Music.

---

## Research Contributions

1.  **Framework Innovation**: Introduction of a self-generated cross-modal alignment framework that resolves the trade-off between acquiring new auditory capabilities and retaining existing language abilities.
2.  **Resource Release**: Creation and public release of **DeSTA-AQA5M**, a massive task-agnostic dataset for future research.
3.  **Empirical Validation**: Provided empirical evidence underscoring the importance of data construction pipelines over architectural tweaks in developing robust Large Audio Language Models.

---

## Performance & Results

DeSTA2.5-Audio demonstrated superior performance across major benchmarks:

*   **Dynamic-SUPERB Phase-1**:
    *   **Score**: 69.53 Average
    *   **Comparison**: Significantly outperformed Qwen2-Audio-Instruct (51.69).
    *   **Strengths**: Content, Paralinguistics, and Degraded speech tasks.

*   **Speech-IFEval (Instruction Following)**:
    *   **Score**: 93.89% IFrate
    *   **Comparison**: Nearly double the performance of Qwen2 (47.11).

*   **SAKURA (Multi-hop Reasoning)**:
    *   **Score**: 69.85 (Led the leaderboard).

*   **Dynamic-SUPERB Phase-2**:
    *   **Score**: 3.42 Average with **14 wins** in individual tasks.

*   **Additional Benchmarks**:
    *   **MMAU**: 57.50 Average.
    *   **VoiceBench**: 74.52 Overall Score.