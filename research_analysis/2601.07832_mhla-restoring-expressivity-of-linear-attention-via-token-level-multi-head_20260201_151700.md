# MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head

*Kewei Zhang; Ye Huang; Yufan Deng; Jincheng Yu; Junsong Chen; Huan Ling; Enze Xie; Daquan Zhou*

---

> ### üìä Quick Facts
> 
> **Topic:** Multi-Head Linear Attention (MHLA)
> **Complexity:** Linear $O(Nd_{\phi})$
> **Top Metric:** +41% improvement in Video Generation
> **Hardware:** Validated on NVIDIA H100 (up to 4096x4096)
> **Quality Score:** 8/10

---

## Executive Summary

Linear attention mechanisms face a critical failure mode known as "**global context collapse**," a phenomenon where rank limitations lead to a loss of representational diversity and sparsity. This bottleneck causes standard linear models to underperform compared to their softmax counterparts, restricting the scalability of high-performance tasks in computer vision and NLP.

The authors introduce **Multi-Head Linear Attention (MHLA)**, a novel architecture that restores expressivity by fundamentally altering the multi-head strategy. Unlike traditional approaches that partition heads along the feature dimension, MHLA divides heads along the **token dimension**, partitioning the input sequence into non-overlapping blocks. This method computes local key-value summaries for each block and mixes them using a learnable coefficient matrix initialized with a locality bias.

This design decouples performance from representational diversity loss, prevents context collapse, and maintains linear time complexity without requiring auxiliary modules such as depthwise separable convolutions. The approach demonstrates significant quantitative improvements, including a **41% relative gain in video generation**, proving successful in bridging the gap between computational efficiency and high performance.

---

## Key Findings

*   **üîç Identification of Critical Failure Mode:** Defined 'global context collapse' as the primary reason for performance degradation in existing linear attention methods, specifically the loss of representational diversity.
*   **üöÄ Significant Performance Gains:**
    *   **ImageNet Classification:** +3.6%
    *   **NLP Tasks:** +6.3%
    *   **Image Generation:** +12.6%
    *   **Video Generation:** +41%
*   **‚ö° Retained Efficiency:** Maintained linear time complexity, avoiding the computational overhead typically associated with high expressivity.
*   **‚ôªÔ∏è Expressivity Recovery:** Successfully recovered the expressive power usually reserved for softmax attention mechanisms.

---

## Methodology

The authors propose **Multi-Head Linear Attention (MHLA)** to address the limitations of standard linear attention.

*   **Partitioning Strategy:** MHLA utilizes a unique strategy that divides attention heads along the **token dimension** rather than the feature dimension.
*   **Preservation of Diversity:** This mechanism preserves representational diversity and prevents global context collapse.
*   **Auxiliary-Free Design:** The architecture maintains linear complexity without the need for auxiliary modules like depthwise separable convolutions.

---

## Technical Details

MHLA addresses the "Global Context Collapse" in standard linear attention, which causes rank limitation (rank bound by feature dimension $d$) and loss of sparsity.

**Mechanism Workflow:**
1.  **Sequence Splitting:** The input sequence is split into $M$ non-overlapping blocks.
2.  **Local Summaries:** Local key-value summaries ($S_b, z_b$) are computed for each block.
3.  **Learnable Mixing:** A learnable coefficient matrix ($M_c$) is introduced to mix these summaries based on block affinity.
    *   *Initialization:* Initialized with a locality bias.
    *   *Constraints:* Constrained to be non-negative and normalized.
4.  **Output Calculation:** The final output is calculated via query adaptivity, combining block selection and intra-block reweighting.

**System Efficiency:**
*   **Complexity:** Maintains linear time complexity $O(Nd_{phi})$ using efficient GEMM operations.
*   **Masking:** Supports chunkwise parallel causal masking.

---

## Results & Contributions

### Performance Benchmarks
MHLA demonstrates superior performance over baselines in Class-conditional ImageNet generation (256x256):

| Model | Metric | Linear Attention | Softmax Attention | **MHLA (Ours)** |
| :--- | :--- | :--- | :--- | :--- |
| **DiT-S/2** | FID ‚¨áÔ∏è | 89.7 | 68.4 | **59.8** |
| **DiT-XL/2**| FID ‚¨áÔ∏è | 28.63 | 19.47 | **19.17** |

### Core Contributions
1.  **Theoretical Diagnosis:** Provided the first theoretical diagnosis of linear attention performance degradation defined as 'global context collapse.'
2.  **Architecture Innovation:** Introduced the MHLA architecture which decouples performance from representational diversity loss via token-level head division.
3.  **Complexity Validation:** Provided theoretical validation that MHLA maintains linear complexity while recovering softmax attention expressivity.
4.  **Broad Empirical Validation:** Demonstrated superior performance across computer vision (ImageNet), NLP, and video generation.
5.  **Hardware Efficiency:** Confirmed stability and efficiency on NVIDIA H100 GPUs at resolutions up to 4096x4096.