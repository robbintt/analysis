---
title: 'Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive
  Study on Contrastive Captioners'
arxiv_id: '2512.12824'
source_url: https://arxiv.org/abs/2512.12824
generated_at: '2026-02-03T20:08:38'
quality_score: 8
citation_count: 1
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners

*N. K. B. M. P. K. B. Narasinghe; Uthayasanker Thayasivam*

---

> ### **Quick Facts Sidebar**
>
> *   **Model Architecture:** CoCa (ViT-L/14)
> *   **Pre-training Data:** laion2B-s13B-b90k + MS-COCO
> *   **Evaluation Dataset:** Mini-ImageNet
> *   **Key Strategy:** Low-Rank Adaptation (LoRA) & Hybrid Prototyping
> *   **Novel Loss Function:** Hybrid Objective (SupCon + Cross-Entropy)
> *   **Quality Score:** 8/10

---

## **Executive Summary**

This research addresses the critical gap in understanding how to effectively adapt generative-contrastive hybrid foundation models, specifically Contrastive Captioners (CoCa), for few-shot image classification tasks. While dual-encoder architectures like CLIP are well-understood, the adaptation of single-encoder models like CoCa—which unify visual and textual representations—remains underexplored. This gap is significant because as foundation models scale, efficient adaptation to downstream tasks with extreme data scarcity (few-shot settings) is essential for practical deployment.

The paper investigates why standard adaptation techniques often fail with these models and seeks to establish best practices for leveraging their unique latent spaces. The key innovation lies in a comprehensive empirical study that hierarchically evaluates adaptation strategies for the CoCa visual backbone (ViT-L/14), ranging from training-free hybrid prototyping to deep parameter adaptation. The authors introduce a hybrid loss function that combines standard Cross-Entropy with Supervised Contrastive (SupCon) loss to optimize feature representation.

Technically, the study implements Low-Rank Adaptation (LoRA) by injecting trainable low-rank matrices into specific multi-layer perceptron blocks ($mlp.c\_fc$ and $mlp.c\_proj$) of later transformer layers. This approach allows for deep parameter adaptation while maintaining parameter efficiency, contrasting CoCa’s behavior against established dual-encoder benchmarks.

The study demonstrates that LoRA fine-tuning significantly outperforms linear probing, particularly when configured with the newly proposed hybrid loss objective, which consistently surpassed standard Cross-Entropy loss alone. A critical discovery, termed **"Augmentation Divergence,"** revealed that while strong data augmentation is essential for stabilizing LoRA fine-tuning, it actively degrades performance in linear probing regimes. Experiments conducted on Mini-ImageNet across N-shot configurations ($N \in \{1, 3, 5, 10, 20\}$) showed that training configurations for regularization, rank, and sampling are highly sensitive to data scarcity. Furthermore, simple prototype-based approaches achieved remarkably strong baselines, highlighting CoCa's robust zero-shot transfer capabilities.

---

## **Key Findings**

*   **Augmentation Divergence:** A critical discovery indicating that strong data augmentation degrades linear probing performance in few-shot settings but is essential for stabilizing LoRA fine-tuning.
*   **Superiority of Hybrid Objectives:** Models utilizing hybrid objectives with Supervised Contrastive (SupCon) loss consistently outperform those using standard Cross-Entropy loss alone.
*   **Sensitivity to Data Scarcity:** Training configurations regarding regularization, rank, and sampling strategies are highly sensitive to the degree of data scarcity.
*   **Efficacy of PEFT on CoCa:** Deep parameter adaptation via LoRA is an effective strategy for the CoCa visual backbone, significantly outperforming linear probing approaches.

---

## **Methodology**

The researchers conducted a comprehensive empirical study focused on adapting the visual backbone of Contrastive Captioners (CoCa) for few-shot image classification. The methodology involved:

1.  **Hierarchical Evaluation:** Systematically evaluating a hierarchy of adaptation strategies, ranging from training-free hybrid prototyping to deep parameter adaptation using Low-Rank Adaptation (LoRA).
2.  **Comparative Analysis:** Specifically analyzing CoCa's distinct latent space response to Parameter-Efficient Fine-Tuning (PEFT), contrasting it with dual-encoder architectures like CLIP.
3.  **Experimental Setup:** Conducting experiments on Mini-ImageNet using fixed training and testing pools across various N-shot configurations to ensure robustness.

---

## **Technical Details**

| Component | Specification |
| :--- | :--- |
| **Base Model** | Contrastive Captioners (CoCa) with ViT-L/14 architecture |
| **Pre-training** | Pre-trained on laion2B-s13B-b90k and fine-tuned on MS-COCO |
| **Adaptation Strategy 1** | **Hybrid Prototypes:** Training-free method fusing visual and textual embeddings via weighted combination |
| **Adaptation Strategy 2** | **Linear Probing:** Frozen encoder with a trainable head and strong augmentation |
| **Adaptation Strategy 3** | **LoRA Fine-Tuning:** Injecting low-rank matrices into `mlp.c_fc` and `mlp.c_proj` of later transformer blocks |
| **Loss Function** | Hybrid loss: $L_{total} = L_{CE} + \lambda(t) L_{SupCon}$ |
| **Evaluation Protocol** | Mini-ImageNet, N-shot configurations ($N \in \{1, 3, 5, 10, 20\}$) |

---

## **Results**

*   **Augmentation Impact:** Confirmed that strong data augmentation is essential for stabilizing LoRA Fine-Tuning but degrades performance in Linear Probing.
*   **Loss Function Performance:** The hybrid loss objective (CE + SupCon) outperformed standard Cross-Entropy loss alone.
*   **Adaptation Efficacy:** LoRA Fine-Tuning proved more effective than Linear Probing for the CoCa backbone.
*   **Configuration Sensitivity:** Training configurations (specifically regarding regularization and rank) were found to be highly sensitive to data scarcity.
*   **Baseline Strength:** Simple prototype-based approaches achieved remarkable baselines, indicating strong zero-shot transfer capabilities of the CoCa backbone.

---

## **Contributions**

*   **Bridging the Research Gap:** Addressed the lack of understanding regarding how generative-contrastive hybrid models (CoCa) adapt to downstream tasks with extreme data scarcity.
*   **Empirical Reference Framework:** Provided empirical reference settings for scaling regularization, rank selection, and sampling strategies.
*   **Optimization Insights:** Uncovered the critical role of data augmentation in different fine-tuning paradigms and validated the use of hybrid loss functions (SupCon).

---

**References:** 1 citations