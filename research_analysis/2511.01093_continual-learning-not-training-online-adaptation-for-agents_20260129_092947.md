# Continual Learning, Not Training: Online Adaptation For Agents

*Aman Jaglan; Jarrod Barnes*

---

### ðŸ“Š Quick Facts

| Metric | Result | Impact |
| :--- | :--- | :--- |
| **Success Rate** | **54.1%** (GPT-5-mini) |  20.4 percentage points; 13% higher than GPT-5 High |
| **Cost Reduction** | **86%** | Significant decrease in computational overhead |
| **Token Consumption** | **78,118** | 45% reduction in student token usage |
| **Cross-Incident Accuracy** | **28%  41%** | Improvement via 'frozen pamphlets' without retraining |
| **Benchmark** | ExCyTIn-Bench (Microsoft) | Validated on cyberthreat investigation tasks |

---

## Executive Summary

Traditional AI agent adaptation relies heavily on gradient-based retraining and weight modification, a process that is computationally expensive and prone to catastrophic forgetting. In dynamic environments requiring real-time responses, such as cyberthreat investigation, the need for continual learning conflicts with the prohibitive costs and latency of constantly updating model weights. This paper addresses the challenge of enabling agents to adapt to new, complex tasks immediately without triggering retraining cycles or losing previously acquired knowledge.

The authors introduce **ATLAS** (Adaptive Teaching and Learning System), a gradient-free, continual learning framework that shifts adaptation from the model level to the system level. ATLAS utilizes a dual-agent architecture that decouples the "Teacher" (responsible for reasoning and strategy) from the "Student" (responsible for execution). Instead of backpropagation, the system employs an "active learning substrate"a persistent memory that stores reward trajectories and actively compresses them into generalizable behavioral rules. A meta-level control policy orchestrates these adjustments dynamically at inference time, using "frozen pamphlets" to transfer learned skills across different incidents without modifying the underlying model weights.

Evaluated on Microsofts ExCyTIn-Bench using a cyberthreat investigation scenario, the ATLAS system utilizing the smaller GPT-5-mini model achieved a **54.1%** success rate on Incident #5a **20.4 percentage point** improvement over the baseline. Remarkably, this smaller model outperformed the larger GPT-5 (High) model by 13% while reducing computational costs by 86% and cutting student token consumption by 45%. Furthermore, cross-incident validation demonstrated that using "frozen pamphlets" improved accuracy on new tasks from 28% to 41% without any retraining, while also shifting the systems output behavior from verbose exploration to structured reasoning.

This research establishes a viable alternative to standard fine-tuning by validating inference-time orchestration as a superior method for agent adaptation. By demonstrating that efficient system design can enable smaller models to significantly outperform larger ones, the paper challenges the prevailing "bigger is better" narrative. The shift toward a system-centric paradigm offers a scalable solution for deploying AI in dynamic environments, proving that continual adaptation can be achieved through strategic memory and control layers rather than computationally intensive weight updates.

---

## Key Findings

*   **Smaller Model Superiority:** The ATLAS system using the **GPT-5-mini** model achieved a **54.1%** success rate on the ExCyTIn-Bench, outperforming the larger GPT-5 (High) model by **13%**.
*   **Massive Cost Efficiency:** The approach reduced computational costs by **86%** and lowered student token consumption by **45%** (dropping to 78,118 tokens).
*   **Zero-Retention Transfer:** Cross-incident validation showed that "frozen pamphlets" improved accuracy on new tasks from **28% to 41%** without requiring any retraining.
*   **Behavioral Shift:** The system successfully shifted AI output patterns from "verbose exploration" to "structured reasoning."
*   **Orchestration over Training:** The study establishes that inference-time orchestration is a viable and efficient alternative to gradient-based retraining.

---

## Methodology

The researchers employed a **dual-agent architecture** called ATLAS (Adaptive Teaching and Learning System) designed to operate within a "system-centric" paradigm that prioritizes adaptive efficiency.

*   **Architecture:** The system decouples the reasoning process (Teacher) from the execution process (Student).
*   **Adaptation Strategy:** Instead of modifying model weights, ATLAS uses a persistent learning memory to store "distilled guidance."
*   **Dynamic Adjustment:** An orchestration layer adjusts strategies dynamically specifically at inference time.
*   **Evaluation:** The system was rigorously tested on Microsoft's **ExCyTIn-Bench** using cross-incident validation protocols.

---

## Technical Details

**Core Architecture**
*   **System:** ATLAS (Adaptive Teaching and Learning System).
*   **Structure:** Dual-agent design (Teacher for reasoning, Student for execution).
*   **Approach:** Gradient-free adaptation within the inference loop (no weight updates or backpropagation).

**Memory & Control**
*   **Active Learning Substrate:** Functions as a persistent memory, storing reward trajectories.
*   **Compression:** Performs active compression to synthesize stored trajectories into generalizable behavioral rules.
*   **Meta-Level Control Policy:** Used for the dynamic adjustment of strategies.

**Skill Transfer**
*   **Frozen Pamphlets:** Mechanisms used for cross-incident skill transfer, allowing the application of learned rules to new tasks.

**Distinctions**
Differentiates itself from other methods by focusing on strategic refinement rather than information retrieval:
*   Distinct from *Training-based* methods.
*   Distinct from *Prompt Optimization*.
*   Distinct from *RAG (Retrieval-Augmented Generation)*.
*   Distinct from standard *Episodic Memory*.

---

## Results

**Primary Benchmark (Microsoft ExCyTIn-Bench - Incident #5)**
*   **Success Rate:** Improved from **33.7% (Baseline)** to **54.1% (ATLAS)**.
    *   Represents a **20.4 percentage point** absolute increase.
    *   Outperformed GPT-5 (High) by **13%**.
*   **Efficiency:**
    *   Token consumption reduced to **78,118** (45% reduction).
    *   Computational costs reduced by **86%**.

**Cross-incident Validation**
*   **Accuracy:** Increased from **28%** to **41%** using "frozen pamphlets" without retraining.
*   **Output Quality:** Observable shift from verbose exploration to structured reasoning.

---

## Contributions

1.  **Gradient-Free Continual Learning:** Introduction of a framework that shifts adaptation to system-level orchestration, enabling real-time adjustment without catastrophic forgetting.
2.  **Novel Architecture:** A dual-agent system that separates reasoning and execution roles to allow for flexible external control.
3.  **Empirical Validation:** Provision of validated results on Microsofts open-source ExCyTIn-Bench, a cyberthreat investigation benchmark featuring causally annotated traces.

---

**Quality Score:** 9/10
**References:** 4 citations