# Bayesian continual learning and forgetting in neural networks

*Djohan Bonnet; Kellian Cottart; Tifenn Hirtzlin; Tarcisius Januel; Thomas Dalgaty; Elisa Vianello; Damien Querlioz*

***

## Quick Facts

> **Overall Quality Score:** 8/10  
> **Total Citations:** 38  
> **Proposed Method:** MESU (Metaplasticity from Synaptic Uncertainty)  
> **Key Achievement:** 91.37% Accuracy on 200-task Permuted MNIST  
> **Core Feature:** Boundary-free learning with robust epistemic uncertainty estimates

***

## Executive Summary

### Problem
Neural networks typically suffer from catastrophic forgetting when learning sequentially, as the acquisition of new information overwrites established knowledge, creating a critical stability-plasticity dilemma. Furthermore, most established continual learning (CL) methods depend on explicit task boundaries—knowing precisely when one task ends and another begins—which severely limits their applicability to real-world, streaming data environments where such segmentation is unavailable.

### Innovation
The authors introduce **Metaplasticity from Synaptic Uncertainty (MESU)**, a Bayesian framework that models synaptic weights as probability distributions. The core innovation lies in using synaptic uncertainty (variance) to directly control plasticity: weights with low uncertainty are stabilized to preserve knowledge, while weights with high uncertainty undergo larger updates. This creates a boundary-free method that operates on streaming mini-batches using the reparameterization trick for variational inference.

### Results
MESU demonstrated superior performance across benchmarks. On the 200-task Permuted MNIST experiment, it achieved an average accuracy of **91.37%** (outperforming EWC's 88.50%) and an Out-of-Distribution (OOD) Detection ROC AUC of **0.95**. In Task Incremental Learning on CIFAR-10/100, MESU achieved higher mean accuracy than EWC, Synaptic Intelligence (SI), and Adam baselines, with performance curves closely approximating the ideal.

### Impact
This research resolves the stability-plasticity trade-off by merging biological metaplasticity principles with Bayesian inference. By eliminating the dependency on task boundaries, MESU enables robust continual learning in realistic, unsupervised streaming scenarios. Its capacity to generate reliable epistemic uncertainty estimates enhances OOD detection, which is vital for safety-critical applications.

***

## Key Findings

*   **Superior Performance:** MESU outperformed established continual learning techniques on 200 sequential permuted MNIST tasks in terms of accuracy, learning capacity, and OOD detection.
*   **Task-Agnostic Operation:** Without relying on explicit task boundaries, MESU consistently outperformed conventional techniques during incremental CIFAR-100 training.
*   **Plasticity-Stability Balance:** The framework balances plasticity and stability to mitigate catastrophic forgetting while retaining the ability to learn new tasks.
*   **Uncertainty Estimation:** MESU provides robust epistemic uncertainty estimates, enabling effective out-of-distribution detection.

***

## Methodology

The authors introduce **Metaplasticity from Synaptic Uncertainty (MESU)**, a Bayesian framework for neural networks. The approach updates network parameters based on their uncertainty, preserving critical knowledge (likely low uncertainty) while releasing outdated information (likely high uncertainty).

*   **Unified Approach:** MESU unifies concepts from metaplasticity, Bayesian inference, and Hessian-based regularization.
*   **Streaming Data Adaptation:** Unlike other methods, it adapts to streaming data without requiring explicit task boundaries.
*   **Inference Mechanism:** It generates output statistics by sampling network weights multiple times.

***

## Technical Details

*   **Framework Type:** Bayesian continual learning framework.
*   **Weight Modeling:** Synaptic weights are modeled as probability distributions defined by mean ($\mu$) and variance ($\sigma^2$).
*   **Plasticity Control:** Uncertainty (variance) directly controls plasticity.
*   **Learning Rate Modification:** Replaces the global learning rate with synaptic variance in update equations. High-uncertainty weights undergo larger updates; low-uncertainty weights are stabilized.
*   **Optimization:** Uses the reparameterization trick for variational inference.
*   **Operational Mode:** Operates without task boundaries; treats mini-batches as tasks with a fixed memory window $N$.
*   **Key Issues Addressed:** Catastrophic forgetting, catastrophic remembering, and vanishing uncertainty.
*   **Theoretical Foundations:** Relates to the Bayesian synapse hypothesis and second-order optimization methods.

***

## Contributions

*   **Trade-off Resolution:** MESU resolves the trade-offs between catastrophic forgetting and remembering through a biologically-inspired balance.
*   **Boundary-Free Learning:** Presents a boundary-free continual learning method that adapts to streaming data without the need for task segmentation.
*   **Unified Theory:** Offers a unified theoretical perspective merging biological concepts with Bayesian inference and Hessian-based regularization.
*   **Enhanced Detection:** Contributes a mechanism for reliable epistemic uncertainty estimation, enhancing out-of-distribution detection capabilities.

***

## Results

*   **200 Tasks Permuted MNIST:**
    *   **Accuracy:** 91.37% (vs EWC 88.50%).
    *   **Memory Rigidity Resilience:** 233.27.
    *   **OOD Detection ROC AUC:** 0.95.
    *   **Outperformed:** Baselines like EWC, SI, and FOO-VB.
*   **Domain Incremental Animal Classification:**
    *   MESU maintained high accuracy on previous tasks.
    *   Standard SGD showed significant forgetting.
*   **Task Incremental Learning (CIFAR-10/100):**
    *   **Setup:** Split into 11 tasks.
    *   **Performance:** Demonstrated higher mean accuracy than EWC, SI, and Adam baselines.
    *   **Trend:** Performance curves closer to the ideal than competitors.