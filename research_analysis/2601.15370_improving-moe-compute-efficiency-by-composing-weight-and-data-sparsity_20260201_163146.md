# Improving MoE Compute Efficiency by Composing Weight and Data Sparsity

*Maciej Kilian; Oleg Mkrtchyan; Luke Zettlemoyer; Akshat Shrivastava; Armen Aghajanyan*

***

### üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Validation Loss Reduction** | ~15% compared to sparse baselines |
| **VLM Inference Throughput** | ~1.4x increase |
| **Vision Token Filtering** | ~50% routed to null experts |
| **Text Token Filtering** | ~5% routed to null experts |

***

## Executive Summary

Current Mixture-of-Experts (MoE) architectures limit efficiency by relying solely on weight sparsity‚Äîprocessing every token despite activating only a subset of parameters. While data sparsity (skipping low-value tokens) offers superior theoretical efficiency, it is notoriously difficult to implement in causal autoregressive models. Standard expert-choice routing methods inherently violate strict causal dependencies by making routing decisions based on future tokens, creating a train-inference mismatch that degrades model quality.

This research addresses the fundamental challenge of unifying weight and data sparsity to achieve a more compute-efficient scaling frontier without breaking the autoregressive constraints required for coherent generation. The authors introduce "**zero-compute**" (null) experts into a standard causal token-choice MoE framework, allowing the model to dynamically skip computation for specific tokens without exiting the computation graph.

Instead of removing tokens, the method adds null experts to the routing pool; when a token is routed to a null expert, it consumes zero FLOPs. Crucially, this preserves strict causality because the decision to skip computation is made simultaneously with expert selection rather than retroactively. The system employs a shared load balancing loss across both real and null experts, enforcing uniform utilization.

Under iso-compute conditions, the proposed method outperforms weight-sparse-only MoE baselines. In downstream tasks, the architecture demonstrates "implicit modality awareness," automatically routing nearly 50% of vision tokens to null experts compared to only 5% of text tokens. This aggressive filtering of high-volume, low-information visual data results in significant throughput gains, all while maintaining comparable or superior accuracy.

***

## üîë Key Findings

*   **Enhanced Efficiency Frontier:** Combining weight sparsity with data sparsity yields a more compute-efficient frontier than weight sparsity alone.
*   **Performance Gains:** The method achieves measurable improvements in training loss and downstream performance at matched expected FLOPs.
*   **Implicit Modality Awareness:** The model automatically learns to handle different modalities distinctively, routing vision tokens to null experts (skipping compute) much more aggressively than text tokens.
*   **Causality Preservation:** The approach solves the causality violation problem by recovering the benefits of data sparsity within causal autoregressive models without train-inference mismatch.

***

## ‚öôÔ∏è Methodology

The research team introduced a novel modification to the standard token-choice MoE architecture to enable hybrid sparsity.

1.  **Introduction of Null Experts:** "Zero-compute" (null) experts are added to the standard routing pool.
2.  **Resource Elimination:** Routing a token to a null expert effectively consumes zero computational resources (FLOPs).
3.  **Load Balancing:** The standard load balancing loss is retained and applied to all experts (including null ones). This forces uniform utilization across the entire pool, effectively creating data sparsity "in expectation" while strictly maintaining a causal structure.

***

## üî¨ Technical Details

**Core Architecture**
*   **Mechanism:** Introduces "null experts" into a causal MoE framework to compose weight and data sparsity.
*   **Routing Matrix:** Models routing as a matrix with dual constraints:
    *   Limiting experts per token (Weight Sparsity).
    *   Limiting tokens per expert (Data Sparsity).
*   **Compute Logic:** By adding zero-computation 'null experts', the system assigns tokens to these experts to skip processing ($FLOPs = 0$), enabling variable compute per token while maintaining strict causality.

**Training Objective**
*   **Optimization Strategy:** Uses a load balancing objective applied to both real and null experts.
*   **Distribution:** Encourages a uniform token distribution across all experts to achieve data-sparse allocation.
*   **Target Application:** Specifically optimized for Vision-Language Models (VLMs) to handle data heterogeneity effectively.

***

## üèÜ Contributions

*   **Novel Technique:** A new method to implement data sparsity in causal token-choice MoE models without violating causality, effectively solving the train-inference mismatch associated with expert-choice routing.
*   **Optimization Strategy:** A targeted optimization strategy for Vision-Language Models (VLMs) that manages data heterogeneity by intelligently filtering high-volume, low-information tokens.
*   **Empirical Evidence:** Comprehensive empirical results demonstrating that the composition of weight and data sparsity is superior to isolating weight sparsity when operating under constant compute budgets.

***

## üìà Results

**Validation & Training**
*   Compared to a weight-sparse-only MoE baseline under iso-compute conditions, the method demonstrated measurable improvements in training loss.
*   Achieved a **~15% reduction in validation loss** compared to standard sparse models.

**Modality Handling (VLMs)**
*   The model exhibited "implicit modality awareness," routing vision tokens to null experts (skipping computation) significantly more often than text tokens.
*   **~50% of vision tokens** were routed to null experts.
*   **~5% of text tokens** were routed to null experts.

**Efficiency & Throughput**
*   The aggressive filtering of visual data resulted in a **throughput increase of roughly 1.4x** during Vision-Language Model (VLM) inference.
*   The explicit load balancing mechanism successfully stabilized training, ensuring theoretical FLOPs savings translated directly into wall-clock efficiency gains.

***

*ÂàÜÊûêÊù•Ê∫ê: Research Paper Analysis*