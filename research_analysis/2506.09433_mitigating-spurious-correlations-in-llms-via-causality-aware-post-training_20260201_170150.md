# Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training

*By Shurui Gui; Shuiwang Ji*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **References:** 38 Citations
> *   **Sample Efficiency:** State-of-the-art comparable performance with **100 ID samples**
> *   **Key Benchmark:** CLadder & PrOntoQA
> *   **Model Scale:** 3B-scale models outperforming larger baselines

---

## Executive Summary

### Problem
Large Language Models (LLMs) frequently exhibit critical failures in **Out-of-Distribution (OOD)** scenarios. These failures stem from a reliance on spurious correlationsâ€”statistical shortcuts learned during pre-trainingâ€”rather than robust causal reasoning. While these biases allow models to perform well on standard benchmarks, they cause significant performance drops when the underlying event context changes, even if the logical structure remains constant. Standard Supervised Fine-Tuning (SFT) often fails to eliminate these biases and may inadvertently reinforce them.

### Innovation
The authors introduce **Causality-Aware Post-Training (CAPT)**, a framework grounded in Structural Causal Models (SCM). The technical innovation involves modeling relationships between the Prompt ($X$), Answer ($Y$), Logic Structure ($S$), and Event ($E$) using a Directed Acyclic Graph. CAPT formalizes the prediction probability $P(Y|X)$ to isolate the bias term $P(e|X,s)$. The process is decomposed into two unbiased steps:
1.  **Event Estimation:** Robustly extracting event variable $E$ using specialized prompts.
2.  **Event Intervention:** Intervening on $E$ to decouple domain-specific spurious correlations from pure reasoning structure ($S$).

### Results
Empirical validation on the **CLadder benchmark** and **PrOntoQA dataset** demonstrates that CAPT provides superior generalization compared to standard baselines.
*   A **3B-scale model** fine-tuned with CAPT outperformed traditional SFT and larger LLMs on both In-Distribution (ID) and OOD tasks.
*   The method achieved state-of-the-art comparable performance using only **100 ID fine-tuning samples**, confirming high sample efficiency.
*   CAPT proved robust against high-level event perturbations that typically cause standard models to fail.

### Impact
This research establishes a new paradigm for bias mitigation, proving that robust generalization can be achieved through causality-aware structural adjustments rather than simply scaling model size or data volume. By enabling smaller models to outperform larger ones on causal reasoning tasks with minimal data, CAPT offers a pathway toward more efficient, reliable AI developmentâ€”particularly for low-resource applications and dynamic environments with unpredictable data distributions.

---

## Key Findings

*   **Mitigation of OOD Failures:** LLMs frequently fail on OOD samples due to spurious correlations; CAPT effectively mitigates these pre-training biases without introducing new biases during fine-tuning.
*   **Superior Generalization:** Models fine-tuned with CAPT exhibit better generalization than traditional Supervised Fine-Tuning (SFT) and larger LLMs on both ID and OOD tasks.
*   **High Sample Efficiency:** CAPT achieves state-of-the-art comparable performance using only **100 ID fine-tuning samples**.
*   **Validation:** The approach was rigorously validated on the **CLadder benchmark** and the **PrOntoQA dataset**.
*   **Bias-Free Fine-Tuning:** The framework ensures the post-training phase does not incur additional fine-tuning biases.

---

## Methodology

The authors propose the **Causality-Aware Post-Training (CAPT)** framework to address the limitations of standard fine-tuning. The core innovation lies in decomposing a biased prediction process into two distinct, unbiased steps:

1.  **Event Estimation:** This step focuses on robustly extracting the event variable $E$ from the input $X$. This is achieved using well-crafted prompts designed to isolate the specific event context.
2.  **Event Intervention:** This step intervenes on the extracted event $E$. The goal is to separate domain-specific event spurious correlations from the underlying reasoning structure, effectively isolating and reducing pre-existing biases.

By splitting the process, the model can isolate biases acquired during pre-training, thereby enhancing generalization ability while ensuring the post-training phase remains free from new biases.

---

## Contributions

*   **A Novel Mitigation Strategy:** Introduction of CAPT as a targeted post-training technique specifically designed to address spurious correlations that hinder OOD generalization.
*   **Bias-Free Fine-Tuning:** A methodological advancement that reduces pre-training bias through structural decomposition. This approach safeguards against the introduction of new biases during the fine-tuning phase.
*   **Efficiency and Performance Benchmarking:** The authors provide empirical evidence that 3B-scale models trained with CAPT outperform standard SFT and larger models. This establishes a new standard for sample efficiency in causal and logical reasoning tasks.

---

## Technical Details

The technical implementation of Causality-Aware Post-Training (CAPT) relies on a rigorous mathematical foundation using Structural Causal Models (SCM).

**Structural Components**
*   **Variables:** The model employs specific variables within the causal graph:
    *   **X:** Prompt
    *   **Y:** Answer
    *   **S:** Logic Structure
    *   **E:** Event/Context
*   **Graph Structure:** A Directed Acyclic Graph (DAG) structure is defined as:
    $$E \rightarrow X \leftarrow S \rightarrow Y$$

**Mathematical Formulation**
*   The framework formalizes the LLM prediction probability $P(Y|X)$.
*   It identifies bias introduced by the term $P(e|X, s)$, representing the probability of the event given the prompt and structure.

**Architecture Decomposition**
The fine-tuning process is explicitly divided into two steps:
1.  **Event Estimation:** Robustly extracts the event variable $E$ from input $X$ using prompts.
2.  **Event Intervention:** Intervenes on $E$ to separate domain-specific event spurious correlations from the underlying reasoning structure ($S$).

---

## Results

The evaluation of CAPT highlights its effectiveness in logical reasoning and causal tasks:

*   **Improved Generalization:** CAPT consistently outperformed standard Supervised Fine-Tuning (SFT) and larger LLMs on both In-Distribution (ID) and Out-of-Distribution (OOD) tasks.
*   **Sample Efficiency:** The method demonstrated exceptional efficiency, achieving state-of-the-art comparable performance with only **100 ID fine-tuning samples**.
*   **Robustness:** Validated on the **CLadder benchmark** and **PrOntoQA dataset**, CAPT showed robustness against high-level event perturbations.
*   **Targeted Mitigation:** The results confirm that CAPT successfully targets event-level biases, mitigating specific failures that are common in standard fine-tuned models.