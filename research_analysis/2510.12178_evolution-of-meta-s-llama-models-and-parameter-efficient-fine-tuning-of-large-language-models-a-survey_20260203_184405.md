---
title: 'Evolution of meta''s llama models and parameter-efficient fine-tuning of large
  language models: a survey'
arxiv_id: '2510.12178'
source_url: https://arxiv.org/abs/2510.12178
generated_at: '2026-02-03T18:44:05'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Evolution of Meta's LLaMA Models and Parameter-Efficient Fine-Tuning of Large Language Models: A Survey

*Abdulhady Abas Abdullah; Arkaitz Zubiaga; Seyedali Mirjalili; Amir H. Gandomi; Fatemeh Daneshfar; Mohammadsadra Amini; Alan Salam Mohammed; Hadi Veisi*

---

> ### **Quick Facts**
> *   **Model Evolution:** 7B → 288B parameters (Mixture-of-Experts & Multimodal).
> *   **Context Window:** Up to 10 million tokens (LLaMA 4 Scout).
> *   **Efficiency:** 65B model fine-tunable on a single 48GB GPU (via QLoRA).
> *   **Key Methods:** LoRA, LLaMA-Adapter V1/V2, LLaMA-Excitor, QLoRA.
> *   **Performance:** LLaMA 1 (13B) outperformed GPT-3 (175B).
> *   **Quality Score:** 9/10.

---

## Executive Summary

### Problem
The rapid scaling of the LLaMA series, evolving from 7B parameters to 288B parameters, presents a critical computational challenge. As models grow in size and complexity—incorporating Mixture-of-Experts (MoE) and multimodal capabilities—the resources required for full fine-tuning become prohibitive. This survey addresses the tension between this architectural evolution and the need for accessible, efficient adaptation methods. With the shift towards massive context windows and complex frameworks, the cost of updating model weights for specific domains necessitates a move away from traditional full-parameter training. The paper consolidates the fragmented landscape of Parameter-Efficient Fine-Tuning (PEFT) to provide a clear technical path for adapting these massive models without requiring industrial-scale compute.

### Innovation
The core contribution is a comprehensive technical synthesis of the LLaMA ecosystem alongside a rigorous analysis of five dominant PEFT strategies: **LoRA**, **LLaMA-Adapter V1/V2**, **LLaMA-Excitor**, and **QLoRA**. The authors detail the architectural progression of LLaMA, including the integration of multimodal capabilities and the shift towards sparse architectures. Technically, the survey elucidates how PEFT methods function by freezing base model weights and injecting trainable adapters. For instance, LoRA is explained through the low-rank decomposition formula $\Delta W = \frac{\alpha}{r}BA$, while QLoRA combines this with 4-bit quantization to further reduce memory footprint. The survey also maps the modern training pipelines involving Prompt Generation, Rejection Sampling, Reward Model Training, and Direct Preference Optimization (DPO), offering a blueprint for efficient model specialization.

### Results
The survey presents empirical evidence demonstrating that efficient adaptation can yield performance comparable to or exceeding that of significantly larger models. Key findings include that **LLaMA 1 (13B) outperformed GPT-3 (175B)**, and the 65B variant was competitive with PaLM-540B and Chinchilla-70B. The architectural analysis highlights the progression to LLaMA 3.1, which supports up to 288B parameters and a 128k context window, and LLaMA 3.2, which introduces multimodal support. In terms of efficiency, results show that PEFT matches full fine-tuning performance; specifically, **QLoRA enables the fine-tuning of a 65B parameter model on a single 48GB GPU**, reducing trainable parameters by orders of magnitude. Furthermore, PEFT-adapted models have demonstrated superior performance in specialized benchmarks within legal and medical domains.

### Impact
This survey serves as a vital resource for the technical community, validating the paradigm shift from scaling raw model size to optimizing training efficiency. By demonstrating that smaller, PEFT-fine-tuned models can outperform larger, unadapted baselines, the authors provide a strong argument for the economic and practical viability of open-source models in enterprise settings. The identification of successful real-world implementations in high-stakes fields like law and medicine underscores the transformative potential of these techniques. Ultimately, the paper not only consolidates current knowledge but also outlines a roadmap for future research, specifically targeting persistent challenges in context scaling and robustness, thereby guiding the development of next-generation, accessible AI systems.

---

## Key Findings

*   **Model Evolution:** The LLaMA series has significantly scaled from 7B–65B parameters to 288B parameters, incorporating multimodal capabilities and Mixture-of-Experts (MoE) architectures.
*   **PEFT Efficiency:** Parameter-Efficient Fine-Tuning (PEFT) allows for effective model adaptation with drastically reduced computational overhead compared to full fine-tuning.
*   **Performance Parity:** Fine-tuned LLaMA models utilizing PEFT strategies can outperform larger baseline models that have not been specialized.
*   **Key Methods:** Five prominent PEFT methods—**LoRA**, **LLaMA-Adapter V1/V2**, **LLaMA-Excitor**, and **QLoRA**—have been successfully applied to the ecosystem.
*   **Real-World Application:** PEFT-fine-tuned models have shown success in critical domains such as legal and medical fields.
*   **Persistent Challenges:** Despite successes, challenges remain regarding context scaling and robustness.

---

## Methodology

The paper employs a comprehensive survey methodology that synthesizes technical literature and empirical data. It describes the architectural progression of the LLaMA family, defines the scope of Parameter-Efficient Fine-Tuning (PEFT), and reviews five specific PEFT methods regarding their mechanisms and efficiency. The approach involves:

1.  Structurally comparing adapter architectures.
2.  Analyzing benchmark results.
3.  Examining practical use cases.
4.  Identifying challenges to outline future research directions.

---

## Contributions

*   **Consolidated Survey:** Provides a "one-stop" survey detailing the evolutionary trajectory of the LLaMA series.
*   **Technical Analysis:** Offers a detailed breakdown of five PEFT methods tailored for LLaMA, including their specific mechanisms and trade-offs.
*   **Benchmarking:** Contributes structured analysis and benchmark results demonstrating that parameter-efficient fine-tuning allows smaller models to outperform larger baselines.
*   **Use Case Validation:** Highlights real-world implementations in legal and medical domains.
*   **Future Roadmap:** Provides a clear roadmap for future research directions in the field.

---

## Technical Details

### Architecture & Encodings
*   **Core Architecture:** Transformer decoder with **GELU** activation.
*   **Positional Encodings:**
    *   LLaMA 1 & 2: RoPE (Rotary Positional Embeddings).
    *   LLaMA 3: Extended context embeddings.
*   **LLaMA 4:** Shifts to a sparse **Mixture-of-Experts (MoE)** architecture using a Switch Transformer-style framework with a learned router.

### PEFT Mechanisms
*   **Base Weights:** PEFT mechanisms freeze the base model weights to preserve pre-trained knowledge.
*   **LoRA (Low-Rank Adaptation):** Learns a low-rank update to minimize trainable parameters.
    *   Formula: $\Delta W = \frac{\alpha}{r}BA$
*   **QLoRA:** Combines LoRA with **4-bit quantization** for maximum memory efficiency.
*   **Inference:** LoRA adapters can be merged into the base model for zero inference overhead.

### Training Pipeline
1.  Prompt Generation
2.  Rejection Sampling
3.  Reward Model Training
4.  Supervised Fine-Tuning (SFT)
5.  Direct Preference Optimization (DPO)

---

## Results

*   **LLaMA 1 Performance:**
    *   The 13B model outperformed **GPT-3 (175B)**.
    *   The 65B model was competitive with **Chinchilla-70B** and **PaLM-540B**.
*   **LLaMA 2:** Outperformed existing open-source chat models at the time of release.
*   **LLaMA 3.x & 4 Specifications:**
    *   **LLaMA 3.1:** Supports up to 405B parameters and 128k context.
    *   **LLaMA 3.2:** Adds multimodal support.
    *   **LLaMA 4:** Introduces MoE and a 10 million token context window.
*   **Efficiency Metrics:**
    *   PEFT models match the performance of full fine-tuning.
    *   **QLoRA** enables fine-tuning a 65B model on a single **48GB GPU**.
    *   **LLaMA 4 Scout** runs on a single NVIDIA H100 GPU.
    *   LoRA reduces trainable parameters by orders of magnitude.

---

**Quality Score:** 9/10  
**References:** 40 citations