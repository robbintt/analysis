# International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty

*Rebecca Scholefield; Samuel Martin; Otto Barten*

***

## ðŸ“Š Quick Facts & Metrics

| Category | Details |
| :--- | :--- |
| **Core Mechanism** | Compute Threshold (measured in FLOP) |
| **Governance Model** | International Network of AI Safety Institutes (AISIs) |
| **Institutional Analogy** | IAEA (International Atomic Energy Agency) |
| **Regulatory Strategy** | Framework Convention (Two-step process) |
| **Lowest Proposed Threshold** | $10^{21}$ FLOP (Miotti and Wasil) |
| **Highest Proposed Threshold** | $10^{26}$ FLOP (US EO 14110 / CA SB 1047) |
| **Review Cycles** | 1 year (annual) to every couple of months |
| **Key Risk Capabilities** | Cyber-offense, deception, persuasion, self-proliferation |

***

> ### ðŸ“‹ Executive Summary
> 
> As AI capabilities rapidly outpace regulatory frameworks, the field of international safety governance is plagued by fragmentation, with 2023 proposals diverging widely on risk thresholds and enforcement mechanisms. To address this dangerous governance vacuum and clarify the path to a unified standard, the authors conducted a comprehensive literature review of international AI safety proposals published from 2023 onwards. Their analysis systematically categorizes this discourse by assessing the feasibility of five critical operational processes: building scientific consensus, standardization, auditing, verification, and incentivization.
> 
> The authors propose a **"conditional AI safety treaty"** anchored by a quantifiable **"compute threshold"** as the primary technical trigger for international oversight. By utilizing computeâ€”measured in FLOPâ€”as an empirically grounded proxy for risk, this architecture automatically mandates rigorous protocols for models exceeding defined limits. The operational governance structure centers on an international network of AI Safety Institutes (AISIs), modeled after the IAEA, endowed with the authority to enforce audits and mandate development pauses. This system employs a framework convention approach with dynamic review cycles to continuously adjust thresholds, ensuring regulations remain effective despite improvements in algorithmic efficiency.
> 
> The analysis synthesizes a wide variance in proposed risk appetites, quantifying specific compute thresholds that range from **$10^{21}$ FLOP to $10^{26}$ FLOP**, with specific benchmarks cited from the EU AI Act and US Executive Order. Within these bounds, the paper identifies specific capabilities targeted for evaluation, including cyber-offense, deception, persuasion, political strategy, weapons acquisition, and self-proliferation. Furthermore, the results highlight technical debates regarding oversight metrics, specifically whether restrictions should apply solely to pre-training compute or be expanded to include post-training and inference compute, alongside alternative metrics such as model parameters and energy usage.

***

## ðŸ”‘ Key Findings

*   **Consensus Mapping:** Identified specific areas of consensus and disagreement regarding risk thresholds, regulations, and types of international agreements from 2023 proposals.
*   **Feasibility Framework:** Governance feasibility relies on five key processes: building scientific consensus, standardization, auditing, verification, and incentivization.
*   **Compute Threshold Mechanism:** Proposed a "compute threshold" as the primary mechanism to determine when AI development requires rigorous international oversight.
*   **Governance Institution:** Suggested an international network of AI Safety Institutes (AISIs) with authority to mandate audits and pause development for unacceptable risks.
*   **Adaptive Strategy:** Emphasized that an effective treaty must combine immediately implementable measures with a flexible, adaptive structure.

## ðŸ› ï¸ Methodology

The authors conducted a literature review of international AI safety proposals published from 2023 onwards. The analysis categorized these proposals by risk thresholds, regulatory frameworks, and agreement types, while also utilizing related literature to assess the feasibility of five key operational processes: scientific consensus, standardization, auditing, verification, and incentivization.

## ðŸ“ Contributions

*   **Discourse Synthesis:** Provided a synthesized review of the post-2023 discourse on international AI safety agreements, clarifying consensus and disagreement.
*   **Conditional Treaty Concept:** Introduced a specific "conditional AI safety treaty" recommendation defined by a compute threshold.
*   **Institutional Design:** Proposed the creation of an international network of AI Safety Institutes (AISIs) equipped with enforcement powers.
*   **Pragmatic Strategy:** Contributed a pragmatic, dual-focused strategy balancing immediate action with long-term adaptability.

## âš™ï¸ Technical Details

### Governance Architecture
*   **Risk Proxy:** Utilizes quantifiable "compute thresholds" (measured in FLOP) based on empirical scaling laws.
*   **Workflow:**
    1.  Identify models exceeding thresholds.
    2.  Evaluate for dangerous capabilities (e.g., cyber-offense).
    3.  Enforce audits/pauses via international AISIs.
*   **Adaptability:** Allows dynamic revision of thresholds to counter algorithmic efficiency.

### Operational Mechanics
*   **Review Cycles:** Ranging from annual meetings to reviews every couple of months.
*   **Regulatory Process:** Recommends framework conventions (two-step regulatory process).
*   **Metric Debate:** Technical discussion exists regarding whether the metric should include:
    *   Only pre-training compute.
    *   Post-training and inference compute.

### Alternative Metrics
*   Model parameters.
*   Hardware counts.
*   Energy/carbon usage.

## ðŸ“ˆ Results

### Proposed Risk Thresholds
The paper extracts specific quantitative metrics for risk thresholds from various proposals:

| Proposal | Compute Threshold |
| :--- | :--- |
| **Miotti and Wasil** | $10^{21}$ FLOP |
| **Bilge** | $10^{23}$ FLOP |
| **Trager and others** | $10^{24}$ FLOP |
| **EU AI Act and PauseAI** | $>10^{25}$ FLOP |
| **US Executive Order 14110 & CA SB 1047** | $10^{26}$ FLOP |

### Evaluation Capabilities
The framework targets the evaluation of specific dangerous capabilities, including:
*   Cyber-offense
*   Deception
*   Persuasion
*   Political strategy
*   Weapons acquisition
*   Long-horizon planning
*   AI development
*   Situational awareness
*   Self-proliferation

***

**Document Quality Score:** 8/10