---
title: Is (Selective) Round-To-Nearest Quantization All You Need?
arxiv_id: '2505.15909'
source_url: https://arxiv.org/abs/2505.15909
generated_at: '2026-02-03T18:50:21'
quality_score: 8
citation_count: 38
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Is (Selective) Round-To-Nearest Quantization All You Need?

*Alex Kogan*

---

> ### ðŸ“‹ Quick Facts
>
> *   **Model Tested:** Llama-3.1 (8B & 405B)
> *   **Precisions Targeted:** 4-bit (RTN-4) and 8-bit (RTN-8)
> *   **Hardware:** 8x Nvidia H100 GPUs (80GB)
> *   **Software Stack:** vLLM (v0.6.4), PyTorch 2.5.1, CUDA 12.6
> *   **Key Metric:** Near-optimal **4x** improvement in token generation throughput vs. 16-bit GEMM
> *   **Deployment Capability:** Enables Llama3.1-405B on a single 4x H100 node

---

## Executive Summary

The rapid scaling of Large Language Models (LLMs) has created an urgent need for efficient quantization techniques to reduce memory footprint and computational costs. Recently, the field has trended toward complex quantization algorithms (e.g., GPTQ, AWQ) that require extensive calibration data and non-trivial compute overhead to maintain model accuracy. This paper addresses the critical question of whether such complexity is actually necessary, or if simpler, traditional methods like Round-to-Nearest (RTN) can achieve comparable performance with significantly lower resource requirements.

The core innovation is a revitalized focus on **Round-to-Nearest (RTN) quantization**, enhanced by a selective precision strategy and optimized via Marlin kernels. Unlike data-dependent methods, this approach is data-free, converting model weights to 4-bit or 8-bit integers by simply scaling and rounding to the nearest value. To recover accuracy lost during aggressive quantizationâ€”particularly at lower bit-widthsâ€”the authors introduce a "selective precision" framework. This identifies specific layers or modules that are sensitive to quantization and upcasts them to higher precision (e.g., 16-bit) rather than applying computationally expensive global optimization algorithms. The method leverages high-performance mixed-precision GEMM operations (Marlin kernels) within a patched vLLM framework to maximize throughput and enable on-the-fly quantization.

Evaluations on **Llama-3.1 8B** demonstrate that RTN-8 maintains parity with the full-precision baseline across rigorous benchmarks. Using the `lm_eval` toolkit, the authors measured performance on Wikitext for perplexity and five zero-shot tasks (WinoGrande, ARC, TruthfulQA, CommonSenseQA, and PubMedQA) for accuracy. The baseline 16-bit model achieved an accuracy of **66.07** and perplexity of **8.64**, while RTN-8 achieved **66.13** accuracy and **8.65** perplexity, effectively matching baseline performance. Performance benchmarks using Marlin kernels revealed a near-optimal **4x improvement** in token generation throughput over standard 16-bit GEMM operations for batch sizes between 16 and 32. Furthermore, the efficiency of the method enables the practical deployment of massive models; specifically, the Llama3.1-405B model can be deployed on a single 4x Nvidia H100 node, validating the feasibility of the 4-bit targets.

This research challenges the prevailing assumption that advanced quantization methods are universally superior to simpler approaches. By validating RTN as a viable, high-performance alternative, the paper offers a path to drastically lower the cost and complexity of LLM deployment without sacrificing output quality. The ability to perform on-the-fly quantization in milliseconds and run massive models on limited hardware positions this work as a significant practical contribution for engineers and researchers looking to optimize inference efficiency. However, it is important to note that the evaluation scope is currently limited to the Llama-3.1 architecture, leaving room for future validation across other model families.

---

## Key Findings

*   **Cost-Performance Advantage:** Round-to-Nearest (RTN) quantization is significantly cheaper to implement than advanced alternatives while delivering comparable accuracy and superior token generation throughput.
*   **Viability of RTN:** Contrary to recent trends favoring complex quantization methods, RTN remains a viable and practical choice for quantizing Large Language Models (LLMs).
*   **Selective Precision Enhancement:** The accuracy of standard RTN can be gradually improved by selectively increasing the data precision format for specific model layers and modules rather than applying complex global algorithms.

---

## Methodology

The study implements RTN quantization utilizing the recent **Marlin kernels** to optimize performance. To address accuracy limitations, the authors employ a **selective precision strategy**, which involves increasing data precision only for specific layers and modules identified as critical for maintaining model fidelity.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Approach** | Round-to-Nearest (RTN) methodology; scaling model weights and rounding them to the nearest integer. |
| **Targets** | 4-bit (RTN-4) and 8-bit (RTN-8) precision. |
| **Data Requirement** | **Data-free** method; requires no calibration data. |
| **Speed & Deployment** | Allows for on-the-fly quantization (milliseconds to seconds); deployable on hardware with insufficient HBM (e.g., Llama3.1-405B on 4x Nvidia H100 GPUs). |
| **Optimization Strategy** | Selective precision enhancement increases precision for specific layers/modules. |
| **Software Environment** | Patched vLLM framework (v0.6.4), Python 3.11, PyTorch 2.5.1, and CUDA 12.6. |
| **Kernel Analysis** | Analyzes Marlin kernels for high-performance mixed-precision GEMM operations. |

---

## Results

Experiments were conducted on 8x Nvidia H100 GPUs (80GB) using Wikitext for perplexity and an average of 6 zero-shot tasks (WinoGrande, ARC, TruthfulQA, CommonSenseQA, PubMedQA) for accuracy. The `lm_eval` toolkit was used.

### Performance Metrics (Llama-3.1 8B)

| Configuration | Accuracy | Perplexity |
| :--- | :--- | :--- |
| **Baseline (16-bit)** | 66.07 | 8.64 |
| **RTN (8-bit)** | 66.13 | 8.65 |

*   **Observation:** RTN (8-bit) showed nearly identical performance to the baseline.
*   **Throughput:** Marlin kernels achieved a close to optimal **4x improvement** over 16-bit GEMM calculations for batch sizes of 16â€“32.

---

## Contributions

*   **Challenging Established Conventions:** The paper dispels the established view that modern, advanced quantization methods are universally superior to RTN in terms of performance.
*   **Demonstrating Efficiency:** It provides evidence that a simpler method (RTN) can outperform complex alternatives in critical metrics like token generation throughput and application cost without sacrificing accuracy.
*   **Optimization Framework:** It introduces a practical methodology for enhancing RTN accuracy through selective precision increases, offering a balanced approach between computational efficiency and model performance.

---

**Quality Score:** 8/10  
**References:** 38 citations