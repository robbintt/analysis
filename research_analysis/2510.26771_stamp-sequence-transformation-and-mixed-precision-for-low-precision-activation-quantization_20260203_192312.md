---
title: 'STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation
  Quantization'
arxiv_id: '2510.26771'
source_url: https://arxiv.org/abs/2510.26771
generated_at: '2026-02-03T19:23:12'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization

*Marco Federici; Riccardo Del Chiaro; Boris van Breugel; Paul Whatmough; Markus Nagel*

---

## üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Primary Focus** | LLMs & Large Vision Models (LVMs) |
| **Key Innovation** | Sequence-dimension transformation |
| **Top Performance** | ~17.9x SQNR improvement on PixArt-$\Sigma$ |

---

## üìù Executive Summary

This research addresses the persistent challenge of quantizing activations in generative AI models, specifically Large Language Models (LLMs) and Large Vision Models (LVMs), to bit-widths below eight. While weight quantization is a well-established optimization, reducing activation precision typically results in sharp accuracy degradation due to the high dynamic range and outlier distributions inherent in activation data. This limitation is a significant bottleneck for inference efficiency, as high-precision activations require substantial memory bandwidth and storage. Solving this issue is critical for enabling the deployment of massive generative models on resource-constrained hardware without compromising their generative quality.

The paper introduces **STaMP (Sequence Transformation and Mixed Precision)**, a novel framework that shifts the focus of transformation-based quantization from the channel dimension to the sequence dimension. By exploiting strong local correlations in linguistic and visual data, STaMP applies a linear, invertible sequence transformation ($L$) to concentrate activation energy into a small subset of tokens. This energy concentration facilitates a mixed-precision scheme where high-energy tokens are retained at higher precision to stabilize accuracy, while the bulk of the data is quantized to significantly lower bit-widths. To ensure computational efficiency, STaMP approximates the optimal but expensive Karhunen-Lo√®ve Transform using efficient alternatives like the **Discrete Cosine Transform (DCT)**, **Walsh-Hadamard Transform (WHT)**, and **Discrete Wavelet Transform (DWT)**, minimizing reconstruction error without altering model weights.

STaMP demonstrates significant improvements in signal fidelity and model performance across state-of-the-art architectures. On the PixArt-$\Sigma$ model with 4-bit activations, the method achieved a Signal-to-Quantization-Noise Ratio (SQNR) of 5.00, a ~7.8x improvement over the baseline of 0.64. When integrated with feature transformations, the SQNR reached 11.47, representing a ~17.9x improvement over the baseline. Crucially, the method successfully preserved accuracy on LLaMA v3 8B, validating its effectiveness for both language and vision models. Empirical analysis confirmed that DCT and DWT transforms effectively approximate optimal energy distribution, resulting in significantly lower error bounds than uniform quantization schemes and a marked reduction in visual artifacts in generated images.

This work significantly advances the state of the art in model compression by providing a practical solution for sub-8-bit activation quantization in generative models, a domain where standard techniques often fail. By demonstrating that sequence dimension transformations can effectively mitigate accuracy loss, STaMP offers a new direction for optimizing inference in Large Language and Vision Models. The method's compatibility with existing quantization techniques and its reliance on computationally efficient transforms suggest it can be readily integrated into current deployment pipelines, potentially leading to major reductions in memory footprint and computational costs for next-generation AI systems.

---

## üîë Key Findings

*   **Accuracy Preservation:** STaMP preserves model accuracy at low bit-widths, effectively addressing the sharp degradation typically associated with quantizing activations below eight bits.
*   **Generative Model Performance:** Demonstrates significant improvements on both Large Language Models (LLMs) and Large Vision Models (LVMs) by exploiting strong local correlations in data.
*   **Mixed-Precision Efficiency:** Achieves efficiency via a mixed-precision strategy that maintains a small subset of tokens at higher precision to stabilize accuracy.
*   **Compatibility:** The method is fully compatible with established quantization techniques.

---

## üõ†Ô∏è Methodology

STaMP proposes a shift in how transformations are applied to neural network activations:

*   **Dimension Shift:** Applies linear transformations specifically along the **sequence dimension** rather than the channel dimension to leverage inherent local correlations in linguistic and visual data.
*   **Reparameterization:** Reparameterizes activation tensors to facilitate the quantization process.
*   **Mixed-Precision Scheme:**
    *   Utilizes a select subset of tokens within intermediate activations.
    *   Retains this subset at higher precision.
    *   Allows the bulk of the data to be quantized to lower bit-widths.

---

## ‚öôÔ∏è Technical Details

*   **Quantization Paradigm:** Operates on the sequence dimension (left-multiplying) rather than the feature dimension. This exploits local correlations to concentrate activation energy into a subset of tokens for higher bit-width allocation.
*   **Mathematical Objective:** Utilizes a linear, invertible sequence transformation ($L$) to minimize reconstruction error $E[\|L^{-1}Q(LX) - X\|_2^2]$ without altering weights.
*   **Theoretical Basis:** Relies on a theoretical error upper bound indicating that assigning more bits to high-energy tokens exponentially reduces total error.
*   **Transformations:**
    *   *Optimal:* Karhunen-Lo√®ve Transform (KLT) is computationally expensive.
    *   *Approximations:* STaMP uses efficient approximations to achieve energy concentration:
        *   **DCT** (Discrete Cosine Transform)
        *   **WHT** (Walsh-Hadamard Transform)
        *   **DWT** (Discrete Wavelet Transform)
*   **Complexity:** Ranges from $O(ds)$ to $O(sd \log s)$.

---

## üìà Results

*   **PixArt-$\Sigma$ (4-bit activations):**
    *   STaMP SQNR: **5.00** (Baseline: 0.64) $\rightarrow$ **~7.8x improvement**.
    *   Combined with feature transformations: SQNR reached **11.47** $\rightarrow$ **~17.9x improvement** over baseline.
*   **Visual Quality:** Significant reduction in visual artifacts in generated images.
*   **Validation:** Successfully validated on LLaMA v3 8B (LLM) and PixArt-$\Sigma$ (LVM).
*   **Empirical Analysis:** Confirmed that DCT/DWT transforms successfully approximate the optimal energy distribution of KLT, resulting in lower theoretical error bounds than uniform quantization schemes.

---

## üèÜ Contributions

*   **Novel Framework:** Introduces STaMP, a framework shifting transformation-based quantization focus from the channel dimension to the sequence dimension.
*   **Viable Solution:** Provides a working solution for the challenge of quantizing activations below 8 bits in generative AI models.
*   **Comprehensive Evaluation:** Thoroughly evaluated on state-of-the-art LLM and LVM architectures, proving practical applicability to current systems.