# A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety

*Camille FranÃ§ois; Ludovic PÃ©ran; Ayah Bdeir; Nouha Dziri; Will Hawkins; Yacine Jernite; Sayash Kapoor; Juliet Shen; Heidy Khlaaf; Kevin Klyman; Nik Marda; Marie Pellat; Deb Raji; Divya Siddarth; Aviya Skowron; Joseph Spisak; Madhulika Srikumar; Victor Storchan; Audrey Tang; Jen Weedon*

---

> ### ðŸ“Š Quick Facts
>
> *   **Hugging Face Growth:** 880% increase in Generative AI repositories (160k â†’ 1.57m) between Jan 2023 and Nov 2024.
> *   **Industry Preference:** 46% of Fortune 500 leaders prefer open-source models.
> *   **Expert Engagement:** Over 45 experts from academia, industry, civil society, and government.
> *   **Output:** 12-member working group produced a 40-page document over 6 weeks.
> *   **Impact:** Garnered over 1,800 signatures on a letter; informed the February 2025 French AI Action Summit.
> *   **Quality Score:** 9/10

---

## Executive Summary

This research addresses the critical tension between **openness** and **safety** in artificial intelligence, challenging the prevailing narrative that open-source technologies inherently compromise security. The paper highlights that the current safety landscape is plagued by technical deficits, specifically a scarcity of multimodal benchmarks, limited defenses against prompt-injection attacks, and a lack of infrastructure for participatory mechanisms involving affected communities. This problem is significant because it restricts the ability to perform necessary scrutiny and diverse oversight. As the capabilities of AI systems expand, relying solely on closed, proprietary models creates an opaque ecosystem where safety assurances cannot be independently verified or broadly enforced.

The key innovation presented is a **paradigm shift** that redefines AI safety from a static model property to a dynamic system property achieved through ecosystem-wide infrastructure. Technically, the framework proposes achieving safety through interoperable, decentralized components such as future-proof content filters and rigorous agentic safeguards, rather than relying exclusively on the security of model weights. The research agenda prioritizes five specific directions:

1.  Participatory inputs
2.  Expanded harm taxonomies
3.  Ecosystem-wide safety infrastructure
4.  Agentic safeguards
5.  Technical interventions against prompt injections

By mapping the existing content safety filter ecosystem and proposing new R&D roadmaps, the approach facilitates pluralism and technical scrutiny, allowing openness to act as an accelerator for safety rather than a liability.

The study synthesized insights from a six-week preparatory program involving over forty-five experts from academia, industry, civil society, and government. The working group produced a comprehensive 40-page document that garnered over 1,800 signatures of support. To contextualize the rapid adoption of open technologies, the paper cites significant market metrics: an 880% increase in Hugging Face Generative AI repositories (rising from 160,000 in January 2023 to 1.57 million in November 2024) and survey data indicating that 46% of Fortune 500 leaders prefer open-source models. These results underscore the urgency of developing safety protocols that can scale with the booming open-source ecosystem.

The findings have already begun to shape the high-level discourse on AI governance, directly informing the agenda of the February 2025 French AI Action Summit. By establishing a detailed research agenda and mapping technical interventions for deploying safe models, the paper provides a foundational blueprint for integrating openness into international safety standards. Its contribution lies in moving the field beyond theoretical debates to actionable technical roadmaps, empowering developers and policymakers to implement the infrastructure required for "future-proof" safety and ensuring that the deployment of open AI remains secure and beneficial.

---

## Key Findings

*   **Openness Enhances Safety:** Openness facilitates scrutiny and diverse oversight, acting as a catalyst for safer AI systems.
*   **Critical Technical Gaps:** There is a notable scarcity of multimodal benchmarks and limited defenses against prompt-injection attacks.
*   **Infrastructure Deficit:** A significant lack of infrastructure exists for participatory mechanisms that involve affected communities in the safety process.
*   **Ecosystem-Wide Requirement:** Safety cannot be achieved by models alone; it requires ecosystem-wide infrastructure, including future-proof content filters and rigorous agentic safeguards.

---

## Methodology

The research employed a **solutions-oriented, participatory methodology** centered around collaborative working groups. The process involved:

*   **Convening:** Gathering over forty-five experts from diverse sectors, including academia, industry, civil society, and government.
*   **Preparation:** A six-week preparatory program designed to synthesize diverse insights.
*   **Event:** The findings were derived from the *Columbia Convening on AI Openness and Safety*, held in San Francisco on November 19, 2024.

---

## Contributions

This research establishes a foundational framework for integrating openness with AI safety through the following contributions:

*   **Research Agenda:** Established a comprehensive research agenda specifically targeting AI safety within the context of open-source technologies.
*   **Technical Mapping:** Provided a detailed mapping of technical interventions and available open-source tools for deploying safe models.
*   **Safety Filter Roadmap:** Mapped the content safety filter ecosystem and proposed a definitive roadmap for future R&D.
*   **Priority Directions:** Defined five critical priority research directions:
    1.  Participatory inputs
    2.  Future-proof content filters
    3.  Ecosystem-wide safety infrastructure
    4.  Rigorous agentic safeguards
    5.  Expanded harm taxonomies
*   **Policy Influence:** Recommendations directly informed the agenda of the February 2025 French AI Action Summit.

---

## Technical Details

The research agenda proposes a fundamental technical pivot: viewing **AI safety as a system property rather than a model property**, achieved through openness and ecosystem-wide infrastructure.

### Core Components
*   **Future-Proof Content Filters:** Mechanisms designed to remain effective as model capabilities evolve.
*   **Agentic Safeguards:** Protocols specific to agent-based systems to ensure safe autonomy.
*   **Participatory Mechanisms:** Technical structures allowing for community involvement in oversight.

### Identified Gaps
*   **Multimodal Benchmarks:** Severe lack of standardized testing for non-text inputs/outputs.
*   **Prompt-Injection Defenses:** Insufficient technical countermeasures against prompt engineering attacks.

### Strategic Priorities
*   **Interoperability:** Ensuring safety tools work across different models and platforms.
*   **Decentralization:** Moving away from centralized points of failure or control.
*   **Pluralism:** Supporting diverse approaches and use cases.

---

## Results

The research and convening yielded both quantitative metrics regarding the AI landscape and qualitative outputs regarding the consensus of experts:

*   **Market Growth:** Documented an **880% increase** in Hugging Face Generative AI repositories, growing from 160,000 (Jan 2023) to 1.57 million (Nov 2024).
*   **Executive Sentiment:** 46% of Fortune 500 leaders indicated a preference for open-source models.
*   **Convening Outputs:**
    *   Engagement of over 45 experts.
    *   Production of a substantive 40-page document by a 12-member working group over 6 weeks.
    *   Generation of over 1,800 signatures on a supporting letter.

---

**Quality Score:** 9/10
**References:** 0 citations