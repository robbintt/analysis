---
title: 'Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering
  Provides No Defense Against Neural Adversarial Transfer'
arxiv_id: '2601.21323'
source_url: https://arxiv.org/abs/2601.21323
generated_at: '2026-02-03T20:27:09'
quality_score: 9
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering Provides No Defense Against Neural Adversarial Transfer

*Authors: Achraf Hsain; Ahmed Abdelkader; Emmanuel Baldwin Mbaya; Hamoud Aljamaan*

---

> ### **Quick Facts**
> *   **Dataset:** CIFAR-10
> *   **Surrogate Model:** VGG16 (DNN)
> *   **Target Models:** KNN, Decision Tree, Linear SVM, Kernel SVM, Shallow NN
> *   **Key Phenomenon:** Attack Hierarchy Reversal (FGSM > PGD)
> *   **Max Accuracy Drop:** 59.1% at $\epsilon = 4/255$
> *   **Quality Score:** 9/10

---

## Executive Summary

This research addresses a critical gap in understanding the scope of adversarial attacks: determining whether vulnerabilities are inherent only to Deep Neural Networks (DNNs) or if they extend to classical Machine Learning (ML) pipelines that rely on handcrafted feature engineering. Specifically, the paper investigates the **"protective hypothesis,"** which suggests that using feature extractors like Histogram of Oriented Gradients (HOG) could defend against adversarial examples by creating an information bottleneck that filters out malicious perturbations.

This problem matters because if feature engineering fails to provide security, the threat model for adversarial attacks expands significantly beyond differentiable neural networks to encompass traditional, non-differentiable systems previously assumed to be robust.

The key innovation is the first comprehensive empirical study establishing adversarial transferability from a DNN surrogate (VGG16) to classical ML classifiers (KNN, SVM, Decision Trees, Shallow NN) utilizing HOG features across eight distinct configurations. Technically, the authors identify a novel phenomenon termed **"attack hierarchy reversal."** Unlike standard DNN behavior where iterative attacks (PGD) are more potent than single-step attacks (FGSM), this study reveals that FGSM consistently outperforms PGD against classical ML pipelines.

The study definitively refutes the protective hypothesis, showing that HOG-based pipelines suffer substantial accuracy losses ranging from **16.6% to 59.1%** at a perturbation budget of $\epsilon = 4/255$. Remarkably, FGSM caused greater degradation than PGD in 100% of the experimental cases, validating the attack hierarchy reversal. Sensitivity analysis indicated that doubling the perturbation budget to $\epsilon = 8/255$ resulted in non-linear degradation, increasing accuracy drops by up to 30 percentage points (e.g., Kernel SVM drops increased from 58.3% to 82.3%). Statistical analysis further revealed that HOG Block Size is strongly positively correlated with robustness ($r=0.60$), whereas perturbation magnitude is strongly negatively correlated ($r=-0.65$).

This paper significantly broadens the security threat model by demonstrating that adversarial vulnerabilities are not artifacts of end-to-end differentiability but are fundamental properties of image classification systems as a whole.

---

## Key Findings

*   **Refutation of the Protective Hypothesis:** Feature engineering via HOG does **not** defend against adversarial transfer. The study recorded accuracy drops ranging from **16.6% to 59.1%**.
*   **Discovery of Attack Hierarchy Reversal:** FGSM caused greater degradation than PGD in **100%** of classical ML cases. This is contrary to standard DNN behavior, where iterative attacks (PGD) are typically stronger.
*   **Mechanism of Vulnerability:**
    *   **Iterative attacks (PGD):** Overfit to surrogate-specific features that are filtered out during HOG extraction.
    *   **Single-step attacks (FGSM):** Generate perturbations that survive the feature engineering process.
*   **Fundamental Nature of Vulnerability:** Adversarial vulnerability is a fundamental property of image classification systems rather than an artifact of end-to-end differentiability.
*   **Limited Mitigation:** Block normalization provides partial mitigation but remains insufficient as a standalone defense.

---

## Methodology

The study utilized a transfer learning setup to test the robustness of classical ML pipelines against adversarial examples generated from a DNN.

*   **Surrogate Model:** VGG16 (DNN).
*   **Attack Algorithms:** FGSM (Fast Gradient Sign Method) and PGD (Projected Gradient Descent).
*   **Target Models:** Five classical classifiers:
    *   K-Nearest Neighbors (KNN)
    *   Decision Tree (DT)
    *   Linear SVM (LSVM)
    *   Kernel SVM (KSVM)
    *   Shallow Neural Network (ANN)
*   **Feature Extraction:** Histogram of Oriented Gradients (HOG) across eight configurations to create information bottlenecks.
*   **Dataset:** CIFAR-10.
*   **Evaluation Metric:** Relative accuracy drop of target classifiers.

---

## Technical Details

### Pipeline Configuration
The study evaluates feature-engineered pipelines using HOG features fed into classical classifiers. Baselines used CNN transfer learning with AlexNet as the target and VGG16 as the surrogate.

### HOG Parameters
The authors varied the following parameters to test robustness:
*   **Block Size:** 2, 3
*   **Cell Size:** 6, 10
*   **Orientations:** 3, 9
*   **Perturbation Budget ($\epsilon$):** 4/255, 8/255

### Attack Mechanism
Attacks are generated against the VGG16 surrogate using FGSM and PGD and transferred to the HOG-based classifiers to test if HOG quantization acts as a defense.

### Evaluation Metrics
*   Classification Accuracy
*   Relative Drop
*   Accuracy Retention
*   Pearson correlation coefficient (for statistical analysis)

---

## Results

*   **Hypothesis Refutation:** The hypothesis that HOG features defend against adversarial transfer was refuted. Relative accuracy drops ranged from **16.6% to 59.1%** at $\epsilon = 4/255$.
*   **Attack Hierarchy:** FGSM caused greater degradation than PGD in 100% of cases. This is attributed to PGD overfitting to high-level features lost in HOG extraction.
*   **Budget Sensitivity:** Non-linear degradation was observed. Doubling $\epsilon$ increased the relative drop by up to 30 percentage points (e.g., KSVM drop from 58.3% to 82.3%).
*   **Statistical Correlations:**
    *   **Block Size:** Strongly positively correlated with robustness ($r=0.60$).
    *   **Epsilon:** Strongly negatively correlated with robustness ($r=-0.65$).
    *   **Cell Size & Orientations:** Negligible impact.
*   **Configuration Performance:**
    *   **Best (C5 - Block=3):** Achieved highest retention (69-83%).
    *   **Worst (C8 - $\epsilon=8$):** Achieved lowest retention (18-43%).
    *   **Decision Trees:** Showed consistent retention (~40-60%).
*   **Comparison:** Robustness observed in prior works was likely due to feature fusion or adversarial training, not HOG alone.

---

## Contributions

1.  **First Comprehensive Study:** The first extensive study on adversarial transfer from DNNs to classical ML pipelines relying on handcrafted features (HOG).
2.  **Broadening the Security Threat Model:** Demonstrated that security risks extend beyond differentiable neural networks to non-differentiable, classical paradigms.
3.  **Theoretical Insight on Attack Transfer:** Identified 'attack hierarchy reversal,' providing new understanding that iterative attacks fail to transfer due to overfitting to surrogate features, while single-step attacks succeed.

---

**Quality Score:** 9/10  
**References:** 19 citations