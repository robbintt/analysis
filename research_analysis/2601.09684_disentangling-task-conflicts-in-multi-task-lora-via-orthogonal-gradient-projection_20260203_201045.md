---
title: Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection
arxiv_id: '2601.09684'
source_url: https://arxiv.org/abs/2601.09684
generated_at: '2026-02-03T20:10:45'
quality_score: 9
citation_count: 4
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection

*Ziyu Yang; Guibin Chen; Yuxin Yang; Aoxiong Zeng; Xiangquan Yang*

---

> ### ðŸ“Š Quick Facts
> - **Quality Score:** 9/10
> - **Benchmark:** GLUE
> - **Performance Recovery:** 95% of the gap between MTL and STL closed
> - **Computational Overhead:** Negligible
> - **Key Innovation:** Orthogonal Gradient Projection on LoRA subspaces

---

## Executive Summary

**Problem**
While Low-Rank Adaptation (LoRA) has become the standard for efficient parameter fine-tuning of Large Language Models (LLMs), applying it to Multi-Task Learning (MTL) introduces significant optimization challenges known as "negative transfer." This phenomenon occurs when gradient updates from distinct tasks conflict, causing the model to perform worse than if it were trained on tasks individually. The authors identify that this issue is exacerbated by the low-rank constraint inherent to LoRA, which restricts the optimization landscape and reduces the degrees of freedom available for learning. This "bottleneck conflict" increases gradient collisions, making it difficult for standard joint training methods to find a shared parameter space that benefits all tasks without sacrificing performance.

**Innovation**
To resolve these conflicts, the authors propose Ortho-LoRA, a novel optimization strategy tailored to the bipartite structure of LoRA adapters (matrices $A$ and $B$). Technically, the method operates by identifying conflicting gradientsâ€”defined as those with negative cosine similarities between tasksâ€”and projecting them onto the orthogonal complement of the other tasks' gradients within the intrinsic LoRA subspace. A key component of this approach is "Structure-Aware Decoupling," which applies these orthogonal projections independently to the $A$ and $B$ matrices. This mechanism allows the model to disentangle task-specific conflicts while preserving useful transfer information, all without altering the parameter efficiency or the low-rank structure of the adapters.

**Results**
Extensive experiments on the GLUE benchmark validate the efficacy of Ortho-LoRA in mitigating the performance trade-offs typically associated with parameter-efficient MTL. The method successfully recovers 95% of the performance gap between standard Multi-Task Learning and Single-Task Learning (STL) baselines. Remarkably, Ortho-LoRA achieves performance comparable to independent single-task LoRA fine-tuning while strictly maintaining parameter efficiency. Additionally, the authors report that these significant accuracy improvements come with negligible computational overhead, ensuring that the method remains practical for real-world deployment.

**Impact**
This research makes a significant contribution to the field of efficient NLP by formally diagnosing the geometric limitations of low-rank constraints in multi-task scenarios. By demonstrating that task interference can be effectively managed through orthogonal gradient projection, Ortho-LoRA establishes a new state-of-the-art approach for parameter-efficient MTL. This work suggests that it is possible to train versatile, multi-task models that match the accuracy of single-task specialists without incurring high computational costs, thereby encouraging broader adoption of efficient multi-task training strategies in resource-constrained environments.

---

## Key Findings

*   **Negative Transfer in LoRA MTL:** Combining Multi-Task Learning (MTL) with Low-Rank Adaptation (LoRA) leads to "negative transfer," where conflicting gradient updates degrade performance compared to single-task fine-tuning.
*   **Low-Rank Bottlenecks:** Performance degradation is exacerbated by the low-rank constraint inherent to LoRA, which restricts the optimization landscape and reduces the degrees of freedom for gradient updates.
*   **Efficacy of Ortho-LoRA:** The proposed Ortho-LoRA method effectively mitigates task interference and outperforms standard joint training baselines.
*   **Gap Recovery:** Ortho-LoRA recovers **95%** of the performance gap between multi-task learning and single-task fine-tuning baselines.
*   **Computational Efficiency:** The method achieves significant performance improvements with negligible computational overhead.

---

## Methodology

The authors propose **Ortho-LoRA**, a gradient projection method specifically tailored to the bipartite structure (matrices $A$ and $B$) of LoRA adapters.

The method operates dynamically through the following process:
1.  **Identification:** It identifies conflicting gradients arising from distinct tasks.
2.  **Projection:** These conflicting gradients are projected onto the orthogonal complement of each other within the intrinsic LoRA subspace.
3.  **Optimization:** This allows the model to disentangle task conflicts without altering parameter efficiency, navigating the restricted optimization landscape more effectively.

---

## Technical Details

The paper identifies **'bottleneck conflict'** in Multi-Task Learning using LoRA, where low-rank constraints reduce degrees of freedom and increase gradient collisions. Ortho-LoRA is proposed to mitigate this via two specific mechanisms:

1.  **Orthogonal Gradient Projection**
    *   Detects conflicting gradients based on negative cosine similarity.
    *   Projects these gradients onto the normal plane of other tasks to minimize interference.

2.  **Structure-Aware Decoupling**
    *   Applies the orthogonal projection independently to LoRA matrices $A$ and $B$.
    *   Ensures that useful transfer information is preserved while resolving conflicts within the specific structure of the adapters.

---

## Contributions

*   **Problem Identification:** The study formally identifies and analyzes how the low-rank constraint in LoRA intensifies negative transfer and task conflicts in multi-task learning scenarios.
*   **Algorithmic Innovation:** It introduces Ortho-LoRA, a novel optimization strategy that utilizes orthogonal gradient projection within the LoRA subspace to resolve interference between tasks.
*   **Empirical Validation:** The paper provides extensive experimental evidence on the GLUE benchmark, demonstrating that parameter-efficient MTL can achieve performance comparable to single-task fine-tuning without significant computational costs.

---

## Results

Ortho-LoRA successfully recovers 95% of the performance gap between standard Multi-Task Learning (MTL) and Single-Task Learning (STL) baselines. It consistently outperforms standard joint training baselines and achieves performance comparable to independent single-task LoRA fine-tuning. Crucially, these gains are achieved with negligible computational overhead, as validated by evaluations on the GLUE benchmark.

---

## Assessment

*   **Quality Score:** 9/10
*   **References:** 4 citations