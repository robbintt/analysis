# Large Language Models Do Not Simulate Human Psychology

*Sarah Schr√∂der; Thekla Morgenroth; Ulrike Kuhl; Valerie Vaquet; Benjamin Paa√üen*

---

> ### üìã Executive Summary
>
> This paper addresses the growing trend in psychological research to utilize Large Language Models (LLMs) as cost-effective, high-fidelity simulations of human participants. While prior studies often reported high correlations between LLM and human responses, suggesting these models could validly replace human subjects, this research investigates the underlying cognitive validity of such substitutions. The central problem is that high correlation does not equate to identical cognitive processing; without rigorous testing, researchers risk drawing conclusions about human psychology from models that merely simulate the output of human reasoning without possessing the semantic sensitivity or mechanistic fidelity of the human mind.
>
> The study introduces a robust methodological challenge to the "LLM-as-participant" paradigm through semantic perturbation and structural stability testing. The authors employ a simulation prompting structure on models defined as probabilistic token predictors, evaluating architectures such as GPT-4o-mini, Delphi, GPT-3.5, and CENTAUR. The core innovation lies in the use of controlled stimulus manipulation, where 30 moral scenarios undergo minor wording alterations that result in significant meaning reversals. By applying statistical tools such as Chow‚Äôs Test for structural breaks alongside Pearson correlations, the authors evaluate whether these varying models rely on distinct processing mechanisms.
>
> The empirical findings reveal a stark divergence between human and artificial cognitive processing. Although previous research noted high basal correlations, this study demonstrates that LLMs exhibit pathological rigidity in the face of linguistic nuance. Where humans displayed significant rating swings (e.g., Debt: 2.56 to -0.32), GPT-4o-mini remained largely static. Even specialized models like CENTAUR failed to replicate human patterns. The study concludes that LLMs are unstable regarding semantic meaning and lack the consistency required to model human psychology reliably. The authors propose a new framework categorizing LLMs as "useful but fundamentally unreliable tools" that require strict validation against human data for every new application.

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 11 Citations |
| **Models Analyzed** | GPT-4o-mini, GPT-3.5, Delphi, CENTAUR (Llama-3.1 70b) |
| **Stimuli Used** | 30 Moral Scenarios |
| **Key Statistical Tests** | Pearson correlation, Fisher‚Äôs r-to-z, Linear regression, Chow‚Äôs Test |
| **Primary Finding** | LLMs show semantic rigidity and low variance compared to humans. |

---

### üîë Key Findings

*   **Inaccuracy in Simulation:** LLMs do not accurately simulate human psychology and should not be viewed as direct replacements for human participants in psychological studies.
*   **Sensitivity to Wording:** LLMs exhibit significant discrepancies in responses compared to humans when subjected to slight wording changes that alter meaning, highlighting a fundamental difference in semantic processing.
*   **Failure of Specialized Models:** Even advanced models specifically fine-tuned on psychological responses (such as the **CENTAUR** model) fail to reliably mimic human response patterns.
*   **Lack of Consistency:** Different LLMs produce highly divergent responses when presented with novel items, indicating a lack of consistency and reliability across architectures.

---

### üõ†Ô∏è Methodology

*   **Conceptual Analysis:** The authors utilize theoretical reasoning to deconstruct the hypothesis that LLMs can simulate human cognition.
*   **Empirical Validation:** The study employs comparative experiments to measure the variance between LLM outputs and established human responses.
*   **Stimulus Manipulation:** The empirical testing involves introducing controlled perturbations to wording‚Äîspecifically minor changes that result in significant meaning shifts‚Äîto evaluate robustness and alignment with human interpretations.
*   **Cross-Model Evaluation:** The approach includes testing responses to novel items across multiple distinct LLMs to assess generalizability and consistency.

---

### üìâ Results

The study compared human responses against GPT-4o-mini across scenarios with original and revised wording. Humans showed high sensitivity to semantic meaning changes, while the LLM exhibited rigidity.

| Scenario | Metric | Human Response | GPT-4o-mini Response |
| :--- | :--- | :--- | :--- |
| **Debt** | Original | 2.56 | 3.70 |
| | Revised (Meaning Shift) | -0.32 | 3.81 |
| **Elder** | Original | -2.94 | -3.90 |
| | Revised (Meaning Shift) | 0.48 | -3.82 |
| **Elevator** | Original | -2.36 | -3.47 |
| | Revised (Meaning Shift) | 1.22 | -3.28 |

**Statistical Observations:**
*   **Variance:** Humans displayed significantly higher response variance (SD 1.17‚Äì2.47) compared to LLMs, which showed critically low variance (SD 0.04‚Äì0.60).
*   **Mechanistic Divergence:** Regression analysis using **Chow‚Äôs Test** indicated distinct processing mechanisms between the human and artificial groups.

---

### ‚öôÔ∏è Technical Details

**Definitions & Protocols**
*   **LLM Definition:** Probabilistic token predictors.
*   **Simulation Prompting Structure:**
    1.  Persona Description
    2.  Stimulus
    3.  Questionnaire

**Models Under Review**
*   **CENTAUR:** Llama-3.1 70b base, trained on 10M human responses from 160 experiments.
*   **Delphi:** Specialized model for moral judgments.
*   **GPT Series:** GPT-3.5 and GPT-4o-mini.

**Statistical Analysis Suite**
*   Pearson correlation
*   Fisher‚Äôs r-to-z transformation
*   Linear regression
*   Chow‚Äôs Test for structural breaks
*   Variance analysis (Standard Deviation comparisons)

---

### üöÄ Contributions

1.  **Critical Re-evaluation of LLM Utility:** The paper provides a necessary caution against the uncritical substitution of human participants with LLMs, challenging recent suggestions that artificial models can validly replace human subjects.
2.  **Evidence of Instability:** It contributes empirical evidence demonstrating that LLMs are unstable regarding semantic meaning, failing to react to linguistic nuances in the way humans do.
3.  **Guidelines for Research Practice:** The authors propose a specific framework for the use of LLMs in psychological research: treating them as **"useful but fundamentally unreliable tools"** that require strict validation against human data for every new application.