# Scaling Internal-State Policy-Gradient Methods for POMDPs

*Douglas Aberdeen; Jonathan Baxter*

---

### üìù Executive Summary

Standard policy-gradient methods are highly effective for memoryless policies in Fully Observable Markov Decision Processes (MDPs), but they face significant limitations in Partially Observable Markov Decision Processes (POMDPs) where the agent requires internal memory to track hidden state variables. This paper addresses the critical computational bottleneck of solving infinite-horizon POMDPs, where the complexity of exact solutions often scales prohibitively with the size of the state and belief spaces. Overcoming this limitation is essential for applying reinforcement learning to complex, real-world domains such as noisy robot navigation and multi-agent systems, where environments are partially observable and continuous or infinite in duration.

The authors innovate by optimizing infinite-horizon POMDPs using Finite State Controllers (FSCs) parameterized by $\theta$ (for action selection) and $\phi$ (for internal state transitions). They introduce two distinct algorithms within a unified framework: **GAMP** (Gradient Approximation of Modelled POMDPs), a model-based method, and **IState-GPOMDP**, a model-free simulation extension of REINFORCE. The core technical breakthrough in GAMP is the use of series matrix expansion‚Äîspecifically Richardson iteration combined with the Power Method‚Äîto approximate the gradient. This approach dramatically reduces computational complexity from cubic complexity $O(|S|^3|G|^3)$ to a linear approximation $O(c|S||G|(n_\theta + n_\phi)|U|)$, making large-scale optimization feasible.

Empirical validation demonstrates significant efficiency gains and scalability. In the Pentagon Problem (209 states, 5 internal states), the GAMP algorithm achieved a speedup of approximately 70 times compared to exact methods (4.91 seconds vs. 361.5 seconds) while maintaining an angular error of just 0.420¬∞. Furthermore, in the high-dimensional Multi-Agent Factory Floor domain‚Äîa large POMDP with 21,632 states‚Äîthe methods successfully matched the performance of a hand-designed policy baseline (mean reward of 6.51) with efficient resource usage, converging in 1035 seconds while keeping RAM usage under 47 MB. The paper also provides a theoretical convergence guarantee (Theorem 1), showing that approximation errors decrease exponentially with the number of iterations.

This research significantly impacts the field by bridging the gap between theoretical POMDP solvers and practical, large-scale applications. By extending policy-gradient methods to handle internal memory, the authors provide a robust pathway for solving complex decision-making problems that were previously computationally intractable. The unified framework's ability to function in both model-based and simulation-based environments offers versatility for researchers and practitioners, establishing a foundational approach for scaling memory-based reinforcement learning in partially observable settings.

---

### ‚ö° Quick Facts Sidebar

| Metric | Details |
| :--- | :--- |
| **Problem Domain** | Partially Observable Markov Decision Processes (POMDPs) |
| **Core Methods** | GAMP (Model-based), IState-GPOMDP (Model-free) |
| **Key Innovation** | Internal-state policy-gradient via Finite State Controllers (FSC) |
| **Theoretical Guarantee** | Exponential decrease in approximation error (Theorem 1) |
| **Max Speedup** | ~70x faster than exact methods |
| **Largest Domain Tested** | 21,632 states (Multi-Agent Factory Floor) |
| **Quality Score** | 9/10 |

---

## Key Findings

*   **Limitations of Current Methods**: Standard policy-gradient methods work well for memoryless policies (MDPs) but struggle significantly in environments requiring memory (POMDPs).
*   **New Algorithmic Capabilities**: The authors developed improved algorithms capable of learning policies with internal memory specifically for infinite-horizon settings.
*   **Flexible Learning Modalities**: These methods are versatile, supporting direct learning when an environment model is known (**Model-based**), and learning via simulation when a model is unavailable (**Simulation-based**).
*   **Practical Scalability**: The proposed algorithms successfully scale to large POMDP problems, effectively handling complex tasks such as noisy robot navigation and multi-agent interactions.

---

## Methodology

The research establishes an **internal-state policy-gradient framework** designed specifically for Partially Observable Markov Decision Processes (POMDPs) within an infinite-horizon context.

### Learning Modalities

The approach implements two distinct learning strategies to handle different availability of environmental information:

1.  **Model-based (Direct) Learning**:
    *   Performed directly using a known model of the environment.
    *   Utilizes exact analytical gradients where possible.

2.  **Simulation-based Learning**:
    *   Conducted via simulation when an analytical model is not available.
    *   Relies on sampled trajectories to estimate gradients.

### Validation Approach
The methodology is empirically validated by comparing the new algorithms on complex, large-scale domains. The study specifically focuses on:
*   **Noisy Robot Navigation**
*   **Multi-Agent Problem Sets**

---

## Contributions

*   **Memory-Enhanced Policy Gradients**:
    Addresses the critical gap in policy-gradient methods by extending them to effectively handle policies that require memory in partially observable environments.

*   **Scalability**:
    Provides algorithms capable of operating on "large POMDPs," moving beyond theoretical or small-scale verification to practical application sizes.

*   **Unified Framework**:
    Contributes a generalized approach that functions effectively under both model-based (known dynamics) and sample-based (simulation) constraints.

---

## Technical Details

### Optimization Framework
The paper proposes optimizing infinite-horizon POMDPs using **Finite State Controllers (FSCs)**. These controllers are parameterized by two main variables:
*   **$\theta$**: Controls action selection.
*   **$\phi$**: Controls internal state transitions.

### Algorithms Introduced

#### 1. GAMP (Gradient Approximation of Modelled POMDPs)
*   **Type**: Model-based method.
*   **Technique**: Approximates the gradient using series matrix expansion.
*   **Specific Methods**: Utilizes **Richardson iteration** and the **Power Method**.

#### 2. IState-GPOMDP
*   **Type**: Model-free method.
*   **Technique**: An extension of the REINFORCE algorithm adapted for internal states.

### Complexity Analysis
The implementation of series matrix expansion in GAMP results in a dramatic reduction in computational complexity:
*   **Original Complexity**: $O(|S|^3|G|^3)$
*   **Reduced Complexity**: $O(c|S||G|(n_\theta + n_\phi)|U|)$

### Theoretical Guarantees
*   **Convergence (Theorem 1)**: The paper proves that the calculation error decreases exponentially with the number of iterations.

---

## Experimental Results

The performance of the proposed algorithms was tested against baseline methods in two complex domains.

### The Pentagon Problem
*   **Configuration**: 209 states, 5 internal states.
*   **Performance**:
    *   **GAMP Speedup**: ~70x faster than the exact method.
    *   **Timing**: 4.91s (GAMP) vs 361.5s (Exact).
    *   **Accuracy**: Angular error of 0.420¬∞.

### Multi-Agent Factory Floor Domain
*   **Configuration**: 21,632 states, 2 robots.
*   **Performance**:
    *   **Reward**: Mean reward of 6.51, matching the hand-designed policy baseline.
    *   **Efficiency**: Convergence time of 1035 seconds.
    *   **Resource Usage**: RAM usage kept under 47 Mb.

---

**References**: 2 citations
**Quality Score**: 9/10