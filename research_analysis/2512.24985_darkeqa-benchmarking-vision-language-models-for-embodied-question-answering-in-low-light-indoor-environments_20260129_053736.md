# DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments

*Yohan Park; Hyunwoo Ha; Wonjun Jo; Tae-Hyun Oh*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Benchmark** | DarkEQA |
| **Task** | Embodied Question Answering (EQA) |
| **Degradation Scale** | L0 (Well-lit) â†’ L5 (Severe Dark/Noise) |
| **Simulation Type** | Physics-based (Linear RAW space) |
| **Key Models Tested** | LLaVA, InternVL2, Qwen2-VL, GPT-4o |
| **Primary Metric** | Accuracy (%) |
| **Quality Score** | 9/10 |

---

> **ðŸ“‹ Executive Summary**
>
> Current Vision-Language Models (VLMs) designed for Embodied Question Answering (EQA) suffer from a critical **perception bottleneck** that severely hinders reliability in low-light conditions. While state-of-the-art models perform well under ideal lighting, they lack the robustness required for 24/7 operation in real-world environments where visual degradation is common. Existing benchmarks fail to assess these capabilities because they predominantly evaluate models using well-lit datasets, systematically overlooking the failures that arise when agents must navigate and answer questions based on egocentric observations corrupted by darkness and sensor noise.
>
> To address this, the authors introduce **DarkEQA**, an open-source benchmark framework that simulates low-light conditions with high physical fidelity rather than relying on synthetic post-processing. The key technical innovation is a physics-based simulation pipeline that models illumination drops and sensor noise directly in **linear RAW space**, which is then converted into viewable observations using an ISP-inspired rendering pipeline. This approach allows for the evaluation of EQA primitives across a controlled, multi-level degradation scale (L0 to L5), providing a rigorous standard for testing embodied agents from well-lit to near-total darkness.
>
> Evaluation across the L0 to L5 degradation scale revealed a systematic performance collapse for all tested VLMs, including LLaVA-v1.6-mistral-7b, InternVL3_5, Qwen3-VL, and GPT-4o. The study highlights a critical quantitative failure: for the most vulnerable primitivesâ€”*Room Type Recognition* and *Object Attribute â€“ Color*â€”accuracy under severe degradation (L4, L5) fell below the **GPT-4 Blind LLM** text-only baseline. This indicates that misleading visual input in low-light conditions is more detrimental than having no visual input at all. Furthermore, evaluation of Low-Light Image Enhancement (LLIE) as a mitigation strategy produced contradictory results; while pre-processing improved accuracy at severe levels (L4, L5), it actually decreased performance at moderate degradation levels (L1â€“L3).

---

## Key Findings

*   **Systematic Failure:** State-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models exhibit distinct limitations and failures under low-light conditions.
*   **Perception Bottleneck:** The primary challenge for VLMs in dark environments is visual degradation hindering the interpretation of egocentric observations.
*   **Benchmarking Gap:** Existing benchmarks fail to assess 24/7 operation capabilities, focusing predominantly on ideal lighting and overlooking visual degradations.

## Methodology

The authors introduced **DarkEQA**, an open-source benchmark designed to evaluate Embodied Question Answering (EQA) primitives across multi-level low-light conditions. The approach is characterized by:

*   **Physics-Based Simulation:** Visual degradations are modeled in linear RAW space to simulate illumination drops and sensor noise, avoiding synthetic post-processing.
*   **ISP-Inspired Rendering:** A pipeline converts simulated RAW data into viewable observations for evaluating egocentric QA under controlled degradations.

## Technical Details

**Framework Overview**
DarkEQA evaluates VLMs on EQA tasks in low-light indoor environments using egocentric visual observations.

**Degradation Scale**
*   **L0:** Original, well-lit (Baseline).
*   **L1â€“L5:** Progressively darker and noisier conditions.

**Models Evaluated**
*   **VLMs:** LLaVA-v1.6-mistral-7b, LLaVA-OneVision-1.5-8B, InternVL3_5 (8B & 30B), Qwen3-VL (8B & 32B).
*   **Baselines:** GPT-4o (Upper Bound), GPT-4 Blind LLM (Text-only).
*   **Intervention:** SOTA Low-Light Image Enhancement (LLIE) pre-processing pipeline.

## Results

*   **Main Metric:** Accuracy (%) measured across degradation levels L0 through L5.
*   **Systematic Decline:** All VLMs, including GPT-4o, showed a drop in accuracy as low-light conditions intensified.
*   **LLIE Efficacy:** 
    *   **Positive:** Significant accuracy improvements at severe degradation levels (**L4, L5**).
    *   **Negative:** Decreased performance at moderate levels (**L1â€“L3**), indicating bias toward specific degradation levels.
*   **Task Vulnerability:**
    *   *Room Type Recognition* and *Object Attribute â€“ Color* were the most impacted.
    *   Under severe conditions, performance dropped below the **GPT-4 Blind LLM** baseline due to misleading visual input.

## Contributions

*   **Resource Release:** Released the **DarkEQA** benchmark and dataset, an open-source resource for low-light embodied QA.
*   **Technical Innovation:** Developed a framework simulating low-light conditions with physical fidelity by operating in linear RAW space and incorporating sensor noise.
*   **Analysis:** Provided comprehensive baseline and attribution analysis regarding the robustness of current SOTA VLMs and LLIE models in dark environments.

---

**Paper Score:** 9/10  
**References:** 40 citations