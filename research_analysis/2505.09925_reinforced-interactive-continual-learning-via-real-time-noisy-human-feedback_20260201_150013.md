# Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback

*Yutao Yang; Jie Zhou; Junsong Li; Qianjun Pan; Bihao Zhan; Qin Chen; Xipeng Qiu; Liang He*

***

> ### üìä Quick Facts
>
> *   **Quality Score:** 6/10
> *   **References:** 40 Citations
> *   **Key Datasets:** FewRel, TACRED
> *   **Core Framework:** RiCL (Reinforced interactive Continual Learning)
> *   **Primary Focus:** Noisy human feedback, Catastrophic forgetting, Streaming data

***

### üìã Executive Summary

This research addresses a critical limitation in traditional Continual Learning (CL): the dependence on static datasets with perfectly clean labels. In real-world human-in-the-loop (HITL) scenarios, AI agents must learn from streaming data where human feedback is often subjective, inconsistent, or noisy. This introduces a dual challenge for existing models, which must adapt to new information without succumbing to "catastrophic forgetting," while simultaneously preventing corruption from erroneous human labels. Solving this is essential for enabling AI systems to evolve dynamically alongside users in production environments without accruing performance-degrading errors from unreliable inputs.

The authors propose **RiCL** (Reinforced interactive Continual Learning), a unified framework that integrates Large Language Models (LLMs) to process noisy, real-time feedback through a three-stage architecture designed to maximize learning efficiency. The **Temporal Consistency-aware Purifier (TCP)** dynamically distinguishes between clean and noisy samples using Generalized Cross-Entropy Loss and logit margin confidence, effectively routing data to prevent corruption. The **Interaction-aware Direct Preference Optimization (IPO)** component reframes human feedback as a preference signal rather than a hard label, aligning the model's intent with human corrections by maximizing the logit difference between corrected and predicted labels. Finally, **Noise-resistant Contrastive Learning (NCL)** utilizes text augmentation‚Äîsuch as synonym replacement and random insertion‚Äîto build robust feature representations that rely on data relationships rather than potentially faulty label fidelity.

The framework was rigorously validated against state-of-the-art baselines combining online continual learning and noisy-label learning methods on the FewRel and TACRED datasets. Experimental results demonstrate that RiCL substantially outperforms these existing approaches, confirming robustness against realistic noise patterns found in human interaction. The system successfully navigated the trade-off between plasticity and stability, effectively retaining prior knowledge while acquiring new skills from dynamic streams, thereby proving its viability for real-world interaction constraints where labels are inherently unreliable. The significance of this work lies in its departure from the "clean label assumption," establishing a new paradigm for interactive continual learning that reflects the messy reality of human-AI interaction.

***

## üîë Key Findings

*   **Superior Performance:** The proposed RiCL framework substantially outperforms existing combinations of state-of-the-art online continual learning and noisy-label learning methods.
*   **Robustness to Noise:** Experimental validation on benchmark datasets (FewRel and TACRED) confirms the framework's robustness against realistic noise patterns found in human feedback.
*   **Mitigation of Catastrophic Forgetting:** The framework successfully enables AI models to learn new skills dynamically from streaming data while retaining prior knowledge, effectively addressing 'catastrophic forgetting.'
*   **Robust Representations:** By leveraging noise-resistant contrastive learning, the model can capture robust data representations without relying on potentially unreliable labels.
*   **Real-World Viability:** The system proves capable of handling real-world interaction constraints where data is streamed dynamically and labels are inherently noisy.

***

## üõ†Ô∏è Methodology

The authors propose RiCL (Reinforced interactive Continual Learning), a framework leveraging Large Language Models (LLMs) to learn from real-time, noisy human feedback. The methodology is built upon three specific technical components:

1.  **Temporal Consistency-aware Purifier**: A module that analyzes data streams to distinguish between clean and noisy samples.
2.  **Interaction-aware Direct Preference Optimization (DPO)**: A strategy used to align the model's behavior with human intent.
3.  **Noise-resistant Contrastive Learning**: A mechanism that captures robust feature representations by exploiting inherent data relationships.

***

## ‚öôÔ∏è Technical Details

RiCL is a human-in-the-loop continual learning framework designed for streaming data. It utilizes a three-stage architecture:

### 1. Temporal Consistency-aware Purifier (TCP)
*   **Function:** Distinguishes clean and noisy samples based on prediction stability.
*   **Mechanism:** Employs Generalized Cross-Entropy (GCE) Loss and a logit margin confidence approach.
*   **Output:** Sorts data into **Clean** and **Noisy Buffers**.

### 2. Interaction-aware Direct Preference Optimization (IPO)
*   **Function:** Aligns the LLM with human intent by treating feedback as preferences.
*   **Mechanism:** Maximizes the logit difference between human-corrected labels and alternative predicted labels sampled from the model's distribution.

### 3. Noise-resistant Contrastive Learning (NCL)
*   **Function:** Learns robust text representations to mitigate the impact of label noise.
*   **Technique:** Utilizes data augmentation methods including:
    *   Synonym replacement
    *   Random insertion
    *   Random swap
    *   Random deletion

***

## üìà Contributions

*   **New Paradigm:** Introduces a new interactive continual learning paradigm that moves beyond static datasets with fixed labels to address the realities of streaming, real-time human-annotated data.
*   **Noise Limitation Addressed:** Explicitly addresses the limitation of the 'clean label assumption' in traditional continual learning by developing mechanisms to process and learn from noisy feedback.
*   **Robust Representation Method:** Contributes a method for learning robust representations through contrastive learning that relies on data relationships rather than label fidelity.
*   **Unified Solution:** Provides a unified solution (RiCL) that integrates temporal purification and preference optimization to solve the dual challenges of dynamic updates and noise in interactive settings.

***

## üß™ Results

The framework was validated on the **FewRel** and **TACRED** datasets with the following outcomes:

*   RiCL substantially outperformed existing combinations of state-of-the-art online continual learning and noisy-label learning methods.
*   It demonstrated confirmed robustness against realistic noise patterns inherent in human feedback.
*   The model successfully mitigated catastrophic forgetting by retaining prior knowledge while learning new skills.
*   It captured robust data representations, proving viable for real-world interaction constraints characterized by dynamic data streaming and noisy labels.