---
title: 'OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling
  Transformations for Better Distribution Fitting'
arxiv_id: '2501.13987'
source_url: https://arxiv.org/abs/2501.13987
generated_at: '2026-02-03T19:35:04'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting

*Xing Hu; Yuan Cheng; Dawei Yang; Zukang Xu; Zhihang Yuan; Jiangyong Yu; Chen Xu; Zhe Jiang; Sifan Zhou*

***

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Accuracy Retention:** 99.5% (W4-only quantization)
> *   **Gap Reduction:** 32% reduction in performance gap (W4A4KV4 on LLaMA-3-8B)
> *   **Key Innovation:** Learnable Orthogonal & Scaling Transformations
> *   **Novel Metric:** Quantization Space Utilization Rate (QSUR)
> *   **Optimization:** KL-Top loss function
> *   **Quality Score:** 9/10
> *   **References:** 40 citations

***

## Executive Summary

Aggressively quantizing Large Language Models (LLMs) to 4-bit precision for weights, activations, and KV caches (**W4A4KV4**) is essential for reducing memory footprint and accelerating inference. However, this process often results in significant performance degradation due to the difficulty of mapping high-dimensional, continuous data distributions into a discrete, limited quantization space.

This paper introduces **OSTQuant**, a novel Post-Training Quantization (PTQ) algorithm designed to optimize distribution fitting through learnable equivalent transformations. The authors propose the **Quantization Space Utilization Rate (QSUR)**, a rigorous metric defined as the ratio of dataset hypervolume to quantization space hypervolume, to mathematically assess quantizability.

To maximize QSUR, OSTQuant employs an optimal linear transformation combining:
*   **Orthogonal Transformation:** Rotates the data distribution.
*   **Scaling Transformation:** Adjusts the magnitude of the distribution.

This approach aligns the data more naturally with the quantization grid. Furthermore, the method utilizes a **KL-Top loss function** to mitigate optimization noise and preserve semantic information, ensuring robustness even with limited calibration data. These transformations are fused directly into the model weights, incurring minimal runtime overhead.

**Performance Impact:** In W4-only settings, OSTQuant achieves 99.5% accuracy retention. More notably, in W4A4KV4 configurations on LLaMA-3-8B, it reduces the performance gap relative to the full-precision baseline by **32%** compared to current state-of-the-art methods.

---

## Key Findings

*   **High Accuracy Retention:** OSTQuant achieves **99.5%** accuracy retention in W4-only quantization scenarios.
*   **Significant Gap Reduction:** In aggressive W4A4KV4 settings on LLaMA-3-8B, the method reduces the performance gap by **32%** compared to state-of-the-art alternatives.
*   **Effective Quantizability Assessment:** The introduced **Quantization Space Utilization Rate (QSUR)** is proven to be an effective metric for evaluating how well data fits within the quantization space.
*   **Robust Optimization:** The **KL-Top loss function** successfully mitigates optimization noise and preserves semantic information even when calibration data is limited.

---

## Methodology

The core of OSTQuant revolves around a mathematical approach to optimizing how data occupies the quantization space.

1.  **Metric Definition (QSUR):**
    The methodology introduces the Quantization Space Utilization Rate (QSUR) to evaluate data quantizability through mathematical derivations, specifically measuring the ratio of dataset hypervolume to quantization space hypervolume.

2.  **Learnable Equivalent Transformations:**
    OSTQuant employs learnable equivalent transformations to optimize distributions across the quantization space. This consists of two main components:
    *   **Orthogonal Transformation:** Used to rotate the data.
    *   **Scaling Transformation:** Used to adjust the data scale.

3.  **Optimization Strategy:**
    The method utilizes a **KL-Top loss function** designed to minimize noise and preserve semantic information in Post-Training Quantization (PTQ) scenarios, specifically addressing challenges related to scarce calibration data.

---

## Contributions

*   **New Evaluation Metric:** Proposal of **QSUR** as a new evaluation metric for assessing data fit within quantization space, moving beyond empirical heuristics.
*   **Advanced PTQ Algorithm:** Development of **OSTQuant**, a sophisticated PTQ algorithm utilizing orthogonal and scaling transformations for superior distribution fitting.
*   **Optimization Stability:** Introduction of the **KL-Top loss function** to ensure optimization stability and semantic richness when calibration data is scarce.
*   **Performance Baselines:** Establishment of new performance baselines for aggressive **W4A4KV4 quantization** on modern LLMs, specifically LLaMA-3-8B.

---

## Technical Details

**Mathematical Foundation**
*   **QSUR Definition:** Defined as the ratio of dataset hypervolume to quantization space hypervolume. The theoretical maximum QSUR achievable via transformation is $\frac{\pi^{d/2}}{\Gamma(d/2+1) 2^d}$.
*   **Optimal Transformation:** The method models data as a Gaussian distribution and derives an optimal linear transformation:
    $$T = c \cdot \Lambda^{-1/2} Q^\top$$
    This formulation is designed to maximize QSUR.

**Architecture & Implementation**
*   **Structure:** Combines a global orthogonal transformation ($R_{res}$) with layer-specific scaling for both attention and FFN layers.
*   **Efficiency:** Transformations are fused with weights to ensure minimal runtime overhead.
*   **Loss Function:** Utilizes a KL-Top loss function for optimization with limited calibration data.

**Data Observations**
*   Analysis indicates that LLaMA-2 7B weights and activations generally follow a Gaussian pattern.
*   Activations exhibit higher inter-channel variance disparities compared to weights.

---

## Results

*   **W4-Only Quantization:** Achieved **99.5%** accuracy retention.
*   **W4A4KV4 Quantization (LLaMA-3-8B):** Reduced the performance gap by **32%** compared to state-of-the-art methods.
*   **QSUR Analysis:** Demonstrated a higher density of quantization points within the data ellipse compared to SmoothQuant and QuaRot methods, validating the efficacy of the distribution fitting approach.

***

*Document formatted based on analysis quality score: 9/10*