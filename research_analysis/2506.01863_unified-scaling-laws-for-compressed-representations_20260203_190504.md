---
title: Unified Scaling Laws for Compressed Representations
arxiv_id: '2506.01863'
source_url: https://arxiv.org/abs/2506.01863
generated_at: '2026-02-03T19:05:04'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Unified Scaling Laws for Compressed Representations

*Andrei Panferov; Alexandra Volkova; Ionut-Vlad Modoranu; Vage Egiazarian; Mher Safaryan; Dan Alistarh*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40 references
> *   **Fit Error (Sparsity):** 4.7 √ó 10‚Åª‚Å¥
> *   **Fit Error (Quantization):** 2.1 √ó 10‚Åª¬≥
> *   **Core Metric:** `GMSE` (Gaussian Mean Squared Error)
> *   **Key Property:** Multiplicative Capacity (`œÅ(R‚ÇÅ) * œÅ(R‚ÇÇ)`)

---

## üìù Executive Summary

### **Problem**
As deep learning models grow in size, compression techniques such as sparsity and quantization have become essential for efficient deployment. However, the field lacks a unified theoretical framework to predict how these diverse compression methods impact model performance. Currently, evaluating the efficiency of different compressed representations relies heavily on expensive, iterative experimentation. This paper addresses the need for a generalized scaling law that can accurately forecast model behavior across a wide spectrum of compression formats, removing the need for resource-intensive trial-and-error in model selection.

### **Innovation**
The core innovation is the introduction of a unified scaling law formulation that integrates a novel "capacity" metric, denoted as $\rho(R)$. Technically, this metric is derived by measuring a representation's ability to fit random Gaussian data (GMSE), which serves as a robust proxy for the parameter efficiency of the compressed format. The proposed scaling law is formulated as $Loss(N, D) \sim A \cdot (N \cdot \rho(R))^{-\alpha} + B \cdot D^{-\beta} + E$, where $N$ is parameter count and $D$ is data size. Crucially, the authors demonstrate that this capacity metric is multiplicative; when compression formats are combined (e.g., sparsity plus quantization), the total capacity is the product of individual capacities ($\rho(R_{combined}) = \rho(R_1) \cdot \rho(R_2)$). This property allows the framework to scale composably.

### **Results**
The proposed unified scaling law demonstrates high precision, achieving fit errors of $4.7 \times 10^{-4}$ for sparsity and $2.1 \times 10^{-3}$ for quantization, which is comparable to or better than prior specialized laws. Experimental validation confirmed the multiplicative property of capacity with approximation errors generally around $10^{-2}$. Additionally, the framework successfully predicted algorithmic optimality, showing that Quantization-Aware Training runs align with noise injection predictions and that the QuEST algorithm operates near the information-theoretic lower bound. The analysis also revealed that backward pruning heuristics generally outperform magnitude pruning baselines.

### **Impact**
This research provides a significant theoretical advancement by offering a standardized, low-cost method to compare the inherent accuracy potential of disparate compressed representations. By establishing that scaling laws govern compressed models both individually and compositionally, the authors enable the derivation of optimized training algorithms for complex formats, such as sparse-quantized models. The ability to predict performance via a simple capacity metric allows practitioners to make more informed architectural decisions, ultimately reducing the computational overhead associated with training and deploying state-of-the-art compressed networks.

---

## üîë Key Findings

*   **Unified Predictability:** A general scaling law formulation can accurately predict model performance across a wide range of compressed representations.
*   **The 'Capacity' Metric:** A simple 'capacity' metric, defined by the representation's ability to fit random Gaussian data, robustly predicts parameter efficiency.
*   **Composability:** The scaling laws are valid for individual compression techniques and when multiple formats are combined.
*   **Algorithmic Improvement:** The framework enables the direct comparison of accuracy potential and the derivation of improved algorithms for training sparse-quantized models.

---

## üõ†Ô∏è Methodology

The authors investigated the interplay between scaling laws and compression techniques by validating a generalized scaling law formulation over compressed representations. They utilized a specific 'capacity' metric derived from fitting random Gaussian data to test predictability, allowing them to evaluate parameter efficiency across diverse formats and extend the framework to optimize training algorithms.

---

## ‚ú® Contributions

*   **Validation of General Scaling Laws:** Provided evidence that a unified scaling framework governs model performance for compressed representations individually and compositionally.
*   **Definition of a Predictive Metric:** Identified a 'capacity' metric based on random Gaussian data fitting that predicts parameter efficiency.
*   **Practical Frameworks for Comparison:** Developed a method to compare the inherent accuracy potential of different compressed formats.
*   **Optimized Training Algorithms:** Derived superior algorithms for training over sparse-quantized formats based on the scaling laws.

---

## ‚öôÔ∏è Technical Details

The paper proposes a unified scaling law and a novel method for calculating representation capacity.

**The Scaling Law Formula:**
`Loss(N, D) ~ A * (N * œÅ(R))^-Œ± + B * D^-Œ≤ + E`

**Capacity Metric:**
The capacity term `œÅ(R)` is derived from the representation's ability to fit random Gaussian data (GMSE(R)). It is modeled via the parametric function:
`e^œÅ(GMSE(R)) = L * tanh(F * log_{1/4}(GMSE(R)))^C`

**Optimization & Composition:**
*   **Optimizer:** Adam optimizer with the Straight-Through Estimator (STE).
*   **Composability:** Representation capacity is multiplicative across different compression types:
    `œÅ(R_combined) = œÅ(R_1) * œÅ(R_2)`

---

## üìà Results

*   **Fit Quality:** The Unified scaling law achieves comparable or superior fit quality compared to prior specialized laws, with fit errors of **4.7e-4** for sparsity and **2.1e-3** for quantization.
*   **Algorithmic Validation:** Experiments validated the framework's ability to predict algorithmic optimality, showing a near-perfect fit between Quantization-Aware Training runs and noise injection predictions.
*   **Lower Bound Confirmation:** Confirmed that the QuEST algorithm operates near the information-theoretic lower bound.
*   **Composability Validation:** The multiplicative property of capacity was validated with approximation errors generally around **10^-2**.
*   **Pruning Insights:** Additional analysis indicated that group count and outliers impact capacity, and backward pruning heuristics can outperform magnitude pruning baselines.