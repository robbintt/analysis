---
title: 'Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis'
arxiv_id: '2508.1102'
source_url: https://arxiv.org/abs/2508.11020
generated_at: '2026-02-03T19:23:52'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis

*Aakash Kumar; Emanuele Natale*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Core Focus:** Theoretical Deep Learning & Efficiency
> *   **Key Methods:** Number Partitioning Problem, Random Subset Sum
> *   **Precision:** Finite-precision / Quantized Networks
> *   **Result Type:** Theoretical Bounds & Guarantees

---

## Executive Summary

This research addresses the theoretical gap in applying the Strong Lottery Ticket Hypothesis (SLTH) to finite-precision, or quantized, neural networks. While SLTH has established that dense, randomly initialized networks contain subnetworks matching target performance in continuous settings, prior work relied heavily on approximation methods. This limitation is significant because quantization is a critical technique for deploying efficient models on edge devices, yet there has been a lack of rigorous theoretical justification for finding exact subnetwork representations within discrete weight constraints. The paper aims to resolve whether a randomly initialized network can be pruned to *exactly* represent a target quantized network without any approximation error.

The key innovation is the extension of the SLTH framework to discrete domains through a novel number-theoretic approach. Instead of tackling the real-valued Random Subset Sum Problem used in continuous settings, the authors leverage the Number Partitioning Problem and foundational results by Borgs et al. to handle discrete integers. They introduce a specific quantization operator and mixed-precision architectures (specifically $\gamma$-double and $\gamma$-triple mixed precision) where layer outputs are quantized. By transforming the Subset Sum Problem into a Number Partitioning Problem, the authors derive a mechanism that allows for the exact extraction of a target network from a random initialization, shifting the theoretical paradigm from approximation to exact representation.

The study provides rigorous mathematical bounds and theoretical guarantees rather than empirical validation. The authors established optimal overparameterization bounds, proving that the initial network size must scale linearly with the number of precision bits. Under this optimal scaling condition, the probability of successfully finding a valid subset sum to represent the target network is $1 - O(1/n^{1/7})$. Additionally, the work demonstrates exponential improvements over prior quantized bounds, confirming that the target discrete neural network is represented exactly via pruning with zero approximation error.

This work significantly bridges the theoretical understanding of two major efficiency techniques: quantization and pruning. By providing rigorous justification for low-precision (e.g., binary) networks, it offers a solid mathematical basis for constructing efficient models directly from untrained, overparameterized random networks. The shift from approximation-based proofs to exact representation marks a substantial advancement in theoretical deep learning, potentially influencing how researchers approach the design and initialization of hardware-efficient neural networks in the future.

---

## Key Findings

*   **Extension of SLTH to Quantized Settings:** The Strong Lottery Ticket Hypothesis (SLTH) framework has been successfully extended from continuous settings to finite-precision (quantized) neural networks.
*   **Exact Representation:** Unlike prior SLTH results in continuous settings that relied on approximation, this work demonstrates that target discrete neural networks in a quantized setting can be represented **exactly** via pruning.
*   **Optimal Overparameterization Bounds:** The authors proved optimal bounds regarding the necessary overparameterization of the initial random network, defining it as a function of the target network's precision.
*   **New Theoretical Insights for Subset Sum:** By leveraging the Number Partitioning Problem, the study derives new theoretical results for the Random Subset Sum Problem specifically within a quantized context.

---

## Technical Details

**Framework & Theory**
*   **SLTH Extension:** The paper proposes a theoretical framework extending the Strong Lottery Ticket Hypothesis (SLTH) to finite-precision networks, shifting from approximation-based proofs to exact representation.
*   **Mathematical Basis:** It leverages the Random Number Partitioning Problem (RNPP) and moment estimates regarding phase transitions to handle discrete integers instead of the real-valued Random Subset Sum Problem.

**Quantization Strategy**
*   **Weight Definition:** The strategy defines weights over a finite set and utilizes a specific quantization operator.
*   **Architectures:** Introduces mixed-precision architectures ($\gamma$-double and $\gamma$-triple mixed precision) where layer outputs are quantized.
*   **Problem Transformation:** The approach utilizes a problem transformation that equates the Subset Sum Problem to the Number Partitioning Problem to represent a target network exactly via pruning without training.

---

## Methodology

The authors utilized a theoretical approach that bridges number theory and neural network pruning. Specifically, they built upon foundational results established by Borgs et al. regarding the Number Partitioning Problem. They used these foundations to derive new theoretical results for the Random Subset Sum Problem adapted to a quantized setting. These mathematical derivations were then applied to generalize the Strong Lottery Ticket Hypothesis (SLTH) to accommodate finite-precision networks.

---

## Results

The key results consist of mathematical bounds and theoretical guarantees rather than empirical validation.

*   **Scaling Condition:** The paper derives an optimal scaling condition requiring the initial network size to scale linearly with the number of precision bits.
*   **Probability Guarantee:** Under this condition, the probability of finding a valid subset sum is $1 - O(1/n^{1/7})$.
*   **Exponential Improvement:** The work claims exponential improvements over prior quantized bounds.
*   **Exactness:** Establishes that the target discrete neural network is represented exactly (zero approximation error) via pruning.

---

## Contributions

*   **Bridging Quantization and Pruning Theory:** The work addresses a significant theoretical gap by showing how the SLTH applies to quantized networks, enhancing the theoretical understanding of efficiency techniques like quantization.
*   **Rigorous Justification for Low-Precision Networks:** It provides a theoretical basis for constructing extremely low-recision (e.g., binary) networks by pruning large, randomly initialized networks.
*   **Shift from Approximation to Exactness:** The contribution moves the field beyond approximation capabilities, offering a theoretical mechanism for the exact representation of discrete neural networks through pruning.