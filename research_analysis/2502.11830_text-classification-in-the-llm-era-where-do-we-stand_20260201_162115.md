# Text Classification in the LLM Era -- Where do we stand?
*Authors: Sowmya Vajjala; Shwetali Shimangaud*

<hr>

### ‚ö° Quick Facts

| Metric | Details |
| :--- | :--- |
| **Scope** | 32 Datasets across 8 Languages |
| **Models Evaluated** | Qwen2.5-7B, Aya23-8B, Aya-Expanse-8B, GPT-4, FastFit |
| **References** | 24 Citations |
| **Quality Score** | 5/10 |

---

## üìÑ Executive Summary

Modernizing NLP infrastructure forces organizations to balance the high computational costs of Large Language Model (LLM) inference against the expensive manual annotation required for training smaller, specialized models. This paper addresses the critical operational gap of whether resource-intensive supervised learning on human-labeled data still provides superior performance efficiency compared to prompt-based generative approaches. By rigorously evaluating the operational boundaries of LLMs, the study seeks to determine if contemporary generative models can truly supersede traditional, fine-tuned pipelines in text classification tasks.

The research introduces a comprehensive comparative framework that contrasts LLM-centric strategies against upper-bound baselines derived from fully supervised human-labeled datasets. Methodologically, the authors evaluate three distinct technical setups across 32 datasets spanning 8 languages: Zero-Shot Prompting using proprietary models (GPT-4) and open-source variants (Qwen2.5-7B, Aya23-8B, Aya-Expanse-8B) via the Instructor library; Few-Shot Fine-Tuning utilizing FastFit on the *paraphrase-multilingual-mpnet-base-v2* architecture with 10 examples per label; and a Synthetic Data strategy employing a multi-source aggregation of outputs from GPT-4, Qwen2.5-7B, and Aya-Expanse-8B. This granular approach allows for a precise analysis of trade-offs between generative inference and trained classifiers.

---

## üîç Key Findings

*   **Sentiment vs. General Tasks:** Zero-shot approaches demonstrate strong performance specifically for sentiment classification tasks but are generally outperformed by other methods for the remaining text classification tasks.
*   **Efficacy of Synthetic Data:** Classifiers built using synthetic data sourced from multiple LLMs achieve better performance than those relying solely on zero-shot open LLMs.
*   **Language Disparities:** Significant performance variations exist across the 8 languages evaluated, with disparities persisting regardless of the classification scenario (zero-shot, few-shot, or synthetic data).
*   **Prompt Language Nuance:** Counter-intuitively, using English prompts resulted in better performance across the board than prompts in the target dataset's native language.

---

## üõ†Ô∏è Methodology

The study conducted a comparative evaluation using **32 datasets spanning 8 languages**. The researchers contrasted Large Language Model (LLM) strategies against baseline classifiers constructed using complete human-labeled datasets derived from smaller pre-trained language models.

The study investigated three primary distinct strategies:
1.  **LLM Strategies:** Zero-shot classification, few-shot fine-tuning, and synthetic data-based classifiers.
2.  **Baselines:** Supervised classifiers trained on human-labeled data using smaller PLMs.
3.  **Comparison:** Analyzing the trade-offs between generative inference costs and training data requirements.

---

## ‚öôÔ∏è Technical Details

The analysis covers three distinct technical setups:

*   **Zero-Shot Prompting**
    *   **Models:** Qwen2.5-7B, Aya23-8B, Aya-Expanse-8B, and GPT-4.
    *   **Implementation:** Used English prompts and the `Instructor` library for structured output.
*   **Few-Shot Fine-Tuning**
    *   **Model:** `paraphrase-multilingual-mpnet-base-v2`.
    *   **Technique:** Utilized FastFit with 10 examples per label.
*   **Synthetic Data Generation**
    *   **Strategy:** Employed a multi-source approach using outputs from GPT-4, Qwen2.5-7B, and Aya-Expanse-8B.

---

## üìä Results

*   **General Performance:** Zero-shot LLMs perform competitively in sentiment classification but are generally outperformed by few-shot fine-tuning or classifiers trained on synthetic data for general tasks.
*   **Synthetic Data Superiority:** Classifiers using multi-source synthetic data outperform zero-shot open LLMs.
*   **Variance:** Evaluation across 32 datasets and 8 languages shows significant performance variance regardless of the method used.
*   **Language Effect:** English prompts yielded better results than prompts in the target dataset language.

---

## üí° Contributions

1.  **Broad Analysis:** Provided a broad comparative analysis of LLMs versus smaller pre-trained language models within the specific context of text classification.
2.  **Actionable Insights:** Offered actionable insights for developers regarding the selection of appropriate classification methodologies (e.g., when to use synthetic data versus zero-shot) and the pitfalls regarding language support.
3.  **Synthetic Data Validation:** Demonstrated the potential and limitations of using synthetic data generated by multiple LLMs as a replacement or enhancement for human-labeled training data.