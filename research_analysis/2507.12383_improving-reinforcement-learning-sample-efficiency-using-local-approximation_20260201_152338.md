# Improving Reinforcement Learning Sample-Efficiency using Local Approximation

*Mohit Prashant; Arvind Easwaran*

***

> ### **Quick Facts**
>
> *   **Quality Score**: 7/10
> *   **Core Algorithm**: Probabilistic Delayed Q-Learning (PDQL)
> *   **Primary Improvement**: Reduces sample-complexity to **O(SA log A)**
> *   **Key Innovation**: State Locality Principle (eliminating $\log S$ dependency)
> *   **References**: 27 citations

***

## Executive Summary

**Problem**
Reinforcement learning (RL) agents frequently suffer from sample inefficiency, requiring prohibitive amounts of interaction with their environment to converge on optimal policies, particularly within infinite-horizon settings. This inefficiency constrains the practical application of RL to domains where data collection is expensive, unsafe, or time-consuming. Existing theoretical solutions often scale poorly with the size of the state ($S$) and action ($A$) spaces. Specifically, many prior algorithms incur dependencies on the state-space size (e.g., $\log S$) that hinder their scalability when applied to complex, high-dimensional problems where exhaustive global sampling is infeasible.

**Innovation**
The core innovation presented is the **"State Locality Principle,"** a theoretical framework asserting that the value of a state is dependent significantly only on other states within its immediate transition vicinity. Leveraging this principle, the authors developed Probabilistic Delayed Q-Learning (PDQL), a novel model-free PAC-MDP algorithm. PDQL utilizes local approximation by decomposing the infinite-horizon Markov Decision Process (MDP) into smaller, manageable sub-MDPs constructed from state-space subsets. A key technical distinction is the separation of metric and transition distance: while value relevance is assumed to decay with metric distance, the algorithm formally proves that the sampling effort required to learn a state is independent of states separated by a sufficient number of transitions. This allows PDQL to treat sampling as a strictly localized property, avoiding the need for global state-space sampling to achieve local value convergence.

**Results**
The study establishes a rigorous sample-complexity bound of:
$$O\left(\frac{SA}{\epsilon^3 (1-\gamma)^3} \log\left(\frac{A}{\delta(1-\gamma)}\right) \log\left(\frac{1}{\epsilon}\right) \log\left(\frac{1}{\delta}\right)\right)$$

Most significantly, this results in an asymptotic complexity of **O(SA log A)**, marking a logarithmic factor improvement over existing literature by successfully eliminating the dependency on the state-space size ($\log S$). In direct comparisons, PDQL demonstrates superior efficiency over baselines; it avoids the high computational costs associated with global sampling while maintaining competitive dependencies on approximation accuracy ($\epsilon^{-3}$) and the discount factor horizon ($(1-\gamma)^{-3}$). Empirical validation confirmed that PDQL generalizes $\epsilon$-optimal solutions significantly faster than competing methods.

**Impact**
This paper makes a substantial theoretical contribution by formalizing the concept that value dependence and sampling requirements are localized properties within a metric state-space. By providing sharper Probably Approximately Correct (PAC) bounds that guarantee optimality without requiring exhaustive global sampling, the work challenges the scalability constraints of traditional RL approaches. The elimination of the $\log S$ dependency demonstrates that global state-space sampling is unnecessary for local convergence, paving the way for developing more scalable algorithms capable of operating efficiently in large state spaces. This advancement reduces the computational and resource barriers to deploying RL in real-world, complex scenarios.

***

## Key Findings

*   **Reduced Sample-Complexity**: The study successfully reduces the sample-complexity of reinforcement learning to **O(SA log A)** timesteps, representing a logarithmic factor improvement over existing literature.
*   **State Locality Principle**: There is an inverse relationship between transition distance and value dependence; values of states are only significantly relevant to other states within their immediate vicinity.
*   **Independent Sampling Effort**: The sample-complexity required to learn the optimal value of a state is independent of the samples needed for states that are a sufficient number of transitions away.
*   **Empirical Validation**: Experimental comparisons confirm that the proposed algorithm offers significant performance improvements over prior methods.

***

## Methodology

*   **Local Approximation**: The core method involves approximating the original infinite-horizon Markov Decision Process (MDP) using smaller MDPs constructed from subsets of the original state-space.
*   **Theoretical Decomposition**: The approach leverages the premise that state values and sample requirements are locally dependent (transition-wise) rather than globally uniform.
*   **Algorithm Design**: The authors constructed a specific PAC-MDP (Probably Approximately Correct Markov Decision Process) algorithm to extend these theoretical sample-complexity improvements to a model-free setting.

***

## Technical Specifications

| Component | Description |
| :--- | :--- |
| **Algorithm Name** | Probabilistic Delayed Q-Learning (PDQL) |
| **Type** | Model-free, PAC-MDP |
| **Core Mechanism** | Uses local approximation and state locality to improve sample-efficiency. |
| **State Space ($S$)** | Metric space where transitions are local. |
| **Transition Kernel** | Defined over states within one unit distance ($S'_s$), bounded by action space size ($A$). |
| **State Decomposition** | Constructs sub-MDPs from subsets of the state-space. |
| **Value Relevance** | Assumes the relevance of a state's value decays with distance. |
| **Sampling Assumption** | Sample complexity is independent of distant states. |
| **Policy Strategy** | Employs a greedy policy to maximize cumulative discounted rewards. |

***

## Results & Contributions

### Contributions
*   **Sharper PAC Bounds**: Derivation of new Probably Approximately Correct (PAC) bounds on asymptotic sample-complexity that are sharper than those currently available in literature for infinite-horizon MDPs.
*   **Model-Free Algorithm Development**: Introduction of a novel PAC-MDP algorithm capable of achieving the O(SA log A) sample-complexity in an infinite-horizon, model-free setting.
*   **Theoretical Insight**: Formalization of the concept that sampling effort and value relevance are localized properties within the state-space.

### Complexity Comparison
The following comparison highlights the performance of PDQL relative to established baselines regarding dependency on the approximation parameter ($\epsilon$) and the discount factor horizon ($1-\gamma$).

| Algorithm | $\epsilon$ Dependency | $(1-\gamma)$ Dependency | Log Dependency |
| :--- | :---: | :---: | :---: |
| **Delayed Q-Learning** | $O(\epsilon^{-4})$ | $O((1-\gamma)^{-8})$ | - |
| **Speedy Q-Learning** | $O(\epsilon^{-2})$ | $O((1-\gamma)^{-4})$ | - |
| **Variance Reduced Q-Learning** | $O(\epsilon^{-2})$ | $O((1-\gamma)^{-3})$ | - |
| **PDQL (Proposed)** | **$O(\epsilon^{-3})$** | **$O((1-\gamma)^{-3})$** | **$\log A$** |

**Note:** PDQL uniquely eliminates the logarithmic dependency on the state-space size ($\log S$), achieving a total asymptotic complexity of $O(SA \log A)$.