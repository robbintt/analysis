---
title: Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery
arxiv_id: '2601.20088'
source_url: https://arxiv.org/abs/2601.20088
generated_at: '2026-02-03T20:26:47'
quality_score: 7
citation_count: 10
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery

*Meng Xin; Sweta Priyadarshi; Jingyu Xin; Bilal Kartal; Aditya Vavre; Asma Kuriparambil Thekkumpate; Zijia Chen; Ameya Sunil Mahabaleshwarkar; Ido Shahaf; Akhiad Bercovich; Kinjal Patel; Suguna Varshini Velury; Chenjie Luo; Zhiyu Cheng; Jenny Chen; Chen-Han Yu; Wei Ping; Oleg Rybakov; Nima Tajbakhsh; Oluwatobi Olabiyi; Dusan Stosic; Di Wu; Song Han; Eric Chung; Sharath Turuvekere Sreenivas; Bryan Catanzaro; Yoshi Suhara; Tijmen Blankevoort; Huizi Mao*

---

## üìã Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Target Format** | NVFP4 (Nvidia 4-bit Floating Point) |
| **Baseline** | BF16 Performance Parity |
| **Core Method** | Quantization-Aware Distillation (QAD) |
| **Loss Function** | KL (Kullback-Leibler) Divergence |
| **Validated Architectures** | AceReason Nemotron, Nemotron 3 Nano, Llama Nemotron Super v1 |
| **Quality Score** | 7/10 |
| **References** | 10 Citations |

---

## üìù Executive Summary

The deployment of Large Language Models (LLMs) and Vision-Language Models (VLMs) is often hindered by the prohibitive memory and computational costs of high-precision inference. While Nvidia‚Äôs 4-bit Floating Point (NVFP4) offers a compelling efficiency solution, it introduces substantial accuracy degradation. Recovering this accuracy is particularly challenging for modern models that have undergone complex post-training pipelines, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF). Traditional Quantization-Aware Training (QAT) proves inadequate for these models, as direct fine-tuning in low precision leads to significant training instability and high engineering complexity, creating a barrier to the production adoption of 4-bit inference.

To address these instability and complexity issues, the paper introduces **Quantization-Aware Distillation (QAD)**, a novel method designed to recover inference accuracy for NVFP4 quantization. Unlike traditional QAT, which fine-tunes the quantized student model directly on raw data, QAD utilizes a distillation framework where a full-precision (BF16) "teacher" model guides the quantized "student." Technically, the method optimizes the student by minimizing the Kullback-Leibler (KL) divergence loss between the output distributions of the teacher and student. This allows the quantized model to replicate the behavior of the full-precision model without needing to relearn the underlying data distribution. Crucially, QAD is architected for data efficiency, functioning robustly even without access to the complete original training dataset or perfect data coverage.

Validation across diverse post-trained architectures‚Äîincluding AceReason Nemotron, Nemotron 3 Nano, and Llama Nemotron Super v1‚Äîdemonstrates that QAD successfully restores model accuracy to achieve **parity with the BF16 baseline**. The results indicate that QAD effectively negates the performance penalty typically associated with NVFP4 quantization. Comparative studies further establish QAD's superiority over traditional QAT, offering enhanced effectiveness and stability. Notably, the method maintained consistent accuracy recovery even when trained on imperfect or limited data subsets, proving its robustness against variations in data quality while avoiding the convergence failures common in QAT approaches.

This research establishes QAD as a superior operational best practice for NVFP4 inference accuracy recovery in modern LLMs and VLMs. By resolving the inherent instability and engineering complexity of applying traditional QAT to models derived from multi-stage post-training pipelines, QAD significantly lowers the barrier to quantization. This advancement enables organizations to deploy highly efficient 4-bit models without sacrificing the capabilities acquired during post-training, thereby facilitating the broader adoption of low-precision inference in production environments.

---

## üîë Key Findings

*   **Accuracy Recovery:** Quantization-Aware Distillation (QAD) successfully recovers the accuracy of Large Language Models (LLMs) and Vision-Language Models (VLMs) quantized to NVFP4, bringing them to near-BF16 performance levels.
*   **Superior Stability:** QAD demonstrates superior effectiveness and stability compared to traditional Quantization-Aware Training (QAT), particularly for models that have undergone complex, multi-stage post-training pipelines.
*   **Data Efficiency:** The approach is data efficient, robust to data quality and coverage, and capable of recovering accuracy without requiring access to the full original training dataset.
*   **Broad Validation:** The method was validated across a diverse range of post-trained architectures, including AceReason Nemotron, Nemotron 3 Nano, and Llama Nemotron Super v1, showing consistent results.

---

## ‚öôÔ∏è Methodology

The researchers utilized **Quantization-Aware Distillation (QAD)**, a technique that involves distilling knowledge from a full-precision teacher model into a quantized student model. The specific implementation relies on optimizing the model using a **KL (Kullback-Leibler) divergence loss function** to align the outputs of the quantized student with the full-precision teacher.

---

## üõ† Technical Details

| Aspect | Description |
| :--- | :--- |
| **Proposed Method** | Quantization-Aware Distillation (QAD) |
| **Target Quantization** | NVFP4 (Nvidia 4-bit Floating Point) |
| **Supported Models** | Large Language Models (LLMs) and Vision-Language Models (VLMs) |
| **Specific Architectures** | AceReason Nemotron, Nemotron 3 Nano, Llama Nemotron Super v1 |
| **Comparison** | Offers superior effectiveness and stability over traditional Quantization-Aware Training (QAT) |
| **Data Requirements** | Data efficient; robust to data quality variations; does not require full original training dataset |

---

## ‚ú® Contributions

*   **Operational Best Practice:** Establishes QAD as a superior operational best practice for NVFP4 inference accuracy recovery in modern LLMs.
*   **Workflow Compatibility:** Addresses the specific challenges of contemporary LLM workflows by providing a solution that avoids the training instability and engineering complexity inherent in applying traditional QAT to models derived from multi-stage pipelines.
*   **Lowered Barrier:** Contributes a method that lowers the barrier to quantization by being robust to imperfect data coverage, removing the strict requirement for full training data to maintain model accuracy.

---

## üìä Results

*   **Performance Parity:** Models quantized to NVFP4 using QAD achieve accuracy levels comparable to BF16 baseline performance.
*   **Stability:** The method demonstrates superior stability compared to traditional QAT.
*   **Consistency:** Results were consistent across diverse architectures (AceReason Nemotron, Nemotron 3 Nano, Llama Nemotron Super v1).
*   **Data Robustness:** Accuracy recovery was successful without the full original training dataset.

---

*Analysis Quality Score: 7/10 | References: 10 citations*