# Offline Constrained Reinforcement Learning under Partial Data Coverage

*Kihyuk Hong; Ambuj Tewari*

---

> ### ðŸ“Š Quick Facts
>
> *   **Proposed Algorithm:** PDOCRL (Primal-Dual Offline Constrained RL)
> *   **Sample Complexity:** $\tilde{O}((C^*)^2/((1-\gamma)^2 \epsilon^2))$ â€” achieving the optimal $\tilde{O}(\epsilon^{-2})$ rate.
> *   **Key Innovation:** Eliminates need for complex regularization via "All-Policy State-Value Realizability."
> *   **Data Requirements:** Partial data coverage (no fully exploratory data needed).
> *   **Extraction:** Distribution-free (no prior knowledge of data-generating distribution required).
> *   **Efficiency:** Oracle-efficient (avoids intractable searches).

---

## Executive Summary

### Problem
This research addresses the challenge of **Offline Constrained Reinforcement Learning (Offline CRL)**, a critical framework for deploying safe agents in environments where online interaction is risky or expensive. A major hurdle in this domain is the stringent data requirement for "full coverage"â€”the necessity that historical datasets cover all possible state-action pairs. Since real-world data is rarely exhaustive, existing theoretical solutions often fail because they rely on fully exploratory data or require complex regularization and prior knowledge of the data-generating distribution. This paper focuses on developing an algorithm that functions effectively under **"partial data coverage,"** where the dataset is limited, while maintaining theoretical guarantees on both performance and safety.

### Innovation
The authors propose **PDOCRL**, an "oracle-efficient" primal-dual algorithmâ€”meaning it achieves optimality by making computationally tractable calls to standard regression oracles, avoiding the intractable searches that plague previous methods. The core technical innovation is an **LP-based formulation over occupancy measures**, utilizing a specific realizability assumption (All-Policy State-Value Realizability). This assumption guarantees that all saddle points of the Lagrangian correspond to optimal solutions, effectively eliminating the need for the complex regularization techniques required by prior analyses. Furthermore, the method employs an **Importance Weight Transformation** that shifts optimization to Model Importance Weights. This allows the algorithm to estimate the Lagrangian and extract policies without requiring knowledge of the underlying transition kernel or the data-generating distribution.

### Results
The proposed algorithm achieves a sample complexity of $\tilde{O}((C^*)^2/((1-\gamma)^2 \epsilon^2))$, attaining the optimal rate of $\tilde{O}(\epsilon^{-2})$ for identifying an $\epsilon$-optimal policy. This represents a significant theoretical improvement over regularized approaches, which typically suffer from a slower convergence rate of $O(\epsilon^{-4})$; the $\epsilon^{-2}$ rate implies that the method requires substantially less data to achieve a target accuracy. In comparative evaluations against baselines such as PDCA, MBCL, and POCC, PDOCRL is the only method that supports partial data coverage while remaining oracle-efficient. It matches the best-in-class sample complexity of methods like POCC but distinguishes itself by removing the requirement to know the data distribution or utilize auxiliary function classes.

### Impact
This work significantly advances the field of offline RL by relaxing the data coverage conditions required for constrained policy optimization, making theoretical guarantees applicable to a wider range of real-world datasets. By demonstrating that regularization is unnecessary under appropriate realizability assumptions, the authors provide a cleaner theoretical framework that simplifies the proof of optimality. The elimination of dependencies on auxiliary function classes and the data-generating distribution bridges the gap between theoretical analysis and practical implementation, offering a more computationally efficient and viable path for deploying safe, constrained RL agents in environments with limited data.

---

## Key Findings

*   **Achieved Sample Complexity:** The proposed algorithm achieves a sample complexity of $O(\epsilon^{-2})$ for identifying an $\epsilon$-optimal policy, functioning effectively even under partial data coverage rather than requiring fully exploratory data.
*   **Elimination of Regularization:** By introducing a specific realizability assumption, the approach ensures that all saddle points of the Lagrangian are optimal, thereby removing the need for the complex regularization techniques required by previous analyses.
*   **Distribution-Free Policy Extraction:** The method successfully extracts policies without requiring prior knowledge of the data-generating distribution, significantly enhancing its practical applicability.
*   **Computational Efficiency:** The algorithm offers an oracle-efficient solution, overcoming the computational inefficiencies often associated with existing algorithms for offline constrained RL.

---

## Methodology

The authors utilize an oracle-efficient primal-dual algorithm grounded in a **linear programming (LP) formulation**. The approach relies on a **realizability assumption** regarding the function approximation, which guarantees that saddle points of the Lagrangian correspond to optimal solutions. **Lagrangian decomposition** is employed to facilitate the extraction of policies from the pre-collected dataset without accessing the underlying data distribution.

---

## Technical Details

The proposed algorithm (**PDOCRL**) utilizes the Linear Programming (LP) formulation of Reinforcement Learning, optimized for offline constrained settings by optimizing over occupancy measures $\mu$ instead of policies. The method employs Lagrangian relaxation to convert the constrained LP into a saddle point problem.

Key technical innovations include:

1.  **Elimination of Regularization Terms**
    The method utilizes 'Assumption B (All-Policy State-Value Realizability)' to ensure saddle point optimality, effectively bypassing the need for computational regularization.

2.  **Importance Weight Transformation**
    This component shifts optimization to Model Importance Weights (MIW) defined as $w = \mu/\mu_D$. This allows the algorithm to estimate the Lagrangian without knowledge of the transition kernel $P$.

3.  **Distribution-Free Extraction**
    The algorithm retrieves the policy directly from $\mu$, removing the necessity to know the data-generating distribution $\mu_D$.

---

## Contributions

*   **Relaxed Data Requirements:** This work significantly relaxes the data coverage conditions typically required for offline constrained RL, demonstrating that $O(\epsilon^{-2})$ sample complexity is possible without fully exploratory datasets.
*   **Simplified Analysis:** The research provides a cleaner theoretical framework by demonstrating that under realizability assumptions, regularization is unnecessary, simplifying the proof of optimality compared to prior works.
*   **Practical Algorithm Design:** By removing the dependency on auxiliary function classes and knowledge of the data-generating distribution, this contribution bridges the gap between theoretical guarantees and practical implementation in offline constrained settings.

---

## Results

PDOCRL achieves a sample complexity of $\tilde{O}((C^*)^2/((1-\gamma)^2 \epsilon^2))$, attaining an optimal rate of $O(\epsilon^{-2})$ compared to the $O(\epsilon^{-4})$ rate of regularized approaches.

**Comparative Analysis:**
When compared against baselines (PDCA, MBCL, POCC), PDOCRL demonstrates distinct advantages:
*   **Supports Partial Data Coverage:** Unlike methods requiring full coverage.
*   **Oracle Efficient:** Avoids the computational bottlenecks of previous algorithms.
*   **Reduced Dependencies:** Unlike POCC, it does not require knowledge of the data distribution $\mu_D$ or auxiliary function classes.
*   **Best-in-Class Performance:** Matches the superior sample complexity of $(C^*)^2/((1-\gamma)^2\epsilon^2)$ found in top-tier methods.

---

### Paper Metrics

*   **Quality Score:** 9/10
*   **References:** 40 citations