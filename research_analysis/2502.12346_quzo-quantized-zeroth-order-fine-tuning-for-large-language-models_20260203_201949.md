---
title: 'QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models'
arxiv_id: '2502.12346'
source_url: https://arxiv.org/abs/2502.12346
generated_at: '2026-02-03T20:19:49'
quality_score: 8
citation_count: 34
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models

*Jiajun Zhou; Yifan Yang; Kai Zhen; Ziyue Liu; Yequan Zhao; Ershad Banijamali; Athanasios Mouchtaris; Ngai Wong; Zheng Zhang*

***

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 34 |
| **Memory Reduction** | 2.94x (LLaMA2-7B) |
| **Precision Support** | INT8, INT4, FP8 |
| **Core Advantage** | Eliminates backpropagation & STE |

***

## üìù Executive Summary

Fine-tuning Large Language Models (LLMs) is computationally prohibitive for most applications due to the substantial memory footprint required for storing full-precision weights and computing gradients via backpropagation. While quantization offers a path to reduce memory, existing low-precision fine-tuning methods face critical accuracy degradation. This is primarily caused by the reliance on the Straight-Through Estimator (STE), which introduces significant estimator errors and bias when approximating gradients for discrete weights. Consequently, traditional first-order optimization often fails under extreme low-precision constraints (INT4/INT8), creating a barrier for memory-efficient model adaptation.

The authors introduce **QuZO (Quantized Zeroth-Order Optimization)**, the first framework designed for memory-efficient, full-weight fine-tuning of LLMs in extreme low-precision environments. QuZO eliminates the need for backpropagation and the problematic STE by operating directly on low-bit weights using only forward passes. The core technical contribution is a Quantized Randomized Gradient Estimator utilizing "Dual-Seed Stochastic Quantization." This method generates perturbation vectors using two independent seeds with optimized stochastic rounding. Because stochastic rounding is unbiased ($E[Q(x)] = x$), using two independent seeds ensures that the statistical expectation of the difference between perturbed points aligns with the true gradient, effectively canceling out the quantization bias inherent in STE methods.

Empirically, QuZO achieves superior accuracy compared to first-order methods in INT8 and INT4 settings while delivering comparable results in FP8 configurations. Crucially, the framework demonstrates performance parity with the MeZO baseline across a comprehensive range of tasks, including GLUE, Multi-Choice, and Generation tasks. On the LLaMA2-7B model, QuZO recovers FP16-level accuracy in INT4 settings, maintaining within 1% of full-precision performance (e.g., achieving ~92.6% on GLUE SST-2 compared to ~93.1% for full precision). In terms of hardware efficiency, fine-tuning LLaMA2-7B with QuZO reduces memory costs by **2.94x** compared to standard full-finetuning methods, a reduction attributed to the elimination of backward passes and the storage of high-precision gradients.

This research significantly broadens the feasibility of deploying and adapting LLMs on resource-constrained hardware. By proving that zeroth-order optimization can match or exceed the accuracy of first-order methods in low-bit regimes while maintaining parity with existing zeroth-order baselines like MeZO, QuZO challenges the conventional reliance on backpropagation for LLM fine-tuning. The successful removal of STE-related errors resolves a major bottleneck in quantized training. Consequently, QuZO establishes a new standard for memory-efficient full-weight fine-tuning, potentially enabling on-device customization of massive models without the need for enterprise-grade GPU clusters.

***

## üîë Key Findings

*   **Superior Low-Precision Accuracy:** QuZO achieves superior accuracy in **INT8** and **INT4** settings compared to first-order methods, while delivering comparable results in **FP8** settings.
*   **Performance Parity:** Delivers performance comparable to the MeZO baseline across GLUE, Multi-Choice, and Generation tasks.
*   **Significant Memory Reduction:** Reduces memory costs by **2.94x** during the fine-tuning of LLaMA2-7B.
*   **Error Mitigation:** Successfully mitigates estimator errors by avoiding the low-precision straight-through estimator (STE).

***

## üß™ Methodology

The authors propose the **Quantized Zeroth-Order (QuZO)** framework, designed to fine-tune LLMs using only low-precision (4- or 8-bit) forward passes. This approach fundamentally changes the optimization process by:

1.  **Eliminating Backpropagation:** Removing the need to store gradients or compute backward passes.
2.  **Avoiding STE:** Bypassing the low-precision straight-through estimator, which is a known source of bias in quantized training.
3.  **Optimized Stochastic Rounding:** Employing this technique to mitigate the increased bias caused by quantization, ensuring the optimization process remains stable at low bit-widths.

***

## ‚ú® Contributions

*   **Framework Introduction:** Introduction of **QuZO**, the first framework for zeroth-order fine-tuning of LLMs under extreme low-precision constraints (4- and 8-bit).
*   **Bias Mitigation Strategy:** Development of a novel bias mitigation strategy using optimized stochastic rounding to specifically address quantization bias.
*   **Empirical Validation:** Empirical demonstration that zeroth-order optimization can match or exceed first-order accuracy while achieving substantial memory efficiency gains (nearly 3x reduction) on large-scale models like LLaMA2-7B.

***

## ‚öôÔ∏è Technical Details

*   **Core Mechanism:** QuZO (Quantized Zeroth-Order Optimization) fine-tunes quantized LLMs using a zeroth-order optimizer directly on low-bit weights. It eliminates the need for backpropagation and the Straight-Through Estimator (STE).
*   **Quantized Randomized Gradient Estimator (RGE):** The method introduces a new estimator utilizing "Dual-Seed Stochastic Quantization."
*   **Mathematical Foundation:** This technique creates an unbiased estimator using two independently quantized perturbation vectors ($u_{i,1}$ and $u_{i,2}$) derived from the same source.
*   **Implementation Specs:**
    *   Utilizes stochastic rounding.
    *   Performs direct quantized weight updates.
    *   Requires **2n forward passes** per iteration (where $n$ is the dimension) without any backward passes.

***

## üìà Results

*   **Accuracy Comparison:**
    *   **INT8/INT4:** Superior accuracy compared to first-order methods.
    *   **FP8:** Comparable results to first-order methods.
*   **Benchmark Performance:** Performs comparably to MeZO on GLUE, Multi-Choice, and Generation tasks.
*   **Hardware Efficiency:**
    *   Fine-tuning LLaMA2-7B with QuZO reduces memory costs by **2.94x** compared to standard methods.
    *   This reduction is achieved by eliminating backpropagation.
*   **Robustness:** The method is robust for INT8 and INT4 formats, overcoming limitations of previous quantization attempts and avoiding STE-related estimator errors.

***
**References:** 34 citations