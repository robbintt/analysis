---
title: 'When Less is More: 8-bit Quantization Improves Continual Learning in Large
  Language Models'
arxiv_id: '2512.18934'
source_url: https://arxiv.org/abs/2512.18934
generated_at: '2026-02-03T18:46:46'
quality_score: 8
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models

*Michael S. Zhang; Rishi A. Ruia; Arnav Kewalram; Saathvik Dharmapuram; Utkarsh Sharma; Kevin Zhu*

---

> ### ðŸ“Œ Quick Facts
> 
> *   **Base Model**: LLaMA-3.1-8B
> *   **Precision Levels Tested**: FP16, INT8, INT4 (NF4)
> *   **Key Performance Gain**: 8-15% improvement in forward accuracy for quantized models
> *   **Replay Buffer Efficiency**: Quantized models require only 0.1%â€“2% buffer vs. higher needs for FP16
> *   **Hardware**: NVIDIA B200 (180 GB VRAM)
> *   **Paper Rating**: 8/10
> *   **Citations**: 26

---

## Executive Summary

Continual learning in Large Language Models (LLMs) faces the critical challenge of **catastrophic forgetting**, where the acquisition of new knowledge severely degrades performance on previously learned tasks. Conventionally, high-precision floating-point formats (FP16) are preferred to maximize model capacity and retention. However, this paper addresses the under-explored trade-off between numerical precision and the stability-plasticity dilemma.

This research is vital for deploying adaptive LLMs in dynamic environments where efficient resource utilization is key. The key innovation is the establishment of **quantization-induced noise as an implicit regularization mechanism** that enhances continual learning dynamics. Technically, the authors systematically evaluated LLaMA-3.1-8B using LoRA adapters across FP16, INT8, and INT4 (NF4) precisions within a multi-stage pipeline (NLU, Math, Code).

The authors hypothesized and demonstrated that the noise inherent in lower-precision quantization prevents the model from overfitting to the gradient descent of new tasks. By treating quantization not merely as a compression technique but as a structural regularizer, the study reveals how lower-bit representations can naturally mitigate the shift in task distributions without requiring explicit complex regularization algorithms.

Empirical findings indicate that quantized models outperform FP16 baselines in final task forward accuracy, with INT8 and INT4 achieving an **8-15% improvement**. In a striking departure from expected norms, **INT4 quantization nearly doubled performance in Code generation tasks** compared to FP16. While lower precision initially amplified forgetting in the absence of rehearsal, the introduction of minimal replay data reversed this trend.

This research significantly shifts the paradigm for efficient LLM deployment by proving that "less is more" regarding numerical precision in continual learning. It challenges the dogma that higher precision is universally better, providing actionable guidelines that recommend INT8 as the optimal balance for plasticity and retention.

---

## Key Findings

*   **Performance Improvement:** Quantized models (INT8, INT4) demonstrated an **8-15% improvement** over FP16 on final task forward accuracy, successfully overcoming initial performance gaps.
*   **Implicit Regularization:** Quantization acts as implicit regularization; the induced noise prevents overfitting to new task gradients, thereby mitigating catastrophic forgetting.
*   **Code Generation Boost:** INT4 quantization nearly doubled performance in Code generation tasks compared to the FP16 baseline.
*   **Buffer Efficiency:** Extremely small replay buffers (**0.1%**) significantly boost retention. INT8 offers the optimal balance of plasticity and retention, requiring significantly less replay data than FP16 models to achieve comparable results.

---

## Methodology

The researchers conducted a systematic investigation into the interplay between **quantization precision** (FP16, INT8, INT4) and **replay buffer strategies** within Large Language Models (LLMs).

*   **Evaluation Scope:** The parameters were evaluated across a continual learning setup involving Natural Language Understanding (NLU), Math, and Code generation tasks.
*   **Primary Metric:** The study focused on tracking **'forward accuracy'** to measure the trade-off between learning plasticity and knowledge retention.
*   **Pipeline:** The training followed a sequential 3-stage pipeline:
    1.  NLU
    2.  Math + Replay
    3.  Code + Replay

---

## Contributions

*   **Challenging Conventions:** The study challenges the conventional assumption that higher precision is universally preferable, showing that **INT8 correlates with superior continual learning dynamics**.
*   **Theoretical Framework:** It provides a theoretical framework hypothesizing that quantization-induced noise acts as **implicit regularization** to resist catastrophic forgetting.
*   **Practical Guidelines:** The paper offers practical deployment guidelines, recommending specific replay buffer sizes for different tasks and noting that quantized models require significantly less replay data than FP16 models to achieve comparable retention.

---

## Technical Details

| Parameter | Configuration |
| :--- | :--- |
| **Base Model** | LLaMA-3.1-8B |
| **Adapter** | LoRA (r=8, alpha=16, dropout=0.0) with frozen base weights |
| **Quantization** | FP16, INT8, and INT4 (NF4) via BitsAndBytes |
| **Replay Buffer** | Uniform random replay sizes ranging from 0% to 20% |
| **Training** | 1 epoch per dataset |
| **Hardware** | NVIDIA B200 (180 GB VRAM), 28 vCPUs |
| **Tasks** | NLU (8 datasets), Math (GSM8K), Code (HumanEval) |
| **Hyperparameters** | LR $2 \times 10^{-4}$, Batch size 8 (train) / 32 (eval) |

---

## Results

**Accuracy Comparison**
*   **INT8** performed within **1-2%** of the FP16 baseline.
*   **INT4** degraded by **3-5%** initially but showed superior recovery with replay.

**Impact of Replay Buffers**
*   **0% Replay:** NLU accuracy dropped significantly (INT4: 72.31% â†’ 42.50%; FP16: 77.26% â†’ 42.50%), indicating quantization amplifies forgetting without rehearsal.
*   **20% Replay:** The performance gap narrowed (INT4: 78.95%, FP16: 77.91%).

**Overall Performance**
*   Quantized models (INT8/INT4) showed **8-15% improvement** in forward accuracy over FP16.
*   **INT4 nearly doubled performance** in Code generation.

**Theoretical Implication**
*   Findings suggest quantization noise acts as an implicit regularizer.

**Recommendations**
*   **NLU:** Requires **1-2%** replay buffer.
*   **Math:** Needs **5-20%** replay (higher for lower precision).
*   **Code:** Needs **5-20%** replay.

---

## Paper Metadata

*   **Quality Score:** 8/10
*   **References:** 26 citations