# Applying Large Language Models in Knowledge Graph-based Enterprise Modeling: Challenges and Opportunities

*Benedikt Reitemeyer; Hans-Georg Fill*

***

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 6/10
> *   **References:** 40 Citations
> *   **Primary Model:** ChatGPT-4o
> *   **Target Languages:** ArchiMate, BPMN
> *   **Integration Strategies:** Ex-post Semantic Lifting, A-priori Generation
> *   **Key Metrics:** Prioritization Sequence (RQ1), Proposal Probability (RQ2), Relationship Typing (RQ3)

***

## Executive Summary

This research addresses the critical challenge of transitioning Large Language Models (LLMs) from academic theory to practical, industrial-grade applications in enterprise modeling. As enterprises seek to automate the generation of complex models defined by standards like ArchiMate and BPMN, the reliability and semantic consistency of LLMs remain significant barriers. The core issue involves bridging unstructured natural language with formal semantic representations to determine whether LLMs can independently handle the intricacies of enterprise architecture or if they require substantial human oversight to be viable in production environments.

The innovation lies in a robust Knowledge Graph (KG)-based framework that integrates ChatGPT-4o into the modeling pipeline to ground outputs in formal semantics. Technically, the approach structures data using RDF and JSON-LD formats, with logic enforced through the Web Ontology Language (OWL). The authors introduce a semantic framework distinguishing between "Type Semantics," "Inherent Semantics," and "Pragmatics," supplemented by an expert survey to gauge professional perspectives on deployment. Two distinct integration methodologies are proposed: Ex-post Semantic Lifting (where the LLM annotates existing models) and A-priori Generation (where KGs serve as direct input sources).

Experimental results comparing Human Experts and ChatGPT-4o focused on three specific metrics: Prioritization Sequence (RQ1), Proposal Probability (RQ2), and Relationship Typing accuracy (RQ3). The study yielded quantitative evidence indicating that LLM outputs exhibit minimal variability, demonstrating high consistency in generation sequences (RQ1). However, results also highlighted a performance trade-off: while the model proved reliable for specific, constrained tasks regarding Proposal Probability (RQ2), there was a measurable decline in reliability as task complexity increased, particularly in maintaining accuracy for complex relationship typing (RQ3) in enterprise architecture languages.

These findings significantly influence the field by redefining the role of LLMs from autonomous creators to essential "building blocks" within a broader machine-supported workflow. The evidence establishes that while LLMs offer a viable path toward industrial application, a strict "human-in-the-loop" methodology remains mandatory. Consequently, organizations must shift their implementation strategy to deploy LLMs as tools requiring expert supervision to navigate the limitations of task complexity and ensure the precision of the final enterprise architecture.

***

## Key Findings

*   **Output Consistency:** LLM-based enterprise model generations exhibit minimal variability in their output.
*   **Task Constraints:** The application of LLMs is currently constrained to specific tasks.
*   **Complexity vs. Reliability:** Reliability declines as task complexity increases.
*   **Human Necessity:** Human supervision and intervention by modeling experts are essential to ensure the accuracy and integrity of generated models.
*   **Industrial Shift:** LLMs function as a building block for machine-supported generation, marking a shift from purely academic research to industrial application.

***

## Methodology

*   **Knowledge Graph Approach:** Employed a knowledge graph-based framework for enterprise modeling.
*   **Expert Survey:** Conducted a survey to gauge professional perspectives on the technology.
*   **Experimental Model:** Performed experiments specifically utilizing ChatGPT-4o to test model generation capabilities.

***

## Technical Details

**Core Technologies**
*   **Primary Agent:** ChatGPT-4o
*   **Semantic Representation:** Knowledge Graphs (KG) and Ontologies (structured as triples)
*   **Logic Handling:** Web Ontology Language (OWL)

**Data Formats**
*   RDF
*   JSON-LD

**Semantic Framework**
*   **Type Semantics:** Meaning of elements.
*   **Inherent Semantics:** Labels and attributes.
*   **Pragmatics:** Context and user-specific usage.

**Integration Methodologies**
1.  **Ex-post Semantic Lifting (Annotation):** Annotating existing models.
2.  **A-priori Generation:** Using KGs directly as input sources.

**Target Domain & Tools**
*   **Languages:** ArchiMate, BPMN
*   **Tools:** ADONIS, SAP Signavio

***

## Results

The experimental study compared Human Experts against ChatGPT-4o in a task involving mapping ArchiMate modeling elements to domain descriptions. The study targeted the following metrics:

*   **RQ1: Prioritization Sequence:** LLM outputs exhibit minimal variability, showing high consistency.
*   **RQ2: Proposal Probability:** The model is reliable for specific, constrained tasks.
*   **RQ3: Relationship Typing:** A decline in reliability was observed as task complexity increased.

**Conclusion:** While LLMs show promise, human supervision and intervention are necessary to ensure accuracy and integrity as complexity grows.

***

## Contributions

*   **Role Evaluation:** Assessed the shifting role of LLMs in enterprise modeling from academic research to industrial implementation.
*   **Empirical Evidence:** Provided evidence regarding the limitations (task constraints and reliability) and consistencies (low variability) of LLMs in generating enterprise models.
*   **Workflow Validation:** Established the necessity of a human-in-the-loop workflow, validating that expert intervention is critical for maintaining model quality.