---
title: Effective Quantization of Muon Optimizer States
arxiv_id: '2509.23106'
source_url: https://arxiv.org/abs/2509.23106
generated_at: '2026-02-03T18:22:32'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Effective Quantization of Muon Optimizer States

*Aman Gupta; Rafael Celente; Abhishek Shivanna; D. T. Braithwaite; Gregory Dexter; Shao Tang; Hiroto Udagawa; Daniel Silva; Rohan Ramanath; S. Sathiya Keerthi*

---

> ### **Executive Summary**
>
> Training Large Language Models (LLMs) imposes significant memory demands, primarily due to the storage of optimizer states in full precision (FP32). Standard optimizers like AdamW require substantial memory overhead, which limits the maximum model size and batch size that can be trained on available hardware. While existing 8-bit quantization methods for AdamW reduce this footprint, they frequently suffer from instability, often necessitating dynamic quantization schemes to prevent training divergence. This paper addresses the critical challenge of reducing optimizer memory consumption without compromising training stability or the convergence quality of large-scale models.
>
> The authors introduce **8-bit Muon**, a novel implementation of the Muon optimizer that leverages blockwise quantization to compress optimizer states from FP32 to 8-bit. The approach utilizes a hybrid architecture, applying Muon to matrix-valued hidden parameters and AdamW to non-matrix parameters. Technically, Muon employs a momentum update followed by orthogonalization via Newton-Schulz iterations to approximate SVD, combined with a parameter update that incorporates decoupled weight decay and RMS scaling. A key innovation is the method's inherent robustness; unlike 8-bit AdamW variants that typically rely on dynamic quantization for stability, 8-bit Muon maintains stable convergence under both linear and dynamic quantization schemes.
>
> The proposed 8-bit Muon optimizer achieves a **74% reduction in memory footprint** compared to full-precision Muon and up to an **86% reduction** compared to standard 32-bit AdamW. In large-scale pre-training experiments involving a 1.6B parameter model on 4B FineWeb tokens, 8-bit Muon matched the validation loss of full-precision Muon while consistently outperforming both standard AdamW and 8-bit AdamW. Additionally, fine-tuning the Llama 3.2 3B model on post-training data demonstrated competitive performance against full-precision baselines, confirming that the quantization process does not degrade model quality.
>
> This research advances the field of efficient deep learning by demonstrating that low-precision optimizers can successfully match full-precision performance during LLM pre-training. By overcoming the stability limitations of 8-bit AdamW—specifically through support for linear quantization—the authors provide a more versatile and memory-efficient solution for training foundation models. The accompanying theoretical analysis of quantization robustness, combined with substantial empirical validation, suggests that 8-bit Muon is a viable and superior alternative for practitioners seeking to maximize hardware utilization without sacrificing model accuracy.

---

### ⚡ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Memory Reduction** | ~74% vs. FP32 Muon; ~86% vs. 32-bit AdamW |
| **Quantization Stability** | Supports both Linear and Dynamic schemes |
| **Model Scale Tested** | Up to 1.6B parameters (Pre-training) & 3B (Fine-tuning) |
| **Key Advantage** | Stable linear quantization (unlike 8-bit AdamW) |
| **Quality Score** | 9/10 |

---

## Key Findings

*   **Significant Memory Reduction:** The 8-bit Muon optimizer achieves approximately a **74% reduction** in memory footprint compared to full-precision Muon.
*   **Quantization Robustness:** Unlike 8-bit AdamW variants, which typically require dynamic quantization for stability, 8-bit Muon maintains stability under both **linear and dynamic quantization** schemes.
*   **Superior Pre-training Performance:** In experiments pre-training a 1.6B parameter model on 4B FineWeb tokens, 8-bit Muon matched the performance of full-precision Muon while outperforming both standard AdamW and 8-bit AdamW.
*   **Effective Fine-tuning:** The optimizer demonstrated competitive results when fine-tuning the Llama 3.2 3B model on post-training data.

---

## Methodology

The researchers developed an 8-bit variant of the Muon optimizer by applying **blockwise quantization** to the optimizer states. This implementation was designed to support both linear and dynamic quantization schemes.

The approach was validated through extensive experimentation, including:
1.  **Large-scale pre-training tasks** (1.6B model on 4B tokens).
2.  **Fine-tuning** of established models (Llama 3.2 3B).

Additionally, the authors provided a theoretical analysis to explain the observed robustness of Muon states under quantization.

---

## Technical Details

The approach employs a hybrid architecture and specific mathematical formulations to ensure efficiency and stability.

*   **Architecture:** Hybrid system using the Muon optimizer for matrix-valued hidden parameters and AdamW for non-matrix parameters.
*   **Muon Update Rule:**
    1.  **Momentum Update:** Standard momentum accumulation.
    2.  **Orthogonalization:** Via Newton-Schulz iteration to approximate SVD.
    3.  **Parameter Update:** Incorporates decoupled weight decay and RMS scaling (using $0.2 \cdot \sqrt{\max(m, n)}$).
*   **Quantization Scheme:**
    *   Reduces states from FP32 to 8-bit using blockwise quantization.
    *   **Linear Quantization:** Maps values to `[-127, 127]`.
    *   **Dynamic Quantization:** Allocates indices based on data density.
*   **Stability:** Unlike 8-bit AdamW, 8-bit Muon remains stable under both linear and dynamic quantization schemes.

---

## Results

*   **Memory Efficiency:** 8-bit Muon reduces optimizer state memory by approximately **74%** compared to full-precision Muon and up to **86%** compared to standard 32-bit AdamW.
*   **Pre-training Performance:**
    *   In experiments up to 1.6 billion parameters on the FineWeb dataset, it achieved validation loss within **1%-2%** of full-precision Muon for smaller models.
    *   For the 1.6B model, performance matched full-precision Muon, consistently outperforming AdamW and 8-bit AdamW.
*   **Fine-tuning Performance:** Fine-tuning Llama 3.2 3B on `tulu-3-sft-mixture` showed competitive performance against full-precision baselines.
*   **Robustness:** The method exhibits robustness to quantization, maintaining stability under both linear and dynamic schemes.

---

## Contributions

*   **Introduction of 8-bit Muon:** The first implementation of the Muon optimizer utilizing blockwise quantization to reduce memory overhead.
*   **Advancement in Quantization Stability:** Demonstrating that Muon can support stable linear quantization, overcoming a limitation common in 8-bit AdamW variants which rely primarily on dynamic quantization for stability.
*   **Empirical Validation:** Providing evidence that low-precision (8-bit) Muon can match full-precision performance in LLM pre-training while surpassing existing AdamW baselines.
*   **Theoretical Insight:** Contributing a theoretical perspective that aids in understanding the robustness of optimizer states when subjected to quantization.

---

**Quality Score:** 9/10  
**References:** 40 citations