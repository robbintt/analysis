---
title: Multi-Modal Continual Learning via Cross-Modality Adapters and Representation
  Alignment with Knowledge Preservation
arxiv_id: '2511.06723'
source_url: https://arxiv.org/abs/2511.06723
generated_at: '2026-02-03T19:07:10'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Multi-Modal Continual Learning via Cross-Modality Adapters and Representation Alignment with Knowledge Preservation

*Evelyn Chee; Wynne Hsu; Mong Li Lee*

---

> ### **Quick Facts Sidebar**
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Core Approach:** Pre-Trained Models (PTMs) + Mixture-of-Experts (MoE)
> *   **Key Innovation:** Cross-Modality Adapter with Frozen Experts
> *   **Top Performance (AVE Dataset):**
>     *   Class-Incremental Learning (CIL): **63.1%**
>     *   Domain-Incremental Learning (DIL): **68.4%**
> *   **Primary Scenarios:** Class-Incremental & Domain-Incremental Learning

---

## Executive Summary

**Multi-Modal Continual Learning (MMCL)** presents the complex challenge of learning from continuous streams of multi-sensory data (e.g., audio and video) without succumbing to catastrophic forgetting. The core difficulty lies in balancing **plasticity**—the ability to adapt to new classes or domains—with **stability**—the retention of existing knowledge—while maintaining robust cross-modal interactions over time. This research addresses a critical gap in leveraging frozen Pre-Trained Models (PTMs) for MMCL, aiming to exploit the rich representations of PTMs to mitigate performance degradation on previous tasks without the prohibitive computational cost of full model retraining.

The authors propose a framework built upon frozen PTMs, introducing three technical components to manage incremental learning:

1.  **Cross-Modality Adapter:** Utilizing a Mixture-of-Experts (MoE) structure inserted into transformer blocks; a gating network dynamically selects top experts for current tasks while freezing the weights of experts used in prior tasks to prevent overwriting.
2.  **Representation Alignment Loss:** Employs contrastive learning to align modality-specific features with a joint representation, ensuring distinct modal information is preserved.
3.  **Regularization of Representation Relationships:** Maintains previous task knowledge, combined with a composite loss function that includes Classification, Logits-Distillation (scaled by the square root of time steps), Alignment, and Preservation losses.

The framework achieves **state-of-the-art performance**, consistently outperforming established baselines including DER++, CoPe, LwF, and EWC across multiple datasets. On the AVE dataset in Class-Incremental Learning (CIL) scenarios, the model achieved an average accuracy of **63.1%**, significantly surpassing CoPe (58.2%) and DER++ (56.5%). In Domain-Incremental Learning (DIL) settings on the same dataset, the method attained **68.4%** accuracy, compared to 63.5% for the next best competitor. Ablation studies confirmed that cross-modality adapters provided enhanced accuracy over traditional modality-specific adapters, while the proposed representation alignment strategy successfully formed distinct feature clusters for different modalities, avoiding the mixed clusters observed with standard contrastive loss.

This work advances the field of continual learning by providing a comprehensive, pre-trained model-based solution specifically architected for the complexities of multi-modal data. By demonstrating that a Mixture-of-Experts approach with frozen experts can effectively decouple adaptation from preservation, the research establishes a new benchmark for MMCL.

---

## Key Findings

*   **Superior Performance:** The proposed framework consistently outperforms existing baseline methods across multiple multi-modal datasets.
*   **Scenario Versatility:** Achieves higher overall accuracy in both Class-Incremental Learning (CIL) and Domain-Incremental Learning (DIL) scenarios.
*   **Balanced Learning:** The model effectively balances adapting to new tasks with reduced catastrophic forgetting.
*   **Human-Like Perception:** Multi-modal continual learning yields substantial benefits by leveraging diverse sensory inputs akin to human perception.
*   **Adapter Efficacy:** Cross-modality adapters demonstrated enhanced accuracy compared to modality-specific adapters on the AVE dataset.

---

## Methodology

The research proposes a framework built upon **pre-trained models** tailored specifically for multi-modal continual learning. The methodology consists of three core components designed to handle the plasticity-stability trade-off:

1.  **Cross-Modality Adapter:** Features a mixture-of-experts (MoE) structure designed for efficient information integration across modalities.
2.  **Representation Alignment Loss:** A technique to foster robust multi-modal representations by aligning features.
3.  **Regularization of Representation Relationships:** A mechanism to preserve knowledge from previous tasks and mitigate catastrophic forgetting.

---

## Contributions

*   **Novel Framework Design:** Introduction of a comprehensive pre-trained model-based framework specifically addressing multi-modal continual learning.
*   **Architectural Innovation:** Development of a cross-modality adapter using a mixture-of-experts structure.
*   **Knowledge Preservation Technique:** Proposal of a combined strategy using representation alignment losses and relationship regularization to mitigate catastrophic forgetting.
*   **Empirical Validation:** Demonstrated state-of-the-art performance in both class-incremental and domain-incremental settings.

---

## Technical Details

### Architecture
*   **Base Models:** Utilizes frozen Pre-Trained Models (PTMs) for each modality to ensure stability.
*   **MMEncoder:** Consists of **B blocks**, determined by the shallowest PTM, with Transformer layers distributed evenly.

### Cross-Modality Adapters
*   **Structure:** Inserted into every block to capture interactions using a Mixture-of-Experts (MoE) structure with a gating network.
*   **Forgetting Prevention:** Experts used in earlier tasks have **frozen weights** to prevent forgetting.
*   **Gating Mechanism:**
    *   Averages input activations.
    *   Concatenates them.
    *   Generates contribution weights via softmax to activate top experts.
*   **Expert Processing:**
    *   Involves down-projecting activations with modality-specific weights.
    *   Generates attention maps using features from other modalities.

### Representation Strategy
*   **Alignment:** Aligns modality-specific representations with a joint representation using a **contrastive loss** to preserve modality-specific information.

### Loss Function
The total loss function combines four distinct components:
1.  **Classification Loss**
2.  **Logits-Distillation Loss:** Scales with the square root of time steps.
3.  **Alignment Loss**
4.  **Preservation Loss**

---

## Results

*   **Overall Accuracy:** The framework consistently outperforms existing baselines, balancing adaptation to new tasks with the reduction of catastrophic forgetting.
*   **CIL & DIL Performance:** Demonstrated superior performance in both Class-Incremental Learning (CIL) and Domain-Incremental Learning (DIL) scenarios.
*   **AVE Dataset Specifics:**
    *   Cross-modality adapters showed enhanced accuracy compared to modality-specific adapters.
    *   Representation alignment successfully preserved modality-specific information, forming distinct clusters (unlike standard contrastive loss which results in mixed clusters).
*   **Key Metrics Tracked:**
    *   Prediction Error
    *   Catastrophic Forgetting
    *   Overall Accuracy
    *   Representation Alignment Quality