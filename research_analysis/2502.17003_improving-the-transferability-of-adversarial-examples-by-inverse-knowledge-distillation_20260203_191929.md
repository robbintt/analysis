---
title: Improving the Transferability of Adversarial Examples by Inverse Knowledge
  Distillation
arxiv_id: '2502.17003'
source_url: https://arxiv.org/abs/2502.17003
generated_at: '2026-02-03T19:19:29'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation

*Wenyuan Wu; Zheng Liu; Yong Chen; Chao Su; Dezhong Peng; Xu Wang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Dataset:** ImageNet
> *   **Technique:** Inverse Knowledge Distillation (IKD)
> *   **Key Performance:** 15-20% improvement in Attack Success Rate (ASR) over baselines
> *   **Compatibility:** Integrates with FGSM, I-FGSM, and other gradient-based attacks

---

## Executive Summary

Current transfer-based adversarial attack algorithms struggle with a critical limitation: generated adversarial examples often fail to generalize effectively to unseen target models. This suboptimal transferability stems primarily from the discrepancy between the source (surrogate) model used to generate the attack and the actual target model. Existing methods tend to **overfit to the specific architectural nuances** and decision boundaries of the source model, resulting in adversarial samples that are ineffective in black-box scenarios where the target's internals are unknown.

Addressing this transferability gap is essential for accurately evaluating the robustness of deep learning systems. To overcome source model overfitting, the authors propose **Inverse Knowledge Distillation (IKD)**, a novel method that integrates seamlessly into standard gradient-based attack pipelines.

Unlike traditional knowledge distillation, which minimizes the difference between a student and teacher model, IKD **maximizes the disparity** between the surrogate model's behavior on benign inputs versus adversarial inputs. Technically, this is accomplished by augmenting the standard cross-entropy loss ($L_{hard}$) with a soft loss term ($L_{soft}$) utilizing KL Divergence.

The efficacy of IKD was validated through extensive experimentation on the **ImageNet dataset**. The results demonstrate that IKD significantly enhances the transferability of adversarial samples, achieving an Attack Success Rate (ASR) exceeding **80%** when transferring from ResNet-50 to Inception-v3 (an improvement of approx. 15-20% over standard baselines). This research sets a new benchmark for transfer-based attacks, emphasizing the need for defenses that account for generalized adversarial patterns.

---

## Key Findings

*   **Root Cause Identified:** Existing adversarial attack algorithms suffer from suboptimal performance primarily because they fail to account for the discrepancies between source and target models.
*   **Gradient Diversity:** The proposed Inverse Knowledge Distillation (IKD) method successfully promotes diversity in attack gradients, which mitigates the issue of adversarial samples overfitting to specific model architectures.
*   **Seamless Integration:** IKD integrates seamlessly with gradient-based attack methods, enabling the generation of adversarial samples with superior generalization capabilities across unseen models.
*   **Validated Success:** Extensive validation on the ImageNet dataset confirms that IKD significantly improves transferability and attack success rates in black-box attack scenarios.

---

## Methodology

The authors propose **Inverse Knowledge Distillation (IKD)**, a novel method designed to bridge the gap between source and target models. The core components of the methodology include:

1.  **Distillation-Inspired Loss:** The approach utilizes a distillation-inspired loss function that is integrated into standard gradient-based attack pipelines.
2.  **Gradient Diversification:** By focusing on diversifying the attack gradients, the methodology prevents the optimization process from overfitting to the specific architectural nuances of the source model.
3.  **Generalization Focus:** This ensures that the generated adversarial perturbations remain effective when transferred to other models, addressing the core challenge of adversarial robustness.

---

## Technical Details

The paper proposes Inverse Knowledge Distillation (IKD) to enhance the transferability of adversarial examples in black-box attacks by maximizing the disparity between a surrogate model's behavior on benign versus adversarial inputs.

### Optimization Objective
The optimization objective is defined as:

$$ \text{Maximize: } L_{hard}(f(x_{adv}), y) + \beta \cdot L_{soft}(f(x_{adv}), f(x)) $$

Where:
*   $L_{hard}$ is the standard **cross-entropy loss**.
*   $L_{soft}$ uses **KL Divergence** to maximize the distributional difference between the model's output on adversarial examples ($x_{adv}$) and benign examples ($x$).
*   $\beta$ is a weighting hyperparameter.

### Mechanism
*   **Enhanced Diversity:** This method enhances gradient diversity to prevent overfitting to the surrogate model.
*   **Directional Exploration:** It works by exploring directions that maximize the difference between benign and adversarial outputs.
*   **Constraint:** The optimization is subject to an $L_{\infty}$ norm constraint to ensure the perturbations remain imperceptible.

---

## Results & Performance

Validated on the ImageNet dataset, IKD demonstrated superior performance in several key areas:

*   **Higher Attack Success Rate (ASR):** IKD achieves a higher ASR in black-box scenarios compared to standard methods.
*   **Superior Generalization:** The method demonstrates superior generalization capabilities and successfully mitigates overfitting to the surrogate model's architecture.
*   **Compatibility:** IKD is compatible with existing gradient-based attack methods such as **FGSM** and **I-FGSM**.
*   **Quantitative Metrics:**
    *   **ResNet-50 $\to$ Inception-v3:** Achieved an ASR exceeding **80%** (approx. 15-20% improvement over MI-FGSM).
    *   **Standard $\to$ VGG-19:** Improved transferability from roughly 50% with standard methods to over **70%**.

---

## Contributions

*   **Novel Technique:** Introduction of the Inverse Knowledge Distillation (IKD) technique as a solution to the limitations of current transferability-based adversarial attacks.
*   **Strategic Approach:** A strategic approach to reduce overfitting to source model architectures by enforcing gradient diversity, addressing a core challenge in adversarial robustness.
*   **Demonstrated Efficacy:** Demonstrated substantial improvements in the effectiveness of black-box attacks, contributing to the understanding of model vulnerability and security.
*   **Comprehensive Validation:** Provided comprehensive evidence of efficacy through extensive experiments on the complex ImageNet dataset across a wide variety of model architectures (e.g., ResNet, VGG, DenseNet, Inception).

---
**References:** 40 citations