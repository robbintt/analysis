---
title: Scaling Embeddings Outperforms Scaling Experts in Language Models
arxiv_id: '2601.21204'
source_url: https://arxiv.org/abs/2601.21204
generated_at: '2026-02-03T18:39:20'
quality_score: 8
citation_count: 9
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Scaling Embeddings Outperforms Scaling Experts in Language Models

*Hong Liu; Jiaqi Zhang; Chao Wang; Xing Hu; Linkun Lyu; Jiaqi Sun; Xurui Yang; Bo Wang; Fengcun Li; Yulei Qian; Lingtong Si; Yerui Sun; Rumei Li; Peng Pei; Yuchen Xie; Xunliang Cai*

---

> ### **Quick Facts**
> *   **Model Name:** LongCat-Flash-Lite
> *   **Total Parameters:** 68.5 Billion
> *   **Active Parameters:** ~3 Billion
> *   **Activation Ratio:** ~4.3%
> *   **Quality Score:** 8/10
> *   **Citations:** 9

---

## Executive Summary

As language models scale to accommodate complex reasoning tasks, the computational cost of inference becomes a critical bottleneck. The industry standard for addressing this is **Mixture-of-Experts (MoE)**, which activates only a subset of network parameters per token. However, MoE architectures introduce significant complexities, including routing overhead and memory management challenges.

This paper investigates whether an alternative approach—**scaling the model's embedding layers** rather than its expert layers—can provide a superior performance-efficiency trade-off, challenging the prevailing dominance of MoE in large-scale model training.

The authors propose "Embedding Scaling" as an orthogonal dimension for sparsity, centered on a novel **N-gram Embedding Layer** that processes sequences of $N$ tokens rather than single tokens. This architecture allows for a massive increase in parameter count without a proportional increase in active computation. The study is distinguished by its methodological rigor, employing a systematic analysis of architectural variables—including parameter budgeting, model width, and depth—to characterize optimal scaling regimes.

To realize practical benefits, the paper introduces **"Embedding Amplification,"** a training optimization technique designed to stabilize the learning of these massive embedding tables, alongside tailored system-level optimizations. These optimizations include speculative decoding specifically designed for N-grams and custom CUDA kernels, effectively converting the theoretical sparsity of large embeddings into tangible inference acceleration.

The researchers introduced **LongCat-Flash-Lite**, a 68.5 billion parameter model with an extremely low activation ratio of approximately 4.3% (~3 billion active parameters). This model outperformed specific MoE baselines on the Pareto frontier, achieving a better balance between computational efficiency and task performance.

This work establishes embedding scaling as a potent and viable alternative to Mixture-of-Experts, effectively expanding the design space for efficient large language models. By demonstrating that the majority of parameters can be allocated to embeddings while maintaining or improving performance on complex tasks, the authors challenge the assumption that conditional computation must rely on expert routing.

---

## Key Findings

*   **Superior Trade-off:** Embedding scaling achieves a superior performance-efficiency trade-off (Pareto frontier) compared to Mixture-of-Experts (MoE) in specific regimes.
*   **Architectural Dependence:** The efficacy of embedding scaling is governed by architectural factors, specifically parameter budgeting and the interplay between model width and depth.
*   **Inference Acceleration:** Sparsity inherent in embedding scaling can be converted into tangible inference speedups through tailored system optimizations and speculative decoding.
*   **Model Performance:** LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated parameters, surpasses MoE baselines and is highly competitive in agentic and coding domains.

---

## Methodology

The authors utilized a comprehensive analysis and experimental framework to compare sparsity scaling strategies. Their approach included:

1.  **Systematic Characterization:** Analyzing the impact of architectural variables such as parameter budgeting, width, and depth.
2.  **System Implementation:** Implementing tailored system optimizations and speculative decoding techniques to realize practical speedups.
3.  **Model Training:** Training the 68.5B parameter LongCat-Flash-Lite model from scratch, specifically designed to leverage massive embedding parameters based on the insights gained.

---

## Technical Details

The paper proposes 'Embedding Scaling' as an alternative to Mixture-of-Experts (MoE). Below are the core technical components:

*   **Core Architecture:** N-gram Embedding Layer designed to process sequences of N tokens.
*   **Optimization Techniques:**
    *   **Collision Handling:** Managed via vocabulary sizing.
    *   **Embedding Amplification:** A specific training optimization to stabilize learning.
    *   **Integration:** Per-layer embedding integration.
*   **Scaling Properties:**
    *   **Width:** Enhanced advantages in wider models.
    *   **Depth:** Diminishing returns observed in deeper models.
*   **System Optimizations:**
    *   Sparsity utilization.
    *   Speculative decoding tailored for N-grams.
    *   Optimized CUDA kernels.

---

## Contributions

*   **New Scaling Dimension:** Establishes embedding scaling as a potent and orthogonal dimension for sparsity scaling, offering a viable alternative to Mixture-of-Experts (MoE).
*   **Bridging Theory and Practice:** Demonstrates how to bridge the gap between theoretical sparsity and practical application by converting embedding sparsity into real inference acceleration.
*   **State-of-the-Art Baseline:** Introduces LongCat-Flash-Lite as a new state-of-the-art baseline, proving that allocating a significant majority of parameters to embeddings can yield superior results in complex tasks like coding and agentic workflows.

---

## Results

The LongCat-Flash-Lite model (68.5B total parameters, ~3B activated parameters, ~4.3% activation ratio) demonstrates the following:

*   **Performance-Efficiency:** A superior performance-efficiency trade-off compared to MoE baselines on the Pareto frontier.
*   **Task Performance:** Achieved highly competitive results in **Agentic** and **Coding** tasks, surpassing the provided MoE baselines.
*   **Inference Speed:** Inference speedups were attributed to sparsity and kernel optimizations.
*   **Architectural Analysis:** Quantitative analysis confirms embedding scaling provides disproportionately higher benefits for wider models, while deeper models show saturation.