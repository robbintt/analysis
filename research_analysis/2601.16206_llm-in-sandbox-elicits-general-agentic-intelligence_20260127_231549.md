---
title: LLM-in-Sandbox Elicits General Agentic Intelligence
arxiv_id: '2601.16206'
source_url: https://arxiv.org/abs/2601.16206
generated_at: '2026-01-27T23:15:49'
quality_score: 9
citation_count: 20
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-in-Sandbox Elicits General Agentic Intelligence

*Huatong Song, Shaohan Huang, Wayne Xin, Daixuan Cheng, Guoxin Chen, Li Dong, Yuxian Gu, Rong Wen*

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 20 |
| **Environment** | Docker Container (~1.1 GB Ubuntu) |
| **Key Mechanism** | ReAct Loop / Code Sandbox |
| **Best Performance Gain** | +24.2% (Mathematics) |
| **Training Approach** | Zero-shot & Non-Agentic RL |

---

## üìë Executive Summary

Current Large Language Models (LLMs) are primarily designed for static textual processing, a limitation that hinders their ability to solve complex, real-world problems requiring external resource access, long-term context management, or interaction with system tools. While LLMs excel at code generation, their potential to act as general-purpose agents capable of autonomous reasoning in non-code domains‚Äîsuch as scientific analysis or multi-step problem solving‚Äîremains underexploited.

This paper addresses the critical challenge of eliciting "general agentic intelligence," investigating whether LLMs can autonomously utilize interactive environments to solve tasks beyond their training data without the need for extensive, task-specific fine-tuning. The authors propose **"LLM-in-Sandbox,"** a novel framework that confines an LLM within a lightweight Docker container (running a virtual Ubuntu environment of ~1.1 GB) to solve non-code tasks interactively. The system operates on a ReAct loop, granting the model access to tools such as `execute_bash`, file editors, and code interpreters.

A key technical advancement is the introduction of **"LLM-in-Sandbox-RL,"** a specialized reinforcement learning protocol designed to enhance sandbox exploration capabilities. Distinctively, this RL method utilizes only non-agentic data for training, allowing the model to improve its ability to navigate system tools and manage resources efficiently in post-trained settings. The framework requires models to autonomously install domain-specific dependencies (e.g., OpenJDK 21) at runtime, ensuring a flexible and dynamic interaction environment.

Extensive experiments across Mathematics, Physics, Chemistry, Biomedicine, Long-Context Understanding, and Instruction Following demonstrated substantial performance gains in zero-shot settings. High-capacity models showed marked improvements, including Qwen3-Coder-30B-A3B (+24.2% in Mathematics), MiniMax-M2 (+14.4% in Chemistry), and DeepSeek-V3.2-Thinking (+14.4% in Instruction Following). The LLM-in-Sandbox-RL protocol successfully validated its ability to enhance agentic exploration capabilities in post-trained settings. Conversely, the framework revealed a reliance on model strength, as weaker models like Qwen3-4B-Instruct suffered performance degradation (-25.0% in Long-Context Understanding).

The study also provided a systematic analysis of efficiency, highlighting that the framework's minimal shared image allows for effective resource management without static pre-installation of all dependencies. This research provides compelling empirical evidence that the agentic capabilities necessary for tool use and environmental interaction are innate to strong LLMs and can be elicited without explicit training. By bridging the gap between static text generation and dynamic system interaction, the LLM-in-Sandbox framework establishes a new paradigm for developing generalist agents capable of operating across diverse scientific and technical domains. The accompanying open-source Python package facilitates immediate deployment and system-level integration, paving the way for future AI systems that can autonomously leverage computational resources to solve complex, multi-faceted problems.

---

## üîç Key Findings

*   **Innate Generalization:** Strong LLMs possess the innate ability to generalize code sandbox usage for non-code tasks, spontaneously using the environment for external resources and context management without additional training.
*   **Broad Performance Gains:** The LLM-in-Sandbox framework achieves significant performance improvements across diverse domains, including mathematics, physics, chemistry, biomedicine, and long-context understanding.
*   **Effective RL Training:** Non-agentic Reinforcement Learning training (LLM-in-Sandbox-RL) successfully enhances agentic capabilities specifically for sandbox exploration.
*   **Robust Flexibility:** The framework demonstrates robust generalization and performance improvements in both training-free (zero-shot) settings and post-trained settings.

---

## üèóÔ∏è Methodology

The methodology centers on shifting LLMs from static textual processors to dynamic agents through the following approaches:

*   **Sandbox Environment:** The approach places LLMs inside a code sandbox (a virtual computer/Docker container) to enable interactive exploration.
*   **Contextual Learning:** It leverages the generalization capabilities of pre-trained strong LLMs in a training-free setting to interact with the sandbox in-context.
*   **Specialized RL Protocol:** For post-training enhancement, the method utilizes 'LLM-in-Sandbox Reinforcement Learning' (LLM-in-Sandbox-RL). This protocol is designed to improve sandbox exploration skills using **only non-agentic data**.
*   **Validation:** The method is validated through extensive experiments spanning scientific and reasoning tasks, with rigorous analysis on computational and system efficiency.

---

## ‚öôÔ∏è Technical Details

**The Sandbox Environment:**
*   **System:** Docker container running Ubuntu.
*   **Image Size:** Minimal single shared image (~1.1 GB).
*   **Pre-installed Software:** Python and standard scientific libraries.
*   **Runtime Requirements:** Models must autonomously install domain-specific tools as needed.

**Available Tools:**
*   `execute_bash`: For external resource access and command line operations.
*   `str_replace_editor`: For file management and edits.
*   Code Execution: For running scripts.
*   Task Submission: For finalizing output.

**Workflow Architecture:**
*   **Framework:** Based on the **ReAct** (Reason + Act) framework.
*   **Loop:** The system loops between tool generation and execution until a submission event occurs or a turn limit is reached.
*   **I/O:** Inputs provided via prompt or pre-placed files; final outputs written to a specific file path.

---

## üìà Results

Experiments were conducted across Mathematics, Physics, Chemistry, Biomedicine, Long-Context Understanding, and Instruction Followin tasks, comparing the framework against Vanilla LLM baselines.

### Performance Highlights (Zero-Shot)

| Model | Domain | Performance Change |
| :--- | :--- | :--- |
| **Qwen3-Coder-30B-A3B** | Mathematics | **+24.2%** |
| **MiniMax-M2** | Chemistry | **+14.4%** |
| **DeepSeek-V3.2-Thinking** | Instruction Following | **+14.4%** |
| **Qwen3-4B-Instruct** | Long-Context Understanding | **-25.0%** |

**Qualitative Analysis:**
*   Successful autonomous installation of dependencies (e.g., OpenJDK 21).
*   Effective use of command-line tools for context management.
*   **Note:** Weaker models (e.g., Qwen3-4B) struggled with the complexity of the environment, resulting in performance drops compared to static processing.

---

## ‚ú® Core Contributions

1.  **Novel Framework:** Introduces 'LLM-in-Sandbox,' a system that unlocks general agentic intelligence in non-code domains by leveraging code execution environments.
2.  **Empirical Evidence:** Provides proof that LLMs can spontaneously and autonomously use complex system tools for non-coding purposes without explicit training.
3.  **Innovative Training Method:** Proposes a novel reinforcement learning method that enhances exploration skills using non-agentic data.
4.  **Open Source Deployment:** Contributes an open-source Python package to facilitate the real-world deployment and system-level integration of agentic LLMs.