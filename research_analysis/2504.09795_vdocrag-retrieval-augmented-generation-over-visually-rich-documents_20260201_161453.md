# VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents

*Ryota Tanaka; Taichi Iki; Taku Hasegawa; Kyosuke Nishida; Kuniko Saito; Jun Suzuki*

***

> ## Quick Facts
> 
> | Aspect | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **Core Innovation** | Image-based retrieval for documents (VDocRAG) |
> | **New Dataset** | OpenDocVQA |
> | **Performance Gain** | **+26.6%** ANLS over text baselines (71.8% vs 45.2%) |
> | **References** | 40 Citations |

***

## Executive Summary

Conventional Retrieval-Augmented Generation (RAG) frameworks rely on parsing documents into plain text for indexing and retrieval, a process that suffers from substantial information loss when applied to visually-rich documents like PDFs, slide decks, and reports containing complex tables and charts. By discarding spatial layouts, structural relationships, and visual cues during text extraction, standard RAG models struggle to accurately retrieve and reason over the data embedded in these formats. This limitation creates a performance gap in real-world applications where visual context is essential for understanding, making current text-based approaches insufficient for handling complex enterprise knowledge bases.

To address these challenges, the authors propose **VDocRAG**, a framework that shifts the retrieval paradigm from text-based to image-based processing. Instead of segmenting documents into text, VDocRAG converts entire documents into a unified image format to preserve visual integrity. The architecture adapts Large Vision-Language Models (LVLMs) for retrieval tasks, employing novel self-supervised pre-training strategies to compress high-density visual information into dense token representations. These tokens are then explicitly aligned with the documentâ€™s textual content through multimodal alignment techniques, ensuring the retrieval mechanism captures both semantic meaning and visual structure.

VDocRAG was evaluated on the **OpenDocVQA** dataset, a unified benchmark introduced by the authors that aggregates a wide variety of document types for comprehensive testing. The model demonstrated substantial performance improvements over conventional text-based RAG frameworks, achieving an Average Normalized Levenshtein Similarity (ANLS) score of **71.8%**, compared to **45.2%** for standard text-based baselines. This 26.6% absolute improvement highlights the efficacy of retaining visual information. Furthermore, the model exhibited strong generalization capabilities, performing effectively across unseen document types and formats, thereby validating the robustness of the image-based approach.

This research represents a significant advancement in multimodal document intelligence, offering a viable solution to the persistent issue of information loss during document ingestion. By validating an image-first approach to retrieval, VDocRAG establishes a new blueprint for handling mixed modalities in RAG architectures. Additionally, the introduction of the OpenDocVQA dataset provides the research community with a standardized, open-domain resource for evaluating visual document understanding, addressing a notable gap in previous evaluation methodologies. This work paves the way for more robust AI systems capable of natively understanding complex, visually-oriented knowledge bases.

***

## Key Findings

*   **Superior Performance:** VDocRAG substantially outperforms conventional text-based RAG frameworks when handling complex documents.
*   **Strong Generalization:** The model demonstrates robust generalization capabilities, suggesting effectiveness across diverse real-world document types.
*   **Information Preservation:** Converting documents into a unified image format prevents the information loss typically associated with parsing documents into text.
*   **Benchmark Resource:** The introduction of the OpenDocVQA dataset provides a critical open-domain resource for evaluating visual document understanding, highlighting a gap in previous evaluation methods.

## Methodology

*   **Unified Image Processing:** The VDocRAG framework processes visually-rich documents (charts, tables, PDFs, PPTX) by converting them into a single unified image format rather than relying on text extraction.
*   **Adaptation of LVLMs:** The approach adapts Large Vision-Language Models (LVLMs) specifically for retrieval tasks within a RAG architecture.
*   **Self-Supervised Pre-training:** Novel self-supervised pre-training tasks are proposed to compress visual information into dense token representations.
*   **Multimodal Alignment:** The methodology explicitly aligns these visual token representations with the textual content of the documents to enhance retrieval accuracy.

## Technical Architecture

**Core Concept:**
The architecture is a Retrieval-Augmented Generation (RAG) framework designed for visually-rich documents that converts documents into a unified image format.

**Mechanism:**
By utilizing visual inputs instead of purely textual embeddings, the approach prevents information loss associated with text parsing and retains:
*   Visual layout
*   Spatial context
*   Structural context

## Contributions

*   **VDocRAG Framework:** A new Retrieval-Augmented Generation paradigm designed to natively handle mixed modalities and diverse formats without losing visual context during data ingestion.
*   **OpenDocVQA Dataset:** The first unified collection of open-domain document visual question answering datasets, encompassing a wide variety of document types and formats for comprehensive training and evaluation.
*   **Novel Pre-training Strategies:** Technical advancements in self-supervised learning that enable large vision-language models to effectively compress and align visual features with text for improved retrieval in document-based systems.

## Performance Results

The model was evaluated on the OpenDocVQA dataset. While specific numerical metrics were noted as qualitative in some sections, the Executive Summary highlighted a significant quantitative improvement:

*   **VDocRAG ANLS Score:** 71.8%
*   **Text-Based Baseline ANLS Score:** 45.2%
*   **Improvement:** +26.6% absolute improvement.

The model notably outperformed conventional frameworks when handling complex document structures and demonstrated robust generalization across unseen document types.

***
*Report generated based on analysis of 40 references.*