---
title: 'GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying
  LLM Agents'
arxiv_id: '2511.00802'
source_url: https://arxiv.org/abs/2511.00802
generated_at: '2026-02-03T13:46:40'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents

*Jie JW Wu; Ayanda Patrick Herlihy; Ahmad Saleem Mirza; Ali Afoud; Fatemeh Fard*

---

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Proposed Framework** | `two_agent` |
> | **Reliability Rate** | 100% (0% compilation failures) |
> | **Avg. Optimization Improvement** | 106.7% |
> | **Success Rate** | 45% (Tied with CrewAI) |
> | **Baseline Comparison** | Outperformed AutoGen (34%) |
> | **Key Libraries** | Open Bandit Pipeline (OBP), Scope-RL |

---

## Executive Summary

Off-Policy Evaluation (OPE) is a critical technique for enabling offline A/B testing in high-stakes domains like healthcare and finance, where deploying suboptimal policies online poses significant risks. However, optimizing OPE estimators to minimize relative estimation error remains a complex, manual, and resource-intensive challenge. This paper addresses the need to automate the optimization of OPE systems, aiming to enhance performance without the financial and safety costs associated with live online experimentation. The authors specifically tackle the difficulty of optimizing standard estimators within a contextual bandit setting using historical data.

The key innovation is "**GrowthHacker**," a novel framework that utilizes Large Language Model (LLM) agents to perform automated code modification for OPE optimization. The authors formulate the optimization task as minimizing relative estimation error, formulating it as a code optimization task $(C, \theta_0, D)$ over Direct Method (DM), Inverse Probability Weighting (IPW), and Doubly Robust (DR) estimators. To reduce system complexity while preserving optimization effectiveness, the researchers developed a specialized two-agent architecture: a Prompter/Analyzer Agent identifies code weaknesses and generates modification instructions, while a Coder Agent implements these changes to ensure functional correctness. This iterative loop leverages the Open Bandit Pipeline (OBP) and Scope-RL libraries and is validated using large-scale real-world datasets, distinguishing it from more generalized approaches.

The proposed two-agent framework demonstrated significant robustness and effectiveness compared to established multi-agent baselines like CrewAI and AutoGen. It achieved a **100% reliability rate** with zero compilation failures and an average optimization improvement of **106.7%**. In terms of success ratesâ€”the frequency of achieving performance gainsâ€”the two-agent framework and CrewAI both reached 45%, substantially outperforming the AutoGen baseline at 34%. The study highlighted the instability of general-purpose frameworks, noting that baselines experienced compilation errors exceeding 20% due to deprecated syntax, incorrect API usage, and context degradation, whereas GrowthHacker maintained consistent execution based on relative estimation error and Mean Squared Error (MSE).

This research is the first to investigate and validate the use of code-modifying LLM agents specifically for optimizing OPE results, establishing a new intersection between agentic AI and causal inference. By introducing "The GrowthHacker Benchmark," the authors provide a standardized resource utilizing large-scale real-world data for the community to evaluate automated OPE methods. This work provides a pathway for organizations to adopt rigorous offline A/B testing pipelines, significantly reducing the costs and risks of online deployment while leveraging autonomous agents to systematically enhance system performance.

---

## Key Findings

*   **Superior Reliability:** The proposed `two_agent` framework achieved **100% reliability**, experiencing zero compilation failures, whereas baseline frameworks exhibited high failure rates.
*   **Significant Performance Gains:** The framework demonstrated an **average optimization improvement of 106.7%**.
*   **High Success Rate:** In terms of success rates (frequency of achieving gains), the `two_agent` framework and CrewAI reached **45%**, significantly outperforming the AutoGen baseline at **34%**.
*   **Validation of Concept:** The study validates the feasibility of using code-modifying LLM agents to act as automated "growth hackers" for OPE systems, enabling optimization without risky online testing.
*   **Iterative Effectiveness:** LLM-based agents can effectively optimize OPE performance through iterative cycles of code modification and evaluation.

---

## Methodology

The researchers developed **"GrowthHacker,"** a benchmark utilizing large-scale real-world datasets with established OPE protocols. The method employs a specific workflow designed to automate code refinement:

1.  **Iterative Cycle:** Agents optimize code, evaluate results, and initiate new optimization cycles based on performance feedback.
2.  **Implementation:** Baselines were implemented using the **Open Bandit Pipeline (OBP)** and **Scope-RL** libraries.
3.  **Framework Design:** A specific `two_agent` framework was developed to reduce system complexity while preserving optimization effectiveness.
4.  **Comparison:** This framework served as the primary method for comparison against other agentic AI frameworks like CrewAI and AutoGen.

---

## Technical Details

### Problem Formulation
The paper proposes 'GrowthHacker,' a system utilizing LLM agents to automate Off-Policy Evaluation (OPE) optimization. The problem is formulated as a code optimization task $(C, \theta_0, D)$ aimed at minimizing relative estimation error within a contextual bandit setting.

### Estimators Used
*   Direct Method (DM)
*   Inverse Probability Weighting (IPW)
*   Doubly Robust (DR) estimators

### Architecture
The architecture employs a **Two-Agent System**:
*   **Prompter/Analyzer Agent:** Identifies weaknesses in the code and generates modification instructions.
*   **Coder Agent:** Implements the changes to ensure functional correctness.

### Workflow
The process is iterative:
1.  **Analysis:** Agents analyze current code performance.
2.  **Proposal:** Modifications are proposed based on analysis.
3.  **Execution:** Changes are implemented and executed to compute performance metrics (Relative Estimation Error and MSE).

---

## Results

The evaluation of the GrowthHacker framework yielded the following results:

*   **Proposed Framework (`two_agent`):**
    *   **Reliability:** 100%
    *   **Avg. Improvement:** 106.7%
    *   **Success Rate:** 45%

*   **Baseline Comparisons:**
    *   **CrewAI:** 45% Success Rate.
    *   **AutoGen:** 34% Success Rate.

*   **Baseline Failure Analysis:**
    *   Baseline frameworks exhibited high failure rates, with compilation errors exceeding **20%** on the OBP dataset.
    *   Failures were attributed to deprecated syntax, incorrect API usage, and context degradation.
    *   Evaluation was based on relative estimation error and Mean Squared Error (MSE).

---

## Contributions

1.  **Novel Application:** The first paper to investigate and demonstrate the use of coding LLMs and agentic AI specifically for optimizing Off-Policy Evaluation (OPE) results.
2.  **Benchmark Introduction:** Introduces **"The GrowthHacker Benchmark,"** a comprehensive benchmark for standardizing the evaluation of automated OPE optimization.
3.  **Framework Innovation:** Developed the optimized `two_agent` framework, offering a streamlined approach to agentic AI that balances complexity and effectiveness.
4.  **Industry Impact:** Provided a pathway for high-stakes domains to use offline A/B testing (OPE) for evaluation, mitigating the costs and risks associated with online A/B testing.

---

**Quality Score:** 9/10  
**References:** 40 citations