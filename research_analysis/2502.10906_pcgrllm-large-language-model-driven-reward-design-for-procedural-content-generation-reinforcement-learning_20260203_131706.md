---
title: 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content
  Generation Reinforcement Learning'
arxiv_id: '2502.10906'
source_url: https://arxiv.org/abs/2502.10906
generated_at: '2026-02-03T13:17:06'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning

*In-Chang Baek; Sung-Hyun Kim; Sam Earle; Zehua Jiang; Noh Jin-Ha; Julian Togelius; Kyung-Joong Kim*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Performance Gain** | Up to **415%** improvement |
| **Speedup** | **17x** faster than CPU environments |
| **Top Strategy** | Tree-of-Thought (ToT) |
| **Environment** | JAX-optimized 16x16 2D Grid |
| **Quality Score** | **8/10** |
| **References** | 40 Citations |

---

## Executive Summary

> Procedural Content Generation via Reinforcement Learning (PCGRL) offers a powerful method for automating game design, but it is frequently bottlenecked by the difficulty of designing effective reward functions. Traditionally, this process requires extensive manual effort and deep domain knowledge from human experts to define what constitutes "good" content, creating a high barrier to entry for developers. This paper addresses the challenge of automating reward design by investigating whether Large Language Models (LLMs) can reliably replace human intuition in generating and refining the reward functions necessary for training complex game AI agents.
>
> The authors propose **PCGRLLM**, a novel architecture that integrates LLMs into the PCGRL pipeline through a feedback-driven loop to automate reward design. Crucially, the method is evaluated on a story-to-reward generation task within a two-dimensional environmentâ€”a JAX-optimized 16x16 grid with 7 tile typesâ€”demonstrating the system's ability to translate narrative requirements into game logic. The pipeline utilizes a four-stage process: **Refinement**, where reasoning-based prompts (Chain-of-Thought, Tree-of-Thought, or Graph-of-Thought) generate initial reward code; **Self-Alignment**, which adjusts rewards based on random agent interaction statistics; **Training**, where the PCGRL agent learns; and **Feedback**, where terminal states are analyzed to refine the reward function. To mitigate hallucinations, feedback is restricted to a single specific point per iteration.
>
> The PCGRLLM framework achieved substantial performance improvements of **415%** and **40%** across two state-of-the-art LLMs, demonstrating strong generalizability. Regarding reasoning strategies, Tree-of-Thought (ToT) yielded the highest accuracy (**0.329**), significantly outperforming Chain-of-Thought (CoT) at 0.156 and Graph-of-Thought (GoT) at 0.095. An ablation study underscored the necessity of specific feedback loops, which enabled performance growth from 0.033 to 0.156, whereas generic or no feedback resulted in stagnation. Additionally, while using a Heuristic (Oracle) evaluator provided a performance boost of +0.172, relying on LLM Self-Evaluation led to a performance decrease of -0.039.
>
> This research validates that LLMs possess the requisite capabilities for complex reward generation, marking a shift away from purely human-defined heuristic design in game AI. By successfully automating the reward function design process, PCGRLLM significantly reduces the dependency on manual domain knowledge, thereby lowering the barrier to entry for developers utilizing procedural content generation.

---

## Key Findings

*   **Significant Performance Gains:** The proposed PCGRLLM architecture achieved improvements of **415%** and **40%**, demonstrating strong generalizability across two state-of-the-art LLMs.
*   **LLM Capabilities:** Experiments confirm that LLMs possess essential capabilities for complex content generation tasks.
*   **Reduced Dependency:** The approach effectively reduces dependency on human domain knowledge and manual effort in game AI development.
*   **Strategy Superiority:** Tree-of-Thought (ToT) significantly outperformed other reasoning strategies like Chain-of-Thought and Graph-of-Thought.

---

## Technical Methodology

The PCGRLLM framework utilizes a feedback-driven loop to automate reward function design for Procedural Content Generation via Reinforcement Learning (PCGRL).

### System Architecture
The pipeline consists of four distinct stages designed to iteratively improve the reward function:

1.  **Refinement:** Uses reasoning-based prompts to generate initial reward code.
2.  **Self-Alignment:** Adjusts rewards based on random agent interaction statistics to ensure trainability.
3.  **Training:** The actual training phase for the PCGRL agent.
4.  **Feedback:** Analyzes terminal states to provide specific feedback to the LLM.

### Reasoning Strategies
The system compares three distinct prompt engineering strategies:
*   **Chain-of-Thought (CoT)**
*   **Tree-of-Thought (ToT)**
*   **Graph-of-Thought (GoT)**

### Environment & Constraints
*   **Environment:** A JAX-optimized 16x16 2D grid featuring 7 tile types.
*   **Performance:** Provides a **17x speedup** over traditional CPU environments.
*   **Hallucination Mitigation:** Feedback is strictly limited to one point per iteration to prevent model hallucinations.

---

## Results & Analysis

### Reasoning Strategy Performance
Accuracy over 6 iterations varied significantly by strategy:

| Strategy | Accuracy |
| :--- | :--- |
| **Tree-of-Thought (ToT)** | **0.329** (Highest) |
| **Chain-of-Thought (CoT)** | **0.156** |
| **Graph-of-Thought (GoT)** | **0.095** (Peaked) |

### Ablation Study
*   **Heuristic (Oracle) Evaluator:** Resulted in an average performance improvement of **+0.172**.
*   **LLM Self-Evaluation:** Led to a negative performance change of **-0.039**.

### Feedback Analysis
The specificity of feedback played a critical role in model performance:
*   **Specific Feedback:** Enabled growth from **0.033 to 0.156**.
*   **Generic / No Feedback:** Showed negligible or stagnant performance.

---

## Contributions

*   **Framework Introduction:** Introduced the **PCGRLLM framework**, which integrates feedback loops and advanced prompt engineering for reward design in PCGRL.
*   **Validation of Utility:** Proved that LLMs can be effectively utilized for reward generation, validating their utility in complex AI pipelines.
*   **Workflow Optimization:** Lowered the barrier to entry for game AI development by reducing the need for manual, domain-specific knowledge.