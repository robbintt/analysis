---
title: On the Failure of Latent State Persistence in Large Language Models
arxiv_id: '2505.10571'
source_url: https://arxiv.org/abs/2505.10571
generated_at: '2026-02-03T13:06:43'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# On the Failure of Latent State Persistence in Large Language Models

*Jen-tse Huang; Kaiser Sun; Wenxuan Wang; Mark Dredze*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Target Model:** GPT-4o-2024-08-06
> *   **Core Concept:** Latent State Persistence (LSP)
> *   **Key Metric:** Empirical State Metric (ESM)

---

## Executive Summary

This paper addresses a fundamental limitation in Large Language Models (LLMs): the inability to maintain stable, unexpressed internal representations over extended interactions, a phenomenon the authors term the **"Latent State Persistence" (LSP) gap**. This issue is critical because LLMs frequently exhibit "concept drift" and self-contradiction during tasks requiring a consistent hidden commitment, such as multi-turn logic games or mental arithmetic.

The research highlights that without a mechanism for latent state persistence, models cannot effectively simulate human-like working memory or proactive planning. The key innovation is the formalization of LSP as a quantifiable metric and the introduction of a rigorous diagnostic framework comprising three distinct games. Empirical testing on GPT-4o revealed significant failures, confirming that current LLMs function as **reactive post-hoc reasoners** rather than agents capable of proactive planning. This distinction has significant implications for AI alignment, explaining phenomena like sycophancy, where models prioritize immediate user feedback over internal consistency.

---

## Key Findings

*   **Probabilistic Inconsistency:** LLMs fail to consistently allocate probability mass in a Number Guessing Game, indicating an inability to maintain a stable latent commitment.
*   **Concept Drift:** In Yes-No Games, models exhibit "concept drift," leading to self-contradictions due to the lack of Latent State Persistence (LSP).
*   **State Management Failure:** LLMs fail to manage variable binding and state evolution in Mathematical Mentalism if the initial state is not explicit.
*   **Reactive vs. Proactive:** Evidence suggests LLMs function as reactive post-hoc solvers rather than proactive planners.
*   **Alignment Risks:** The lack of persistent latent states leads to alignment risks, such as sycophancy, where models prioritize immediate feedback over internal logic.

---

## Methodology

The researchers employed a novel experimental framework designed to quantify the "Latent State Persistence" gapâ€”the capacity to maintain unexpressed internal representations. The study utilized three distinct games to isolate different aspects of this failure:

1.  **Number Guessing Game:** Used to test probabilistic consistency and the ability to hold a hidden number.
2.  **Yes-No Game:** Employed to evaluate concept consistency over increasing numbers of questions.
3.  **Mathematical Mentalism:** Designed to assess variable binding and state evolution, specifically when the initial state is absent from the context.

---

## Technical Details

The study operationalizes LSP using a formalized approach and rigorous definitions:

*   **Formal Game Definition:** The Yes-No Game is defined where the Object Space ($X$) has $k$ ordered attributes.
*   **Trial Structure ($T$):** A trial includes instruction, acknowledgment, and query/response pairs.
*   **Persistence Hypothesis:**
    *   Requires an agent to maintain a fixed latent variable $x$.
    *   Responses must be generated deterministically:
        $$y_t = \mathbb{I}(f_a(x) \text{ op } f_a(x_{ref}))$$
    *   Complexity requirement is $O(1)$.
*   **Target Model:** GPT-4o-2024-08-06.
*   **Decoding Parameters:** Analysis was conducted under varying Temperature ($T$) and Top-p ($P$) settings to rule out decoding noise as the cause of failure.

---

## Results

*   **Empirical State Metric (ESM):** The ESM for GPT-4o never converges to zero, indicating a failure to maintain internal variables.
*   **Robustness of Failure:** In the Number Guessing Game, ESM scores remained non-zero across all configurations:
    *   Baseline: **1.085**
    *   Low Top-p: **0.84**
    *   *Conclusion:* The failure is intrinsic to the architecture, not merely decoding noise.
*   **Distributional Bias:** Analysis revealed a specific concentration bias on the number **7**, illustrating the model's reliance on superficial patterns rather than stable internal logic.

---

## Contributions

*   **New Metric:** Introduces **Latent State Persistence (LSP)** as a critical metric for evaluating internal representation fidelity.
*   **Diagnostic Framework:** Provides a framework comprising three specific games to rigorously test the limitations of transformer architectures in handling hidden states.
*   **Cognitive Divergence:** Highlights the fundamental divergence between autoregressive transformers and human-like cognition, clarifying that current LLMs lack mechanisms for human-like working memory or proactive planning.