# Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning

*Minghao Yang; Ren Togo; Guang Li; Takahiro Ogawa; Miki Haseyama*

---

> ### **Quick Facts**
> *   **Core Focus:** Multi-Task Learning (MTL) Efficiency
> *   **Architecture:** LoRA-Based Mixture of Experts (MoE)
> *   **Key Mechanism:** Adaptive Shared Experts (ASE) with Joint Normalization
> *   **Benchmark:** PASCAL-Context
> *   **Parameter Strategy:** Fine-grained scaling (More experts, lower rank)
> *   **Analysis Quality Score:** 6/10

---

## Executive Summary

**Problem: Transitions and Efficiency in Multi-Task Learning**
This research tackles the inherent inefficiencies in transitioning from Single-Task Learning (STL) to Multi-Task Learning (MTL), specifically targeting redundant adaptation and poor knowledge sharing. Standard MTL architectures often struggle to balance task-specific feature acquisition with generalizable knowledge retention, leading to parameter inefficiency and destructive interference between tasks. Resolving these bottlenecks is critical for developing scalable deep learning systems that manage complex, multi-modal tasks without requiring prohibitive computational costs or physical model size.

**Innovation: Adaptive Shared Experts (ASE) and Fine-Grained LoRA**
The authors introduce the Adaptive Shared Experts (ASE) framework, a Mixture-of-Experts (MoE) architecture built upon Low-Rank Adaptation (LoRA). The core technical innovation is a dual-expert structure featuring a novel gating mechanism that jointly normalizes shared expert weights with sparse expert weights, fostering better collaboration between general and specialized components. Additionally, the researchers employ a "fine-grained" parameter strategy: scaling up the number of LoRA experts while proportionally reducing their rank. This design maintains a fixed overall parameter budget while significantly enhancing knowledge granularity and sharing capabilities.

**Results: Superior Performance on PASCAL-Context**
The proposed method was extensively validated on the PASCAL-Context benchmark across diverse model configurations. The results demonstrate that the ASE framework consistently yields performance gains, outperforming baseline methods in overall efficacy. Empirical findings confirm that the integration of adaptive shared experts and the fine-grained scaling strategy leads to superior multi-task performance—specifically improvements in mean Intersection over Union (mIoU)—compared to standard LoRA-based or traditional MoE approaches. The validation highlights the framework's ability to maintain high accuracy across diverse tasks.

**Impact: A Blueprint for Scalable Parameter Efficiency**
The significance of this work lies in its practical solution to the parameter-efficiency bottleneck in MTL. By successfully decoupling shared knowledge from task-specific specialization through a low-rank, fine-grained MoE design, the ASE framework provides a blueprint for building more scalable and adaptable AI models. This research influences the field by validating that increased model capacity—achieved through a higher number of low-rank experts—facilitates better knowledge transfer without expanding the physical model footprint, encouraging further exploration of sparse and adaptive architectures for large-scale applications.

---

## Key Findings

*   **Mitigation of Redundancy:** The proposed Adaptive Shared Experts (ASE) framework successfully mitigates redundant adaptation and inefficient knowledge sharing issues that typically occur when transitioning from Single-Task Learning (STL) to Multi-Task Learning (MTL).
*   **Enhanced Specialization:** Jointly normalizing the gating weights of shared experts with sparse experts significantly enhances expert specialization and cooperation.
*   **Efficient Scaling:** A "fine-grained" design strategy—increasing the number of LoRA experts while proportionally reducing their rank—enables more effective knowledge sharing without increasing the overall parameter budget.
*   **Consistent Validation:** Extensive validation on the PASCAL-Context benchmark confirms that the ASE method consistently yields performance improvements across diverse model configurations.

---

## Methodology

The researchers propose a Mixture-of-Experts (MoE) framework constructed upon Low-Rank Adaptation (LoRA). The core of their methodology involves the introduction of "Adaptive Shared Experts" (ASE). In this system, shared experts are assigned gating weights computed by a router; crucially, these weights are jointly normalized with those of the sparse experts.

To optimize parameter efficiency, the authors employ a fine-grained expert design, which scales up the number of experts while scaling down their individual ranks, maintaining a comparable total parameter budget.

---

## Technical Details

The paper introduces Adaptive Shared Experts (ASE), a LoRA-Based Mixture of Experts (MoE) framework designed to address issues in transitioning to Multi-Task Learning (MTL).

*   **Objective:** To resolve redundant adaptation and inefficient knowledge sharing in MTL.
*   **Architecture:** Utilizes Low-Rank Adaptation (LoRA) modules to parameterize experts within a dual structure of shared and sparse experts.
*   **Gating Mechanism:** Features a 'joint normalization' process that balances the use of general (shared) and specialized (sparse) knowledge.
*   **Parameter Strategy:** Employs a fine-grained parameter strategy that maintains a fixed overall budget by increasing the number of LoRA experts while decreasing the rank of each module.

---

## Core Contributions

*   **Framework Innovation:** Introduction of the Adaptive Shared Experts (ASE) model within LoRA-based MoE to specifically address the transition challenges from STL to MTL.
*   **Routing Mechanism:** A novel gating strategy that integrates shared experts into the routing process via joint normalization, fostering better collaboration between specialized and shared components.
*   **Parameter Efficiency Strategy:** Development of a fine-grained expert scaling method that improves knowledge sharing capabilities without inflating the model size.
*   **Benchmark Validation:** Comprehensive evaluation on the PASCAL-Context dataset under unified training settings, providing strong empirical evidence for the superiority of the fine-grained and adaptive shared expert designs in MTL.

---

## Results & Validation

*   **Benchmark:** PASCAL-Context
*   **Performance:** The method was validated across diverse model configurations. Results indicate that the ASE framework consistently yielded performance improvements and outperformed baseline methods.
*   **Metrics:** Empirical evidence supports improvements in mean Intersection over Union (mIoU) compared to standard approaches.

---

**Analysis Quality Score:** 6/10  
**References Included:** 0