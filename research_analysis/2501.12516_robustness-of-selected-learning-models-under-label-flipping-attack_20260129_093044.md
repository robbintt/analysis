# Robustness of Selected Learning Models under Label-Flipping Attack
*Sarvagya Bhargava; Mark Stamp*

***

### **Quick Facts**
| Metric | Detail |
| :--- | :--- |
| **Dataset** | Malicia (11,688 Windows malware binaries) |
| **Models Tested** | 10 (SVM, RF, GNB, GBM, LightGBM, XGBoost, MLP, CNN, MobileNet, DenseNet) |
| **Top Performer** | **Multilayer Perceptron (MLP)** |
| **Most Brittle** | Boosting Models (XGBoost, LightGBM) |
| **Quality Score** | ⭐ 8/10 |
| **Citations** | 21 references |

***

## Executive Summary

This paper addresses the critical security vulnerability of machine learning (ML) and deep learning (DL) systems to **label-flipping attacks**, a specific type of data poisoning where adversaries manipulate training labels to degrade model performance. This issue is particularly urgent in high-stakes domains such as malware detection, where compromised models can fail to identify malicious software, leading to significant security breaches.

The research aims to resolve the uncertainty regarding how different architectural families—ranging from classical statistical methods to modern deep learning—react when exposed to systematic data corruption. The key innovation is a rigorous, **empirical comparative benchmarking framework** designed to evaluate model robustness under identical adversarial conditions.

The technical methodology utilizes the **Malicia dataset** to simulate label-flipping attacks by systematically injecting misleading labels into the training data at increasing percentages. The results demonstrate significant variance in robustness, with the **Multilayer Perceptron (MLP)** outperforming all other tested algorithms. While complex deep learning models like DenseNet and CNNs achieved high baseline accuracy, they displayed steep degradation curves (dropping below 70% at 40% corruption). Conversely, MLP maintained accuracy above 85–90% even at high corruption levels. Notably, boosting algorithms proved the most brittle, often falling below 80% at just 20–30% poisoning rates.

***

## Key Findings

*   **Significant Variation in Robustness:** There is no one-size-fits-all answer for security; robustness varies significantly depending on the specific architecture of the model.
*   **Superiority of MLP:** The **Multilayer Perceptron (MLP)** model outperformed other tested algorithms, demonstrating the optimal combination of initial high accuracy and resistance to adversarial disruption.
*   **No Universal Immunity:** Neither traditional machine learning models nor deep learning models are universally immune to the accuracy degradation caused by misleading labels in the training data.
*   **Architectural Stability:** Robustness is model-dependent, with some architectures maintaining stability better than others as the percentage of flipped labels increases.
*   **Failure of Boosting Models:** Boosting models (e.g., XGBoost, LightGBM) are hypothesized to be susceptible to failure because they focus on correcting misjudgments (hard examples), a mechanism that adversarial label flipping exploits effectively.

***

## Methodology

The researchers employed an **empirical comparative analysis framework** using a malware dataset to simulate realistic adversarial conditions.

*   **Dataset:** Malicia dataset (11,688 Windows malware binaries).
*   **Algorithms Evaluated:** A total of **10 distinct algorithms** were tested across three categories:
    1.  **Classic Machine Learning:** Support Vector Machines (SVM), Random Forest, Gaussian Naive Bayes (GNB).
    2.  **Boosting Models:** Gradient Boosting Machine (GBM), LightGBM, XGBoost.
    3.  **Deep Learning Models:** Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), MobileNet, DenseNet.
*   **Attack Simulation:** The robustness of these models was assessed by training them on data subjected to adversarial label-flipping attacks. The percentage of misleading labels was systematically varied to measure the corresponding impact on model accuracy.

***

## Technical Details

**Research Focus**
Evaluating the robustness of ML and DL models against label-flipping adversarial attacks within the malware detection domain.

**Data Processing**
*   **Source:** Malicia dataset containing 11,688 Windows malware binaries.
*   **Filtering:** Originally from 48 families; classes with fewer than 50 samples were filtered out to ensure statistical validity.

**Experimental Setup**
*   **Attack Vector:** Simulating label-flipping attacks on the dataset to mislead the training process.
*   **Model Taxonomy:** Analyzed models were categorized into:
    *   Classic Machine Learning (SVM, Random Forest, GNB)
    *   Boosting Models (GBM, LightGBM, XGBoost)
    *   Deep Learning Models (MLP, CNN, MobileNets, DenseNets)

***

## Results

The study yielded critical insights into how architectural choices influence security:

*   **MLP Performance:** Demonstrated superior resilience, often maintaining accuracy above **85–90%** even at high corruption levels.
*   **Deep Learning Degradation:** Complex models (DenseNet, CNN) achieved high baseline accuracy (>97%) but suffered steep degradation, with CNNs dropping below **70%** at a 40% label-flipping rate.
*   **Boosting Brittleness:** Boosting algorithms (XGBoost, LightGBM) were the most brittle, frequently plummeting below **80%** accuracy at 20–30% poisoning rates.
*   **Mechanism of Failure:** The results support the hypothesis that boosting algorithms are particularly vulnerable because their focus on "hard examples" leads them to overfit to the flipped labels.

***

## Contributions

1.  **Comprehensive Benchmarking:** Provided a direct comparative analysis of robustness across a diverse range of traditional machine learning and deep learning models under identical adversarial conditions.
2.  **Adversarial Resilience Characterization:** Offered empirical insights into the inherent stability of specific architectures (e.g., MLP, CNN, XGBoost) when facing data poisoning attacks via label flipping.
3.  **Guidance for Model Selection:** Identified the **Multilayer Perceptron (MLP)** as a particularly strong candidate for applications requiring a balance of high baseline accuracy and resilience to training data corruption.

***
*References: 21 citations*