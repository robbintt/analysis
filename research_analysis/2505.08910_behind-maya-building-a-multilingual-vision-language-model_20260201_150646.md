# Behind Maya: Building a Multilingual Vision Language Model

*Nahid Alam; Karthik Reddy Kanjula; Surya Guthikonda; Timothy Chung; Bala Krishna S Vegesna; Abhipsha Das; Anthony Susevski; Ryan Sze-Yin Chan; S M Iftekhar Uddin; Shayekh Bin Islam; Roshan Santhosh; Snegha A; Drishti Sharma; Chen Liu; Isha Chaturvedi; Genta Indra Winata; Ashvanth. S; Snehanshu Mukherjee; Alham Fikri Aji*

> ### ðŸ” Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Languages** | 8 (Hindi, Bengali, Tamil, Telugu, Marathi, Urdu, Malayalam, Kannada) |
> | **Approach** | Data-centric (LLaVA adaptation) |
> | **Output** | Open-source Model & Dataset |
> | **Quality Score** | 6/10 |
> | **References** | 40 Citations |

---

## Executive Summary

Current state-of-the-art Vision-Language Models (VLMs) exhibit a critical performance disparity, functioning effectively on academic benchmarks for high-resource languages while failing significantly in low-resource languages. This limitation stems from a lack of cultural context and training data diversity, causing models to hallucinate or misinterpret visual content outside the Western or English-centric paradigm. As a result, the deployment of multimodal AI is severely restricted in diverse global regions, creating an urgent need for systems that can accurately comprehend and describe images across varied linguistic and cultural landscapes.

To bridge this gap, the researchers introduce **"Maya,"** a VLM developed through a strictly data-centric approach that synthesizes a multilingual training corpus without modifying the underlying model architecture. The technical innovation lies in the comprehensive augmentation of the LLaVA pretraining dataset into eight distinct low-resource languages. By translating and culturally localizing this visual data, the methodology optimizes the model for linguistic and cultural nuance, allowing it to bridge the gap between high and low-resource performance through targeted data scaling rather than architectural changes.

The study demonstrates Mayaâ€™s capability to significantly outperform standard VLMs in low-resource contexts, particularly in qualitative scenarios where existing models typically fail. Specific examples of success include the accurate identification and description of culturally specific objects, such as regional attire (e.g., sarees or sherwanis), local festivals, and distinct architectural landmarks. The primary significance of this work is the open-source release of the Maya model, the synthesized multilingual dataset, and the complete codebase, which collectively lower the barrier to entry for future research in multilingual multimodal AI.

---

## Key Findings

*   **Performance Gap in Current VLMs:** State-of-the-art models function well on benchmarks for widely spoken languages but fail on low-resource languages due to a lack of cultural context capabilities.
*   **Dataset Efficacy:** Adapting the LLaVA pretraining dataset into eight distinct languages enables the effective training of multilingual vision-language models.
*   **Cultural Comprehension:** The proposed *Maya* model successfully enhances both linguistic and cultural comprehension within vision-language tasks across multiple languages.
*   **Open Source Impact:** The project provides an open-source solution (code and model), significantly lowering the barrier to entry for future research in multilingual multimodal AI.
*   **Global Applicability:** Existing VLMs are limited in diverse global scenarios; this research addresses that limitation by focusing on varied cultural contexts.

---

## Methodology

The researchers employed a **data-centric approach** to build the Maya model. Their methodology focused on the construction of a specialized training corpus rather than solely on architectural innovations.

*   **Foundation:** Utilization of the existing LLaVA pretraining dataset.
*   **Extension:** Creation of a multilingual image-text pretraining dataset encompassing eight specific languages.
*   **Training:** The augmented dataset was used to train a vision-language model specifically optimized for multilingual and culturally diverse inputs.

---

## Technical Details

**Model Overview**
*   **Name:** Maya
*   **Type:** Vision-Language Model (VLM)
*   **Strategy:** Multilingual extension using a data-centric approach.

**Training Architecture**
*   **Base Dataset:** Adapted LLaVA pretraining dataset.
*   **Optimization:** Geared toward linguistic and cultural comprehension to bridge the gap between high and low-resource languages.

**Availability**
*   **Access:** Codebase and model weights are open-source.

**Missing Information**
*   Specific details regarding the neural network backbone.
*   Parameter count.
*   Projection layer mechanisms.
*   Specific training hyperparameters.

---

## Contributions

The project delivers two primary assets to the research community:

1.  **Multilingual Dataset Creation**
    The release of a multilingual image-text pretraining dataset covering eight languages, derived from the LLaVA dataset. This provides a critical resource for low-resource language research in computer vision.

2.  **Open-Source Multilingual VLM**
    The development and open-sourcing of Maya, a Vision-Language Model that supports eight languages, specifically designed to improve cultural and linguistic understanding in vision-language tasks.

---

## Results

**Qualitative Performance**
*   Maya successfully enhances performance in low-resource languages where current state-of-the-art VLMs fail.
*   Improved cultural comprehension, including the accurate identification of regional attire, festivals, and landmarks.
*   Effective linguistic coverage across the eight supported languages.

**Quantitative Performance**
*   **Status:** No specific quantitative metrics, accuracy percentages, or benchmark performance scores are provided in the analysis text.
*   **Benchmarks:** Specific benchmark datasets and statistical tables are absent.
*   **Note:** The paper notes the absence of standard quantitative benchmarks for these specific languages but highlights the volume of the dataset as a key quantitative contribution (spanning the full volume of the LLaVA corpus).

---

**Report Quality Score:** 6/10  
**Total References:** 40 Citations