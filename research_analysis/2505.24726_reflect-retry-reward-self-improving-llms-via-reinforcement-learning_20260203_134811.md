---
title: 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning'
arxiv_id: '2505.24726'
source_url: https://arxiv.org/abs/2505.24726
generated_at: '2026-02-03T13:48:11'
quality_score: 9
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning

*Shelly Bensal; Umar Jamil; Christopher Bryant; Melisa Russak; Kiran Kamble; Dmytro Mozolevskyi; Muayad Ali; Waseem AlShikh*

---

## Executive Summary

This research addresses the critical challenge of enhancing Large Language Model (LLM) reliability on complex reasoning and function-calling tasks without relying on expensive synthetic data generation or massive parameter scaling. As the demand for specialized AI agents grows, the industryâ€™s dependence on costly data curation pipelines and prohibitively large foundation models becomes unsustainable. The authors aim to bridge this gap by enabling models to self-correct and improve using minimal supervision signals, specifically binary feedback, thereby reducing the resource barriers for deploying high-performance, autonomous systems.

The core innovation is the "Reflect, Retry, Reward" ($R^3$) framework, a reinforcement learning loop designed to instill self-correction capabilities using only binary supervision. The process triggers upon task failure: the model generates self-reflective commentary to analyze its error, attempts the task again using this reflection as context, and receives a reward if the retry succeeds. Technically, the framework utilizes Group Relative Policy Optimization (GRPO) to estimate advantages through group comparisons, eliminating the need for a separate critic network. Crucially, the reward mechanism employs token-level masking to specifically incentivize the tokens generated during the self-reflection phase, ensuring the model learns the meta-skill of correction reasoning rather than simply memorizing specific correct answers.

The proposed method achieved substantial performance gains across rigorous benchmarks, demonstrating a **34.7% improvement** on the Countdown math equation task and an **18.1% increase** on the APIGen function-calling dataset. Perhaps the most striking result is the efficiency of the approach; smaller fine-tuned models ranging from 1.5 to 7 billion parameters consistently outperformed base models up to ten times their size (e.g., 70B+ parameters). Evaluations conducted on models such as Qwen2, Llama3.1, and Phi3.5-mini against larger baselines confirmed that targeted reinforcement learning can surpass the performance advantages typically reserved for significantly larger architectures, using only 60,000 function calls and 4,211 tools for evaluation.

The significance of this research lies in demonstrating that high reliability and self-improvement can be decoupled from massive scale and synthetic data proliferation. By proving that minimal binary feedback is sufficient to drive complex behavioral improvements, the $R^3$ framework offers a viable path for deploying advanced AI capabilities in low-resource environments. This work shifts the paradigm toward training models that learn to reason through their failures, paving the way for more efficient, adaptable, and cost-effective AI systems that generalize effectively across various architectures without the need for extensive data engineering.

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Total Citations:** 11
> *   **Math Performance:** +34.7% (Countdown Task)
> *   **Function Calling:** +18.1% (APIGen)
> *   **Efficiency:** 1.5Bâ€“7B models > 10x larger models
> *   **Data Requirement:** Binary feedback only (No synthetic data)

---

## Key Findings

*   **Significant Performance Gains:** The framework achieved a **34.7% increase** in math equation writing (Countdown task) and an **18.1% increase** in function calling (APIGen).
*   **Efficiency Over Scale:** Smaller fine-tuned models (**1.5B to 7B parameters**) successfully outperformed base models **10 times their size**.
*   **Data Efficiency:** The method works effectively with only **binary feedback** and requires **no synthetic data**.
*   **Generalizability:** Demonstrated effectiveness across various architectures, showing broad applicability.

---

## Methodology

The **Reflect, Retry, Reward ($R^3$)** framework employs a reinforcement learning process consisting of three distinct steps:

1.  **Reflect**
    The model generates self-reflective commentary to analyze the specific cause of its failure.
2.  **Retry**
    The model attempts the task again, utilizing the generated reflection as contextual input to guide the new attempt.
3.  **Reward**
    If the retry is successful, the tokens generated during the self-reflection phase are specifically rewarded. This incentivizes the model to produce higher-quality reflections in the future.

---

## Technical Details

The technical implementation focuses on optimizing the self-correction loop without heavy computational overhead.

*   **Framework Name:** Reflect, Retry, Reward ($R^3$)
*   **Trigger Mechanism:** Self-improvement loop triggered only by task failure using binary supervision.
*   **Optimization Algorithm:** Utilizes **Group Relative Policy Optimization (GRPO)**.
    *   Removes the need for a separate critic network.
    *   Estimates advantages through group comparisons.
*   **Reward Mechanism:**
    *   Uses **token-level masking** to target self-reflection tokens specifically.
    *   Zeros out advantages for non-reflection tokens.
*   **Learning Objective:** Ensures the model learns the **meta-skill of correction reasoning** rather than memorizing specific task answers.

---

## Results

The study utilized the **APIGen dataset** (comprising 60,000 function calls and 4,211 tools) to evaluate various models against larger baselines.

*   **Benchmarks:**
    *   **Countdown Equation Task:** 34.7% performance increase.
    *   **APIGen Dataset:** 18.1% performance increase.
*   **Model Comparisons:**
    *   **Tested Models:** Qwen2 (1.5B, 7B), Llama3.1 (8B), Phi3.5-mini.
    *   **Baselines:** Qwen2-72B, Llama3.1-70B.
*   **Outcome:** Small fine-tuned models (1.5Bâ€“7B) significantly outperformed the massive 70B+ parameter baselines using only binary feedback without synthetic data.

---

## Contributions

*   **Novel Mechanism:** Introduction of the 'Reflect, Retry, Reward' ($R^3$) mechanism, enabling LLMs to self-improve on complex tasks without expensive synthetic data.
*   **Efficiency Evidence:** Provided evidence that smaller, fine-tuned models can surpass significantly larger foundation models through this specific training method.
*   **Low-Resource Solution:** Identified a viable approach for enhancing model reliability using minimal binary feedback, suitable for low-resource environments.

---
**References:** 11 citations