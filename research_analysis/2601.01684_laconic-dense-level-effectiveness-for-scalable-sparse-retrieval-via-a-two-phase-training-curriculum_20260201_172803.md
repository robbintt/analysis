# LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum

*Zhichao Xu; Shengyao Zhuang; Crystina Zhang; Xueguang Ma; Yijun Tian; Maitrey Mehta; Jimmy Lin; Vivek Srikumar*

---

> ### ðŸ“Š Quick Facts
>
> *   **Benchmark Score:** 60.2 nDCG@10
> *   **Global Ranking:** 15th (MTEB Retrieval)
> *   **Memory Reduction:** 71% less than dense models
> *   **Model Variants:** 1B, 3B, 8B Parameters
> *   **Training Data:** ~9M pairs + 690K triplets
> *   **Hardware:** Commodity CPU capable
> *   **Quality Score:** 8/10

---

## Executive Summary

The field of information retrieval has historically faced a significant trade-off between effectiveness and efficiency. While Dense Retrieval (DR) models have achieved state-of-the-art accuracy, they require substantial computational resources and memory footprint due to the need to store and compare high-dimensional vectors. Conversely, Learned Sparse Retrieval (LSR) offers superior efficiency and scalability through inverted indexes but has typically lagged behind DR in retrieval quality. Furthermore, leveraging the capabilities of modern Causal Large Language Models (LLMs) for retrieval tasks is difficult, as their unidirectional attention mechanism is unsuitable for the bidirectional contextualization required for effective document understanding. This paper addresses the challenge of closing the performance gap between sparse and dense models while maintaining the resource efficiency necessary for scalable deployment.

The authors introduce **LACONIC**, a family of learned sparse retrievers built upon the Llama-3 architecture (1B, 3B, and 8B parameters). The core innovation is a novel two-phase training curriculum designed to convert causal LLMs into effective bidirectional retrievers. Phase 1 involves Weakly Supervised Pre-finetuning using LoRA on approximately 9 million query-document pairs to adapt the model for bidirectional attention. Phase 2 focuses on High-signal Finetuning using a curated dataset of 690,000 triplets with hard negatives to maximize discriminative power. The model utilizes the SPLADE framework with a bi-encoder architectureâ€”employing Max-pooling, ReLU activation, and log(1+x) rescalingâ€”optimized via an InfoNCE loss function with FLOPs regularization to enforce sparsity.

LACONIC demonstrates that sparse retrieval can achieve performance comparable to state-of-the-art dense models. On the MTEB Retrieval benchmark, the flagship LACONIC-8B model achieved a score of 60.2 nDCG, ranking 15th globally. Crucially, this performance is attained with drastically reduced resource requirements; the model requires 71% less index memory than equivalent dense retrieval systems. Furthermore, LACONIC exhibits high compute efficiency, training on significantly smaller datasets than many competitors while maintaining the ability to operate effectively on commodity CPU hardware without requiring specialized GPU infrastructure for inference.

This work represents a paradigm shift in the search technology landscape by proving that high accuracy does not necessitate high memory costs. By successfully bridging the gap between learned sparse and dense retrieval, LACONIC enables organizations to deploy state-of-the-art search capabilities on standard, commodity hardware, significantly lowering the barrier to entry for advanced information retrieval systems. The effective conversion of general-purpose causal LLMs into specialized, efficient sparse retrievers opens new avenues for scalable search architectures, suggesting that future systems can prioritize both semantic richness and operational efficiency without compromise.

---

## Key Findings

*   **High Performance:** LACONIC 8B achieved a **60.2 nDCG** score on the MTEB Retrieval benchmark, securing the **15th** rank globally.
*   **Resource Efficiency:** The model utilizes **71% less index memory** than equivalent dense retrieval models, enabling deployment on standard CPU hardware.
*   **Performance Gap Bridge:** Successfully bridges the effectiveness gap between learned sparse retrieval and state-of-the-art dense retrieval models.
*   **Compute Efficiency:** Achieves high retrieval effectiveness while operating on commodity CPU hardware, removing the dependency on expensive GPU infrastructure for inference.

---

## Methodology

The researchers developed LACONIC using a robust **two-phase training curriculum** designed to adapt causal LLMs for retrieval tasks:

1.  **Phase 1: Weakly Supervised Pre-finetuning**
    *   Adapted causal LLMs for bidirectional contextualization.
    *   Utilized approximately 9 million query-document pairs.
    *   Employed LoRA (Low-Rank Adaptation) for efficient parameter updates.

2.  **Phase 2: High-signal Finetuning**
    *   Refined the model's discriminative power using curated hard negatives.
    *   Focused on a smaller, high-quality dataset of 690,000 triplets.
    *   Optimized specifically for distinguishing between relevant and non-relevant documents.

---

## Technical Details

*   **Framework & Backbone**
    *   Utilizes the **SPLADE** framework.
    *   Based on the **Llama-3** family architecture (available in 1B, 3B, and 8B parameter variants).
    *   Modified for bidirectional attention by removing causal masks.

*   **Architecture Components**
    *   **Bi-encoder Architecture:** Separate encoders for queries and documents.
    *   **Representation Generation:**
        *   **Max-pooling:** Used for aggregation.
        *   **ReLU Activation:** Applied to introduce non-linearity and sparsity.
        *   **Rescaling:** Log(1+x) transformation applied to output.

*   **Training Strategy**
    *   **Phase 1:** Pre-finetuning on ~9M pairs using LoRA (weak supervision).
    *   **Phase 2:** Finetuning on 690K triplets with hard negatives.

*   **Objective Function**
    *   **Loss:** InfoNCE loss.
    *   **Regularization:** FLOPs regularization to enforce sparsity.

---

## Results

*   **Benchmarking:** LACONIC-8B scored **60.2 nDCG** on the MTEB Retrieval benchmark (Rank 15th as of January 1, 2026).
*   **Efficiency:** Requires **71% less index memory** compared to equivalent dense models.
*   **Hardware Viability:** Operates efficiently on commodity CPU hardware.
*   **Compute Efficiency:** Demonstates high training efficiency by utilizing significantly smaller datasets than competitors while maintaining state-of-the-art performance.

---

## Contributions

*   **New Model Family:** Introduction of a scalable family of Llama-3 based learned sparse retrievers (1B, 3B, 8B).
*   **Training Innovation:** A novel two-phase training strategy effectively converts causal LLMs into bidirectional sparse retrievers.
*   **Resource-Effective Search:** Delivers dense-level effectiveness with drastically reduced computational and memory constraints, suitable for standard hardware.

---

**References:** 13 citations