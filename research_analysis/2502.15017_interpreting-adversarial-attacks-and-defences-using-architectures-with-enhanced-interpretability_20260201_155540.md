# Interpreting Adversarial Attacks and Defences using Architectures with Enhanced Interpretability

*Akshay G Rao; Chandrashekhar Lakshminarayanan; Arun Rajkumar*

---

### **Quick Facts**

> **Quality Score:** 7/10
> **References:** 40 citations
> **Architecture:** Deep Linearly Gated Networks (DLGN)
> **Core Method:** PGD-AT vs. Standard Training Comparison
> **Key Concept:** Active Sub-network Learning

---

### **Executive Summary**

This paper addresses the fundamental "black box" nature of adversarial robustness in deep neural networks (DNNs). While Projected Gradient Descent Adversarial Training (PGD-AT) is the empirically accepted standard for defense, the internal structural mechanisms that differentiate robust models from standard models remain opaque. This lack of interpretability hinders the development of principled defenses, as practitioners rely on computationally expensive min-max optimization without understanding how internal network geometry and topology change to resist attacks.

To demystify these mechanisms, the authors introduce a methodological shift using Deep Linearly Gated Networks (DLGN)—an architecture employing Gated Affine Linear Units (GALU) across both fully connected and CNN variations. Unlike standard DNNs, DLGNs allow the network to be modeled as a combination of a linear feature network and a gating mechanism, interpreting training as the learning of specific "active subnetworks." Leveraging this architecture, the researchers analyze robustness through two distinct technical lenses: hyperplane geometry (analyzing the alignment and distance of decision boundaries) and topological sub-network diversity (examining the overlap and exclusivity of active paths).

The study demonstrates that while both standard and robust models derive hyperplanes resembling principal components, PGD-AT hyperplanes are aligned significantly farther from data points compared to those in standard models. Crucially, robust models exhibited high sub-network diversity, characterized by non-overlapping active subnetworks for different classes, which prevents attack-induced gating overlaps. Experiments involving embedded PCA layers revealed a critical divergence: standard training was resilient to dimensionality reduction, whereas adversarial training resulted in significant drops in both clean and robust accuracy under PGD-40 attacks. This confirms that robust models rely on preserving the full variance of data directions—specifically the "thin shell"—rather than relying solely on low-dimensional subspaces.

This work provides a transparent structural characterization of adversarial defense, bridging the gap between empirical optimization and theoretical understanding. It establishes that adversarial robustness is intrinsically linked to the specific geometric organization of decision boundaries and the structural segregation of internal representations. By defining a comparative framework based on geometric alignment and gating pattern analysis, the paper offers the research community actionable metrics for evaluating model stability and designing inherently secure architectures.

---

## Key Findings

*   **Hyperplane Geometry:** In both robust (PGD-AT) and standard (STD-TR) models, hyperplanes resemble principal components; however, PGD-AT hyperplanes are aligned **farther from data points** compared to standard models.
*   **Sub-network Diversity:** Robust models trained with PGD-AT create **diverse, non-overlapping active subnetworks** across different classes.
*   **Attack Mechanism:** The creation of non-overlapping subnetworks is cited as a key factor in preventing **attack-induced gating overlaps**, thereby enhancing robustness.
*   **Representation Nature:** Qualitative visualizations and quantitative analyses reveal distinct differences in gating patterns and internal representations between adversarially trained and standard models.

---

## Methodology

The study employs a comparative analytical approach to understand the structural differences between robust and standard models.

*   **Architecture Selection:** The study utilizes **Deep Linearly Gated Networks (DLGN)**—including variations with fully connected layers and CNN layers—chosen for their superior interpretability compared to standard deep networks.
*   **Model Comparison:** The researchers contrast Deep Neural Networks (DNNs) trained using **Projected Gradient Descent (PGD) adversarial training** against those trained with **standard procedures (STD-TR)**.
*   **Feature Network Analysis:** By identifying feature networks as the sole medium for adversarial attacks, the authors analyze these networks through:
    *   **Geometric properties:** Hyperplane alignment and relation to PCA.
    *   **Topological properties:** Sub-network overlap.
*   **Analytical Techniques:** The approach combines path activity analysis to trace active subnetworks with qualitative visualizations to inspect gating patterns and representation learning.

---

## Technical Details

*   **Core Architecture:** Introduces **Deep Linearly Gated Neural Networks (DLGN)**, an improvement over Deep Gated Neural Networks (DGN) that renders the feature network entirely linear while handling gating via a **Gated Affine Linear Unit (GALU)**.
*   **Framework Concept:** Views training as **active sub-network learning** using Neural Path Features (NPF) and Neural Path Values (NPV).
*   **Optimization Strategy:**
    *   Robustness is achieved through **Projected Gradient Descent Adversarial Training (PGD-AT)**, framed as a min-max optimization problem.
    *   Compared against **Standard Training (STD-TR)** baselines.
*   **Analytical Metrics:**
    *   Hyperplane geometry analysis.
    *   PCA similarity analysis between model weights and training data principal components.

---

## Results

Experimental findings highlight the distinct structural behaviors of robust versus standard models:

*   **Geometric Alignment:** While both standard and robust models have hyperplanes resembling principal components, **PGD-AT hyperplanes are aligned farther from data points**.
*   **Structural Defense:** Robust models exhibit diverse, non-overlapping active sub-networks across classes, creating a defense against **attack-induced gating overlaps**.
*   **Dimensionality Reduction Impact:** Experiments with embedded PCA layers demonstrated that:
    *   Standard training is resilient to dimensionality reduction.
    *   Adversarial training leads to significant drops in both clean and robust accuracy (**PGD-40**).
*   **Data Variance Implication:** The drop in accuracy under PCA suggests that preserving full data variance is **critical for robustness**.
*   **Weight Similarity:** PCA-weight similarity distributions differ significantly between standard and adversarially trained models.

---

## Contributions

1.  **Interpretability of Robustness:** Provides a transparent lens into the 'black box' of adversarial robustness by leveraging an explainable architecture (DLGN) to reveal why PGD-AT is effective.
2.  **Structural Defense Characterization:** Establishes a link between the geometry of decision boundaries (distance of hyperplanes from data) and the structural organization of active subnetworks (diversity and non-overlap) as critical components of adversarial defense.
3.  **Comparative Framework:** Offers a comprehensive set of metrics—ranging from geometric alignment to gating pattern visualizations—for evaluating the internal stability of models under adversarial stress.