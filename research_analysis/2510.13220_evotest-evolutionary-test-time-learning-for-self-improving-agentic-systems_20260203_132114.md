---
title: 'EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems'
arxiv_id: '2510.1322'
source_url: https://arxiv.org/abs/2510.13220
generated_at: '2026-02-03T13:21:14'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems

*Yufei He; Juncheng Liu; Yue Liu; Yibo Li; Tri Cao; Zhiyuan Hu; Xinxing Xu; Bryan Hooi*

---

> ### ðŸ“Œ Quick Facts
>
> *   **Methodology:** Gradient-free evolutionary framework (No backpropagation).
> *   **Key Architecture:** Dual-role system (Actor Agent & Evolver Agent).
> *   **Benchmark:** Jericho Test-Time Learning (J-TTL).
> *   **Performance:** Achieved **100%** win rate in 'Detective' and 'Library' games (Baselines: 0%).
> *   **Efficiency:** ~7x higher score than Reflexion in 'Zork1'; converges in <10 episodes.
> *   **Quality Score:** 8/10.

---

## Executive Summary

Current AI agentic systems face a fundamental limitation: they lack the capability to autonomously acquire complex skills within novel environments without pre-training or fine-tuning. This inability to learn "on the fly" restricts the utility of agents in dynamic, real-world scenarios where environments are unknown and constantly changing. Established adaptation methods, such as reflection-based mechanisms and reinforcement learning (RL), struggle to improve performance in these settings; specifically, these approaches fail to generalize well to unseen tasks or suffer from severe data inefficiency.

The authors introduce **EvoTest**, a gradient-free evolutionary framework designed for test-time learning that operates without modifying underlying model weights. Instead of relying on backpropagation, EvoTest evolves the agentic system's configuration using a cooperative dual-role architecture:
*   **Actor Agent:** Executes the primary task.
*   **Evolver Agent:** Analyzes episode transcripts to propose structural revisions.

The learnable configuration is defined as a tuple comprising policy prompts ($p$), deployment-time memory ($M$), hyperparameters ($h$), and tool-use routines ($u$). Through evolutionary operatorsâ€”such as prompt mutation, memory updates, and tool refinementâ€”and selection governed by the Upper Confidence Bound (UCB) algorithm, the system leverages rich narrative feedback to iteratively optimize its decision-making logic.

**Evaluations on the J-TTL benchmark demonstrate EvoTest's superior performance and data efficiency.** Crucially, EvoTest achieved a **100% win rate** in the complex 'Detective' and 'Library' games, whereas all reflection-based baselines (Reflexion, MemGPT, MemoryBank) failed completely with a **0% win rate**. Regarding the magnitude of improvement, EvoTest significantly outperformed competitors in total score; for example, in 'Zork1', EvoTest achieved an average score of approximately **35**, which is roughly **7x higher** than Reflexion's score of **5**. Furthermore, EvoTest demonstrated high sample efficiency, effectively converging to optimal strategies within fewer than **10 episodes**, while RL methods proved too slow and data-inefficient to achieve comparable results.

---

## Key Findings

*   **Critical Limitation in Current AI:** Existing AI agents cannot effectively learn complex skills on the fly in novel environments, significantly limiting their practical utility.
*   **Benchmark Shortcomings:** Established adaptation methods, including reflection and reinforcement learning, struggle to improve performance on the new Jericho Test-Time Learning (J-TTL) benchmark.
*   **Superior Performance:** EvoTest consistently outperforms strong baselines, including reflection-based and memory-only agents.
*   **Complex Task Mastery:** EvoTest is the only evaluated method capable of achieving victory in specific complex scenarios like the 'Detective' and 'Library' games.
*   **Method Failure:** Analysis shows that reflection methods failed to alter decision logic effectively, while RL was too slow and data-inefficient for these scenarios.

---

## Methodology

The research proposes **EvoTest**, an evolutionary test-time learning framework designed to function without gradients or fine-tuning. The core methodology relies on a cooperative two-agent design and an evolutionary approach to system configuration:

1.  **Dual-Role System:**
    *   **Actor Agent:** Responsible for executing the primary task within the environment.
    *   **Evolver Agent:** Analyzes episode transcripts to propose structural revisions and improvements.
2.  **Configuration Evolution:** Instead of updating model weights, the framework evolves the entire agentic system. This involves:
    *   Rewriting system prompts.
    *   Updating memory.
    *   Tuning hyperparameters.
    *   Learning tool-use routines.
3.  **Gradient-Free Operation:** The approach avoids backpropagation entirely, relying instead on evolutionary strategies to adapt the agent's behavior at test time.

---

## Technical Details

*   **Framework Type:** Gradient-free, evolutionary test-time learning (does not update model weights).
*   **Base Model:** Operates on a fixed backbone LLM.
*   **Architecture:** Cooperative two-agent design (Actor Agent + Evolver Agent).
*   **Learnable Configuration:** Represented as a tuple $\theta = (p, M, h, u)$:
    *   $p$: Policy prompt.
    *   $M$: Deployment-time memory (success/failure logs).
    *   $h$: Hyperparameters (temperature/exploration settings).
    *   $u$: Tool-use routines (memory interaction/state abstraction).
*   **Update Mechanism:** Utilizes evolutionary operators including:
    *   Prompt mutation.
    *   Memory update.
    *   Hyperparameter tuning.
    *   Tool-use refinement.
*   **Selection Strategy:** Upper Confidence Bound (UCB).
*   **Feedback Loop:** Leverages rich narrative feedback rather than sparse rewards.

---

## Results

Evaluations were conducted on the **J-TTL (Jericho Test-Time Learning) benchmark**, utilizing text-based games modeled as POMDPs (Partially Observable Markov Decision Processes), including *Detective*, *Library*, *Zork1*, *Zork3*, *Balances*, and *Temple*.

**Protocol:**
*   $K$ consecutive episodes resetting to the same initial state.
*   Step limit of 110 per episode.
*   Primary metric: Total Score (Return) per episode.

**Outcomes:**
*   **vs. Reflection/Memory Agents:** EvoTest consistently outperformed baselines like Reflexion, MemGPT, and MemoryBank.
*   **vs. Reinforcement Learning:** RL methods were found to be too slow and data-inefficient compared to EvoTest.
*   **Unique Achievement:** EvoTest was the sole method to achieve victory in 'Detective' and 'Library' games.

---

## Contributions

*   **J-TTL Benchmark:** Introduction of the Jericho Test-Time Learning Benchmark to measure progress in test-time learning capabilities.
*   **EvoTest Framework:** Development of a gradient-free self-improvement framework that evolves system configurations rather than relying on backpropagation.
*   **Empirical Validation:** Demonstrated the limitations of current state-of-the-art methods (Reflection, RL) and established EvoTest as the new state-of-the-art solution for continuous on-the-fly adaptation.

---

**Quality Score:** 8/10  
**References:** 40 citations