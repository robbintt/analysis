# Capturing Individual Human Preferences with Reward Features

*André Barreto; Vincent Dumoulin; Yiran Mao; Nicolas Perez-Nieves; Bobak Shahriari; Yann Dauphin; Doina Precup; Hugo Larochelle*

---

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **References:** 38 citations
> *   **Key Architecture:** Reward Features Model (RFM)
> *   **Variants Tested:** RFM(8), RFM(32), RFM(128)
> *   **Top Competitors:** GPT-4o, Gemini 1.5 Pro
> *   **Primary Metrics:** Test Accuracy, Win Rate

---

## Executive Summary

Traditional Reinforcement Learning from Human Feedback (RLHF) relies on monolithic reward models that aggregate diverse human preferences into a single function. This approach fails in contexts characterized by high disagreement or subjectivity, such as creative writing or conversational style, because it cannot distinguish between individual user values. This limitation presents a critical barrier to aligning Large Language Models (LLMs) with specific user groups, restricting the applicability of current alignment techniques in heterogeneous environments where broad consensus is absent.

The authors introduce the **Reward Features Model (RFM)**, a novel architecture that decomposes reward modeling into the learning of shared general features and user-specific weights. Technically, RFM posits that individual preferences are derived from a linear combination of a latent set of reward features ($\phi$) shared across the population. By learning these underlying features and explicitly distinguishing between raters to handle conflicting signals, the model can specialize to specific individuals or groups through the estimation of simple linear weights. This approach enables zero-shot or few-shot adaptation to new users not present in the original training data, contrasting with complex adaptive methods by offering a streamlined architecture.

The proposed architecture demonstrated significant advantages over baselines, evaluated using Test Accuracy and Win Rate across 10 runs. In "Scenario 2" experiments—characterized by high disagreement—non-adaptive baselines degraded to chance performance, and adaptive in-context models suffered performance losses; however, RFM remained robust, maintaining performance levels comparable to the low-disagreement "Scenario 1" control. Benchmark comparisons against state-of-the-art In-Context LLMs revealed that RFM matches or exceeds the performance of Gemini 1.5 Pro and GPT-4o while utilizing a simpler architecture. Additionally, adaptation efficiency analysis demonstrated that RFM scales effectively, improving accuracy as the number of adaptation examples per user increased from 10 to 90.

This research establishes a new framework for personalizing RLHF, addressing the fundamental challenge of modeling human disagreement without the instability often associated with adaptive in-context learning. By demonstrating that individual differences can be efficiently captured through linear feature decomposition, the authors provide a path toward LLMs that respect diverse user values through parameter-efficient adaptation.

---

## Key Findings

*   **Limitations of Traditional RLHF:** Standard RLHF is insufficient for contexts with high potential for disagreement. It relies on a single, monolithic reward model that fails to distinguish between individual preferences.
*   **Mathematical Representation:** Individual human preferences can be effectively captured and mathematically represented as a **linear combination of a shared set of general reward features**.
*   **Rapid Specialization:** The proposed method allows for the quick specialization of a reward model to a specific individual or group, even in cases where that individual's preferences were **not present in the original training data**.
*   **Performance in High Disagreement:** In experiments involving Large Language Models (LLMs), the proposed architecture significantly outperforms baselines when the training data exhibits high levels of disagreement.
*   **Simplicity & Stability:** Compared to adaptive counterparts that utilize in-context personalization, this model either matches or exceeds performance while utilizing a simpler architecture and providing **more stable training dynamics**.

---

## Core Contributions

*   **Personalization Framework for RLHF:** Introduced a novel method for specializing reward models to individuals or groups, addressing the critical gap of handling human disagreement in LLM training.
*   **Linear Feature Decomposition:** Established that individual differences in preference can be efficiently modeled through a linear combination of underlying reward features, enabling zero-shot or few-shot adaptation to new users.
*   **Architectural Optimization:** Demonstrated that feature-based adaptation can achieve state-of-the-art performance (particularly in high-disagreement scenarios) while offering a simpler architectural design and greater training stability than existing adaptive methods like in-context personalization.

---

## Methodology

The authors propose a specialized architecture for RLHF that decomposes the reward modeling process. Instead of training a single static model, the approach involves learning a set of general reward features.

**Core Mechanism**
The approach assumes that specific user preferences are derived from a linear combination of these learned features. Once the general features are learned, the model uses this linear relationship to adapt (specialize) the reward model to a specific person or group.

**Validation Strategy**
The methodology was validated by comparing the proposed architecture against:
1.  **Non-adaptive models**
2.  **Adaptive models** using in-context personalization

Performance was evaluated across varying degrees of disagreement in the data to test robustness.

---

## Technical Overview

*   **Architecture Name:** Reward Features Model (RFM)
*   **Mathematical Basis:** Models human preferences as a linear combination of shared general reward features (Equation 7).
*   **Feature Learning:** Learns a set of underlying reward features ($\phi$) shared across users.
*   **Conflict Handling:** Explicitly distinguishes between raters during training to handle conflicting signals.
*   **Adaptation:** Enables rapid specialization to held-out users with few examples by estimating linear weights.
*   **Variants Tested:** RFM(8), RFM(32), and RFM(128).
*   **Comparison Targets:** Non-adaptive baselines, Adaptive Linear baselines, and In-Context LLMs (Gemini 1.5 Pro, GPT-4o).

---

## Experimental Results

**Metrics**
*   Primary metrics utilized were **Test Accuracy** and **Win Rate**.
*   Reported with 95% confidence intervals over 10 runs.

**Scenarios**
*   **Scenario 2 (High Disagreement):**
    *   Non-adaptive baselines failed to chance.
    *   Adaptive baselines degraded.
    *   **RFM Result:** Remained robust, maintaining performance roughly equal to the control Scenario 1.

**Benchmark Comparisons**
*   RFM matches or exceeds the performance of **GPT-4o** and **Gemini 1.5 Pro** with a simpler architecture.

**Adaptation Efficiency**
*   Analysis demonstrated that RFM scales high accuracy effectively.
*   Performance improved as the number of adaptation examples increased from **10 to 90**.