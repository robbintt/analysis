# Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs

*Lukas Edman; Alexander Fraser*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Challenge** | BabyLM Challenge 2025 (Strict-Small Track) |
| **Method** | Adaptive Masked Language Modeling (AMLM) |
| **Key Update Frequency** | Every 200 batches |
| **EMA Decay Factor ($\lambda$)** | 0.2 |
| **Global Masking Rate** | 15% |
| **Quality Score** | 7/10 |

---

## Executive Summary

This research addresses the inefficiency of standard **Masked Language Modeling (MLM)** in pretraining BabyLMsâ€”small-scale language models constrained by limited data. The paper identifies that static, uniform masking fails to create a curriculum learning environment, preventing small models from focusing on tokens they have not yet mastered. Furthermore, it highlights the difficulty small models face in morphological generalization, which is essential for robust performance on downstream linguistic tasks.

To overcome these limitations, the authors propose **Adaptive Masked Language Modeling (AMLM)**, a dynamic strategy that correlates masking difficulty with the model's predictive confidence. Rather than applying random masking, AMLM assigns higher masking probabilities to tokens with lower prediction confidence (higher error), focusing learning on difficult structures. This mechanism relies on smoothed accuracy or inverted cross-entropy, with probabilities updated every 200 batches using an Exponential Moving Average (EMA). The architecture is further enhanced with an **N-hot Embedding Architecture** to explicitly inject character-level sub-token information, directly aiding morphological processing.

In the strict-small track of the 2025 BabyLM Challenge, the proposed model significantly outperformed standard MLM baselines, delivering substantial improvements across both GLUE and SuperGLUE benchmarks. The model demonstrated particular strength in morphological generalization, showing marked gains in tasks such as adjective nominalization and past tense conversion.

This work establishes a new performance benchmark for the BabyLM Challenge and provides empirical evidence that optimizing the pretraining objective is as critical as architectural design for small-scale models. By validating the efficacy of adaptive, confidence-based masking, the paper suggests that future pretraining efforts should transition away from static random masking toward dynamic curriculum learning.

---

## Key Findings

*   **Significant Performance Boost:** Achieved a substantial increase in performance on **(Super)GLUE** tasks compared to standard Masked Language Modeling (MLM).
*   **Morphological Generalization:** Demonstrated that incorporating **sub-token embeddings** explicitly increases the model's ability to generalize morphologically.
*   **Challenge Victory:** The proposed strategy successfully outperformed the baseline in the **strict-small track** of the 2025 BabyLM Challenge.
*   **Curriculum Efficacy:** Validated that dynamic masking based on token difficulty is more effective for small models than static uniform masking.

---

## Methodology

### 1. Adaptive Masked Language Modeling (AMLM)
The authors implemented an improved form of MLM that dynamically adapts the probabilities of token masking to create a curriculum learning effect.
*   **Dynamic Probability:** The masking probability is determined based on the model's current ability to predict the token, rather than relying on static uniform masking.
*   **Focus on Difficulty:** Tokens that the model struggles with (low confidence) are masked more frequently, forcing the model to focus on areas where it is "underperforming."

### 2. Sub-token Embeddings
To enhance morphological processing, the team integrated sub-token embeddings directly into the model architecture. This allows the model to access finer-grained, character-level information, which is crucial for understanding word structure and formation.

---

## Technical Details

The implementation relies on specific hyperparameters and architectural choices to optimize the pretraining process:

*   **Update Frequency:** Probabilities are updated every **200 batches**.
*   **Smoothing Mechanism:** Uses an **Exponential Moving Average (EMA)** with a decay factor of **0.2** to stabilize the probability updates.
*   **Scoring Metrics:** Token difficulty is scored using either:
    *   Smoothed accuracy (hard metric)
    *   Inverted cross-entropy (soft metric)
*   **Global Constraint:** The sequence average masking rate is mathematically constrained to remain at **15%**, ensuring standard data coverage is maintained.
*   **Architecture:** Utilizes an **N-hot Embedding Architecture** to explicitly incorporate character-level sub-token information.

---

## Contributions

*   **Novel Optimization:** Introduced a new optimization for Masked Language Modeling that correlates masking difficulty directly with the model's predictive confidence.
*   **Empirical Evidence:** Provided concrete data supporting the use of sub-token embeddings for improving morphological generalization in small-scale language models.
*   **Benchmark Performance:** Established a new performance benchmark in the strict-small track of the BabyLM Challenge, offering a high-performing model architecture for future research.

---

## Results

The proposed method delivered measurable improvements across several key areas:

*   **Benchmark Success:** Achieved substantial performance increases on **GLUE** and **SuperGLUE** benchmarks compared to standard MLM baselines.
*   **Challenge Ranking:** Outperformed the baseline in the **BabyLM Challenge 2025** strict-small track.
*   **Morphological Tasks:** Successfully improved performance on:
    *   Adjective nominalization
    *   Past tense conversion
*   **Validation of Hyperparameters:** The experimental results validated the chosen configuration (Lambda 0.2, timestep 200 batches, baseline 15%) as effective for curriculum learning in low-resource regimes.

---

**References:** 4 citations