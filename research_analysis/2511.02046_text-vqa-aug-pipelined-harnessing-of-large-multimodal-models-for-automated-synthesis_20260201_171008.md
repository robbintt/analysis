# Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis

*Soham Joshi; Shwet Kamal Mishra; Viswanath Gopalakrishnan*

***

> ### **Quick Facts**
>
> *   **Dataset Size:** ~72,000 QA pairs
> *   **Source Images:** 44,000 unique images
> *   **Initial Pool:** 9 million images (OpenImages V5)
> *   **Manual Annotation:** 0% (Fully Automated)
> *   **Pipeline Stages:** 8 Steps
> *   **Quality Score:** 8/10

---

## Executive Summary

The creation of large-scale datasets for text-centric Visual Question Answering (text-VQA) is currently hindered by the prohibitive cost and effort required for manual human annotation. As the demand for robust models capable of reading and reasoning about text within images increases, the scarcity of high-quality, annotated training data has emerged as a critical bottleneck. This paper addresses the challenge of automating the dataset synthesis lifecycle, aiming to eliminate the reliance on skilled human annotators while ensuring the generation of faithful, high-volume data necessary for training advanced machine learning systems.

The authors introduce "Text-VQA Aug," an end-to-end, eight-step serial pipeline that integrates Large Multimodal Models (LMMs) with specialized computer vision algorithms to automate data synthesis and validation simultaneously. The system utilizes a cascaded architecture where the output of one component streams directly into the next: it begins with image curation from OpenImages V5, followed by text spotting using the GLASS OCR model. A critical innovation is the use of Kosmos-2 for local context identification and visual grounding, which extracts high-resolution object crops for LLaVA-R to generate detailed captions. The pipeline concludes with an automated aggregation stage—employing case-insensitive matching, thresholding, and grouping—followed by prompt-engineered question generation and post-processing to ensure data fidelity and reduce noise.

The proposed pipeline demonstrates significant scalability, successfully synthesizing approximately 72,000 Question-Answer pairs derived from 44,000 unique images. The system processed a massive initial pool of 9 million images, filtering it down to 200,000 text-containing images before executing the synthesis workflow. A primary result of this research is the achievement of zero manual human annotation; the system managed the entire synthesis and validation process autonomously. This validates the pipeline's ability to integrate complex components—ranging from OCR to visual grounding—into a cohesive, functional workflow capable of operating at industrial scale.

This research makes a substantial contribution to the field by presenting the first known pipeline capable of automatically synthesizing and validating a large-scale text-VQA dataset. By removing the manual annotation bottleneck, the work provides a scalable framework that can accelerate data generation for multimodal AI research. The immediate impact is the release of a substantial new resource (72K QA pairs) for the community, but the broader significance lies in establishing a blueprint for automated dataset creation. This approach suggests a potential paradigm shift away from labor-intensive labeling toward pipeline-driven automation, facilitating faster iteration and development of vision-language models.

---

## Key Findings

*   **Zero Human Annotation:** The proposed pipeline successfully synthesizes a faithful, large-scale text-VQA dataset without the need for manual human annotation.
*   **Significant Scale:** The system achieved significant scale, generating approximately **72,000 QA pairs** derived from around **44,000 images**.
*   **Cohesive Integration:** The pipeline demonstrates scalability and integrates multiple complex components (OCR, ROI detection, captioning) into a cohesive, end-to-end process for simultaneous synthesis and validation.
*   **First of its Kind:** This is the first pipeline capable of automatically synthesizing and validating a dataset of this magnitude for text-VQA.

---

## Methodology

The researchers developed an end-to-end pipeline that streams the output of one component into the next to automate the entire workflow. This method harnesses specific algorithms and models to process vision and language modalities effectively.

**Key Elements:**
*   **OCR Detection and Recognition:** Utilized for accurate text spotting.
*   **Region of Interest (ROI) Detection:** Identifies specific areas within the image for analysis.
*   **Caption & Question Generation:** Uses Large Multimodal Models (LMMs) to understand context and generate relevant text.
*   **Validation:** Ensures the fidelity of the synthesized data automatically.

---

## Technical Details

The Text-VQA Aug system is an **8-step serial pipeline** designed to automate the synthesis of Question-Answer pairs. It employs a cascaded architecture of Large Multimodal Models and specialized algorithms:

*   **Step 1: Image Curation**
    *   Sourcing images from OpenImages V5.
*   **Step 2: Text Spotting**
    *   Utilizing the **GLASS model** for OCR (Optical Character Recognition).
*   **Step 3: Local Context Identification**
    *   Using **Kosmos-2** for visual grounding and extracting high-resolution object crops.
*   **Step 4: Caption Generation**
    *   Leveraging **LLaVA-R** to create detailed image captions.
*   **Step 5 & 6: Aggregation & Answer Selection**
    *   Employing a specific algorithm for case-insensitive matching, thresholding, grouping, and sorting.
*   **Step 7 & 8: Question Generation & Post-Processing**
    *   utilizing prompt engineering for question generation and noise reduction.

---

## Contributions

*   **Elimination of Bottleneck:** Addresses the bottleneck of 'skilful human annotation' by providing a fully automated alternative for creating text-VQA datasets.
*   **Novel Pipeline:** Presents the first known pipeline proposed for the automated synthesis and validation of large-scale text-VQA datasets.
*   **Resource Creation:** Includes the actual creation of a substantial new dataset resource (**72K QA pairs**) for the research community to utilize.

---

## Results

*   **Output Volume:** The pipeline produced approximately **72,000 synthesized QA pairs**.
*   **Image Source:** Data derived from **44,000 unique images**.
*   **Filtering Process:** Processed an initial set of 200,000 text-containing images, which were filtered from a pool of **9 million images** in OpenImages V5.
*   **Autonomy:** The system achieved **zero manual human annotation**.
*   **Validation:** Described as the first pipeline capable of automatically synthesizing and validating a text-VQA dataset of this magnitude.

---
**References:** 40 citations