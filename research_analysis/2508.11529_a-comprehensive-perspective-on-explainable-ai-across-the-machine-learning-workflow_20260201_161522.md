# A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow

*George Paterakis; Andrea Castellani; George Papoutsoglou; Tobias Rodemann; Ioannis Tsamardinos*

---

### ðŸ“Š Quick Facts

*   **Quality Score:** 8/10
*   **References:** 40 Citations
*   **Study Type:** Conceptual Framework and Gap Analysis
*   **Key Innovation:** Holistic Explainable AI (HXAI) Framework
*   **Primary Focus:** User-centric XAI across the entire ML workflow

---

## Executive Summary

Current Explainable AI (XAI) research suffers from a narrow, model-centric focus that limits its practical utility. Existing methods predominantly target the interpretation of individual model predictions while neglecting the broader "upstream" decisions in data preparation and the "downstream" requirements of quality assurance and communication. This creates a critical disconnect, described by the authors as an **"asymmetry of skills,"** where technical outputs fail to meet the cognitive and actionable needs of diverse stakeholders such as domain experts, data analysts, and data scientists. Without addressing these workflow-wide gaps, organizations struggle to deploy AI systems that are truly transparent and trustworthy across the entire data-analysis lifecycle.

The paper introduces the **Holistic Explainable AI (HXAI)** framework, a user-centric paradigm that integrates computer science with principles from cognitive science and Human-Computer Interaction (HCI). Technically, HXAI proposes a unified taxonomy comprising six distinct stages of the machine learning workflow: Data, Analysis Setup, Learning Process, Model Output, Model Quality, and Communication Channel. To operationalize this framework, the authors architect a solution utilizing LLM-augmented agents. These agents possess planning and contextual augmentation capabilities, allowing them to dynamically orchestrate diverse explanation techniques. Instead of raw technical outputs, these agents translate complex artifacts into tailored narratives that align with the specific requirements of different user roles.

As a foundational and conceptual study, the paper does not present experimental performance metrics for a deployed system. Instead, the primary results derive from a rigorous gap analysis utilizing a newly developed **112-item question bank** designed to evaluate tool coverage against stakeholder needs. This evaluation of contemporary toolchains revealed significant deficiencies, specifically identifying that current literature and tools heavily bias toward model prediction explanations while largely ignoring data explainability and result communication. The survey quantitatively highlighted that existing tooling fails to provide the clear, actionable, and cognitively manageable insights required by non-technical stakeholders.

This work significantly advances the field by establishing a standardized vocabulary and a comprehensive benchmark for assessing XAI systems. By defining a unified workflow taxonomy and distinguishing between **"interpretability"** (how a decision was made) and **"explainability"** (why a decision matters to the human), the paper reduces the terminological ambiguity that currently hinders interdisciplinary collaboration. The introduction of the 112-item evaluation metric provides a concrete tool for future researchers to assess the completeness of their solutions. Furthermore, the proposal to use LLM agents to bridge the gap between developers and domain experts sets a new architectural direction for creating responsible AI systems that genuinely support human decision-making.

---

## Key Findings

*   **Limitations of Conventional XAI:** Current methods focus narrowly on individual predictions while ignoring upstream decisions and downstream quality checks within the workflow.
*   **Critical Coverage Gaps:** A survey of contemporary tools using a 112-item question bank revealed significant deficiencies in meeting the specific needs of domain experts, data analysts, and data scientists.
*   **User-Centric Requirements:** Effective explanations must be clear, actionable, and cognitively manageable, requiring grounding in human explanation theories, HCI principles, and empirical user studies.
*   **LLM-Augmented Orchestration:** AI agents embedded with Large Language Models (LLMs) can bridge the gap between developers and domain experts by orchestrating diverse explanation techniques into stakeholder-specific narratives.

---

## Methodology

*   **Theoretical Framework Development:** Developed the Holistic Explainable Artificial Intelligence (HXAI) framework by synthesizing theories of human explanation, HCI principles, and empirical user studies.
*   **Taxonomy Construction:** Created a unified taxonomy consisting of six distinct workflow components aligned with the requirements of different user roles.
*   **Gap Analysis via Surveying:** Generated a 112-item question bank based on user needs to rigorously evaluate the coverage of existing toolchains.
*   **Conceptual Demonstration:** Demonstrated the application of HXAI by proposing AI agents with LLMs to translate technical artifacts into tailored narratives.

---

## Contributions

*   **Holistic Explainable AI (HXAI):** A novel, user-centric framework that extends explanation capabilities beyond individual model predictions to the entire data-analysis workflow.
*   **Unified Workflow Taxonomy:** A standardized taxonomy that operationalizes the six stages of the machine learning workflow, reducing terminological ambiguity and facilitating rigorous analysis of AI tools.
*   **Comprehensive Evaluation Metric:** The creation of a detailed 112-item question bank that serves as a benchmark for assessing how well current tools address stakeholder needs.
*   **Interdisciplinary Synthesis:** A critical synthesis of literature from multiple disciplines (CS, cognitive science, HCI) to advance the understanding of trust and responsible AI deployment.
*   **Architectural Innovation for Communication:** A proposal for using LLM-embedded agents to dynamically translate complex technical artifacts into accessible, stakeholder-specific narratives.

---

## Technical Details

### HXAI Framework Components
The framework integrates technical and user-centered explainability across six stages:
1.  **Data**
2.  **Analysis Setup**
3.  **Learning Process**
4.  **Model Output**
5.  **Model Quality**
6.  **Communication Channel**

### Architecture & Configuration
*   **Agent Capabilities:** Utilizes an LLM-augmented agent with planning, external tool usage, and contextual augmentation capabilities.
*   **Stakeholder Categories:** Defines three primary user roles:
    *   Data Scientists
    *   Data Analysts
    *   Domain Experts

### Key Definitions
*   **Interpretability:** Explaining *how* a decision was made (technical mechanism).
*   **Explainability:** The process explaining *why* to bridge the gap between model and human (contextual reasoning).

---

## Results

**Nature of Study:** The provided text is conceptual and foundational, containing no experimental results or quantitative performance metrics for the HXAI framework.

**Survey Findings:**
*   The Abstract references a 112-item survey that identified significant deficiencies in current tool coverage.
*   **Qualitative Findings:**
    *   **Asymmetry of Skills:** Technical outputs fail to meet decision-maker needs.
    *   **Coverage Gaps:** Literature focuses predominantly on model predictions rather than data or result explainability.