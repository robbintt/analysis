# Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning

*Yash Jhaveri; Harley Wiltzer; Patrick Shafto; Marc G. Bellemare; David Meger*

---

> ### **Quick Facts**
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 40 Citations |
> | **Core Mechanism** | Temperature Decoupling Gambit |
> | **Focus** | Theoretical Convergence Guarantees |

---

## Executive Summary

### **The Problem**
This research addresses the theoretical gaps within Entropy-Regularized Reinforcement Learning (ERL) and distributional RL, specifically the lack of formal convergence guarantees for policies and return distributions as entropy regularization vanishes. Standard RL methods often converge to arbitrary optimal policies in scenarios with multiple equally rewarding solutions, effectively operating as a "black box" where the specific behavioral characteristics of the converged policy are uncontrolled. Furthermore, while value function convergence is well-studied, the convergence of the full return distribution—crucial for risk-sensitive and distributional applications—remains under-explored in the context of entropy-regularized frameworks.

### **The Innovation**
The core innovation introduced is the **Temperature Decoupling Gambit**, a theoretical intervention that separates the temperature parameter governing value estimation (Target Temperature, `σ`) from the parameter controlling policy sampling (Execution Temperature, `τ`). By enforcing the condition that `lim(τ→0) σ(τ)/τ = 0`, the framework allows the entropy regularization to gradually decay (`τ → 0`) while maintaining a sufficiently "soft" value target to ensure stability. This mechanism enables the accurate estimation of return distributions for the limiting policy without compromising the theoretical guarantees required for convergence.

### **The Results**
The paper provides rigorous theoretical proofs for convergence rather than empirical benchmarks.
*   **Theorem 3.2** establishes that standard ERL values converge to a reference-optimal 'skyline' rather than the classical optimal value.
*   **Theorem 3.9** proves that under the Temperature Decoupling Gambit, policies converge to a **Reference-Optimal Policy** that preserves diversity.
*   **Theorem 3.10** guarantees that the return distributions converge to the reference-optimal distribution under the Wasserstein-based metric.

### **The Impact**
These findings significantly advance the theoretical foundations of distributional RL by extending convergence guarantees from scalar value functions to full return distributions. By explicitly characterizing which optimal policy is learned, the framework moves beyond standard RL methods that accept arbitrary maximum-reward solutions, offering practitioners a mechanism to enforce desirable properties such as diversity and interpretability. This ensures that agents do not simply collapse to a single arbitrary action when multiple optimal choices exist, thereby improving the reliability and robustness of deployed RL systems.

---

## Key Findings

*   **Guaranteed Convergence to Specific Policies:** The research establishes a theoretical framework that guarantees convergence of policy optimization to a specific optimal policy with definable characteristics.
*   **Interpretability and Diversity Preservation:** The method enables the realization of an interpretable, diversity-preserving optimal policy as the regularization temperature approaches zero (e.g., uniform sampling of optimal actions).
*   **Convergence of Derived Objects:** The framework ensures that the policy, value functions, and return distributions converge within the entropy-regularized setting.
*   **Accurate Return Distribution Estimation:** The proposed approach allows for estimation of the return distribution associated with the diversity-preserving optimal policy to arbitrary accuracy.

---

## Methodology

The proposed framework relies on a theoretical approach incorporating two primary mechanisms:

1.  **Vanishing Entropy Regularization**
    This mechanism gradually reduces the entropy regularization term to guide the policy toward a specific structure.

2.  **Temperature Decoupling Gambit**
    This is the core algorithmic intervention. It decouples the temperature parameter to facilitate the estimation of return distributions for the limiting policy without compromising theoretical guarantees.

---

## Contributions

*   **Characterization of Learned Policies:** The framework explicitly characterizes *which* optimal policy will be learned, effectively addressing the black-box nature of standard RL methods.
*   **Theoretical Advancement in Distributional RL:** The paper provides formal convergence theorems for entropy-regularized reinforcement learning, extending guarantees from value functions to full return distributions.
*   **Control over Optimal Behavior:** The work introduces a mechanism for enforcing desirable properties (such as diversity and uniform sampling) in the limiting optimal policy, rather than accepting arbitrary maximum-reward solutions.

---

## Technical Details

### **Problem Formulation**
*   Utilizes Markov Decision Processes (MDPs) where policies are defined as probability kernels.
*   Employs **Entropy-Regularized RL (ERL)** with an objective maximizing expected reward against KL-divergence from a reference policy.

### **Core Architectural Innovation**
*   **Temperature Decoupling Gambit:** Separates the Target Temperature (`σ`) for value estimation from the Execution Temperature (`τ`) for policy sampling.
*   **Mathematical Constraint:** Requires that `lim(τ→0) σ(τ)/τ = 0`.

### **Theoretical Components**
*   **Boltzmann-Gibbs Policy:** Used to model the policy structure.
*   **Bellman Reference-Optimality Operator:** Key operator in the update process.
*   **Return Distribution Functions:** Analyzed via Wasserstein-based metrics.

---

## Results

The findings present theoretical convergence guarantees rather than empirical benchmarks:

*   **Theorem 3.2:** Establishes that standard ERL values converge to a reference-optimal 'skyline' rather than the classical optimal value.
*   **Theorem 3.9:** Proves that under the gambit with `σ/τ → 0`, policies converge to a Reference-Optimal Policy.
    *   Measured by Total Variation distance for discrete action spaces.
    *   Measured by weak convergence for continuous spaces.
*   **Theorem 3.10:** Ensures return distributions converge to the reference-optimal distribution under the metric `d_{p;p',ω}`.
*   **Diversity Quantification:** The approach quantifiably preserves diversity by behaving as a uniform sampler over optimal actions.