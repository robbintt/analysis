# Prospective Learning in Retrospect
*Yuxin Bai; Cecelia Shuai; Ashwin De Silva; Siyu Yu; Pratik Chaudhari; Joshua T. Vogelstein*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **Core Focus:** Non-stationary data & Sequential Decision Making
> *   **Framework:** Prospective Learning (vs. PAC)
> *   **Key Experiment:** Prospective Foraging
> *   **Reproducibility:** Open-source code available
> *   **Citations:** 0

---

### **Executive Summary**

The paper addresses the fundamental inadequacy of the traditional Probably Approximately Correct (PAC) learning framework for real-world AI applications. PAC relies on the assumption that data is independent and identically distributed (i.i.d.), which fails in dynamic environments where data distributions are non-stationary and objectives evolve. The authors argue that patching PAC models is insufficient and that a foundational shift is required to handle stochastic processes where the data is not static.

The authors propose **"prospective learning,"** a framework that mathematically departs from PAC by treating data as a stochastic process. The technical core involves augmenting the input space with time ($X \leftarrow X \times T$), allowing the hypothesis space to map inputs and time $(x_t, t)$ to a sequence of predictors $h: \mathbb{N} \times X \to Y$. The objective shifts from minimizing empirical error to minimizing integrated future loss (Prospective Risk). This is implemented via Prospective-MLP, which utilizes sinusoidal time embeddings similar to Transformer positional encodings and a modified Empirical Risk Minimizer (Prospective ERM) to minimize cumulative prospective loss over past observations.

The proposed framework demonstrated measurable improvements over existing baselines. Specifically, Prospective-MLP achieved convergence to the Bayes risk on synthetic benchmarks, whereas task-agnostic online continual learning methods failed to improve upon chance-level prospective risk. These gains were validated across three specific environments: a Periodic Process (switching tasks every 10 steps), an Infinite Task Process (defined by time-varying thresholds), and a Dependent Structured Process (a Hierarchical HMM with evolving governance). Additionally, the framework performed competitively against standard Reinforcement Learning methods in sequential decision-making scenarios.

This work establishes prospective learning as a rigorous alternative to PAC for non-stationary environments, extending the framework's generalizability from static problems to complex, multi-step interactions. The inclusion of the "Prospective Foraging" experiment is critical, as it validates the framework's ability to optimize for future consequences rather than just past errors, proving its efficacy in planning-heavy scenarios. The authors have released open-source code to facilitate reproducibility, providing a mathematically grounded foundation for future AI systems required to operate in dynamic, evolving conditions.

---

## Key Findings

*   **Inadequacy of PAC:** The traditional Probably Approximately Correct (PAC) learning framework is insufficient for real-world AI applications because it fails to account for non-stationary data distributions and evolving objectives.
*   **Framework Solution:** The prospective learning framework successfully addresses the limitations of PAC learning in dynamic environments.
*   **Performance Gains:** The proposed enhancements to the prospective learning framework yield improved algorithmic performance and better numerical results compared to previous iterations.
*   **Generalizability:** The framework is generalizable enough to handle complex sequential decision-making scenarios, as demonstrated in foraging tasks.

---

## Technical Details

### Core Framework
Prospective Learning deviates from the PAC framework by assuming data is drawn from a **stochastic process** rather than i.i.d. from a fixed distribution.
*   **Input Space:** Augmented with time $T$ ($X \leftarrow X \times T$), mapping inputs to $(x_t, t)$.
*   **Hypothesis Space:** Outputs an infinite sequence of predictors or a time-varying function $h: \mathbb{N} \times X \to Y$.

### Loss and Risk
The objective is to minimize integrated future loss rather than past error.
*   **Prospective Loss:** $\bar{\ell}(h, z_{>t}) = \sum_{s>t} w(s - t)\ell(h(x_s), y_s)$
*   **Prospective Risk:** The conditional expectation of this loss given observed history.

### Algorithm
*   **Prospective ERM:** Acts as a modified Empirical Risk Minimizer that minimizes cumulative prospective loss over all past observation points, functioning as a strong prospective learner for finite stochastic processes.

### Architecture
*   **Prospective-MLP:** Takes concatenated input $(\phi(s), x_s)$, where $\phi(s)$ is a time embedding inspired by Transformer positional encodings using sinusoidal functions.

---

## Methodology

The authors build upon the existing mathematical framework of **'prospective learning'** rather than modifying traditional PAC-based models.
*   **Refinement:** The study involves refining the algorithms within the prospective learning framework to optimize performance.
*   **Testbed:** The methodology specifically applies the framework to sequential decision-making problems, using foraging scenarios as the primary testbed.
*   **Validation:** The approach is validated through numerical results and the release of open-source code for reproducibility.

---

## Results

### Performance
*   **Bayes Convergence:** Prospective-MLP achieves convergence to the Bayes risk on synthetic and image data stochastic processes.
*   **Benchmark Superiority:** In comparisons, task-agnostic online continual learning methods fail to improve upon chance-level prospective risk on periodic process benchmarks, whereas prospective learning succeeds.
*   **RL Comparison:** In the 'Prospective Foraging' scenario, the framework performs competitively with standard Reinforcement Learning methods.

### Benchmark Specifications
1.  **Periodic Process:** Features 2 distinct tasks that switch exactly every 10 time steps.
2.  **Infinite Task Process:** Defines labels $y_t=0$ if $x_t > t$, with inputs drawn from a uniform distribution shifting by a small gradient.
3.  **Dependent Structured Task Process:** A Hierarchical HMM with 4 tasks governed by two separate Markov chains, where chain governance changes every 10 time-steps.

---

## Contributions

*   **Algorithmic Stability:** Significant improvements to the algorithms and numerical stability of the prospective learning framework.
*   **Sequential Extension:** Extension of prospective learning beyond static learning problems to sequential decision-making scenarios (specifically foraging).
*   **Open Science:** Provision of open-source code (hosted on GitHub) to facilitate reproducibility and further research in the community.