# Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction

*Yujia Tong; Jingling Yuan; Chuang Hu*

---

### üìä Quick Facts

| Metric | Detail | Impact |
| :--- | :--- | :--- |
| **Top-1 Accuracy Gain** | +5.72% (ResNet-18, 2-bit) | State-of-the-art performance |
| **Data Efficiency** | Effective with as low as 1% data | Reduces storage needs significantly |
| **Time Reduction** | ~61% faster training (20% data) | Enables rapid edge deployment |
| **Key Innovation** | Relative Entropy Coreset + Cascaded Correction | Solves small-scale coreset limitations |
| **Privacy** | On-device training capability | No sensitive data transfer required |

---

## üìã Executive Summary

Quantization-Aware Training (QAT) is vital for deploying efficient deep learning models on edge devices, particularly with aggressive low-bit quantization. However, standard QAT is computationally expensive and typically requires full labeled datasets, creating latency and memory bottlenecks on edge hardware. Additionally, privacy concerns prevent sending sensitive data to the cloud for training. The researchers introduce **QuaRC**, a novel framework designed to optimize QAT for edge devices through intelligent data selection and error correction.

QuaRC operates via a two-phase mechanism: **Relative Entropy Coreset Selection (RES)**, which identifies data subsets exposing quantization discrepancies using KL Divergence, and **Cascaded Layer Correction (CLC)**, which aligns intermediate feature outputs to minimize error accumulation. This approach shifts the focus from generic data diversity to quantization-aware error correction.

The framework demonstrates remarkable efficacy in data-scarce environments. On ImageNet-1K with ResNet-18 (2-bit quantization, 1% data), QuaRC achieved a **5.72% improvement** in Top-1 accuracy. Efficiency benchmarks on CIFAR-100 showed that training with only 20% of the data achieved comparable accuracy to full dataset training while reducing training time by approximately **61%**. This research establishes a viable pathway for privacy-preserving, low-bit quantization on commercial edge applications.

---

## üîë Key Findings

*   **Significant Accuracy Improvement:** Achieved a **5.72%** boost in Top-1 accuracy for ResNet-18 (2-bit quantization) using only 1% of the training data compared to state-of-the-art methods.
*   **Efficacy with Minimal Data:** The framework effectively operates with extremely small data subsets (as low as 1%), overcoming the accuracy degradation usually associated with small-scale coreset limitations.
*   **Error Reduction:** The Cascaded Layer Correction (CLC) mechanism successfully aligns intermediate outputs with full-precision models, significantly reducing error accumulation during training.
*   **Privacy-Preserving Edge Solution:** Enables efficient retraining directly on edge devices, eliminating the need to transfer sensitive data to the cloud.

---

## üß† Methodology

QuaRC employs a two-phase alternating process to optimize Quantization-Aware Training:

1.  **Relative Entropy Coreset Selection (RES):**
    *   Replaces traditional diversity-based criteria.
    *   Utilizes a **Relative Entropy Score** (based on KL Divergence) to select data subsets that specifically expose quantization errors.

2.  **Cascaded Layer Correction (CLC):**
    *   Aligns the intermediate layer outputs of the quantized model with the full-precision model.
    *   Aims to minimize the accumulation of layer-wise errors throughout the network depth.

---

## üèÜ Contributions

*   **Edge-Optimized QAT Framework:** Integrates intelligent coreset selection to drastically reduce computational costs on resource-constrained edge devices.
*   **Advanced Coreset Selection Criterion:** Introduces the use of Relative Entropy to specifically target quantization discrepancies rather than relying on generic data diversity.
*   **Innovative Error Correction Mechanism:** Proposes Cascaded Layer Correction (CLC), a feature-level alignment strategy to correct drift during training.
*   **Benchmark Performance:** Achieved state-of-the-art results for low-bit quantization scenarios under strict data-scarce conditions.

---

## ‚öôÔ∏è Technical Details

**Framework Architecture**
QuaRC alternates between Coreset Selection and Model Training phases. It reduces the complexity of dataset terms to a fraction $S$, enabling efficiency.

**Selection Algorithm**
*   **Relative Entropy Coreset Selection (RES):** Uses KL Divergence to calculate a Relative Entropy Score. This score identifies data batches where the quantized model deviates most significantly from the full-precision model.

**Loss Functions & Optimization**
*   **Cascaded Layer Correction (CLC):** Employs a specific loss function ($L_{CLC}$) to align intermediate feature maps between the quantized teacher and student models.
*   **Total Loss:** Combines standard Knowledge Distillation loss with the CLC loss to ensure both output accuracy and intermediate feature consistency.
*   **Convergence:** Theoretical convergence is proven under the assumption of Lipschitz-smooth loss functions.

---

## üìà Results

**ImageNet-1K Performance (ResNet-18, 2-bit)**
*   **1% Data Usage:** QuaRC achieved a **+5.72%** improvement in Top-1 accuracy over state-of-the-art baselines.

**Ablation Study (MobileNetV2)**
*   The Cascaded Layer Correction (CLC) component alone yielded an **8.41%** performance increase over the ACS baseline.

**Efficiency Benchmarks (CIFAR-100)**
| Configuration | Top-1 Accuracy | Training Time | Time Reduction |
| :--- | :--- | :--- | :--- |
| **QuaRC (1% Data)** | *Reported Efficient* | 9.55 mins | Significant |
| **QuaRC (20% Data)** | **67.18%** | **29.32 mins** | **~61%** |
| **Full Dataset** | 68.72% | 75.22 mins | Baseline |

*Note: At 20% data usage, QuaRC significantly outperformed the ACS baseline (65.03% accuracy) while approaching full-dataset accuracy.*

---

**Quality Score:** 9/10
**References:** 40 Citations