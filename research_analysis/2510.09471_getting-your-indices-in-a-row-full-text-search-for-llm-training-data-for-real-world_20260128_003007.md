---
title: 'Getting Your Indices in a Row: Full-Text Search for LLM Training Data for
  Real World'
arxiv_id: '2510.09471'
source_url: https://arxiv.org/abs/2510.09471
generated_at: '2026-01-28T00:30:07'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Getting Your Indices in a Row: Full-Text Search for LLM Training Data for Real World

*Anastasiia Kucherenko, Alexander Sternfeld, Andrei Kucharavy, Altemir Mari*

***

### ðŸ“Š Quick Facts
| Metric | Value |
| :--- | :--- |
| **Total Corpus** | 15.2 tera-tokens |
| **Indexed Volume** | 8.6 tera-tokens (58% coverage) |
| **Hardware** | Alps supercluster (arm64 architecture) |
| **Core Technology** | Elasticsearch (parallel indices) |
| **Filtering Model** | Llama3-70B-Instruct |
| **Comparison vs Benchmarks** | > RedPajama (1.4T) & Infinigram (4.6T) |

***

## Executive Summary

> This research addresses the critical lack of transparency and auditability in the massive datasets used to train modern Large Language Models (LLMs). As training corpora expand into the tens of tera-tokens, ensuring data quality, adhering to copyright regulations, and detecting unsafe content become computationally daunting challenges.

The study presents a scalable full-text indexing pipeline that bridges enterprise-grade search software with modern, energy-efficient high-performance computing (HPC) architectures. By porting and optimizing **Elasticsearch on the arm64-based Alps supercluster**, the authors successfully indexed **8.6 tera-tokens**â€”covering 58% of the 15.2 tera-token corpus for the Apertus LLM family. This volume substantially exceeds existing benchmarks like RedPajama (1.4 T tokens) and Infinigram (4.6 T tokens).

Technically, the system utilizes distributed indexing via sharding, parallel processing, and Parquet file streaming. Unlike previous solutions offering limited n-gram lookup, this system supports full-text search capabilities including fuzzy search and boolean logic operators. This work fundamentally advances the field by providing a "jailbreak-agnostic" tool for LLM safety, validating the viability of energy-efficient arm64 architectures, and resulting in one of the largest offline, curated open web search engines available today.

***

## Key Findings

*   **Scalability Achievement:** Successfully indexed 8.6 tera-tokens out of 15.2 tera-tokens for the Apertus LLM family, demonstrating feasibility at web scale.
*   **Infrastructure Viability:** Elasticsearch was successfully ported to and operated on energy-efficient arm64 architectures (specifically the Alps supercluster).
*   **New Safety Mechanism:** The resulting full-text index functions as a jailbreak-agnostic tool for detecting critical data within the training corpus.
*   **Dual-Purpose Utility:** The pipeline produced a massive, offline, curated open web search engine as a byproduct of the safety auditing process.

***

## Methodology

The study employed a robust full-text indexing pipeline designed specifically for massive datasets. The implementation relied on **Elasticsearch** configured with parallel indices and deployed on the **arm64 supercluster infrastructure (Alps)**.

*   **Primary Data Source:** 15.2 T token training corpus for the Apertus LLM family.
*   **Processing Approach:** Distributed indexing via sharding and parallel processing.
*   **Data Ingestion:** Utilized Parquet file streaming to handle high-volume throughput efficiently.

***

## Technical Details

The system architecture integrates several advanced hardware and software components to process web-scale data efficiently.

**Infrastructure & Hardware**
*   **Platform:** Alps supercluster (arm64 architecture).
*   **Software:** Elasticsearch optimized for distributed indexing.
*   **Data Handling:** Sharding, parallel processing, and Parquet file streaming.

**Data Pipeline (Apertus)**
*   **Datasets:** FineWeb and StarCoder.
*   **Quality Filtering:** Employed Llama3-70B-Instruct (score >= 2).
*   **Sanitization:** PII anonymization, copyright removal, and multi-language toxicity filtering.

**Indexing Mechanics**
*   **Operation:** `parallel_bulk` operations.
*   **Optimization:** Heavily tuned thread counts and chunk sizes for hardware throughput.
*   **Capabilities:** Supports fuzzy search and logic operators, offering superior flexibility compared to Infinigram.

***

## Results

The implementation achieved a significant breakthrough in scale and functionality:

*   **Volume Indexed:** 8.6 tera-tokens indexed, covering 58% of the total 15.2 tera-token training corpus.
*   **Benchmark Performance:** Exceeded volumes found in RedPajama (1.4 T tokens) and Infinigram (4.6 T tokens).
*   **Data Subsets:** Successfully indexed high-value subsets including FineWeb-Edu, FineWeb-2-HQ, FineWeb-2, and StarCoder.
*   **Query Capability:** Enabled fast, flexible keyword and semantic queries.
*   **Deliverables:** Produced a dual-purpose output consisting of a safety auditing index and a massive offline curated open web search engine.

***

## Key Contributions

*   **Hardware-Software Integration:** Demonstrated that enterprise-grade search software can be ported to modern arm64 supercomputing infrastructure.
*   **Feasibility of Web-Scale Indexing:** Proved that full-text indexing at the scale of modern LLM training datasets is technically feasible.
*   **Advancement in LLM Safety:** Established that full-text indices can ensure LLM safety in a jailbreak-agnostic manner.
*   **Sustainable Computing Practices:** Validated energy-efficient hardware for large-scale data processing to facilitate greener computation.

***

### Document Details
**Quality Score:** 8/10  
**References:** 40 citations