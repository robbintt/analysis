---
title: Towards Undistillable Models by Minimizing Conditional Mutual Information
arxiv_id: '2507.00012'
source_url: https://arxiv.org/abs/2507.00012
generated_at: '2026-02-03T18:32:08'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Towards Undistillable Models by Minimizing Conditional Mutual Information

*Linfeng Ye; Shayan Mohajer Hamidi; En-hui Yang*

---

> ### ðŸ“Š Quick Facts
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 40 Citations |
> | **Primary Focus** | Intellectual Property Protection / Adversarial Machine Learning |
> | **Key Method** | CMI Minimized (CMIM) Training |
> | **Defense Type** | Proactive / Black-box |

---

## Executive Summary

This paper addresses the critical threat Knowledge Distillation (KD) poses to the intellectual property of Deep Neural Networks (DNNs). The authors propose **CMI Minimized (CMIM)**, a proactive defense strategy that renders models "undistillable" without sacrificing performance.

### **Problem**
Current defensive measures against model stealing are inadequate. They often impose a heavy accuracy-security trade-off, degrading the model's utility, or fail against adaptive distillation strategies that bypass obfuscation. There is a need for a defense that ensures extraction attacks offer no advantage while maintaining the teacher model's predictive performance.

### **Innovation**
The authors introduce the **CMIM** method, an algorithm grounded in information theory. It links vulnerability to the **Conditional Mutual Information (CMI)** between inputs and outputs given the true label. By jointly minimizing standard Cross-Entropy (CE) loss and CMI, the algorithm collapses output probability distributions of samples sharing a label into a single, highly concentrated distribution. This is achieved by minimizing the KL divergence between individual predictions ($q_X$) and the class-conditional average distribution. Crucially, CMI minimization is applied across the entire temperature spectrum to neutralize adaptive attacks.

### **Results**
Experimental evaluation confirms that CMIM achieves undistillability without sacrificing accuracy.
*   **CIFAR-10:** CMIM achieved **95.13%** accuracy vs. **94.11%** for standard CE.
*   **CIFAR-100:** CMIM achieved **73.49%** accuracy vs. **70.59%** for standard CE.
*   Knockoff student models distilled from CMIM teachers consistently underperformed compared to students trained independently with Label Smoothing.

### **Impact**
This work advances adversarial machine learning by establishing a quantitative link between cluster concentration (CMI) and distillation vulnerability. The ability to enhance predictive accuracy (by over 1% on CIFAR-10 and nearly 3% on CIFAR-100) while resisting theft sets a new standard for defensive training, shifting the paradigm from reactive obfuscation to intrinsic robustness.

---

## Key Findings

*   **Cluster Concentration:** Undistillable DNNs possess highly concentrated output probability distributions for samples sharing the same label, ideally collapsing into a single distribution per label.
*   **CMIM Efficacy:** The proposed CMIM method effectively renders models undistillable; knockoff students distilled from CMIM models consistently underperform students trained independently.
*   **Accuracy Advantage:** The CMIM training method achieves higher prediction accuracy for the teacher model compared to training with standard cross-entropy (CE) loss alone, avoiding the typical accuracy-security trade-off.

---

## Methodology

The research approach relies on measuring the concentration of output probability clusters using **Conditional Mutual Information (CMI)**.

*   **Proposed Algorithm:** The authors introduce the **CMI Minimized (CMIM)** method, a novel training algorithm.
*   **Joint Optimization:** CMIM jointly minimizes the conventional Cross-Entropy (CE) loss and the CMI values of the model's outputs.
*   **Robustness Mechanism:** To ensure robustness against adaptive attacks, CMI minimization is applied to temperature-scaled clusters across the entire temperature spectrum, rather than a single temperature.

---

## Technical Details

The paper proposes CMIM (CMI Minimized), a training method designed to render Deep Neural Networks (DNNs) undistillable by minimizing the Conditional Mutual Information (CMI) between input data and predicted output given the true label.

**Core Mechanism**
*   **Intra-class Variation Minimization:** The method works by collapsing output probability distributions for samples sharing a label into a single distribution.
*   **Mathematical Formulation:** This involves minimizing the Kullback-Leibler (KL) divergence between individual predictions ($q_X$) and the class-conditional average ($s_Y$).

**Implementation**
*   **Empirical Approximation:** Implementation relies on empirical approximations calculated over mini-batches to determine the class average distribution ($s_y^{emp}$) and empirical CMI.

**Defense Scope**
*   **Black-box Defense:** The method targets black-box defense scenarios.
*   **Resistance:** It ensures robustness against logit-free distillation techniques such as 'power transformation'.
*   **Definition of Distillability:** The paper defines distillability as a scenario where a knockoff student trained via Knowledge Distillation outperforms a student trained independently with Label Smoothing. CMIM ensures this condition is not met.

---

## Contributions

*   **Proactive Defense Strategy:** Introduces a proactive defense strategy for protecting the intellectual property of DNNs by preventing effective knowledge distillation.
*   **Theoretical Link:** Establishes a link between cluster concentration (measured via CMI) and distillation vulnerability, contributing the CMIM algorithm to the field of adversarial machine learning.
*   **Dual-Benefit Solution:** Demonstrates a solution where defensive training improves the model's primary prediction accuracy while simultaneously negating the benefits of model stealing.

---

## Results

The evaluation of the CMIM training method yielded significant outcomes regarding both model accuracy and security:

*   **Teacher Accuracy:** CMIM achieves higher prediction accuracy for the teacher model compared to standard Cross-Entropy (CE) loss, successfully avoiding the typical accuracy-security trade-off.
*   **Student Failure:** Knockoff students distilled from CMIM models consistently underperform students trained independently with Label Smoothing.
*   **Undistillability:** By ensuring that distilled students do not outperform independent students, CMIM fulfills the paper's definition of an undistillable model.
*   **Comparison:** The paper asserts that unlike previous defense methods cited, models trained with CMIM successfully resist distillation attempts.

---

*Paper Analysis Quality Score: 9/10*