# A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies

*Hassan Wasswa; Hussein Abbass; Timothy Lynar*

---

> ### ðŸ’¡ Quick Facts
> * **Quality Score:** 8/10
> * **References:** 40 citations
> * **Total Instances:** >3.2 Million NetFlow instances
> * **Feature Count:** 84 features (extracted via CICFlowMeter)
> * **Best Performer:** Post-Training Quantization (PTQ)
> * **Top Speedup:** ~6x (PTQ)
> * **Top Compression:** Up to 25x (QAT)

---

## Executive Summary

The rapid expansion of the Internet of Things (IoT) has created a vast attack surface for botnets, necessitating robust, real-time detection mechanisms. However, standard deep learning models are typically computationally intensive and memory-heavy, rendering them unsuitable for resource-constrained edge devices. This paper addresses the critical challenge of transforming high-accuracy, cloud-based deep learning intrusion detection systems into lightweight, efficient models capable of operating directly on limited IoT hardware without compromising security efficacy.

The researchers propose a hybrid architecture combining a Variational Autoencoder (VAE) with a Multi-Layer Perceptron (MLP) to optimize feature processing. The VAE acts as a dimensionality reducer, compressing high-dimensional network traffic dataâ€”84 features extracted via CICFlowMeterâ€”into compact 8-dimensional latent vectors, which are then used to train the MLP classifier. A key technical contribution is the systematic evaluation of two specific optimization strategies applied to this framework: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), allowing for a granular analysis of how different quantization methods affect model performance and efficiency.

Evaluated against the N-BaIoT and CICIoT2022 datasets comprising over 3.2 million instances, PTQ proved to be the superior optimization strategy. It maintained detection performance comparable to the unquantized baseline (e.g., accuracy of 0.9971 vs. 0.9980 on N-BaIoT) while achieving an approximate 6x inference speedup and a 21x reduction in model size. Conversely, QAT achieved a slightly higher compression rate (up to 25x) but resulted in a noticeable decline in accuracy (dropping to 0.9683 on N-BaIoT) and lower inference speedup (3x). Consequently, PTQ demonstrated the best balance of accuracy preservation and computational efficiency.

This study provides empirical evidence that sophisticated botnet detection is viable at the network edge, challenging the necessity of cloud dependency for IoT security analysis. By validating that Post-Training Quantization offers a highly favorable trade-off between model compression and detection accuracy, the authors offer a practical pathway for deploying effective security solutions on constrained devices. These findings advance the field of edge AI by enabling security protocols that are both high-performance and economically scalable across heterogeneous IoT networks.

---

## Key Findings

*   **PTQ Superiority:** Post-Training Quantization (PTQ) incurred only a marginal reduction in detection accuracy compared to the original unquantized model, while Quantization-Aware Training (QAT) experienced a more noticeable decline.
*   **Performance & Efficiency:**
    *   **PTQ:** Achieved a **6x** inference speedup and a **21x** reduction in model size.
    *   **QAT:** Achieved a higher compression rate of **24x** but suffered from a lower inference speedup of **3x**.
*   **Practical Viability:** The study demonstrates that quantization is a practical technique for enabling effective deep learning-based botnet detection on resource-constrained IoT devices.

---

## Methodology

The researchers employed a systematic approach to evaluate model compression and detection performance:

*   **Framework:** Utilized a VAE-MLP framework.
    *   A pretrained **Variational Autoencoder (VAE)** was employed to compress high-dimensional training data into **8-dimensional latent vectors**.
    *   These vectors were used to train a **Multi-Layer Perceptron (MLP)** classifier.
*   **Strategies Evaluated:** The study compared two main quantization strategies against an unquantized baseline:
    1.  **Quantization-Aware Training (QAT)**
    2.  **Post-Training Quantization (PTQ)**
*   **Metrics:** Assessment focused on detection performance (Accuracy, F1), storage efficiency, and inference latency.
*   **Datasets:** Validation was performed using standard IoT botnet benchmark datasets:
    *   **N-BaIoT**
    *   **CICIoT2022**

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Architecture** | Hybrid Variational Autoencoder (VAE) + Multi-Layer Perceptron (MLP) |
| **Feature Extraction** | CICFlowMeter (extracting 84 distinct features) |
| **Latent Dimensionality** | 8-dim vectors |
| **Data Volume** | >3.2 million NetFlow instances |
| **Baseline Comparison** | Unquantized model vs. PTQ vs. QAT |

---

## Results

The performance was rigorously tested across two major datasets.

### N-BaIoT Dataset Results
*   **PTQ Performance:** Maintained performance comparable to the unquantized model.
    *   **Accuracy:** 0.9971 (vs 0.9980 baseline)
    *   **F1-Score:** 0.9948 (vs 0.9941 baseline)
    *   **Efficiency:** ~20.5x model size reduction; ~5.89x inference speedup.
*   **QAT Performance:** Resulted in a noticeable accuracy drop.
    *   **Accuracy:** 0.9683
    *   **Efficiency:** Slightly higher compression (~23.9x) but lower speedup (~2.36x).

### CICIoT2022 Dataset Results
*   **PTQ Performance:** Showed negligible impact on metrics.
    *   **Accuracy:** 0.9853
    *   **F1-Score:** 0.9297
    *   **Efficiency:** ~20.5x compression; ~5.92x speedup.
*   **QAT Performance:** Showed a slight decline in performance.
    *   **Accuracy:** 0.9847
    *   **F1-Score:** 0.9233
    *   **Efficiency:** ~25.3x compression; ~2.34x speedup.

**Conclusion:** Overall, PTQ provided the best balance of speed, compression, and accuracy.

---

## Contributions

*   **Lightweight Framework:** Proposed a VAE-MLP framework specifically designed to reduce dimensionality to 8-latent vectors, addressing the computational intensity of standard deep learning methods for IoT.
*   **Systematic Comparison:** Provided a clear comparison of QAT and PTQ strategies, clarifying the trade-offs between accuracy preservation and compression efficiency in botnet detection contexts.
*   **Empirical Evidence:** Delivered benchmarking evidence proving that significant model compression (>20x) and speedup (>5x) are achievable, supporting the viability of edge-based security solutions.