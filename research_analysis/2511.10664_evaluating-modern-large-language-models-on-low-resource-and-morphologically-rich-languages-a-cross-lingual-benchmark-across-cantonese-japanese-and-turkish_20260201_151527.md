# Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages: A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish

*Chengxuan Xia; Qianye Wu; Hongbin Guan; Sixuan Tian; Yilun Hao; Xiaoyu Wu*

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **Languages Covered:** Cantonese, Japanese, Turkish
> *   **Models Evaluated:** 7 (GPT-4o, GPT-4, Claude 3.5 Sonnet, LLaMA 3.1, Mistral Large 2, LLaMA-2 Chat, Mistral 7B)
> *   **Task Categories:** 4 (Open-domain QA, Summarization, Translation, Dialogue)
> *   **Citations:** 40 References

---

## Executive Summary

Current Large Language Model (LLM) evaluation is heavily skewed toward English and high-resource languages, leaving a critical gap in understanding how state-of-the-art models perform on low-resource and morphologically rich languages. This deficiency limits the global deployment of AI systems, as models often struggle with the linguistic complexities of agglutinative languages (like Turkish), the nuanced colloquialisms of dialects (like Cantonese), or the intricate honorifics of high-context languages (like Japanese).

This paper addresses this issue by rigorously assessing whether modern proprietary and open-source LLMs can maintain high performance in fluency, factual accuracy, and cultural grounding when operating outside the English-centric paradigm. The authors introduce a specialized cross-lingual benchmark featuring three linguistically diverse languages—Cantonese, Japanese, and Turkish—evaluated across four high-level tasks: open-domain question answering, document summarization, English-to-target translation, and culturally grounded dialogue.

Technically, the study employs a hybrid evaluation methodology that augments standard automated metrics (BLEU, ROUGE, BERTScore) with human assessments from native speakers. The human evaluation focuses on three distinct dimensions—Fluency, Accuracy, and Cultural Appropriateness—using a 5-point Likert scale. The benchmark includes 100 domain-specific questions and 50 documents adapted by native speakers, with models configured using greedy decoding and explicit system prompts to ensure consistency.

The evaluation reveals a significant performance hierarchy: proprietary models (GPT-4o, GPT-4, Claude 3.5 Sonnet) substantially outperformed open-source counterparts (LLaMA 3.1, Mistral Large 2, LLaMA-2 Chat, Mistral 7B) across all languages and metrics. Specifically, GPT-4o demonstrated the most robust multilingual capabilities, while Claude 3.5 Sonnet achieved competitive accuracy in knowledge and reasoning tasks. In contrast, smaller open-source models exhibited notable lags in fluency and factual accuracy. Qualitative analysis identified specific failure modes across the board: models struggled significantly with Turkish agglutinative morphology (morpheme combination errors) and failed to capture the cultural nuance of Cantonese colloquialisms. This study provides a critical baseline for the NLP community, quantifying the "performance gap" between proprietary and open-source models in multilingual contexts. By releasing the benchmark dataset and detailed evaluation results as open-source resources, the authors facilitate reproducibility and enable future research to target specific weaknesses in morphological generalization and cultural grounding. The findings highlight that scaling model size alone does not resolve deep linguistic challenges, suggesting that future training paradigms must prioritize diverse, culturally relevant data to achieve truly global language understanding.

---

## Methodology

The researchers conducted a comprehensive evaluation designed to test the limits of multilingual capabilities in low-resource settings.

*   **Models Evaluated:** Seven state-of-the-art LLMs were tested:
    *   *Proprietary:* GPT-4o, GPT-4, Claude 3.5 Sonnet.
    *   *Open-Source:* LLaMA 3.1 (70B), Mistral Large 2, LLaMA-2 Chat (13B), Mistral 7B Instruct.
*   **Benchmark Design:** A newly created cross-lingual benchmark covering **Cantonese**, **Japanese**, and **Turkish**.
*   **Task Categories:**
    1.  Open-domain question answering.
    2.  Document summarization.
    3.  English-to-X translation.
    4.  Culturally grounded dialogue.
*   **Evaluation Strategy:** A hybrid approach combining:
    *   **Human Evaluation:** Native speakers rated fluency, factual accuracy, and cultural appropriateness.
    *   **Automated Metrics:** Standard NLP metrics including BLEU and ROUGE.
    *   **Qualitative Analysis:** Deep-dive error analysis to identify specific failure modes.

---

## Technical Details

*   **Benchmark Composition:**
    *   **Open-Domain QA:** 100 questions.
    *   **Summarization:** 50 documents.
    *   **Data Adaptation:** All data adapted by native speakers to ensure linguistic and cultural validity.
*   **Model Configuration:**
    *   **Decoding Strategy:** Greedy decoding.
    *   **Temperature:** 0.
    *   **Max Tokens:** 1024.
    *   **Prompts:** Explicit system prompts were used for consistency.
*   **Human Protocol:**
    *   Annotated by 3 native speakers per language.
    *   **Scoring Dimensions:** Fluency, Accuracy, Cultural Appropriateness.
    *   **Scale:** 5-point Likert scale.
*   **Automated Metrics:** QA Accuracy, ROUGE-1/2/L, BERTScore.

---

## Key Findings

The analysis highlighted significant disparities between model types and persistent challenges in specific linguistic domains:

*   **Proprietary Dominance:** The largest proprietary models (**GPT-4o**, **GPT-4**, **Claude 3.5 Sonnet**) generally outperform open-source models across all evaluated languages and tasks.
*   **Specific Model Strengths:** **GPT-4o** demonstrates robust multilingual capabilities in cross-lingual tasks, while **Claude 3.5 Sonnet** achieves competitive accuracy specifically in knowledge and reasoning benchmarks.
*   **Performance Gaps in Open Source:** Smaller open-source models (**LLaMA-2 Chat 13B**, **Mistral 7B Instruct**) lag substantially behind proprietary large models in fluency and factual accuracy.
*   **Linguistic Limitations:** All models exhibit significant gaps in culturally nuanced understanding and morphological generalization, struggling specifically with:
    *   **Turkish:** Agglutinative morphology issues.
    *   **Cantonese:** Difficulties with colloquialisms.

---

## Results

The evaluation utilized QA Accuracy, Summarization scores (ROUGE and BERTScore), and Human Evaluation scores. The data confirmed a distinct performance hierarchy:

1.  **Top Performers:** Proprietary models (GPT-4o, GPT-4, Claude 3.5 Sonnet) consistently ranked highest.
2.  **Open-Source Lag:** Open-source models showed significant deficits in fluency and accuracy compared to their proprietary counterparts.
3.  **Error Analysis:**
    *   **Morphological Issues:** Pronounced deficiencies in morphological generalization, particularly with the agglutinative structure of Turkish.
    *   **Cultural Nuance:** A failure to capture the cultural context and nuance required for Cantonese colloquialisms.

---

## Contributions

This research makes three primary contributions to the field of Natural Language Processing:

*   **New Benchmark:** Introduction of a rigorous new cross-lingual benchmark designed to test LLM proficiency in low-resource and morphologically rich languages across diverse, high-level tasks.
*   **Comparative Analysis:** A detailed comparative study revealing the specific limitations of current SOTA models regarding morphological generalization and cultural grounding.
*   **Open Resources:** Release of open-source resources including the benchmark dataset, detailed evaluation results, and analysis to foster reproducibility and drive future research in linguistically diverse and culturally aware AI.