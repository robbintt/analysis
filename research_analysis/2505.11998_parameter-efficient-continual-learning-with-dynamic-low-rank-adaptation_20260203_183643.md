---
title: Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation
arxiv_id: '2505.11998'
source_url: https://arxiv.org/abs/2505.11998
generated_at: '2026-02-03T18:36:43'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation

*Prashant Shivaram Bhat; Shakib Yazdani; Elahe Arani; Bahram Zonooz*

---

> ### ðŸ“Š Quick Facts
>
> *   **Framework Name:** PEARL
> *   **Core Technique:** Dynamic Low-Rank Adaptation (LoRA)
> *   **Parameter Efficiency:** Trains only **0.5% â€“ 1%** of total model parameters
> *   **Top Accuracy (CIFAR-100):** **55.23%** (vs. 41.55% static LoRA baseline)
> *   **Architecture Support:** ResNet, ConvNets, Vision Transformers (ViT)
> *   **Quality Score:** 7/10
> *   **Citations:** 40

---

## Executive Summary

Continual Learning (CL) faces the critical challenge of **catastrophic forgetting**, where learning new tasks degrades performance on previous ones. While rehearsal buffers effectively mitigate this, they introduce substantial memory overhead and potential data privacy risks. Parameter-efficient methods like Low-Rank Adaptation (LoRA) offer a resource-efficient alternative but suffer from **rank sensitivity**; selecting a fixed rank is often suboptimal for diverse task sequences, leading to inefficient resource utilization and instability.

The authors introduce **PEARL**, a rehearsal-free CL framework that optimizes LoRA through a novel **Dynamic Rank Allocation** mechanism. Unlike standard LoRA, which utilizes a static rank, PEARL employs **Proximity-Based Adaptation** to automatically determine the optimal rank for each task based on geometric relationships in the parameter space. Technically, the method computes a task vector as the difference between fine-tuned weights and reference weights, decomposes this vector using Singular Value Decomposition (SVD), and applies a spectral threshold derived from task proximity. This allows the framework to dynamically allocate higher ranks to dissimilar tasks and lower ranks to similar ones.

**PEARL demonstrates superior empirical performance** across standard benchmarks:
*   **Split CIFAR-100:** Achieved **55.23%** accuracy vs. Static LoRA (Rank 8) at **41.55%**.
*   **TinyImageNet:** Achieved **46.83%** accuracy vs. Static LoRA at **34.33%**.

The significance of PEARL lies in resolving the tension between high performance and resource efficiency. By eliminating rehearsal buffers, it offers a practical solution to memory constraints and data privacy issues, making lifelong learning feasible on edge devices without the computational costs of heavy rehearsal.

---

## Key Findings

*   **Superior Performance:** PEARL consistently outperforms baseline methods across various Continual Learning scenarios.
*   **Architecture Versatility:** Demonstrates robust performance on diverse architectures, including ResNet, Separable Convolutional Networks, and Vision Transformers (ViT).
*   **Resolution of Rank Sensitivity:** Successfully addresses the limitations of LoRA rank selection by optimizing resource allocation dynamically.
*   **Rehearsal-Free Efficiency:** Effectively mitigates catastrophic forgetting without the need for stored data (exemplars) or heavy computational schedules.

---

## Methodology

PEARL is a rehearsal-free Continual Learning framework based on parameter-efficient fine-tuning. Its core innovation lies in its ability to adapt to new tasks while preserving pre-trained knowledge without storing previous data.

*   **Dynamic Rank Allocation:** Assigns rank to LoRA components dynamically during the training process rather than relying on a fixed hyperparameter.
*   **Proximity-Based Adaptation:** Utilizes reference task weights to determine rank based on weight proximity in the parameter space.
*   **Resource Optimization:** The approach optimizes resource allocation based on task similarity; higher ranks are allocated for dissimilar tasks to prevent knowledge degradation.

---

## Technical Details

### Core Architecture
PEARL utilizes a structure consisting of disjoint task-specific parameters ($\theta_t$) and reference task parameters ($\theta_r$).

*   **Adapter Placement:** Adapters are applied to:
    *   Convolutional layers
    *   Linear projections
    *   Self-attention mechanisms

### Dynamic Rank Selection Mechanism
The system determines the optimal rank through a geometric analysis of the parameter space:
1.  **Task Vector Calculation:** Computes the difference between fine-tuned weights and reference weights.
2.  **Decomposition:** Decomposes the task vector via Singular Value Decomposition (SVD).
3.  **Thresholding:** Determines the optimal rank based on a spectral threshold that approximates task proximity.

### Training & Inference
*   **Training:** Involves a two-stage fine-tuning process where LoRA matrices are initialized from the decomposed SVD components.
*   **Inference Strategies:**
    *   **Task-IL:** Selects sub-networks explicitly by Task ID.
    *   **Class-IL:** Passes inputs through all sub-networks and selects the output with the maximum activation.

---

## Contributions

*   **Novel Framework (PEARL):** Introduces a parameter-efficient CL framework that eliminates the need for rehearsal buffers.
*   **Adaptive Rank Selection Mechanism:** Automates the rank assignment process using geometric relationships in parameter space, removing manual hyperparameter tuning.
*   **Mitigation of Catastrophic Forgetting:** Addresses the issue of forgetting without compromising the integrity of pre-trained knowledge.
*   **Comprehensive Benchmarking:** Provides extensive evaluation across diverse vision architectures, demonstrating the method's generalizability.

---

## Results & Performance

PEARL rivals rehearsal-based methods like Experience Replay (ER) in accuracy while completely eliminating memory dependency.

| Benchmark | PEARL Accuracy | Static LoRA (Rank 8) Accuracy |
| :--- | :--- | :--- |
| **Split CIFAR-100** | **55.23%** | 41.55% |
| **TinyImageNet** | **46.83%** | 34.33% |

**Additional Outcomes:**
*   Successfully aligned with the **NECIL benchmark** for non-exemplar Class-Incremental Learning.
*   Validated robustness on **ResNet-18** and **Vision Transformer (ViT)** architectures.

---
*Analysis based on 40 citations. Quality Score: 7/10*