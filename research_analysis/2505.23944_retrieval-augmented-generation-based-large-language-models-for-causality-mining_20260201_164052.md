# Retrieval Augmented Generation based Large Language Models for Causality Mining

*Thushara Manjari Naduvilakandy; Hyeju Jang; Mohammad Al Hasan*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 25 Citations
> *   **Core Methodology:** Retrieval-Augmented Generation (RAG) with Dynamic Prompting
> *   **Top Performing Model:** GPT-4o
> *   **Key Datasets:** SemEval, ADE, Li et al. (4,082 sentences total)
> *   **Peak Performance:** F1 Score of 0.93 (Li et al. Dataset - Causality Detection)
> *   **Improvement Range:** 6% to 30% relative F1 score improvement over baselines

---

## Executive Summary

Causality mining is essential for understanding semantic relationships within text, but current methodologies face significant trade-offs between efficiency and accuracy. Unsupervised methods suffer from poor generalization and frequently necessitate human intervention to correct errors. Conversely, supervised approaches are constrained by the scarcity of large, annotated training datasets available for causality-specific tasks. This paper addresses the critical lack of training data that limits the performance of traditional supervised models like DEPBERT, highlighting the need for a method that can leverage the power of Large Language Models (LLMs) without requiring extensive fine-tuning or labeled resources.

The authors introduce a **Retrieval-Augmented Generation (RAG) framework** designed to enhance LLM performance through dynamic prompting rather than static prompt engineering. Technically, the system constructs an external "Fewshot Example DB" comprising 4,082 sentences from SemEval, ADE, and Li et al. datasets, indexed by causal connectives extracted using GPT-3.5-turbo. The framework implements two dynamic retrieval schemes: **Pattern RAG**, which retrieves examples with high lexical connective similarity (>90%), and **kNN+Pattern RAG**, a hybrid approach that combines lexical matching with sentence embedding similarity. This mechanism allows the model to dynamically retrieve and inject the most relevant context-specific examples into the prompt, compensating for the lack of fine-tuning data.

Empirical testing across three datasets using five different LLMs demonstrates that the proposed dynamic RAG methods outperform static, zero-shot, and supervised baselines. In causality detection, GPT-4o utilizing the kNN+Pattern scheme achieved superior results, with F1 scores of 0.90 on SemEval and ADE, and 0.93 on the Li et al. dataset; notably, GPT-3.5 improved its SemEval accuracy from 0.68 (zero-shot) to 0.86. In causality extraction, LLMs dominated the supervised DEPBERT baseline (F1 0.12), with GPT-4o achieving an F1 score of 0.80 on SemEval and 0.84 on ADE using the dynamic schemes. Overall, the approach yielded relative F1 score improvements ranging from 6% to 30%.

This study establishes that dynamic, retrieval-augmented prompting is a fundamentally superior strategy for applying LLMs to complex information extraction tasks compared to traditional static prompting. By successfully augmenting generative models with external knowledge, the paper provides a viable solution to the data scarcity bottleneck that plagues supervised causality mining. As the first comprehensive work to apply RAG-based dynamic prompting specifically to this domain, it offers a validated, scalable framework that sets a new standard for future research in automated semantic extraction and relationship mining.

---

## Key Findings

*   **Methodological Limitations:** Unsupervised causality mining methods suffer from poor generalization and require human intervention, while supervised methods are constrained by the scarcity of large training datasets.
*   **RAG Efficacy:** The integration of Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) performance specifically for causality detection and extraction tasks.
*   **Dynamic vs. Static:** Proposed RAG-based dynamic prompting schemes significantly outperform traditional static prompting schemes in causality mining tasks.
*   **Empirical Validation:** The superiority of the dynamic prompting approach was empirically validated through extensive experiments testing five different LLMs across three separate datasets.

---

## Methodology

The researchers utilized a **Retrieval-Augmented Generation (RAG) framework** to implement dynamic prompting schemes. Unlike static prompting, this approach dynamically retrieves relevant information to augment the input provided to the LLMs. This methodology aims to compensate for the lack of large-scale training data by leveraging the generative capabilities of LLMs guided by context-specific, retrieved knowledge.

The performance of these RAG-based prompts was benchmarked against static prompting baselines across various models and datasets.

---

## Technical Details

The paper proposes a comprehensive RAG framework to enhance LLM performance in causality mining without fine-tuning.

### System Architecture
*   **External Database (Fewshot Example DB):**
    *   **Source Data:** Constructed from SemEval, ADE, and Li et al. datasets.
    *   **Volume:** 4,082 sentences.
    *   **Indexing:** GPT-3.5-turbo extracts causal connectives for indexing.
    *   **Storage:** Stores up to 10 random examples per unique connective (2,365 total instances).
*   **Dynamic Prompting Schemes:**
    1.  **Pattern RAG:** Matches connectives with >90% similarity.
    2.  **kNN+Pattern RAG:** A hybrid approach combining connective matching and sentence embedding similarity.
*   **Task Formulation:**
    *   **Causality Detection:** Formulated as binary classification.
    *   **Causality Extraction:** Formulated as sequence labeling.
    *   **Constraints:** Mechanisms to prevent overlap between cause and effect phrases.

---

## Results

Experiments across SemEval, ADE, and Li et al. datasets demonstrated that dynamic RAG methods (Pattern and kNN+Pattern) outperformed Zero-shot, Random few-shot, and supervised (DEPBERT) baselines on models like GPT-3.5, GPT-4o, Llama3-8b, and Gemma2-9b-it.

### Causality Detection Performance
| Model / Method | Dataset | Accuracy | F1 Score |
| :--- | :--- | :---: | :---: |
| **GPT-4o (kNN+Pattern RAG)** | **SemEval** | **0.91** | **0.90** |
| **GPT-4o (kNN+Pattern RAG)** | **ADE** | **0.90** | **0.90** |
| **GPT-4o (kNN+Pattern RAG)** | **Li et al.** | **0.96** | **0.93** |
| GPT-3.5 (Pattern RAG) | SemEval | 0.86 | (Significant rise from 0.68 Acc in Zero-shot) | - |

### Causality Extraction Performance
*   **Baseline Comparison:** LLMs dominated the supervised DEPBERT baseline (SemEval F1 0.12).
*   **GPT-4o Results:**
    *   **SemEval (Pattern RAG):** Acc 0.89 / F1 0.80
    *   **ADE (kNN+Pattern RAG):** Acc 0.89 / F1 0.84
*   **Overall Improvement:** Dynamic prompting yielded relative F1 score improvements of **6% to 30%**.

---

## Contributions

*   **Data Scarcity Solution:** The paper provides a solution to the lack of large training datasets in supervised causality mining by leveraging the few-shot/zero-shot capabilities of LLMs via prompt engineering.
*   **Novel Application:** The authors introduce the first comprehensive work applying RAG-based dynamic prompting specifically to the domain of causality detection and mining.
*   **Strategic Validation:** The study establishes that dynamic, retrieval-augmented prompting is a superior strategy compared to static prompting, providing a validated new direction for applying LLMs to complex information extraction tasks.

---

**Document Details:**
*   **Quality Score:** 8/10
*   **Citations:** 25 references included in analysis.