# Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition

*Katharina Hoedt; Arthur Flexer; Gerhard Widmer*

***

### üìä Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Domain** | Music Emotion Recognition (MER) |
| **Key Datasets** | Soundtracks (360 excerpts), Mid-Level Features (5000 snippets) |
| **Architecture** | VGG-style CNN (A2E, A2M2E, A2B2E) |
| **Attack Method** | Basic Iterative Method (BIM) |
| **Optimization** | Adam (80/10/10 split) |
| **Key Innovation** | 7-neuron bottleneck joint optimization (A2M2E) |

***

### üìù Executive Summary

Deep learning models, particularly in complex domains like Music Emotion Recognition (MER), frequently achieve high accuracy by relying on spurious correlations within training data rather than learning robust, semantic features. This dependency renders standard black-box architectures highly vulnerable to adversarial perturbations‚Äîminor, often imperceptible input modifications that cause significant performance degradation.

This paper addresses the critical trade-off between accuracy and stability, investigating whether the inherent transparency of a model‚Äôs architecture correlates with its resilience against such attacks and whether interpretability can serve as a built-in defense mechanism without the heavy computational costs of traditional adversarial training.

The authors introduce a comparative framework evaluating standard black-box deep models against inherently interpretable architectures. The core technical innovation lies in the **A2M2E model**, a VGG-style CNN designed with a constrained 7-neuron bottleneck layer. Unlike the standard A2E black-box model, which maps audio features directly to emotion labels, A2M2E utilizes a **joint optimization process**. This process minimizes error on the primary emotion regression task while simultaneously minimizing Mean Squared Error (MSE) on mid-level audio features using a separate bottleneck dataset. This constraint forces the interpretable model to learn salient, generalizable representations rather than exploiting non-robust statistical shortcuts.

The experiments highlight a distinct divergence between clean-data performance and adversarial robustness:
*   **Unperturbed Data:** The standard black-box model (A2E) achieved the highest correlation (**Corr: 0.73 ¬± 0.02**), outperforming the interpretable A2M2E (**Corr: 0.67 ¬± 0.02**).
*   **Under Attack (BIM, Œµ=0.001):** A2M2E demonstrated significantly greater stability, with a lower delta in MAE compared to standard models.
*   **Efficiency:** Crucially, the robustness of the interpretable A2M2E was comparable to adversarially trained models, yet achieved without substantial computational overhead.

This research provides significant empirical evidence linking model interpretability to adversarial robustness, challenging the notion that complex black-box models are superior despite their opacity. By demonstrating that forcing a model to learn interpretable mid-level features naturally reduces susceptibility to spurious correlations, the paper identifies interpretable architectures as a computationally efficient alternative to costly adversarial training defenses.

***

## üîë Key Findings

*   **Superior Robustness:** Inherently interpretable deep models demonstrate greater robustness against minor irrelevant perturbations compared to standard black-box models.
*   **Cost Efficiency:** These interpretable models achieve robustness levels similar to adversarially trained models but with significantly lower computational overhead.
*   **Vulnerability of Black-Boxes:** Black-box models are highly vulnerable to adversarial perturbations due to their reliance on spurious correlations rather than semantic features.
*   **Architecture Matters:** Model architecture choices directly affect susceptibility to attacks; constrained bottlenecks encourage the learning of robust features.

## üß™ Methodology

The researchers conducted a comparative analysis within the domain of **Music Emotion Recognition (MER)**. The study framework involved:

1.  **Model Selection:** Three distinct architectures were evaluated:
    *   **Inherently Interpretable Model:** A deep learning model designed with transparency constraints.
    *   **Standard Black-Box Model:** A traditional deep neural network without interpretability constraints.
    *   **Adversarially Trained Model:** A black-box model trained specifically to resist attacks (serving as a robustness baseline).
2.  **Testing Protocol:** All models were subjected to **adversarial examples**‚Äîinputs modified with minor, often imperceptible perturbations.
3.  **Evaluation Metrics:** Performance was measured based on generalization capabilities and stability (robustness) against these perturbations.

## ‚öôÔ∏è Technical Details

### Data & Processing
*   **Datasets:**
    *   *Training:* Soundtracks (360 excerpts, ~17s duration).
    *   *Bottleneck Training:* Mid-Level Features Dataset (5000 snippets).
*   **Targets:** 8 regression tasks.
*   **Audio Processing:**
    *   Resampling: 22.05 kHz
    *   Segment Length: 10 seconds
    *   Representation: Log-scaled spectrograms (Frame size 2048, Hop 705).

### Model Architectures (VGG-style CNN)
| Acronym | Type | Description |
| :--- | :--- | :--- |
| **A2E** | Black-box | Maps input directly to output (Audio to Emotion). |
| **A2M2E** | Interpretable | Uses a **7-neuron bottleneck** with joint optimization on mid-level feature MSE and emotion MSE. |
| **A2B2E** | Black-box Control | Uses the same bottleneck as A2M2E but trains **only** on emotion MSE. |

### Training & Attack Configuration
*   **Adversarial Attack:** Basic Iterative Method (BIM) for untargeted regression.
    *   Epsilon (Œµ): 0.001
    *   Step size: 0.002
*   **Defense:** Adversarial training conducted every 5th epoch.
*   **Training Setup:** 80/10/10 (train/validation/test split) using the Adam optimizer.

## üìà Results

### Standard Performance (Clean Data)
While the black-box model slightly outperformed the interpretable model on clean data, the difference was marginal.
*   **A2E (Black-box):** Corr `0.73 ¬± 0.02` | MAE `0.10 ¬± 0.01`
*   **A2M2E (Interpretable):** Corr `0.67 ¬± 0.02` | MAE `0.11 ¬± 0.01`
*   **aA2E (Adv. Trained):** Corr `0.67 ¬± 0.03` | MAE `0.10 ¬± 0.00`

### Robustness Analysis (Under Attack)
*   **A2M2E vs. Black-Box:** A2M2E demonstrated significantly greater robustness against minor perturbations than standard black-box models (A2E, A2B2E).
*   **Efficiency Check:** A2M2E achieved robustness comparable to adversarially trained models (aA2E, aA2B2E) but **without** the associated computational overhead of adversarial training.

## üìù Contributions

*   **Empirical Evidence:** The paper provides strong empirical evidence linking model interpretability directly to robustness against adversarial attacks.
*   **Efficient Alternative:** It identifies interpretable architectures as a viable, computationally efficient alternative to resource-intensive adversarial training.
*   **Domain Advancement:** Advances the understanding of robustness in Music Emotion Recognition (MER) by highlighting how architecture choices affect a model's susceptibility to spurious correlations.

***

**Quality Score:** 9/10