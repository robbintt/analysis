---
title: 'Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via
  Expert-Choice Routing'
arxiv_id: '2505.00315'
source_url: https://arxiv.org/abs/2505.00315
generated_at: '2026-02-03T20:18:09'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing

*Piotr Piƒôkos; R√≥bert Csord√°s; J√ºrgen Schmidhuber*

***

### ‚ö° Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 40 |
| **Complexity** | Reduced from $O(T^2)$ to $O(k^2 + T)$ |
| **Best Performance Gain** | 27% better perplexity (Tiny Model) |
| **Key Innovation** | Expert-Choice Routing for Attention |

***

## üìù Executive Summary

Standard Transformer architectures rely on dense self-attention mechanisms that incur quadratic computational complexity $O(T^2)$ relative to sequence length. This bottleneck restricts the efficient processing of long sequences and results in significant memory and compute demands, particularly during inference due to the growing Key-Value (KV) cache. While various sparse attention mechanisms have been proposed to mitigate these costs by reducing the number of attended tokens, they have historically suffered from performance degradation, failing to match the accuracy of dense baselines. Consequently, the field lacks a solution that successfully reduces computational overhead without sacrificing the model's predictive capability.

The authors introduce **Mixture of Sparse Attention (MoSA)**, a novel architecture that applies Mixture of Experts (MoE) principles to attention mechanisms via Expert-Choice Routing. Unlike static sparse patterns or fixed routing methods, MoSA utilizes a learned, content-based router to dynamically select a subset of $k$ tokens from a sequence of length $T$ for each attention head. This selection occurs prior to the projection of Queries, Keys, and Values, ensuring that computations are performed only on the most relevant tokens. By tailoring the attention pattern to the specific input data, MoSA reduces the complexity per head to $O(k^2 + T)$. This efficiency allows the architecture to increase the total number of attention heads within the same computational budget, fostering greater specialization among heads.

Empirical evaluations on the C4 dataset demonstrate that MoSA is the only sparse attention variant tested that consistently outperforms the dense Transformer baseline under an identical fixed FLOP budget. The improvements in perplexity are substantial across model sizes: the Tiny model (28M) achieved 16.39 versus the dense baseline‚Äôs 22.46 (a 27% improvement); the Small model (113M) scored 12.85 versus 16.01 (19.7% better); and the Large model (516M) reached 10.58 versus 12.20 (13.3% better). Furthermore, MoSA realized significant resource efficiencies, reducing wall-clock training time, memory usage, and KV-cache size compared to dense models, while other sparse baselines like Fixed Sparse and Routing Transformer failed to match dense performance levels.

This research challenges the prevailing assumption that sparse attention must come at the cost of model accuracy. By successfully integrating expert-choice routing with content-based sparsity, MoSA establishes a new paradigm where subquadratic complexity can be achieved while simultaneously improving perplexity. The ability to drastically reduce KV-cache sizes and training memory requirements has profound implications for deploying large language models on resource-constrained hardware and processing extended contexts. Ultimately, MoSA validates dynamic, learnable sparsity as a superior alternative to dense attention, paving the way for more efficient and scalable Transformer architectures.

***

## üîë Key Findings

*   **Superior Performance over Dense Baselines:** MoSA is the only sparse attention variant tested that outperforms the dense transformer baseline, achieving up to **27% better perplexity** for an identical compute budget.
*   **Reduced Computational Complexity:** By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$.
*   **Significant Resource Savings:** MoSA models are faster in wall-clock time, require less training memory, and drastically reduce the size of the KV-cache compared to dense baselines.
*   **Increased Specialization:** The efficiency gains allow for the deployment of more attention heads within the same computational budget, enabling higher specialization among heads.

***

## üß™ Methodology

The authors propose **Mixture of Sparse Attention (MoSA)**, a mechanism inspired by Mixture of Experts (MoE) that utilizes expert-choice routing. Unlike static sparse patterns, MoSA employs dynamic, learned, content-based sparsity to dynamically select tokens for each attention head, allowing for arbitrary sparse attention patterns.

The method operates by selecting a subset of **k tokens** from a sequence of length **T** for each head. This selection allows the architecture to increase the total number of heads without exceeding the computational budget of a standard dense model.

***

## üöÄ Contributions

*   **Novel Attention Architecture:** Introduction of MoSA, a new attention mechanism that successfully applies expert-choice routing to the domain of sparse attention, enabling learnable, content-based sparsity.
*   **Validation of Dynamic Sparsity:** Empirical evidence supporting the hypothesis that dynamic, content-based sparsity can lead to more efficient attention mechanisms that do not suffer from the performance degradation typical of previous subquadratic methods.
*   **Efficiency Benchmarking:** Demonstration that sparse attention can simultaneously improve perplexity and reduce resource usage (memory, KV-cache, and training time) relative to dense self-attention.

***

## ‚öôÔ∏è Technical Details

| Feature | Description |
| :--- | :--- |
| **Core Mechanism** | Replaces standard dense multi-head attention (MHA) with a mixture of sparse attention heads using Expert-Choice Routing. |
| **Token Selection** | Each head selects a subset of tokens ($k$) from the sequence ($T$) based on content-based scores computed via a learned router and sigmoid function. |
| **Projection** | Q, K, V projections are computed **only** for the selected tokens. |
| **Masking** | An adaptive causal mask is applied to maintain causality. |
| **Complexity** | Reduced from $O(T^2)$ to $O(k^2 + T)$. |
| **Differentiation** | **Contrast with Fixed Sparse:** MoSA is data-dependent. <br> **Contrast with Routing Transformer:** MoSA uses a learned router optimized end-to-end and subsamples before projection. |

***

## üìä Results

On the C4 dataset under a fixed FLOP budget, MoSA consistently outperformed the Dense baseline in Perplexity across all model sizes. Baselines Fixed Sparse and Routing Transformer failed to match dense performance.

### Performance Comparison

| Model Size | MoSA Perplexity | Dense Baseline | Improvement |
| :--- | :---: | :---: | :---: |
| **Tiny (28M)** | 16.39 | 22.46 | **-27.0%** |
| **Small (113M)** | 12.85 | 16.01 | **-19.7%** |
| **Medium (210M)** | 11.06 | 13.95 | **-20.7%** |
| **Large (516M)** | 10.58 | 12.20 | **-13.3%** |

MoSA also achieves drastic reduction in KV-cache size and allows for more attention heads within the budget.

***

**Quality Score:** 8/10  
**References:** 40 citations