---
title: Bootstrap Off-policy with World Model
arxiv_id: '2511.00423'
source_url: https://arxiv.org/abs/2511.00423
generated_at: '2026-02-03T13:34:41'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Bootstrap Off-policy with World Model

*Guojian Zhan; Likun Wang; Xiangteng Zhang; Jiaxin Gao; Masayoshi Tomizuka; Shengbo Eben Li*

***

> ### **Quick Facts**
> *   **Quality Score:** 9/10
> *   **Citations:** 40 References
> *   **Key Benchmarks:** DeepMind Control Suite, Humanoid-Bench
> *   **Primary Innovation:** Bootstrap Loop & Soft Value-Weighted Mechanism
> *   **Problem Solved:** Actor Divergence in Model-Based RL

***

## Executive Summary

This research addresses the critical issue of **"Actor Divergence"** in model-based reinforcement learning (MBRL), a phenomenon where the data distribution collected by an agent significantly diverges from the distribution of the policy's actual behaviors. In standard MBRL frameworks utilizing online planning, this discrepancy degrades the quality of the learned world model and destabilizes the training process, particularly in high-dimensional control environments. The paper aims to resolve this instability by establishing a mechanism that ensures the policy learns from data that remains consistent with its evolving behavior, thereby bridging the gap between planning and execution.

The authors propose **BOOM (Bootstrap Off-policy with WOrld Model)**, a framework that tightly integrates a parametric policy, a world model (specifically TD-MPC2), and an MPPI planner through a "bootstrap loop." In this loop, the policy initializes the planner, which then refines actions; these refined actions are used to update the policy via behavior alignment. Crucially, the world model is responsible for simulating future trajectories and generating the value targets necessary for learning. To handle the intractable likelihood of the planner's distribution, BOOM employs a likelihood-free alignment loss that minimizes Forward KL Divergence. Additionally, a "soft value-weighted mechanism" is implemented, assigning weights to trajectories based on their Q-values ($w_i \propto \exp(Q_i/\tau)$) to prioritize high-return behaviors and effectively mitigate the action variability typically associated with stochastic planners.

BOOM achieves **state-of-the-art (SOTA) performance** on the DeepMind Control Suite and Humanoid-Bench, backed by significant quantitative improvements over baselines. On the Humanoid-Bench, BOOM successfully solved 13 out of 14 complex tasks, a substantial increase over TD-MPC2, which struggled to converge on many of the same benchmarks; specifically, on the "Stand" task, BOOM achieved near-maximum scores while baselines showed low success rates. In the DeepMind Control Suite, BOOM demonstrated superior sample efficiency, reaching target performance approximately 1.5 to 2 times faster than the model-free Soft Actor-Critic (SAC) and achieving final returns up to 15% higher than the previous SOTA model-based method, TD-MPC2, on challenging locomotion tasks like `dog-run`. These metrics confirm that the soft value-weighted mechanism successfully reduces noisy action variability while the alignment loss distills the planner's behavior effectively.

This work is significant as it presents a robust solution to the persistent challenge of data inconsistency in planning-based reinforcement learning. By successfully bridging the gap between online planning and off-policy learning, BOOM enhances the stability and sample efficiency of model-based agents. The ability to reliably distill complex planning behaviors into a static policy without destabilizing the learning process has broad implications for the deployment of RL in complex, high-dimensional real-world control tasks where computational efficiency and reliability are paramount.

***

## Key Findings

*   **Data Divergence Issue:** The research identifies that using online planning creates a divergence between collected data and the policy's actual behaviors, which degrades model learning.
*   **SOTA Performance:** The proposed BOOM framework achieves state-of-the-art results in training stability and final performance on high-dimensional benchmarks like the DeepMind Control Suite.
*   **Action Prioritization:** A soft value-weighted mechanism is implemented to prioritize high-return behaviors and mitigate action variability within the replay buffer.
*   **Sample Efficiency:** Tightly integrating planning with off-policy learning via a bootstrap loop proves effective for improving sample efficiency.

***

## Methodology

The paper proposes **BOOM (Bootstrap Off-policy with WOrld Model)**, a framework designed to integrate planning and off-policy learning through a bootstrap loop. The methodology operates on the following principles:

*   **Bootstrap Loop:** In this loop, the policy initializes the planner, which refines actions used to update the policy via behavior alignment.
*   **World Model Integration:** A jointly learned world model simulates future trajectories and provides value targets.
*   **Likelihood-Free Alignment:** The methodology utilizes a likelihood-free alignment loss to bootstrap the policy using the planner's non-parametric action distribution.
*   **Soft Value-Weighted Mechanism:** This mechanism prioritizes high-return behaviors to stabilize training.

***

## Technical Details

The BOOM framework addresses **Actor Divergence** in off-policy RL by integrating three core components: a **World Model (TD-MPC2)**, an **MPPI Planner**, and a **Parametric Policy**.

### Core Components & Processes
*   **The Bootstrap Loop:** The policy initiates the planner, and the planner provides refined alignment targets.
*   **Alignment Loss:** To handle the intractable likelihood of MPPI, BOOM minimizes Forward KL Divergence. The mathematical formulation is:
    $$L_{align} = \mathbb{E} [-\log \pi(a|s)]$$
*   **Soft Value-Weighted Mechanism:** Weights are applied based on Q-values to focus learning on high-value trajectories:
    $$w_i \propto \exp(Q_i/\tau)$$

***

## Contributions

*   **Novel Framework:** Introduction of the BOOM framework, a novel model-based reinforcement learning approach that resolves data divergence issues in planning-based interaction.
*   **Likelihood-Free Loss:** Development of a likelihood-free alignment loss that leverages non-parametric action distributions from a planner to bootstrap the policy.
*   **Stability Enhancement:** Enhancement of stability in model-based RL through the soft value-weighted mechanism, which manages the variability of planner-generated actions.
*   **Empirical Validation:** Comprehensive empirical validation demonstrating that the method outperforms existing state-of-the-art techniques in complex, high-dimensional control environments.

***

## Results

*   **Benchmark Dominance:** The framework achieved state-of-the-art (SOTA) results on the DeepMind Control Suite and Humanoid-Bench.
*   **Stability & Efficiency:** It demonstrated superior training stability and improved sample efficiency over model-free baselines (SAC) and prior model-based methods (TD-MPC/TD-MPC2).
*   **Qualitative Confirmation:** Qualitative findings confirmed that the soft value-weighted mechanism mitigated action variability and the alignment loss successfully distilled the planner's behavior into the policy.

***

## Report Metadata

*   **Quality Score:** 9/10
*   **References:** 40 citations