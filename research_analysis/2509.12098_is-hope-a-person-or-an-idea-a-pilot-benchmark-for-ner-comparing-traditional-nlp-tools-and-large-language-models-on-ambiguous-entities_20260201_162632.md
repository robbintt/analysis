# Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities

*Payam Latifi*

---

> ### dd Quick Facts
> *   **Quality Score:** 7/10
> *   **Total Citations:** 10
> *   **Dataset Size:** 119 Tokens
> *   **Models Evaluated:** 6 (3 LLMs, 3 Traditional)
> *   **Top Performer:** Gemini-1.5-flash
> *   **Primary Metric:** F1-score

---

> ### dd Executive Summary
>
> Named Entity Recognition (NER) remains a complex challenge in natural language processing, particularly when resolving ambiguous contexts that require pragmatic understanding rather than simple capitalization rules, such as distinguishing whether "Hope" refers to a person or an abstract concept. While Large Language Models (LLMs) offer advanced contextual reasoning compared to the speed of traditional NLP tools, there is insufficient empirical data directly comparing these paradigms on edge cases involving semantic disambiguation. This paper addresses the critical need to evaluate whether the computational overhead of LLMs yields tangible accuracy improvements over standard pipelines, specifically focusing on how different architectures handle ambiguous tokens and multi-word spans.
>
> The study’s key innovation is the development of a pilot "diagnostic" benchmark—a manually annotated dataset of 119 annotated target instances specifically designed to test NER performance on difficult, ambiguous entities spanning PERSON, LOCATION, ORGANIZATION, DATE, and TIME categories. To ensure rigorous evaluation, the researchers established a comparative framework assessing three established non-LLM tools (NLTK, spaCy, Stanza) against three general-purpose LLMs (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The methodology employed single-shot prompting for the LLMs and utilized strict token-level alignment against a unified gold standard using NLTK's `word_tokenize`, ensuring a direct, "apples-to-apples" comparison of entity extraction capabilities across distinct architectures.
>
> The experiments revealed distinct performance trade-offs, with Gemini-1.5-flash achieving the highest average F1-score across all tested systems. LLMs demonstrated superior capabilities in context-sensitive tasks, significantly outperforming traditional tools in identifying PERSON entities, whereas traditional tools, particularly Stanza, proved more reliable for structurally rigid tags like LOCATION and DATE. Analysis of the dataset composition—comprising 56 entities (~47%) and 63 non-entities (~53%)—showed that while LLMs struggled with boundary detection for multi-token spans and pragmatic inference for time expressions, traditional systems faltered when semantic nuance was required to disambiguate capitalized common nouns. The study noted that no statistical significance tests were performed due to the pilot scale of the data (n=119).
>
> This research provides critical, actionable insights for NLP engineers, suggesting that the optimal choice of NER system should be dictated by the specific entity types being targeted rather than a blanket preference for modern architecture. It validates the use of LLMs for scenarios requiring deep semantic disambiguation of proper nouns while advocating for the continued use of traditional pipelines for high-precision tasks involving standardized, structurally rigid data. By exposing specific weaknesses in boundary detection and variability across LLMs, the paper establishes a necessary baseline for future research into hybrid models that aim to combine the contextual reasoning of generative AI with the structural reliability of classic NLP.

---

## Key Findings

*   **Performance Hierarchy:** **LLMs generally outperformed** conventional NLP tools in recognizing context-sensitive entities, specifically person names. **Gemini-1.5-flash** achieved the highest average F1-score among all evaluated systems.
*   **Consistency in Structure:** Traditional NLP tools, particularly **Stanza**, demonstrated greater consistency and reliability in handling structured tags such as **LOCATION** and **DATE**.
*   **LLM Variability:** Significant variability exists among LLMs regarding their ability to process temporal expressions and multi-word organization names.
*   **Architecture Trade-offs:** While LLMs offer superior **contextual understanding**, traditional tools remain competitive for specific, structurally rigid tasks.

---

## Methodology

The researchers constructed a pilot benchmark consisting of a carefully annotated dataset of **119 tokens** spanning five entity types: PERSON, LOCATION, ORGANIZATION, DATE, and TIME. Six systems were evaluated and compared using F1-scores against this manual gold standard:

*   **Traditional Non-LLM Tools:** NLTK, spaCy, Stanza
*   **Large Language Models:** Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B

The dataset was designed to act as a "diagnostic" tool, specifically targeting ambiguous entities and multi-word spans to stress-test the systems' semantic understanding.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Task** | Named Entity Recognition (NER) |
| **Target Entities** | PERSON, LOCATION, ORGANIZATION, DATE, TIME |
| **Evaluated Tools** | **Non-LLM:** NLTK, spaCy, Stanza<br>**LLMs:** Gemini-1.5-Flash, DeepSeek-V3, Qwen-3-4B |
| **Tokenizer** | `NLTK's Punktword_tokenize` (used for strict alignment) |
| **LLM Prompting** | Single-shot prompting |
| **Gemini Config** | Temp 0.2, Google Python SDK |
| **DeepSeek/Qwen Config** | Temp 0.1, 1000 max tokens, OpenRouter |
| **Data Source** | Custom, manually annotated 'diagnostic' dataset designed with ambiguity |

---

## Results

*   **Top Model:** **Gemini-1.5-Flash** achieved the highest average F1-score.
*   **Entity Specifics:**
    *   **LLMs:** Outperformed traditional tools on context-sensitive **PERSON** entities.
    *   **Stanza:** Demonstrated better consistency for **LOCATION** and **DATE** tags.
*   **Dataset Composition:** 119 total tokens consisting of 56 Entities (~47%) and 63 Non-Entities (~53%).
*   **Identified Challenges:**
    *   Boundary detection for multi-token spans.
    *   Semantic disambiguation of capitalized common nouns.
    *   Pragmatic inference for time expressions.
*   **Limitations:** No statistical significance tests were performed due to the small pilot scale (n=119).

---

## Contributions

*   **Unique Dataset:** Introduction of a small-scale, high-quality annotated dataset specifically designed to test NER performance on ambiguous entities.
*   **Direct Comparison:** A side-by-side performance evaluation between established traditional NLP pipelines and modern general-purpose LLMs on the same task.
*   **Empirical Insights:** Provided evidence highlighting the trade-offs between LLMs (contextual understanding) and traditional tools (structural consistency), offering practical guidance for model selection based on specific entity types.