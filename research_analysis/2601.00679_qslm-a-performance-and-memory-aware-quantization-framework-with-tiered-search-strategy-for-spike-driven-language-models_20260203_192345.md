---
title: 'QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search
  Strategy for Spike-driven Language Models'
arxiv_id: '2601.00679'
source_url: https://arxiv.org/abs/2601.00679
generated_at: '2026-02-03T19:23:45'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models

*Rachmad Vidya Wicaksana Putra; Pasindu Wickramasinghe; Muhammad Shafique*

---

> ### ðŸ“Š Quick Facts
>
> *   **Memory Footprint Reduction:** Up to **86.5%**
> *   **Power Consumption Reduction:** Up to **20%**
> *   **SST-2 Accuracy:** **84.4%**
> *   **WikiText-2 Perplexity:** **23.2**
> *   **Quality Score:** **9/10**
> *   **References:** 40 Citations

---

## Executive Summary

Spike-driven Language Models (SLMs) offer a promising path toward energy-efficient natural language processing by leveraging spiking neural networks (SNNs). However, deploying these models on resource-constrained embedded devices remains a significant challenge due to their substantial memory footprints. While manual quantization can reduce memory usage, it is an unscalable and labor-intensive process that fails to account for the complex trade-offs between energy efficiency, memory constraints, and model accuracy. This paper addresses the critical need for an automated optimization framework capable of bridging the gap between the theoretical energy efficiency of SLMs and the practical hardware limitations of edge devices.

The authors introduce **QSLM**, the first performance- and memory-aware quantization framework specifically designed for spike-driven language models. The core innovation is a **tiered search strategy** that performs architecture and sensitivity analysis to identify network hierarchies, then applies quantization at global, block, and module levels. This automated process utilizes a multi-objective optimization function governed by user-defined constraints for performance (`constA`) and memory (`constM`). By manipulating a hyperparameter `alpha`, the framework dynamically adjusts the priority between accuracy and memory savings, automatically selecting optimal weight precision reductions without the need for manual intervention.

QSLM demonstrates substantial efficiency gains while maintaining competitive task performance. The framework achieved a memory footprint reduction of up to **86.5%** and reduced power consumption by up to **20%**. On the SST-2 sentiment classification task, the quantized model retained high accuracy at **84.4%**, while achieving a perplexity score of **23.2** on the WikiText-2 text generation task. Experiments further validated the framework's flexibility, showing that varying the `alpha` parameter successfully navigated the trade-off landscape to select specific model candidates (e.g., Candidate 7 vs. Candidate 8) that met strict memory caps, such as 400MB or 420MB.

This work significantly advances the field of neuromorphic computing by providing a scalable solution for deploying SLMs on edge hardware. By automating the quantization process, QSLM eliminates the bottleneck of manual tuning and enables the practical use of spike-driven architectures in power-sensitive environments. The framework establishes a new standard for optimizing SLMs, paving the way for broader adoption of energy-efficient language models in mobile and embedded systems where both memory and energy budgets are strictly limited.

---

## Key Findings

*   **Significant Memory Reduction:** The framework achieves a reduction in memory footprint of up to **86.5%**, making SLMs viable for embedded devices.
*   **Lower Power Consumption:** Power consumption is reduced by up to **20%**, with specific candidates showing reductions between 3.2% and 11.6% compared to the baseline.
*   **High Accuracy Retention:** Despite aggressive quantization, the model maintains high performance with **84.4%** accuracy on the SST-2 sentiment classification task.
*   **Competitive Text Generation:** Achieved a perplexity score of **23.2** on WikiText-2, demonstrating efficacy in generation tasks.
*   **Constraint Satisfaction:** Successfully meets strict performance and memory constraints simultaneously, balancing the trade-off effectively.

---

## Methodology

The QSLM framework utilizes an automated, multi-step process designed to optimize pre-trained Spike-driven Language Models without manual intervention.

1.  **Architecture and Sensitivity Analysis**
    *   Identifies the network hierarchy.
    *   Analyzes layer sensitivity to determine quantization potential.

2.  **Tiered Quantization Strategy**
    *   Employs a hierarchical approach using three levels:
        *   **Global-level:** Broad quantization settings.
        *   **Block-level:** Intermediate grouping adjustments.
        *   **Module-level:** Fine-grained component optimization.

3.  **Multi-objective Optimization**
    *   Utilizes a performance-and-memory trade-off function.
    *   Automatically selects optimal quantization settings based on the analysis.

---

## Technical Details

QSLM is a specialized quantization framework for Spike-driven Language Models (SLMs) that reduces weight parameter precision through a tiered search strategy.

| Component | Description |
| :--- | :--- |
| **Search Strategy** | Selects optimal quantized model candidates by reducing weight parameter precision. |
| **Constraints** | Governed by user-defined parameters:<br>â€¢ **constA:** Performance/Accuracy constraint<br>â€¢ **constM:** Memory footprint constraint |
| **Hyperparameter ($\alpha$)** | Controls the trade-off between performance and memory.<br>â€¢ **$\alpha = 0$:** Prioritizes accuracy.<br>â€¢ **$\alpha > 0$:** Adjusts priority toward memory savings. |

---

## Results

The proposed framework was evaluated on standard NLP benchmarks, demonstrating the ability to navigate the accuracy-memory trade-off via the $\alpha$ hyperparameter.

*   **Efficiency Metrics:**
    *   **86.5%** reduction in memory footprint.
    *   **20%** reduction in power consumption (max).
*   **Task Performance:**
    *   **SST-2:** 84.4% Accuracy.
    *   **WikiText-2:** 23.2 Perplexity (abstract) / 24.0 (detailed).
*   **Trade-off Validation:**
    *   Varying $\alpha$ values successfully selected different candidates (e.g., Candidate 7 vs. Candidate 8) to meet specific memory caps (e.g., 400MB vs 420MB).

---

## Contributions

*   **Automated Solution:** Addresses the scalability issues of manual quantization for SLMs by providing a fully automated framework.
*   **Bridging the Gap:** Successfully connects the energy efficiency of Spike-driven models with the memory limitations of resource-constrained embedded devices.
*   **Novel Framework:** Introduces QSLM as the first known performance- and memory-aware quantization framework tailored specifically for spike-driven language models.