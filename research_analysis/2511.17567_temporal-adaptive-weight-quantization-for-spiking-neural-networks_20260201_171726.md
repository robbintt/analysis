# Temporal-adaptive Weight Quantization for Spiking Neural Networks
*Han Zhang; Qingyan Meng; Jiaqi Wang; Baiyu Chen; Zhengyu Ma; Xiaopeng Fan*

---

> ### **Quick Facts**
> *   **Quality Score:** 9/10
> *   **Quantization:** 1.58-bit representation
> *   **Accuracy Loss:** As low as 0.22% (ImageNet)
> *   **Energy Consumption:** 0.63mJ
> *   **Operations:** 4.12M (ops/params)
> *   **Key Mechanism:** Temporal-adaptive allocation (Bio-inspired)

---

## Executive Summary

Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient neuromorphic computing, yet their deployment on resource-constrained hardware is hindered by a persistent efficiency-accuracy trade-off. Traditional weight quantization methods, often necessary to reduce memory footprints, typically result in substantial accuracy degradation, creating a barrier to practical application. This paper addresses this critical challenge by seeking a method to drastically reduce model precision and energy consumption without sacrificing the high accuracy required for complex tasks.

The authors introduce **Temporal-adaptive Weight Quantization (TaWQ)**, a bio-inspired framework grounded in the concept of astrocyte-mediated synaptic modulation found in biological nervous systems. Technically, the method innovates by integrating weight quantization directly into the temporal dynamics of SNNs rather than relying on static values. It employs a 1.58-bit weight representation and utilizes addition-based operations (+1, -1) to enable the adaptive allocation of weights along the temporal dimension, optimizing resource usage while maintaining synaptic efficacy.

Experimental results verified on both static datasets (ImageNet) and neuromorphic datasets (CIFAR10-DVS) demonstrate that TaWQ effectively resolves the tension between efficiency and precision. On ImageNet, the method achieves a negligible accuracy loss of only 0.22% compared to full-precision baselines, consistently outperforming 8-bit Post-Training Quantization. Hardware implementation on a 910 NPU validated these computational gains, recording specific energy efficiency metrics of 4.12M operations/parameters and energy consumption of 0.63mJ.

This research significantly advances the state-of-the-art for low-power neuromorphic computing by providing empirical evidence that ultra-low-bit quantization is feasible for large-scale, complex classification tasks. By proving that high-performance SNNs can operate with minimal energy overhead and near-lossless accuracy, TaWQ establishes a viable pathway for deploying sophisticated neuromorphic systems on edge devices, setting a new benchmark for future hardware design.

---

## Key Findings

*   **Minimal Accuracy Loss:** The proposed method achieves a negligible quantization loss of only **0.22%** on the ImageNet dataset, successfully preserving model performance at ultra-low bit widths.
*   **High Energy Efficiency:** Temporal-adaptive Weight Quantization (TaWQ) retains high energy efficiency, specifically recording metrics of **4.12M operations/parameters** and **0.63mJ energy consumption**.
*   **Broad Applicability:** The approach was validated on both static datasets (ImageNet) and neuromorphic datasets (CIFAR10-DVS), demonstrating robustness across different data modalities.

---

## Methodology

The authors propose **Temporal-adaptive Weight Quantization (TaWQ)**, a method inspired by astrocyte-mediated synaptic modulation found in biological nervous systems.

*   **Core Technical Mechanism:** Involves incorporating weight quantization directly with the temporal dynamics of Spiking Neural Networks.
*   **Adaptive Allocation:** This allows for the adaptive allocation of ultra-low-bit weights along the temporal dimension, optimizing how weights are utilized over time rather than using static values.

---

## Contributions

*   **Resolution of the Efficiency-Accuracy Trade-off:** Addresses the primary challenge in SNN weight quantizationâ€”reducing energy consumption without incurring significant accuracy degradation.
*   **Bio-inspired Temporal Integration:** Introduces a novel quantization framework that leverages the temporal dimension of SNNs, drawing a direct parallel to biological astrocyte functions to improve artificial network design.
*   **Validation on Complex Benchmarks:** Provides extensive experimental evidence that ultra-low-bit quantization is viable for large-scale, complex tasks like ImageNet classification, advancing the state-of-the-art for low-power neuromorphic computing.

---

## Technical Details

The following configuration was used to achieve the results:

| Parameter | Specification |
| :--- | :--- |
| **Weight Representation** | 1.58-bit |
| **Operations** | Exclusive use of addition (+1, -1), eliminating multiplication |
| **Hardware Platform** | 910 NPU |
| **Precision Constraints** | Logic: 1.58-bit/32-bit; Execution: 16-bit weights (due to FP16 constraints) |
| **Batch Size** | Fixed at 32 |
| **Input Processing (SHD)** | Spatio-temporal binning to reduce dimensions to Bx100x140 |
| **Optimizer** | AdamW |
| **Scheduler** | Cosine Annealing |
| **Learning Rate** | 1e-3 |

---

## Experimental Results

### Performance on ImageNet
The TaWQ (1.58-bit) model successfully outperformed 8-bit PTQ and binary networks across several architectures:

| Model | TaWQ Accuracy | Accuracy Drop vs. 32-bit |
| :--- | :--- | :--- |
| **Spikingformer** | 77.42% | 0.22% |
| **QKFormer** | 82.94% | 1.28% |
| **SEWResNet** | 62.06% | 1.16% |

### Hardware Metrics (910 NPU)
*   **Memory Footprint:** Constant at 6.88 G (due to 16-bit conversion).
*   **Latency:** Nearly identical to baselines (e.g., QKFormer increased by only 0.2 ms).
*   **Energy Efficiency:** Achieved 4.12M operations/parameters and 0.63mJ energy consumption.

---
**References:** 40 citations