---
title: Agentic Reasoning and Refinement through Semantic Interaction
arxiv_id: '2510.02157'
source_url: https://arxiv.org/abs/2510.02157
generated_at: '2026-01-28T00:51:16'
quality_score: 8
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Agentic Reasoning and Refinement through Semantic Interaction

*Xuxin Tang, Agentic Reasoning, Virginia Tech, Semantic Interaction, Kirsten Whitley, Xuan Wang, Rehema Abulikemu, Eric Krokos*

---

### ðŸ“ Executive Summary

This research addresses the critical limitation of Large Language Models (LLMs) in precisely incorporating sequential semantic interactions during iterative sensemaking and report generation processes. While LLMs are proficient at initial text generation, they struggle to update documents contextually based on user manipulations within visual workspaces (such as annotations or reorganization). Existing approaches often fail by either regenerating entire reportsâ€”which sacrifices the precision of user editsâ€”or by applying updates without reasoning, which results in low recall and a loss of context. This gap hinders effective human-LLM collaboration, as users cannot reliably guide the AI to refine complex narratives through visual interaction.

The authors introduce **VIS-ReAct**, a novel agentic framework utilizing the "Reason and Act" (ReAct) paradigm to enable targeted, context-aware refinement. The core innovation lies in a dual-agent architecture that decouples the interpretation of user intent from the execution of text updates. The framework follows a four-step pipeline: (1) Workspace Serialization converts the visual state to structured text; (2) Semantic Interaction Extraction isolates new user inputs ($SI_n$); (3) Agent 1 (Analysis) interprets these inputs to infer intent and generates a "contextual refinement guidance" plan; and (4) Agent 2 (Refinement) executes this plan to update the report. By explicitly reasoning about sequential interactions before acting, the system maintains logic-driven coherence throughout the editing process.

In experiments using GPT-4o-mini on the "The Sign of Crescent" dataset (41 reports and 35 test pairs), VIS-ReAct outperformed both standard regeneration baselines and the non-reasoning VIS-Act method. The framework achieved a paragraph-level F1-score of **0.887**, significantly higher than the Baseline (0.858) and VIS-Act (0.782). Specifically, VIS-ReAct attained a Precision of 0.951 and a Recall of 0.831. Qualitative analysis further demonstrated high semantic fidelity, with the system successfully resolving context-dependent issues like abbreviations, and provided transparent inference through "condensed analysis" logs that allow users to verify the LLM's interpretation of their actions.

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Model Used** | GPT-4o-mini |
> | **Dataset** | The Sign of Crescent (41 reports) |
> | **Best F1-Score** | 0.887 (VIS-ReAct) |
> | **Precision** | 0.951 |
> | **Recall** | 0.831 |
> | **References** | 22 citations |

---

## Key Findings

*   **Superior Performance:** The VIS-ReAct framework significantly outperforms baseline methods and the VIS-Act approach in report generation tasks.
*   **Targeted Refinement:** The system achieves superior results in targeted refinement, ensuring high semantic fidelity and transparent inference processes.
*   **Granularity Management:** The approach effectively manages various types of interactions and adapts to different levels of interaction granularity.
*   **Enhanced Transparency:** Implementing a dedicated reasoning step significantly enhances the transparency of human-LLM collaboration, allowing users to understand AI intent.

---

## Methodology

The proposed method utilizes a two-agent framework designed to integrate visual semantic interactions with LLM text generation.

*   **Framework Architecture:** Built upon the **VIS-ReAct** structure.
*   **Agent 1 (Analysis):** A primary LLM agent responsible for interpreting new semantic interactions to infer user intentions. It generates a specific refinement plan based on these inferences.
*   **Agent 2 (Refinement):** A secondary LLM agent that executes the plan generated by the first agent to update the report accordingly.
*   **Process Flow:** The framework reasons about sequential semantic interactions within visual workspaces to steer the LLM, enabling iterative, logic-driven refinement rather than blind regeneration.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Framework Name** | VIS-ReAct |
| **Core Concept** | An agentic framework utilizing the 'Reason and Act' (ReAct) paradigm for targeted, context-aware refinement of LLM-generated reports based on semantic interactions. |
| **Workflow** | **Four-step pipeline:** <br>1. **Workspace Serialization:** Converts visual workspace to structured text.<br>2. **Semantic Interaction Extraction ($SI_n$):** Isolates new interactions like annotations or reorganization.<br>3. **LLM Analysis:** Agentic Reasoning to infer human intent and generate 'contextual refinement guidance'.<br>4. **LLM Refinement:** Generates the updated report with BLUF structure. |
| **Comparison** | VIS-ReAct improves upon the **Baseline (ReSPIRE)**, which regenerates everything causing low precision, and **VIS-Act**, which updates based only on interactions causing low recall and context loss. |

---

## Results

**Experimental Setup**
The study utilized GPT-4o-mini on the "The Sign of Crescent" dataset, comprising 41 reports and 35 test pairs.

**Quantitative Metrics**
Performance was evaluated using Precision, Recall, and F1-score at the paragraph level:
*   **VIS-ReAct:** Precision (0.951), Recall (0.831), F1-score (0.887)
*   **Baseline:** F1-score (0.858)
*   **VIS-Act:** F1-score (0.782)
*   *Result:* VIS-ReAct outperformed both competing methods.

**Qualitative Results**
*   **High Semantic Fidelity:** Successfully resolved isolated interactions with missing context (e.g., handling abbreviations correctly).
*   **Transparent Inference:** Provided "condensed analysis" logs that allowed users to verify the LLM's interpretation of their actions.

---

## Contributions

*   **Solution to Sequential Refinement:** Addresses the specific limitation of LLMs in precisely incorporating sequential semantic interactions during the iterative sensemaking and report writing process.
*   **Multi-Agent Design:** Introduces a dual-agent architecture that separates the interpretation of user intent (analysis) from the execution of text updates (refinement).
*   **Benchmarking:** Provides comparative evidence against non-reasoning approaches (VIS-Act), establishing the value of explicit reasoning stages in visual analytics and human-AI collaboration.