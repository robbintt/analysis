---
title: 'GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge'
arxiv_id: '2507.05740'
source_url: https://arxiv.org/abs/2507.05740
generated_at: '2026-01-27T21:38:41'
quality_score: 9
citation_count: 2
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge

*Phong Nguyen, Suzhou City, Simon Razniewski, Exploring Factual, Suzhou Search, Yujia Hu, Shrestha Ghosh, Suzhou Port, Suzhou Zoo, Massive Knowledge*

***

## Executive Summary

Large Language Models (LLMs) encode vast amounts of factual knowledge within their parameters, yet this knowledge remains "implicit," making it difficult to verify, query with precision, or analyze systematically. Traditional methods for converting this implicit knowledge into structured Knowledge Bases (KBs) are often labor-intensive or fail to scale effectively. This paper addresses the challenge of materializing the internal factual knowledge of a state-of-the-art LLM (GPT-4.1) into a structured, queryable format, thereby enabling researchers to inspect, browse, and perform quantitative analysis on the model's understanding of the world.

The key innovation is a "Massive-Recursive LLM Knowledge Materialization" pipeline that utilizes a Recursive Elicitation & Consolidation approach. The process employs a Breadth-First Search (BFS) strategy, starting from seed entities and recursively using the LLM itself to perform Named-Entity Recognition (NER) and extract constrained triples. To handle the redundancy inherent in generative outputs, the authors implement a greedy clustering algorithm that canonicalizes relations and classes based on label embedding similarity. This technique aggressively condenses the graph—merging 936,000 raw relations into 381,000 and reducing classes from 220,000 to 32,000—to create a coherent, densely interlinked RDF structure suitable for logical reasoning.

The resulting GPTKB v1.5 comprises 6.1 million entities and 100 million triples, materialized over 18 days at a total compute cost of $14,136. The framework achieves a triple accuracy of 75.5% (automated evaluation) and 75% (manual evaluation), with subject verifiability reaching 85.3%. Compared to the previous version (v1.1), this iteration represents a 110% increase in the number of entities and a 44 percentage point increase in accuracy, while also successfully eliminating major entity anomalies present in earlier iterations.

This research validates the feasibility of using massive-recursive techniques to automate the construction of large-scale Knowledge Bases directly from LLMs. By releasing GPTKB v1.5 as a public resource, the authors provide a novel analytical framework that supports complex SPARQL querying and link-traversal exploration of model behavior. This bridges the gap between neural and symbolic AI, enabling the community to perform comparative analysis and factual auditing of foundation models in ways that were previously impossible with text-only interfaces.

***

## Quick Facts

| Metric | Value |
| :--- | :--- |
| **Total Triples** | 100 Million |
| **Total Entities** | 6.1 Million |
| **Materialization Cost** | $14,136 |
| **Construction Time** | 18 Days |
| **Accuracy (Auto)** | 75.5% |
| **Accuracy (Manual)** | 75% |
| **Subject Verifiability** | 85.3% |
| **Base Model Used** | GPT-4.1 |

***

## Key Findings

*   **Massive-Scale Materialization:** Successful construction of GPTKB V1.5, a densely interlinked knowledge base with 100 million triples extracted entirely from GPT-4.1.
*   **Cost-Effectiveness:** Demonstrated that the entire massive knowledge base could be materialized for a relatively low cost of $14,000.
*   **Accessibility of Implicit Knowledge:** Proved that the implicit factual knowledge stored within LLM parameters can be externalized into structured data suitable for browsing and analysis.
*   **Queryable Structure:** Established a framework that supports complex structured querying (SPARQL) for precise retrieval and quantitative analysis of model knowledge.

## Methodology

The research employs a **Massive-Recursive LLM Knowledge Materialization** approach, defined by the following components:

*   **Recursive Process:** Utilizes the LLM recursively to extract and structure its own internal knowledge into explicit facts.
*   **Entity-Centric Extraction:** Focuses specifically on generating entity nodes and defining predicate-object relationships to build a coherent graph structure.
*   **Interlinking Strategy:** Creates a densely interlinked web of data, designed explicitly to facilitate link-traversal exploration and establish logical connections between data points.

## Technical Details

The system architecture and data processing pipeline are built upon the following technical specifications:

### Architecture
*   **Approach:** Recursive Elicitation & Consolidation
*   **Traversal:** Breadth-First Search (BFS) initiated from a seed entity.
*   **Extraction:** Utilizes LLM-based Named-Entity Recognition (NER) and constrained decoding to generate triples.

### Redundancy Mitigation
*   **Algorithm:** Greedy clustering algorithm to merge relations and classes.
*   **Metric:** Canonicalization based on label embedding similarity.
*   **Impact:**
    *   Relations reduced from 936k to 381k.
    *   Classes reduced from 220k to 32,000.

### Storage & Hosting
*   **Format:** RDF (Turtle syntax).
*   **Meta-relations:** Includes 'bfsLayer' and 'bfsParent' attributes.
*   **Tech Stack:** Python Django, Nginx, and OpenLink Virtuoso.

## Contributions

1.  **Resource Contribution (GPTKB V1.5):** The release of a massive, publicly accessible knowledge base and a demonstrator application for the research community.
2.  **Analytical Framework:** Introduction of three distinct use cases for analyzing LLM knowledge:
    *   Link-traversal exploration.
    *   SPARQL-based querying.
    *   Comparative analysis.
3.  **Advancement in Automated KB Construction:** Validation of massive-recursive techniques as a viable method for the automated construction of Knowledge Bases using Large Language Models.

## Results

The performance and scale of GPTKB v1.5 show significant improvements over previous iterations:

*   **Construction Metrics:**
    *   Built using GPT-4.1 over 18 days.
    *   Total compute cost: $14,136.
    *   Final Output: 6.1 million entities and 100 million triples.
*   **Evaluation Metrics:**
    *   Triple Accuracy: 75.5% (Automated) / 75% (Manual).
    *   Subject Verifiability: 85.3%.
*   **Comparative Growth (vs v1.1):**
    *   **110% increase** in the number of entities.
    *   **44 percentage point increase** in accuracy.
    *   Eliminated major entity anomalies found in earlier versions.

***

**Document Quality Score:** 9/10  
**References:** 2 citations