---
title: Delta Knowledge Distillation for Large Language Models
arxiv_id: '2509.14526'
source_url: https://arxiv.org/abs/2509.14526
generated_at: '2026-02-03T18:45:31'
quality_score: 9
citation_count: 2
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Delta Knowledge Distillation for Large Language Models
*Yihan Cao; Yanbin Kang; Zhengming Xing; Ruijie Jiang*

---

> ### üìä Quick Facts
> *   **Teacher Model:** Qwen-2.5-7B
> *   **Student Model:** Qwen-2.5-1.5B
> *   **Primary Datasets:** Ultrachat-200k, OpenMathReasoning
> *   **Core Innovation:** Behavioral Shift Alignment
> *   **Top Performance:** ROUGE-1 of **0.5407** (Ultrachat-200k)

---

### üìÑ Executive Summary

This research addresses the critical inefficiency of deploying Large Language Models (LLMs) and the limitations of current Knowledge Distillation (KD) techniques used to compress them. Standard token-level KD approaches typically minimize the Kullback-Leibler (KL) divergence between the output distributions of a large teacher model and a smaller student model. The authors identify a theoretical flaw in this practice: the assumption that the student and teacher share the same optimal representation space. Because student models are significantly smaller and less capable than their teachers, forcing them to mimic absolute output distributions often leads to suboptimal performance and a loss of nuanced knowledge.

The authors introduce **Delta Knowledge Distillation (Delta-KD)**, a novel framework designed to align student models with the teacher‚Äôs representation space through "Behavioral Shift Alignment." Instead of mimicking the teacher's absolute output probabilities, Delta-KD encourages the student to replicate the specific distributional shift (the "Delta") that the teacher model acquired during its Supervised Fine-Tuning (SFT) phase. Technically, the method utilizes a shift operator defined as $\Delta(p1, p2) = p1/p2$ to construct a synthetic target distribution.

The efficacy of Delta-KD was empirically validated using Qwen-2.5-7B as the teacher and Qwen-2.5-1.5B as the student across the Ultrachat-200k and OpenMathReasoning datasets. Delta-KD consistently outperformed standard baselines. The results indicate significant improvements in capturing bi-gram patterns and maintaining long-form coherence compared to prior distillation methods. This work represents a meaningful advancement in model compression by demonstrating that preserving relative behavioral shifts is more effective than mimicking absolute distributions.

---

### üîë Key Findings

*   The proposed **Delta Knowledge Distillation (Delta-KD)** method substantially improves the performance of the student model.
*   Delta-KD is more effective at **preserving the teacher model's knowledge** compared to prior approaches.
*   The efficacy of the method was confirmed through empirical results using **ROUGE metrics**.
*   Prior work relies on a **flawed assumption** that student and teacher models share the same optimal representation space, which frequently does not hold true.

---

### üß™ Methodology

The authors propose **Delta Knowledge Distillation (Delta-KD)**, a novel extension of standard token-level knowledge distillation. Rather than simply minimizing the KL divergence between output distributions‚Äîwhich assumes identical optimal representation spaces‚ÄîDelta-KD encourages the student to approximate an optimal representation space by explicitly preserving the distributional shift (referred to as **'Delta'**) that the teacher model acquired during its Supervised Finetuning (SFT) phase.

This approach shifts the focus from mimicking absolute probabilities to replicating the relative changes in behavior that occur during the fine-tuning process.

---

### ‚öôÔ∏è Technical Details

The Delta-KD method focuses on **Behavioral Shift Alignment**, where the student model replicates the teacher's behavioral shift from pretraining to fine-tuning rather than mimicking absolute output distributions.

**Core Components:**

*   **Shift Operator:** Defined as $\Delta(p1, p2) = p1/p2$.
*   **Target Distribution:**
    $$ \pi^*_s(y_t|x) = \frac{1}{Z(x, y)} \cdot \pi^{ft}_t(y_t|x) \cdot \left( \frac{\pi^{raw}_s(y_t|x)}{\pi^{raw}_t(y_t|x)} \right)^\alpha $$
*   **Loss Function:**
    $$ L = \lambda L_{sft} + (1-\lambda)L_{delta-KD} $$
    *   Minimizes the KL divergence between the student and the synthetic target.
    *   Combined with standard SFT loss.

**Hyperparameters:**
*   **Temperature ($\tau$)**
*   **Alpha ($\alpha$)**
*   **Lambda ($\lambda$)**

**Architecture:**
*   Utilizes **decoupled inference**, deploying teacher models as standalone services to manage memory constraints effectively.

---

### üí° Contributions

*   Identified a **critical theoretical limitation** in existing token-level KD approaches regarding the assumption of shared optimal representation spaces between teacher and student models.
*   Introduced **Delta-KD**, a new distillation framework designed to align student models with the teacher's representation space by leveraging the specific changes (shifts) learned during SFT.
*   Provided a **more effective method** for compressing Large Language Models that outperforms traditional KL divergence-based distillation in terms of both performance metrics and knowledge preservation.

---

### üìà Results

Experiments were conducted using **Qwen-2.5-7B** (Teacher) and **Qwen-2.5-1.5B** (Student) on Ultrachat-200k and OpenMathReasoning datasets. Delta-KD consistently outperformed baselines such as SFT, FKL, RKL, SeqKD, and MiniLLM.

**Performance Metrics:**

| Dataset | ROUGE-1 | ROUGE-2 | ROUGE-L |
| :--- | :--- | :--- | :--- |
| **Ultrachat-200k** | **0.5407** | **0.2599** | **0.3425** |
| **OpenMathReasoning** | **0.5125** | **0.2664** | **0.2465** |

The results demonstrate significant gains in capturing bi-gram patterns and long-form coherence.

---

### üìö References
*   2 citations included in analysis.