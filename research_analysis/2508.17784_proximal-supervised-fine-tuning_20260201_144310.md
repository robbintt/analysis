# Proximal Supervised Fine-Tuning

*Wenhong Zhu; Ruobing Xie; Rui Wang; Xingwu Sun; Di Wang; Pengfei Liu*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **OOD Improvement** | +12% accuracy increase over standard SFT baseline |
| **Training Stability** | Stable for up to 4 epochs (vs. 1 epoch for SFT) |
| **Quality Score** | 8/10 |
| **Citations** | 12 References |
| **Key Benchmarks** | GSM8K (Mathematics), TruthfulQA (Human Value Alignment) |

---

## Executive Summary

> Standard Supervised Fine-Tuning (SFT), the dominant paradigm for adapting Large Language Models (LLMs), suffers from critical instability and generalization limitations. During training, models frequently experience "entropy collapse," a phenomenon where output diversity diminishes, leading to overfitting and the deterioration of pre-existing capabilities known as policy drift. This fragility compromises model robustness and results in poor performance on Out-of-Distribution (OOD) data. Because SFT degrades the model's exploration capacity, it creates a weak foundation for subsequent alignment stages, rendering downstream Reinforcement Learning algorithms like PPO less effective.
>
> The authors introduce **Proximal Supervised Fine-Tuning (PSFT)**, a method that theoretically reframes SFT as a specific instance of Reinforcement Learning policy gradient methods with fixed positive advantages. Drawing on Trust-Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), PSFT integrates a trust-region constraint directly into the SFT objective. Technically, this is achieved by adapting a clipped surrogate objective that restricts policy updates to a trust region defined by the probability ratio between new and reference policies. The method employs a dynamic reference policy that evolves during training, ensuring the model adapts to the target task without sacrificing its prior capabilities or stability.
>
> Evaluations on the **GSM8K** mathematics and **TruthfulQA** human value alignment benchmarks demonstrate that PSFT matches standard SFT on In-Distribution data while dramatically improving OOD generalization. Specifically, PSFT improved OOD accuracy by **12%** compared to the standard SFT baseline. Crucially, the method exhibits quantifiable robustness; while standard SFT models typically suffer entropy collapse and performance degradation after just **one epoch**, PSFT maintains stable entropy and high performance across extended training durations of **up to four epochs**. This stability ensures that PSFT-tuned models act as superior foundations for post-training, achieving better results when subsequently optimized with RL algorithms like PPO and GRPO.
>
> This paper establishes a vital theoretical bridge between supervised fine-tuning and RL policy optimization, offering a mathematically grounded solution to the fragility of current fine-tuning paradigms. By resolving the trade-off between adaptation and stability, PSFT provides a path to more robust foundation models that retain diverse capabilities post-tuning.

---

## Key Findings

*   **Superior OOD Performance:** PSFT significantly outperforms standard Supervised Fine-Tuning (SFT) in out-of-domain scenarios while maintaining equivalent performance in-domain.
*   **Training Stability:** The method exhibits stability during prolonged training periods, effectively avoiding entropy collapse.
*   **Post-training Foundation:** Models tuned with PSFT provide a stronger foundation for subsequent post-training optimization stages compared to those tuned with standard SFT.
*   **Policy Preservation:** The trust-region approach effectively constrains policy drift, preventing the deterioration of prior capabilities typically seen during fine-tuning.

---

## Methodology

The authors propose a novel theoretical framework for fine-tuning by bridging the gap between Supervised Learning and Reinforcement Learning:

1.  **Reframing SFT:** The authors reframe Supervised Fine-Tuning (SFT) as a specific instance of policy gradient methods characterized by constant positive advantages.
2.  **RL Integration:** Drawing from Reinforcement Learning (RL), specifically Trust-Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), the authors introduce a trust-region objective into the fine-tuning process.
3.  **PSFT Algorithm:** Proximal SFT (PSFT) incorporates a trust-region constraint that limits the extent of policy updates during tuning. This constraint balances the need for adaptation to new data with the requirement for stability.

---

## Technical Details

### Formulation & Objective
*   **Language Modeling as MDP:** The paper formulates language modeling as a Markov Decision Process (MDP) defined by a state space (partial sequences) and an action space (next tokens).
*   **SFT as Policy Gradient:** Supervised Fine-Tuning is framed as a specialized Policy Gradient case with fixed advantage and offline dataset sampling.

### Core Mechanism: PSFT
*   **Clipped Surrogate Objective:** PSFT adapts the clipped surrogate objective from Proximal Policy Optimization (PPO) to SFT.
*   **Trust Region Constraint:** The method constrains policy updates to a trust region around a reference policy ($\pi_{\theta_old}$).
*   **Loss Function:** The loss function maximizes the clipped probability ratio within a range defined by epsilon ($\epsilon$).

### Optimization Strategy
*   **Dynamic Reference Policy:** Optimization features a dynamic reference policy that evolves during training.
*   **Warm-up Phase:** Includes a warm-up phase using standard SFT to reduce initial sampling bias.

---

## Results

### Performance
*   **In-Domain:** PSFT achieves performance equivalent to standard SFT on In-Domain data.
*   **Out-of-Distribution (OOD):** Significantly outperforms standard SFT on OOD generalization tasks.

### Robustness Analysis
*   **Entropy Stability:** Effectively mitigates entropy collapse; whereas standard SFT often deteriorates, PSFT maintains stability during prolonged training.
*   **Epoch Longevity:** Standard SFT models degrade after just one epoch; PSFT maintains high performance for up to four epochs.

### Downstream Impact
*   **RL Readiness:** Models tuned with PSFT provide a stronger foundation for subsequent Reinforcement Learning (e.g., PPO, GRPO) by preserving exploration capacity.

---

## Major Contributions

1.  **New Objective Function:** Introduction of Proximal SFT (PSFT), which integrates trust-region mechanisms into supervised fine-tuning to resolve generalization issues.
2.  **Theoretical Foundation:** Derivation of a theoretical link between SFT and RL policy gradient methods, providing a mathematical basis for stabilization.
3.  **Problem Resolution:** Resolution of tuning instability, poor generalization, and prior capability deterioration in foundation models by preserving model entropy and stability.

---

**Paper Quality Score:** 8/10  
**References:** 12 citations