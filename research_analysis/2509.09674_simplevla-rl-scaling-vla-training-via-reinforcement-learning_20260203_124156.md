---
title: 'SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning'
arxiv_id: '2509.09674'
source_url: https://arxiv.org/abs/2509.09674
generated_at: '2026-02-03T12:41:56'
quality_score: 9
citation_count: 14
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning

*Haozhan Li; Yuxin Zuo; Jiale Yu; Yuhao Zhang; Zhaohui Yang; Kaiyan Zhang; Xuekai Zhu; Yuchen Zhang; Tianxing Chen; Ganqu Cui; Dehui Wang; Dingxiang Luo; Yuchen Fan; Youbang Sun; Jia Zeng; Jiangmiao Pang; Shanghang Zhang; Yu Wang; Yao Mu; Bowen Zhou; Ning Ding*

---

> ### ðŸ“Š Quick Facts
>
> *   **Framework:** Built upon the `veRL` library
> *   **RoboTwin 2.0 Success Rate:** Up to **99.1%**
> *   **Data Efficiency (Single Trajectory):** **91.7%** (vs 17.3% SFT)
> *   **Real-World Success:** **60â€“70%** (vs ~4% baseline)
> *   **Emergent Behavior:** Discovered novel 'pushcut' skill
> *   **Primary Advantage:** Reduces dependency on large-scale human trajectories

---

## Executive Summary

**Problem**
Current Vision-Language-Action (VLA) models rely heavily on Supervised Fine-Tuning (SFT) using large-scale human demonstration datasets. This dependency creates critical bottlenecks for robotics, including the high cost of data collection, data scarcity, and poor generalization when facing distribution shifts or novel environments. Furthermore, SFT approaches often struggle with long-horizon action planning and complex manipulation tasks, limiting the adaptability and autonomy of robotic agents in real-world scenarios.

**Innovation**
To overcome these limitations, the authors introduce SimpleVLA-RL, a scalable reinforcement learning framework built upon the `veRL` library and tailored for VLA architectures. The core innovation lies in shifting from imitation-based SFT to RL, utilizing VLA-specific trajectory sampling, multi-environment rendering, and optimized loss computation to enable efficient training at scale. The framework integrates exploration-enhancing strategies and is applied to models like OpenVLA-OFT, allowing them to learn from environmental interaction rather than static data. This approach optimizes the model for long-horizon planning and generalization across spatial, object, and goal dimensions.

**Results**
SimpleVLA-RL demonstrates state-of-the-art performance, significantly outperforming the RDT model on RoboTwin benchmarks and achieving superior results on the LIBERO benchmark. It achieved a **70.4%** success rate on RoboTwin 1.0 and up to **99.1%** on RoboTwin 2.0, surpassing standard baselines which ranged from 38.3% to 48.9%. The framework exhibits exceptional data efficiency; with only a single trajectory, it achieved a **91.7%** success rate, outperforming SFT-only (17.3%) and even SFT trained on full datasets (86.5%). In real-world evaluations, SimpleVLA-RL maintained success rates of approximately 60â€“70%, whereas baselines dropped as low as 4%. Additionally, the RL training process unlocked an emergent behavior known as 'pushcut'â€”a novel manipulation skill not present in the original training data.

**Impact**
This research bridges the gap between Reinforcement Learning and VLA models, validating RL as a superior alternative to SFT for robotic manipulation. By providing a highly scalable infrastructure that reduces dependency on human trajectories, the work addresses fundamental challenges of data scarcity and generalization in the field. The emergence of novel skills like 'pushcut' highlights the potential for RL-driven models to discover creative solutions beyond human demonstration, paving the way for more robust, autonomous, and general-purpose robotic systems.

---

## Key Findings

*   **State-of-the-Art Performance:** Achieves SOTA results on the LIBERO benchmark and outperforms the $RT_0$ model on RoboTwin benchmarks.
*   **Superiority over SFT:** The RL approach surpasses Supervised Fine-Tuning (SFT) in real-world tasks and enables robust generalization under distribution shifts.
*   **Data Efficiency:** Significantly reduces the dependency on large-scale human trajectories while maintaining high performance.
*   **Emergent Behavior:** Discovered a novel manipulation skill termed **'pushcut'**, which was not present in the original training data.
*   **Benchmark Dominance:** Achieved up to 99.1% success rate on RoboTwin 2.0, compared to baseline ranges of 38.3%â€“48.9%.

---

## Methodology

The SimpleVLA-RL framework is designed to efficiently scale Reinforcement Learning for Vision-Language-Action models. The technical implementation includes:

*   **Core Infrastructure:** Built upon the `veRL` library, specifically tailored for VLA architectures.
*   **Training Optimizations:**
    *   VLA-specific trajectory sampling.
    *   Scalable parallelization strategies.
    *   Multi-environment rendering capabilities.
    *   Optimized loss computation.
*   **Exploration Strategy:** Integration of exploration-enhancing strategies to handle complex benchmarks.
*   **Model Application:** The framework was applied to OpenVLA-OFT to successfully transition the model from supervised learning to reinforcement learning.

---

## Technical Details

SimpleVLA-RL is an efficient Reinforcement Learning framework designed to address the limitations of traditional Supervised Fine-Tuning (SFT).

| Feature | Description |
| :--- | :--- |
| **Core Approach** | Utilizes RL to train and scale VLA models instead of relying on large-scale human trajectories (SFT). |
| **Optimization Focus** | Long-horizon action planning. |
| **Data Efficiency** | High performance maintained even under conditions of data scarcity. |
| **Generalization** | Multi-dimensional capabilities across **Spatial**, **Object**, and **Goal** dimensions. |
| **Emergent Skill** | **'Pushcut'**: A novel skill discovered during the RL training process, absent from initial data. |

---

## Performance Results

SimpleVLA-RL demonstrated significant performance improvements over SFT-only and RDT baselines across various testing environments.

### Benchmark Success Rates
*   **RoboTwin 1.0:** **70.4%** (Baselines: ~38.3%â€“48.9%)
*   **RoboTwin 2.0:** Up to **99.1%** (Baselines: ~38.3%â€“48.9%)

### Data Efficiency Tests
*   **SimpleVLA-RL (1 Trajectory):** **91.7%**
*   **SFT (Full Trajectories):** 86.5%
*   **SFT-Only:** 17.3%

### Real-World Evaluations
*   **SimpleVLA-RL:** Approximately **60â€“70%**
*   **Baselines:** As low as **4%**

### Generalization
The model exhibited strong generalization on unseen tasks (Grasping, Pushing, Moving objects) where SFT models failed.

---

## Contributions

1.  **Bridging Fields:** Successfully bridges Reinforcement Learning and Vision-Language-Action models, translating RL benefits to robotic long-horizon action planning.
2.  **Infrastructure:** Provides a highly scalable RL infrastructure specifically designed for training VLA models.
3.  **Validation:** Validates RL as a superior alternative to SFT for robotic manipulation.
4.  **Problem Solving:** Addresses critical challenges in data scarcity and generalization within the robotics domain.

---

**Quality Score:** 9/10
**References:** 14 citations