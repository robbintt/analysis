---
title: 'Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks'
arxiv_id: '2505.24876'
source_url: https://arxiv.org/abs/2505.24876
generated_at: '2026-02-03T12:35:20'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks

*Tajamul Ashraf; Amal Saqib; Hanan Ghani; Muhra AlMahri; Yuhao Li; Noor Ahsan; Umair Nawaz; Jean Lahoud; Hisham Cholakkal; Mubarak Shah; Philip Torr; Fahad Shahbaz Khan; Rao Muhammad Anwer; Salman Khan*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Benchmark Size** | 828 Manually Verified Tasks |
| **Task Environments** | 6 (Web, Security, Driving, Sports, etc.) |
| **Tool Ecosystem** | 14 Distinct Tools |
| **SOTA Performance** | < 50% Full-Chain Success Rate |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |

---

## Executive Summary

Current Large Multimodal Models (LMMs) have demonstrated proficiency in single-turn, static visual tasks, yet their capability to perform complex, multi-step agentic reasoning remains largely unproven. The research addresses a critical gap in the evaluation of vision-centric agents, highlighting that existing benchmarks are inadequate for assessing true agentic potential. These prior methods rely heavily on synthetic data, single-turn queries, and limited visual modalities, failing to capture the complexity of real-world scenarios where agents must perform sequential reasoning, utilize tools, and adapt to dynamic environments. This discrepancy makes it difficult to gauge the actual readiness of LMMs for deployment in autonomous systems, necessitating a more rigorous evaluation framework that mirrors authentic, multi-modal decision-making processes.

The core innovation of this work is the introduction of **Agent-X**, a large-scale benchmark specifically designed to evaluate deep, multimodal reasoning in agentic tasks. Technically, Agent-X employs a formal task structure $S_i$ comprising six distinct components: Multimodal Context ($V_i$), Query ($Q_i$), Tool Subset ($T_i$), Reasoning Trace ($R_i$), Final Answer ($A_i$), and Justification ($J_i$). The benchmark encompasses 828 manually verified tasks across six real-world environmentsâ€”ranging from web browsing and autonomous driving to security surveillanceâ€”and supports single-image, multi-image, and video inputs. Furthermore, the authors introduce a novel, fine-grained evaluation protocol that moves beyond simple final-score metrics to assess step-level correctness, logical coherence, and the efficacy of tool integration within a defined ecosystem of 14 distinct tools (perception, visual operation, math, and artistic).

Evaluation results reveal a significant performance deficit among state-of-the-art proprietary and open-source LMMs, including GPT, Gemini, and Qwen families. Despite their advanced capabilities, these models achieved a **full-chain success rate of less than 50%** on the Agent-X benchmark. The analysis identifies specific bottlenecks: models struggle primarily with deep reasoning chains and the effective integration of sequential tool use, rather than mere visual perception. In terms of scale and complexity, Agent-X surpasses existing benchmarks like GAIA (466 tasks) and GTA (229 tasks), providing a substantially more challenging dataset that exposes the limitations of current architectures in handling interpretive and generative tasks within authentic visual contexts.

This research significantly influences the field by establishing a new standard for evaluating vision-centric agents, shifting the focus from static question-answering to dynamic, process-oriented assessment. By identifying specific weaknesses in deep reasoning and tool use, the paper provides a clear roadmap for future research, urging the community to move beyond optimizing for single-turn responses. The public release of the Agent-X data and code facilitates reproducibility and enables the development of next-generation models capable of true agentic behavior. Ultimately, Agent-X serves as a critical reality check, demonstrating that while LMMs are progressing, substantial advancements are required to achieve reliable performance in complex, real-world applications.

---

## Key Findings

*   **Significant Performance Gap:** Even state-of-the-art Large Multimodal Models (LMMs), including GPT, Gemini, and Qwen families, struggle with complex, multi-step vision tasks, achieving less than **50%** full-chain success on the benchmark.
*   **Reasoning and Tool-Use Bottlenecks:** Current models face critical limitations in deep reasoning capabilities and the effective integration of tool use within sequential, real-world scenarios.
*   **Inadequacy of Existing Benchmarks:** Current evaluation methods are insufficient because they rely on synthetic data, single-turn queries, and limited visual modalities, failing to assess the multi-step reasoning required in authentic agentic environments.

---

## Methodology

The research methodology follows a rigorous three-pronged approach to constructing the Agent-X benchmark:

*   **Benchmark Construction:** The researchers developed 'Agent-X,' a large-scale benchmark comprising **828 agentic tasks** utilizing authentic visual contexts such as images, multi-image comparisons, videos, and instructional text.
*   **Task Categorization:** The tasks span six distinct real-world environments:
    *   General visual reasoning
    *   Web browsing
    *   Security and surveillance
    *   Autonomous driving
    *   Sports
    *   Math reasoning
*   **Evaluation Protocol:** A fine-grained, step-level evaluation framework was introduced to assess the process, rather than just the final output. This framework specifically measures:
    *   Correctness of reasoning steps
    *   Logical coherence
    *   Effectiveness of tool usage throughout the decision-making chain

---

## Technical Details

### Formal Task Structure
The Agent-X benchmark evaluates vision-centric agents using a formal task structure $S_i$ comprising six components:

| Component | Symbol | Description |
| :--- | :---: | :--- |
| Multimodal Context | $V_i$ | Visual input (images/videos) |
| Query | $Q_i$ | The specific task or question |
| Tool Subset | $T_i$ | Specific tools available for the task |
| Reasoning Trace | $R_i$ | The step-by-step logic process |
| Final Answer | $A_i$ | The concluding output |
| Justification | $J_i$ | The rationale behind the answer |

### Construction & Tools
*   **Pipeline:** Utilizes a semi-automated construction pipeline involving LMM generation and human verification.
*   **Task Types:** Classified into Factual, Interpretive, and Generative.
*   **Tool Ecosystem:** Includes 14 distinct tools categorized into:
    1.  Perception
    2.  Visual Operation
    3.  Math
    4.  Artistic Tasks

---

## Contributions

*   **Agent-X Benchmark:** Introduction of a comprehensive, large-scale benchmark designed specifically for evaluating deep, multi-step reasoning in vision-centric agents using real-world, multimodal data.
*   **Advanced Evaluation Framework:** A novel evaluation methodology that moves beyond final-score metrics to provide granular analysis of step-level reasoning, logical coherence, and tool integration efficacy.
*   **Critical Analysis of SOTA Models:** A systematic evaluation of leading proprietary and open-source models that identifies specific weaknesses in current LMM capabilities, providing a clear direction for future research in agentic reasoning models.
*   **Resource Availability:** The public release of the data and code to facilitate further research and reproducibility in the field.

---

## Results

*   **Scale Comparison:** Agent-X comprises 828 manually verified agentic tasks, surpassing existing benchmarks like **GAIA** (466) and **GTA** (229) in scale and complexity.
*   **Modality Support:** Supports single-image, multi-image, and video inputs.
*   **Performance Bottlenecks:** State-of-the-art LMMs (GPT, Gemini, Qwen) achieve a full-chain success rate of less than 50%. The main bottlenecks are identified as deep reasoning and sequential tool integration.
*   **Key Differentiators:** Distinguished by supporting hybrid annotation and executable tools across:
    *   Real-world Queries
    *   Multimodal Inputs
    *   Deep Reasoning
    *   Executable Tools