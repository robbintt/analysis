---
title: 'AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language
  Models'
arxiv_id: '2510.02669'
source_url: https://arxiv.org/abs/2510.02669
generated_at: '2026-02-03T12:45:11'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models

*Bo Ma; Hang Li; ZeHua Hu; XiaoFan Gui; LuYao Liu; Simon Liu*

---

### ðŸ“Š Quick Facts

| **Metric** | **Value** | **Context** |
| :--- | :--- | :--- |
| **Performance Gain** | +1.0% to +7.1% | Compared to state-of-the-art methods |
| **Cost Reduction** | 3% to 5% | Relative cost of 58% (vs. 100% baseline) |
| **Benchmarks** | 6 | GSM8K, MATH, HumanEval, MBPP, MultiArith, GAIA |
| **Operator Fusion Success** | 75% | Resulting in 12 fused operators |
| **Quality Score** | 9/10 | Based on analysis review |
| **Citations** | 40 | Reference count |

---

## Executive Summary

This research addresses the inherent inefficiency and rigidity of existing Large Language Model (LLM) systems that rely on static, monolithic architectures or manually crafted multi-agent configurations. Current approaches often utilize a "one-size-fits-all" strategy, applying uniform resource allocation regardless of query complexity or domain requirements. This limitation leads to suboptimal performance and inflated inference costs, as systems lack the agility to scale computational effort up or down based on the specific demands of a task. As LLMs are deployed in increasingly diverse and complex environments, the need for architectures that can dynamically balance accuracy with computational efficiency has become a critical challenge for the field.

The authors introduce **AutoMaAS**, a novel framework that integrates Neural Architecture Search (NAS) principles with Multi-Agent Systems (MAS) to create a self-evolving architecture for LLMs. Technically, the system employs Dynamic Operator Lifecycle Management to automatically generate, fuse, and eliminate agents (operators) based on real-time performance-cost analysis. This is supported by a multi-objective optimization function that simultaneously targets accuracy and inference cost, adjusting parameters dynamically in response to input query complexity. Furthermore, the framework utilizes online feedback integration and decision tracing mechanisms to continuously refine the architecture, ensuring the system evolves autonomously rather than relying on static, pre-defined configurations.

In evaluations across six benchmarksâ€”GSM8K, MATH, HumanEval, MBPP, MultiArith, and GAIAâ€”AutoMaAS achieved performance improvements of **1.0% to 7.1%** compared to state-of-the-art methods. Specific high scores included 98.8% on MultiArith, 97.2% on HumanEval, and 95.4% on GSM8K. Crucially, the framework realized a relative cost of **58%** compared to a Single-Agent Chain of Thought baseline (100%), representing a **3% to 5% reduction** in inference costs over previous automated design methods. Ablation studies confirmed the importance of the dynamic lifecycle and multi-objective optimization, with operator fusion achieving a 75% success rate and operator modification a 67% success rate.

This research establishes a new standard for automated multi-agent system design by validating that self-evolving architectures can outperform static, hand-crafted models. By demonstrating that continuous online feedback and dynamic resource allocation are viable for LLM deployment, AutoMaAS provides a blueprint for building cost-efficient, high-performance AI agents. The frameworkâ€™s proven generalizability across various datasets and LLM backbones (including Claude-3.5-Sonnet and GPT-4) suggests a significant shift away from manual architecture tuning toward automated, adaptive systems capable of maintaining high accuracy while strictly managing computational resources.

---

## Key Findings

*   **Performance Efficiency:** The AutoMaAS framework achieved a performance improvement of **1.0% to 7.1%** while simultaneously reducing inference costs by **3% to 5%** compared to state-of-the-art methods.
*   **Dynamic Adaptation:** Unlike existing monolithic approaches, AutoMaAS successfully adapts resource allocation based on query complexity and domain requirements, avoiding rigid, one-size-fits-all solutions.
*   **Generalizability:** The framework demonstrates superior transferability across six different benchmarks and remains effective when applied to various datasets and LLM backbones.
*   **Validation of Approach:** The integration of neural architecture search principles with multi-agent systems proves effective for continuous architecture refinement through online feedback.

---

## Methodology

The researchers developed AutoMaAS, a self-evolving framework that utilizes Neural Architecture Search (NAS) principles and Automated Machine Learning (AutoML) to discover optimal multi-agent configurations. The methodology centers on dynamic operator lifecycle management and includes four specific technical components:

1.  **Automatic Operator Management:** Performance-cost analysis drives the generation, fusion, and elimination of operators.
2.  **Cost-aware Optimization:** Dynamic optimization is achieved through real-time parameter adjustment to manage resources effectively.
3.  **Continuous Refinement:** The architecture is refined continuously via online feedback integration.
4.  **Interpretability Mechanisms:** Decision tracing is implemented to provide transparency into the system's operations.

---

## Technical Details

AutoMaAS integrates Neural Architecture Search (NAS) with Multi-Agent Systems (MAS) to create a self-evolving architecture for Large Language Models.

### Core Innovations
*   **Dynamic Operator Lifecycle Management:** Manages active operators and eliminates inefficient ones.
*   **Multi-Objective Cost Optimization:** Optimizes for accuracy and inference cost simultaneously.
*   **Online Feedback Integration:** Enables the system to learn and adapt from ongoing interactions.
*   **Enhanced Interpretability Mechanisms:** Provides transparency into decision-making processes.

### Operational Mechanisms
*   **Operator Evolution:** Involves the Fusion, Modification, and Elimination of operators.
*   **Priority Adaptation:** Adjusts system behavior based on input query needs.

---

## Contributions

*   **Novel Framework:** Introduction of AutoMaAS, the first self-evolving multi-agent architecture search framework specifically designed to address the limitations of monolithic automated design in LLM-powered systems.
*   **Resource-Responsive Design:** A shift from static agent configurations to dynamic architectures that can intelligently allocate resources based on the specific complexity of a query and domain needs.
*   **Methodological Innovations:** Contribution of four specific mechanismsâ€”automatic operator lifecycle management, dynamic cost-aware optimization, online feedback loops, and decision tracingâ€”that collectively enable self-evolving agent systems.
*   **New Paradigm Establishment:** Establishment of a new standard for automated multi-agent system design that balances performance gains with cost efficiency across diverse LLM architectures.

---

## Results

AutoMaAS outperformed Single-Agent, Hand-crafted Multi-Agent, and other Automated Design methods across all benchmarks.

### Benchmark Scores
| Benchmark | Score |
| :--- | :--- |
| **MultiArith** | 98.8% |
| **HumanEval** | 97.2% |
| **GSM8K** | 95.4% |
| **MBPP** | 88.8% |
| **MATH** | 57.1% |
| **GAIA** | 20.7% |

### Efficiency & Analysis
*   **Cost Efficiency:** Achieved a relative cost of **58%** compared to Single-Agent CoT (100% baseline).
*   **Ablation Studies:** Highlighted Dynamic Lifecycle, Fixed Operator Pool evolution, and Multi-Objective Optimization as critical components.
*   **Operator Success Rates:**
    *   **Fused Operators:** 12 created (75% success rate).
    *   **Modified Operators:** 15 created (67% success rate).
*   **Generalizability:** Demonstrated strong performance across datasets and LLMs (Claude-3.5-Sonnet, GPT-4).

---

**Analysis Quality Score:** 9/10 | **References:** 40 citations