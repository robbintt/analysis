# BERT-VQA: Visual Question Answering on Plots
*Tai Vu; Robert Yang*

---

## üìã Executive Summary

This research investigates the applicability of state-of-the-art Visual Question Answering (VQA) techniques, specifically cross-modality alignment mechanisms, to structured scientific plots. The core hypothesis was that natural image methods might be over-engineered for this specific domain.

To test this, the authors developed **BERT-VQA** and a "Joint Fusion" variant to benchmark against a traditional LSTM/ResNet baseline using the PlotQA dataset. The evaluation revealed a surprising result: there was no significant performance advantage for the complex transformer-based models. While BERT-VQA achieved roughly **56.6% accuracy**, the baseline achieved **56.37%**. These findings demonstrate that computationally expensive alignment mechanisms are not essential for interpreting structured plots, suggesting that efficient architectures tailored to scientific visualizations may outperform blindly applied natural image models.

---

## üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Total Dataset Size** | 175,650 questions |
| **Task Type** | Binary Classification ('Yes'/'No') |
| **Class Balance** | Perfectly Balanced |
| **Top Accuracy (BERT-VQA)** | ~56.6% |
| **Baseline Accuracy** | ~56.37% |
| **Primary Architecture** | VisualBERT / ResNet 101 / LSTM |

---

## üîç Key Findings

*   **Hypothesis Disproven:** The core hypothesis that the cross-modality module in VisualBERT is essential for aligning plot components with question phrases was **disproved**.
*   **Architecture Variation:** There is a notable variation in the appropriateness of different model architectures. Specifically, the cross-modality alignment found in general VisualBERT models does not translate as effectively to the structured domain of plots as initially expected.
*   **Domain Specificity:** The study highlights that the "Plot Question Answering" subtask presents specific difficulties that general natural image VQA models do not automatically resolve.
*   **Complexity vs. Necessity:** The results offer counter-intuitive insights, suggesting that complex cross-modality alignment modules may not be necessary for structured data like plots.

---

## üõ†Ô∏è Methodology

The study involved the development and comparison of three distinct architectures to evaluate performance on the PlotQA dataset:

1.  **BERT-VQA (Primary Model):** Based on the VisualBERT architecture, utilizing a pretrained ResNet 101 as the image encoder. It explores the potential of vision-language transformers.
2.  **Joint Fusion Variant:** An extension of BERT-VQA that adds a parallel global CNN features path (ResNet 101) concatenated with VisualBERT outputs. This aims to capture global trends that might be missed by region-based processing alone.
3.  **Baseline Architecture:** A traditional sequential model consisting of an LSTM (for language processing), a CNN (for vision), and a shallow classifier, utilizing late fusion for predictions.

---

## üí° Contributions

*   **Model Introduction:** Introduced BERT-VQA, a specific application of VisualBERT tailored to the subtask of Visual Question Answering (VQA) on scientific plots.
*   **Comparative Analysis:** Provided a direct comparative analysis between a transformer-based multimodal approach (VisualBERT) and a traditional sequential baseline (LSTM/CNN) within the context of scientific plots.
*   **Theoretical Insights:** Offered valuable counter-intuitive insights regarding the necessity of complex cross-modality alignment modules for plot data, refining the understanding of how vision and language domains interact on structured images.

---

## üìù Technical Details

### Dataset Configuration
*   **Source:** Sub-set of the PlotQA dataset.
*   **Splits:**
    *   **Train:** 87,962 questions
    *   **Validation:** 43,770 questions
    *   **Test:** 43,918 questions
*   **Distribution:**
    *   **Structure:** 42.8%
    *   **Reasoning:** 40.7%
    *   **Data Retrieval:** 16.5%

### Architectural Comparison
| Architecture | Components | Strategy |
| :--- | :--- | :--- |
| **Baseline** | LSTM, ResNet 101, GloVe embeddings | Late fusion of sequential features |
| **Modified VisualBERT** | Faster R-CNN, ResNet 101, VisualBERT Encoder | Region embeddings passed through transformer |
| **Joint Fusion** | ResNet 101 (Global), VisualBERT (Regional) | Concatenation of global CNN path with VisualBERT outputs |

---

## üìà Results

The experimental results supported the conclusion that general alignment techniques are less effective on structured plots than on natural images.

*   **Performance Parity:** The complex BERT-VQA models did not significantly outperform the LSTM/CNN baseline. The difference in accuracy was negligible (~0.2%).
*   **Metric Exclusions:** Specific granular performance metrics (beyond accuracy) were excluded from the source text.
*   **Data Balance:** The experimental setup confirmed a perfectly balanced class distribution, ensuring that accuracy scores were not skewed by class dominance.

---

***Quality Score:** 7/10 | **References:** 7 citations*