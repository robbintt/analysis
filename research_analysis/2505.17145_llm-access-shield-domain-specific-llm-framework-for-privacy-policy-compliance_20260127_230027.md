---
title: 'LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance'
arxiv_id: '2505.17145'
source_url: https://arxiv.org/abs/2505.17145
generated_at: '2026-01-27T23:00:27'
quality_score: 5
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance

*Access Shield, Zhihua Xiao, Peifung E. Lam, Cailing Cai, Policy Compliance, Technology Research, Applied Science, Hong Kong, Yu Wang*

***

> ### üìä Quick Facts
>
> *   **Dataset Size:** 3,853 messages
> *   **Training Split:** 2,311 messages (60%)
> *   **Testing Split:** 1,542 messages (40%)
> *   ** Unsafe Prompts:** 71% of total data
> *   **Base Model:** Llama-3.2-3B-Instruct
> *   **PII Categories:** 6 Types (Email, ID, Phone, Fax, Bank, Monetary)
> *   **Encryption Method:** Format-Preserving Encryption (FPE)

***

## Executive Summary

### üõ°Ô∏è The Problem
The integration of Large Language Models (LLMs) into enterprise workflows creates a critical vector for Personally Identifiable Information (PII) leakage through user prompts. This paper addresses the immediate challenge of balancing the operational benefits of generative AI with the stringent requirements of data privacy regulations. Without effective intervention, sensitive user data submitted to LLMs risks exposure, creating a need for a robust mechanism that sanitizes inputs to ensure compliance before data processing occurs.

### üí° The Innovation
The authors propose **"LLM Access Shield,"** a modular framework designed to intercept and sanitize user inputs via a six-step pipeline. The system's core technical innovation is the combination of a fine-tuned classification model with **Format-Preserving Encryption (FPE)**. Utilizing a Llama-3.2-3B-Instruct model refined via Supervised Fine-Tuning (SFT), the framework classifies prompts as "Safe" or "Unsafe." For sensitive data, FPE is applied to fields such as emails, bank account numbers, and monetary values. Unlike standard redaction, FPE encrypts this data while preserving its semantic structure and formatting, allowing the LLM to process context and validate formats without accessing the raw PII.

### üìà The Results
The framework was evaluated using a dataset of 3,853 messages. The evaluation revealed a high prevalence of privacy risks, with **71% of messages** labeled "Unsafe," of which approximately 13-14% were multi-labeled. The frequency analysis identified specific PII types requiring detection:
*   **Email addresses:** 24.90%
*   **Monetary Values:** 18.74%
*   **Phone Numbers:** 11.61%

### üöÄ The Impact
This research advances AI safety by operationalizing privacy compliance through a customizable architecture that resolves the utility-privacy trade-off. By employing Format-Preserving Encryption, the LLM Access Shield enables the generation of coherent, contextually relevant outputs without exposing sensitive data. This modular design provides a practical structural solution for highly regulated industries, such as finance and healthcare, facilitating the secure adoption of generative AI tools while maintaining strict data governance standards.

***

## Key Findings

*   **High Risk Volume:** A vast majority (71%) of user prompts in the dataset were classified as "Unsafe," indicating a critical need for automated filtering in enterprise environments.
*   **Multi-Label Complexity:** Approximately 13-14% of unsafe messages contained multiple types of sensitive data, necessitating a detection system capable of identifying overlapping PII categories.
*   **Top Sensitivity Vectors:** Email addresses and monetary values constitute the highest risk categories, combining for nearly 44% of all detected sensitive data points.
*   **Compliance Architecture:** The proposed framework successfully integrates a privacy pipeline that maintains data utility (format validation) while ensuring strict confidentiality through encryption.

***

## Methodology

The methodology is defined by a specific **6-step modular workflow** designed to intercept, sanitize, and reconstruct data:

1.  **Prompt Submission:** The user submits a raw query to the system.
2.  **Prompt Analysis:** The DLMS (Data Leakage Management System) analyzes the prompt to classify it as 'Safe' or 'Unsafe'.
3.  **Sensitive Data Encryption:** If 'Unsafe', the SDA Module applies **Format-Preserving Encryption (FPE)** to the identified sensitive data. This preserves the semantic structure (e.g., length, character type) required for downstream processing.
4.  **LLM Processing:** The sanitized prompt is sent to the LLM for processing. The model can understand the context and format constraints without seeing the real data.
5.  **Response Generation:** The LLM generates a response based on the encrypted context.
6.  **Sensitive Data Decryption:** The system intercepts the response to decrypt the sensitive data, restoring the original values for the final user output.

***

## Technical Details

### Framework Architecture
The framework is built upon two primary modules:
*   **DLMS (Data Leakage Management System):** Responsible for the prompt analysis and classification phase.
*   **SDA Module:** Responsible for the Sensitive Data Anonymization (Encryption and Decryption).

### Model specifications
*   **Base Model:** Llama-3.2-3B-Instruct
*   **Training Method:** Supervised Fine-Tuning (SFT)
*   **Objective:** Binary classification (Safe vs. Unsafe) and multi-label detection of PII types.

### PI Taxonomy
The system identifies and categorizes sensitive data into the following customizable taxonomy:

*   **T1:** Email Address
*   **T2:** Personal ID
*   **T3:** Phone Number
*   **T4:** Fax Number
*   **T5:** Bank Account Number
*   **T6:** Monetary Values

### Encryption Technology
*   **Format-Preserving Encryption (FPE):** This specific encryption method is chosen over standard hashing or redaction to ensure that the LLM can still validate the *format* of the data (e.g., ensuring an email has an "@" symbol or a bank account has the correct digit count) without ever seeing the actual sensitive values.

***

## Results & Evaluation

### Dataset Composition
| Subset | Count | Percentage |
| :--- | :--- | :--- |
| **Total Messages** | 3,853 | 100% |
| **Training** | 2,311 | 60% |
| **Testing** | 1,542 | 40% |

### Class Distribution
The dataset is imbalanced, reflecting real-world scenarios where policy violations are frequent:
*   **Safe Prompts:** 29%
*   **Unsafe Prompts:** 71%

### Test Set:PII Frequency Distribution
The following table illustrates the prevalence of specific data types within the test set ( Unsafe / Multi-label):

| PII Category | Frequency in Test Set |
| :--- | :--- |
| **T1: Email Address** | 24.90% |
| **T6: Monetary Values** | 18.74% |
| **T3: Phone Number** | 11.61% |
| **T5: Bank Account** | 11.28% |
| **T2: Personal ID** | 10.44% |
| **T4: Fax Number** | 8.63% |

*Note: The provided text did not include specific performance metrics such as Accuracy, Precision, Recall, or F1-score.*

***

**Report Quality Score:** 5/10  
**References:** 40 citations