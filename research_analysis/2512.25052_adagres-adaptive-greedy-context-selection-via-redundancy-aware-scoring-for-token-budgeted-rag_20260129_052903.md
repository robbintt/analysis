# AdaGReS: Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG

*Chao Peng; Bin Wang; Zhilei Long; Jinfang Sheng*

---

> ### ðŸ“Š Quick Facts
> * **Framework Name:** AdaGReS
> * **Primary Focus:** Token-budgeted RAG optimization
> * **Core Mechanism:** Redundancy-aware greedy selection
> * **Theoretical Guarantee:** $\epsilon$-approximate submodularity
> * **Test Domains:** Open-domain QA, Biomedical corpora
> * **Quality Score:** 8/10
> * **References:** 8 citations

---

## Executive Summary

### **Problem**
Standard Retrieval-Augmented Generation (RAG) pipelines rely predominantly on top-k retrieval mechanisms that prioritize high similarity scores while ignoring semantic overlap. This approach frequently retrieves redundant or near-duplicate chunks, a critical inefficiency that wastes valuable token budget and introduces noise, thereby degrading downstream generation quality. As Large Language Models (LLMs) operate under strict context window limits, maximizing the utility of every token is essential.

### **Innovation**
The authors introduce **AdaGReS**, a redundancy-aware context selection framework designed to optimize RAG inputs under strict token constraints. The core innovation is a novel set-level objective function, $F(q, C)$, which maximizes query-chunk relevance while actively minimizing intra-set redundancy. Crucially, AdaGReS eliminates manual hyperparameter tuning through a closed-form, instance-adaptive calibration mechanism that dynamically adjusts the redundancy penalty ($\beta$) for each specific query.

### **Results**
Evaluations across open-domain question answering (ODQA) and high-redundancy biomedical corpora demonstrate that AdaGReS consistently outperforms standard top-k retrieval and static-weight Maximal Marginal Relevance (MMR) baselines. The framework achieves superior redundancy control, preserving token budget for distinct information, which translates to measurable improvements in end-to-end answer quality (EM and F1 scores).

### **Impact**
This research addresses a fundamental inefficiency in current RAG systems by shifting the focus from simple retrieval to intelligent, budget-aware context optimization. By automating the trade-off between relevance and redundancy, AdaGReS reduces operational burdens and provides theoretical guarantees on performance, paving the way for more reliable and cost-effective generative AI applications in specialized domains.

---

## Key Findings

*   **Redundancy in Standard RAG:** Standard top-k retrieval frequently retrieves redundant or near-duplicate chunks, consuming valuable token budget and degrading generation quality.
*   **Superior Context Quality:** The AdaGReS framework consistently improves redundancy control and context quality, leading to better end-to-end answer quality compared to baselines.
*   **Cross-Domain Robustness:** The method demonstrates robustness across diverse domains, specifically on open-domain question answering and high-redundancy biomedical corpora.
*   **Theoretical Guarantees:** The proposed objective function exhibits $\epsilon$-approximate submodularity, providing near-optimality guarantees for the greedy strategy.

---

## Methodology

The authors present **AdaGReS**, a redundancy-aware context selection framework for token-budgeted RAG. The methodology operates as follows:

1.  **Objective Optimization:** It optimizes a set-level objective that maximizes query-chunk relevance while simultaneously minimizing intra-set redundancy penalties.
2.  **Greedy Selection:** The framework performs greedy selection of chunks under a strict token-budget constraint based on marginal gains.
3.  **Adaptive Calibration:** It introduces instance-adaptive calibration via a closed-form solution to dynamically balance the relevance-redundancy trade-off, eliminating the need for manual hyperparameter tuning.

---

## Contributions

1.  **Closing the RAG Gap:** Addresses the gap in standard RAG pipelines by introducing a framework that explicitly penalizes redundancy to maximize token utility.
2.  **Automated Tuning:** Develops a novel, closed-form, instance-adaptive calibration mechanism that automates the balancing of relevance versus redundancy, removing the need for manual tuning.
3.  **Theoretical Rigor:** Provides a rigorous theoretical analysis establishing the $\epsilon$-approximate submodularity of the objective function to support greedy algorithm usage.
4.  **Practical Applicability:** Demonstrates practical applicability and superior performance on diverse datasets, including general domain and high-redundancy biomedical domains.

---

## Technical Details

**Framework Name:** AdaGReS (Adaptive Greedy Context Selection via Redundancy-Aware Scoring)

**Goal:** Optimize RAG context selection by balancing query relevance against intra-set redundancy under strict token budget constraints.

**Inputs:**
*   Query embedding ($q \in \mathbb{R}^d$)
*   Candidate chunk embeddings ($V$)
*   Token Budget ($T_{max}$)

**Scoring Function:**
$$F(q, C) = \alpha S_{qC}(q, C) - \beta S_{CC}(C)$$
*   Where $S_{qC}$ calculates relevance (sum of cosine similarities).
*   Where $S_{CC}$ calculates redundancy (sum of pairwise cosine similarities).

**Algorithm:**
*   Greedy selection maximizing marginal gain:
    $$\Delta F(x|C_{cur}) = \alpha \cdot \text{sim}(q, x) - \beta \sum_{c \in C_{cur}} \text{sim}(x, c)$$
*   Subject to token limits.
*   Includes instance-adaptive, closed-form calibration for $\beta$.

**Theoretical Guarantees:**
*   Exhibits $\epsilon$-approximate submodularity.
*   Allows the greedy algorithm to provably achieve near-optimal solutions.

---

## Results

*   **Redundancy Control:** AdaGReS shows improved redundancy control compared to standard top-k retrieval and baselines like static-weight MMR, significantly reducing wasted token budget.
*   **End-to-End Quality:** The method yields better end-to-end answer quality in generation tasks.
*   **Domain Robustness:** The framework demonstrates robustness across open-domain question answering (ODQA) and high-redundancy biomedical corpora.
*   **Validation:** Theoretical results confirm $\epsilon$-approximate submodularity and provide near-optimality guarantees for the greedy strategy.

---

**Quality Score:** 8/10 | **References:** 8 citations