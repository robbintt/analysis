# Adversarial Robustness in One-Stage Learning-to-Defer

*Yannis Montreuil; Letian Yu; Axel Carlier; Lai Xing Ng; Wei Tsang Ooi*

---

### ðŸ“Š Quick Facts

| **Metric** | **Details** |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **References** | 40 Citations |
| **Key Benchmarks** | CIFAR-10, CIFAR-100, YearPredictionMSD |
| **Attack Budget** | $\epsilon = 8/255$ (PGD) |
| **Core Focus** | Adversarial Robustness, One-Stage L2D, Cost-Sensitive Training |

---

## Executive Summary

### ðŸš¨ The Problem
Learning-to-Defer (L2D) systems, designed to balance AI predictions with human expertise, are critically vulnerable in end-to-end, one-stage architectures. The study reveals that malicious adversarial perturbations can simultaneously compromise prediction accuracy and manipulate deferral logic. In classification, this leads to erroneous predictions or resource exhaustion via unnecessary expert consultation. In regression (e.g., YearPredictionMSD), attacks can manipulate continuous values to induce wrong deferrals or bypass oversight entirely. This poses a severe barrier to reliable human-AI collaboration in high-stakes fields like healthcare and autonomous navigation.

### ðŸ’¡ The Innovation
The authors introduce a **cost-sensitive adversarial training framework** tailored for one-stage L2D architectures. Technically, the system uses a single hypothesis to output scores for class predictions and expert decisions. The core innovation is a **Surrogate Deferral Loss**, a tractable cross-entropy-based function that accounts for prediction error and expert consultation costs (scaling and fixed costs). This formulation specifically hardens deferral boundaries against manipulation, supported by rigorous theoretical proofs (Bayes and H-consistency) ensuring optimality against worst-case perturbations.

### ðŸ“ˆ The Results
Validated on CIFAR-10, CIFAR-100, and YearPredictionMSD using Projected Gradient Descent (PGD) attacks ($\epsilon=8/255$), the results were decisive:
*   **Standard L2D:** Suffered catastrophic failure, dropping to **0%** robust accuracy.
*   **Proposed Method:** Successfully recovered performance, achieving **45â€“50%** robust accuracy on CIFAR-10 (comparable to standard adversarial training).
*   **Clean Data:** Maintained high baseline accuracy without degradation.
*   **Regression:** Significantly mitigated targeted attacks that manipulate deferral decisions.

### ðŸš€ The Impact
This work bridges the gap between adversarial robustness and efficient human-in-the-loop systems. By offering a unified defense mechanism that secures both classification and regression tasksâ€”recovering performance from near-failure to functional levelsâ€”the authors provide a practical blueprint for securing L2D deployments. The establishment of formal consistency guarantees ensures future automated decision-support systems can maintain reliability in hostile environments, a vital prerequisite for safe AI adoption in critical applications.

---

## Key Findings

*   **Critical Vulnerability:** L2D systems are highly susceptible to adversarial perturbations that compromise both prediction accuracy and deferral decisions.
*   **Bridged Gap:** Addresses the lack of research regarding robustness in end-to-end one-stage settings.
*   **Enhanced Resistance:** Proposed methods enhance resistance to both **untargeted** and **targeted** attacks.
*   **Performance Retention:** The framework maintains high performance on clean data despite robustness enhancements.
*   **Cross-Domain Efficacy:** The approach is effective across both **Classification** and **Regression** tasks.

---

## Contributions

The research makes four primary contributions to the field:

1.  **Identification of Gap:** Pinpoints the lack of adversarial robustness research in one-stage Learning-to-Defer systems.
2.  **Unified Framework:** Provides a cost-sensitive adversarial training framework applicable to both classification and regression tasks.
3.  **Theoretical Foundation:** Establishes a rigorous theoretical foundation with formal proofs of consistency (H-consistency, (R, F)-consistency, and Bayes consistency).
4.  **Empirical Validation:** Delivers comprehensive empirical validation of the proposed methods on standard benchmark datasets.

---

## Methodology

The research methodology focuses on the evaluation and mitigation of adversarial risks within one-stage L2D architectures.

*   **Framework Development:** Development of a dedicated framework to analyze how adversarial perturbations affect joint training.
*   **Loss Formulation:** Proposal of cost-sensitive adversarial surrogate losses to train robust models.
*   **Attack Formalization:** Formalization of specific attack vectors designed to disrupt the joint training process of the classifier and the deferral mechanism.
*   **Guarantees:** Establishment of rigorous theoretical guarantees to ensure the method's validity.

---

## Technical Details

The paper introduces a sophisticated **One-Stage Learning-to-Defer (L2D) framework** based on a score-based architecture.

### Architecture & Decision Rule
*   **Hypothesis:** A single hypothesis $h: \mathcal{X} \to \mathbb{R}^{K+J}$ maps inputs to scores.
*   **Outputs:**
    *   $K$ dimensions for class predictions.
    *   $J$ dimensions for expert decision outputs.
*   **Action Space:** The decision rule selects the action with the maximum score from an augmented action space $A_c = Y \cup \{K+1, \dots, K+J\}$.

### Loss Functions
*   **True Deferral Loss ($\ell^c_{def}$):** Combines the 0-1 prediction loss with a deferral cost ($c^c_j$).
    *   *Cost Components:* Includes expert error scaling ($\alpha_j$) and fixed consultation costs ($\beta_j$).
*   **Surrogate Deferral Loss ($\Phi^c_{def}$):** Derived from the cross-entropy family to make optimization tractable during training.

### Robustness & Attacks
*   **Robust Optimization:** Incorporates adversarial robustness through the minimization of robust surrogate losses with tractable relaxations.
*   **Theoretical Backing:** backed by proofs for Bayes-consistency, H-consistency, and $(R, F)$-consistency.
*   **Attack Types:**
    *   **Untargeted Attacks:** Seek to maximize loss and degrade overall performance.
    *   **Targeted Attacks:** Seek to force a specific outcome (e.g., incorrect deferral).

---

## Results & Evaluation

### Evaluation Setup
*   **Benchmarks:** Image Classification (CIFAR-10, CIFAR-100) and Tabular Regression (YearPredictionMSD).
*   **Attack Method:** Projected Gradient Descent (PGD).
*   **Perturbation Budget:** $\epsilon = 8/255$.

### Outcomes
*   **Qualitative:** The framework was reported to substantially improve robustness against both untargeted and targeted attacks while outperforming baseline L2D systems across both classification and regression tasks.
*   **Quantitative Highlights:**
    *   Standard L2D systems dropped to **0%** robust accuracy under attack.
    *   The proposed method recovered robust accuracy to approximately **45%â€“50%** on CIFAR-10.
    *   Performance on clean data remained high, showing no degradation compared to non-robust baselines.

---

**References:** 40 citations listed in original analysis.