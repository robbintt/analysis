---
title: A Selective Quantization Tuner for ONNX Models
arxiv_id: '2507.12196'
source_url: https://arxiv.org/abs/2507.12196
generated_at: '2026-02-03T20:24:29'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A Selective Quantization Tuner for ONNX Models

*Nikolaos Louloudakis; Ajitha Rajan*

---

> ### ðŸ“Š Quick Facts
> *   **Accuracy Improvement:** Up to **54.14%** lower accuracy loss vs. full quantization.
> *   **Size Retention:** Maintains up to **98.18%** of model size reduction.
> *   **Scope:** Evaluated on **4 ONNX models** (MobileNetV2, EfficientNet, ShuffleNet, T5).
> *   **Hardware:** Tested on both **CPU and GPU** devices.
> *   **Core Technique:** Pareto Front-based optimization.
> *   **Quality Score:** 8/10

---

## Executive Summary

Deploying deep learning models in resource-constrained environments often requires model quantization to reduce memory footprint and improve inference latency. However, a significant challenge in this domain is the **accuracy-efficiency trade-off**: while full quantization maximizes compression, it frequently introduces unacceptable accuracy degradation by uniformly reducing precision across all layers. Furthermore, hardware accelerators often possess heterogeneous capabilities, where certain operations may not support specific quantized formats, rendering a fully quantized model unusable on specific target hardware.

This paper addresses the need for a more granular approach to quantization that preserves model accuracy while still delivering substantial efficiency gains across diverse hardware architectures. The authors introduce **SeQTO (Selective Quantization Tuner)**, an end-to-end framework designed to automate the selective quantization of ONNX models. Unlike traditional methods that apply uniform quantization, SeQTO employs a multi-stage pipeline to generate mixed-precision model variants, quantizing specific layers or tensors while keeping others in full precision.

The system deploys these variants onto actual hardware (CPUs and GPUs) to profile real-world performance metrics. A core technical innovation is the use of **Pareto Front-based objective minimization**, which mathematically navigates the trade-offs between accuracy and model size to identify optimal configurations. This allows the framework to dynamically determine the best layers to quantize based on empirical data rather than static heuristics.

Empirical evaluations across four ONNX models demonstrate that SeQTO successfully mitigates the accuracy pitfalls associated with full quantization. The framework achieved up to **54.14% lower accuracy loss** compared to fully quantized baselines. Crucially, this improvement in accuracy did not come at the cost of efficiency; SeQTO retained up to **98.18%** of the model size reduction seen in fully quantized models. These results were validated across two different quantization settings and on both CPU and GPU devices, confirming the framework's versatility and robustness.

SeQTO represents a significant advancement in the practical deployment of neural networks by moving beyond binary full-quantization strategies. By providing a tool that mathematically optimizes the balance between accuracy and resource usage, it empowers engineers to deploy high-performance models on hardware with strict resource constraints or limited operation support.

---

## Key Findings

*   **Significant Accuracy Recovery:** SeQTO effectively identifies high-quality selectively quantized models, achieving up to **54.14% lower accuracy loss** compared to fully quantized baselines.
*   **Retained Efficiency:** Despite the selective approach, the framework retains the efficiency benefits of quantization, maintaining up to **98.18%** of the size reduction seen in fully quantized models.
*   **Broad Versatility:** The evaluation demonstrated effectiveness across four ONNX models, two quantization settings, and both CPU and GPU devices.
*   **Hardware Compatibility:** The framework addresses limitations in resource-constrained hardware accelerators that may not support all quantized operations.

---

## Methodology

The authors propose SeQTO, a framework designed to automate the selective quantization of ONNX models. The methodology operates through a structured multi-stage pipeline:

1.  **Generation:** It creates selectively quantized model variants where specific layers are quantized while others remain at full precision.
2.  **Deployment & Execution:** These models are deployed and executed across diverse hardware accelerators (CPUs and GPUs) to gather real-world performance data.
3.  **Profiling & Evaluation:** The system profiles the models based on key metrics such as accuracy and model size.
4.  **Optimization:** It applies Pareto Front-based objective minimization to navigate the trade-offs between accuracy and efficiency, identifying the optimal candidates.
5.  **Visualization:** The framework provides visualizations of the results to aid in analysis.

---

## Technical Details

*   **Framework Name:** SeQTO (Selective Quantization Tuner)
*   **Target:** ONNX models (targets specific layers or tensors for quantization while keeping others in higher precision).
*   **Hardware Support:** Operates across CPU and GPU devices.
*   **Quantization Techniques:**
    *   Post-Training Quantization (PTQ)
    *   Quantization-Aware Training (QAT)
    *   Mixed Precision
*   **Search Strategies:** Leverages sensitivity analysis or differentiable search.
*   **Supported Formats:** Integer and potentially Power-of-Two quantization.
*   **Benchmarks:** ImageNet, MobileNetV2, EfficientNet, ShuffleNet, T5.

---

## Contributions

*   **SeQTO Framework:** A comprehensive end-to-end tool that integrates selective quantization, deployment, and execution specifically for ONNX models on heterogeneous hardware.
*   **Optimized Trade-off Resolution:** A solution to the complex challenge of balancing accuracy and efficiency in resource-constrained environments, moving beyond binary full-quantization approaches.
*   **Pareto Front-based Optimization:** The introduction of a multi-objective optimization strategy that mathematically identifies the best-performing model configurations from a set of candidates.
*   **Hardware Compatibility:** Addressing the limitation of resource-constrained hardware accelerators that may not support all quantized operations by enabling selective precision tuning.

---

## Results

*   **Accuracy Loss:** Reduced by up to **54.14%** compared to fully quantized baselines.
*   **Model Size:** Retained up to **98.18%** of the model size reduction.
*   **Evaluation Scope:** Conducted on 4 ONNX models across 2 quantization settings on both CPU and GPU hardware.

---

**Quality Score:** 8/10  
**References:** 40 citations