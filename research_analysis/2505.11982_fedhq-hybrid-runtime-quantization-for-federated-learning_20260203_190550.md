---
title: 'FedHQ: Hybrid Runtime Quantization for Federated Learning'
arxiv_id: '2505.11982'
source_url: https://arxiv.org/abs/2505.11982
generated_at: '2026-02-03T19:05:50'
quality_score: 9
citation_count: 10
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# FedHQ: Hybrid Runtime Quantization for Federated Learning

*Zihao Zheng; Ziyao Wang; Xiuping Cui; Maoliang Li; Jiayu Chen; Yun; Liang; Ang Li; Xiang Chen*

---

### üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Max Training Acceleration** | 2.47x |
| **Max Accuracy Improvement** | 11.15% |
| **System Overhead** | ~1.7 min (init) + ~14.2 min (adjustment) |
| **Core Strategy** | Hybrid PTQ + QAT |
| **Datasets Tested** | CIFAR10, CIFAR100, SVHN |
| **Model Architecture** | ResNet-18 |

---

> ## üìù Executive Summary
> 
> Federated Learning (FL) faces significant efficiency and accuracy challenges due to the inherent heterogeneity of client devices and data distributions. While model quantization is a standard technique to reduce communication overhead and computational costs, existing approaches typically force a binary choice between Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ offers speed but often sacrifices accuracy, particularly on non-IID data, while QAT preserves accuracy but incurs high computational latency. This paper addresses the critical need to move beyond this static binary selection to optimize the trade-off between training speed and model accuracy across diverse, resource-constrained edge environments.
> 
> The authors introduce **FedHQ**, a hybrid runtime quantization framework that dynamically allocates optimal quantization strategies (PTQ or QAT) to individual clients based on their specific constraints. FedHQ utilizes a two-stage adaptive allocation mechanism comprising a coarse-grained global initialization and a fine-grained machine learning-based adjustment. Technically, the system employs a dual-analysis model: a **Hardware-related Speed Analysis** to model training speed based on device specifications and batch sizes, and a **Data-distribution-related Accuracy Analysis** using offline fitting to determine an "Accuracy Significance" metric ($SigAcc_m$). By normalizing these metrics, the server maps clients into a 2D geometric space, automatically assigning QAT to clients with high hardware capacity but low accuracy potential, and PTQ to clients where speed is the bottleneck.
> 
> In experiments conducted using ResNet-18 on CIFAR10, CIFAR100, and SVHN datasets, FedHQ demonstrated superior performance compared to baselines such as FedAQT, FedFQ, and FedRH. With a trade-off parameter set to $\xi=0.2$, FedHQ achieved end-to-end training times of 221.0 minutes (67.78% accuracy) on CIFAR10, 150.7 minutes (46.35% accuracy) on CIFAR100, and 163.1 minutes (93.01% accuracy) on SVHN. The framework achieved a maximum training acceleration of **2.47x** and up to **11.15%** improvement in accuracy. Crucially, the system overhead for the allocation mechanism was negligible, requiring only approximately 1.7 minutes for coarse-grained initialization and 14.2 minutes for fine-grained adjustment.
> 
> This research significantly advances the field of efficient Federated Learning by demonstrating that a hybrid, automated approach to quantization outperforms static, uniform methods. By successfully modeling the trade-off boundaries between hardware constraints and data distribution variances, FedHQ provides a practical solution for handling system and statistical heterogeneity in real-world deployments.

---

## üîë Key Findings

*   **Performance Gains:** The FedHQ framework achieves up to **2.47x training acceleration** and up to **11.15% accuracy improvement** compared to existing baselines.
*   **Hybrid Superiority:** A hybrid strategy combining PTQ and QAT balances speed and accuracy significantly better than utilizing either method in isolation.
*   **Heterogeneity Management:** Hardware-related and data-distribution-related analyses successfully model trade-off boundaries to effectively handle both device and data heterogeneity.
*   **Efficiency:** The proposed system adds negligible extra overhead to the total training process.

---

## üõ†Ô∏è Methodology

The paper proposes **FedHQ**, a hybrid runtime quantization framework designed to automatically allocate optimal quantization strategies to clients.

The core methodology includes:

1.  **Strategy Combination:** It combines **Post-Training Quantization (PTQ)** and **Quantization-Aware Training (QAT)** to leverage the benefits of both.
2.  **Heterogeneity Analysis:** It utilizes hardware-related and data-distribution-related analysis to model difficulties caused by the heterogeneity of client devices and data.
3.  **Adaptive Allocation:** The framework features an adaptive allocation mechanism composed of a two-stage process:
    *   **Coarse-grained Global Initialization:** For rapid initial strategy distribution.
    *   **Fine-grained ML-based Adjustment:** For precise optimization based on ongoing feedback.

---

## üß© Contributions

*   **Hybrid Approach:** Introduction of a hybrid quantization approach for Federated Learning that moves beyond the traditional binary choice of PTQ or QAT.
*   **Analytical Models:** Development of analytical models that account for hardware constraints and data distribution variances to effectively identify trade-off boundaries.
*   **FedHQ Framework:** Design of the FedHQ framework, providing an automated mechanism for strategy selection via coarse-grained initialization and fine-grained machine learning adjustments.

---

## ‚öôÔ∏è Technical Details

**Quantization Strategy**
*   **Hybrid:** Combines PTQ (Post-Training Quantization) for speed and QAT (Quantization-Aware Training) for accuracy.

**Analysis Mechanism**
*   **Hardware-related Speed Analysis:** Models training speed based on client hardware specifications and batch sizes.
*   **Data-distribution-related Accuracy Analysis:** Uses offline fitting to distributions and employs an Accuracy Significance metric ($SigAcc_m$).

**Allocation Logic**
*   The server normalizes **Speed** and **Accuracy Significance** metrics.
*   Clients are mapped into a **2D geometric space**.
    *   **High Speed / Low Accuracy:** Assigned QAT.
    *   **High Accuracy / Low Speed:** Assigned PTQ.

**System Architecture**
*   **Coarse-grained Global Initialization:** Enables rapid allocation of strategies.
*   **Fine-grained ML-based Adjustment:** Utilizes a Cost Model for precise, ongoing optimization.

---

## üìà Results

**Experimental Setup**
*   **Model:** ResNet-18
*   **Datasets:** CIFAR10, CIFAR100, SVHN
*   **Hardware:** NVIDIA A6000 GPUs
*   **Baselines:** FedAQT, FedFQ, FedRH

**Performance Metrics ($\xi=0.2$)**
*   **CIFAR10:** 221.0 min (67.78% accuracy)
*   **CIFAR100:** 150.7 min (46.35% accuracy)
*   **SVHN:** 163.1 min (93.01% accuracy)

**System Efficiency**
*   Achieved max acceleration of **2.47x** and up to **11.15%** accuracy improvement over baselines.
*   **Overhead:**
    *   Coarse-grained initialization: ~1.7 minutes
    *   Fine-grained adjustment: ~14.2 minutes
*   Total overhead considered negligible compared to total training time.

---

**Quality Score:** 9/10
**References:** 10 citations