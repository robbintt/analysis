# Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs

*Zhining Liu; Ziyi Chen; Hui Liu; Chen Luo; Xianfeng Tang; Suhang Wang; Joy Zeng; Zhenwei Dai; Zhan Shi; Tianxin Wei; Benoit Dumoulin; Hanghang Tong*

---

> ### üìä Quick Facts
 >
 > *   **Quality Score:** 8/10
 > *   **References:** 35 Citations
 > *   **Models Analyzed:** LLaVA, Qwen, Gemma, InternVL
 > *   **Key Innovation:** Visual Evidence Augmentation (VEA)
 > *   **Core Metric:** RAPT (Relative Attention per Token)

---

## üìù Executive Summary

This research addresses the critical reliability gap in Vision-Language Models (VLMs) by investigating why models generate incorrect answers despite possessing the necessary visual information. The problem is significant because previous analyses could not distinguish between failures of **perception** (failing to "see" the object) and failures of **reasoning** (failing to utilize what is seen).

The authors identify that a major source of VLM errors is the **"Seeing but Not Believing"** phenomenon: deep layers of the network often correctly perceive and attend to relevant visual evidence, yet the model generates an incorrect answer because textual priors override visual data. This issue is shown to be prevalent across major VLM families, including LLaVA, Qwen, Gemma, and InternVL.

The authors introduce a dual innovation comprising a diagnostic metric and a training-free intervention:
1.  **RAPT:** A metric quantifying the shift in attention mass between textual and visual modalities.
2.  **VEA (Visual Evidence Augmentation):** An inference-time intervention that identifies "Visual Grounding Layers," extracts attention vectors, and creates a highlighting mask to overlay onto the input image.

The analysis yields quantitative evidence of a persistent attentional imbalance, confirming that image tokens receive less attention per token than text tokens. While deep layers successfully shift focus to act as "visual grounders," the model frequently produces incorrect answers due to a "blind faith in text," leading to errors such as **False Rejection**, **Hallucination**, and **Partially Correct responses**. This work provides a granular diagnostic framework and a low-cost path to improving model reliability without fine-tuning.

---

## üîç Key Findings

*   **The Core Disconnect:** VLM failures often stem from an inability to leverage visual evidence for reasoning rather than a failure to perceive the evidence itself.
*   **Attention Dynamics:** There is a distinct shift in attention across layers; shallow layers focus predominantly on textual input, while deeper layers begin to attend to visual evidence.
*   **"Seeing but Not Believing":** This phenomenon describes instances where models correctly perceive and identify visual evidence in deep layers but proceed to output incorrect answers based on textual priors.
*   **Widespread Prevalence:** The issue is consistent across major VLM architectures, including LLaVA, Qwen, Gemma, and InternVL.

---

## ‚öôÔ∏è Technical Details

The methodology and technical implementation rely on layer-wise analysis and a novel inference-time intervention.

### 1. RAPT (Relative Attention per Token)
A metric designed to quantify focus shifts between textual and visual modalities.
*   **Calculation:** The ratio of section-average attention mass per token to the input-average attention value.
*   **Purpose:** To measure the imbalance of attention between image and text tokens.

### 2. VEA (Visual Evidence Augmentation)
A training-free inference-time intervention specifically for decoder-only Transformer VLMs.

**The Process:**
1.  **Identification:** VEA identifies **Visual Grounding Layers ($L_{VG}$)** by selecting the top 10% of layers with the highest AUROC scores against human-annotated bounding boxes (from the VisualCOT dataset).
2.  **Extraction:** Attention vectors are extracted from these specific $L_{VG}$ layers.
3.  **Refinement:** Denoising and smoothing techniques are applied to these vectors to create a precise highlighting mask.
4.  **Augmentation:** The mask is overlaid onto the input image during final generation to force the model to utilize the visual evidence it has already identified.

---

## üìà Results & Analysis

The study revealed several critical patterns regarding model behavior and error types:

*   **Persistent Attentional Imbalance:** Across major architectures, image tokens consistently receive less attention per token compared to text tokens.
*   **Layer-Specific Behavior:**
    *   **Shallow Layers:** Display uniform attention or focus on question text.
    *   **Deep Layers:** Act as "visual grounders" with sparse, concentrated highlighting that correlates strongly with human attention.
*   **Prevalence of Errors:**
    *   **False Rejection:** The model rejects a correct visual concept.
    *   **Hallucination:** Textual priors override the actual visual evidence present in the image.
    *   **Partially Correct:** The response is marginally correct but misses key details visible in the image.
*   **Root Cause:** These errors are largely attributed to the model's "blind faith in text" and an inherent architectural bias favoring textual modality over visual data.

---

## ‚úÖ Contributions

*   **Diagnostic Advancement:** Provides a new framework for understanding VLM reliability by isolating errors as a disconnect between perception and reasoning.
*   **Phenomenology:** Clearly defines and characterizes the **'Seeing but Not Believing'** phenomenon.
*   **Training-Free Solution:** Presents VEA, a method to bridge the perception-reasoning gap without requiring expensive fine-tuning or additional training.
*   **Taxonomy of Errors:** Offers a classification of VLM errors (False Rejection, Hallucination, etc.) based on attentional dynamics.