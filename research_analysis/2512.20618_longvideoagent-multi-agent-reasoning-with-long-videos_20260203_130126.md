---
title: 'LongVideoAgent: Multi-Agent Reasoning with Long Videos'
arxiv_id: '2512.20618'
source_url: https://arxiv.org/abs/2512.20618
generated_at: '2026-02-03T13:01:26'
quality_score: 9
citation_count: 16
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LongVideoAgent: Multi-Agent Reasoning with Long Videos

*Runtao Liu; Ziyi Liu; Jiaqi Tang; Yue Ma; Renjie Pi; Jipeng Zhang; Qifeng Chen*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Benchmark** | LongTVQA & LongTVQA+ |
| **Top Accuracy** | **71.56%** (vs. 67.90% baseline) |
| **Architecture** | Multi-Agent System (Master + Grounding + Vision) |
| **Training Method** | Reinforcement Learning (GRPO) |
| **Focus** | Hour-long video reasoning & temporal grounding |

---

## üìù Executive Summary

This paper addresses the critical challenge of reasoning over **hour-long video content**, a task that pushes the limits of current multimodal systems due to the sheer volume of data and strict context window constraints. The core issue is the trade-off between content compression‚Äîwhich reduces video to fit within limited context‚Äîand the retention of fine-grained details necessary for accurate episode-level understanding. Existing methods often rely on lossy summarization techniques that discard crucial visual cues and temporal relationships, making it difficult for models to answer complex questions that require precise localization of evidence within long durations of footage.

The authors introduce **LongVideoAgent**, a novel multi-agent framework designed to decompose long-video reasoning into a collaborative, iterative process managed by a Master LLM. This coordinator directs two specialized frozen sub-agents: a **Grounding Agent** responsible for temporal localization to identify relevant video segments, and a **Vision Agent** that extracts targeted textual observations from visual content to supplement subtitles. Technically, the system operates within a defined step limit (K steps), allowing the Master Agent to choose between specific actions such as `<watch>`, `<request_grounding>`, and `<answer>`. Crucially, the Master Agent is optimized using **Reinforcement Learning (GRPO)** to maximize conciseness and cooperation efficiency, rather than relying solely on standard prompting, thereby reducing hallucinations and redundant tool usage.

Evaluations on the newly introduced **LongTVQA** and **LongTVQA+** datasets demonstrate that LongVideoAgent significantly outperforms strong non-agent baselines, traditional single-pass Multimodal Large Language Models (MLLMs), and prior agentic works like VideoAgent. Specifically, LongVideoAgent achieves approximately **71.56% accuracy** on the LongTVQA benchmark, notably surpassing VideoAgent, which achieves **67.90%**. The results indicate that the Reinforcement Learning training paradigm is essential for strengthening the reasoning and planning capabilities of the master agent; ablation studies show that removing GRPO training leads to a performance drop, confirming its role in enabling efficient multi-agent cooperation and precise evidence retrieval.

The significance of this work lies in its effective resolution of the tension between broad content compression and the need for granular analysis, proving that agent-based systems can preserve fine-grained cues often missed by current summarization methods. By generating interpretable reasoning trajectories, the framework offers transparency into the decision-making process, a vital attribute for building trust in automated video analysis systems. Furthermore, the paper advances the field by releasing LongTVQA and LongTVQA+, new episode-level benchmarks based on TVQA that provide a rigorous, large-scale resource for future research into long-video understanding and multimodal agent cooperation.

---

## üîë Key Findings

*   **Superior Performance:** The proposed multi-agent system significantly outperforms strong non-agent baselines on episode-level question answering tasks using the **LongTVQA** and **LongTVQA+** datasets.
*   **RL Enhanced Reasoning:** Reinforcement learning training specifically strengthens the reasoning and planning capabilities of the master agent, leading to more efficient multi-agent cooperation.
*   **Overcoming Summarization Limits:** The framework successfully overcomes the limitations of lossy summarization by combining precise temporal grounding with targeted visual observation extraction.
*   **Interpretable Trajectories:** Utilizing a master agent to coordinate specific sub-agents results in interpretable reasoning trajectories while preserving fine-grained cues often missed by current compression methods.

---

## üèóÔ∏è Methodology

The system employs a hierarchical multi-agent structure designed to handle long-context video understanding efficiently:

1.  **Master LLM Coordinator:** Acts as the central planner.
    *   Manages the overall reasoning strategy.
    *   Operates within a defined **step limit** to manage the scope of reasoning.
    *   Trained using **Reinforcement Learning (RL)** to encourage behaviors that are concise, correct, and efficient.
2.  **Specialized Sub-Agents:**
    *   **Grounding Agent:** Responsible for localizing question-relevant video segments (temporal grounding).
    *   **Vision Agent:** Responsible for extracting targeted textual observations from the visual content to complement subtitles.
3.  **Process Flow:** The master agent coordinates the sub-agents iteratively, gathering information until it can confidently answer the question or hits the step limit.

---

## ‚öôÔ∏è Technical Details

The LongVideoAgent architecture is built upon a strict operational loop designed to maximize efficiency and accuracy.

### Architecture Components
*   **Master Agent:** The central controller. It is the only agent trained during the process (via GRPO), while sub-agents remain frozen.
*   **Grounding Agent:** Handles temporal localization.
*   **Vision Agent:** Handles visual extraction and textual observation.

### Operational Loop
*   **Iterative Steps:** The system operates in a loop up to **K steps**.
*   **Context Accumulation:** Context is progressively accumulated from:
    *   Subtitles
    *   Segment tags
    *   Visual observations
*   **Action Space:** The Master Agent decides between three specific actions based on a strict system prompt:
    *   `<watch>`
    *   `<request_grounding>`
    *   `<answer>`
*   **Optimization:** The system prompt is designed to minimize hallucinations and unnecessary tool calls.

### Training Strategy
*   **Algorithm:** Uses **Generalized Reinforcement Learning (GRPO)**.
*   **Objective:** Optimize the Master Agent for:
    *   Accuracy
    *   Conciseness
    *   Cooperation efficiency

---

## üìà Results

*   **Benchmark Dominance:** LongVideoAgent significantly outperforms non-agent baselines on the LongTVQA and LongTVQA+ datasets.
*   **Accuracy Metrics:**
    *   **LongVideoAgent:** ~**71.56%** accuracy.
    *   **VideoAgent (Baseline):** **67.90%** accuracy.
*   **Method Validation:** Reinforcement Learning (GRPO) training was found to strengthen reasoning and planning capabilities, resulting in more efficient cooperation. Ablation studies confirmed that removing GRPO training causes a performance drop.
*   **Framework Efficiency:** The framework overcomes 'lossy summarization' by preserving fine-grained cues through precise temporal grounding and targeted visual observation.
*   **Comparison:** It generates interpretable reasoning trajectories and outperforms traditional single-pass MLLMs and earlier agentic works like VideoAgent.

---

## üß† Contributions

1.  **Novel Framework:** Introduction of a novel multi-agent reasoning framework designed for hour-long videos that addresses the trade-off between content compression and detailed analysis, improving temporal grounding and the capture of fine-grained cues.
2.  **Training Paradigm:** A training paradigm for the coordinating agent using reinforcement learning to optimize the efficiency and conciseness of the multi-agent collaboration process.
3.  **New Datasets:** The proposal and release of **LongTVQA** and **LongTVQA+**, new episode-level datasets aggregated from TVQA and TVQA+ specifically designed to evaluate long-video reasoning capabilities.

---

**Quality Score:** 9/10  
**References:** 16 citations