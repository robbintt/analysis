# Mixture of Raytraced Experts

*Andrea Perin; Giacomo Lagomarsini; Claudio Gallicchio; Giuseppe Nuti*

### **Quick Facts**

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Training Speed** | 10% – 40% reduction in epochs |
| **Fashion MNIST** | ~0.90 Accuracy (1-28 experts dynamic scaling) |
| **CIFAR-10** | ~0.86 Test Accuracy |
| **Key Innovation** | No load balancing required; Variable graph depth/width |

## Executive Summary

Standard Mixture of Experts (MoE) architectures face inherent trade-offs between computational efficiency and training stability. Specifically, they typically rely on rigid computation budgets (e.g., fixed top-k routing) and require complex load-balancing loss mechanisms to ensure even expert utilization and prevent collapse. This paper addresses these limitations by proposing a method that removes the necessity for load-balancing constraints while supporting dynamic, variable-width computation graphs, thereby offering a path to more efficient and adaptable deep learning models.

The authors introduce the **"Mixture of Raytraced Experts" (MRE)**, a novel stacked MoE architecture that utilizes iterative stochastic routing via stacked gates rather than traditional top-k selection. The system dynamically constructs computational graphs of variable width and depth by sampling sequences of experts, a process theoretically modeled as "**activation rays**" emanating from a Poisson process.

Technically, the model employs Recurrent Neural Network (RNN)-style sequence unfolding for training and Gumbel-softmax reparameterization to ensure differentiability. Crucially, MRE utilizes "**homogeneous derivatives**" to maintain optimization stability, effectively eliminating the need for explicit load-balancing mechanisms while allowing the model to provide approximate solutions progressively throughout the calculation cycle.

Empirical evaluation demonstrates that MRE successfully reduces training time while maintaining high accuracy compared to established baselines. The proposed method achieved a reduction in training epochs by **10% to 40%** relative to standard approaches. On benchmark tasks, the model demonstrated dynamic scaling capability—activating between 1 to 28 experts on Fashion MNIST to reach approximately **0.90 accuracy**—and achieved a test accuracy of about **0.86 on CIFAR-10**. These results show that MRE achieves comparable or superior performance to standard top-k MoE models and recent work by Yue et al. (2025), but without the overhead of load-balancing requirements.

This research validates a significant shift in MoE design, proving that architectures can support dynamic, variable computation graphs without sacrificing accuracy or training stability.

## Key Findings

*   **Training Efficiency:** The proposed method achieved a reduction in training epochs by **10% to 40%** compared to standard approaches.
*   **Accuracy Retention:** The model demonstrated comparable or higher accuracy than existing architectures despite the reduction in training time.
*   **Elimination of Load Balancing:** Unlike traditional Mixture of Experts (MoE) models, this approach functions effectively **without requiring load-balancing mechanisms**.
*   **Dynamic Accuracy Scaling:** The model yields predictions that improve in accuracy sequentially as the computation cycles through the experts' sequence, rather than using a fixed computation budget.
*   **Variable Computation:** The architecture successfully generates computational graphs with **variable width and depth** for given samples.

## Methodology

*   **Architecture:** The authors developed a "Mixture of Raytraced Experts," which functions as a **stacked Mixture of Experts (MoE)** architecture.
*   **Graph Generation:** The method dynamically selects sequences of experts to construct computational graphs that possess variable width and depth.
*   **Training Protocol:** The model is trained by iteratively sampling from a set of candidate experts.
*   **Sequence Unfolding:** The training process involves unfolding the expert sequence in a manner analogous to the training of **Recurrent Neural Networks (RNNs)**.

## Technical Details

*   **Routing Mechanism:** Utilizes iterative stochastic routing with stacked gates to generate activation masks, differing from standard top-k selection.
*   **Activation Theory:** Features a 'firing rate' governing the probability of expert activation sequentially, theoretically modeled as '**activation rays**' from a Poisson process.
*   **Graph Structure:** Supports dynamic computation graphs of variable width and depth that build progressively.
*   **Optimization Strategy:** Training employs RNN-style unfolding and **Gumbel-softmax reparameterization** for differentiability.
*   **Stability Control:** The approach eliminates explicit load-balancing mechanisms by using '**homogeneous derivatives**'.
*   **Progressive Output:** Provides approximate solutions throughout the calculation cycle rather than just at the end.

## Contributions

*   **Novel Architecture:** Introduction of a new MoE variant that supports dynamic selection of expert sequences, breaking away from fixed computation requirements.
*   **Efficiency Optimization:** Demonstrated that dynamic computational graphs can significantly reduce training epochs (10–40%) while maintaining high accuracy.
*   **Simplified Training Requirements:** Established a viable MoE training methodology that removes the necessity for complex load-balancing mechanisms.
*   **Research Direction:** Validated a new path for designing faster and more expressive models within the MoE domain.

## Results

*   **Efficiency:** The method reduced training epochs by **10% to 40%** compared to standard approaches.
*   **Fashion MNIST:**
    *   Demonstrated dynamic scaling (1 to 28 experts).
    *   Reached approximately **0.90 accuracy**.
*   **CIFAR-10:**
    *   Achieved a test accuracy of about **0.86**.
*   **Comparative Performance:** Compared to standard top-k MoE and Yue et al. (2025), MRE achieved comparable or higher accuracy with the added benefits of variable-width computation graphs and no load-balancing requirements.

---
**Quality Score:** 8/10
**References:** 6 citations