# Text classification using machine learning methods
*Bogdan Oancea*

---

> ### ðŸ“‹ Quick Facts
> *   **Primary Focus:** E-commerce product classification
> *   **Top Embedding Technique:** FastText
> *   **Best Performing Classifiers:** SVM, Logistic Regression, Random Forest
> *   **Performance Ranking:** FastText > Word2Vec â‰ˆ GloVe â‰ˆ TF-IDF â‰ˆ Count Vectorization
> *   **References:** 3 Citations
> *   **Quality Score:** 6/10

---

## Executive Summary

This research addresses the challenge of classifying short, unstructured text stringsâ€”specifically product namesâ€”within a commercial e-commerce context. The core technical hurdle involves converting variable-length descriptions that are often sparse, noisy, and prone to typographical errors into numeric vector representations suitable for machine learning. Accurate categorization is essential for optimizing downstream workflows such as search functionality and inventory management, necessitating robust feature extraction methods capable of capturing semantic meaning from limited textual data where traditional methods often fail.

The study introduces a rigorous comparative framework evaluating five distinct word embedding techniquesâ€”Count Vectorization, TF-IDF, Word2Vec, FastText, and GloVeâ€”to map raw text into vector space. The technical approach isolates the vectorization method as the primary variable, feeding these embeddings into a suite of supervised machine learning classifiers including Linear models (Logistic Regression), Probabilistic models (Multinomial Naive Bayes), Instance-based learners (kNN), Tree-based methods (Decision Trees), and Non-linear models (Artificial Neural Networks and Support Vector Machines). This methodology allows for a direct analysis of how different feature extraction strategies, particularly FastTextâ€™s reliance on sub-word information to handle morphological variations, impact classification performance.

Empirical results established a distinct performance hierarchy among the tested embeddings: **FastText > Word2Vec â‰ˆ GloVe â‰ˆ TF-IDF â‰ˆ Count Vectorization**. FastText yielded superior results due to its ability to generate vectors for out-of-vocabulary words via sub-word n-grams. Regarding classifiers, the study identified Support Vector Machines (SVM), Logistic Regression, and Random Forests as the top-performing architectures, demonstrating impressive accuracy. In contrast, kNN, Naive Bayes, and single Decision Trees showed lower efficacy, with Random Forests correcting previous misconceptions to rank among the elite performers in this specific context.

This paper provides data scientists and engineers with a critical benchmark for implementing text classification systems, offering validated performance hierarchies rather than general recommendations. By confirming FastText as the optimal embedding technique when paired with SVM, Logistic Regression, or Random Forests, the study directs the field toward a high-accuracy framework capable of handling the linguistic complexities inherent in real-world product data. These findings enable technical teams to optimize NLP pipelines with confidence, ensuring robust handling of sparse and noisy textual inputs.

---

## Key Findings

*   **Superior Embedding:** The **FASTTEXT** word embedding technique yielded the best results among all tested methods, surpassing Count Vectorization, TF-IDF, Word2Vec, and GloVe.
*   **Top Classifiers:** Support Vector Machines (SVM), Logistic Regression, and Random Forests demonstrated impressive accuracy in classifying the data.
*   **Effective Strategy:** The experiment confirms that converting product names from text into numeric vectors is an effective strategy for automatic classification.
*   **Reason for Success:** FastText's superior performance is attributed to its utilization of sub-word information, allowing it to better handle morphological variations and typos.

---

## Methodology

The research employed a rigorous experimental pipeline to evaluate the efficacy of different text representation methods combined with various machine learning algorithms.

1.  **Data Preprocessing:** Product names were converted into numeric vectors using various word embedding techniques to prepare them for machine learning models.
2.  **Embedding Evaluation:** Five distinct embedding methods were evaluated:
    *   Count Vectorization
    *   TF-IDF (Term Frequency-Inverse Document Frequency)
    *   Word2Vec
    *   FASTTEXT
    *   GloVe (Global Vectors for Word Representation)
3.  **Classifier Training:** The resulting vectors were used to train a diverse set of machine learning classifiers, including:
    *   Logistic Regression
    *   Multinomial Naive Bayes
    *   k-Nearest Neighbors (kNN)
    *   Artificial Neural Networks
    *   Support Vector Machines (SVM)
    *   Decision Trees

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Learning Type** | Supervised Machine Learning |
| **Task** | Text classification of product names |
| **Pipeline Architecture** | Raw Text $\rightarrow$ Feature Extraction (Numeric Vectors) $\rightarrow$ Classification |

**Evaluated Vectorization Methods:**
*   **FastText:** (Top Performer) Utilizes sub-word information.
*   Word2Vec
*   GloVe
*   TF-IDF
*   Count Vectorization

**Tested Classification Algorithms:**
*   Support Vector Machines (SVM)
*   Logistic Regression
*   Random Forests
*   *Note: kNN, Naive Bayes, and Decision Trees were tested but showed lower efficacy compared to the top three.*

---

## Results & Performance

The study established a clear performance ranking based on the ability of the system to categorize product names accurately.

*   **Embedding Hierarchy:** FastText demonstrated superior performance over other vectorization methods.
    *   **Ranking:** FastText > Word2Vec â‰ˆ GloVe â‰ˆ TF-IDF â‰ˆ Count Vectorization
*   **Classifier Efficacy:** All elite classifiers (SVM, Logistic Regression, Random Forests) demonstrated impressive accuracy.
*   **Validation:** The study successfully validated the hypothesis that converting product names to numeric vectors is a robust approach for classification.
*   **Observation:** Random Forests performed notably well, correcting previous misconceptions about its efficacy in this specific context.

---

## Contributions

The research makes the following significant contributions to the field of Natural Language Processing (NLP) and e-commerce data handling:

*   **Comparative Analysis:** Provides a detailed comparative analysis of five distinct word embedding methods specifically for product classification.
*   **Benchmarking:** Benchmarks various machine learning architectures against each other using the same dataset.
*   **Optimization Framework:** Identifies the specific combination of **FASTTEXT** embedding with **SVM**, **Logistic Regression**, or **Random Forests** as a high-accuracy framework.
*   **Workflow Improvement:** Offers technical teams a validated path for optimizing automatic product categorization workflows.

---

*Document generated based on analysis provided.*