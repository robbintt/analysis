---
title: Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM
arxiv_id: '2505.05772'
source_url: https://arxiv.org/abs/2505.05772
generated_at: '2026-02-03T12:57:08'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM
*Zehao Fan; Garrett Gagnon; Zhenyu Liu; Liu Liu*

---

> ### üìä Quick Facts
>
> *   **Latency Reduction (vs. Token-wise):** 19% ‚Äì 31%
> *   **Energy Reduction (vs. Full KV):** 45% ‚Äì 67%
> *   **Latency Reduction (vs. Full KV):** 54% ‚Äì 74%
> *   **Models Evaluated:** LongChat-7B-v1.5-32K, LLaMA3.1-8B
> *   **Target Architecture:** HBM-PIM (AttAcc)
> *   **Core Technique:** Semantic Clustering + Hardware-Aligned Mapping

---

## üìù Executive Summary

This research addresses the critical inefficiency of deploying Long Context Large Language Models (LLMs) on Processing-in-Memory (PIM) architectures. While sparse attention mechanisms are widely used to manage the growing memory footprint of Key-Value (KV) caches, they generate irregular memory access patterns that are fundamentally incompatible with the physical layout of PIM hardware. This mismatch results in severe workload imbalance and underutilization of PIM banks, negating the bandwidth advantages of PIM and creating energy and latency bottlenecks during the decoding phase.

The authors propose **STARC** (Sparse ATTention Remapping with Clustering), a hardware-software co-design scheme that reorganizes sparse data to align with PIM constraints. Instead of retrieving data token-wise based on position, STARC employs semantic clustering using cosine-based K-means to group semantically similar Key and Value vectors. These clusters are then mapped to contiguous memory rows that match the physical structure of PIM banks. By matching incoming queries to precomputed cluster centroids, the system retrieves relevant clusters in parallel (fetching 8 vectors per access), enabling efficient parallel processing while maintaining a dynamic structure that re-clusters periodically to handle evolving context during inference.

Evaluated on LongChat-7B and LLaMA3.1-8B using the AttAcc simulator, STARC significantly outperforms existing sparse attention baselines (Quest, InfiniGen, SparQ). Compared to standard token-wise sparsity methods, STARC reduces attention layer latency by **19%‚Äì31%** and energy consumption by **19%‚Äì27%**. When tested against a full KV cache retrieval strategy with a budget of 1024, it achieved substantial reductions of **54%‚Äì74%** in latency and **45%‚Äì67%** in energy, contributing to a total system latency reduction of 6%‚Äì14%. Crucially, these efficiency gains were achieved without sacrificing accuracy, with performance comparable to state-of-the-art sparse attention methods on LongBench and PG-19 benchmarks.

This work bridges the gap between algorithmic sparsity and PIM architecture, demonstrating that data layout is as critical as computation in modern AI accelerators. By solving the workload imbalance inherent in current PIM designs, STARC enables practical, energy-efficient long-context inference on hardware designed for high bandwidth. This approach establishes a new paradigm for cluster-granularity data mapping, paving the way for future PIM designs that can effectively handle the irregular access patterns of generative AI models.

---

## üîë Key Findings

*   **Performance vs. Standard Sparsity:** STARC reduces attention-layer latency by **19%‚Äì31%** and energy consumption by **19%‚Äì27%** compared to standard token-wise sparsity methods on HBM-PIM systems.
*   **Performance vs. Full KV Cache:** Under a KV cache budget of 1024, the method achieves significant reductions of **54%‚Äì74%** in latency and **45%‚Äì67%** in energy consumption compared to full KV cache retrieval.
*   **System-Wide Impact:** The optimizations contribute to a total system latency reduction of **6%‚Äì14%**.
*   **Accuracy Retention:** The method maintains model accuracy comparable to state-of-the-art sparse attention methods while addressing workload imbalance and resource utilization issues in current PIM designs.
*   **Workload Balance:** Successfully resolves the mismatch between irregular sparse access patterns and the physical row/bank structure of PIM architectures.

---

## üß© Methodology

The authors propose STARC (Sparse ATTention Remapping with Clustering), a data mapping scheme specifically designed for Processing-in-Memory (PIM) architectures. The methodology consists of three core phases:

1.  **Semantic Clustering**
    *   Groups Key-Value (KV) pairs based on **semantic similarity** rather than their physical token position.
    *   Utilizes cosine-based K-means clustering to identify related vectors.

2.  **Hardware-Aligned Mapping**
    *   Maps the generated clusters to **contiguous memory regions**.
    *   Ensures alignment with PIM bank architectures to maximize parallel access efficiency.

3.  **Cluster-Granularity Retrieval**
    *   Matches incoming queries against **precomputed centroids** to identify the most relevant clusters.
    *   Performs selective attention and parallel processing on the identified clusters rather than scanning individual tokens.

---

## ‚öôÔ∏è Technical Details

| Component | Specification | Description |
| :--- | :--- | :--- |
| **Algorithm** | Cosine-based K-means (KMeans++) | Used for grouping semantically similar K and V vectors. |
| **Cluster Granularity** | $\approx$ 32 tokens/cluster | Calculated as sequence length divided by 32. |
| **Data Structure** | Growing Clustering Structure | Handles differences between prefill and decoding stages. |
| **Re-clustering Frequency** | Every 128 steps | Dynamically updates clusters to maintain relevance during inference. |
| **Optimization** | Retrieval Disable | Disables retrieval in the first two layers to reduce overhead. |
| **Hardware Base** | AttAcc PIM Architecture | Built upon the AttAcc simulator environment. |
| **Parallelism** | 8 vectors/access | Fetches 8 complete key/value vectors in parallel per access. |
| **Alignment** | Physical Memory Row Size | Clusters are aligned to match physical memory row sizes for PIM efficiency. |

---

## üìà Results

The system was evaluated on **LongChat-7B-v1.5-32K** and **LLaMA3.1-8B** against baselines including Quest, InfiniGen, and SparQ using the AttAcc simulator.

*   **vs. Token-wise Sparsity Methods:**
    *   **Latency:** Reduced by 19%‚Äì31%
    *   **Energy:** Reduced by 19%‚Äì27%

*   **vs. Full KV Cache (Budget 1024):**
    *   **Latency:** Reduced by 54%‚Äì74%
    *   **Energy:** Reduced by 45%‚Äì67%
    *   **Total System Latency:** Reduced by 6%‚Äì14%

*   **Accuracy & Recall:**
    *   STARC maintains higher recall rates than baselines.
    *   Achieved accuracy comparable to state-of-the-art sparse attention methods on the **LongBench** and **PG-19** benchmarks.

---

## üèÜ Contributions

*   **Bridging the Sparsity-PIM Gap:** Identified that current PIM designs fail to handle irregular sparse KV cache patterns and introduced a solution to bridge this gap.
*   **Hardware-Software Co-design:** Introduced a mapping scheme that aligns software-level semantic clustering with hardware-level memory bank structures to enable efficient parallel processing.
*   **Efficient Long-Context Inference:** Enabled significant reductions in memory bandwidth bottlenecks and energy costs associated with growing KV caches, facilitating efficient long-context inference for LLMs without compromising accuracy.

---

**Quality Score:** 9/10  
**References:** 40 citations