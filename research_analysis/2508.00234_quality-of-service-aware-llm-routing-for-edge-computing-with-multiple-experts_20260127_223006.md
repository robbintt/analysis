---
title: Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts
arxiv_id: '2508.00234'
source_url: https://arxiv.org/abs/2508.00234
generated_at: '2026-01-27T22:30:06'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts

*Zhiying Feng, Zhi Zhou, Qiong Wu, Jin Yang, Deke Guo, Xu Chen*

---

## ðŸ“‘ Executive Summary

This research addresses the critical conflict between the intensive resource demands of Large Language Models (LLMs) and the hardware limitations of edge computing environments. Although LLMs offer sophisticated inference capabilities, their high requirements for GPU memory and energy often exceed the capacity of distributed edge devices, creating a bottleneck for real-time applications. The core challenge lies in maintaining high inference quality while strictly adhering to Quality-of-Service (QoS) requirementsâ€”specifically low latency and energy efficiencyâ€”across heterogeneous edge infrastructure. Without intelligent resource management, the deployment of generative AI at the edge is forced to rely on centralized cloud processing, introducing latency and privacy concerns.

The authors propose a novel **Deep Reinforcement Learning (DRL)** framework utilizing the **Soft Actor-Critic (SAC)** algorithm to create a QoS-aware routing mechanism. This system dynamically routes inference requests to a distributed set of heterogeneous "expert" models (Mixture of Experts) based on real-time operational constraints. To accurately capture the complex state of the system, the framework employs a **Heterogeneous Graph Attention Network (HAN)** for Dynamic State Abstraction, encoding features such as output length, GPU memory availability, and historical latency. The architecture further integrates sophisticated system modeling with **Waiting and Running Queues** featuring Iteration-Level Scheduling (inspired by Orca/vLLM) to optimize KV cache management.

The proposed framework was rigorously evaluated against multiple baselinesâ€”including BERT Router, Round-Robin, Shortest Queue First, and a Baseline RLâ€”using the mix-instruct dataset across models such as Alpaca-native, Chatglm-6b, and Mpt-7b-instruct. The implementation, configured with a 2-layer, 4-head HAN trained for 1 million steps on an RTX 4090, delivered concrete quantitative improvements. The SAC-based routing mechanism achieved a **15.2% improvement in average QoS score** compared to the best-performing heuristic baseline. Additionally, the system reduced the **Average Latency per Token by 21.8%** and increased **GPU Memory Utilization efficiency by over 18%** relative to standard Round-Robin scheduling.

---

> ### âš¡ Quick Facts & Key Metrics
>
> *   **Core Algorithm:** Soft Actor-Critic (SAC) Deep Reinforcement Learning
> *   **Network Architecture:** Heterogeneous Graph Attention Network (HAN)
> *   **Dataset:** mix-instruct
> *   **Training Hardware:** NVIDIA RTX 4090 (1M steps)
> *   **QoS Improvement:** **+15.2%** vs. BERT Router
> *   **Latency Reduction:** **-21.8%** Avg. Latency per Token
> *   **Resource Efficiency:** **+18%** GPU Memory Utilization

---

## ðŸ” Key Findings
*   **Optimization Goal:** The research aims to optimize the trade-off between inference quality and resource usage (Quality-of-Service) within an edge environment.
*   **Core Challenge:** It addresses the specific difficulty of deploying computationally expensive Large Language Models (LLMs) on resource-constrained edge devices.
*   **Mixture of Experts:** The solution leverages a distributed system utilizing specialized smaller models rather than relying on a single large monolithic model.
*   **Dynamic Adaptation:** The system dynamically adapts to varying constraints such as latency, energy consumption, and accuracy requirements.

---

## ðŸ§© Methodology & Contributions

### Proposed Methodology
The paper proposes a comprehensive framework designed to manage LLM inference at the edge:
*   **QoS-Aware Routing Strategy:** An algorithmic approach that determines which "expert" model should handle a specific user request based on current system state and constraints.
*   **Mixture of Experts (MoE):** Utilization of heterogeneous expert models to distribute the computational load effectively.
*   **Deep Reinforcement Learning:** employes DRL to learn optimal routing policies over time, balancing exploration and exploitation of resources.

### Primary Contributions
Based on the analysis, the main contributions of this work include:
1.  **QoS-Aware Routing Mechanism:** Development of an intelligent routing system capable of making real-time decisions based on service constraints.
2.  **MoE Integration:** Successful integration of a Mixture of Experts framework specifically tailored for the limitations of edge computing infrastructure.
3.  **Graph-Based State Representation:** Use of Heterogeneous Graph Attention Networks to effectively model the complex relationships between requests and available edge resources.

---

## âš™ï¸ Technical Details

The implementation of the proposed system involves several sophisticated technical components:

#### 1. Deep Reinforcement Learning (DRL) Framework
*   **Algorithm:** Soft Actor-Critic (SAC).
*   **Function:** Routes LLM inference requests to heterogeneous edge experts by learning a policy that maximizes a specific reward function.

#### 2. Dynamic State Abstraction
*   **Architecture:** Heterogeneous Graph Attention Network (HAN).
*   **Configuration:** 2 layers with 4 attention heads.
*   **Purpose:** Encodes complex operational features including output length, GPU memory availability, and historical latency into a state representation the agent can understand.

#### 3. System Model & Scheduling
*   **Infrastructure:** Edge servers with finite GPU memory.
*   **Queue Management:** Utilizes separate **Waiting** and **Running Queues**.
*   **Scheduling:** Implements **Iteration-Level Scheduling** (inspired by Orca/vLLM) to manage KV caches efficiently, preventing memory fragmentation and optimizing throughput.

#### 4. Reward Function
*   **Objective:** A QoS-aware reward function that maximizes service quality (Response Score).
*   **Penalties:** Heavily penalizes latency violations and inefficient resource usage to ensure strict adherence to Service Level Agreements (SLAs).

---

## ðŸ“Š Evaluation Results

The proposed system was tested against several strong baselines using the **mix-instruct** dataset and various LLMs (**Alpaca-native**, **Chatglm-6b**, **Mpt-7b-instruct**).

### Baselines Compared
*   BERT Router
*   Round-Robin
*   Shortest Queue First
*   Baseline RL

### Performance Metrics
*   **Average QoS (`j`)**: Significantly improved over heuristic methods.
*   **Response Score**: Optimized for higher accuracy and relevance.
*   **Average Latency per Token**: Reduced by **21.8%**.
*   **GPU Memory Utilization**: Increased efficiency by **>18%**.

### Outcome
The proposed algorithm demonstrated superior capability in handling heterogeneous and dynamic workloads, ensuring high-quality inference while respecting the strict resource constraints of edge computing environments.

---
**Document Rating:** 8/10  
**References:** 40 Citations