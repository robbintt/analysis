---
title: Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for
  LLMs
arxiv_id: '2602.01064'
source_url: https://arxiv.org/abs/2602.01064
generated_at: '2026-02-03T19:01:51'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs

*Ruihan Jin; Pengpeng Shao; Zhengqi Wen; Jinyang Wu; Mingkuan Feng; Shuo Yang; Chu Yuan Zhang; Jianhua Tao*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 7/10
> *   **References:** 40 Citations
> *   **Core Concept:** Knowledge Purification (KP)
> *   **Primary Metric:** Average Accuracy
> *   **Datasets:** OBQA, ARC, Riddle, PQA
> *   **Teacher Models Used:** FLAN-T5, Llama 2-chat, BioMistral-7B, Llama-3.1-8B-Instruct

---

## Executive Summary

This research addresses the critical challenge of **"knowledge conflicts"** inherent in Multi-Teacher Knowledge Distillation (MTKD) for Large Language Models (LLMs). While distilling knowledge from multiple diverse teacher models theoretically offers a student model a broader knowledge base, conflicting rationales and outputs from different teachers often degrade performance rather than enhance it. The authors highlight a significant scalability issue: as the number of teachers increases, the student model suffers from information overload and contradiction, leading to a measurable decline in accuracy—specifically dropping to the **40.0–44.0 range**.

The core innovation proposed is the **"Knowledge Purification" (KP) framework**, a strategy designed to aggregate and consolidate multiple, potentially conflicting teacher rationales into a single, unified rationale before distillation. Technically, the framework transforms multi-source knowledge using a purification function, optimizing a combined loss function defined as:

$$L_{MTKD-KP} = L_{PR} + \lambda L_{DL-KP}$$

Where $L_{PR}$ represents the prediction loss and $L_{DL-KP}$ represents the distillation loss calculated on the purified knowledge. The authors developed and evaluated five distinct purification methods to achieve this consolidation: Knowledge Aggregation via a global LLM, Plackett-Luce Ranking, a PLM Classifier, a Similarity-based Router, and an RL-based Teacher Selection agent.

Experimental analysis utilizing the TinyLLM framework quantified the severity of knowledge conflicts and the efficacy of the proposed solution. Evaluations were conducted on Multiple Choice Question Answering (MCQA) datasets (OBQA, ARC, Riddle, and PQA). The authors confirmed that while multi-teacher distillation without purification caused accuracy to plummet, the implementation of the five purification methods successfully counteracted this decline. Notably, the **Router-based methods** performed best, demonstrating superior generalization capabilities. By enabling the effective use of multiple teachers without the associated penalty of knowledge conflicts, this work paves the way for developing more powerful, lightweight student models.

---

## Key Findings

*   **Conflict Resolution:** The proposed "Knowledge Purification" concept effectively alleviates knowledge conflicts arising from distilling knowledge from multiple teacher models.
*   **Performance Improvement:** The five proposed purification methods successfully resolve conflicts and improve overall student model performance.
*   **Robust Generalization:** Router-based purification methods demonstrated robust generalization capabilities compared to other approaches.
*   **Resource Efficiency:** Purification consolidates multiple rationales into a single rationale, addressing the high resource demands and processing overhead of traditional multi-teacher distillation.

---

## Methodology

The authors introduced the **"Knowledge Purification" framework** to streamline the distillation process. The core methodology involved:

1.  **Aggregation and Consolidation:** Developing a mechanism to aggregate and consolidate rationales from multiple teacher LLMs into a single, unified rationale.
2.  **Method Evaluation:** Proposing and evaluating five distinct purification methods derived from various analytical perspectives.
3.  **Performance Measurement:** Testing these methods in a multi-teacher distillation setting to specifically measure their impact on efficiency and conflict reduction.

---

## Contributions

*   **Formalization:** Definition and formalization of "Knowledge Purification" as a strategy to resolve the tension between leveraging multiple teachers and mitigating knowledge conflicts.
*   **Technique Development:** Development of five specific purification techniques to handle different types of rationale conflicts.
*   **Optimization Identification:** Identification of router-based methods as particularly effective for generalization, helping to optimize the distillation process.
*   **Practical Deployment:** Advancement of techniques enabling the practical deployment of powerful, lightweight models by making the distillation process significantly more efficient.

---

## Technical Details

**Framework:** Knowledge Purification (KP) for Multi-Teacher Knowledge Distillation (MTKD).

**Objective:** Mitigate knowledge conflicts by transforming multiple teacher rationales into a single consolidated rationale using a purification function.

**Loss Function:**
$$L_{MTKD-KP} = L_{PR} + \lambda L_{DL-KP}$$

**Five Proposed Purification Methods:**

1.  **Knowledge Aggregation:** Uses a global LLM to synthesize inputs.
2.  **Plackett-Luce Ranking:** Utilizes probabilistic rationale ranking.
3.  **PLM Classifier:** Employs a PLM encoder and MLP to calculate routing probabilities.
4.  **Similarity-based Router:** Uses cosine similarity between question and teacher embeddings.
5.  **RL-based Teacher Selection:** Utilizes a reinforcement learning agent to select the optimal teacher.

---

## Results

*   **Performance Degradation:** Experimental analysis on the TinyLLM framework (extended with FLAN-T5 xlarge, Llama 2-chat, BioMistral-7B, and Llama-3.1-8B-Instruct) showed that performance significantly declines as the number of teachers increases without purification.
*   **Quantitative Drop-off:** A graph on Page 2 illustrates the performance drop-off (range **40.0–44.0**) as more teachers are added without purification, confirming the impact of knowledge conflicts.
*   **Evaluation Tasks:** Testing was conducted on Multiple Choice Question Answering (MCQA) tasks using datasets **OBQA, ARC, Riddle, and PQA**.
*   **Metric:** Average Accuracy was used as the primary evaluation metric.

---
*Quality Score: 7/10 | References: 40*