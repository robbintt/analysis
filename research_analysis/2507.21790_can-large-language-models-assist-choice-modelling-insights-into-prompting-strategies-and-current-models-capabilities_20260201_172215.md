# Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities

***Georges Sfeir; Gabriel Nova; Stephane Hess; Sander van Cranenburgh***

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Models Evaluated** | 13 versions of 6 leading LLMs |
| **Focus Area** | Discrete Choice Models (MNL) |
| **Top Performer (Fit)** | Claude 3.5 Sonnet |
| **Most Robust** | GPT Models |
| **References** | 10 Citations |

---

## Executive Summary

Discrete Choice Modelling (DCM), specifically the specification of Multinomial Logit (MNL) models, is a sophisticated, manual process requiring experts to map observed data to utility functions while determining linearity, interactions, and heterogeneity. This paper addresses the critical question of whether **Large Language Models (LLMs)** can function as viable assistive agents in this domain to reduce the manual burden and expert bias associated with model specification. This inquiry is significant because integrating LLMs into econometric workflows promises to increase efficiency and accessibility but requires rigorous validation against the mathematical and theoretical standards of behavioral science.

The authors introduce a systematic benchmarking framework that treats DCM specification as a semi-structured workflow suitable for Transformer architectures. The technical novelty lies in a comprehensive evaluation of thirteen versions of six leading LLMs (including GPT-4o and Claude 3.5 Sonnet) across a multi-dimensional experimental design. This design varies the **modelling goal** (suggestion vs. estimation), **prompting paradigm** (Zero-Shot vs. Chain-of-Thought), and **information availability** (ingestion of full datasets versus data dictionaries only). The approach goes beyond simple accuracy checks to assess model validity, **complexity** (evaluating parsimony and the risk of overfitting), and **behavioral soundness**, defined as the theoretical plausibility of estimated parameters (e.g., confirming a negative marginal utility for cost).

The study reveals a distinct performance hierarchy where proprietary models substantially outperformed open-weight models. **Claude 3.5 Sonnet** achieved the highest statistical performance, generating specifications with a McFaddenâ€™s $\rho^2$ (goodness-of-fit) comparable to the expert benchmark, achieving values of approximately 0.30â€“0.33 on standard datasets like SwissMetro. **GPT-4o** demonstrated the highest overall robustness, while open-weight models frequently failed validity checks. A key quantitative finding was that providing LLMs solely with a data dictionary (mean attributes) often yielded superior behavioral soundness compared to inputting the full dataset. Furthermore, GPT-4o uniquely demonstrated full end-to-end capabilities by successfully generating and executing Python code for model estimation, a feat other models failed to accomplish reliably.

This research establishes the foundational potential of LLMs as practical tools in the choice modelling toolkit, proving they can provide tangible utility in MNL specification beyond theoretical speculation. By delineating the limitations of current open-weight models and identifying the superiority of specific proprietary counterparts and data-input strategies, the paper offers immediate, actionable workflow guidance for practitioners. The findings influence the field by validating specific prompting strategies and setting a new benchmark for future research into AI-augmented econometric analysis, demonstrating that LLMs can effectively adhere to the rigorous behavioral requirements of decision science.

---

## Key Findings

*   **Proprietary vs. Open:** Proprietary LLMs successfully generated valid and behaviourally sound utility specifications, whereas open-weight models struggled significantly.
*   **Best Performing Model:** **Claude 3.5 Sonnet** produced the best-fitting models, achieving goodness-of-fit comparable to expert benchmarks.
*   **Robustness:** **GPT models** were identified as the most robust across different testing scenarios.
*   **Data Efficiency:** Some LLMs performed better when provided with only a data dictionary (mean attributes) rather than the full dataset.
*   **End-to-End Capability:** **GPT o3** uniquely demonstrated end-to-end capability by estimating models via self-generated code.

---

## Methodology

The study employed a systematic evaluation framework involving:

*   **Scope:** Thirteen versions of six leading LLMs were tested.
*   **Configurations:** Five different configurations were tested across three key dimensions:
    1.  **Modelling Goal:** Suggestion vs. Estimation.
    2.  **Prompting Strategy:** Zero-Shot vs. Chain-of-Thoughts.
    3.  **Information Availability:** Full dataset vs. Data Dictionary only.
*   **Process:** Specifications generated by LLMs were implemented and estimated.
*   **Assessment Criteria:** Models were graded on goodness-of-fit, behavioral plausibility, and complexity.

---

## Technical Details

*   **Target Domain:** Discrete Choice Models (DCMs), specifically Multinomial Logit (MNL).
*   **Workflow Nature:** Treated as a semi-structured process.
*   **LLM Task:** Map observed data to utility functions by deciding on:
    *   Linearity
    *   Interactions
    *   Heterogeneity
*   **Framework Assessment:**
    *   *Specification Assistance*
    *   *End-to-End Execution*
    *   *Input Comparison:* Full datasets vs. Data dictionaries.
*   **Prompting Paradigms:**
    *   Zero-Shot
    *   Chain-of-Thought
*   **Architecture & Comparison:**
    *   Utilizes Transformer architectures.
    *   Benchmarked against static metaheuristics, grammar-based approaches, and ML methods (SHAP and RL).

---

## Results

*   **Performance Leader:** **Claude 3.5 Sonnet** produced specifications with the best statistical fit (McFaddenâ€™s $\rho^2$).
*   **Robustness Leader:** **GPT models** showed the highest overall robustness.
*   **Model Class Performance:** Proprietary LLMs generally outperformed open-weight models in generating valid specifications.
*   **Input Strategy:** Data dictionaries often yielded better performance than full datasets regarding behavioral soundness.
*   **Capabilities:** **GPT o3** uniquely demonstrated end-to-end capability by generating and executing code for model estimation.
*   **Evaluation Metrics:** Models were evaluated on:
    *   *Validity:* Mathematical correctness.
    *   *Behavioral Soundness:* Theoretical plausibility.

---

## Contributions

1.  **Established Potential:** Confirms the potential of LLMs as assistive agents in choice modelling for Multinomial Logit models.
2.  **Benchmarking:** Provides comprehensive benchmarking of current capabilities and limitations regarding model specification.
3.  **Practical Guidance:** Offers actionable workflow guidance, including specific recommendations on model selection and prompting strategies.

---

**References:** 10 citations