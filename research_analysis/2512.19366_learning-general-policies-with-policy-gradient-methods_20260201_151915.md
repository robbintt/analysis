# Learning General Policies with Policy Gradient Methods

*Simon StÃ¥hlberg; Blai Bonet; Hector Geffner*

***

### QUICK FACTS

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total References** | 15 Citations |
| **Core Methodology** | Actor-Critic, Graph Neural Networks (GNNs) |
| **Research Domain** | Deep Reinforcement Learning & Classical Planning |
| **Key Problem** | Systematic Generalization in DRL |

***

## Executive Summary

> This research addresses the critical challenge of achieving systematic generalization in Deep Reinforcement Learning (DRL), a hurdle where classical symbolic planning methods traditionally excel but DRL often struggles. While classical approaches can generate general policies applicable to unseen problem instances using first-order representations, DRL typically depends on fixed vector inputs, limiting its ability to scale and generalize across varying problem sizes and structures. The authors aim to resolve the disconnect between these fields, investigating whether the inability of DRL to generalize is an inherent flaw in the learning algorithms or a consequence of architectural constraints and problem formulation.
>
> The key innovation is a hybrid framework that models general policies as state transition classifiers, using Graph Neural Networks (GNNs) to approximate value functions within an Actor-Critic architecture. Technically, the approach treats policy learning as the discrimination between "good" transitions (where value decreases, $V(s') < V(s)$) and "bad" transitions, utilizing Stochastic Gradient Descent to scale optimization. Framing DRL as an approximation of Dynamic Programming (DP), the authors introduce algorithm-agnostic interventions to overcome expressivity limitations: specifically, the use of derived predicates and alternative cost structures. These additions allow the model to extract relevant features from the environment without requiring explicit, large-scale feature pools characteristic of classical methods.
>
> The study primarily presents theoretical findings rather than extensive quantitative benchmarks, defining policy validity through the metric of value function reduction against plan cost. A fundamental result demonstrates an inescapable tradeoff between generalization and optimality; specifically, the authors prove that optimal general policies cannot exist for arbitrary instances (such as Blocksworld) under polynomial-time constraints due to computational tractability. The analysis identifies that DRL generalization failures are caused by the expressive limitations of GNNs and these inherent tradeoffs, rather than the policy gradient algorithms themselves. While specific numerical performance data is absent from the provided text, the work establishes that introducing derived predicates effectively mitigates these limitations by enabling the network to distinguish state transitions that standard features miss.

***

## Key Findings

*   **Scalable Generalization:** Actor-critic methods can generalize effectively without suffering from the scalability issues often associated with classical planning methods.
*   **Root Cause of DRL Limitations:** Failures in DRL generalization are not caused by the RL algorithms themselves, but rather by the expressive limitations of GNNs and fundamental generalization-optimality tradeoffs.
*   **Generalization vs. Optimality:** There is a fundamental tradeoff where general policies may be required to sacrifice optimality in specific instances to maintain general applicability.
*   **Mitigation Strategies:** These limitations can be effectively mitigated using derived predicates and alternative cost structures without the need to alter the core DRL algorithms.

***

## Methodology

The proposed methodology integrates classical planning principles with deep learning techniques through a hybrid approach:

1.  **Policy Modeling:** Policies are modeled as classifiers of state transitions, distinguishing between beneficial and detrimental moves.
2.  **Neural Representation:** Graph Neural Networks (GNNs) are utilized to represent value functions, allowing the system to handle complex state structures.
3.  **Optimization Framework:** An actor-critic method is employed for optimization, leveraging Stochastic Gradient Descent to handle scalability.
4.  **Structural Refinement:** The method incorporates derived predicates and alternative cost structures to refine the learning process and improve feature extraction.

***

## Technical Details

The approach represents general policies by classifying state transitions $(s, s')$ into **good** and **bad** categories.

*   **Architecture:** Uses Graph Neural Networks (GNNs) to approximate value functions within an Actor-Critic framework.
*   **Optimization:** Utilizes Stochastic Gradient Descent to process non-symbolic states and handle scalability.
*   **Training Paradigm:** Training is unsupervised, employing policy optimization algorithms derived from Dynamic Programming (DP).
*   **Theoretical Framing:** The DRL approach is framed as an approximation of exact DP, where the network selects features to distinguish between transitions.

***

## Theoretical Results

*Note: The provided text focuses on theoretical metrics and findings rather than specific quantitative experimental data.*

*   **Validity Metrics:** A valid policy is defined by the value function metric $V(s)$, requiring that a valid transition satisfies $V(s') < V(s)$. Plan Cost is defined by action sequence length.
*   **Generalization-Optimality Tradeoff:** A key theoretical finding is that optimal general policies do not exist for arbitrary instances (e.g., Blocksworld) under polynomial-time constraints due to computational intractability.
*   **Optimization:** The text provides Bellman equations for optimization contexts.
*   **Analysis of Failure Modes:** Identifies GNN expressive limitations and the generalization-optimality tradeoff as the primary sources of Deep RL limitations.

***

## Contributions

1.  **Cross-Disciplinary Bridge:** Successfully bridges the fields of Deep Reinforcement Learning and classical planning to define conditions for systematic generalization.
2.  **Clarification of Limitations:** Clarifies that DRL generalization failures stem from neural architecture expressivity and domain constraints, rather than the algorithms themselves.
3.  **Scalable Pathway:** Provides a scalable path to general policies that avoids the necessity for extensive feature pools.
4.  **Algorithm-Agnostic Solutions:** Offers interventions such as derived predicates and cost structure modifications that improve generalization without changing the underlying RL algorithms.