---
title: 'TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents'
arxiv_id: '2507.01823'
source_url: https://arxiv.org/abs/2507.01823
generated_at: '2026-02-03T18:54:48'
quality_score: 9
citation_count: 36
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents

*Dmytro Kuzmenko; Nadiya Shvai*

---

> ### **Quick Facts**
> * **Benchmark:** MT30 (30 Tasks)
> * **Teacher Parameters:** 317M
> * **Student Parameters:** 1M
> * **Compression Rate:** ~99.7%
> * **Top Score:** 28.45 (Normalized)
> * **Quantization:** FP16 (50% size reduction)
> * **Improvement vs Baseline:** +48.5%

---

## Executive Summary

The research addresses the critical challenge of deploying large-scale, model-based reinforcement learning (RL) agents in resource-constrained environments. While high-capacity "world models" like TD-MPC2 have demonstrated exceptional performance in multi-task control, their massive parameter counts—often exceeding hundreds of millions—render them impractical for real-world applications where compute power and memory are limited. Bridging the gap between the high performance of large foundation models and the efficiency required for edge deployment is a significant bottleneck in the field of embodied AI.

The authors introduce **TD-MPC-Opt**, a knowledge distillation framework designed to transfer complex policy capabilities from a massive teacher agent to a highly compact student model. Technically, the method employs a 317M-parameter pre-trained TD-MPC2 agent as a teacher to train a student model with only 1M parameters. The training objective integrates the standard TD-MPC2 loss components (consistency, reward, and value) with a specific distillation loss defined as the Mean Squared Error (MSE) between the teacher's and student's reward predictions, weighted by a coefficient of 0.4.

To further optimize the model for deployment, the authors apply FP16 post-training quantization, effectively halving the model's memory footprint. Evaluations on the MT30 benchmark, which spans 30 distinct tasks, demonstrate that the proposed method successfully retains performance despite extreme compression. The distilled 1M-parameter student model achieved a normalized score of 28.45, representing a substantial 48.5% improvement over the baseline score of 18.93 and a 2.77% improvement over a 1M-parameter model trained from scratch (27.36).

These results confirm that the approach can compress an agent by over 99% (from 317M to 1M parameters) while actually enhancing performance relative to training small models independently. This work sets a new standard for efficiency in multi-task reinforcement learning, proving that complex knowledge representations from large world models can be effectively consolidated into architectures with as few as 1 million parameters. By providing a practical solution for deploying high-performing agents on hardware-constrained devices, the research broadens the applicability of advanced model-based RL to robotics and embedded systems.

---

## Key Findings

*   **State-of-the-Art Performance:** Achieved a normalized score of **28.45** on the MT30 benchmark, significantly surpassing the baseline score of 18.93.
*   **Extreme Compression:** Successfully compressed a massive 317M parameter multi-task agent into a highly compact 1M parameter model while retaining (and improving) performance.
*   **Quantization Efficiency:** Reduced the distilled model size by approximately 50% using FP16 post-training quantization.
*   **Knowledge Consolidation:** Demonstrated that the distillation technique effectively captures and consolidates complex multi-task knowledge from large world models.

---

## Methodology

The research introduces a knowledge transfer framework for model-based reinforcement learning. The core methodology involves efficiently distilling a large, high-capacity teacher agent into a compact student model using the MT30 benchmark. To further address resource constraints, the authors apply FP16 post-training quantization to the distilled model, optimizing it for size and efficiency without sacrificing performance.

1.  **Teacher-Student Framework:** Utilizes a pre-trained TD-MPC2 agent (317M parameters) to guide a smaller student agent (1M parameters).
2.  **Distillation Process:** The student learns to mimic the reward predictions of the teacher while maintaining its own reinforcement learning objectives.
3.  **Post-Training Optimization:** Applies FP16 quantization to the final model to reduce memory footprint for deployment.

---

## Technical Details

### System Architecture
*   **Teacher Model:** Pre-trained TD-MPC2 (317M parameters)
*   **Student Model:** Compact architecture (1M parameters)

### Loss Function
*   **Components:** 
    *   Original TD-MPC2 loss (consistency, reward, value)
    *   Distillation loss (MSE between teacher and student reward predictions)
*   **Weighting Coefficient:** 0.4 for the distillation loss component.

### Training Configuration
*   **Training Steps:** Up to 1,000,000 steps
*   **Batch Size:** Ranging from 128 to 1024

### Optimization Techniques
*   **Methods Tested:** FP16, Mixed Precision, and INT8.
*   **FP16 Impact:** Reduces model size by approximately 50% with minimal performance degradation.

---

## Performance Results

On the MT30 benchmark (30 tasks), the distilled FP16 1M parameter model demonstrated superior efficiency and capability:

*   **Benchmark Score:** Achieved a normalized score of **28.45**.
*   **vs. Baseline:** Represents a **48.5% improvement** over the baseline score of 18.93.
*   **vs. Scratch Training:** Achieved a **2.77% improvement** compared to training the 1M model from scratch (27.36).
*   **Compression Success:** Successfully compressed the agent from 317M parameters down to 1M parameters (>99% compression).

---

## Contributions

*   **Practical Deployment Solution:** Provides a viable method for deploying large world models in resource-constrained environments.
*   **New Performance Standard:** Establishes a new benchmark for compact models by demonstrating high capability in a 1M parameter architecture.
*   **Novel Insights:** Offers new perspectives on knowledge representation within large world models and its consolidation into smaller architectures.
*   **Open Source:** Contributes to the community by releasing the code for reproducibility and further research.

---

**Quality Score:** 9/10  
**References:** 36 citations