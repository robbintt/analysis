---
title: 'MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment'
arxiv_id: '2506.14199'
source_url: https://arxiv.org/abs/2506.14199
generated_at: '2026-02-03T12:38:44'
quality_score: 9
citation_count: 18
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MAS-LitEval: Multi-Agent System for Literary Translation Quality Assessment

*Junghwan Kim; Kieun Park; Sohee Park; Hyungguk Kim; Bongwon Suh*

***

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 18 Citations
> *   **Top Performing Model:** claude-3.7-sonnet (OTQS: 0.890)
> *   **Evaluation Focus:** Terminology, Narrative, & Style
> *   **Key Innovation:** Reference-free Multi-Agent System (MAS)

***

## Executive Summary

### Problem
This paper addresses the fundamental inadequacy of traditional automated evaluation metricsâ€”such as BLEU, METEOR, and ROUGEâ€”in assessing literary translations. These legacy metrics rely heavily on lexical overlap, which allows them to measure surface-level accuracy but renders them incapable of capturing narrative consistency, stylistic fidelity, or cultural nuance. This limitation presents a significant challenge for the field of Translation Quality Assessment (TQA), as high n-gram overlap does not correlate with the preservation of the artistic and lyrical qualities essential to literature. Consequently, there is a critical evaluation gap where current automated tools fail to distinguish between a technically accurate translation and a high-quality literary one.

### Innovation
To bridge this gap, the authors introduce **MAS-LitEval**, a reference-free Multi-Agent System (MAS) powered by Large Language Models (LLMs). The framework utilizes a parallel processing architecture comprising three specialized agents:
*   **Terminology Consistency** (employing NER)
*   **Narrative Perspective Consistency**
*   **Stylistic Consistency**

A coordinator agent aggregates the outputs of these agents into an Overall Translation Quality Score (OTQS) using a weighted formula ($OTQS = 0.3 \times S_T + 0.3 \times S_N + 0.4 \times S_S$), assigning the highest priority to stylistic elements. The system processes text in 4096-token chunks with global context management, implemented in Python using spaCy and LLM APIs to ensure scalability and depth of analysis.

### Results
In Korean-to-English translation experiments on *The Little Prince* and *A Connecticut Yankee in King Arthur's Court*, MAS-LitEval provided a more granular assessment of literary quality than traditional metrics. The **claude-3.7-sonnet** model achieved the highest OTQS (0.890 and 0.880, respectively), followed by gpt-4o, while open-source models like Llama-3.1-8B scored significantly lower (~0.71).

Crucially, the system exposed the limitations of lexical metrics: while gpt-4o achieved a higher BLEU score (0.30) than claude-3.7-sonnet (0.28) on *The Little Prince*, MAS-LitEval ranked claude-3.7-sonnet higher due to its superior lyrical fidelity. The OTQS demonstrated strong correlation with the human-centric WMT-KIWI metric (0.93) but weak correlation with BLEU (0.62), METEOR (0.70), and ROUGE.

### Impact
The introduction of MAS-LitEval represents a paradigm shift in the automated evaluation of literary texts, moving the field beyond rigid lexical matching toward a holistic understanding of translation quality. By providing a scalable tool that effectively measures narrative and stylistic preservation, this framework empowers researchers and translators to rigorously assess cultural nuances in machine-translated literature. This work not only offers a functional solution for current TQA challenges but also sets a new standard for developing LLMs capable of handling the complexities of creative and artistic translation.

***

## Key Findings

*   **Inadequacy of Traditional Metrics:** Metrics like BLEU and METEOR fail to capture narrative consistency and stylistic fidelity due to their sole focus on lexical overlap.
*   **Superior Performance:** The proposed MAS-LitEval system outperformed traditional metrics in assessing literary translations.
*   **High Achievement:** Top-performing models evaluated by MAS-LitEval achieved scores of up to **0.890** in capturing literary nuances.
*   **Multi-Dimensional Assessment:** The system effectively assesses translations across three critical dimensions:
    *   Terminology
    *   Narrative
    *   Style

## Methodology

The study implements a Multi-Agent System (MAS) utilizing Large Language Models (LLMs). The framework evaluates translations based on three specific pillars: terminology, narrative, and style.

**Testing Protocol:**
*   **Corpus:** Translations of two distinct literary works: *The Little Prince* and *A Connecticut Yankee in King Arthur's Court*.
*   **Source Models:** Translations generated by various LLMs.
*   **Benchmarking:** Performance was measured by direct comparison against traditional lexical overlap metrics.

## Contributions

*   **New Framework:** Introduction of **MAS-LitEval**, a scalable and nuanced framework specifically designed for Translation Quality Assessment (TQA) in literary contexts.
*   **Bridging the Gap:** Addressing the critical oversight in current automated metrics by prioritizing narrative consistency and stylistic fidelity over simple lexical matching.
*   **Functional Tool:** Providing a practical tool for translators and researchers to better assess the preservation of cultural nuances and stylistic elements in machine-translated literature.

## Technical Details

**System Architecture**
*   **Type:** Reference-free, multi-agent system utilizing parallel processing.
*   **Components:** Three specialized agents and one coordinator.
    *   **Terminology Consistency Agent:** Uses NER (Weight: 0.3)
    *   **Narrative Perspective Consistency Agent:** (Weight: 0.3)
    *   **Stylistic Consistency Agent:** (Weight: 0.4)
    *   **Coordinator:** Aggregates agent scores.

**Scoring Formula**
The Overall Translation Quality Score (OTQS) is calculated on a scale of 0 to 1:
$$OTQS = 0.3 \times S_T + 0.3 \times S_N + 0.4 \times S_S$$

**Implementation Specs**
*   **Chunking:** Texts are processed in 4096-token chunks with global context management.
*   **Tech Stack:** Implemented in Python using spaCy and LLM APIs.

## Results

**Model Performance (Korean-to-English)**
*   **claude-3.7-sonnet:** Highest OTQS (0.890 for *The Little Prince*, 0.880 for *A Connecticut Yankee...*).
*   **gpt-4o:** Followed closely as the second-highest performer.
*   **Llama-3.1-8B:** Scored significantly lower, approximately 0.71.

**Metric Correlation**
*   **Strong Correlation:** OTQS vs. WMT-KIWI (**0.93**)
*   **Weak Correlation:**
    *   OTQS vs. BLEU: 0.62
    *   OTQS vs. METEOR: 0.70
    *   OTQS vs. ROUGE: [Low]

**Qualitative Insight**
In *The Little Prince* experiment, gpt-4o achieved a BLEU score of 0.30 compared to claude-3.7-sonnet's 0.28. However, MAS-LitEval ranked claude-3.7-sonnet higher, attributing the success to better **lyrical fidelity** and **poetic nuance**.

---
**Paper Rating:** 9/10 | **References:** 18 Citations