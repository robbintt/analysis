# Can the capability of Large Language Models be described by human ability? A Meta Study
*Mingrui Zan; Yunquan Zhang; Boyang Zhang; Fangming Liu; Daning Cheng*

---

> ### ðŸ“Š Quick Facts
>
> *   **Models Analyzed:** >80 (Focus on 52 models â‰¤ 7B parameters)
> *   **Benchmarks Utilized:** 37 (mapped to human abilities)
> *   **Taxonomy:** 6 Primary Abilities, 11 Sub-abilities
> *   **Core Metric:** Spearmanâ€™s Rank Correlation Coefficient ($r_s$)
> *   **Quality Score:** 9/10

---

## Executive Summary

Current evaluation protocols for Large Language Models (LLMs) frequently rely on benchmarks designed to measure human cognitive capabilities, implicitly assuming that skills in artificial systems mirror the interrelated structure of human intelligence. This paper addresses the critical question of whether human-centric ability metrics serve as a valid ontology for describing LLM capabilities. This issue is significant because if LLMs acquire and organize skills differently than humans, applying frameworks designed for biological cognition may lead to incorrect conclusions about a model's general intelligence, safety, and reliability in real-world applications.

The key innovation is a meta-analytical framework that maps 37 diverse evaluation benchmarks onto a hierarchical taxonomy of human abilities, comprising 6 primary and 11 sub-abilities. To ensure a comprehensive scope, the study aggregates performance data from over 80 open-source Large Language Models, providing a broad empirical basis. To isolate structural characteristics from raw performance differences, the authors utilized Spearmanâ€™s Rank Correlation Coefficient ($r_s$) rather than absolute scores. The study employs a distance metric defined as $d_s = \sqrt{2(1 - r_s)}$ to perform Hierarchical Clustering on model performance rankings. This methodological choice mitigates the confounding effects of scaling laws and varying benchmark score ranges, allowing for a direct comparison between machine-generated clusters and the human-ability-based classifications.

The analysis confirms that while human ability metrics effectively categorize capabilities in models with fewer than 10 billion parameters, the underlying cognitive architecture differs fundamentally from humans. While distinct human abilities typically exhibit moderate to strong correlations ($|r_s| \ge 0.3$), the same abilities within LLMs show near-zero correlation ($|r_s| < 0.3$), suggesting disjointed rather than integrated skills. Furthermore, the research demonstrates that LLM capabilities are not static but vary dynamically across parameter scales; the structure and correlation of skills evolve non-linearly as model size increases.

This research provides empirical evidence that the "cognitive structure" of LLMs is fundamentally distinct from that of humans, challenging the anthropomorphic view of artificial intelligence. By demonstrating that LLM capabilities are largely uncorrelated and scale-dependent, the paper cautions against inferring general intelligence from narrow task performance. The established framework offers the field a standardized lens for model evaluation, urging researchers and developers to consider the fragmented nature of LLM skills when designing training regimes or assessing model robustness across different domains.

---

## Key Findings

*   **Applicability of Human Metrics:** It is confirmed that the capabilities of LLMs with fewer than 10 billion parameters can be effectively described using human ability metrics.
*   **Divergence in Ability Correlations:** While certain abilities are interrelated in humans, these same abilities appear nearly uncorrelated within LLMs, suggesting a fundamental structural difference in how skills are acquired or organized in artificial models.
*   **Scale-Dependent Capabilities:** The capabilities exhibited by LLMs are not static but vary significantly depending on the parameter scale of the model.

---

## Methodology

*   **Meta-Study Data Collection:** Aggregated performance data from over 80 Large Language Models.
*   **Benchmark Categorization:** Utilized 37 evaluation benchmarks, categorized into a hierarchical structure of 6 primary abilities and 11 sub-abilities based on human aspects.
*   **Cluster Analysis:** Clustered the resulting performance rankings into several categories and compared the machine-generated clusters against the human-ability-based classifications to assess alignment.

---

## Technical Details

**Framework Definition**
The paper proposes a framework to taxonomize LLM capabilities by human abilities, defining ability as consistent performance across task variations.

**Data Scope & Normalization**
*   **Benchmarks:** 37 benchmarks mapped to 6 primary and 11 sub-ability categories, covering over 80 models.
*   **Scaling Mitigation:** To mitigate scaling law effects, the analysis focuses on models with fewer than 10 billion parameters (specifically 52 models â‰¤ 7B).
*   **Metric Selection:** Absolute scores are discarded in favor of relative rankings using Spearmanâ€™s Rank Correlation Coefficient.

**Clustering Algorithm**
Hierarchical Clustering is applied using a distance metric derived from Spearmanâ€™s coefficient:
$$d_s = \sqrt{2(1 - r_s)}$$
This metric is used to identify structural patterns and compare machine-generated clusters against human taxonomies.

---

## Results

The study defines correlation thresholds as **weak** ($|r_s| < 0.3$), **moderate** ($0.3 \le |r_s| < 0.7$), and **strong** ($|r_s| \ge 0.7$). Due to high variance in score ranges across benchmarks (e.g., OpenBookQA vs. Math), ranking-based metrics were deemed essential.

Key observations include:
*   Human metrics can describe capabilities in models < 10B parameters.
*   LLM abilities show near-zero correlation with one another, diverging from the interrelated nature of human abilities.
*   Capabilities are scale-dependent.
*   The analysis categorizes abilities into six groups: **Natural Language**, **Examination**, **Reasoning**, **Knowledge**, **Multilingual**, and **Coding**.

---

## Contributions

*   **Framework for LLM/Human Comparison:** Established a structured framework that maps LLM evaluation benchmarks to specific human abilities (primary and sub-abilities), providing a new lens for model evaluation.
*   **Insight into Cognitive Architecture:** Provided empirical evidence regarding the differences in cognitive structure between biological and artificial intelligence, specifically regarding the correlation of distinct skills.
*   **Parameter Scale Analysis:** Offered specific insights into how the "human-ness" or applicability of human metrics to LLMs changes as model size increases (validating it for <10B parameters).

---
*Report generated based on 10 citations.*