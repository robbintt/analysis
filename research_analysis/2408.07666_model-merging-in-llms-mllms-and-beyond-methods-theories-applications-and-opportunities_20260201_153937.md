# Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities

*Enneng Yang; Li Shen; Guibing Guo; Xingwei Wang; Xiaochun Cao; Jie Zhang; Dacheng Tao*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Primary Contribution:** First comprehensive taxonomic framework for model merging.
> *   **Key Technique:** Task Arithmetic & Task Vectors.
> *   **Operational Metric:** 1Ã— Parameter Compression (Zero inflation).
> *   **Resource Availability:** Public GitHub repository curated.

---

## Executive Summary

Adapting Large Language Models (LLMs) and Multimodal LLMs (MLLMs) for specialized tasks traditionally requires costly retraining and access to massive proprietary datasets. While model merging offers a compelling alternativeâ€”synthesizing multiple expert models into a single unified model without raw data or significant retrainingâ€”the field suffers from severe fragmentation. With techniques spread across more than ten machine learning subdomains, a lack of standardized taxonomy has hindered the adoption and systemic improvement of these methods. This paper addresses this gap by providing the first comprehensive review to consolidate the literature, clarifying how disparate merging techniques relate to one another and establishing a unified theoretical framework for the field.

The paperâ€™s core contribution is a novel taxonomic framework that categorizes the merging process into **Pre-Merging** and **During Merging** stages. Pre-merging focuses on preparing models for compatibilityâ€”such as aligning weight basins or transforming architecturesâ€”while During Merging strategies are classified by how they resolve parameter interference. Rather than simply listing acronyms, the survey distinguishes methods by their approach to conflict resolution: Basic Merging utilizes task vectors for arithmetic operations; Subspace-based methods (e.g., Ties-Merging) prune irrelevant parameters; and Optimization-based methods use gradient updates to minimize loss. This structure reframes the problem from simple weight averaging to the geometric manipulation of model parameters, establishing Task Vectors as a fundamental component for enabling complex capabilities like analogical reasoning and targeted forgetting.

The surveyâ€™s comparative analysis highlights critical performance trade-offs between efficiency and accuracy. A key operational finding is that successful merging consistently compresses multi-expert knowledge into a **$1\times$ parameter model**, maintaining inference efficiency without bloat. However, performance varies significantly by methodology: naive weight averaging is shown to be generally inadequate due to parameter interference, whereas Task Arithmetic remains highly effective for multi-task learning. The results further distinguish methods by resource intensity; Tuning-Free approaches (e.g., Ties-Merging) offer the highest efficiency by requiring no gradient updates and often operating without data, while Training-Based methods (e.g., AdaMerging) achieve robustness at the cost of additional optimization cycles and labeled data dependency.

By mapping the application of merging techniques across LLMs, MLLMs, continual learning, and few-shot learning, this paper provides a roadmap that transforms model merging from a set of isolated tricks into a standardized discipline. The establishment of a clear taxonomy and the release of a curated open-source repository of papers and theories accelerate the adoption of merging as a viable, resource-efficient strategy for model empowerment. Ultimately, this work guides future research toward addressing the remaining challenges in interference management, promoting a shift away from expensive retraining toward more sustainable, composition-based AI development.

---

## Key Findings

*   **Efficient Empowerment:** Model merging is identified as a highly efficient technique that bypasses the need for raw training data collection and expensive computational resources.
*   **Literature Gap Addressed:** The study fills a significant void in the current literature by providing the first systematic and thorough review of model merging techniques.
*   **Broad Applicability:** Techniques have wide-ranging applications, currently spanning Large Language Models (LLMs), Multimodal LLMs (MLLMs), and over ten distinct machine learning subfields (e.g., continual learning, multi-task learning, few-shot learning).
*   **Remaining Challenges:** Despite its prevalence, the field faces unresolved challenges that require further investigation and resolution.

---

## Methodology

*   **Comprehensive Survey:** The authors conducted a systematic review of existing literature to consolidate the current state of model merging.
*   **Taxonomic Classification:** A novel taxonomic approach was developed and utilized to exhaustively categorize and discuss existing model merging methods.
*   **Domain-Specific Analysis:** The study involved analyzing applications of model merging across specific architectural domains (LLMs, MLLMs) and various machine learning settings (continual, multi-task, few-shot learning).
*   **Gap Analysis:** The methodology included identifying current limitations in the field to derive a roadmap for future research.

---

## Technical Details

The paper establishes a technical framework for merging parameters $\{\Theta^{(1)}, ..., \Theta^{(T)}\}$ from $T$ expert models into a single set $\Theta_{merge}$ without accessing original training data.

### Categorization Strategy

#### 1. Pre-Merging Methods
Focus on preparing models for compatibility before the merge process.
*   **Merger-Friendly Fine-Tuning:** Aims for weight disentanglement via linearization along the tangent space or sharpness-aware minimization (e.g., SAFT-Merge).
*   **Architecture Transformation:** Converts heterogeneous architectures into homogeneous ones to allow merging.
*   **Weight Re-basin/Alignment:** Aligns weight spaces to facilitate optimal merging.

#### 2. During Merging Strategies
Focus on the actual mathematical operation of combining weights.
*   **Basic Merging:**
    *   *Simple Averaging:* Often found to be ineffective due to interference.
    *   *Task Arithmetic:* Utilizes task vectors $\tau_t = \Theta^{(t)} - \Theta^{(0)}$ for multi-task learning, forgetting, and analogies.
*   **Weighted-based Merging:** Uses metrics to determine the importance of weights (e.g., Fisher-Merging, RegMean).
*   **Subspace-based Merging:**
    *   *Sparse Methods:* Prunes irrelevant parameters (e.g., Ties-Merging, DARE).
    *   *Low-Rank Methods:* Decomposes weights for efficient merging (e.g., HO-GSVD).
*   **Optimization-based Merging:** Uses data-free optimization to minimize loss (e.g., AWD).
*   **Routing-based Merging:** Dynamically selects parameters or experts during inference.

### Operational Metrics & Classification

| Classification | Description | Examples | Data Requirement |
| :--- | :--- | :--- | :--- |
| **Tuning-Free** | Require no additional gradient updates. | Task Arithmetic, Ties-Merging, DARE | Data-Free |
| **Training-Based** | Require additional optimization cycles. | AdaMerging, Fisher-Merging | Validation or Labeled Data |

### Weighting Granularity
*   Varies across **Global**, **Task**, **Layer**, and **Parameter** levels.

---

## Results

*   **Compression Efficiency:** Merging methods consistently compress knowledge into a **1Ã— parameter model** size, maintaining high inference efficiency.
*   **Methodological Efficacy:**
    *   Simple averaging is unacceptable in most scenarios due to parameter interference.
    *   The **Task Vector** formulation is identified as fundamental for enabling capabilities like forgetting and analogical reasoning.
*   **Validation:** The breadth and utility of these techniques were validated across ten machine learning subfields.

---

## Contributions

*   **Systematic Overview:** Provided the first comprehensive overview of model merging methods, theories, and applications to bridge the gap in existing literature.
*   **New Taxonomy:** Proposed a new taxonomic framework that offers a structure for classifying and understanding the wide array of existing model merging methods.
*   **Cross-Domain Analysis:** Mapped the application of these techniques across a wide spectrum of fields, specifically highlighting their utility in LLMs, MLLMs, and more than ten machine learning subfields.
*   **Future Insights:** Highlighted specific remaining challenges and discussed critical future research directions to guide further development.
*   **Resource Curation:** Compiled and made available a comprehensive list of papers related to model merging methods and theories via a public GitHub repository.