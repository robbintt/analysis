# Continual Neural Topic Model
*Charu Karakkaparambil James; Waleed Mustafa; Marius Kloft; Sophie Fellenz*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Wiki Dataset Perplexity:** 553 (CoNTM) vs. 597 (DETM)
> *   **NYTimes Topic Diversity:** 0.56 (CoNTM) vs. 0.35 (DETM)
> *   **Key Innovation:** Global-Local Parameter Split with continuous prior updates.

---

## Executive Summary

Current topic modeling paradigms face a critical trade-off between capturing temporal evolution and preserving long-term knowledge. Traditional Dynamic Topic Models (DTMs) effectively model changes over time but rely on batch processing, requiring access to the entire corpus for retraining, which hinders scalability. Conversely, Online Topic Models handle streaming data but are plagued by catastrophic forgetting, losing information from previous time steps as they adapt to new distributions. This paper addresses the need for a continual learning framework capable of incrementally updating topics from data streams without sacrificing historical context or necessitating full data reprocessing, a requirement for modern, large-scale applications.

The authors introduce the **Continual Neural Topic Model (CoNTM)**, a novel framework built upon the Dirichlet Variational AutoEncoder (DVAE). The core technical innovation is a **Global-Local Parameter Split** mechanism, which decouples global parameters—responsible for long-term memory—from local parameters used for time-specific adaptations. Rather than relying on memory replay or storing historical data, CoNTM utilizes a continuously updated global prior distribution. Topic evolution is modeled through a perturbation strategy where local topics are mathematically derived as transformations of global topics.

Empirical validation on standard benchmarks including **Wiki**, **NYTimes**, and **NE** datasets demonstrates CoNTM’s quantitative superiority over strong baselines such as DETM, DLDA, and Dynamic BERTopic. On the **Wiki** dataset, CoNTM achieved a predictive perplexity of **553**, significantly outperforming DETM (**597**) and DLDA (**609**), while also securing a Topic Diversity score of **0.68** compared to DETM’s **0.52**. In the **NYTimes** experiments, CoNTM again led with a perplexity of **1174** (versus DETM's **1240**) and a Topic Diversity of **0.56**, drastically higher than DETM's **0.35**.

Crucially, in evaluations of catastrophic forgetting, the online OLDA baseline suffered severe performance degradation over time, whereas CoNTM maintained high stability. This work successfully bridges the gap between dynamic and online topic modeling by combining temporal awareness with adaptability, offering a scalable solution for real-world applications like social media monitoring.

---

## Key Findings

*   **Superior Performance:** CoNTM consistently outperforms Dynamic Topic Models (DTMs) in both topic quality and predictive perplexity.
*   **Online Adaptability:** The model effectively captures topic changes in an online setting without requiring the entire corpus at once.
*   **Enhanced Diversity:** CoNTM demonstrates the ability to learn more diverse topics compared to existing methods (e.g., Wiki: 0.68 vs DETM: 0.52).
*   **Temporal Dynamics:** It provides a better representation of temporal changes in topics than both Dynamic and Online Topic Models.
*   **Memory Retention:** The model successfully learns new tasks without forgetting previously learned information, effectively mitigating catastrophic forgetting.

---

## Methodology

The Continual Neural Topic Model (CoNTM) is designed to handle the continuous influx of text data. Its methodology is centered on the concept of a **global prior distribution**, which serves as the persistent memory bank for the model.

1.  **Core Mechanism:** Instead of reprocessing the entire history of data (as in batch processing) or relying on limited memory replay, CoNTM continuously updates a global prior distribution at each subsequent time step.
2.  **Continual Learning:** As new data arrives, the model processes it and updates the global prior. This ensures that the model adapts to new information while preserving the knowledge structure acquired from previous steps.
3.  **Knowledge Preservation:** By anchoring the learning process to this evolving global prior, CoNTM maintains stability in the learned topics, avoiding the "drift" often seen in standard online models.

---

## Technical Details

The architecture of CoNTM is built upon the **Dirichlet Variational AutoEncoder (DVAE)**, enhanced with specific mechanisms to handle continual learning.

*   **Base Architecture:** Utilizes a **DVAE** with a Dirichlet prior for the document-topic distribution to enforce sparsity.
*   **Global-Local Parameter Split:**
    *   **Global Parameters:** Responsible for retaining long-term information and historical context.
    *   **Local Parameters:** Used for capturing time-specific patterns.
    *   **Purpose:** This split mitigates catastrophic forgetting by protecting global memory while allowing local adaptation.
*   **Topic Evolution Strategy:**
    *   Employs a **perturbation strategy**.
    *   Local topics are modeled as transformations of global topics.
*   **Generative Process:**
    1.  Derive local topics (perturbations of global topics).
    2.  Draw document-topic proportions via a **Dirichlet distribution**.
    3.  Draw words via a **Multinomial distribution**.
*   **Inference:** Performed using **Variational Inference** to maximize the Evidence Lower Bound (ELBO).

---

## Contributions

*   **Bridging the Gap:** Combines the strengths of Dynamic and Online Topic Models while solving their respective weaknesses (batch processing requirements and lack of long-term memory).
*   **Novel Framework:** Introduces the **Continual Neural Topic Model (CoNTM)**, a framework specifically designed for continual learning in topic modeling.
*   **Technical Mechanism:** Proposes a technical mechanism for continual learning using a continuously updated global prior distribution to enable learning without forgetting.
*   **Empirical Validation:** Provides evidence that neural continual learning methods can surpass traditional Dynamic Topic Models in metrics such as perplexity and topic coherence.

---

## Results

CoNTM was evaluated against probabilistic DTMs (e.g., DLDA, DETM), algorithmic DTMs (e.g., Dynamic BERTopic), and online topic models (e.g., OLDA).

**Quantitative Outcomes:**

*   **Wiki Dataset:**
    *   **Perplexity:** 553 (CoNTM) vs. 597 (DETM) vs. 609 (DLDA).
    *   **Diversity:** 0.68 (CoNTM) vs. 0.52 (DETM).
    *   **NPMI (Quality):** 0.098.
*   **NYTimes Dataset:**
    *   **Perplexity:** 1174 (CoNTM) vs. 1240 (DETM).
    *   **Diversity:** 0.56 (CoNTM) vs. 0.35 (DETM).
    *   **NPMI (Quality):** 0.108.

**Qualitative Outcomes:**

*   **Catastrophic Forgetting:** Unlike the OLDA baseline, which degraded over time, CoNTM maintained high stability on past tasks.
*   **Temporal Dynamics:** Offered superior representation of how topics evolve over time compared to static online methods.
*   **Adaptability:** Successfully adapted to topic changes in an online setting without needing the full corpus at initialization.