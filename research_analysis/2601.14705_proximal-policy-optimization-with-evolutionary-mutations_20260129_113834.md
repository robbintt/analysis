# Proximal Policy Optimization with Evolutionary Mutations

*Casimir Czworkowski; Stephen Hornish; Alhassan S. Yasin*

---

> ### 5 Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Novelty:** Hybrid RL (PPO + Evolutionary Mutations)
> *   **Performance:** 3/4 environments with significant improvement
> *   **Statistical Test:** Welch's two-sample t-test ($\alpha = 0.05$)
> *   **Tech Stack:** Stable-Baselines3, PyTorch

---

## Executive Summary

This research addresses the persistent challenge of **premature convergence** in Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm. While PPO is favored for its stability, it frequently stagnates in local optima, particularly within environments featuring sparse rewards or complex dynamics. The authors introduce **Proximal Policy Optimization with Evolutionary Mutations (POEM)**, a hybrid algorithm designed to solve this stagnation without sacrificing the sample efficiency that makes PPO dominant.

POEM functions by monitoring the Kullback-Leibler (KL) divergence between the current policy and an exponential moving average of previous policies. When divergence drops below a threshold, the algorithm triggers an adaptive mutation mechanism, injecting random noise into policy parameters to encourage exploration. Empirical validation across four OpenAI Gym environments confirms that this evolutionary approach significantly enhances performance in complex control tasks, offering a new pathway for adaptive exploration in reinforcement learning.

---

## Key Findings

*   **Superior Performance:** POEM significantly outperformed standard PPO in three out of four tested environments (**BipedalWalker**, **CarRacing**, and **MountainCar**).
*   **Statistical Significance:** Performance improvements were rigorously confirmed using **Welch's t-test**, ruling out random variance as the cause for success.
*   **Handling Premature Convergence:** The evolutionary mutation mechanism successfully addressed premature convergence, validating that integrating evolutionary principles with policy gradient methods improves the exploration-exploitation tradeoff.
*   **LunarLander Exception:** No significant improvement was found on the **LunarLander** environment (where POEM adopted a distinct "rapid descent" strategy compared to PPO's "hovering" approach).
*   **Consistency:** In successful environments, POEM demonstrated higher success rates and lower variance in performance compared to the baseline.

---

## Technical Methodology

### Algorithm Design
POEM is an extension of Proximal Policy Optimization implemented with **Stable-Baselines3** and **PyTorch**. It aims to mitigate stagnation through the following mechanism:

1.  **Stagnation Detection:** The algorithm monitors the **Kullback-Leibler (KL) divergence** between the current policy ($\pi_\theta$) and an exponential moving average reference policy ($\pi_{\hat{\theta}}$).
2.  **Adaptive Mutation:** When the KL divergence drops below a specific threshold (indicating a lack of progress), adaptive random noise is injected into the policy parameters to force exploration.
3.  **Loss Modification:** The loss function is augmented with a diversity bonus to explicitly encourage divergence from the historical reference policy.

### Mathematical Formulation
The total loss function used in POEM is defined as:

$$L_{total}(\theta) = L_{PPO}(\theta) - \lambda_{div} D_{KL}(\pi_\theta \Vert \pi_{\hat{\theta}}) + \alpha_{vf} L_{VF}(\theta) - \alpha_{ent} H(\pi_\theta)$$

Where:
*   $L_{PPO}$ is the standard PPO clipped surrogate loss.
*   $\lambda_{div}$ is the diversity coefficient scaling the KL divergence penalty.
*   $H(\pi_\theta)$ is the entropy bonus.

### Experiment Setup
*   **Environments:**
    *   **Modified CarRacing:** Custom reward, stacked grayscale frames.
    *   **LunarLander:** Resource-aware fuel mechanics.
    *   **MountainCar & BipedalWalker:** Standard versions with sparse rewards.
*   **Optimization:** Hyperparameters were fine-tuned via **Bayesian optimization**.

---

## Evaluation & Results

The method was evaluated over 10 experiments using **Welchâ€™s two-sample t-test ($\alpha = 0.05$)**. The results demonstrate a clear advantage for the POEM algorithm in navigation and locomotion tasks.

| Environment | Outcome | T-Score | P-Value | Notes |
| :--- | :--- | :--- | :--- | :--- |
| **CarRacing** | **Significant Win** | -6.3987 | **0.0002** | POEM showed superior performance with low variance. |
| **MountainCar** | **Significant Win** | -6.2431 | **< 0.0001** | POEM consistently reached the goal. |
| **BipedalWalker**| **Significant Win** | -2.0642 | 0.0495 | Higher success rate (12/15) vs PPO (7/15). |
| **LunarLander** | **No Significance**| -1.8707 | 0.0778 | POEM used "rapid descent"; PPO used "hovering". |

---

## Contributions

*   **Hybrid Architecture:** A novel reinforcement learning algorithm bridging policy gradient methods (PPO) with evolutionary algorithms to mitigate local convergence.
*   **Adaptive Exploration:** A new mechanism based on KL divergence measures against a historical moving average of policies, allowing dynamic escape from local optima.
*   **Empirical Validation:** Proof that evolutionary-inspired parameter mutations can enhance state-of-the-art policy gradient methods in complex control tasks involving sparse rewards.