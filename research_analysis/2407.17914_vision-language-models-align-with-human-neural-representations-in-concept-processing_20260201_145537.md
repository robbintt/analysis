# Vision-Language Models Align with Human Neural Representations in Concept Processing

*Anna Bavaresco; Marianne de Heer Kloots; Sandro Pezzelle; Raquel Fern√°ndez*

---

> ### üìä Quick Facts
> 
> *   **Quality Score**: 8/10
> *   **Citations**: 40
> *   **Participants**: 16
> *   **Concepts Tested**: 180 English concept words
> *   **Primary Metric**: Spearman‚Äôs œÅ (Representational Similarity Analysis)
> *   **Key Insight**: Encoders > Generative models for brain alignment.

---

## üìã Executive Summary

As artificial intelligence advances toward multimodal systems that integrate vision and language, understanding how these models relate to human cognitive processing is critical. While extensive research has mapped language-only models to neural activity, there is a significant gap in understanding how Vision-Language Models (VLMs) align with the human brain during concept processing. It remains unclear whether incorporating visual modalities results in representations that are more human-like than text-only models, or if specific architectural approaches are required to simulate the brain's ability to process concepts across varying contextual environments.

This study provides a systematic neuro-alignment assessment of ten distinct VLM architectures‚Äîranging from contrastive and encoder-only to generative models‚Äîagainst language-only baselines using fMRI data from the Pereira et al. (2018) dataset. The technical innovation lies in the granular application of Representational Similarity Analysis (RSA) under two strictly controlled conditions: concepts presented with pictures (visual context) and concepts embedded in sentences (textual context). Crucially, the authors employed ablation studies to distinguish between alignment derived from genuine pretraining knowledge versus sensitivity to inference-time context, allowing them to isolate which models rely on deep conceptual learning versus superficial contextual strategies.

The study found that VLMs generally achieve significantly higher neural alignment with human brain activity (measured via Spearman‚Äôs œÅ) compared to language-only counterparts (e.g., BERT, Llama3) across both visual and textual contexts within the left-hemisphere language and visual networks. Specifically, vision-language encoders demonstrated superior alignment compared to generative VLMs. While VLMs maintained their advantage over text-only models regardless of context, the architecture determined the *source* of this alignment: only LXMERT (encoder) and IDEFICS2 (generative) displayed robust brain alignment based on pretraining-learned concepts. In contrast, other VLMs relied heavily on input context during inference to achieve their alignment scores.

These findings hold substantial significance for both cognitive science and AI development, challenging the assumption that larger, generative multimodal models inherently possess more human-like internal representations. By empirically demonstrating that discriminative encoders currently outperform generative models in neuro-alignment and that high performance can stem from superficial inference strategies, this research provides a roadmap for building more cognitively plausible architectures. The distinction drawn between pretraining-based learning and context sensitivity offers a new metric for evaluating model "intelligence," guiding the field toward training regimes that foster genuine, human-like conceptual understanding rather than mere contextual pattern matching.

---

## üîç Key Findings

*   **VLM Superiority:** Vision-Language Models (VLMs) generally outperform language-only counterparts in aligning with human neural representations across both visual and textual context conditions.
*   **Architecture Matters:** Only specific VLM architectures‚Äînotably **LXMERT** and **IDEFICS2**‚Äîderive brain alignment from genuinely human-like concept learning during pretraining. Others rely heavily on inference-time context.
*   **Encoders vs. Generative:** Vision-language encoders demonstrate a higher degree of alignment with human brain activity compared to generative VLMs.
*   **Context Independence:** VLMs maintain their advantage over text-only models regardless of whether concept words are accompanied by visual or textual context.

---

## ‚öôÔ∏è Methodology

The study utilized a rigorous approach to compare model activations against biological neural data:

*   **Data Source:** Analysis was conducted using existing fMRI brain response datasets recording human reactions to concept words.
*   **Conditions:** Brain responses were measured under two distinct conditions:
    1.  **Visual Context:** Concept words presented alongside pictures.
    2.  **Textual Context:** Concept words embedded within sentences.
*   **Core Analysis:** Measured the representational alignment (similarity) between internal model representations and fMRI responses.
*   **Ablation Studies:** Performed controlled ablation studies to determine if alignment stemmed from pretraining knowledge (deep learning) or sensitivity to inference-time context (superficial processing).

---

## üõ†Ô∏è Technical Details

### Models Evaluated
The study evaluated ten models categorized into VLMs and language-only baselines.
*   **Language-Only Baselines:** BERT, Mistral, Llama3, GloVe.
*   **VLM Classifications:**
    *   *Contrastive:* CLIP, ALIGN
    *   *Vision-Language Encoders:* Single-stream (e.g., VisualBERT) and two-stream (e.g., LXMERT)
    *   *Generative:* IDEFICS2, LLaVA-NeXT

### Experimental Setup
*   **Dataset:** Subset of the *Pereira et al. (2018)* fMRI dataset.
*   **Participants:** 16 individuals.
*   **Stimuli:** 180 English concept words.
*   **Brain Regions:** Focus on the Left-hemisphere Language network and Visual network.

### Analysis Metrics
*   **Technique:** Representational Similarity Analysis (RSA).
*   **Distance Metric:** Pairwise cosine distances of averaged responses to create Representational Dissimilarity Matrices (RDMs).
*   **Alignment Score:** Measured via Spearman‚Äôs œÅ.

---

## üìà Results

The primary metric for alignment was **Spearman‚Äôs œÅ**, revealing the following:

1.  **General Alignment:** VLMs generally demonstrated higher neural alignment with human brain representations than language-only models across both visual and textual contexts.
2.  **Architectural Performance:** Vision-language encoders exhibited higher brain alignment compared to generative VLMs.
3.  **Source of Alignment:** Only specific architectures (specifically `LXMERT` and `IDEFICS2`) derived their alignment from pretraining-based concept learning. Other VLMs relied more on inference-time context mechanisms.
4.  **Context Consistency:** VLMs maintained their advantage over text-only models regardless of whether the context was a Picture or a Sentence.

---

## üöÄ Contributions

*   **Systematic Assessment:** Provides a comprehensive evaluation of how different VLM architectures align with human neural processing, addressing a significant gap in the literature.
*   **Nuanced Understanding:** Offers a distinction between models that simulate human-like conceptual learning and those utilizing superficial inference strategies.
*   **Empirical Evidence:** Provides evidence suggesting discriminative vision-language encoders are currently more neuro-aligned than generative VLMs for concept processing.
*   **Open Science:** Contributes code and materials to reproduce experiments, facilitating open-source reproducibility.