---
title: 'Through a Compressed Lens: Investigating The Impact of Quantization on Factual
  Knowledge Recall'
arxiv_id: '2505.13963'
source_url: https://arxiv.org/abs/2505.13963
generated_at: '2026-02-03T18:27:32'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Through a Compressed Lens: Investigating The Impact of Quantization on Factual Knowledge Recall

*Qianli Wang; Mingyang Wang; Nils Feldhus; Simon Ostermann; Yuan Cao; Hinrich Schütze; Sebastian Möller; Vera Schmitt*

***

> ### **Quick Facts**
>
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **References** | 40 Citations |
> | **Models Analyzed** | Llama 3 (8B), Qwen 2.5 (7B) |
> | **Best Performing Method** | BitSandBytes (bnb8) |
> | **Key Datasets** | LRE, TwoHop-Fact |
> | **Primary Focus** | Factual Knowledge Recall (FKR) |

***

## Executive Summary

This paper addresses the critical gap in understanding how Post-Training Quantization (PTQ) impacts the internal mechanisms of factual knowledge retrieval in Large Language Models (LLMs). While PTQ is a standard technique for reducing the computational footprint of LLMs, it is widely assumed that reducing bit precision inevitably leads to information loss and degraded performance in Factual Knowledge Recall (FKR). However, the specific structural and functional degradation within the model's internal circuits—specifically the "knowledge neurons" responsible for memorization—remains underexplored.

This study aims to bridge that gap by investigating whether efficient, quantized models necessarily compromise their reliability as knowledge bases, moving beyond simple accuracy metrics to analyze the preservation of interpretability and reasoning capabilities. The key innovation is a granular, multi-level experimental framework that evaluates quantization effects at the neuron, layer, and model levels. The study investigates weight-only PTQ methods—including AWQ, GPTQ, and the bitsandbytes library—across 4-bit and 8-bit precisions on Llama 3 (8B) and Qwen 2.5 (7B) architectures.

By leveraging "knowledge neurons" theory and Latent Multi-hop Reasoning Analysis, the researchers isolate how compression affects the internal circuits responsible for knowledge memorization. The study reveals quantifiable impacts on FKR performance across different configurations. On the Llama 3 8B model using the LRE dataset, the full-precision baseline achieved 98.53% accuracy. The bitsandbytes 8-bit configuration demonstrated the highest preservation, maintaining 98.37% accuracy. While the relationship between bit precision and performance was not strictly monotonic—with specific lower-precision instances paradoxically enhancing FKR—the overall data confirms that smaller models exhibit higher vulnerability to knowledge loss.

This research significantly influences the field by empirically challenging the assumption that lower bit precision strictly correlates with inferior model performance. By identifying the bitsandbytes library as the most robust method for maintaining factual integrity, the authors provide actionable insights for practitioners aiming to deploy efficient models without sacrificing reliability.

***

## Key Findings

*   **Information Loss:** Quantization typically results in information loss within LLMs, leading to diminished Factual Knowledge Recall (FKR).
*   **Model Size Vulnerability:** The negative impact of quantization is amplified in smaller models (e.g., 7B parameters) compared to larger ones.
*   **Non-Monotonic Performance:** Reduced bit precision does not consistently lead to inferior performance; in specific instances, quantization may paradoxically enhance FKR.
*   **Best-in-Class Method:** BitSandBytes demonstrates the highest preservation of the original full-precision model's FKR compared to other tested methods.
*   **Viable Compression:** Despite variability, the degradation in performance remains modest, affirming quantization as an effective compression strategy.

***

## Technical Details

**Analysis Scope**
*   **Levels:** Neuron, Layer, and Model level analysis of Factual Knowledge Recall (FKR).

**Models Under Study**
*   **Llama 3 (8B)**
*   **Qwen 2.5 (7B)**

**Quantization Methods**
*   **Type:** Post-Training Quantization (PTQ) - Weight-only.
*   **Techniques:**
    *   AWQ (Activation-aware Weight Quantization)
    *   GPTQ (GPT Quantization)
    *   BitSandBytes (bnb8 / bib8)

**Evaluation Framework**
*   **Knowledge Memorization Analysis:** Based on "knowledge neurons" theory.
*   **Latent Multi-hop Reasoning Analysis:** Adapted from Yang et al. (2024).

**Datasets**
*   **LRE:** Used for one-hop triplets.
*   **TwoHop-Fact:** Used for compositional reasoning tasks.

***

## Methodology

The researchers utilized a comprehensive experimental framework involving three common quantization techniques tested across distinct bit widths. The evaluation focused on two specific cognitive tasks:
1.  **Knowledge Memorization:** Assessing the model's ability to store and retrieve static facts.
2.  **Latent Multi-hop Reasoning:** Evaluating the model's ability to perform complex, compositional reasoning.

To derive insights, the study combined standard performance testing with interpretability-driven analyses to examine how compression affects the internal retrieval of stored knowledge.

***

## Contributions

*   **Bridging the Gap:** Addresses the underexplored area of how quantization specifically impacts Factual Knowledge Recall (FKR).
*   **Challenging Assumptions:** Provides empirical evidence challenging the assumption that lower bit precision strictly degrades performance.
*   **Comparative Assessment:** Offers a comparative assessment of quantization techniques, identifying BitSandBytes as the most robust method for maintaining factual knowledge.
*   **Actionable Insights:** Delivers critical insights regarding the vulnerability of smaller models to quantization-induced knowledge loss.

***

## Results

*   **General Impairment:** Quantization generally impairs FKR, with a more pronounced negative impact on smaller models like Qwen 2.5 (7B).
*   **Llama 3 8B Performance (LRE Dataset):**
    *   **Baseline:** 98.53%
    *   **BitSandBytes 8-bit:** 98.37% (Delta: -0.16%)
    *   **GPTQ 4-bit:** 97.12% (Delta: -1.41%)
    *   **AWQ 4-bit:** 97.00% (Delta: -1.53%)
*   **Reasoning Task Performance (TwoHop-Fact):**
    *   **Baseline:** 90.33%
    *   **BitSandBytes 8-bit:** 89.67% (Delta: -0.66%)
    *   **AWQ 4-bit:** 85.67% (Delta: -4.66%)
*   **Qualitative Failures:** Analysis illustrated specific retrieval errors; for example, the AWQ 4-bit model incorrectly identified Beyoncé's father as "Jay-Z" rather than the correct "Matthew Knowles."

***

**Document Rating:** 7/10  
**Total Citations:** 40