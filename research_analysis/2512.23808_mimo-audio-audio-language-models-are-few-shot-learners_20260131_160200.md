# MiMo-Audio: Audio Language Models are Few-Shot Learners

*Xiaomi LLM-Core Team; :; Dong Zhang; Gang Wang; Jinlong Xue; Kai Fang; Liang Zhao; Rui Ma; Shuhuai Ren; Shuo Liu; Tao Guo; Weiji Zhuang; Xin Zhang; Xingchen Song; Yihan Yan; Yongzhe He; Cici; Bowen Shen; Chengxuan Zhu; Chong Ma; Chun Chen; Heyu Chen; Jiawei Li; Lei Li; Menghang Zhu; Peidian Li; Qiying Wang; Sirui Deng; Weimin Xiong; Wenshan Huang; Wenyu Yang; Yilin Jiang; Yixin Yang; Yuanyuan Tian; Yue Ma; Yue Yu; Zihan Zhang; Zihao Yue; Bangjun Xiao; Bingquan Xia; Bofei Gao; Bowen Ye; Can Cai; Chang Liu; Chenhong He; Chunan Li; Dawei Zhu; Duo Zhang; Fengyuan Shi; Guoan Wang; Hailin Zhang; Hanglong Lv; Hanyu Li; Hao Tian; Heng Qu; Hongshen Xu; Houbin Zhang; Huaqiu Liu; Jiangshan Duo; Jianguang Zuo; Jianyu Wei; Jiebao Xiao; Jinhao Dong; Jun Shi; Junhao Hu; Kainan Bao; Kang Zhou; Linghao Zhang; Meng Chen; Nuo Chen; Peng Zhang; Qianli Chen; Qiantong Wang; Rang Li; Shaohui Liu; Shengfan Wang; Shicheng Li; Shihua Yu; Shijie Cao; Shimao Chen; Shuhao Gu; Weikun Wang; Wenhan Ma; Xiangwei Deng; Xing Yong; Xing Zhang; Xu Wang; Yifan Song; Yihao Zhao; Yingbo Zhao; Yizhao Gao; Yu Cheng; Yu Tu; Yudong Wang; Zhaojun Huang; Zhengju Tang; Zhenru Lin; Zhichao Song; Zhipeng Xu; Zhixian Zheng; Zihan Jiang*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Size** | 7 Billion Parameters (7B) |
| **Training Data** | >100 Million Hours |
| **Training Tokens** | 0.0T â€“ 2.0T |
| **Architecture** | Modular (Tokenizer, Patch Encoder, LLM, Patch Decoder) |
| **Key Innovation** | Few-Shot Learning via Next-Token Prediction |
| **SOTA Benchmark** | COMET ~71.0 (S2S) & ~70.2 (S2T) |
| **Quality Score** | 8/10 |

---

## Executive Summary

> This research addresses the fragmentation and limited generalization of traditional task-specific audio models by investigating whether Large Language Model (LLM)-centric training paradigms can establish a unified audio foundation model. By validating that NLP scaling principles apply to the audio domain, the paper aims to overcome the reliance on isolated, task-specific architectures and move toward generalized, intelligent audio systems capable of complex reasoning and generation.
>
> The core innovation is the empirical validation of "next-token prediction" scaling laws for audio, achieved through a modular architecture featuring a MiMo-Audio-Tokenizer, Patch Encoder, a 7B parameter LLM, and Patch Decoder. The model is trained on a dataset exceeding 100 million hours using a two-stage strategy: Understanding Training followed by Understanding-Generation Joint Training, scaling from 0.0T to 2.0T tokens. Crucially, the methodology incorporates specific "thinking mechanisms" into both audio understanding and generation processes to enhance reasoning and includes an instruction-tuned variant (MiMo-Audio-7B-Instruct) curated on a diverse corpus to improve task adherence.
>
> At the 2.0T token checkpoint, the model demonstrates significant scaling and emergent capabilities, achieving approximately 60% accuracy on 5-shot SpeechMMLU (T2S), 35% on 5-shot SpeechMMLU (S2S), and 70% similarity on 16-shot Voice Conversion. The instruction-tuned variant establishes a new open-source State-of-the-Art (SOTA) standard, securing COMET scores of ~71.0 on Big Bench Audio (S2S) and ~70.2 (S2T), performance that approaches closed-source models like GPT-4o. Furthermore, the model exhibits strong generalization to unseen tasks absent from its training data, including style transfer, speech editing, and realistic long-form speech continuation.
>
> This work bridges the gap between NLP and audio processing, proving that massive scaling of next-token prediction facilitates the emergence of few-shot learning in audio. By achieving a new performance ceiling for open-source models and releasing both Base and Instruct checkpoints alongside a comprehensive evaluation suite, the authors provide a robust infrastructure for the community, accelerating the development of advanced, reasoning-capable audio foundation models.

---

## Key Findings

*   **Emergence of Few-Shot Learning**: Scaling pretraining data to over 100 million hours enables the model to develop few-shot learning capabilities, allowing generalization to new tasks with minimal examples.
*   **Open-Source Benchmark Dominance**: The base model (MiMo-Audio-7B-Base) achieves State-of-the-Art (SOTA) performance among open-source models on both speech intelligence and audio understanding benchmarks.
*   **Generalization to Unseen Tasks**: The model demonstrates the ability to perform tasks absent from its training data, such as voice conversion, style transfer, speech editing, and realistic long-form speech continuation.
*   **Superior Instruction Following**: The instruction-tuned variant (MiMo-Audio-7B-Instruct) achieves open-source SOTA on audio understanding and spoken dialogue benchmarks, approaching or surpassing closed-source models.

## Technical Details

**System Architecture**
*   **Components**: The system features a modular design composed of four parts:
    1.  MiMo-Audio-Tokenizer
    2.  Patch Encoder
    3.  7 Billion Parameter Large Language Model (LLM)
    4.  Patch Decoder
*   **Variants**:
    *   `MiMo-Audio-7B-Base`: Pretrained model.
    *   `MiMo-Audio-7B-Instruct`: Instruction-tuned variant.

**Training Methodology**
*   **Tokenization**: Patch-based tokenization approach.
*   **Strategy**: A two-stage training process:
    1.  Understanding Training
    2.  Understanding-Generation Joint Training
*   **Scaling Regime**: Trained across a scaling regime of 0.0T to 2.0T tokens.
*   **Learning Style**: Utilizes few-shot in-context learning.

## Methodology

*   **Pretraining Paradigm**: Adopted a "next-token prediction" pretraining approach mirroring large language models rather than task-specific fine-tuning.
*   **Massive Data Scaling**: Scaled the pretraining dataset to exceed one hundred million hours of audio to facilitate emergent capabilities.
*   **Post-Training and Reasoning**: Curated a diverse instruction-tuning corpus and introduced "thinking mechanisms" into both the audio understanding and generation processes to enhance reasoning.

## Results

*   **Scaling Performance (at 2.0T tokens)**:
    *   ~60% accuracy on 5-shot SpeechMMLU (T2S).
    *   ~35% accuracy on 5-shot SpeechMMLU (S2S).
    *   ~70% similarity on 16-shot Voice Conversion.
*   **Benchmarking**:
    *   MiMo-Audio-7B-Instruct achieved open-source SOTA.
    *   COMET score of ~71.0 on Big Bench Audio (S2S).
    *   COMET score of ~70.2 on Big Bench Audio (S2T).
    *   Performance approaches closed-source models like GPT-4o.
*   **Zero-Shot Generalization**: The model generalizes well to unseen tasks such as Style Transfer, Speech Editing, and Long-form Speech Continuation.

## Contributions

*   **Validation of Scaling Laws for Audio**: Provided empirical evidence that the scaling of next-token prediction pretraining is a viable and effective paradigm for the audio domain.
*   **New Performance Standard**: Established a new performance ceiling for open-source audio models across multiple benchmarks for both general tasks and instruction-following scenarios.
*   **Open Resource Release**: Contributed to the research community by releasing model checkpoints (Base and Instruct variants) and a full evaluation suite.

---
**References:** 24 citations