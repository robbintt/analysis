# Large Language Models For Text Classification: Case Study And Comprehensive Review

*Arina Kostina; Marios D. Dikaiakos; Dimosthenis Stefanidis; George Pallis*

---

> **Quick Facts**
> *   **Quality Score:** 5/10
> *   **Total Citations:** 40
> *   **Evaluation Metrics:** Weighted F1-score, Inference Latency
> *   **Datasets:** FakeNewsNet (Binary), Employee Reviews (Multiclass)
> *   **Key Models:** Llama3, GPT-4, RoBERTa, SVM, Naive Bayes

---

## Executive Summary

> As Large Language Models (LLMs) proliferate, technical leaders face a critical dilemma regarding the cost-benefit trade-off of deploying these resource-intensive models for text classification versus utilizing traditional Machine Learning (ML) and Deep Learning (DL) methods. While LLMs are renowned for their generative capabilities, their high inference latency and computational costs necessitate a rigorous evaluation to determine if their "reasoning" advantages translate to superior performance in non-generative classification tasks. This paper addresses this uncertainty by investigating whether the advanced capabilities of LLMs offer tangible benefits over established baselines across tasks of varying complexity.
>
> The key innovation is a unified comparative evaluation framework that systematically contrasts diverse model architectures—ranging from Traditional ML (Naive Bayes, SVM) and Encoder-only PLMs (RoBERTa) to Decoder-only LLMs (GPT-4-turbo, Llama2/3, Mistral variants)—under strict experimental controls. Technically, the study optimizes the inference environment by deploying models across hybrid hardware (NVIDIA Tesla T4, Groq LPUs, and OpenAI APIs) and employing advanced techniques such as AWQ quantization for local models and vLLM for batching optimization. The methodology ensures fairness through consistent preprocessing (excluding records exceeding 4,096 tokens), hyperparameter tuning via GridSearchCV (5-fold), and standardized prompts with temperature set to zero, creating a robust baseline for measuring Weighted F1-score against response time.
>
> The evaluation utilized two distinct datasets to quantify performance differences: a binary task using FakeNewsNet (214 records) and a multiclass task using Employee Reviews (1,000 records). Results indicated a distinct dependency on task complexity; in the multiclass Employee Reviews scenario, LLMs specifically Llama3 and GPT-4—significantly outperformed traditional baselines in classification accuracy. However, in the binary FakeNewsNet task, traditional ML models (SVM and Naive Bayes) provided a superior performance-to-time ratio, achieving competitive accuracy with substantially lower inference latency than the LLMs. Furthermore, the study highlighted that LLM output is highly sensitive to prompt engineering, with small variations significantly impacting final results.
>
> This research provides a pragmatic framework for decision-makers, challenging the assumption that LLMs are universally superior for all NLP tasks. By exposing the efficiency limitations of LLMs in binary workflows and their strengths in multiclass reasoning, the study advocates for a task-dependent, efficiency-conscious approach to model selection. Although the generalizability of the findings is moderated by the limited scale of the FakeNewsNet dataset (214 records), the work offers valuable benchmarking insights into the latency-accuracy trade-offs across different hardware and quantization levels. It serves as a guide for optimizing NLP solutions, urging practitioners to weigh the operational overhead of generative AI against the often sufficient performance of traditional methods.

---

## Key Findings

*   **Performance by Complexity:** Large Language Models (specifically Llama3 and GPT-4) outperform traditional deep learning and machine learning models in complex scenarios, particularly **multiclass classification**.
*   **Efficiency Trade-off:** While LLMs offer superior reasoning in complex tasks, they require significantly longer inference times. For **binary classification**, simpler traditional models offer a better performance-to-time trade-off.
*   **Impact of Prompting:** Prompting strategies significantly impact model responses, highlighting the critical importance of prompt engineering in achieving optimal results.
*   **Task-Dependent Selection:** Model choice should be task-dependent. Binary tasks favor efficiency (Traditional ML), whereas multiclass tasks benefit more from LLM reasoning capabilities.

---

## Methodology

The researchers employed a **comparative evaluation framework** to assess Large Language Models against state-of-the-art deep learning and machine learning baselines. The analysis utilized two primary case studies:

1.  **Multiclass Classification:** Predicting employee working locations.
2.  **Binary Classification:** Detecting fake news.

The study examined diverse model architectures, sizes, quantization levels, and prompting techniques. Results were evaluated based on:
*   **Weighted F1-scores** (Accuracy)
*   **Inference response times** (Efficiency)

---

## Technical Details

### Models & Architectures
*   **Traditional ML:** Naive Bayes, SVM
*   **Encoder-only PLM:** RoBERTa
*   **Decoder-only LLMs:** GPT-4-turbo, Llama2/3, Mistral variants, Xwin, Zephyr

### Hardware & Deployment
*   **Hybrid Hardware Strategy:**
    *   Groq LPU
    *   NVIDIA Tesla T4 (Google Colab)
    *   Cloud APIs
*   **Optimization Techniques:**
    *   AWQ quantization for local models.
    *   vLLM utilized for batching local models.
    *   Sequential prompting for Groq.
    *   Batched requests for OpenAI.

### Preprocessing & Training
*   **Data Limits:** Excludes records over 4,096 tokens.
*   **Traditional ML Processing:** Uses TfidfVectorizer with GridSearchCV (5-fold).
*   **RoBERTa Configuration:**
    *   Optimizer: Adam
    *   Learning Rate: $1e^{-5}$
    *   Batch Size: 32
*   **Inference Parameters:** Temperature set to 0.

---

## Results

*   **Datasets Utilized:**
    *   **FakeNewsNet:** Binary classification (214 records).
    *   **Employee Reviews:** Multiclass classification (1,000 records).
*   **Quantitative Metrics:** Specific numerical scores (exact accuracy or F1-scores) were not provided in the text.
*   **Performance Outcomes:**
    *   **LLMs (Llama3, GPT-4):** Demonstrated superior performance in multiclass tasks.
    *   **Traditional ML (SVM, NB):** Provided a better performance-to-time ratio for binary tasks.
*   **Qualitative Insight:** Prompt engineering is noted as a critical factor for LLM performance stability.

---

## Contributions

*   **Comprehensive Analysis:** Provided a comparative analysis of diverse LLMs against traditional ML/DL models in text classification contexts.
*   **Dual-Metric Evaluation:** Introduced an evaluation approach that balances **prediction accuracy** (weighted F1-score) with **computational efficiency** (inference response time).
*   **Empirical Benchmarking:** Contributed benchmarking data across different classification complexities, highlighting the specific strengths of Llama3 and GPT-4 in multiclass challenges.