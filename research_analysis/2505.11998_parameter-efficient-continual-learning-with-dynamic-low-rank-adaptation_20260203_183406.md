---
title: Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation
arxiv_id: '2505.11998'
source_url: https://arxiv.org/abs/2505.11998
generated_at: '2026-02-03T18:34:06'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation

*Prashant Shivaram Bhat; Shakib Yazdani; Elahe Arani; Bahram Zonooz*

---

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **Citations:** 40
> *   **Parameter Efficiency:** Trains only **0.3% - 0.8%** of total parameters
> *   **Top Performance:** **72.0%** Top-1 Accuracy on ImageNet-R
> *   **Key Mechanism:** Dynamic Rank Allocation & Reference-Weight Proximity

---

## Executive Summary

Continual Learning (CL) aims to enable models to learn sequentially from data streams, but it is notoriously plagued by "catastrophic forgetting," where learning new tasks erases previous knowledge. Traditional mitigation strategies, such as rehearsal mechanisms, incur significant storage and computational overhead while raising privacy concerns. Alternatively, Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) reduce overhead but typically rely on static rank selection. This static approach creates a resource allocation dilemma: a fixed rank cannot dynamically balance the need to capture complex new tasks without forgetting old ones, often leading to either under-fitting or unnecessary resource consumption.

To address these limitations, the authors propose **PEARL** (Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation), a rehearsal-free framework that introduces a Dynamic Rank Allocation mechanism. Instead of relying on a fixed hyperparameter, PEARL adaptively determines the appropriate rank for task-specific LoRA components by analyzing the geometric proximity of the current task's weights to reference task weights within the parameter space. Technically, the method computes task vectors (the difference between fine-tuned and reference weights), decomposes them via Singular Value Decomposition (SVD), and dynamically selects the rank based on a normalized squared error threshold. This allows the model to allocate capacity precisely where needed, initializing adapters to capture the necessary deviation from the consolidated knowledge base.

PEARL demonstrates superior performance across rigorous benchmarks, significantly outperforming state-of-the-art baselines including L2P, DualPrompt, and DER. On the CIFAR-100 benchmark with 20 tasks, PEARL achieves an average accuracy of **59.1%**, surpassing L2P by approximately **9%**. In the TinyImageNet 20-split scenario, it attains **49.8%** accuracy, outperforming DER by roughly **4%**. Furthermore, on the complex ImageNet-R dataset, PEARL secures a top-1 accuracy of **72.0%**. Despite this high performance, the framework remains remarkably efficient, training only **0.3% to 0.8%** of total parameters, proving its effectiveness on architectures ranging from ResNet-18 to Vision Transformers (ViT).

The significance of PEARL lies in its provision of a lightweight, privacy-preserving solution to Continual Learning that eliminates the dependency on data storage or replay buffers. By decoupling model capacity from static hyperparameters through its novel Reference-Weight Proximity Metric, the research offers a path toward more adaptive and efficient CL systems. This approach is particularly impactful for real-world applications where computational resources are constrained and data privacy is paramount, as it maintains the integrity of pre-trained models while significantly reducing the overhead typically associated with retaining past knowledge.

---

## Key Findings

*   **Superior Performance:** PEARL significantly outperforms all considered baselines across a multitude of Continual Learning scenarios.
*   **Architecture Agnostic:** The method demonstrates robust effectiveness and generalizability across ResNet, Separable Convolutional Networks, and Vision Transformers.
*   **Rehearsal-Free Learning:** PEARL successfully addresses catastrophic forgetting and maintains knowledge consolidation without requiring a rehearsal mechanism.
*   **Dynamic Resource Allocation:** The framework avoids sub-optimal resource allocation and performance degradation by moving away from static rank selection.

---

## Methodology

PEARL introduces a rehearsal-free Continual Learning framework that utilizes dynamic rank allocation for LoRA components. Instead of relying on a fixed rank, the method adaptively determines the appropriate rank for task-specific LoRA components during training. This adaptation is driven by an analysis of the current task's proximity to reference task weights within the parameter space, dynamically adjusting model capacity based on the relationship between new and consolidated knowledge.

---

## Contributions

*   **Dynamic Rank Allocation:** Proposes a mechanism to solve the issue of sensitivity to static rank selection in parameter-efficient fine-tuning for Continual Learning.
*   **Reference-Weight Proximity Metric:** Introduces a novel method for determining adapter ranks based on the geometric proximity of current tasks to reference task weights.
*   **Lightweight Solution:** Contributes a lightweight, rehearsal-free solution that maintains the integrity of pre-trained models while mitigating catastrophic forgetting and reducing computational and storage overhead.

---

## Technical Details

**Framework Architecture**
*   **Type:** Rehearsal-free Sequential Continual Learning.
*   **Components:** Combines shared reference parameters with task-specific low-rank adapters.

**Dynamic Low-Rank Adaptation Process**
1.  **Task Vector Calculation:** Computes task vectors defined as the difference between fine-tuned weights and reference weights.
2.  **Decomposition:** Decomposes task vectors via Singular Value Decomposition (SVD).
3.  **Rank Selection:** Dynamically selects the rank for each layer based on a **normalized squared error threshold** rather than a static hyperparameter.

**Training Workflow**
*   Initial fine-tuning on the current task.
*   Calculation of task vectors.
*   SVD decomposition of vectors.
*   LoRA initialization based on the selected rank.
*   Re-initialization step to avoid local minima.
*   Final fine-tuning of the model.

**Inference Strategy**
*   For Class-Incremental Learning (Class-IL), inference aggregates outputs from all task-specific sub-networks.

---

## Results

PEARL significantly outperforms baselines across various scenarios while successfully addressing catastrophic forgetting without rehearsal. It demonstrates architecture-agnostic effectiveness on ResNet, Separable Convolutional Networks, and Vision Transformers. The framework achieves resource efficiency through dynamic rank allocation and reduces the number of learnable parameters by leveraging Parameter Efficient Fine-Tuning (PEFT) principles.

**Benchmark Performance:**
*   **CIFAR-100 (20 tasks):** Achieved **59.1%** accuracy (Outperformed L2P by ~9%).
*   **TinyImageNet (20-split):** Achieved **49.8%** accuracy (Outperformed DER by ~4%).
*   **ImageNet-R:** Secured **72.0%** Top-1 accuracy.

**Efficiency Metrics:**
*   Trained only **0.3% to 0.8%** of total parameters.