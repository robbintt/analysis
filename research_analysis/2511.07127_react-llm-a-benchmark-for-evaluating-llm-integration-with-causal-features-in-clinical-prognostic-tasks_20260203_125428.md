---
title: 'REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features
  in Clinical Prognostic Tasks'
arxiv_id: '2511.07127'
source_url: https://arxiv.org/abs/2511.07127
generated_at: '2026-02-03T12:54:28'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks

*Linna Wang; Zhixuan You; Qihui Zhang; Jiunan Wen; Ji Shi; Yimin Chen; Yusen Wang; Fanqi Ding; Ziliang Feng; Li Lu*

---

> ### ðŸ“Š Quick Facts
>
> *   **Models Evaluated:** 15 LLMs, 6 Traditional ML models, 3 Causal Discovery algorithms
> *   **Scope:** 7 Clinical outcomes across 2 real-world datasets
> *   **Hardware:** 3 NVIDIA RTX A800 GPUs
> *   **Key Strategy:** In-Context Learning (ICL) identified as most effective prompting strategy
> *   **Quality Score:** 9/10

---

## Executive Summary

Despite the rapid adoption of Large Language Models (LLMs) in healthcare, their efficacy in clinical prognostic tasks compared to established traditional machine learning (ML) methods remains uncertain. Furthermore, while integrating causal features promises improved interpretability and robustness, the practical viability of combining Causal Discovery (CD) with LLMs in complex clinical environments has not been systematically evaluated.

This study introduces **REACT-LLM**, a comprehensive benchmark designed to rigorously evaluate the integration of LLMs with causal features for clinical risk prediction. The research compares 15 state-of-the-art LLMs (including GPT-o1 and Llama-3.1-405b) against 6 traditional ML models and 3 CD algorithms across 7 distinct clinical outcomes derived from 2 real-world datasets. The methodology incorporates five prompting strategiesâ€”In-Context Learning (ICL), Self-Refinement, Role-Playing, Chain-of-Thought, and narrative serializationâ€”to assess how different input configurations affect prognostic performance.

The empirical analysis reveals that traditional ML models, such as XGBoost and Random Forest, consistently outperform LLMs by a significant margin of 10â€“20%. While proprietary LLMs generally surpassed open-source models and In-Context Learning proved the most effective prompting strategy, the integration of causal features yielded only negligible performance gains. Notably, **Gemini-2-Flash** was the sole exception, slightly exceeding traditional ML performance (AUPRC 0.8540 vs. 0.8473) on the EarlyICU task. The limited success of causal integration is attributed to the violation of strict assumptions required by standard CD algorithms when applied to noisy, real-world clinical data.

REACT-LLM provides a critical reference point for the medical AI community, establishing the first standardized benchmark for causal-enhanced LLMs in clinical decision-making. By demonstrating that the theoretical benefits of causal discovery do not yet translate to practical performance gains in this domain, the study identifies a significant barrier to progress: the incompatibility of current CD assumptions with clinical data complexity. This work effectively redirects future research toward developing more robust causal methods that can handle real-world data variance, while validating that traditional ML remains the superior choice for clinical risk prediction in the immediate term.

---

## Key Findings

*   **LLM vs. Traditional ML Performance:** While Large Language Models (LLMs) demonstrate reasonable competence in clinical prognostic tasks, they currently do not outperform traditional machine learning (ML) models.
*   **Limited Gains from Causal Integration:** Integrating causal features derived from Causal Discovery (CD) algorithms into LLMs yields only limited performance improvements in clinical risk prediction.
*   **Barrier to Causal Application:** The primary reason for the limited success of causal integration is that the strict assumptions required by many CD methods are often violated when applied to complex, real-world clinical data.
*   **Potential for Synergy:** Despite the lack of immediate performance breakthroughs, the benchmark reveals a promising synergy between LLMs and causal features, suggesting potential for future development in this area.

---

## Methodology

The authors developed **REACT-LLM**, a systematic benchmark designed to evaluate the integration of LLMs with causal features for clinical risk prediction. The methodology involved:

1.  **Scope:** Evaluating 7 distinct clinical outcomes across 2 real-world datasets.
2.  **Comparison:** Comparing the performance of 15 prominent LLMs against 6 traditional ML models and 3 Causal Discovery (CD) algorithms.
3.  **Objective:** Assessing whether combining LLMs with causal features could enhance prognostic performance and potentially surpass traditional ML methods.

---

## Technical Details

The REACT-LLM benchmark evaluates integrating LLMs with causal features against traditional ML models across the following parameters:

*   **Baselines:** XGBoost, Random Forest, Logistic Regression.
*   **Scope:** 7 outcomes and 2 datasets.
*   **LLMs Evaluated (15):** Including GPT-o1, Claude-3.7-Sonnet, and Llama-3.1-405b.
*   **Causal Discovery Algorithms (3):** Tested for feature generation.
*   **Infrastructure:** Experiments conducted on 3 NVIDIA RTX A800 GPUs using greedy decoding.
*   **Prompting Strategies Tested (5):**
    *   In-Context Learning (ICL)
    *   Self-Refinement (SR)
    *   Role-Playing
    *   Chain-of-Thought (CoT)
*   **Formatting Preferences:**
    *   Open-source models favored JSON formatting.
    *   Smaller models favored narrative serialization.

---

## Results

The experimental analysis provided the following insights:

*   **Traditional ML Dominance:** Traditional ML models consistently outperformed LLMs by a margin of **10â€“20%**.
*   **The Outlier:** **Gemini-2-Flash** was the only model to exceed traditional ML performance (AUPRC 0.8540 vs 0.8473 on EarlyICU).
*   **Model Hierarchy:** Larger models generally outperformed smaller ones, and proprietary LLMs outperformed open-source models.
*   **Best Prompting Strategy:** In-Context Learning (ICL) was identified as the most effective prompting strategy.
*   **Causal Feature Impact:** Integrating causal features resulted in limited performance gains due to the strict assumptions of Causal Discovery methods in clinical data.

---

## Contributions

*   **Creation of a Comprehensive Benchmark:** Introduction of REACT-LLM, the first systematic benchmark specifically designed to assess the integration of LLMs with causal learning in clinical decision-making, addressing a prior gap in comprehensive evaluation tools.
*   **Large-Scale Empirical Analysis:** Provision of a broad comparative study that bridges LLM research, traditional ML, and causal discovery by testing a wide array of models (15 LLMs, 6 ML models, 3 CD algorithms) across multiple clinical scenarios.
*   **Insight into Causal Limitations:** Empirical evidence highlighting the current limitations of applying standard Causal Discovery methods to clinical data due to assumption violations, providing a critical reference point for future research in causal LLMs.

---

**References:** 40 citations