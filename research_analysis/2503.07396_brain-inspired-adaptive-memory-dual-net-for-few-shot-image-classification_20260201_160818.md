# Brain Inspired Adaptive Memory Dual-Net for Few-Shot Image Classification

*Kexin Di; Xiuxing Li; Yuyang Han; Ziyu Li; Qing Li; Xia Wu*

---

> ### Executive Summary
>
> Few-shot image classification faces the inherent challenge of learning discriminative representations from severely limited data, often suffering from high intra-class variability where objects of the same class appear vastly different. Additionally, current methods frequently encounter **"supervision collapse"** when relying on single image-level annotations, failing to semantically align relevant local features without extensive examples. Addressing these issues is critical for advancing computer vision applications where data collection is expensive or impractical.
>
> The authors propose **SCAM-Net** (Generalization-optimized Systems Consolidation Adaptive Memory Dual-Network), a bio-inspired framework mimicking the human brain's Complementary Learning Systems. The architecture utilizes a dual-network structure consisting of a Hippocampus model for short-term adaptation via gradient descent and a Neocortex model for long-term knowledge consolidation updated via Exponential Moving Average (EMA).
>
> Evaluated on standard benchmarks **miniImageNet** and **tieredImageNet**, SCAM-Net demonstrated strong performance, particularly in 1-shot settings. This research is significant for successfully bridging neuroscience principles with deep learning to solve few-shot recognition problems. By effectively resolving supervision collapse under weak supervision, SCAM-Net provides a robust mechanism for identifying semantically relevant local regions amidst high variability.
>
> ---
>
> ### âš¡ Quick Facts
>
> | Metric | Details |
> | :--- | :--- |
> | **Model Name** | SCAM-Net (Systems Consolidation Adaptive Memory Dual-Network) |
> | **Backbone** | Vision Transformer (ViT-S/16) (~22M parameters) |
> | **Pre-training** | Masked Image Modeling (MIM) |
> | **Key Mechanism** | Hippocampus-Neocortex Dual-Network |
> | **Best Performance** | 89.75% (miniImageNet 5-shot) |
> | **Quality Score** | 8/10 |
>
> ---
>
> ## Key Findings
>
> *   **Superior Benchmark Results:** The proposed SCAM-Net model achieved state-of-the-art results on benchmark datasets, outperforming existing methods in few-shot image classification tasks.
> *   **Resolution of Supervision Collapse:** The approach successfully resolves the supervision collapse problem frequently caused by relying on single image-level annotations.
> *   **High Intra-Class Variability Management:** The model overcomes high intra-class variability to effectively identify semantically relevant local features even when provided with limited examples.
> *   **Bio-inspired Memory Integration:** The adaptive memory module, inspired by biological systems, allows for the rapid capturing and integration of semantic features from scarce data.
>
> ---
>
> ## Methodology
>
> The authors propose **SCAM-Net**, a framework drawing inspiration from the human complementary learning system and systems consolidation theories.
>
> *   **Dual-Network Structure:** It utilizes a Hippocampus-Neocortex dual-network structure to manage learning and memory.
> *   **Generalization Optimization:** The framework consolidates structured representations stored in long-term memory (simulating the Neocortex), which are adaptively regulated according to a generalization optimization principle.
> *   **Bio-mimicry:** By mimicking the brain's ability to separate rapid plasticity (Hippocampus) from slow consolidation (Neocortex), the model achieves robust few-shot learning.
>
> ---
>
> ## Technical Specifications
>
> The architecture of SCAM-Net consists of several distinct technical components designed to optimize few-shot learning:
>
> **Architecture Components**
> *   **Hippocampus Model:** Responsible for short-term adaptation via gradient descent.
> *   **Neocortex Model:** Responsible for long-term information storage, updated via Exponential Moving Average (EMA).
> *   **Backbone:** Utilizes a Vision Transformer (ViT-S/16) backbone (~22M parameters) pre-trained with Masked Image Modeling (MIM).
>
> **Interaction & Training**
> *   **Dual-Stream Interaction:** Employs a Consistency Loss (MSE) that constrains the Hippocampus using guidance from the Neocortex.
> *   **Similarity Calculation:** Calculates pair-wise cosine similarity between support and query patch embeddings using block-diagonal masking.
> *   **Adaptive Memory Module:** Located in the Neocortex, it maintains global class representations using CLS tokens. Memory is updated by averaging for existing classes or appending for new ones.
>
> **Loss Function**
> The composite loss function is defined as:
> $$L = L_{CE}(\hat{y}_h, y) + L_{Cons}(\hat{y}_h, \hat{y}_n) + \lambda L_{CE}(\hat{y}_c, y)$$

> ---
>
> ## Performance Evaluation
>
> The model was evaluated on **miniImageNet** and **tieredImageNet** using a ViT-S/16 backbone.
>
> ### miniImageNet Results
> *   **1-Shot:** **75.93%** (Outperformed ViT competitor AMMD by 2.23%)
> *   **5-Shot:** **89.75%** (Outperformed AMMD by 1.63%)
>
> ### tieredImageNet Results
> *   **1-Shot:** **78.89%** (Outperformed AMMD which scored 77.59%)
> *   **5-Shot:** **88.67%** (Trailed behind top competitor AMMD which scored 90.38%)
>
> ---
>
> ## Core Contributions
>
> *   **Novel Framework:** Introduction of the Generalization-optimized Systems Consolidation Adaptive Memory Dual-Network (SCAM-Net), utilizing biological principles to solve complex computer vision problems.
> *   **Adaptive Memory Module:** Development of a module simulating systems consolidation for vastly improved representation storage and retrieval.
> *   **Feature Localization:** Advancement in the capability to locate semantically relevant local regions amidst high intra-class variability and weak supervision conditions.
>
> ---
> **References:** 40 citations | **Quality Score:** 8/10