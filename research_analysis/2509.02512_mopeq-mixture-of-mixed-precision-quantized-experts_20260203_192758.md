---
title: 'MoPEQ: Mixture of Mixed Precision Quantized Experts'
arxiv_id: '2509.02512'
source_url: https://arxiv.org/abs/2509.02512
generated_at: '2026-02-03T19:27:58'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MoPEQ: Mixture of Mixed Precision Quantized Experts
*Krishna Teja Chitty-Venkata; Jie Ye; Murali Emani*

---

> ### üìä Quick Facts
> * **Model Types:** SOTA Vision-Language Models (MolmoE, DeepSeek-VL2)
> * **Precision Levels:** 2-bit, 3-bit, 4-bit
> * **Memory Reduction:** ~75% compared to FP16 baselines
> * **Accuracy Degradation:** <1% on core tasks
> * **Key Innovation:** Hessian trace approximation for sensitivity analysis
> * **Quality Score:** 8/10

---

## üìù Executive Summary

Vision-Language Models (VLMs) utilizing Mixture-of-Experts (MoE) architectures offer superior scalability but suffer from significant memory footprints that hinder deployment on resource-constrained hardware. Existing quantization methods often treat these models monolithically or rely on uniform precision across layers, leading to accuracy degradation that negates the benefits of model compression. The challenge lies in effectively reducing the memory requirements of massive MoE models without compromising the specialized capabilities of individual experts.

The paper introduces **MoPEQ**, a novel Post Training Quantization (PTQ) algorithm that operates at a per-expert granularity rather than a layer-wise level. MoPEQ utilizes **Hessian trace approximation** to evaluate the sensitivity of individual experts, replacing traditional heuristics based on expert activation frequency. By clustering similar experts and assigning specific mixed precisions (2-bit, 3-bit, and 4-bit) based on this sensitivity analysis, the method generates a non-uniform Precision Assignment Map that optimizes the trade-off between memory conservation and accuracy retention.

Evaluated on MolmoE-1B and DeepSeek-VL2 variants (Tiny, Small, and Base) within the VLMEvalKit benchmark, MoPEQ demonstrated substantial memory efficiency‚Äîreducing weight memory requirements by approximately **75%** compared to FP16 baselines‚Äîwhile maintaining performance parity with full-precision models. Specifically, the method limited accuracy degradation to less than **1%** on core vision-language tasks, significantly outperforming uniform quantization baselines which often suffered steeper drops. The analysis revealed that larger model variants (e.g., Base) exhibited higher expert heterogeneity, necessitating a more diverse spread of bit-widths to preserve accuracy compared to smaller models.

This work establishes a new paradigm for decoupling model size from accuracy in MoE architectures, facilitating the deployment of massive models on edge devices. By proving that Hessian trace approximation is superior to activation frequency for determining expert importance‚Äîallowing for aggressive compression of less critical experts without catastrophic failure‚Äîthe research shifts the methodological focus for future quantization techniques. MoPEQ provides the first specific PTQ framework for VLM-MoEs at the expert level, offering a viable path forward for efficient inference in modern, high-capacity vision-language systems.

---

## üîë Key Findings

*   **Efficiency:** The MoPEQ algorithm achieves accuracy comparable to uniform-precision baselines while delivering substantial improvements in memory footprint.
*   **Sensitivity Analysis:** Analyzing expert sensitivity using **Hessian trace approximation** is a viable and effective strategy for optimal bit-width allocation, proving superior or complementary to reliance on activation frequency.
*   **Validation:** The method was validated on State-of-the-Art (SOTA) Vision-Language Models (specifically Deepseek-VL2 variants and MolmoE) within the VLMEvalKit benchmark, demonstrating robustness across different model sizes (tiny, small, base).
*   **Granular Precision:** A granular approach that assigns specific bit widths (2, 3, and 4 bits) to individual experts, rather than uniform layer-wise assignment, successfully maintains model performance.

---

## üî¨ Methodology

The researchers propose MoPEQ, a **Post Training Quantization (PTQ)** algorithm specifically designed for Mixture-of-Experts (MoE) architectures in Large Language and Vision Models. The core operational differentiator is the granularity:

*   **Per-Expert Granularity:** The method assigns optimal bit widths to individual experts rather than treating entire layers uniformly.
*   **Sensitivity vs. Frequency:** Instead of relying on expert activation frequency, the method utilizes **Hessian trace approximation** to analyze and determine the sensitivity of each expert.
*   **Clustering Strategy:** The approach clusters similar experts to balance accuracy retention with memory reduction requirements.
*   **Comprehensive Study:** A thorough analysis was performed regarding the impact of expert activation frequency versus sensitivity across varying bit-widths (2-4 bits) on modern SOTA VLM architectures.

---

## ‚öôÔ∏è Technical Details

MoPEQ employs a granular mixed-precision strategy designed to optimize the Feed-Forward Networks (FFNs) of Vision-Language Models (VLMs).

*   **Precision Strategy:** Assignment of specific bit-widths (**2-bit, 3-bit, 4-bit**) to individual experts.
*   **Optimization Algorithm:** Uses **Hessian trace approximation** to analyze expert sensitivity for bit-width allocation.
*   **Comparative Heuristic:** Contrasts sensitivity analysis against Expert Activation Frequency heuristics.
*   **Output:** Generates a **non-uniform Precision Assignment Map** across model depth and width.
*   **Visual Pattern:** Results in a highly irregular, scattered bit-width distribution that increases in complexity with model size.

---

## üìà Results

The evaluation of MoPEQ was conducted using MolmoE-1B and DeepSeek-VL2 variants (Tiny, Small, Base) on the **VLMEvalKit benchmark**:

*   **Accuracy Parity:** Achieved accuracy comparable to uniform-precision baselines.
*   **Memory Optimization:** Delivered substantial improvements in memory footprint by assigning lower bit-widths to less sensitive experts.
*   **Scalability:** Demonstrated robustness across different model scales.
*   **Distribution Analysis:** Visual analysis confirmed that larger models require a more complex, scattered bit-width distribution to maintain accuracy.

---

## üöÄ Contributions

*   **Novel Algorithm:** Introduction of **MoPEQ**, the first specific Post Training Quantization algorithm detailed in this abstract to apply mixed precision quantization at the individual expert level for VLM-MoEs.
*   **Technique Advancement:** Advancement of quantization techniques by substituting standard activation frequency metrics with **Hessian trace approximation** to determine expert importance and sensitivity.
*   **Hardware Enablement:** Provision of a new method to decouple model size from accuracy in MoE architectures, enabling the deployment of massive models on resource-constrained hardware.
*   **Benchmarking Analysis:** Contribution of a thorough analysis regarding the impact of expert activation frequency versus sensitivity across varying bit-widths (2-4 bits) on modern SOTA VLM architectures.

---

**Quality Score:** 8/10  
**References:** 40 citations