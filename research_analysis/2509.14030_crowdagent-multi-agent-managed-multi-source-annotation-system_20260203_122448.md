---
title: 'CrowdAgent: Multi-Agent Managed Multi-Source Annotation System'
arxiv_id: '2509.1403'
source_url: https://arxiv.org/abs/2509.14030
generated_at: '2026-02-03T12:24:48'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CrowdAgent: Multi-Agent Managed Multi-Source Annotation System

*Maosheng Qin; Renyu Zhu; Mingxuan Xia; Chenkai Chen; Zhen Zhu; Minmin Lin; Junbo Zhao; Lu Xu; Changjie Fan; Runze Wu; Haobo Wang*

---

### ðŸ“Œ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 40 |
| **Core Innovation** | Virtual Annotation Company (Multi-Agent System) |
| **Methodology** | ReAct-style reasoning with end-to-end process control |
| **Best Performance** | 98.21% Accuracy on COV-CTR dataset |

---

## Executive Summary

Current data annotation systems primarily focus on the isolated act of labeling, lacking the infrastructure to manage the complex, end-to-end lifecycle of data production. This creates a critical gap where the optimization of task assignment, the synergistic integration of diverse workforces, and the management of cost-quality trade-offs are treated as separate or manual processes. As reliance on Large Language Models (LLMs) and Small Language Models (SLMs) alongside human annotators grows, the field lacks a unified mechanism to orchestrate these heterogeneous resources efficiently. This paper addresses this limitation by identifying the need for **holistic process control**â€”a system that moves beyond simple labeling to manage the entire "virtual annotation" workflow, ensuring that diverse agents collaborate rationally rather than operating in silos.

The core innovation is **CrowdAgent**, a multi-agent system designed to function as a "virtual annotation company" utilizing ReAct-style reasoning for end-to-end process control. Unlike traditional pipelines, CrowdAgent introduces a hierarchical agent structure comprising a Scheduling Agent (central decision-maker), a QA Agent (performing iterative Bayesian inference with confusion matrices), a Financing Agent (analyzing cost-effectiveness), and diverse Annotation Agents (LLMs, SLMs, and Humans). Technically, the system employs a sophisticated workflow where LLMs generate initial labels using diverse prompts and In-Context Learning (ICL), followed by SLMs that perform denoising via noisy-label learning and Gaussian Mixture Models. The Scheduling Agent uses reflection mechanisms to dynamically assign tasks based on real-time feedback from the QA and Financing agents, ensuring that difficult samples are routed to human experts while routine tasks are handled efficiently by models.

CrowdAgent was rigorously evaluated against strong baselines, including GPT-4o mini, FreeAL, and Active Learning methods, across six diverse multimodal classification tasks. The system achieved state-of-the-art (SOTA) accuracy on all datasets, with specific scores of 89.25% on Human Cri-Info, 89.37% on Cri-Hum, 65.79% on Cri-Dam, 85.66% on MM-IMDb, 98.21% on COV-CTR, and 88.45% on V-SNLI. Notably, CrowdAgent demonstrated significant performance gains on challenging benchmarks, improving accuracy by +8.65% on Cri-Dam and +5.92% on V-SNLI compared to baselines. Furthermore, the system outperformed pure LLM baselines by a substantial margin of 10 to 25 percentage points, validating the efficacy of its collaborative, multi-source approach.

The significance of CrowdAgent lies in its paradigm shift from isolated labeling algorithms to holistic, management-oriented annotation systems. By successfully integrating LLMs, SLMs, and human experts into a synergistic workflow, the research establishes a new standard for process control in data-centric AI. This approach provides a practical framework for researchers and industry practitioners to optimize both the quality and cost of data annotation, moving beyond the limitations of fully automated or purely manual methods. The release of the source code and video demo further amplifies this impact, offering a reproducible and extensible foundation for future research into multi-agent orchestration and resource-aware machine learning pipelines.

---

## Key Findings

*   **Holistic Process Control:** CrowdAgent successfully addresses the critical gap in annotation systems by managing the holistic process controlâ€”integrating task assignment, data annotation, and quality/cost management.
*   **Synergistic Workforce:** Enables Large Language Models (LLMs), Small Language Models (SLMs), and human experts to work together synergistically.
*   **Complex Management:** Effectively manages complex scheduling and quality-cost trade-offs.
*   **Validated Performance:** Extensive experiments on six diverse multimodal classification tasks demonstrate the effectiveness of the proposed system.

---

## Methodology

The researchers introduced **CrowdAgent**, a multi-agent system inspired by real-world crowdsourcing companies. The methodology centers on **end-to-end process control**, which integrates three core components:

1.  **Task Assignment:** A novel methodology for rational task distribution.
2.  **Data Annotation:** The actual labeling execution.
3.  **Quality/Cost Management:** Dynamic oversight of output standards and financial expenditure.

This approach facilitates a collaborative annotation workflow where LLMs, SLMs, and human agents advance together based on rational scheduling decisions.

---

## Contributions

*   Identified and addressed the limitation in current methods that focus solely on labeling by introducing a solution for holistic process control.
*   Developed a unified multi-agent system (CrowdAgent) that manages diverse annotation sources to advance synergistically.
*   Proposed a specific methodology for the rational assignment of tasks to optimize collaboration.
*   Provided extensive experimental validation across six multimodal classification tasks and released the source code and video demo for further research.

---

## Technical Details

CrowdAgent is a multi-agent system designed as a **'virtual annotation company'** utilizing **ReAct-style reasoning**.

### Key Components

*   **Annotation Agents:**
    *   **LLMs:** Utilize diverse prompts and In-Context Learning (ICL).
    *   **SLMs:** Utilize noisy-label learning with Gaussian Mixture Models.
    *   **Humans:** Handle challenging samples.
*   **QA Agent:** Performs iterative Bayesian inference with confusion matrices and profiling.
*   **Financing Agent:** Analyzes cost-effectiveness of LLM, SLM, and Human costs.
*   **Scheduling Agent:** The central decision-maker using reflection mechanisms.

### Workflow
The process proceeds through four stages:
1.  **LLM Labeling:** Initial generation of labels.
2.  **SLM Denoising:** Refinement via noisy-label learning.
3.  **QA Analysis:** Quality assessment and profiling.
4.  **Iterative Assignment:** Dynamic re-routing of tasks based on feedback.

---

## Results

CrowdAgent was evaluated on six multimodal classification tasks against baselines like GPT-4o mini, FreeAL, and Active Learning methods. It achieved **state-of-the-art accuracy on all datasets**:

| Dataset | Accuracy | Notable Gain |
| :--- | :--- | :--- |
| **Human Cri-Info** | 89.25% | - |
| **Cri-Hum** | 89.37% | - |
| **Cri-Dam** | 65.79% | **+8.65%** |
| **MM-IMDb** | 85.66% | - |
| **COV-CTR** | 98.21% | - |
| **V-SNLI** | 88.45% | **+5.92%** |

*   **Overall Performance:** The model demonstrated significant performance gains, particularly on Cri-Dam and V-SNLI.
*   **Benchmark Comparison:** Outperformed pure LLM baselines by **10-25 percentage points**.

---

**References:** 40 citations