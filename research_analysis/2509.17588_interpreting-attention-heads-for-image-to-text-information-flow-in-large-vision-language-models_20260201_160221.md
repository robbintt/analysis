# Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models

*Authors: Jinyeong Kim; Seil Kang; Jiwoo Park; Junhyeok Kim; Seong Jae Hwang*

***

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Models Analyzed** | 10 (LLaVA-1.5, LLaVA-NeXT, InternVL2.5, Qwen2-VL) |
| **Dataset** | 200 COCO Images |
| **Task** | Visual Object Identification |
| **Key Metrics** | Faithfulness > 0.8, Completeness < 0.2 |

***

## Executive Summary

> **Overview**
> Large Vision-Language Models (LVLMs) have achieved remarkable performance in tasks like visual question answering, yet their internal decision-making processes remain opaque "black boxes." Specifically, the mechanism by which these models transfer visual information from image encoders to textual outputs is poorly understood. This lack of mechanistic interpretability poses significant challenges for diagnosing errors, ensuring safety, and improving model architectures. Without understanding how and where cross-modal information flows, researchers are limited to trial-and-error optimization rather than targeted structural improvements.

> **The Solution**
> To address this, the authors introduce "Head Attribution" (HeAr), a novel framework adapted from component attribution methods to dissect LVLMs at a granular level. HeAr quantifies the contribution of individual attention heads by fitting a linear model to the logits generated during the systematic ablation of specific head subsets. Technically, the method employs "Mean Ablation," which replaces specific image key-value pairs with the mean representations derived from the dataset baseline (rather than random noise) to rigorously isolate the signal contributed by each component. By analyzing the resulting attribution coefficients, the researchers can distinguish between critical and redundant heads, as well as trace the propagation of semantic content through the model's layers.

> **Outcomes**
> The study evaluated HeAr across ten distinct models (including LLaVA and Qwen2-VL) using 200 COCO images, validating the method with a Faithfulness score greater than 0.8 and Completeness below 0.2. The analysis revealed significant functional redundancy: in LLaVA-1.5-7B, ablating a single head resulted in less than a 5% logit difference in 80% of cases, with larger models exhibiting more redundancy than smaller ones. Despite this redundancy, the researchers identified a distinct subset of "critical heads" located in middle-to-later layers that drive the majority of image-to-text flow. Crucially, the analysis uncovered a specific Sequential Information Propagation pattern where text information propagates to role-related tokens and the final token before the model integrates image information. Furthermore, the study found that significant image information is embedded not only in object-related tokens but also substantially within background tokens.

> **Conclusion**
> t-SNE visualizations confirmed that these critical heads are activated by semantic image content rather than low-level visual features. This research establishes that attention-head level analysis is not only viable but essential for understanding the internal mechanics of LVLMs. By revealing that visual grounding follows a structured, semantically governed process, the paper provides a concrete roadmap for future interpretability research. The HeAr framework offers the field a powerful new tool for model debugging and selection, shifting the focus from purely performance-based evaluation to a deeper, mechanistic understanding of how multi-modal models process and synthesize information.

***

## Key Findings

*   **Specialized Function of Attention Heads:** A distinct, consistent subset of attention heads is responsible for facilitating the majority of image-to-text information flow, while others prove redundant.
*   **Semantic Governance:** The selection and activation of these critical attention heads are determined by the semantic content of the input image rather than its low-level visual appearance.
*   **Sequential Information Propagation:** Text information propagates to role-related tokens and the final token before the model integrates image information, indicating a specific sequence in cross-modal synthesis.
*   **Image Token Embedding:** Image information is embedded not only in object-related tokens but also significantly in background tokens, expanding the understanding of visual feature storage.
*   **Structured Process:** The internal mechanism of LVLMs for visual question answering follows a structured, interpretable process rather than a chaotic distribution of information.

***

## Methodology

The researchers employed a multi-step approach to dissect the internal workings of Large Vision-Language Models:

1.  **Head Attribution:** The authors developed a novel technique called **'head attribution,'** inspired by component attribution methods, to isolate and evaluate the contribution of individual attention heads.
2.  **Pattern Identification:** This technique was used to identify consistent patterns among attention heads to determine which specific heads play a pivotal role in transferring information.
3.  **Granular Analysis:** The researchers conducted analysis at both the head level (to identify key subsets) and the token level (to trace the specific flow of text vs. image information).

***

## Technical Details

*   **Core Technique:** The approach utilizes **Head Attribution (HeAr)**, adapting component attribution to interpret LVLMs by fitting a linear model to logits generated during repeated ablation of attention head subsets.
*   **Quantification:** Attribution coefficients ($\theta$) quantify each head's contribution to image-to-text information flow.
*   **Ablation Method:** The method employs **Mean Ablation**, which replaces image key/value pairs with baseline mean representations to avoid noise.
*   **Experimental Setup:**
    *   **Task:** Visual Object Identification.
    *   **Models:** 10 models including LLaVA-1.5, LLaVA-NeXT, InternVL2.5, and Qwen2-VL.
    *   **Dataset:** 200 COCO images.
*   **Evaluation Metrics:** Validity is measured using **Faithfulness** ($\pi(x)$) and **Completeness** ($\pi(1-x)$).

***

## Results

*   **Redundancy and Self-Repair:** Single head ablation shows minimal impact (in LLaVA-1.5-7B, 80% of cases saw <5% logit difference), suggesting self-repair mechanisms within the models.
*   **Framework Performance:** Head Attribution achieves **Faithfulness > 0.8** and **Completeness < 0.2** using fewer heads than baselines, indicating high efficiency in identifying critical components.
*   **Model Scaling:** Findings indicate that larger models exhibit more redundancy than their smaller counterparts.
*   **Localization of Critical Heads:** Critical heads are located primarily in **middle-to-later layers** of the network.
*   **Attention Weights vs. Attribution:** There is no correlation between attribution coefficients and standard image attention weights, suggesting traditional attention visualization misses critical flow information.
*   **Semantic Clustering:** t-SNE visualization reveals semantic clustering of head usage based on object categories, confirming semantic governance.

***

## Contributions

*   **Novel Interpretability Framework:** Introduced 'head attribution' as a viable method for dissecting the complex 'black box' of Large Vision-Language Models (LVLMs).
*   **Mechanistic Insight:** Provided empirical evidence shedding light on the mechanism of image-to-text flow, specifically clarifying how semantic content dictates model attention.
*   **Direction for Future Research:** Established that attention-head level analysis is a promising and necessary direction for understanding and improving the internal workings of LVLMs.