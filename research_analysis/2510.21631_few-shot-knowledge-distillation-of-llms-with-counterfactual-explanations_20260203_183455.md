---
title: Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations
arxiv_id: '2510.21631'
source_url: https://arxiv.org/abs/2510.21631
generated_at: '2026-02-03T18:34:55'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations

*Faisal Hamman; Pasan Dissanayake; Yanjun Fu; Sanghamitra Dutta*

---

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Key Innovation:** Counterfactual-explanation-infused Distillation (CoD)
> *   **Data Efficiency:** Achieves superior results with **50%** fewer real samples
> *   **Sample Range:** 8â€“512 samples (Few-shot regime)
> *   **Top Performance Gains:**
>     *   **IMDB:** +12.1% accuracy (at $k=8$)
>     *   **SST2:** +10.2% accuracy (at $k=8$)
> *   **Models Tested:** DeBERTa-v3, Qwen2.5

---

## Executive Summary

Knowledge distillation (KD) is a critical technique for compressing Large Language Models (LLMs) into smaller, efficient student models, yet it faces a significant bottleneck in few-shot learning regimes. Standard KD methods typically require large volumes of labeled data to effectively transfer knowledge from a teacher to a student. In scenarios where data is scarce (e.g., $k=8$ to $512$ samples), traditional approaches struggle to accurately estimate student parameters and align decision boundaries, leading to suboptimal performance. This paper addresses the challenge of data scarcity in task-aware distillation, aiming to maximize model performance when labeled training examples are severely limited.

The authors introduce **Counterfactual-explanation-infused Distillation (CoD)**, a framework that augments the training set using Counterfactual Explanations (CFEs). CFEs are synthetic samples generated through minimal perturbationâ€”using gradient-based methods for synthetic data or LLMs for textâ€”that are specifically designed to flip the teacher model's prediction. CoD optimizes data efficiency by splitting the training budget ($k$) into two equal halves: utilizing $k/2$ real samples and $k/2$ generated CFEs. Grounded in statistical theory for improved parameter estimation and geometric theory for mapping decision boundaries, CoD trains the student model by minimizing a combined loss function comprising task-specific cross-entropy, KL divergence (distillation loss), and Mean Squared Error (MSE) on hidden activations (layer-wise distillation).

Experiments conducted across six text classification datasets with teacher-student pairs (DeBERTa-v3 and Qwen2.5) demonstrated that **CoD significantly outperforms standard KD and Layer-Wise Distillation (LWD) baselines**, particularly in extreme few-shot settings. At a sample size of $k=8$, CoD improved accuracy on the IMDB dataset by **12.1%** (from 0.714 to 0.835) and on SST2 by **10.2%** (from 0.617 to 0.719). While the performance gap narrowed at larger sample sizes, CoD maintained a consistent advantage; at $k=512$, it achieved an accuracy of 0.860 on Amazon Polarity compared to the baseline's 0.846. These results confirm that replacing half of the real training data with boundary-probing CFEs yields superior student-teacher alignment.

This research establishes a significant theoretical and empirical advance in data-efficient model compression by validating CFEs as superior knowledge probes compared to raw data. The paper provides theoretical guarantees from both statistical and geometric perspectives, demonstrating that samples near decision boundaries offer higher information density for knowledge transfer. By proving that high-fidelity distillation can be achieved with half the number of original real-world samples, CoD reduces the dependency on expensive data labeling and curation. This work paves the way for more robust deployment of compressed models in low-resource environments and highlights the potential of explanation-based synthetic data generation in future distillation research.

---

## Key Findings

*   **Superior Few-Shot Performance:** Delivers outstanding results in few-shot regimes ranging from **8 to 512 samples**.
*   **High Data Efficiency:** Effective training using only **half** the number of original real-world samples by supplementing with synthetic data.
*   **Effective Knowledge Probes:** Counterfactual Explanations (CFEs) are proven to be highly effective for mimicking teacher decision boundaries.
*   **Broad Generalizability:** The methodology demonstrates consistent performance across multiple datasets and various LLM architectures.

---

## Methodology

The authors propose **Counterfactual-explanation-infused Distillation (CoD)**, a novel strategy for task-aware knowledge distillation in few-shot learning environments.

*   **Core Concept:** The method infuses **Counterfactual Explanations (CFEs)** into the distillation process. CFEs are defined as inputs that undergo minimum perturbation to flip the teacher model's prediction.
*   **Theoretical Foundation:** The approach is grounded in two main theories:
    *   **Statistical Theory:** For improving parameter estimation.
    *   **Geometric Theory:** For accurately mapping decision boundaries.

---

## Technical Details

### Framework Architecture
The **CoD (Counterfactual Explanation-Infused Distillation)** framework integrates CFEs directly into the knowledge distillation pipeline for few-shot learning.

### Data Strategy
*   **Budget Split:** The total training budget ($k$) is split evenly:
    *   **50%** Real samples
    *   **50%** Generated CFEs
*   **Generation Methods:**
    *   **Gradient-based:** Used for synthetic data.
    *   **LLM-based:** Used for text generation.

### Loss Function
The student model is trained to minimize a composite loss function consisting of three components:
1.  **Task-Specific Loss:** Cross-entropy.
2.  **Distillation Loss:** Kullback-Leibler (KL) divergence.
3.  **Layer-Wise Distillation Loss:** Mean Squared Error (MSE) calculated on hidden activations.

### Experimental Setup
*   **Models:** Experiments utilized **DeBERTa-v3** and **Qwen2.5** as teacher-student pairs.
*   **Scope:** Validated across 6 text classification datasets.

---

## Results

Experiments were conducted in few-shot regimes ($k=8$ to $512$) on six text classification datasets. CoD consistently outperformed Standard KD and Layer-Wise Distillation (LWD).

*   **Extreme Few-Shot ($k=8$):**
    *   **IMDB:** Accuracy improved by **12.1%** (0.714 $\to$ 0.835).
    *   **SST2:** Accuracy improved by **10.2%** (0.617 $\to$ 0.719).
*   **Larger Sample Sizes ($k=512$):**
    *   While the performance gap narrowed compared to extreme few-shot, CoD maintained a lead.
    *   **Amazon Polarity:** CoD achieved **0.860** accuracy vs. **0.846** for the baseline.
*   **Overall Trend:** CoD consistently surpassed LWD baselines, demonstrating that using a mix of $k/2$ real samples and $k/2$ CFEs significantly improves student-teacher alignment.

---

## Contributions

*   **Solves Data Scarcity:** Directly addresses the challenges of limited labeled data in few-shot settings.
*   **Theoretical Guarantees:** Provides rigorous theoretical backing for using CFEs from both statistical and geometric perspectives.
*   **Boundary Optimization:** Demonstrates that CFEs located near decision boundaries provide superior information density for knowledge transfer compared to raw data.