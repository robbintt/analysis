---
title: 'Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous
  Scores'
arxiv_id: '2601.13885'
source_url: https://arxiv.org/abs/2601.13885
generated_at: '2026-01-28T00:52:15'
quality_score: 9
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores

*Nigel Collier, Marco Basaldella, Esma Balk, Alice Pernthaller*

---

> ### Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Evaluation Reduction** | Uses only **2%** of total items |
> | **Ranking Improvement** | **+0.12** Kendall's Tau vs. random sampling |
> | **High-Confidence Accuracy** | **95%** |
> | **Benchmarks Validated** | **5**, covering n-gram, embedding, and LLM judges |
> | **Quality Score** | **9/10** |

---

## Executive Summary

**Problem**
The rapid proliferation of Large Language Models (LLMs) has created a significant scalability challenge for evaluation methodologies. Traditional comprehensive benchmarking is computationally prohibitive, as it requires exhaustively running every model against every item in a dataset to generate reliable rankings. This paper addresses the need to drastically reduce evaluation costs without sacrificing the statistical reliability of model comparisons, a critical requirement for keeping pace with the release of new models.

**Innovation**
The core innovation is the extension of Item Response Theory (IRT)-based Computerized Adaptive Testing (CAT) to accommodate continuous bounded scores (e.g., probabilities or float outputs) rather than binary correct/incorrect outcomes. Technically, the authors replace the standard Bernoulli response model with a heteroskedastic normal distribution to model generative outputs. This "Continuous IRT" model defines the output mean via a logistic function and variance via a noise parameter, estimated using Maximum Likelihood and method-of-moments. Coupled with this is an uncertainty-aware Adaptive Multi-Model Ranking algorithm that estimates pairwise confidence using normal approximation and implements adaptive stopping criteria to dynamically allocate budget only where statistical distinction between models is needed.

**Results**
The proposed framework demonstrates exceptional efficiency, achieving reliable model rankings using only 2% of the total items in a dataset compared to exhaustive evaluation. In comparative tests, the method improved ranking correlation by +0.12 (Kendall’s Tau) relative to random sampling strategies. Additionally, the system achieved 95% accuracy on predictions made with high confidence. These results were validated across five diverse benchmarks, covering n-gram-based metrics (BLEU, ROUGE), embedding-based metrics (BERTScore, COMET), and LLM-as-a-Judge evaluations.

**Impact**
This work establishes a principled statistical foundation for integrating adaptive testing into LLM evaluation workflows. By successfully applying adaptive testing to continuous scores and diverse metric types, the authors demonstrate that superior ranking correlations can be maintained with minimal data. This shift from static sampling to cost-aware, adaptive evaluation provides a path forward for researchers to benchmark models rigorously while significantly reducing computational overhead and resource expenditure.

---

## Key Findings

*   **Drastic Effort Reduction:** The proposed method drastically reduces evaluation efforts, achieving reliable model rankings using only **2%** of the total items.
*   **Superior Correlation:** It outperforms random sampling, improving ranking correlation by **0.12 (Kendall’s Tau)**.
*   **High Accuracy:** The method achieves **95% accuracy** on predictions identified as high-confidence.
*   **Broad Validation:** The approach was successfully validated across **five benchmarks** covering diverse metric types including:
    *   N-gram-based evaluations
    *   Embedding-based evaluations
    *   LLM-as-a-Judge evaluations

---

## Methodology

To address the limitations of traditional exhaustive testing, the researchers extended Item Response Theory (IRT)-based Computerized Adaptive Testing (CAT). This extension was necessary to accommodate the specific nature of LLM outputs, which often manifest as continuous bounded scores rather than simple binary outcomes.

The primary technical changes included:
*   **Distribution Replacement:** Replacing the standard Bernoulli response distribution with a **heteroskedastic normal distribution** to better model generative outputs.
*   **Uncertainty-Aware Ranking:** Implementation of an uncertainty-aware ranker that utilizes adaptive stopping criteria. This allows the system to minimize cost once a reliable ranking has been statistically established.

---

## Contributions

The paper makes three distinct contributions to the field of LLM evaluation:

1.  **Statistical Foundation:** It provides a principled statistical foundation for applying adaptive testing to LLM evaluation using continuous scores.
2.  **Cost-Effective Framework:** It introduces a cost-effective evaluation framework through an uncertainty-aware mechanism equipped with adaptive stopping capabilities.
3.  **Superior Performance:** The work establishes that adaptive testing yields superior ranking correlations with minimal data across various evaluation metrics when compared to static sampling methods.

---

## Technical Details

The proposed method combines a **Continuous Extension of Item Response Theory (IRT)** with an **Adaptive Multi-Model Ranking algorithm**.

### The Continuous IRT Model
*   **Modification:** Updates the standard 1PL model to handle continuous scores in the range [0,1].
*   **Distribution:** Utilizes a **Heteroskedastic Normal distribution**.
    *   **Mean:** Defined by a logistic function.
    *   **Variance:** Dependent on a noise parameter $k$.
*   **Parameter Estimation:**
    *   **Item Difficulty:** Estimated via Maximum Likelihood.
    *   **Noise Parameter:** Estimated via method-of-moments.
    *   **Filtering:** Includes filtering for negative correlations.

### The Adaptive Multi-Model Ranking Algorithm
*   **Confidence Estimation:** Estimates pairwise confidence using normal approximation.
*   **Adaptive Stopping:** Implements stopping criteria based on statistical differentiation between adjacent pairs or budget limits.
*   **Resource Allocation:** Utilizes cost-aware allocation to minimize uncertainty per unit cost.

---

## Results

The deployment of this method yielded significant improvements in evaluation efficiency and reliability:

*   **Reduced Data Requirement:** Achieved reliable rankings using only **2% of total items**.
*   **Improved Metrics:** Improved ranking correlation by **+0.12 (Kendall's Tau)** relative to random sampling.
*   **Precision:** Achieved **95% accuracy** on high-confidence predictions.
*   **Validation Scope:** Tested across **5 benchmarks**, including:
    *   **N-gram-based:** BLEU, ROUGE
    *   **Embedding-based:** BERTScore, COMET
    *   **LLM-as-a-Judge Metrics**

---

**Quality Score:** 9/10  
**References:** 24 citations