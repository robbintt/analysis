# Interpreting Adversarial Attacks and Defences using Architectures with Enhanced Interpretability
*Akshay G Rao; Chandrashekhar Lakshminarayanan; Arun Rajkumar*

---

## Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Total Citations** | 40 |
| **Key Architectures** | Deep Linearly Gated Networks (DLGN), FC-DLGN-W4-D3 |
| **Training Paradigms** | PGD-Adversarial Training (PGD-AT) vs. Standard Training (STD-TR) |
| **Datasets Used** | MNIST, Fashion MNIST |

---

## Executive Summary

Adversarial attacks expose critical vulnerabilities in deep learning, yet the mechanisms behind effective defenses like PGD-Adversarial Training (PGD-AT) remain largely opaque. This research addresses the fundamental challenge of interpreting robustness by moving beyond black-box evaluation metrics to investigate the geometric and architectural properties that distinguish robust models from standard ones.

The key innovation lies in the application of **Deep Linearly Gated Networks (DLGN)**, which linearizes non-linearities via a gating mechanism, decomposing the network into a linear Feature Network and a Value Network. This decomposition enables novel analytical techniques, including **Hyperplane Alignment with PCA** and **Path Activity Analysis**.

Experiments on MNIST and Fashion MNIST yielded concrete quantitative distinctions:
*   **Fully Connected (FC) Layers:** Both PGD-AT and STD-TR models align decision hyperplanes with principal components. However, PGD-AT hyperplanes are positioned significantly farther from data points.
*   **CNN Layers:** The primary driver of defense is the creation of diverse, non-overlapping active subnetworks across classes, preventing "attack-induced gating overlaps" found in standard models.

This研究 shifts the paradigm from empirical evaluation to a mechanistic understanding, suggesting that future designs should prioritize pathway diversity and separation to inherently resist perturbations.

---

## Key Findings

*   **Hyperplane Alignment with PCA:** In both PGD-Adversarial Training (PGD-AT) and Standard Training (STD-TR) models, the decision hyperplanes resemble principal components.
*   **Distance from Data Points:** A critical distinction between robust and standard models is that PGD-AT hyperplanes are aligned farther away from the data points compared to STD-TR models.
*   **Subnetwork Diversity:** PGD-AT models create diverse, non-overlapping active subnetworks across different classes.
*   **Defense Mechanism:** The specific arrangement of paths in PGD-AT models prevents attack-induced gating overlaps, providing a structural explanation for robustness against adversarial attacks.

---

## Methodology

*   **Architecture:**
    *   Utilization of **Deep Linearly Gated Networks (DLGN)**, selected specifically for their enhanced interpretability compared to standard deep networks.
    *   The study analyzes both fully connected (FC) layers and CNN layers within the DLGN framework.
*   **Comparative Analysis:**
    *   Direct comparison between models trained with **PGD adversarial training** (robust models) versus **Standard Training (STD-TR)**.
*   **Property Analysis (FC Layers):**
    *   Investigation of feature networks regarding hyperplane alignment.
    *   Analysis of the relationship between hyperplanes and Principal Component Analysis (PCA).
    *   Examination of sub-network overlap among classes.
*   **Gating Pattern Analysis (CNN Layers):**
    *   Qualitative analysis using visualizations.
    *   Quantitative measures to contrast gating patterns between the two training methods.
*   **Path Activity Analysis:**
    *   A specific technique used to examine the diversity of active subnetworks and the potential for gating overlaps.

---

## Technical Details

*   **Network Architecture**
    *   **Model:** Deep Linearly Gated Networks (DLGN), an evolution of DGNs relying on active sub-network learning.
    *   **Decomposition:** Splits into a linear Feature Network (Neural Path Features/NPFs) and a Value Network (Neural Path Values/NPVs).
    *   **Gating Mechanism:** Defined as `GALU = x * Gate(x')`.

*   **Training Methodologies**
    *   **PGD-Adversarial Training (PGD-AT):** Implemented as a min-max optimization problem.
    *   **Standard Training (STD-TR):** Used as the experimental baseline.

*   **Theoretical Analysis**
    *   **Hyperplane Alignment via PCA:** Evaluates weight alignment using the similarity metric:
        $$C_l = P^T E_l$$

---

## Results

Experiments conducted using **FC-DLGN-W4-D3** on MNIST and Fashion MNIST datasets yielded the following outcomes:

*   **PCA Dimensionality Reduction:** Found to be in conflict with adversarial robustness. Applying PCA caused significant drops in both PGD-40 robust accuracy and Clean Accuracy for PGD-AT models.
*   **Geometric Positioning:** Visualizations revealed that PGD-AT hyperplanes are positioned farther from data points than STD-TR models. Additionally, PGD-AT alters feature network weight alignment with principal components.
*   **Structural Robustness:** The robustness of PGD-AT is attributed to diverse active subnetworks that prevent attack-induced gating overlaps.
*   **Performance Metrics (MNIST):**
    *   **STD-TR:** 98.9% Clean Accuracy, 0% PGD-40 Robust Accuracy.
    *   **PGD-AT:** 96.2% Clean Accuracy, 92.8% PGD-40 Robust Accuracy.
*   **Performance Metrics (Fashion MNIST):**
    *   **PGD-AT:** Achieved 70.3% Robust Accuracy compared to 0% for STD-TR.

---

## Contributions

*   **Interpretability of Robustness:** Provides an interpretable structural analysis of adversarial training, moving beyond black-box performance metrics to understand *why* PGD-AT works.
*   **Geometric Insights:** Offers new insights into the geometric properties of robust models, specifically describing how robust models position their decision boundaries (hyperplanes) relative to data and PCA.
*   **Mechanistic Understanding of Defense:** Identifies the prevention of attack-induced gating overlaps and the maintenance of distinct active subnetworks as key mechanisms in adversarial robustness.
*   **Visualization Framework:** Introduces novel visualization and quantitative techniques to contrast the internal representations and gating behaviors of robust versus standard models.

---

**Quality Score:** 7/10  
**References:** 40 citations