# Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?

*Georg Rehm; Annika Gr√ºtzner-Zahn; Fabio Barth*

---

> ### üìä Quick Facts
>
 > *   **Quality Score:** 9/10
 > *   **References:** 17 Citations
 > *   **Target Scope:** 24 Official EU languages + over 60 regional/minority languages
 > *   **Critical Risk:** 21 languages identified as at risk of digital extinction
 > *   **Primary Strategy:** Multilingual LLMs as a "technological off-ramp"

---

## üìë Executive Summary

**Problem: The Feasibility Crisis of Native Language Models**
This paper confronts the data scarcity barrier threatening the European Union's goal of Digital Language Equality (DLE) by 2030. The authors demonstrate that the massive data requirements for training native Large Language Models (LLMs) render the development of distinct models for every European language technically and economically unfeasible, particularly for the 60+ regional and minority languages currently lacking sufficient pre-training corpora. The study estimates that **21 European languages are at immediate risk of digital extinction** if technological support is dependent on monolingual solutions, creating an urgent need for alternative pathways to support these under-resourced linguistic groups.

**Innovation: The "Technological Off-Ramp" and MLLM Architectures**
The core innovation is a proposed "technological off-ramp" strategy that positions Multilingual LLMs (MLLMs) as a viable substitute for unattainable native models. Technically, this approach relies on cross-lingual generalization and zero-shot transfer learning, allowing models trained on high-resource languages to process low-resource languages with minimal specific training data. The authors provide a granular analysis of specific architectures, contrasting generalist models like **Llama** (focused on 8 languages) and **Gemma 2** (predominantly English) with European initiatives such as **EuroLLM**, **Salamandra**, and **Teuken-7B** (all covering EU-24 languages plus others), as well as **BLOOM** and **Mistral Nemo**. The analysis further delineates model architectures from supporting infrastructure, identifying the **Common European Language Data Space (LDS)** as the necessary decentralized foundation for data sovereignty and lineage tracking.

**Results: Qualitative Assessment of Cross-Lingual Capabilities**
Rather than presenting new experimental benchmarks, this analytical review synthesizes qualitative evidence to assess model performance. The analysis indicates that models such as **Llama** and **Gemma 2** retain strong capabilities in low-resource languages‚Äîincluding Romanian, Slovenian, Slovakian, and Latvian‚Äîeven when trained predominantly on English data. Furthermore, the review finds that models trained on more linguistically balanced datasets can compete effectively with those heavily skewed toward English, with European initiatives like **BLOOM** and **Mistral Nemo** demonstrating consistent performance across the 24 official EU languages. These findings confirm that MLLMs can outperform the hypothetical potential of monolingual models trained on under-resourced data.

**Impact: Redefining the Pathway to Digital Language Equality**
This research significantly influences European Language Technology by validating the "off-ramp" strategy as the only practical pathway to achieving the 2030 DLE goals. By confirming that MLLMs can effectively support under-resourced languages where native development is impossible, the study shifts the strategic focus from monolingual solutions to optimized multilingual approaches. The paper serves as a critical call to action for the systematic adoption of these models, while explicitly identifying open questions regarding data infrastructure implementation that must be resolved to transition this strategy from theoretical validation to widespread practice.

---

## üîë Key Findings

*   **Data Resource Constraints:** Large Language Models (LLMs) require vast pre-training data, making the creation of native LLMs for under-resourced languages technically impossible.
*   **Retention of Capabilities:** Multilingual LLMs retain strong capabilities for under-resourced languages even when trained on minority amounts of data specific to those languages.
*   **The "Off-Ramp" Solution:** Multilingual LLMs offer a 'technological off-ramp', providing a viable and necessary pathway for languages that cannot support their own native models.
*   **Implementation Challenges:** While the theory is sound, the systematic application of this approach currently faces unresolved open questions that must be addressed.

---

## ‚öôÔ∏è Technical Details

**Core Strategy: The Off-Ramp Approach**
*   **Mechanism:** Utilizes Multilingual Large Language Models (MLLMs) to bypass the lack of native training data.
*   **Key Techniques:** Relies heavily on cross-lingual generalization and zero-shot transfer learning.

**Model Architectures Analyzed**
*   **Llama:** Focused on 8 languages.
*   **Gemma 2:** Predominantly trained on English.
*   **European Initiatives:**
    *   **EuroLLM:** EU-24 plus 11 others.
    *   **Salamandra:** EU-24 plus 11 others.
    *   **Teuken-7B:** EU-24.
    *   **BLOOM:** 46 languages.
    *   **Mistral Nemo:** 9 languages.

**Data Infrastructure**
*   **Common European Language Data Space (LDS):**
    *   A decentralized data system.
    *   Enforces data sovereignty.
    *   Tracks data lineage.
    *   Facilitates metadata exchange.

---

## üß™ Methodology

The paper employs a comprehensive analytical and review methodology tailored to the European linguistic context:

1.  **Theoretical Examination:** Analyzes the concept of multilingual LLMs serving as a substitute for native models.
2.  **Landscape Analysis:** Examines the current state of technology support for European languages.
3.  **Synthesis:** Aggregates and synthesizes existing related work to form a cohesive argument.

---

## üí° Contributions

*   **Concept Proposal:** Introduces the specific concept of multilingual LLMs serving as a 'technological off-ramp' for under-resourced languages.
*   **Targeted Analysis:** Provides a focused analysis of Language Technology support concerning European languages and the specific goal of 2030 Digital Language Equality.
*   **Problem Identification:** Clearly identifies the key open questions that must be solved to transition this theoretical approach into systematic practice.

---

## üìà Results

**Performance Insights**
*   **General Models:** Multilingual LLMs like **Llama** and **Gemma 2** demonstrated high accuracy on low-resource languages (e.g., Romanian, Slovenian, Slovakian, Latvian) despite being trained predominantly on English.
*   **Model Comparison:** Multilingual models consistently outperformed the theoretical potential of monolingual models trained on under-resourced data.
*   **Dataset Balance:** Models trained on more equally distributed datasets competed effectively with those trained mainly on English.
*   **European Consistency:** European-developed models (BLOOM, Mistral Nemo) showed consistent performance across all included languages.

**Scope & Risk**
*   **Target:** 24 official EU languages + over 60 regional/minority languages.
*   **Risk Assessment:** 21 languages are currently identified as at risk of digital extinction.
*   **Metrics:** The assessment is primarily qualitative; specific numerical metrics were not the focus of this analytical review.

---

**References:** 17 citations | **Quality Score:** 9/10