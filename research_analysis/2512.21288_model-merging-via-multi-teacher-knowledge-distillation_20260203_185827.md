---
title: Model Merging via Multi-Teacher Knowledge Distillation
arxiv_id: '2512.21288'
source_url: https://arxiv.org/abs/2512.21288
generated_at: '2026-02-03T18:58:27'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Model Merging via Multi-Teacher Knowledge Distillation

*Authors: Seyed Arshan Dalili; Mehrdad Mahdavi*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Key Method** | SAMerging (Sharpness-Aware Minimization) |
| **Calibration Data** | 256 unlabeled samples |
| **Inference Cost** | O(1) |
| **Performance Gain** | Up to **3.1%** improvement over baselines |
| **Benchmarks** | TA-8 (Vision & NLP) |

---

## Executive Summary

> **Problem**
> Model mergingâ€”the practice of combining multiple fine-tuned models into a single multi-task model without accessing original training dataâ€”offers a promising path to efficient deployment. However, existing methods such as Task Arithmetic often rely on heuristic scaling factors that lack theoretical grounding, leading to unstable performance and poor generalization when task distributions diverge. The core challenge addressed in this paper is establishing a principled theoretical framework for model merging that can guarantee generalization capabilities in the absence of training data, thereby moving beyond ad-hoc arithmetic operations to a robust optimization process.
>
> **Innovation**
> The authors propose **SAMerging**, a novel algorithm that reframes model merging as a Multi-Teacher Knowledge Distillation (MT-KD) problem operating on unlabeled data. The key technical innovation is the derivation of the first flatness-aware PAC-Bayes generalization bound specifically for model merging, which links the generalization error of the merged model to the "flatness" of the loss landscape. This framework introduces a "cross-task heterogeneity" term; in plain English, this is a metric that measures the degree to which the data distributions of different tasks differ from one another, effectively quantifying the level of conflict between tasks. Guided by this theory, SAMerging utilizes Sharpness-Aware Minimization (SAM) to optimize learnable layer-wise and task-wise coefficients. By seeking flat minima and minimizing the KL divergence between the student (merged model) and teachers (fine-tuned models), the method formally tightens the upper bound on excess risk.
>
> **Results**
> SAMerging was evaluated against strong baselinesâ€”including Task Arithmetic, Ties-Merging, AdaMerging, and Multi-Task Learningâ€”on the 'TA-8' benchmark spanning Vision and NLP tasks. The method achieved state-of-the-art (SOTA) performance, improving average accuracy by up to **3.1%** over the best baseline. It demonstrated superior robustness to initialization compared to AdaMerging and maintained stable performance across a wide range of scaling factors (0.1â€“1.0). Crucially, these performance gains were achieved without sacrificing efficiency; the method requires only a small, unlabeled calibration set of **256 samples** for optimization, yet maintains O(1) inference and memory costs, matching the operational efficiency of the constituent models.
>
> **Impact**
> This work significantly advances the field of model merging by bridging the gap between empirical heuristics and statistical learning theory. The introduction of a flatness-aware PAC-Bayes bound provides a rigorous mathematical foundation for understanding why merged models generalize, shifting the research focus from simple weight manipulation to loss landscape geometry. By demonstrating that sharpness-aware optimization can effectively solve merging tasks, this paper establishes a new paradigm for geometry-aware model composition, likely influencing future research in parameter-efficient fine-tuning and model soups.

---

## Key Findings

*   **Generalization & Flatness:** Established that the generalization properties of merged models are directly linked to the flatness of the loss landscape via a novel flatness-aware PAC-Bayes generalization bound.
*   **Cross-Task Heterogeneity:** Identified a 'cross-task heterogeneity' term as a critical formal measure that captures the mismatch between priors and target distributions.
*   **KL Divergence & Risk:** Demonstrated that minimizing KL divergence in multi-teacher distillation mathematically tightens the upper bound on the merged model's excess risk.
*   **SOTA Performance:** The proposed SAMerging method achieved state-of-the-art performance on both vision and NLP benchmarks, outperforming existing baselines.

---

## Methodology

The authors employed a rigorous theoretical and practical approach to model merging:

1.  **Theoretical Analysis:** They derived a flatness-aware PAC-Bayes bound tailored for model merging, allowing for the analysis of generalization without access to original training data.
2.  **Problem Reframing:** Model merging was reframed as a multi-teacher knowledge distillation problem. This process operates on unlabeled data, removing the dependency on the original training sets.
3.  **Optimization Strategy:** Guided by the theoretical bound, the authors utilized Sharpness-Aware Minimization (SAM). This technique seeks flat minima by optimizing the student model against teacher models, ensuring robustness and better generalization.

---

## Technical Details

**Core Algorithm: SAMerging**
SAMerging extends Task Arithmetic by introducing learnable layer-wise and task-wise merging coefficients. It formulates the merged model using a pretrained checkpoint and task vectors.

**Theoretical Framework**
*   **Optimization:** Coefficients are optimized via a PAC-Bayes theoretical framework.
*   **Cross-Task Heterogeneity:** A metric introduced to measure prior-target mismatch.
*   **Geometry:** Uses a flatness-aware bound based on squared gradient norms to link generalization to loss landscape geometry.

**Distillation Process**
*   **Formulation:** Frames merging as a Multi-Teacher Knowledge Distillation problem.
*   **Objective:** Minimizes the KL divergence between the student (merged model) and teachers (fine-tuned models) to tighten excess risk bounds.
*   **Optimization Technique:** Employs Sharpness-Aware Minimization (SAM) to seek wider minima for better generalization.

**Operational Requirements**
*   **Calibration:** Requires a small, unlabeled calibration set for offline calibration.
*   **Inference Cost:** Maintains O(1) inference cost.

---

## Contributions

*   **Theoretical Foundation:** Introduced the first flatness-aware PAC-Bayes generalization bound specifically designed for the model merging paradigm.
*   **Formal Definitions:** Defined the 'cross-task heterogeneity' term to formally account for distributional mismatch between tasks.
*   **Novel Algorithm:** Proposed SAMerging, a novel algorithm that utilizes Sharpness-Aware Minimization (SAM) instead of heuristic scaling strategies.
*   **Mathematical Proof:** Provided a formal proof linking the minimization of student-teacher KL divergence to the tightening of the excess risk bound.

---

## Results

*   **Benchmark Evaluation:** Evaluated on the 'TA-8' benchmark against strong baselines including Task Arithmetic, Ties-Merging, AdaMerging, and Multi-Task Learning.
*   **Accuracy & Stability:** Achieved higher and more stable average accuracy across a wide range of scaling factors (0.1â€“1.0).
*   **Robustness:** Demonstrated superior robustness to initialization compared to AdaMerging.
*   **Efficiency:** Reported State-of-the-art (SOTA) performance on Vision and NLP benchmarks while maintaining O(1) inference and memory costs, matching the efficiency of the constituent models.

---
**References:** 40 citations