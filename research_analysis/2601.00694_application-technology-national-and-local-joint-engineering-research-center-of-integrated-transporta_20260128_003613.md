---
title: Application Technology, National and Local Joint Engineering Research Center
  of Integrated Transporta
arxiv_id: '2601.00694'
source_url: https://arxiv.org/abs/2601.00694
generated_at: '2026-01-28T00:36:13'
quality_score: 9
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# PedX-LLM: Vision-and-Knowledge Enhanced Generalizable Pedestrian Crossing Behavior Analysis

*Guocong Zhai, Hong Yang, Qingwen Pu, Kun Xie*

---

> ### **Executive Summary**
>
> This research addresses the critical limitation of poor generalizability in existing pedestrian crossing behavior models. Traditional approaches, which rely on numerical pattern fitting via statistical or supervised learning methods, tend to overfit to specific training environments. Consequently, these models suffer significant performance degradation when deployed in new, unseen locations (zero-shot scenarios). This is a significant hurdle for transportation safety and autonomous driving systems, which require reliable inference of crossing intentions (intersection vs. mid-block) across diverse and dynamic built environments without requiring exhaustive retraining for every new site.
>
> The paper introduces **PedX-LLM**, a novel framework that transitions pedestrian inference from numerical regression to semantic, context-aware reasoning using Large Language Models (LLMs). Technically, the model fine-tunes the LLaMA-2-7B foundation using Low-Rank Adaptation (LoRA) with 4-bit quantization to optimize memory usage, transforming a binary classification task into a natural language generation task. The architecture integrates a multimodal input strategy: it employs a frozen CLIP ViT-L/14 encoder (via LLaVA) to extract visual features and combines this with structured domain knowledgeâ€”such as pedestrian age and weather conditionsâ€”through prompt engineering and token expansion.
>
> PedX-LLM demonstrated robust performance, achieving an in-sample balanced accuracy of **82.0%**, outperforming the best statistical model by 7.9 percentage points and the best supervised learning method by 3.0 percentage points. Crucially, the model exhibited superior zero-shot generalizability, maintaining **66.9%** balanced accuracy across five unseen test sitesâ€”surpassing baseline methods by over 18 percentage points. Furthermore, the study highlighted high sample efficiency; incorporating just five validation examples (few-shot) improved accuracy to **72.2%**.
>
> The significance of this study lies in its empirical validation that shifting from numerical pattern fitting to semantic reasoning allows models to maintain high performance in unseen environments. By quantifying the specific contributions of visual and knowledge modalities, the authors provide a blueprint for overcoming the "site-dependency" that plagues current transportation informatics models.

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **In-Sample Accuracy** | 82.0% |
| **Zero-Shot Accuracy** | 66.9% |
| **Few-Shot Accuracy** | 72.2% (5 examples) |
| **Foundation Model** | LLaMA-2-7B |
| **Quantization** | 4-bit nf4 |
| **Trainable Parameters** | ~32.5M (LoRA) |
| **Quality Score** | 9/10 |

---

## Key Findings

*   **Superior In-Sample Performance**: PedX-LLM achieved **82.0%** balanced accuracy, outperforming the best statistical model by **7.9 percentage points** and the best supervised learning method by **3.0 percentage points**.
*   **Significant Zero-Shot Generalizability**: Achieved **66.9%** balanced accuracy across five unseen test sites, surpassing baseline methods by at least **18 percentage points**.
*   **High Sample Efficiency**: Incorporating five validation examples (few-shot) improved balanced accuracy to **72.2%**.
*   **Impact of Specific Modules**: The vision-augmented module contributed a **2.9%** performance gain, while domain knowledge integration provided an additional **4.1%** improvement.

## Methodology

The study introduces PedX-LLM, a vision-and-knowledge enhanced framework designed to transition pedestrian crossing inference to generalizable behavioral reasoning. The model architecture involves fine-tuning the LLaMA-2-7B foundation model using Low-Rank Adaptation (LoRA). The framework employs a multimodal input strategy, integrating visual features extracted via LLaVA and textual domain knowledge. Validation was conducted using cross-site validation with site-based partitioning to test generalizability in zero-shot and few-shot settings.

## Technical Details

The proposed model, PedX-LLM, utilizes LLaMA-2-7B as the core foundation, transforming binary classification (intersection vs. mid-block crossing) into a natural language generation task by predicting specific tokens.

| Component | Specification |
| :--- | :--- |
| **Core Model** | LLaMA-2-7B |
| **Fine-Tuning** | Low-Rank Adaptation (LoRA) with rank $r=32$ and alpha $\alpha=64$ |
| **Trainable Params** | Approx. 32.5M |
| **Quantization** | 4-bit nf4 (to reduce memory usage) |
| **Vision Module** | LLaVA with frozen CLIP ViT-L/14 encoder (336x336 resolution) |
| **Projection** | Two-layer MLP |
| **Knowledge Integration** | Individual-level and built environment knowledge via prompt engineering and token expansion |
| **Training Hardware** | NVIDIA Quadro RTX 5000 |
| **Optimizer** | AdamW with cosine schedule |

## Results

PedX-LLM achieved an in-sample balanced accuracy of 82.0% on a dataset of 687 observations, outperforming Hierarchical Logistic Regression by 7.9 percentage points and CatBoost by 3.0 percentage points. 

*   **Ablation Studies**: Integrating vision and knowledge provided a 4.1 percentage point gain over a vision-only baseline (77.9%), with 'Age' and 'Weather' being dominant contributing factors.
*   **Generalizability**: The model achieved 66.9% balanced accuracy in zero-shot scenarios on unseen sites, surpassing baselines by at least 18 percentage points.
*   **Few-Shot Learning**: A setting with five examples improved zero-shot accuracy to 72.2%.

## Contributions

*   **Domain-Specific LLM Adaptation**: Addresses the gap in LLM applications by incorporating transportation domain knowledge and visual context.
*   **Overcoming Generalization Limits**: Demonstrates that shifting from numerical pattern fitting to semantic, context-aware reasoning allows models to maintain high performance in unseen environments.
*   **Quantified Multimodal Benefits**: Provides empirical evidence quantifying how specific modalities, such as visual environmental data and structured domain knowledge, contribute to model accuracy and generalizability.

---
**Quality Score:** 9/10  
**References:** 11 citations