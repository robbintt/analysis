# Policy gradient methods for ordinal policies

*SimÃ³n Weinberger; Jairo Cugliari*

---

## ðŸ“‹ Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 6/10 |
| **Best Configuration** | TRPO + Ordinal Policy |
| **Key Advantage** | Faster convergence, higher stability, competitive continuous handling |
| **Scope** | Simulation (Electrochromic Lens) & Real-world Hardware |

---

## ðŸ“ Executive Summary

> Standard policy gradient methods rely on a softmax parametrization that treats discrete actions as independent labels, failing to capture inherent ordinal relationships between actions in industrial control applications. This limitation results in inefficient learning and suboptimal performance, as the algorithm ignores the logical structure where $a_1 < a_2 < \dots < a_K$. The problem is particularly acute in control systems with ordered action classes, such as tinting levels, where disregarding the order leads to high variance in gradient estimation and poor sample efficiency. Addressing this distinct structural gap is essential for applying reinforcement learning effectively to complex physical systems where action sequences are naturally ranked.

To resolve this, the authors introduce an **"Ordinal Policy Parametrization"** that integrates ordinal regression models directly into the policy gradient framework. Technically, the method replaces the standard softmax with a latent variable model defined by $A^* = g_\omega(s) + \epsilon$, where $K-1$ ordered thresholds $\tau$ determine the action boundaries. The probability of selecting action $a$ is calculated as $\pi(a|s) = \sigma(\tau_a - g_\omega(s)) - \sigma(\tau_{a-1} - g_\omega(s))$, explicitly encoding the action order into the policy architecture. This approach is designed as a plug-and-play module compatible with major algorithms including REINFORCE, Natural Policy Gradient (NPG), Trust Region Policy Optimization (TRPO), and Proximal Policy Optimization (PPO). Furthermore, the methodology extends to continuous action spaces by discretizing them into ordered bins while preserving the structural benefits of the ordinal policy.

Experimental results validate the method's superiority in both simulation and real-world environments, specifically demonstrated through an electrochromic lens tint control task featuring 4 ordered action classes. When compared against standard multinomial policies across REINFORCE, NPG, and TRPO configurations, the ordinal parametrization achieved faster convergence, higher final rewards, and improved stability characterized by lower standard deviation. The Trust Region Policy Optimization (TRPO) combined with the ordinal policy emerged as the most effective configuration. Crucially, the study demonstrated that discretizing continuous action spaces using the ordinal policy yields competitive performance relative to traditional continuous control methods, and the method was proven effective in real-world hardware applications rather than just theoretical simulations.

This research significantly broadens the utility of reinforcement learning for industrial control by proving that embedding domain knowledgeâ€”specifically action orderâ€”directly into the policy structure leads to more robust and interpretable solutions. By offering a drop-in component that enhances sample efficiency and solution quality across established algorithms, the ordinal policy parametrization provides a practical path for deploying RL in physical systems where ordered discrete actions are the norm. The ability to competitively handle discretized continuous actions further ensures this approach is versatile enough for a wide range of real-world engineering challenges.

---

## ðŸ” Key Findings

*   **Standard Limitations:** The standard softmax parametrization is insufficient because it fails to capture inherent order relationships between actions.
*   **Industrial Relevance:** The proposed ordinal parametrization successfully addresses real-world industrial challenges.
*   **Continuous Action Handling:** Discretizing continuous action spaces with the ordinal policy yields competitive performance.
*   **Proven Efficacy:** The method is effective in both simulation environments and real-world hardware applications.

---

## ðŸ’¡ Contributions

*   **New Parametrization:** Introduction of a new policy parametrization for discrete actions that explicitly accounts for order relationships.
*   **Framework Integration:** Successful integration of ordinal regression concepts into the policy gradient framework.
*   **Versatility:** Demonstration of the method's versatility in handling continuous action tasks through discretization, broadening its industrial utility.

---

## âš™ï¸ Methodology

The researchers adapted ordinal regression models to the reinforcement learning setting. The core approach involves:

1.  **Incorporating Ordinality:** Integrating the ordinal nature of actions directly into the policy gradient framework rather than using the standard softmax distribution.
2.  **Continuous Space Handling:** For continuous action spaces, the methodology involves discretizing the action space and then applying the ordinal policy.

---

## ðŸ”¬ Technical Details

### Core Concept
**Ordinal Policy Parametrization** replaces standard softmax to leverage order relationships between actions ($a_1 < a_2 < \dots < a_K$).

### Mathematical Formulation
*   **Latent Variable Model:** $A^* = g_\omega(s) + \epsilon$
*   **Thresholds:** Utilizes $K-1$ ordered thresholds $\tau$.
*   **Action Probability:**
    $$ \pi(a|s) = \sigma(\tau_a - g_\omega(s)) - \sigma(\tau_{a-1} - g_\omega(s)) $$

### Algorithm Compatibility
Designed as a plug-and-play module compatible with:
*   REINFORCE
*   Natural Policy Gradient (NPG)
*   Trust Region Policy Optimization (TRPO)
*   Proximal Policy Optimization (PPO)

### Continuous Actions
Handled by discretizing bounded continuous spaces while preserving order.

---

## ðŸ“Š Results

### Simulation Environment
*   **Task:** Electrochromic lens tint control with 4 ordered classes.
*   **Reward System:** Based on user dissatisfaction.

### Training Configuration
*   **Comparison:** Ordinal vs. Multinomial policies.
*   **Algorithms Tested:** REINFORCE, NPG, and TRPO.

### Key Metrics
1.  **Convergence Speed:** Ordinal policies demonstrated **faster convergence**.
2.  **Solution Quality:** Ordinal policies achieved **higher final rewards**.
3.  **Stability:** Ordinal policies showed **lower standard deviation**; *TRPO + Ordinal Policy* was the best performer.
4.  **Continuous Actions:** Discretized ordinal policy showed **competitive performance** relative to continuous policies.

---

**REFERENCES:** 0 citations