# Adversarial Training for Defense Against Label Poisoning Attacks

*Melis Ilayda Bal; Volkan Cevher; Michael Muehlebach*

***

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Core Method** | Stackelberg Game / Bilevel Optimization |
| **Primary Application** | Label Poisoning Defense |
| **Key Benchmark** | FLORAL vs. RoBERTa |

***

> ## ðŸ’¡ Executive Summary
>
> Label poisoning attacks pose a significant threat to machine learning systems reliant on external data sources, as adversaries can manipulate training labels to degrade model performance or induce specific biases. This research addresses the critical challenge of defending against such attacks, specifically focusing on scenarios where the integrity of the training dataset is compromised.
>
> The authors introduce **FLORAL**, a novel adversarial training framework that formulates the defense mechanism as a **non-zero-sum Stackelberg game** involving bilevel optimization. Technically, the method is rooted in Kernel SVMs, where the learner minimizes loss against worst-case label perturbations devised by an attacker operating within a specific budget. The implementation utilizes Projected Gradient Descent (PGD) within the dual space, employing a "Randomized Top-k Rule" that allows the attacker to strategically poison support vectorsâ€”the most critical data points identified by dual variable magnitudes. To ensure computational scalability, the algorithm incorporates a Fixed-Point Iteration method to efficiently approximate projection operators.
>
> **Performance Highlights:**
> *   **FLORAL**: Maintained robust accuracy near **0.90** even under 40% attack budgets.
> *   **RoBERTa**: Accuracy collapsed from ~0.90 to **< 0.65** under the same conditions.
>
> The theoretical analysis further substantiates these findings, proving *Îµ-local asymptotic stability* and confirming that the algorithm converges to a neighborhood of the Stackelberg equilibrium. The significance of this work lies in its demonstration that principled, game-theoretic adversarial training can provide superior defense against label poisoning compared to massive pre-trained foundation models, which are often assumed to be robust due to their scale.

***

## ðŸ”¬ Methodology

The authors propose **FLORAL**, an adversarial training defense strategy rooted in Support Vector Machines (SVMs). The methodology is structured around the following components:

*   **Optimization Framework:** The training process is formulated as a **bilevel optimization framework**, conceptualized as a non-zero-sum Stackelberg game.
*   **Game-Theoretic Model:** The interaction is defined between two agents:
    *   **The Attacker:** Strategically poisons critical training labels.
    *   **The Model:** Aims to recover and minimize loss despite the attack.
*   **Solution Algorithm:** To solve this, the method employs a **Projected Gradient Descent (PGD)** algorithm integrated with kernel SVMs.
*   **Flexibility:** The framework is designed to be flexible, accommodating various model architectures while specifically targeting Kernel SVM classifiers.

***

## âœ… Key Findings

The empirical and theoretical evaluations yielded the following results:

*   **Superior Robust Accuracy:** FLORAL consistently achieves higher robust accuracy compared to robust baselines and foundation models (specifically RoBERTa), even as the attacker's budget increases.
*   **Effectiveness Across Tasks:** Evaluations demonstrate the method's effectiveness across diverse classification tasks.
*   **Successful Mitigation:** The strategy successfully counteracts performance degradation typically caused by label poisoning attacks without compromising the model's primary objective (utility on clean data).
*   **Algorithmic Convergence:** Theoretical analysis confirms that the proposed algorithm possesses verifiable convergence properties to a neighborhood of the Stackelberg equilibrium.

***

## ðŸ§© Technical Details

FLORAL is an adversarial training framework for label poisoning attacks. The technical implementation involves a dual formulation of a nonlinear Kernel SVM.

### Formulation & Algorithms
*   **Stackelberg Game:** The learner minimizes loss based on worst-case label perturbations, while the attacker maximizes the learner's objective by manipulating labels within a budget $k$.
*   **Algorithm 1 (Randomized Top-k Rule):**
    *   The attacker identifies critical points (support vectors) based on dual variable magnitudes.
    *   The learner updates using **Projected Gradient Descent (PGD)** in the dual space.
*   **Algorithm 2 (Optimization via Fixed-Point Iteration):**
    *   Optimizes scalability by approximating the projection operator.
    *   Enforces constraints such as $y^T\beta = 0$ and $0 \leq \lambda \leq C$ efficiently.

### Theoretical Guarantees
*   **Stability Analysis:** Verified via Lemma 1 & 2.
*   **Convergence Proof:** **Theorem 3.1** establishes *Îµ-local asymptotic stability*, proving the algorithm converges to a neighborhood of the Stackelberg equilibrium.

***

## ðŸ“ˆ Results

### Experimental Benchmark: FLORAL vs. RoBERTa
The study compared FLORAL against the RoBERTa foundation model under varying attack budgets.

| Model | Baseline Accuracy (Clean) | Accuracy at 40% Attack Budget |
| :--- | :--- | :--- |
| **RoBERTa** | ~0.90 | < 0.65 (Significant Degradation) |
| **FLORAL** | ~0.90 | **~0.90** (Robust Maintenance) |

### Outcome Analysis
*   **Susceptibility of Foundation Models:** RoBERTa showed high susceptibility to label poisoning, with accuracy dropping drastically as the attack budget increased.
*   **Resilience of FLORAL:** In contrast, FLORAL maintained robust accuracy close to the 'Clean' baseline (~0.90) even under severe 40% attack budgets, successfully mitigating performance degradation without compromising clean data performance.

***

## ðŸ“ Contributions

This paper makes three primary contributions to the field of adversarial machine learning:

1.  **Novel Defense Strategy:** Introduction of **FLORAL**, a new adversarial training method specifically designed to defend against label poisoning attacks in data-sourced environments.
2.  **Theoretical Framework:** Development of a bilevel optimization approach that models the adversarial conflict as a non-zero-sum Stackelberg game, including a rigorous theoretical proof of the algorithm's convergence.
3.  **Empirical Benchmarking:** Presentation of comprehensive empirical evidence showing that the proposed method enhances the resilience of machine learning models more effectively than current foundation models (like RoBERTa) under adversarial conditions.