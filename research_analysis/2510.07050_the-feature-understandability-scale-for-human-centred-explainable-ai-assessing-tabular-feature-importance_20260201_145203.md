# The Feature Understandability Scale for Human-Centred Explainable AI: Assessing Tabular Feature Importance

* Nicola Rossberg; Bennett Kleinberg; Barry O'Sullivan; Luca Longo; Andrea Visentin

---

> ### ‚ö° Quick Facts
> * **Data Types:** Numerical (8 items) & Categorical (9 items) scales
> * **Framework:** Psychometric / Confirmatory Factor Analysis (CFA)
> * **Core Structure:** 2-Factor Model (Understanding & Measurement | Feature-Outcome Relation)
> * **Key Outcome:** Pre-selection based on scores improves global explanation quality
> * **Score:** 9/10

---

## üìù Executive Summary

Current Explainable AI (XAI) efforts predominantly focus on elucidating model outputs, often overlooking a fundamental prerequisite for user trust: the user's comprehension of the input features themselves. This paper addresses a critical gap in human-centred XAI by recognizing that if end-users do not understand the underlying tabular data features, they cannot effectively interpret explanations or audit model decisions. The lack of standardized, quantitative metrics to evaluate how well humans understand specific input variables in supervised classification contexts has hindered the development of truly interpretable systems.

The authors introduce a novel psychometric framework featuring two validated measurement scales‚Äîspecific for numerical (8 items) and categorical data (9 items)‚Äîdesigned to quantify "feature understandability." Technically, the study utilizes Confirmatory Factor Analysis (CFA) to validate a robust two-factor structure for both scales: **"Understanding & Measurement"** (assessing definitions, units, and interpretation) and **"Feature-Outcome Relation"** (assessing relevance and utility). This structure enables the assignment of quantitative scores to input features, establishing a protocol to rank and prioritize features based on user comprehension prior to the model training phase.

The validation process demonstrated strong psychometric properties, with CFA confirming a good model fit and high factor reliability. The study reported high-performing item metrics, including "I know what this feature measures" and "In my opinion, the feature should be used to predict the outcome." Crucially, experiments revealed that pre-selecting features based on these high understandability scores resulted in a statistically significant improvement in the quality of global explanations for supervised machine learning models.

This research significantly advances the domain of human-centred AI by introducing the first quantitative instrument for assessing the understandability of input features, shifting the diagnostic focus from model logic to user cognition.

---

## üîç Key Findings

*   **Validation of Assessment Tools:** The research successfully developed and validated two distinct psychometric scales‚Äîone for numerical data (8 items) and one for categorical data (9 items)‚Äîdesigned to assess user understanding of tabular input features.
*   **Structural Integrity:** Confirmatory Factor Analysis confirmed a robust two-factor structure for both scales, demonstrating a strong relationship between the items and a good model fit.
*   **Quantification of Understandability:** The study established a method to assign quantitative scores to input features, enabling the ranking and prioritization of features based on how well users understand them.
*   **Correlation with Feature Selection:** The findings suggest that pre-selecting features based on these understandability scores can significantly improve the quality of global explanations for supervised machine learning models.

---

## üõ†Ô∏è Methodology

The researchers employed a psychometric approach to construct measurement instruments specifically for supervised classification problems involving tabular data. They differentiated between data types by creating separate scales for numerical and categorical inputs. To ensure the reliability and validity of these instruments, Confirmatory Factor Analysis (CFA) was utilized to verify the two-factor structure of the scales and evaluate the goodness of fit between the survey items and the underlying constructs of user understanding.

---

## ‚öôÔ∏è Technical Details

*   **Framework:** Psychometric quantification of 'feature understandability' via a two-factor CFA-validated model.
*   **Scale Structure:**
    1.  **Understanding & Measurement:** focuses on definitions, units, and interpretation.
    2.  **Feature-Outcome Relation:** focuses on relevance and utility.
*   **Scale Design:** Dual-scale design (8 numerical items, 9 categorical items).
*   **Scoring Protocol:** Generates quantitative scores to facilitate a pre-selection protocol.
*   **Pipeline Usage:** ranking and prioritizing features before model training to optimize global explanations within XAI pipelines.

---

## üìä Results

*   **Model Fit:** CFA confirmed 'good model fit' and robust factor reliability.
*   **High-Performing Metrics:**
    *   *"I know what this feature measures"* (Loading: 0.85, Mean: 3.96)
    *   *"I know what this feature represents"* (Loading: 0.83, Mean: 4.03)
    *   *"In my opinion, the feature should be used to predict the outcome"* (Loading: 0.88, Mean: 3.51)
*   **Performance Impact:** Pre-selecting features based on high understandability scores significantly improved global explanation quality for supervised ML models.

---

## üöÄ Contributions

*   **Introduction of a Novel Metric:** The paper addresses a significant gap in Explainable AI (XAI) by introducing the first quantitative measure designed to assess the degree to which a user understands a specific input feature.
*   **Human-Centred XAI Framework:** By shifting focus from merely explaining model outputs to evaluating the user's comprehension of input features, the work advances the domain of human-centred AI.
*   **Practical Design Implications:** The study provides system designers with a concrete tool to pre-select understandable features during the model design phase, facilitating better global explanations and more effective auditing of AI systems.

---
**Quality Score:** 9/10 | **References:** 40 citations