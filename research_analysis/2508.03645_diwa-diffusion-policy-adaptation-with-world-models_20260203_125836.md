---
title: 'DiWA: Diffusion Policy Adaptation with World Models'
arxiv_id: '2508.03645'
source_url: https://arxiv.org/abs/2508.03645
generated_at: '2026-02-03T12:58:36'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DiWA: Diffusion Policy Adaptation with World Models

*Akshay L Chandra; Iman Nematollahi; Chenguang Huang; Tim Welschehold; Wolfram Burgard; Abhinav Valada*

---

> ### üìä Quick Facts
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Benchmark:** CALVIN (8 Tasks)
> *   **Key Efficiency:** Orders of magnitude reduction in physical interactions (few hundred thousand vs. millions)
> *   **Innovation:** First successful offline adaptation of diffusion policies for real-world skills

---

## üìù Executive Summary

Fine-tuning robotic policies, particularly those based on diffusion models, using Reinforcement Learning (RL) presents a significant challenge due to extreme sample inefficiency. Standard model-free RL methods typically require millions of environment interactions to achieve effective performance, rendering them impractical for real-world robotics where physical trial-and-error is time-consuming, costly, and potentially unsafe.

The authors introduce **DiWA** (Diffusion Policy Adaptation with World Models), a novel framework that decouples policy fine-tuning from the real world by leveraging a learned world model. DiWA operates by training a Recurrent State-Space Model (RSSM) on a dataset of offline play interactions to simulate environment dynamics. This world model creates a latent-space Markov Decision Process (MDP) in which a diffusion policy can be fine-tuned using RL. Technically, the system integrates a conditional denoising diffusion policy with a reward classifier, enabling the agent to maximize cumulative reward entirely offline.

In empirical evaluations across eight distinct tasks on the CALVIN benchmark, DiWA demonstrated superior performance compared to standard imitation learning and model-free baselines such as DPPO. The framework achieved effective adaptation using only a few hundred thousand offline play interactions, representing an improvement in sample efficiency by orders of magnitude. By removing the necessity for physical interaction during the adaptation process, DiWA significantly enhances the safety and feasibility of deploying advanced robotic systems in unstructured environments.

---

## üîë Key Findings

*   **Dramatic Sample Efficiency:** DiWA requires only a few hundred thousand offline interactions for adaptation, compared to the millions required by standard methods.
*   **Superior Benchmark Performance:** Demonstrated state-of-the-art results across eight tasks on the CALVIN benchmark using exclusively offline adaptation.
*   **Safety & Practicality:** Enhances safety by eliminating the risks associated with physical trial-and-error during the fine-tuning phase.
*   **First Successful Demonstration:** Marks the first time fine-tuning of diffusion policies for real-world robotic skills has been successfully demonstrated using an offline world model.

---

## ‚ú® Contributions

*   **Novel Framework:** Introduced DiWA, bridging diffusion policies and efficient RL adaptation using world models, moving away from inefficient environment-dependent formulations.
*   **Reduced Sample Complexity:** Demonstrated a reduction in sample complexity requiring orders of magnitude fewer physical interactions than model-free baselines.
*   **Empirical Validation:** Proved that offline world models can effectively handle the complexities of diffusion policy denoising sequences for reward propagation.
*   **Open Source:** Publicly released the code to facilitate further research and reproducibility.

---

## üß† Methodology

The proposed framework, DiWA, utilizes a model-based approach to address the inefficiencies of fine-tuning diffusion policies with Reinforcement Learning. Instead of relying on direct environment interaction, DiWA leverages a world model trained on a dataset of offline play interactions.

This world model simulates environment dynamics, allowing the RL agent to fine-tune diffusion-based robotic skills entirely offline. This bypasses the need for continuous real-world data collection during the update phase, solving the issue of high sample complexity in physical spaces.

---

## ‚öôÔ∏è Technical Details

**Core Concept:** A framework for fine-tuning diffusion policies offline using reinforcement learning within a learned world model to maximize cumulative reward in the real environment via latent-space updates.

**System Components:**
*   **Recurrent State-Space Model (RSSM):** Utilized for the world model to simulate dynamics.
*   **Diffusion Policy:** Uses a conditional denoising process for action generation.
*   **Reward Classifier:** Integrated to guide the reinforcement learning process.

**Training Phases:**
1.  **World Model Learning:** Trained on play data.
2.  **Policy Pre-training:** Initialized on expert demonstrations.
3.  **Reward Classifier Training:** Learned to distinguish successful outcomes.
4.  **Offline Fine-Tuning:** Performed in latent space with frozen world model parameters.

---

## üìà Results

DiWA was evaluated on the CALVIN benchmark across **8 distinct tasks**. It demonstrated superior performance compared to standard imitation learning and other fine-tuning methods.

*   **Efficiency:** Achieved effective adaptation using only a few hundred thousand offline play interactions.
*   **Comparison:** Required orders of magnitude fewer physical interactions than model-free baselines like DPPO.
*   **Validation:** Validated that offline world models are sufficiently expressive to handle the intricacies of diffusion policy denoising.
*   **Outcome:** Resulted in robust skill acquisition without real-world data collection during the update phase.

---
**References:** 40 citations