---
title: 'MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation'
arxiv_id: '2511.03942'
source_url: https://arxiv.org/abs/2511.03942
generated_at: '2026-01-27T21:06:15'
quality_score: 8
citation_count: 4
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation

*Lun Wu, Zhi Anna, Yoon Kim*

---

## üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Backbone Model** | Llama 3.2 (1B) |
| **Tokenization** | Anticipatory Music Transformer (AMT) scheme |
| **Vocabulary Size** | ~183K (128K Llama + 55K AMT tokens) |
| **Training Hardware** | 4 x H100 GPUs |
| **Training Duration** | ~6 Days |
| **Quality (FAD)** | **0.173** (vs. 0.818 Baseline) |
| **Inference Speed** | **10.0s** (vs. 47.0s Baseline) |
| **Max RTF** | **14.17** (FP8) |

---

## üìù Executive Summary

### Problem
This research addresses the challenge of generating high-quality, complex, multitrack MIDI music directly from natural language descriptions. Previous approaches, such as the Text2midi baseline, have struggled to balance generation fidelity with computational efficiency and controllability. This is a significant problem in the field of AI music generation because effective text-to-MIDI translation requires models capable of handling intricate, multi-instrument sequences while remaining fast enough for practical use, a feat often hindered by the architectural disconnect between text and symbolic music modalities.

### Innovation
The core innovation is **MIDI-LLM**, a specialized adaptation of Large Language Models designed to bridge text and music modalities without requiring custom inference engines. Built on the Llama 3.2 (1B) backbone, the model employs a parameter-adaptation strategy comprising Vocabulary Expansion, Two-Stage Training, and Structure Preservation. Technically, the authors expand the Llama vocabulary by merging it with the Anticipatory Music Transformer (AMT) music vocabulary (55K tokens), representing notes through arrival time, duration, and instrument-pitch tokens. The model is trained in two phases‚ÄîContinued Pretraining (~3.07B tokens) and Supervised Finetuning (~5.1B tokens)‚Äîwhile meticulously maintaining the original LLM parameter structure. This preservation allows the model to leverage the vLLM library for optimized, high-performance processing that non-standard architectures cannot easily utilize.

### Results
MIDI-LLM demonstrates substantial improvements over the Text2midi baseline across quality, alignment, and efficiency metrics. In terms of generation quality measured by Fr√©chet Audio Distance (FAD), MIDI-LLM achieved a superior score of **0.173** compared to Text2midi's 0.818. For text-audio alignment (CLAP score), it reached **22.1** versus the baseline's 18.7. The most striking gains are in efficiency: inference time for a 2K sequence dropped precipitously from **47.0 seconds** (Text2midi) to **10.0 seconds** (MIDI-LLM BF16). Furthermore, the Real-Time Factor (RTF) increased from 0.56 to **14.17** (MIDI-LLM FP8), signaling a shift towards viable real-time generation capabilities.

### Impact
The significance of this research lies in proving that standard LLM architectures can be effectively adapted for complex symbolic music generation while retaining compatibility with high-performance inference libraries like vLLM. By setting a new state-of-the-art benchmark that outperforms existing models in both quality and speed, MIDI-LLM provides a viable pathway for deploying efficient, controllable music generation systems in production environments. This architecture suggests that future generative audio applications need not rely on bespoke, difficult-to-optimize engines, but can instead harness the rapid advancements occurring within the general-purpose LLM ecosystem.

---

## üîë Key Findings

*   **Superior Generation Quality:** Achieves higher quality and control than the Text2midi model.
*   **Significant Speedup:** Demonstrates drastically faster inference speeds, enabling real-time applications.
*   **Modality Bridging:** Successfully bridges text and music modalities using standard LLM parameter structures without the need for custom inference engines.
*   **Complex Sequence Handling:** Effectively generates complex, multitrack MIDI music sequences directly from natural language descriptions.

---

## üõ†Ô∏è Methodology

The authors utilize a **parameter-adaptation strategy** consisting of three core components:

1.  **Vocabulary Expansion:** Extends a pre-existing text LLM vocabulary by integrating MIDI tokens, allowing the model to understand musical notation alongside natural language.
2.  **Two-Stage Training:**
    *   Imbues the model with general text-to-MIDI capabilities.
    *   Focuses on translating specific natural language descriptions into musical sequences.
3.  **Structure Preservation:** Maintains the original LLM parameter structure. This is crucial as it allows the model to utilize the **vLLM library** for optimized processing, ensuring high performance without bespoke engines.

---

## ‚öôÔ∏è Technical Details

*   **Model Backbone:** Llama 3.2 (1B)
*   **Tokenization Scheme:** Anticipatory Music Transformer (AMT) arrival-time MIDI-like tokenization.
    *   Notes are represented via three tokens: **Arrival Time**, **Duration**, and **Instrument-Pitch**.
*   **Vocabulary Architecture:**
    *   Original Llama Vocab: 128K
    *   AMT Music Vocab: 55K
    *   Merged via expansion of the token embedding layer.
*   **Training Pipeline:**
    *   **Stage 1 (Continued Pretraining):** ~3.07B tokens (text and MIDI).
    *   **Stage 2 (Supervised Finetuning):** ~5.1B augmented tokens specifically for text-to-MIDI translation.
*   **Training Resources:** 6 days on 4 x H100 GPUs.

---

## üìà Results & Benchmarks

MIDI-LLM significantly outperformed the Text2midi baseline across all evaluated metrics.

### Performance Comparison

| Metric | MIDI-LLM | Text2midi (Baseline) | Improvement |
| :--- | :--- | :--- | :--- |
| **Quality (FAD)** | **0.173** | 0.818 | Lower is better ‚úÖ |
| **Alignment (CLAP)** | **22.1** | 18.7 | Higher is better ‚úÖ |
| **Inference Time (2K seq)** | **10.0s** (BF16) | 47.0s | ~4.7x Faster |
| **Real-Time Factor (RTF)** | **14.17** (FP8) | 0.56 | ~25x Increase |

---

## üèÜ Contributions

*   **MIDI-LLM Introduction:** Introduced a specialized LLM capable of multitrack MIDI composition directly from text prompts.
*   **Efficient AI Inference:** Demonstrated that standard LLM parameters can be adapted for complex sequences while remaining compatible with high-performance libraries like vLLM.
*   **Benchmark Establishment:** Established a new industry benchmark by outperforming the previous Text2midi model in quality, controllability, and speed.

---

**Quality Score:** 8/10
**References:** 4 citations