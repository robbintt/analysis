# ProxyAttn: Guided Sparse Attention via Representative Heads

*Yixuan Wang; Huang He; Siqi Bao; Hua Wu; Haifeng Wang; Qingfu Zhu; Wanxiang Che*

---

> ### ðŸ“Š Quick Facts
> 
> *   **Speedup (Attention):** Up to 10.3x
> *   **Speedup (Prefill/TTFT):** Up to 2.4x
> *   **Context Length:** 128K
> *   **Models Evaluated:** Llama3.1-8B-Instruct, Qwen2.5-7B-Instruct-1M
> *   **Training Required:** No (Training-free)
> *   **Key Mechanism:** Proxy Head + Dynamic Budgeting

---

## Executive Summary

This research addresses the computational bottleneck of **quadratic complexity** in long-context Large Language Models (LLMs), specifically focusing on the inefficiency of standard attention mechanisms during the prefilling phase. As context windows expand to millions of tokens, the memory and computational cost of calculating attention between every pair of tokens becomes prohibitive. While sparse attention mechanisms offer a potential solution, current methods struggle to maintain model performance at high sparsity rates or require expensive fine-tuning of the model. This creates a critical barrier to deploying long-context LLMs in production environments where both low latency and high accuracy are required.

The key innovation is **ProxyAttn**, a training-free sparse attention framework that departs from traditional sequence-level pruning by instead compressing the attention head dimension. The authors introduce a "Proxy Head Mechanism," which leverages observed redundancy and similarity among attention heads to select a small subset of "representative heads." These proxies are used to approximate the attention scores for all heads within a layer without requiring weight updates. To translate these approximated scores into efficient computation, the method employs a "block-aware dynamic budget estimation." This technique uses the proxy scores to dynamically evaluate the importance of token blocks and allocate computational budgets accordingly, enabling fine-grained sparsity that adapts to the specific input distribution.

Empirical evaluations on Llama3.1-8B-Instruct and Qwen2.5-7B-1M with a 128K context length demonstrate that ProxyAttn substantially outperforms existing baselines like MInference and FlexPrefill. The method achieved up to a **10.3x acceleration** in attention computation and a **2.4x speedup** in Time To First Token (TTFT) during the prefilling stage. Crucially, this efficiency gain was realized without significant performance degradation; ProxyAttn maintained RULER benchmark scores of approximately 75.5â€“76 for Llama3.1-8B and 76.5 for Qwen2.5-7B. These results validate that the model retains its retrieval and reasoning capabilitiesâ€”even at high sparsity ratesâ€”and confirm the hypothesis that attention heads share a significant focus on critical tokens across layers.

ProxyAttn represents a significant advancement in efficient inference by offering a plug-and-play, training-free solution that eliminates the need for costly model retraining. By validating the hypothesis of head similarity, the research shifts the optimization focus from sequence length to head redundancy, providing a new dimension for compression.

---

## Key Findings

*   **Head Similarity:** There is notable similarity among multiple attention heads in LLMs, allowing scores of a few representative heads to effectively approximate all heads.
*   **Significant Acceleration:** ProxyAttn achieves up to **10.3x acceleration** in attention mechanisms and **2.4x acceleration** in prefilling stages.
*   **Performance Retention:** Unlike existing methods, it maintains model performance without significant loss at high sparsity rates.
*   **Precise Evaluation:** The method enables precise evaluation of block importance by combining proxy head scores with dynamic budgets.

---

## Methodology

ProxyAttn is a **training-free sparse attention algorithm** designed to optimize long-text processing in LLMs. Its core methodology involves three components:

1.  **Head Dimension Compression:** It leverages the similarity among attention heads by pooling the scores of selected representative heads to compress the attention head dimension.
2.  **Block-Aware Dynamic Budgeting:** The method employs a block-aware dynamic budget estimation technique to handle variance in sparsity across the input.
3.  **Integrated Evaluation:** It integrates these approximated scores with dynamic budgets for fine-grained block importance evaluation, allowing the model to focus computational resources on the most relevant parts of the sequence.

---

## Contributions

*   **ProxyAttn Framework:** Introduction of a training-free sparse attention mechanism that addresses quadratic complexity limitations in long-context modeling.
*   **Proxy Head Mechanism:** Development of a system that utilizes specific representative heads to approximate full attention scores without the need for additional training.
*   **Adaptive Budgeting:** Creation of a method using block-aware dynamic budget estimation to account for varying sparsity levels effectively.
*   **Empirical Validation:** Comprehensive validation of the head similarity hypothesis and demonstration of superior performance-efficiency trade-offs compared to baselines like MInference and FlexPrefill.

---

## Technical Details

ProxyAttn proposes a shift in optimization strategy, compressing along the **head number dimension** rather than the sequence dimension.

| Component | Description |
| :--- | :--- |
| **Representative Heads** | Selected based on similarity to act as proxies, approximating attention scores for all heads within a layer. |
| **Dynamic Sparse Patterns** | Score approximation is combined with dynamic budgets to determine which tokens to focus on. |
| **Token Consistency (Assumption)** | The assumption that heads focus on consistent tokens across layers. |
| **Shared Ranking (Assumption)** | The assumption that proxy head rankings can accommodate diverse heads due to redundancy found in LLM architectures. |

---

## Results

The evaluation was conducted on **Llama3.1-8B-Instruct** and **Qwen2.5-7B-Instruct-1M** at a **128K context length**, comparing ProxyAttn against MInference and FlexPrefill.

*   **Acceleration Metrics:**
    *   **Attention Computation:** Up to 10.3x speedup.
    *   **Prefilling Stage (TTFT):** Up to 2.4x speedup.
*   **Performance Accuracy (RULER Scores):**
    *   **Llama3.1-8B:** ~75.5â€“76 (achieved at ~2.2xâ€“2.4x speedup).
    *   **Qwen2.5-7B:** ~76.5 (achieved at ~1.8xâ€“1.9x speedup).
*   **Sparsity Analysis:**
    *   The method retains accuracy at high sparsity rates.
    *   Observational studies confirmed shared focus on critical tokens across layers, validating the core hypothesis.

---

**Quality Score:** 8/10  
**References:** 29 citations