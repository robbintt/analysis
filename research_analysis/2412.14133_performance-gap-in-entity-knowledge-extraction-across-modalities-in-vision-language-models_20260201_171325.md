# Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models

*Ido Cohen; Daniela Gottesman; Mor Geva; Raja Giryes*

---

> ### **Quick Facts**
>
> *   **Core Issue:** VLMs suffer up to **18% accuracy loss** when retrieving facts from visual entities versus text entities.
> *   **Benchmark:** **PopVQA** dataset introduced with **15,395** entity-image pairs.
> *   **Architecture Focus:** Primarily **LLaVA** variants (7B to 34B parameters).
> *   **Key Insight:** Critical image processing happens in middle layers, leaving fewer layers for reasoning.
> *   **Resilience:** **Qwen2-VL** showed the highest resilience (4.3% drop).
> *   **Interpretability:** Uses "Cross Patching" and "Activation Patching" to trace information flow.

---

## Executive Summary

This research addresses a critical inefficiency in Vision-Language Models (VLMs): the significant disparity in accessing factual knowledge when entities are presented visually versus textually. While VLMs are designed to integrate visual and textual inputs, the study reveals that these models suffer from a substantial accuracy drop when answering factual questions about visual entities compared to their text-only counterparts. This limitation matters because it exposes a fundamental disconnect between visual perception and the vast semantic knowledge stored within the LLM backbone; models effectively fail to recognize or retrieve information about entities they can "see" but cannot semantically link to their internal training data, hindering their reliability in real-world multimodal applications.

To diagnose this issue, the authors introduce **PopVQA**, a benchmark dataset comprising 15,395 entity-image pairs (including celebrities, landmarks, and paintings) specifically designed to isolate entity recognition from general reasoning. Beyond benchmarking, the study employs mechanistic interpretability to trace information flow within the LLaVA architecture. The methodology utilizes two distinct techniques: **"Cross Patching,"** which swaps hidden states between distinct entities to identify the specific layer where visual information becomes critical to the query answer, and **"Activation Patching with Freezing,"** which freezes image hidden states early in the network to determine if continuous processing is necessary or if earlier layers suffice for entity identification.

The analysis reveals a pronounced performance gap across multiple state-of-the-art architectures. LLaVA-1.5-7B exhibited the most severe decline, with an accuracy drop of **17.7%** (falling from 45.3% on text to 27.6% on image). Similarly, LLaVA-MORE-8B variants showed drops of 16.3%â€“17.6%, while the larger LLaVA-1.6-34B model still faced a 12.1% reduction; notably, Qwen2-VL demonstrated the highest resilience with a minor 4.3% drop. Mechanistic investigations found that meaningful information transfer from image tokens to query tokens is delayed until deep layers. Furthermore, freezing experiments indicated that while LLaVA-7B requires processing through all layers, LLaVA-MORE variants retain significant accuracy even when hidden states are frozen at layer 0, suggesting that critical, resource-intensive image processing occurs in middle layers, thereby limiting the depth available for subsequent reasoning.

By quantifying the entity knowledge gap and identifying the architectural constraints that cause it, this paper provides a roadmap for improving future VLM designs. The finding that middle layers are consumed by visual processing suggests a trade-off between visual understanding and reasoning capabilities, highlighting the need for architectures that can resolve visual entities earlier or more efficiently to free up network capacity for logic generation.

---

## Key Findings

*   **Significant Accuracy Drop:** VLMs exhibit a substantial accuracy drop (up to **18%**) when answering factual questions about visual entities compared to text-based queries.
*   **Delayed Information Flow:** Meaningful information flow from image tokens to query tokens is delayed until the **deep layers** of the network.
*   **Processing Bottleneck:** Critical image processing occurs in the **middle layers**, which restricts the number of layers available for complex reasoning tasks.
*   **Modality Disparity:** There is a clear disparity in a model's ability to leverage internal knowledge depending on whether an entity is encoded via text or vision.

---

## Methodology

The researchers utilized a combination of novel benchmarking and mechanistic interpretability techniques to isolate and analyze the entity knowledge gap:

1.  **PopVQA Dataset Creation:**
    *   Developed a new benchmark dataset specifically designed to isolate **entity recognition** from general QA capabilities.
    *   Curated to force the model to rely on entity knowledge extraction.

2.  **Comparative Benchmarking:**
    *   Evaluated VLMs by switching entity references between **text modality** and **image modality**.
    *   This "switching" mechanism allows for a direct measurement of the performance gap attributable to the input modality.

3.  **Mechanistic Interpretability:**
    *   Employed tools to trace information flow from image tokens to query tokens across model layers.
    *   Used to identify exactly *where* and *how* the visual information is integrated into the reasoning process.

---

## Technical Details

### System Architecture
The analysis centers on the **LLaVA** architecture, which consists of three main components:
*   **Visual Encoder ($g$):** Processes raw image inputs.
*   **Projection Layer ($W$):** Maps visual features to the LLM's embedding space.
*   **LLM ($f$):** Performs the actual reasoning and text generation.

**Input Processing:**
*   Input images ($X_v$) are processed into visual tokens: $H_v = W \cdot Z_v$.
*   Visual tokens are concatenated with tokenized text ($H_t$) to form the input sequence $[H_v, H_t]$ for the LLM.
*   **Hidden States:** Noted as $h^l_{i,v}$ (visual) and $h^l_{i,t}$ (textual).

### Dataset Specifications
*   **Name:** PopVQA
*   **Volume:** 15,395 entity-image pairs derived from Wikidata.
*   **Distribution:**
    *   63.6% Celebrities
    *   18.0% Landmarks
    *   14.6% Paintings
    *   3.8% Brand Logos
*   **Image Specs:** Resized to 336x336 pixels.

### Interpretability Methodologies
1.  **Cross Patching:**
    *   Patches image-token hidden states from an "injected entity" into an "original entity."
    *   **Purpose:** To identify the specific "switching layer" where the model changes its prediction based on the visual patch.

2.  **Activation Patching with Freezing:**
    *   Freezes image hidden states from a source layer through layer 20.
    *   **Purpose:** To determine the necessity of continuous processing and identify layers critical for entity identification.

---

## Results

### Performance Gap Analysis
The study identified a statistically significant performance gap in knowledge extraction between text and image modalities across leading models.

| Model | Text Accuracy | Image Accuracy | Accuracy Drop |
| :--- | :---: | :---: | :---: |
| **LLaVA-1.5-7B** | 45.3% | 27.6% | **17.7%** |
| **LLaVA-MORE-8B (SigLIP)** | 37.7% | 20.1% | **17.6%** |
| **LLaVA-MORE-8B (CLIP)** | 37.8% | 21.6% | **16.3%** |
| **LLaVA-1.6-34B** | 65.6% | 53.4% | **12.1%** |
| **Qwen2-VL** | 47.6% | 43.3% | **4.3%** |

### Information Flow Insights
*   **Transfer Delay:** Critical information transfers from image tokens to query tokens occur primarily in the **middle and deep layers** rather than early on.
*   **Processing Requirements (Freezing Experiments):**
    *   **LLaVA-7B:** Requires processing through *all* layers to maintain performance; freezing early states hurts accuracy significantly.
    *   **LLaVA-MORE Variants:** Exhibit superior resilience. They retain nearly **50%** (SigLIP) and over **30%** (CLIP) of accuracy even when hidden states are frozen from layer 0.
    *   **Implication:** LLaVA-MORE variants are better at entity identification in earlier layers compared to the base LLaVA-7B.

---

## Contributions

*   **Quantified Limitation:** Identified and quantified a specific limitation in VLMs regarding accessing entity knowledge from visual inputs (the "Performance Gap").
*   **Community Resource:** Released the **PopVQA dataset** to assist the research community in evaluating entity recognition and reasoning capabilities.
*   **Architectural Insights:** Provided deep architectural insights revealing that late image processing creates bottlenecks, offering clear pathways and targets for future VLM improvements.

---

**Quality Score:** 9/10  
**References:** 40 citations