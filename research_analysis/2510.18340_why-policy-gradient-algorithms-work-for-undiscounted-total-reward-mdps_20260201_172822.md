# Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs
*Jongmin Lee; Ernest K. Ryu*

---

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 39 Citations |
> | **Core Focus** | Undiscounted Infinite-Horizon MDPs ($\gamma = 1$) |
> | **Key Innovation** | Transient Visitation Measure |
> | **Application** | RL for LLMs (RLHF) |

---

## Executive Summary

Reinforcement Learning (RL) theory has historically depended on a discount factor $\gamma < 1$ to ensure mathematical stability and convergence, particularly in infinite-horizon settings. This reliance creates a significant disconnect with modern applications, such as training Large Language Models via RLHF, which often operate with undiscounted total rewards ($\gamma = 1$) to maximize overall performance. Previous literature frequently deemed undiscounted settings pathological due to ill-defined state visitation measures and gradient discontinuities when $\gamma \to 1$. This paper addresses this critical gap by rigorously analyzing why policy gradient methodsâ€”unlike many value-based approachesâ€”function effectively in undiscounted Markov Decision Processes (MDPs), challenging the conventional wisdom that discounting is strictly necessary for convergence.

The authors introduce a novel theoretical framework anchored in the "**transient visitation measure**" and a structural decomposition of the MDP state space. By restricting the policy search to the interior of the policy space ($\Pi^+$)â€”where policies assign strictly positive probabilities to all actionsâ€”the paper ensures that the classification of states into recurrent and transient classes remains invariant. This stability allows them to define a transient matrix $T_\pi$ with a spectral radius strictly less than 1. Consequently, the authors reformulate the undiscounted value function using a Neumann series, $V_\pi = (I - T_\pi)^{-1} r_\pi$. This reformulation provides a mathematically rigorous tool for analyzing state occupancy and gradients that circumvents the divergence issues inherent in classical discount-based formulations.

The paper delivers several theoretical proofs that validate the framework's robustness, forgoing empirical benchmarks in favor of mathematical analysis. Key findings include **Lemma 3**, which establishes that the value function is continuous over the restricted policy set $\Pi^+$, resolving the discontinuity pathologies typically associated with undiscounted settings. **Lemma 4** demonstrates that for environments with non-negative rewards, the optimal value achieved within this restricted set equals the global optimum, ensuring that the strictly positive probability restriction does not preclude finding optimal solutions. Additionally, the analysis shows that policy gradient methods, which optimize the objective directly, can converge even when the optimal action-value function fails to uniquely specify the policyâ€”a failure mode that plagues value-based methods in this context.

This research significantly bridges the disconnect between classical RL theory and contemporary industrial practice, offering a rigorous theoretical justification for the success of undiscounted policy gradient algorithms used in domains like generative model fine-tuning. By establishing generalized convergence guarantees and introducing the transient visitation measure, the work extends the mathematical foundations of RL to total-reward environments without requiring artificial discounting. These findings not only validate current protocols in LLM training but also provide a principled basis for developing new algorithms optimized for long-horizon tasks where total reward maximization is the primary objective.

---

## Key Findings

*   **Challenges Convention:** The paper establishes that policy gradient methods function effectively in undiscounted expected total-reward infinite-horizon MDPs ($\gamma = 1$), challenging the conventional need for a discount factor.
*   **Structural Invariance:** Under policies with strictly positive action probabilities (e.g., softmax), the classification of MDP states into recurrent and transient states remains invariant.
*   **Solving Pathologies:** The "**transient visitation measure**" resolves the issue where the classical state visitation measure becomes ill-defined when $\gamma = 1$.

---

## Methodology

*   **Theoretical Analysis:** Employs a rigorous mathematical analysis framework to extend policy gradient foundations to infinite-horizon scenarios without discounting.
*   **Structural Decomposition:** Decomposes the MDP state space into recurrent and transient classes, leveraging strictly positive action probabilities.
*   **Conceptual Reformulation:** Introduces and utilizes the "**transient visitation measure**" as the primary tool for analyzing state occupancy instead of the classical measure.

---

## Technical Details

**Theoretical Framework & Assumptions**
The paper proposes a theoretical framework for undiscounted infinite-horizon MDPs with the following specifications:
*   Finite state and action spaces.
*   Bounded rewards.
*   Gamma fixed at 1 ($\gamma = 1$).
*   Assumptions of finiteness for the positive and negative parts of the value function.
*   Policy search is restricted to the interior of the policy space ($\Pi^+$), where policies assign strictly positive probability to all actions.

**Mathematical Approach**
*   **Recurrent-Transient Decomposition:** The approach utilizes a structural breakdown of the state space.
*   **Transient Matrix ($T_\pi$):** Defines a specific matrix with a spectral radius strictly less than 1.
*   **Neumann Series Reformulation:** This allows the value function to be reformulated via a Neumann series:
    $$ V_\pi = (I - T_\pi)^{-1} r_\pi $$
    This reformulation enables gradient analysis without relying on a discount factor.

---

## Results

The paper presents theoretical findings rather than empirical benchmarks. The key mathematical contributions include:

*   **Lemma 3 (Continuity):** Asserts that the value function is continuous over the restricted policy set $\Pi^+$, resolving discontinuity pathologies associated with undiscounted settings.
*   **Lemma 4 (Optimality):** Establishes that for non-negative rewards, the optimal value over the restricted set equals the global optimum, ensuring the restriction does not preclude finding optimal policies.
*   **Failure Pathology Resolution:** The analysis resolves the value-based failure pathology by demonstrating that policy gradient methods optimize the objective directly, allowing convergence even when the optimal action-value function fails to uniquely specify the policy.
*   **Convergence Guarantees:** These results lay the groundwork for convergence guarantees for **Projected** and **Natural Policy Gradient** algorithms.

---

## Contributions

*   **Bridges Theory and Practice:** Addresses the gap between classical RL theory ($\gamma<1$) and modern applications like LLMs ($\gamma=1$).
*   **Novel Mathematical Tool:** Introduces and validates the "*transient visitation measure*" for precise state occupancy calculation.
*   **Generalized Guarantees:** Establishes generalized convergence guarantees for policy-based RL in undiscounted total reward environments.

---

## Evaluation Metrics

*   **Quality Score:** 9/10
*   **Total Citations:** 39