# On the Tension Between Optimality and Adversarial Robustness in Policy Optimization

*Haoran Li; Jiayu Lv; Congying Han; Zicheng Zhang; Anqi Li; Yan Liu; Tiande Guo; Nan Jiang*

> ### ðŸ“Š Quick Facts
> - **Quality Score:** 8/10
> - **References:** 40 citations
> - **Test Environments:** MuJoCo (Hopper, Walker2d, HalfCheetah, Ant)
> - **Performance Gap:** Vanilla ARPO sacrifices 32%â€“72% of Natural Return for robustness.
> - **Robustness Gain:** ARPO improves robustness scores by 46%â€“98% over Standard Policy Optimization.

***

## Executive Summary

This research addresses the fundamental tension between achieving high optimality (natural rewards) and maintaining adversarial robustness in deep reinforcement learning. While theoretical insights often suggest that optimal policies should possess inherent robustness, practical implementations of policy gradient methods fail to realize this synergy. Standard Policy Optimization (SPO) consistently converges to policies that achieve high returns in benign environments but suffer catastrophic failure under adversarial attacks. Conversely, Adversarially Robust Policy Optimization (ARPO) ensures security but incurs a significant penalty in natural performance. This discrepancy presents a critical barrier to deploying RL systems in high-stakes, real-world environments where both efficiency and security are mandatory.

The authors propose **BARPO (Bilevel Adversarially Robust Policy Optimization)**, a novel framework designed to unify standard and robust optimization approaches. The key technical innovation is the identification of a "**landscape reshaping mechanism**," where strong adversaries introduce deceptive sticky First-Order Stationary Policies (FOSPs) that trap traditional optimizers in suboptimal regions. To navigate this distorted landscape, BARPO utilizes a bilevel optimization formulationâ€”a max problem subject to a relaxed inner maximization constraint. By modulating adversary strength rather than applying maximum adversarial pressure at every step, BARPO alleviates the navigational challenges caused by sticky local minima, thereby preserving access to global optima without compromising the structural integrity of the learned policy.

Empirical evaluations on MuJoCo continuous control tasks quantitatively highlight the tradeoff and the efficacy of the proposed method. Standard policies achieved high natural returns but demonstrated extreme vulnerability, often collapsing to negative returns under worst-case attacks. In contrast, vanilla ARPO prioritized defense, improving robustness scores by up to 98% compared to SPO, but at a steep cost: natural returns were significantly lower than standard baselines. The BARPO framework demonstrated empirical superiority over vanilla ARPO, successfully balancing these competing objectives by improving landscape navigability and achieving a better tradeoff profile between natural performance and adversarial resilience.

This paper significantly advances the understanding of adversarial robustness in RL by bridging the gap between theoretical expectations and empirical realities. It moves beyond surface-level observations of performance degradation to provide diagnostic insights into the geometric properties of the loss landscapeâ€”specifically, the role of deceptive sticky FOSPs. By introducing the BARPO framework, the authors offer a practical solution for reconciling robustness with optimality, paving the way for the development of RL agents that are both high-performing and secure.

***

## Key Findings

*   **Fundamental Tension:** Identification of a fundamental tension between optimality and adversarial robustness in practical policy gradient methods, despite theoretical insights suggesting consistency.
*   **Standard vs. Robust Performance:** Standard Policy Optimization (SPO) tends to converge to vulnerable policies with high returns, while Adversarially Robust Policy Optimization (ARPO) favors robust policies with significantly reduced performance.
*   **Landscape Reshaping:** The tradeoff is attributed to a "**landscape reshaping mechanism**" where strong adversaries induce deceptive sticky first-order stationary policies, trapping optimization algorithms.

***

## Methodology

The authors propose **BARPO** (Bilevel Adversarially Robust Policy Optimization), a bilevel optimization framework designed to unify Standard Policy Optimization and Adversarially Robust Policy Optimization.

*   **Modulation Strategy:** The approach focuses on modulating adversary strength to alleviate navigational challenges caused by sticky first-order stationary policies.
*   **Objective:** The primary goal is to improve landscape navigability while preserving global optima, allowing the agent to maintain high natural returns while remaining robust.

***

## Technical Details

### Core Concepts & Mechanisms
*   **Intrinsic State-Adversarial MDP (ISA-MDP):** The approach defines this framework to model adversarial interactions.
*   **Adversarially Robust Policy Optimization (ARPO):** A mechanism using max-min optimization to find an Optimal Robust Policy (ORP) against a strongest adversary.

### Theoretical Derivations
*   **Convergence Issues:** Theoretical derivations provide the Adversarial Policy Gradient and show that ARPO converges to a First-Order Stationary Policy (FOSP) due to the landscape reshaping effect.
*   **Sticky Local Minima:** Adversaries create sticky local minima in the loss landscape, causing distortion between optimality and robustness.

### Proposed Solution
*   **General Bilevel ARPO:** formulated as a max problem subject to a relaxed inner maximization constraint to address the identified distortion.

***

## Experimental Results

Experiments were conducted on MuJoCo continuous control tasks (Hopper, Walker2d, HalfCheetah, Ant) to evaluate the performance of the proposed framework against standard and vanilla robust baselines.

*   **Standard Policy Optimization (SPO):**
    *   Achieved high natural returns.
    *   Suffered catastrophic failure under attack.
    *   Often resulted in negative returns under worst-case attacks (High Vulnerability).

*   **Vanilla ARPO:**
    *   Sacrificed significant natural performance to ensure robustness.
    *   **Natural Return Penalty:** 32% to 72% lower than SPO.
    *   **Robustness Improvement:** Significantly improved robustness scores compared to SPO, ranging from **46% (Hopper)** to **98% (Ant)**.

*   **BARPO:**
    *   Demonstrated empirical superiority over vanilla ARPO.
    *   Successfully balanced natural performance and adversarial resilience by improving landscape navigability.

***

## Contributions

*   **Theoretical Clarification:** Provides theoretical clarification on why theoretical consistency regarding optimality and robustness does not translate to practical performance.
*   **Diagnostic Insight:** Offers diagnostic insight by attributing the robustness-optimality tradeoff to specific geometric properties of the loss landscape (deceptive sticky FOSPs).
*   **Novel Framework:** Introduces the BARPO framework as a practical solution to reconcile robustness with optimality, demonstrating empirical superiority over vanilla ARPO.