---
title: Mixed-Precision Graph Neural Quantization for Low Bit Large Language Models
arxiv_id: '2501.18154'
source_url: https://arxiv.org/abs/2501.18154
generated_at: '2026-02-03T19:15:44'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Mixed-Precision Graph Neural Quantization for Low Bit Large Language Models

*Wanlong Liu; Yichen Xiao; Dingyi Zeng; Hongyang Zhao; Wenyu Chen; Malu Zhang*

---

## ðŸ“‹ Quick Facts

> **Key Metrics & Metadata**
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Proposed Method:** MG-PTQ (Mixed-Precision Graph Neural PTQ)
> *   **Target Bit-width:** < 3 bits
> *   **Test Datasets:** WikiText2, C4
> *   **Benchmark Comparison:** Outperforms GPTQ

---

## Executive Summary

The deployment of Large Language Models (LLMs) in resource-constrained environments is severely hindered by their massive computational and memory requirements. Post-Training Quantization (PTQ) is the standard technique for mitigating these constraints by reducing the bit-width of model weights. However, existing PTQ strategies face a critical performance cliff when operating at extremely low bit-widths (below 3 bits), where significant accuracy degradation occurs.

This paper addresses the challenge of maintaining high inference quality in LLMs under these aggressive compression scenarios, a necessity for enabling advanced AI on edge and mobile devices. The authors introduce **Mixed-Precision Graph Neural PTQ (MG-PTQ)**, a novel framework that integrates Graph Neural Networks (GNNs) into the quantization process.

Unlike traditional methods that treat weights in isolation, MG-PTQ utilizes a GNN module to model dependencies and relationships between different weights within the model. By leveraging GNN information propagation, the system captures the interdependence of parameters to accurately assess weight importance. This analysis drives an adaptive bit-width allocation mechanism, allowing the framework to dynamically assign optimal quantization strategiesâ€”allocating higher precision where necessary and lower precision where it is safeâ€”thereby minimizing overall quantization error.

Extensive experiments conducted on the WikiText2 and C4 datasets validate the efficacy of the proposed approach. The results demonstrate that MG-PTQ establishes new state-of-the-art benchmarks for low-bit quantization. Specifically, the method consistently outperforms the previous leading PTQ method, GPTQ, under low-bit conditions (less than 3 bits). The experiments confirm that MG-PTQ successfully resolves the performance limitations typical of existing strategies, maintaining superior model stability and accuracy even when operating at these extreme compression levels.

This research significantly advances the field of model compression by providing a viable pathway to deploy high-performance LLMs on hardware with strict resource limitations. By solving the "sub-3-bit" quantization problem, MG-PTQ allows developers to drastically reduce model size and memory bandwidth without the substantial performance drop that currently hinders edge deployment. The introduction of graph-based dependency modeling represents a paradigm shift in PTQ, moving the field toward more intelligent, mixed-precision allocation strategies that preserve the semantic richness of LLMs while maximizing hardware efficiency.

---

## Key Findings

*   **Superior Performance:** The proposed **Mixed-precision Graph Neural PTQ (MG-PTQ)** method outperforms the previous state-of-the-art PTQ method, GPTQ.
*   **New Benchmarks:** MG-PTQ establishes new benchmarks for quantization performance specifically under low-bit conditions (**< 3 bits**).
*   **Validated Efficacy:** Extensive experiments conducted on the **WikiText2** and **C4** datasets validate the efficacy of the proposed approach.
*   **Solving Low-Bit Limitations:** The method successfully addresses the performance limitations of existing PTQ strategies, which typically struggle when operating at bit levels lower than 3.

---

## Methodology

The researchers introduce **MG-PTQ** (Mixed-Precision Graph Neural PTQ), a framework that integrates a Graph Neural Network (GNN) module into the Post-Training Quantization process.

The methodology leverages **GNN information propagation** to capture dependencies among target weights. This analysis allows for a more accurate assessment of weight importance, which in turn facilitates:

1.  The adaptive assignment of quantization bit-widths.
2.  The optimized allocation of quantization strategies.

---

## Contributions

*   **Novel Framework Integration:** The introduction of a mixed-precision PTQ approach that utilizes Graph Neural Networks to model weight dependencies, moving beyond traditional quantization strategies.
*   **Enhanced Low-Bit Performance:** A solution to the critical problem of quantization error at extremely low bit-widths (< 3 bits), enabling the deployment of Large Language Models in resource-limited environments without significant degradation in performance.
*   **Optimization Strategy:** The development of a mechanism for adaptive bit-width allocation based on precise weight importance assessment derived from GNN propagation.

---

## Technical Details

| Feature | Description |
| :--- | :--- |
| **Approach Name** | Mixed-Precision Graph Neural PTQ (MG-PTQ) |
| **Core Strategy** | Post-Training Quantization (PTQ) for mixed-precision scenarios |
| **Key Technology** | Graph Neural Network (GNN) to model relationships within model parameters |
| **Primary Goal** | Optimize precision allocation for Low-bit Large Language Models |
| **Focus Area** | Bit-widths lower than 3 bits |

---

## Results

The method is benchmarked against **GPTQ** and outperforms it under low-bit conditions (< 3 bits). Validation was conducted on **WikiText2** and **C4** datasets. The experiments confirm the method successfully addresses the performance degradation typically associated with existing PTQ strategies at bit levels lower than 3.