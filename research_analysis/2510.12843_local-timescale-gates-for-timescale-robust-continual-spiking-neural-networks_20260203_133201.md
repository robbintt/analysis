---
title: Local Timescale Gates for Timescale-Robust Continual Spiking Neural Networks
arxiv_id: '2510.12843'
source_url: https://arxiv.org/abs/2510.12843
generated_at: '2026-02-03T13:32:01'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Local Timescale Gates for Timescale-Robust Continual Spiking Neural Networks

*Ansh Tiwari; Ayush Chauhan*

***

### üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Final Accuracy** | ~51.0% |
| **Retention Rate** | ~95% (3.2pt drop) |
| **Key Baselines Beaten** | HLOP (45‚Äì46%), DSD-SNN (~43%) |
| **Hardware Target** | Intel Loihi |
| **Learning Rule** | Local Updates (No Replay) |
| **Quality Score** | 9/10 |

***

## Executive Summary

**Problem**
Continual learning in Spiking Neural Networks (SNNs) faces the critical challenge of the **stability-plasticity dilemma**‚Äîspecifically, the tendency to catastrophically forget previous tasks when learning new ones. This issue is compounded in neuromorphic computing environments, where models must adhere to strict hardware constraints that favor local learning rules over resource-intensive methods like external replay buffers or non-local weight freezing. Furthermore, real-world temporal data often involves shifting timescales (temporal domain shifts), which traditional single-timescale SNNs struggle to process effectively.

**Innovation**
The researchers propose **Local Timescale Gates (LT-Gate)**, a novel neuron architecture integrating multi-timescale processing with adaptive gating. The approach utilizes a dual-compartment Leaky Integrate-and-Fire (LIF) design, processing information through a fast membrane ($U_f$) for rapid adaptation and a slow membrane ($U_s$) for long-term retention. A learnable, local gating variable ($\gamma$) dynamically combines these states, allowing individual neurons to specialize. The architecture is stabilized through a bio-inspired variance-tracking regularization technique.

**Results**
In a sequential image classification benchmark featuring a temporal domain shift (1000 Hz to 50 Hz), LT-Gate achieved a final combined accuracy of **~51.0%**, significantly outperforming the Hebbian baseline HLOP (45‚Äì46%) and prior SNN method DSD-SNN (~43%). Regarding catastrophic forgetting, LT-Gate demonstrated superior retention, dropping only **3.2 points** in accuracy on the initial task, compared to 5.8 points for HLOP and 7.1 points for DSD-SNN.

**Impact**
This research represents a significant step forward for lifelong learning in neuromorphic computing, successfully narrowing the performance gap between spiking networks and conventional deep learning models. By eliminating external replay buffers and relying solely on local, homeostatic updates, the LT-Gate architecture offers a scalable and efficient pathway for on-chip learning, enabling SNNs in dynamic, real-world environments.

***

## Key Findings

*   **High Accuracy:** LT-Gate achieved a final accuracy of approximately **51%** on a temporal classification benchmark, outperforming recent Hebbian continual-learning baselines and prior SNN methods.
*   **Resolving Stability-Plasticity:** The dual time-constant design successfully resolves the stability-plasticity dilemma by preserving slow contextual information while reacting to fast signals.
*   **Hardware Efficiency:** The model operates using only local updates, eliminating the need for external replay and making it highly efficient for neuromorphic deployment.
*   **Bio-inspired Stabilization:** Activity stabilization is achieved through variance-tracking regularization that mimics biological homeostasis.

***

## Methodology

The researchers propose **Local Timescale Gating (LT-Gate)**, a novel neuron model designed specifically for continual learning. The methodology centers on four core pillars:

1.  **Dual Dynamics:** Each neuron processes information using fast and slow time constants in parallel.
2.  **Adaptive Gating:** A learned local gate adjusts the influence of different timescales dynamically.
3.  **Homeostasis:** Variance-tracking regularization ensures stability without external supervision.
4.  **Hardware Constraints:** The implementation is strictly constrained to local updates to facilitate mapping to Intel's Loihi chip features.

***

## Contributions

*   **Novel Architecture:** Introduction of LT-Gate, a new neuron architecture integrating multi-timescale processing with adaptive gating for SNN continual learning.
*   **Regularization Technique:** Development of a bio-inspired variance-tracking regularization technique for improved retention.
*   **Hardware Compliance:** Demonstration of continual learning capability compliant with neuromorphic hardware constraints (local learning rules) on Intel's Loihi chip.
*   **Benchmarking:** Empirical benchmarking showing multi-timescale gating narrows the performance gap between spiking and conventional deep networks on lifelong learning tasks.

***

## Technical Details

### Architecture & Dynamics
The approach utilizes a dual-compartment architecture (**LT-Gate**) consisting of Fast ($U_f$) and Slow ($U_s$) Leaky Integrate-and-Fire (LIF) membranes.

The governing dynamics are defined as:

*   **Fast Membrane:** `U_f(t+1) = œÅ_f ¬∑ U_f(t) + I(t)`
*   **Slow Membrane:** `U_s(t+1) = œÅ_s ¬∑ U_s(t) + I(t)`

A learnable local gating variable `Œ≥ ‚àà [0, 1]` combines the states to balance stability and plasticity:

*   **Total State:** `U_total(t) = Œ≥ ¬∑ U_s(t) + (1 - Œ≥) ¬∑ U_f(t)`

Soft reset is applied to both compartments upon spiking.

### Stabilization Mechanisms
*   **Variance Tracking Regularization ($L_{var}$):** Targets 2% mean activity to mimic biological homeostasis.
*   **Spike Threshold Calibration:** Implemented via binary search.

### Training Configuration
*   **Algorithm:** Backpropagation Through Time (BPTT) with surrogate gradients.
*   **Optimizer:** Adam (lr=0.001).
*   **Constraints:** No replay buffers or weight freezing used.
*   **Compatibility:** Designed specifically for Intel Loihi hardware.

***

## Results

**Sequential Classification Performance**
In a task involving a temporal domain shift (1000 Hz to 50 Hz), LT-Gate achieved a **final combined accuracy of 51.0%**. This significantly surpassed the baseline models:
*   **HLOP:** 45‚Äì46%
*   **DSD-SNN:** ~43%

**Catastrophic Forgetting Metrics**
LT-Gate demonstrated robust memory retention:
*   **LT-Gate:** Dropped **3.2 points** in accuracy on the first task (retaining ~95%).
*   **HLOP:** Dropped 5.8 points.
*   **DSD-SNN:** Dropped 7.1 points.

**Neuronal Specialization**
The model exhibited robustness to timescale shifts. Analysis showed neurons specializing via the gating parameter:
*   **High Œ≥ values:** Correlated with stability features.
*   **Low Œ≥ values:** Correlated with plasticity features.

***

**Quality Score:** 9/10  
**References:** 40 citations