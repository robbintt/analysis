# VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages

*Jesse Atuhurra; Iqra Ali; Tomoya Iwakura; Hidetaka Kamigaito; Tatsuya Hiraoka*

---

### üìã Quick Facts Sidebar

| **Metric** | **Detail** |
| :--- | :--- |
| **Dataset Size** | 4,126 image-text pairs |
| **Languages** | English, Japanese, Swahili, Urdu |
| **Models Evaluated** | 10 VLMs |
| **Best Model** | GPT-4o |
| **Human Gap** | 6.7% (GPT-4o lag) |
| **Filtering Method** | CLIP ViT-L/14 (Threshold > 0.15) |
| **Task Count** | 8 VL tasks + 1 Unrelatedness task |

---

## ‚ö° Executive Summary

Current evaluations of Vision-Language Models (VLMs) suffer from an **English-centric bias** and an over-reliance on short captions, which significantly limits the assessment of complex, multilingual reasoning capabilities. This narrow focus fails to capture model performance on long-form textual contexts and creates a blind spot regarding how well these systems function in low-resource languages.

Addressing this is essential for developing truly global multimodal systems that can handle the nuances of diverse languages and deeper semantic comprehension beyond simple image-label associations. The authors introduce **VLURes**, a robust benchmark comprising 4,126 high-relevance image-text pairs sourced from Wikinews, Wikipedia, and web logs across 10 distinct image categories.

Unlike existing datasets, VLURes focuses on article-level context, utilizing a technical construction method that employs **CLIP ViT-L/14** to solve a bipartite assignment problem based on pairwise cosine similarity, filtering scores below 0.15. The benchmark is structured around eight vision-and-language tasks‚Äîspanning Image-Only Reasoning (e.g., Object Recognition, Scene Understanding) and Image-Text Reasoning (e.g., Image-Text Matching, Visual Question Answering)‚Äîalong with a pioneering **'unrelatedness' task** designed to detect irrelevant image-text pairings.

Evaluations of ten proprietary and open-source VLMs assessed via both automatic metrics and native speaker evaluations revealed that while **GPT-4o** achieved the top score, it still lagged behind human performance by **6.7%**. The findings highlight a significant capability gap between proprietary and open-source models, as well as uneven performance across fine-grained tasks, with models demonstrating better proficiency in object recognition than in scene understanding. Furthermore, there was a marked performance degradation in low-resource languages (Swahili, Urdu) compared to high-resource languages (English, Japanese), underscoring the challenges of multilingual generalization.

VLURes establishes a critical new standard for multilingual, long-context visual reasoning by providing a rigorous, multi-task framework that extends beyond English-centric evaluations. By releasing high-quality resources for low-resource languages such as Swahili and Urdu, this work provides the research community with essential tools to diagnose and address the limitations of current VLMs. This benchmark will guide the development of more inclusive multimodal systems, pushing the field toward bridging the performance gap between high- and low-resource language processing.

---

## üîç Key Findings

*   **Performance Gap:** Even the best-performing model, **GPT-4o**, lags behind human performance by **6.7%**.
*   **Resource Disparity:** Evaluations revealed significant performance drops in low-resource languages (Swahili, Urdu) compared to high-resource ones (English, Japanese).
*   **Model Divide:** There is a significant performance gap between proprietary and open-source VLMs.
*   **Uneven Capabilities:** Models show inconsistent capabilities across fine-grained tasks; specifically, they perform better at object understanding than scene understanding.

---

## üß™ Methodology

The researchers employed a rigorous process to create and utilize the VLURes benchmark:

1.  **Data Curation:** The VLURes benchmark was curated using web resources (Wikinews, Wikipedia, web logs) covering 10 image categories and featuring long-form text, contrasting with typical short-caption datasets.
2.  **Task Design:** The benchmark includes eight distinct vision-and-language tasks and a specific 'unrelatedness' task to detect irrelevant image-text pairings.
3.  **Evaluation:** Ten VLMs were evaluated by prompting for responses.
4.  **Assessment:** Responses were assessed using a combination of automatic metrics and evaluations by native speakers across four languages: English, Japanese, Swahili, and Urdu.

---

## ‚öôÔ∏è Technical Details

### Task Categorization
The benchmark divides tasks into two primary reasoning categories:

*   **Image-Only Reasoning:**
    *   Object Recognition
    *   Scene Understanding
    *   Relationship Understanding
    *   Semantic Segmentation
    *   Image Captioning
*   **Image+Text Reasoning:**
    *   Image-Text Matching
    *   Unrelatedness
    *   Visual Question Answering

### Data Construction Mechanism
*   **Alignment Model:** Uses CLIP ViT-L/14 for image-text alignment.
*   **Algorithm:** Treats alignment as a bipartite assignment problem based on pairwise cosine similarity.
*   **Filtering:** Images with a score below **0.15** are discarded.
*   **Selection:** The highest scoring image is selected for the text pair.
*   **Quality Control:** Excludes images containing UI tokens or NSFW content.
*   **Rationale Generation:** Prompts VLMs to generate rationales to assist in the evaluation process.

---

## üìä Results

### Dataset Composition
The final dataset comprises **4,126** high-quality image-text pairs distributed as follows:

| Language | Pairs | Avg Length |
| :--- | :---: | :--- |
| **English** | 1,000 | 270 words |
| **Japanese** | 1,000 | 447 characters |
| **Swahili** | 1,130 | 392 words |
| **Urdu** | 996 | 373 words |

### Performance Analysis
*   **Top Performer:** GPT-4o achieved the highest score but remained 6.7% below human baseline.
*   **Language Bias:** Results confirmed a substantial gap between low-resource (Swahili, Urdu) and high-resource languages (English, Japanese).
*   **Model Architecture:** Proprietary models generally outperformed open-source alternatives.
*   **Data Filtering Efficacy:** The CLIP cosine similarity threshold of 0.15 proved effective in maintaining high relevance between text and images.

---

## üè∑Ô∏è Contributions

*   **Benchmark Introduction:** Introduced the VLURes benchmark tailored for multilingual long-text settings.
*   **Resource Expansion:** Released vision-language resources for low-resource languages, specifically **Swahili** and **Urdu**.
*   **Task Innovation:** Developed a pioneering 'unrelatedness' task to evaluate model detection of irrelevant image-text pairs.
*   **Comprehensive Analysis:** Provided a detailed analysis of ten VLMs, highlighting specific limitations in low-resource processing and visual reasoning.

---

*Study Quality Score: 9/10*
*References: 40 citations*