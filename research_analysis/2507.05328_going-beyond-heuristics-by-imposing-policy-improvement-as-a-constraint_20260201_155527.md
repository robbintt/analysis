# Going Beyond Heuristics by Imposing Policy Improvement as a Constraint

*Chi-Chang Lee; Zhang-Wei Hong; Pulkit Agrawal*

***

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **Total Citations:** 40
> *   **Benchmarks Used:** IsaacGym, BI-Dex
> *   **Tasks Evaluated:** 29 (9 Locomotion/Helicopter, 20 Manipulation)
> *   **Core Innovation:** Heuristic Enhanced Policy Optimization (HEPO)

***

## Executive Summary

Reinforcement learning (RL) agents often struggle in environments with sparse rewards, leading practitioners to inject heuristic rewards based on human priors. These approaches traditionally rely on **"policy invariance,"** a theoretical principle intended to ensure heuristics guide exploration without distorting the optimal policy. However, empirical results show that such methods frequently fail to improve policy performance and are susceptible to "**reward hacking**," where agents exploit the heuristic rather than learning the task. This failure forces researchers to expend significant resources on manually tuning trade-off coefficients to balance heuristic and task rewards, limiting the efficiency and accessibility of RL.

The authors propose **Heuristic Enhanced Policy Optimization (HEPO)**, a framework that shifts the focus from theoretical policy invariance to practical **policy improvement**. HEPO treats performance improvement as a hard constraint: the optimized policy must perform at least as well on the true task objective (J) as a heuristic-only policy (π_H). Formally, the method maximizes heuristic rewards H(π) subject to the constraint J(π) ≥ J(π_H). Through Lagrangian relaxation, this is reformulated as a min-max optimization. The Lagrange multiplier α is updated via Stochastic Gradient Descent, increasing to penalize violations of the task constraint and decreasing when the constraint is satisfied, thereby automatically preventing reward hacking without manual coefficient tuning.

HEPO was evaluated on extensive benchmarks including IsaacGym and BI-Dex, outperforming strong baselines such as heuristic-only (H-only), task-only (J-only), linear combinations, HuRL, and the constrained method EIPO. In IsaacGym locomotion tasks, HEPO broke through heuristic performance ceilings to reach design limits (e.g., 8.0 m/s for Humanoid), while EIPO plateaued at velocities roughly 50% lower. In the BI-Dex manipulation suite, HEPO achieved success rates exceeding 90% on complex tasks like door opening, whereas EIPO and other baselines frequently failed to converge.

This research challenges the field's reliance on policy invariance, providing empirical evidence that prioritizing policy improvement yields superior results. By demonstrating that high-performance RL can be achieved using rough, non-expert heuristics, HEPO drastically reduces the engineering effort and computational cost associated with reward design.

***

## Key Findings

*   **Failure of Policy Invariance:** Methods relying on 'policy invariance' fail to drive actual policy improvement and perform poorly in empirical settings.
*   **Superior Benchmark Performance:** The proposed **HEPO framework** consistently achieves superior results on standard benchmarks, often outperforming systems with well-engineered reward functions.
*   **Robustness to Poor Heuristics:** HEPO achieves high performance even when utilizing non-expert or poorly engineered heuristics.
*   **Resource Efficiency:** The framework significantly reduces the human effort and computational resources required for reward design by eliminating manual coefficient tuning.

***

## Methodology

The authors propose **Heuristic Enhanced Policy Optimization (HEPO)**, a plug-and-play optimization framework that shifts the paradigm from pursuing theoretical 'policy invariance' to the practical goal of **'policy improvement.'**

Rather than attempting to make heuristic rewards perfectly compatible with task rewards, HEPO imposes policy improvement as a constraint. This allows the agent to leverage heuristics for guidance while strictly avoiding 'reward hacking' to ensure genuine performance gains.

***

## Technical Details

The paper formalizes the HEPO framework as a constrained policy optimization problem designed to leverage heuristic rewards ($H$) for optimizing task rewards ($J$).

*   **Objective Function:**
    Maximize $J(\pi) + H(\pi)$ subject to the constraint $J(\pi) \geq J(\pi_H)$, where $\pi_H$ is a heuristic-only policy.

*   **Lagrangian Relaxation:**
    The problem is converted to a min-max optimization to handle the constraint dynamically:
    $$ \min_{\alpha \geq 0} \max_{\pi} [ J(\pi) + H(\pi) + \alpha(J(\pi) - J(\pi_H)) ] $$
    This effectively weights the task reward by a dynamic factor $(1+\alpha)$.

*   **Multiplier Update:**
    The multiplier $\alpha$ is updated via Stochastic Gradient Descent.
    *   If task performance falls below the heuristic policy, $\alpha$ increases.
    *   If task performance meets or exceeds the benchmark, $\alpha$ decreases.

*   **Implementation:**
    Implemented with **Proximal Policy Optimization (PPO)**, HEPO trains the enhanced and heuristic policies concurrently using data sharing and importance sampling.

***

## Contributions

1.  **Paradigm Shift:** Introduction of a novel approach that prioritizes maximizing policy improvement over policy invariance, addressing the empirical failures of previous theoretically rigorous methods.
2.  **Framework Development:** Development of the **HEPO framework**, a robust optimization method capable of leveraging human priors while mitigating reward hacking.
3.  **Accessibility Validation:** Validation that high-performance RL can be achieved using non-expert heuristics, lowering the cost and increasing accessibility of RL applications.

***

## Results

Experiments were conducted on IsaacGym (9 locomotion/helicopter tasks) and BI-Dex (20 manipulation tasks) against baselines such as H-only, J-only, J+H, PBRS, HuRL, and EIPO.

*   **IsaacGym Locomotion:**
    *   HEPO consistently broke the heuristic performance ceiling.
    *   **Humanoid:** Reached design limit of **8.0 m/s** (EIPO plateaued ~50% lower).
    *   **Ant:** Reached design limit of **2.0 m/s**.
*   **BI-Dex Manipulation:**
    *   Evaluation metrics included 'Progressing' and 'Goal-reaching'.
    *   HEPO achieved success rates exceeding **90%** on complex tasks (e.g., door opening, pen reorientation).
    *   Baselines like EIPO frequently failed to converge, recording success rates between **0% and 5%**.
*   **Engineering Efficiency:**
    *   Eliminated the need to manually tune the trade-off coefficient $\lambda$.

***

**References:** 40 citations