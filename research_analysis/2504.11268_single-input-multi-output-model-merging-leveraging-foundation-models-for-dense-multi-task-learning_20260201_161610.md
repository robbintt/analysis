# Single-Input Multi-Output Model Merging: Leveraging Foundation Models for Dense Multi-Task Learning

*Juan Garcia Giraldo; Nikolaos Dimitriadis; Ke Wang; Pascal Frossard*

---

>### **EXECUTIVE SUMMARY**
>
>This research addresses the critical failure of existing model merging techniques, such as Task Arithmetic and Ties-Merging, when applied to Single-Input Multi-Output (SIMO) dense prediction tasks. The authors identify **'representation misalignment'** as the root cause, where arithmetically averaging task vectors creates a unified encoder whose features are incongruent with the input distributions expected by pre-existing task-specific decoders. This leads to catastrophic performance collapse in tasks like semantic segmentation and depth estimation.
>
The key innovation is a **post-merging re-alignment framework** that corrects this misalignment without expensive joint fine-tuning by learning lightweight linear transformations. It employs Head Re-alignment for task-specific projections and Joint Representation Re-alignment for a shared subspace, solved via least-squares regression on a small validation set. Experimental results on benchmarks like Taskonomy and NYUv2 demonstrate that while standard merging suffers severe degradation—losing up to 80% of performance—the proposed framework restores accuracy to levels comparable to Multi-Task Learning baselines with high resource efficiency.

---

### **QUICK FACTS**

| Metric | Detail |
| :--- | :--- |
| **Core Focus** | Model merging in Single-Input Multi-Output (SIMO) settings |
| **Primary Base Model** | DINOv2 |
| **Key Problem Identified** | Representation Misalignment between merged encoder and decoders |
| **Performance Drop (Baseline)** | Up to **80%** loss in 5-task dense prediction scenarios |
| **Solution Efficiency** | Requires significantly fewer samples and training steps than joint fine-tuning |
| **Quality Score** | 8/10 |

---

### **KEY FINDINGS**

*   **Fundamental Differences:** Single-Input Multi-Output (SIMO) settings differ fundamentally from Single-Input Single-Output (SISO) settings due to the presence of task-specific decoders and diverse loss objectives.
*   **Representation Misalignment:** Existing model merging methods fail in SIMO settings primarily because of 'representation misalignment' between the merged encoder and the task-specific decoders.
*   **Feature Necessity:** While task arithmetic allows for multi-task capabilities, feature representations must be re-aligned with task-specific heads to maintain performance.
*   **Efficiency:** The proposed architecture rivals traditional multi-task learning in performance but requires significantly fewer samples and training steps to achieve comparable results.

---

### **METHODOLOGY**

The research focuses on **Model Merging** within a **Single-Input Multi-Output (SIMO)** framework, moving beyond previous one-to-one task mappings. The study outline is as follows:

1.  **Structural Analysis:** Analyzes the structural failures of current merging techniques in SIMO architectures, specifically identifying the friction between the unified encoder and distinct decoders.
2.  **Proposed Solution:** The authors propose two computationally efficient fixes to re-align feature representations post-merging.
3.  **Evaluation:** The approach is evaluated against joint fine-tuning and traditional multi-task learning baselines using dense prediction datasets, including:
    *   NYUv2
    *   Cityscapes
    *   A Taskonomy subset

---

### **TECHNICAL DETAILS**

*   **Architecture Paradigm:** Single-Input Multi-Output (SIMO) for dense prediction using a **shared-bottom architecture** (shared encoder with task-specific heads).
*   **Initialization:** Utilizes a DINOv2 base model.
*   **Training Process:** Uses a two-stage fine-tuning process:
    *   *Stage 1:* Trains a randomly initialized head while the encoder is frozen.
    *   *Stage 2:* Fine-tunes both the head and encoder jointly.
*   **Baselines Evaluated:** Task Arithmetic and Ties-Merging. Task vectors are defined as the difference between fine-tuned and initial weights.
*   **Re-alignment Strategy:** Addresses 'representation misalignment' via feature re-alignment strategies (re-aligning the head or joint representation) using a small multi-task validation dataset.

---

### **EXPERIMENTAL RESULTS**

Experimental results on the Taskonomy benchmark highlight the disparities between classification and dense prediction tasks in model merging:

*   **SIMO vs. SISO:** Existing merging methods in SIMO settings show significant performance deterioration compared to classification (CLIP) tasks.
*   **Performance Collapse:**
    *   **2-Task Combinations:** Suffer an approximate **60%** performance drop (retention ~40%).
    *   **5-Task Combinations:** Performance falls below **20%** normalized performance.
*   **Classification Stability:** In contrast, classification tasks preserve almost full performance for 2-task combinations under standard merging methods.
*   **Root Cause:** The study concludes that the steep decline in dense prediction is due to representation mismatch between the merged encoder and task-specific heads, rather than missing task information.

---

### **CONTRIBUTIONS**

*   **Challenge Identification:** Identification and definition of the specific challenges of merging models in dense, multi-task environments.
*   **Root Cause Diagnosis:** Diagnosis of representation misalignment as the root cause of merged model degradation in SIMO settings.
*   **Efficient Framework:** Proposal of an efficient merging framework using simple post-merging re-alignment fixes as an alternative to expensive joint fine-tuning.
*   **Cost Reduction:** Demonstration of offline task analysis which reduces the data and computational burden typically associated with multi-task learning.