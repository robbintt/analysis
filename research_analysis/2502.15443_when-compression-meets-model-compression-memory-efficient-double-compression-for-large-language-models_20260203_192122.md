---
title: 'When Compression Meets Model Compression: Memory-Efficient Double Compression
  for Large Language Models'
arxiv_id: '2502.15443'
source_url: https://arxiv.org/abs/2502.15443
generated_at: '2026-02-03T19:21:22'
quality_score: 8
citation_count: 7
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models

*Weilan Wang; Yu Mao; Dongdong Tang; Hongchao Du; Nan Guan; Chun Jason Xue*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Compression Ratio** | ~2.2x |
| **Memory Footprint Reduction** | 40% |
| **Accuracy Loss** | Negligible |
| **Key Innovation** | Compression-Aware Quantization |
| **Test Model** | OPT-1.3B |
| **Quality Score** | 8/10 |

---

## Executive Summary

Deploying Large Language Models (LLMs) on resource-constrained edge devices remains a significant challenge due to their massive memory requirements. While existing techniques like quantization (e.g., INT8) reduce model size, the resulting memory footprint often exceeds the capacity of limited hardware. The core issue addressed in this paper is the necessity for further size reduction beyond standard quantization without incurring the heavy computational penalties or accuracy degradation typically associated with aggressive compression. The authors highlight that simply applying lossless data compression to quantized models is insufficient due to the random distribution of weights, necessitating a more sophisticated approach to enable LLM inference on memory-limited devices.

The authors propose a **"Double Compression" (DC)** framework that applies lossless compression to already quantized models through a multi-stage pipeline. The key technical innovation is **"compression-aware quantization,"** a pre-processing step that re-scales parameters using a per-channel scaling factor before quantization. This transformation optimizes the weight distribution to maximize compressibility. The pipeline further increases compression ratios by pruning weights based on the Lâˆž-norm of activations to increase the occurrence of zero-valued weightsâ€”raising near-zero values to 58% compared to a 6% baselineâ€”followed by Zstd lossless compression. To address the critical trade-off between memory savings and latency, the authors introduce a speed-adaptive method that dynamically manages the decompression workload within a CPU-GPU architecture, ensuring that high compression does not bottleneck inference speed.

Empirical validation demonstrates that the framework achieves an approximate **2.2x compression ratio** on top of standard quantization, resulting in a **40% reduction in memory footprint** during inference. In tests on the OPT-1.3B model, the proposed method achieved a scaled weight compression ratio of 2.46 compared to a baseline of 1.54. Crucially, this efficiency is gained with negligible loss in accuracy; whereas naive scaling methods caused accuracy to collapse (dropping from 0.57 to 0.32), the DC framework maintained performance stability, retaining accuracy near the original 0.57 baseline. Additionally, the speed-adaptive mechanism effectively resolved the decompression bottleneck, enabling substantial memory savings without the significant inference speed degradation usually associated with deep compression.

---

## Key Findings

*   **High Compression Efficiency:** The proposed framework achieves approximately a **2.2x compression ratio** when applied to quantized Large Language Models (LLMs).
*   **Significant Memory Reduction:** Inference utilizing the compressed model demonstrates a **40% reduction** in memory footprint.
*   **Negligible Performance Degradation:** The substantial memory savings are accomplished with minimal loss in both accuracy and inference speed.
*   **Decompression Bottleneck:** The study identifies decompression as a critical bottleneck in practical deployment scenarios, creating a trade-off between memory usage and latency.
*   **Latency-Memory Balance:** A speed-adaptive method effectively resolves the decompression bottleneck, allowing for efficient memory usage without sacrificing inference speed.

---

## Methodology

The research utilizes a multi-stage **"double compression" pipeline** designed to operate on quantized models. The approach consists of the following phases:

1.  **Compression-Aware Quantization:** A technique that re-scales model parameters before the quantization process to enhance the inherent compressibility of the weights.
2.  **Pruning:** A pruning method is applied following quantization to further increase compression.
3.  **Bottleneck Analysis:** To address latency issues arising from decompression, the authors conducted a detailed analysis of the trade-off between memory usage and speed.
4.  **Optimization:** Implementation of a speed-adaptive method to optimize real-time performance based on the bottleneck analysis.

---

## Contributions

*   **Novel Compression Framework:** Introduction of a comprehensive framework that enables further compression of LLMs post-quantization, specifically targeting the constraints of memory-limited devices.
*   **Optimization of Quantization:** Development of a compression-aware quantization technique that pre-processes parameter re-scaling to maximize weight compressibility.
*   **Resolution of Decompression Overheads:** Provision of a theoretical and practical analysis of the decompression bottleneck, contributing a speed-adaptive mechanism to mitigate latency penalties associated with high compression.
*   **Validation of Efficiency:** Empirical evidence that substantial memory optimization (40%) is achievable without the traditional costs of accuracy or significant inference speed degradation.

---

## Technical Details

The paper proposes a **'Double Compression' (DC)** framework applying lossless compression to INT8 quantized models.

**Pipeline Architecture:**
1.  **Per-Channel Scaling:** Uses factor $s = \max(|X_i|)^\alpha$ to transform weight distribution.
2.  **INT8 Quantization:** Quantization of scaled weights and activations.
3.  **Pruning:** Based on Lâˆž-norm of activations to increase zero-valued weights.
4.  **Lossless Compression:** Utilization of Zstd compression algorithm.

**System Architecture:**
*   Uses a standard **CPU-GPU system**.
*   Implements adaptive compression to balance memory savings against decompression latency.

**Data Observations:**
*   Activations show **58% near-zero values** versus 6% in baseline weights.
*   Demonstrates a **40x larger data range** compared to baseline.

---

## Results

*   **Overall Efficiency:** The framework achieves approximately a **2.2x compression ratio** and **40% reduction** in memory footprint.
*   **OPT-1.3B Performance:** Scaled Weight Compression Ratio (CR) is **2.46** compared to the baseline of **1.54**.
*   **Accuracy Stability:**
    *   Proposed method: Maintained accuracy near baseline (~0.57).
    *   Naive scaling: Reduced accuracy by ~60% (from 0.57 to 0.32).
*   **Bottleneck Resolution:** The identified decompression bottleneck was successfully addressed by the speed-adaptive method.

---

**Quality Score:** 8/10  
**References:** 7 citations