---
title: How well LLM-based test generation techniques perform with newer LLM versions?
arxiv_id: '2601.09695'
source_url: https://arxiv.org/abs/2601.09695
generated_at: '2026-01-27T21:48:02'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# How well LLM-based test generation techniques perform with newer LLM versions?

*Large Language, Renzo Degiovanni, Software Engineering, Michael Konstantinou, Luxuembourg Institute, Mike Papadakis*

> ### üìä Quick Facts
>
> *   **Models Evaluated:** GPT-4o-mini, DeepSeek V3, LLaMA 3.3 (70B)
> *   **Top Performer:** DeepSeek V3 (59.82% line coverage in Plain Method config)
> *   **Proposed Solution:** Hybrid Approach (Class-level Phase 1 + Selective Method-level Phase 2)
> *   **Cost Efficiency:** Hybrid approach reduces API requests by ~20% (10,548 vs 13,228)
> *   **Max Coverage:** Combined suites achieved 53.67% line and 38.74% branch coverage
> *   **Quality Score:** 8/10

---

## üìù Executive Summary

This research addresses the critical need to re-evaluate automated test generation techniques in light of the rapid release of new Large Language Model (LLM) versions, specifically GPT-4o-mini, DeepSeek V3, and LLaMA 3.3. As models evolve with distinct capabilities and cost structures, it is unclear whether complex, heuristic-heavy State-of-the-Art (SOTA) tools retain their advantage over simpler prompting strategies. The study investigates this gap on a benchmark of real-world Java projects, assessing whether sophisticated tooling is still necessary to achieve high coverage or if newer models can achieve comparable results through basic, cost-effective prompting.

The key innovation is a cost-optimized Hybrid generation strategy implemented via the ChatUniTest framework. This technique technically adopts a two-phase process: Phase 1 performs broad class-level generation to capture general system behavior, while Phase 2 selectively targets remaining coverage gaps by applying method-level prompting only to specific uncovered branches. This approach is benchmarked against established SOTA tools (HITS, SymPrompt, TestSpark, CoverUp) and strict baseline configurations, specifically Plain (Class) and Plain (Method) prompting, to isolate the impact of prompt granularity on economic feasibility.

Results indicate that method-level prompting significantly outperforms class-level generation, achieving 49.95% line coverage compared to 33.53%. While aggregated suites reached 53.67% line and 38.74% branch coverage, the proposed Hybrid approach proved highly efficient, matching this efficacy with 52.30% line and 38.84% branch coverage while reducing API requests by approximately 20% (10,548 vs 13,228), translating to direct cost savings. DeepSeek V3 emerged as the strongest performer in the Plain (Method) configuration with 59.82% line coverage, while the study observed robust compilation rates of 78.89% (class) and 82.17% (method), with passing rates hovering around 50%.

The findings highlight a paradigm shift in LLM-based test generation: newer models enable simpler prompting strategies to rival complex SOTA tools regarding effectiveness, while the Hybrid strategy offers superior cost efficiency. This research encourages the software engineering community to re-evaluate existing tooling architectures, suggesting that prioritizing selective, cost-aware generation strategies can significantly reduce testing expenditures without sacrificing software quality.

---

## üîë Key Findings

*   The provided analysis indicates that the abstract section of the original paper was empty; the data presented is derived from the full text and technical details.
*   Method-level prompting consistently outperforms class-level generation in terms of code coverage.
*   The Hybrid approach offers a balance, maintaining high effectiveness while significantly reducing the cost of API interactions.
*   Newer LLMs (specifically DeepSeek V3) allow simpler prompting strategies to compete with complex, heuristic-heavy SOTA tools.

---

## ‚öôÔ∏è Technical Details

**Tools & Frameworks**
*   **Framework:** ChatUniTest
*   **SOTA Tools Evaluated:** HITS, SymPrompt, TestSpark, CoverUp
*   **Baselines:** Plain (Class), Plain (Method)

**Models Analyzed**
*   GPT-4o-mini
*   DeepSeek V3
*   LLaMA 3.3 (70B)

**Hybrid Generation Strategy**
*   **Phase 1:** Performs broad class-level generation to capture general system behavior.
*   **Phase 2:** Conducts selective method-level generation targeting only uncovered branches identified in Phase 1.
*   **Goal:** To reduce API costs without sacrificing coverage granularity.

---

## üìã Methodology

*   *Note: The provided text did not contain a specific methodology section as the abstract was empty. However, the technical details suggest the following approach:*
*   Comparison of SOTA tools against Plain prompting baselines.
*   Evaluation of a proposed Hybrid approach against baseline metrics.
*   Benchmarking conducted on real-world Java projects.

---

## üìù Contributions

*   *Note: The provided text did not explicitly list contributions in the abstract. Based on the analysis, contributions include:*
*   Evaluation of newer LLM versions (GPT-4o-mini, DeepSeek V3, LLaMA 3.3) in the context of test generation.
*   Proposal of a cost-optimized Hybrid generation strategy.
*   Demonstration that simpler prompting strategies on newer models can rival complex SOTA tools.

---

## üìà Results

**Coverage Performance**
*   **Method-level Prompting:** Achieved **49.95%** line coverage.
*   **Class-level Prompting:** Achieved **33.53%** line coverage.
*   **Combined Suites:** Reached **53.67%** line coverage and **38.74%** branch coverage.
*   **Hybrid Approach:** Achieved **52.30%** line coverage and **38.84%** branch coverage.

**Cost Efficiency**
*   **API Requests:**
    *   Hybrid Approach: 10,548
    *   Baseline: 13,228
    *   **Reduction:** ~20% fewer requests with comparable effectiveness.

**Model Performance**
*   **DeepSeek V3:** Highest performance in Plain (Method) configuration at **59.82%** line coverage.

**Code Quality**
*   **Compilation Rates:**
    *   Class-level: 78.89%
    *   Method-level: 82.17%
*   **Passing Rates:** Approximately 50% across tested configurations.

---

**Quality Score:** 8/10 | **References:** 40 citations