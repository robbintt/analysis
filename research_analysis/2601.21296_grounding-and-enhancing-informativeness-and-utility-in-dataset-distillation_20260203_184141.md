---
title: Grounding and Enhancing Informativeness and Utility in Dataset Distillation
arxiv_id: '2601.21296'
source_url: https://arxiv.org/abs/2601.21296
generated_at: '2026-02-03T18:41:41'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Grounding and Enhancing Informativeness and Utility in Dataset Distillation

*Shaobo Wang; Yantai Yang; Guo Chen; Peiru Li; Kaixin Li; Yufa Zhou; Zhaorun Chen; Linfeng Zhang*

---

> **NOTE:** Quick Facts & Metrics
> ---
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Key Achievement:** +6.1% performance improvement on ImageNet-1K (ResNet-18) over previous SOTA.
> *   **Efficiency Gains:** 50x faster and 100x more memory-efficient than training-based methods.
> *   **Method Type:** Training-Free Dataset Distillation

---

## Executive Summary

Dataset distillation aims to synthesize small, synthetic datasets that allow neural networks to achieve performance comparable to training on full, large-scale datasets. This paper addresses the critical limitations of existing distillation methods, which largely rely on heuristic approaches that lack theoretical rigor and suffer from prohibitive computational costs. Specifically, prior methods struggle to balance the extraction of intrinsic sample information with global data utility, leading to suboptimal performance on complex benchmarks like ImageNet-1K. The research highlights the need for a theoretically grounded framework that can efficiently generate high-fidelity distilled datasets without the extensive computational overhead associated with bi-level optimization.

The authors introduce **InfoUtil**, a novel theoretical framework that mathematically formalizes dataset distillation through the dual objectives of "Informativeness" and "Utility." The framework operates as a training-free method that leverages pretrained networks for synthesis. The first component, Informativeness Maximization, employs a game-theoretic approach using Shapley Values to generate binary masks, isolating and retaining critical features from individual samples while minimizing output divergence from the original data. The second component, Utility Maximization, utilizes Gradient Flow to measure the worst-case impact of removing a data point; it employs Gradient Norms as a utility metric to select samples that exert the most significant influence on model convergence. By mathematically combining these two optimization strategies, InfoUtil generates distilled datasets that are both information-rich and globally utility-optimized.

InfoUtil establishes a new state-of-the-art, significantly outperforming previous methods like RDED across multiple benchmarks. On ImageNet-1K using ResNet-18 (IPC=1), the framework achieved a 6.1% performance improvement. Similarly, it recorded substantial gains on CIFAR-10 (+16.7% with ResNet-18, IPC=10) and ImageNet-100 (+16.0% with ResNet-101, IPC=10). The method also demonstrated superior cross-architecture generalization, achieving 17.8% accuracy when transferring from VGG-11 to Swin-V2-Tiny, compared to RDED's 7.8%. Beyond accuracy, InfoUtil offers drastic efficiency improvements as a training-free method: it is approximately 50x faster and 100x more memory-efficient (~1.75 GB vs >241 GB) than the training-based TESLA method.

This research represents a paradigm shift in dataset distillation, moving the field from heuristic, trial-and-error approaches to a mathematically rigorous optimization standard. By defining the concepts of Informativeness and Utility, the authors provide the community with quantifiable metrics to evaluate and guide the distillation process. The practical implications are significant; the drastic reduction in computational cost and memory usage democratizes access to high-quality dataset distillation for large-scale applications. Consequently, InfoUtil sets a new benchmark for the field, offering a robust, theoretically sound foundation for future research in efficient machine learning and data synthesis.

---

## Key Findings

*   **Performance Breakthrough:** The proposed InfoUtil framework achieves a **6.1% performance improvement** over the previous state-of-the-art on ImageNet-1K using ResNet-18.
*   **Theoretical Foundation:** The research establishes a rigorous theoretical foundation defining the relationship between original and synthetic data through the concepts of **'Informativeness'** and **'Utility'**.
*   **Dual-Component Optimization:** Effective synthesis relies on balancing two key strategies:
    *   **Sample-level information extraction** via game theory (Shapley Values).
    *   **Globally influential sample selection** via gradient norms.
*   **High-Quality Synthesis:** This dual optimization is crucial for synthesizing high-quality distilled datasets that outperform existing heuristic methods.

---

## Methodology

The authors propose **InfoUtil**, a theoretical framework designed to balance two specific metrics during dataset synthesis:

1.  **Informativeness Maximization**
    *   Uses a **game-theoretic approach** (Shapley Value) to extract key information from individual samples.
    *   Aims to capture the intrinsic value of each data point.

2.  **Utility Maximization**
    *   Utilizes **Gradient Norm** to select globally influential samples.
    *   Ensures the selected data points have a high impact on the model's global convergence.

The framework mathematically combines these components to generate a distilled dataset that is both information-rich and utility-optimized.

---

## Technical Details

The InfoUtil framework is a **training-free** dataset distillation method based on Informativeness and Utility.

### Step 1: Informativeness Maximization
*   **Mechanism:** Uses **Shapley Values** to create binary masks.
*   **Goal:** Retain critical information within the samples.
*   **Process:** Minimizes the difference between model outputs on original and masked samples, ensuring essential features are preserved.

### Step 2: Utility Maximization
*   **Mechanism:** Employs **Gradient Flow** to measure the worst-case impact of removing a data point.
*   **Metric:** Uses **Gradient Norm** as a utility metric.
*   **Goal:** Select samples with high utilityâ€”those that influence model convergence the most.

### Implementation Notes
*   The framework uses **pretrained networks** for synthesis.
*   It extracts training-stage soft labels for small IPC (Images Per Class).
*   It utilizes converged networks for larger IPC scenarios.

---

## Contributions

*   **Novel Theoretical Framework:** Provides a rigorous theoretical framework for knowledge distillation-based dataset distillation, contrasting sharply with previous heuristic methods.
*   **New Quantifiable Concepts:** Introduces **'Informativeness'** and **'Utility'** to quantify and measure the distillation process.
*   **Mathematical Formalization:** Formalizes the concept of 'optimal dataset distillation,' setting a rigorous standard for future research.
*   **State-of-the-Art Benchmark:** Introduces the InfoUtil framework, establishing a new SOTA with significant accuracy improvements on complex datasets like ImageNet-1K.

---

## Results

InfoUtil significantly outperforms the current State-of-the-Art (SOTA), specifically RDED, across various benchmarks:

### Benchmark Performance
*   **ImageNet-1K (ResNet-18, IPC=1):** +6.1% improvement.
*   **CIFAR-10 (ResNet-18, IPC=10):** +16.7% improvement.
*   **ImageNet-100 (ResNet-101, IPC=10):** +16.0% improvement.

### Cross-Architecture Generalization
*   **Scenario:** VGG-11 to Swin-V2-Tiny.
*   **InfoUtil Accuracy:** 17.8%
*   **RDED Accuracy:** 7.8%

### Efficiency Analysis
*   **Method Classification:** Training-Free.
*   **Speed:** Approximately **50x faster** than the training-based TESLA method.
*   **Memory Usage:** Approximately **100x more memory-efficient** (~1.75 GB vs >241 GB).