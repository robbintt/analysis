# QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision Quantization for Practical Embedded AI Applications

*Jeongseok Kim; Jemin Lee; Yongin Kwon; Daeyoung Kim*

---

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Accuracy Improvement** | Up to **10.28%** |
> | **Inference Speed Increase** | Up to **12.52%** |
> | **Computational Complexity** | Linear `O(n)` |
> | **Inference Runs Required** | 2 (Pre- and Post-quantization) |
> | **Core Technique** | Compiler-based Mixed Precision |
> | **Target Hardware** | Integer-only accelerators |
> | **Optimization Type** | Retraining-free PTQ |

---

## Executive Summary

### The Problem
Deploying deep neural networks on resource-constrained embedded accelerators presents a significant challenge due to the trade-off between computational efficiency and model accuracy. While standard quantization reduces model footprint, existing methodologies often incur high computational costs or severe accuracy degradation. Uniform quantization approaches frequently fail to account for layer-specific sensitivity, leading to suboptimal performance, while advanced techniques like Quantization Aware Training (QAT) require expensive retraining cycles that are often impractical in rapid development cycles. Consequently, there is a critical need for a retraining-free quantization strategy that can efficiently adapt to the hardware constraints of embedded devices without sacrificing inference quality.

### The Innovation
QuantuneV2 addresses these limitations through a compiler-based, mixed-precision quantization framework built as an extension to the Glow Compiler. The core innovation lies in shifting the optimization process to the Intermediate Representation (IR) level, utilizing local metricsâ€”specifically Signal-to-Quantization-Noise Ratio (SQNR), Mean Squared Error (MSE), and SQNR Deltaâ€”to perform stable layer-wise sensitivity analysis. Unlike global metrics, this local approach allows for precise identification of sensitive layers. The framework operates with linear complexity `O(n)`, requiring only a pre- and post-quantization inference run. It assigns 32-bit precision strictly to sensitive layers while applying 8-bit quantization to others, alongside compiler-aware optimizations such as operator fusion and Batch Normalization folding to minimize runtime latency.

### The Results
Experimental results demonstrate that QuantuneV2 effectively bridges the gap between compression and performance, achieving up to a **10.28% improvement in accuracy** and a **12.52% increase in inference speed** compared to baseline methods. The study highlighted the fragility of uniform quantization, noting that quantizing a single layer in models like MobileNetv2 could reduce Top-1 accuracy by up to 1%, a loss QuantuneV2 successfully mitigates. Furthermore, benchmarks on ResNet18v1 showed that the method avoids the inefficiency of leaving entire convolution blocks unquantized, outperforming both QAT and reconstruction-based Post-Training Quantization (PTQ) in terms of deployment efficiency and stability.

### The Impact
This research significantly lowers the barrier to entry for deploying high-performance AI on edge devices by eliminating the requirement for retraining. By validating the efficacy of compiler-level integrationâ€”where runtime latency and IR optimizations are considered alongside quantization logicâ€”QuantuneV2 establishes a new paradigm for practical embedded AI. The ability to deliver robust, hardware-aware model compression with only two inference runs makes this approach highly scalable for industrial applications, offering a viable path toward efficient integer-only accelerator utilization.

---

## Methodology

The researchers developed **QuantuneV2**, a compiler-based framework designed to facilitate mixed-precision quantization without the need for retraining.

*   **Compiler-Level Operation:** The framework operates at the compiler level using Intermediate Representations (IR), allowing for deep integration with hardware-specific optimizations.
*   **Local Metrics Sensitivity Analysis:** Unlike global approaches, QuantuneV2 utilizes local metricsâ€”specifically examining weights, activation values, SQNR, and MSEâ€”to determine layer sensitivity accurately.
*   **Inference-Based Optimization:** The process relies entirely on inference-based optimization. It requires only a pre-quantization inference run and a post-quantization inference run to determine the optimal precision configuration.
*   **Latency Mitigation:** To address runtime latency, the framework employs operator fusion and selects optimal IR configurations during the quantization strategy phase.

---

## Technical Details

QuantuneV2 is implemented as a specific extension to the Glow Compiler architecture.

**Implementation & Architecture**
*   **Compiler Extension:** Modifies the `transformForPrecisionMode()` pass within the Glow Compiler.
*   **Target:** Designed specifically for integer-only accelerators.
*   **Complexity:** Functions with linear computational complexity `O(n)`.

**Two-Phase Process**
1.  **Calibration Stage:** Captures activation histograms using a dataset of 100 random images.
2.  **Mixed-Precision Decision Stage:** Utilizes a sensitivity list to assign precision. Sensitive layers remain at 32-bit, while non-sensitive layers are quantized to 8-bit.

**Sensitivity Analysis Metrics**
*   Employs local metrics including:
    *   **SQNR** (Signal-to-Quantization-Noise Ratio)
    *   **MSE** (Mean Squared Error)
    *   **SQNR Delta**

**Precision Search & Optimization**
*   **Operator Fusion:** Disabled during the sensitivity analysis phase to ensure accurate measurement but applied via a fused graph IR during the final execution stage.
*   **Batch Normalization Folding:** Optimizes performance by mathematically folding Batch Normalization parameters into preceding Convolutional layers.

---

## Key Findings

*   **Performance Gains:** Achieved up to a **10.28% improvement in accuracy** and a **12.52% increase in inference speed**.
*   **Efficiency:** Operates with highly efficient linear computational complexity `O(n)`, requiring only two inference runs to complete the optimization.
*   **Metric Stability:** Provided more stable sensitivity analysis by leveraging local metrics (weights, activations, SQNR, MSE) rather than global approximations.
*   **Runtime Reduction:** Successfully reduced runtime latency through strategic IR optimization and operator fusion techniques.

---

## Experimental Results

The study validated the QuantuneV2 framework against standard approaches with significant outcomes:

*   **Accuracy vs. Speed:** The method achieved an accuracy improvement of up to 10.28% and an inference speed increase of 12.52% compared to baseline methods.
*   **Uniform Quantization Failure:** Experiments demonstrated that uniform quantization (shifting from 32-bit to 8-bit) causes significant accuracy loss. For instance, quantizing a single layer in MobileNetv2 led to a Top-1 accuracy decrease of up to 1%.
*   **Comparison with QAT and PTQ:** QuantuneV2 was identified as more efficient than Quantization Aware Training (QAT) and reconstruction-based Post-Training Quantization (PTQ).
*   **Calibration Impact:** Using a specific calibration dataset resulted in a wider distribution of SQNR values, which enhanced the distinction between sensitive and non-sensitive layers.
*   **ResNet18v1 Optimization:** In tests with ResNet18v1, the method optimized precision assignment effectively, avoiding the inefficiency of leaving all convolution layers unquantized.

---

## Contributions

The study makes three primary contributions to the field of embedded AI:

1.  **Retraining-Free Solution:** Provides a viable mixed-precision quantization solution that requires no retraining, significantly lowering deployment barriers for developers.
2.  **Compiler-Aware Optimization:** Introduces a novel approach by integrating runtime latency and Intermediate Representation (IR) constraints directly into the quantization strategy.
3.  **Stable Analysis Framework:** Delivers a robust sensitivity analysis framework using local metrics, enabling efficient model compression specifically tailored for resource-constrained embedded systems.

---

**Research Paper Analysis**
*   **References:** 40 citations
*   **Quality Score:** 8/10