---
title: A 1Mb mixed-precision quantized encoder for image classification and patch-based
  compression
arxiv_id: '2501.05097'
source_url: https://arxiv.org/abs/2501.05097
generated_at: '2026-02-03T20:13:29'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A 1Mb mixed-precision quantized encoder for image classification and patch-based compression

*Van Thien Nguyen; William Guicquero; Gilles Sicard*

---

## Quick Facts

| Metric | Details |
| :--- | :--- |
| **Memory Constraint** | Strict 1Mb limit |
| **Architecture** | ASIC Neural Network Accelerator |
| **Precision** | Mixed (3-bit, 2-bit, 1-bit) |
| **Dataset Accuracy** | 87.5% on CIFAR-10 |
| **Key Innovation** | Bit-Shift Normalization (BSN) |
| **Dual Capability** | Classification & Compression |

---

## Executive Summary

Deploying sophisticated deep learning models on edge devices is frequently constrained by limited on-chip memory and power budgets. This research addresses the critical challenge of designing a neural network accelerator capable of handling complex computer vision tasks within a strict 1Mb memory limit. Specifically, it tackles the difficulty of maintaining high accuracy and image quality in a dual-task scenario—requiring a single hardware architecture to perform both image classification and image compression—without introducing visual artifacts or exceeding the storage capacity of ultra-low-power edge devices.

The authors propose a specialized ASIC accelerator featuring a reconfigurable, mixed-precision encoder that utilizes 3-bit, 2-bit, and 1-bit quantization to optimize resource usage. To enable stable training of these low-bit networks, the architecture introduces an automatic adaptation mechanism for quantizer scaling factors. Crucially, the system replaces standard Batch Normalization with a novel "Bit-Shift Normalization" (BSN) technique; this approximates scaling factors as powers-of-two, reducing storage requirements to just 4 bits per parameter and eliminating costly affine transformations. The design also incorporates structural pruning of convolutional layers and a patch-based compression workflow that processes data locally on the edge before remote reconstruction.

The implemented system demonstrates high efficiency while strictly adhering to the 1Mb memory constraint. On the CIFAR-10 dataset, the accelerator achieved a classification accuracy of 87.5%. In compression tasks, the patch-based method successfully eliminated block artifacts and operated at a low patch-constant bitrate, outperforming current state-of-the-art patch-based codecs. The use of Bit-Shift Normalization proved highly effective, significantly reducing hardware complexity compared to traditional normalization methods while maintaining model performance.

This research provides a significant proof-of-concept for the feasibility of multi-purpose ASIC accelerators in extreme memory-constrained environments. By demonstrating that a single 1Mb chip can effectively handle both discriminative (classification) and generative (compression) tasks, the work paves the way for more versatile and intelligent edge devices. The introduction of Bit-Shift Normalization offers a valuable, hardware-efficient alternative to Batch Normalization for the broader field of low-bit quantization, potentially influencing future designs of neural network hardware for IoT and embedded applications.

---

## Key Findings

*   **High Efficiency in Small Form Factor:** The proposed ASIC accelerator achieves high functionality while strictly adhering to a **1Mb memory limit**.
*   **Strong Classification Performance:** Under the memory constraint, the encoder achieved **87.5% accuracy** on the CIFAR-10 dataset.
*   **Dual-Task Versatility:** The system demonstrates the capability to handle both **image classification** and **image compression** on a single chip.
*   **Superior Compression Quality:** The patch-based compression method successfully eliminates block artifacts and **outperforms** current state-of-the-art techniques.

---

## Methodology

The research employs a holistic approach combining hardware architecture design with algorithmic optimization:

*   **Reconfigurable Architecture:** Utilization of a mixed-precision encoder capable of switching between 3-bit, 2-bit, and 1-bit quantization.
*   **Optimization Techniques:** Implementation of combined weight and activation quantization alongside structural pruning of convolutional layers.
*   **Training Stabilization:** Introduction of an automatic adaptation mechanism for linear symmetric quantizer scaling factors.
*   **Normalization Innovation:** Replacement of Batch Normalization with layer-shared Bit-Shift Normalization to reduce hardware overhead.
*   **Workflow Strategy:** A split workflow where images are compressed patch-by-patch on the edge device, with reconstruction performed remotely.

---

## Contributions

*   **Proof-of-Concept for Multi-Purpose ASICs:** Demonstrates that ASIC accelerators can be effectively designed for multiple processing levels (classification and compression) within extreme memory constraints.
*   **Adaptive Quantizer Scaling:** Introduces a novel method for the automatic adaptation of quantizer scaling factors to facilitate the stable training of low-bit neural networks.
*   **Bit-Shift Normalization (BSN):** Proposes a layer-shared Bit-Shift Normalization technique as a hardware-efficient alternative to Batch Normalization, significantly reducing storage requirements.

---

## Technical Details

**Architecture & Configuration**
*   **Type:** ASIC neural network accelerator optimized for edge inference.
*   **Memory Limit:** Strictly capped at 1Mb.
*   **Precision:** Reconfigurable mixed-precision encoder (3-bit, 2-bit, 1-bit).

**Algorithmic Innovations**
*   **Quantization:** Utilizes quinary and ternary weights; employs HWMSB activations for 2-bit precision.
*   **Scaling:** Features adaptive scaling for histogram-equidistributed quantization.
*   **Normalization (BSN):** Replaces standard Batch Normalization with Bit-Shift Normalization.
    *   Uses a two-step training process to approximate scaling factors as powers-of-2 (specifically the 0.9-quantile).
    *   Reduces storage to **4 bits** and removes affine transforms.
*   **Pruning:** Structural pruning is applied to convolutional layers to minimize model size.

**Operational Framework**
*   **Classification:** Operates via discriminant patterns.
*   **Compression:** Processes images patch-by-patch with a remote PURENET decoder for reconstruction.

---

## Results

*   **Classification Accuracy:** Achieved **87.5%** on the CIFAR-10 dataset within the 1Mb memory constraint.
*   **Compression Performance:**
    *   Successfully eliminated block artifacts.
    *   Operates at a low patch-constant bitrate.
    *   Outperforms state-of-the-art patch-based codecs.
*   **Hardware Efficiency:**
    *   Demonstrated high efficiency in a small form factor.
    *   Bit-Shift Normalization significantly reduced complexity by storing scale in 4 bits instead of full affine parameters.

---

**Quality Score:** 9/10  
**References:** 40 citations