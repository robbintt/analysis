# Learning quadratic neural networks in high dimensions: SGD dynamics and scaling laws

*G√©rard Ben Arous; Murat A. Erdogdu; Nuri Mert Vural; Denny Wu*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Model:** Two-layer Quadratic Neural Network (2nd Hermite Polynomials)
> *   **Input Distribution:** Isotropic Gaussian $N(0,\boldsymbol{I}_d)$
> *   **Experimental Scale:** Dimensions $d=3200$, Width $r=2400$
> *   **Key Regime:** Extensive-width ($r \asymp d^\beta$) & Power-law signal decay ($\lambda_j \asymp j^{-\alpha}$)

---

## üìù Executive Summary

This paper addresses the critical theoretical challenge of characterizing the dynamics of Stochastic Gradient Descent (SGD) in high-dimensional nonlinear neural networks, specifically within the **"feature learning" regime**. While existing literature has extensively analyzed linear models and the lazy training limit of nonlinear networks, the behavior of SGD when it actively learns features in high dimensions remains poorly understood. The authors aim to bridge this gap by rigorously analyzing optimization trajectories, generalization error, and sample complexity in a setting where the model dimensionality and training data scale proportionally‚Äîa scenario highly relevant to modern deep learning practice but mathematically intractable for most architectures.

The key innovation is the precise characterization of training dynamics through a **Matrix Riccati Ordinary Differential Equation (ODE)** applied to a two-layer network with a quadratic activation function (2nd Hermite polynomial) under isotropic Gaussian inputs. The authors introduce a novel **"matrix monotonicity comparison framework,"** which allows them to establish rigorous convergence guarantees for both the continuous population limit and the discrete finite-sample (online) setting. This technical machinery enables the analysis of "extensive-width" scaling regimes‚Äîwhere network rank scales with dimensionality ($r \asymp d^\beta$)‚Äîand accommodates power-law decaying signal structures ($\lambda_j \asymp j^{-\alpha}$), providing a granular view of the optimization process that standard kernel methods cannot capture.

The study yields explicit scaling laws for prediction risk, demonstrating that the population Mean Squared Error (MSE) follows a distinct power-law decay relative to optimization time, defined as $L \sim t^{-C(\alpha)}$, where the rate depends on the signal's spectral decay. The authors prove that this decay results from **"sequential feature recovery,"** where the network learns orthogonal signal directions one by one. Furthermore, they establish sample complexity bounds that improve upon prior work and match information-theoretic limits up to polylogarithmic factors. Empirical validation using large-scale simulations ($d=3200$, teacher width $r=2400$, decay $\alpha=1$) confirmed the theoretical predictions, showing the predicted power-law decay in loss over compute time.

This work significantly advances the theoretical understanding of high-dimensional optimization by offering one of the first rigorous analyses of feature learning for nonlinear networks beyond the kernel regime. It substantiates the implicit bias of SGD, showing how it naturally prioritizes features based on signal strength. By successfully connecting the continuous population limit with discrete online SGD dynamics, the paper provides a more complete and realistic picture of training performance. These findings lay the groundwork for future theoretical analysis of more complex architectures and offer practitioners deeper insights into how model width, optimization time, and data structure interact to govern generalization.

---

## üîë Key Findings

*   **Sharp Characterization of SGD Dynamics:** The authors provide a precise analysis of Stochastic Gradient Descent (SGD) dynamics for two-layer neural networks with quadratic activation functions (2nd Hermite polynomials) in the feature learning regime.
*   **Derivation of Scaling Laws:** The study establishes explicit scaling laws for prediction risk, detailing the power-law dependencies on optimization time, sample size, and model width.
*   **Regime-Specific Convergence:** Analysis confirms convergence guarantees for infinite-dimensional effective dynamics in both the population limit and the finite-sample (online) discretization settings, even under extensive-width regimes ($r \asymp d^\beta$) and power-law signal decay ($\lambda_j \asymp j^{-\alpha}$).

---

## üõ†Ô∏è Methodology

*   **Model Setup:** The analysis focuses on a high-dimensional regression problem where input data $x$ follows a standard Gaussian distribution $N(0,\boldsymbol{I}_d)$, and the target function relies on orthonormal signal directions.
*   **Activation Function:** The study specifically utilizes the 2nd Hermite polynomial as the activation function $\sigma$ (representing a quadratic neural network).
*   **Mathematical Framework:** The core methodology involves the precise characterization of a matrix Riccati differential equation associated with the training process.
*   **Novel Arguments:** The authors employ novel matrix monotonicity arguments to establish convergence guarantees for the infinite-dimensional effective dynamics.

---

## ‚öôÔ∏è Technical Details

*   **Network Architecture:** Two-layer neural network with a quadratic activation function (2nd Hermite polynomial) and isotropic Gaussian inputs.
*   **Target Model:** Teacher model features orthogonal signal directions with power-law decaying coefficients; student network operates in an extensive-rank regime.
*   **Dynamics Analysis:** Training involves analyzing gradient flow and discrete-time online SGD on the Stiefel manifold.
*   **Tracking Mechanism:** Utilizes a **Matrix Riccati ODE** to track coupled dynamics.
*   **Theoretical Framework:** Introduction of a **'Matrix-monotone comparison framework'** to derive sharp bounds for the discrete evolution.

---

## üìà Results

*   **Power-Law Decay:** The population MSE exhibits a smooth power-law decay ($L \sim t^{-C(\alpha)}$) explained by the superposition of sequential feature recovery curves.
*   **Sequential Recovery:** The analysis confirms sequential feature recovery, establishing sample complexity that improves on prior work and matches information-theoretic limits up to polylogarithmic factors.
*   **Experimental Validation:** Experiments with dimension $d=3200$, teacher width $r=2400$, and decay exponent $\alpha=1$ demonstrated the predicted power-law decay in loss over compute time.

---

## üèÜ Contributions

*   **High-Dimensional Optimization Theory:** Provides rigorous theoretical understanding of optimization and sample complexity for gradient-based training of non-linear networks in high dimensions.
*   **Feature Learning Insights:** Advances the understanding of the "feature learning" regime by explicitly handling extensive-width scaling and power-law decay in signal coefficients.
*   **Bridging Theory and Practice:** Connects analysis of the continuous population limit with the discrete finite-sample (online) setting, offering a more complete picture of SGD performance than either analysis alone.