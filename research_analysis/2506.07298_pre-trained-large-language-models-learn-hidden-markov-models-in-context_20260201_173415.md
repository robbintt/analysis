# Pre-trained Large Language Models Learn Hidden Markov Models In-context

*Yijia Dai; Zhaolin Gao; Yahya Sattar; Sarah Dean; Jennifer J. Sun*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **HMM Configurations** | 234 |
| **Sequence Pairs** | 4,096 |
| **State Dimensions** | 2 to 64 |
| **Mixing Rates** | 0.5 to 0.99 |
| **Key Models** | Qwen, Llama |
| **Quality Score** | 9/10 |
| **Citations** | 40 |

---

## Executive Summary

### Problem
This research addresses the fundamental gap in understanding the mechanisms behind In-Context Learning (ICL) in Large Language Models (LLMs). While LLMs demonstrate remarkable capabilities in retrieving information and performing few-shot learning without parameter updates, the theoretical underpinnings of how they process structured stochastic data remain opaque. Specifically, it is unclear whether ICL can successfully model systems governed by latent variables and sequential dependencies, such as Hidden Markov Models (HMMs). Bridging this gap is critical for moving beyond LLMs as "black boxes" and understanding their potential to learn complex, latent structural dependencies.

### Innovation
The key innovation of this work is the establishment of a rigorous empirical link between classical probabilistic graphical models and modern deep learning. The authors demonstrate that pre-trained LLMs (specifically Qwen and Llama architectures) can infer latent Markovian structures and predict HMM-generated sequences using only ICL, without any fine-tuning. Technically, the methodology employs a standard next-token prediction setup: the model is provided a history of observations ($o_{1:t}$) as context and is tasked with predicting the subsequent observation ($o_{t+1}$). The study evaluates the models against a theoretical framework parameterized by HMM properties, including the number of states ($M$), mixing rates ($\lambda_2$), and transition/emission entropies.

### Results
The study evaluated 234 distinct HMM configurations with state dimensions ranging from 2 to 64 and mixing rates between 0.5 and 0.99, using 4,096 sequence pairs of length 2,048. Results indicate that pre-trained LLMs achieve near-optimal predictive accuracy comparable to the Viterbi algorithm, particularly in environments characterized by low entropy and fast mixing rates.

Performance was quantified using the **Final Accuracy Gap ($\epsilon$)**â€”defined as the deviation from the optimal inference accuracyâ€”and **Convergence Time ($T$)**, defined as the required context window length for predictions to stabilize. The models reliably minimized both $\epsilon$ and $T$ under favorable conditions. However, performance limitations were observed with higher entropy and slower mixing rates. When benchmarked on real-world animal decision-making tasks, the ICL approach demonstrated competitive performance against expert-designed biological models.

### Impact
This paper advances the theoretical understanding of ICL by providing compelling evidence that LLMs can successfully learn and predict sequences generated by HMMs. This finding suggests that transformer architectures are capable of capturing complex, latent temporal dependencies that were previously the domain of specialized classical algorithms. Practically, this positions ICL as a viable diagnostic tool for uncovering hidden structures in complex scientific datasets.

---

## Key Findings

*   **Near-Optimal Accuracy:** Pre-trained LLMs utilizing In-Context Learning (ICL) can model data generated by Hidden Markov Models (HMMs) with accuracy nearly matching theoretical optimal limits on synthetic datasets.
*   **Novel Scaling Trends:** The study identified specific scaling trends in LLM performance that are directly influenced by underlying HMM properties, such as mixing rates and entropy.
*   **Real-World Application:** The ICL approach achieved competitive performance on real-world animal decision-making tasks when compared to expert-designed biological models.
*   **Theoretical First:** This work presents the first evidence that ICL can successfully learn and predict HMM-generated sequences.

---

## Methodology

The researchers employed a **comparative evaluation framework** using pre-trained Large Language Models without fine-tuning, relying entirely on In-Context Learning (ICL).

The models were tested on two distinct types of data:
1.  **Synthetic Data:** A diverse set of synthetic HMMs to establish theoretical baselines and measure predictive accuracy.
2.  **Real-World Data:** Real-world animal decision-making tasks to benchmark effectiveness against expert-designed biological models.

---

## Technical Details

### Model Formulation
*   **Structure:** Hidden Markov Model (HMM) parameterized by $\lambda = (\pi, A, B)$ with states $M$ and observations $L$.
*   **Properties:** Analysis includes stationary distribution, mixing rate ($\lambda_2$), and transition/emission entropies.

### In-Context Learning (ICL) Setup
*   **Models:** Pre-trained Qwen and Llama models.
*   **Constraint:** No parameter updates (weights frozen).
*   **Task:** Predict observation $o_{t+1}$ given history $o_{1:t}$.

### Baselines for Comparison
*   Viterbi Algorithm
*   Direct Conditional Probability
*   Baum-Welch
*   LSTM
*   N-gram models

---

## Contributions

1.  **Bridging Eras:** Establishes a link between classic probabilistic graphical models (HMMs) and modern LLMs by demonstrating that LLMs can infer latent Markovian structures through context.
2.  **Theoretical Deepening:** deepens the theoretical understanding of ICL mechanisms through empirical observations on scaling behavior.
3.  **Practical Guidelines:** Provides practical guidelines for using ICL as a diagnostic tool for analyzing complex data and uncovering hidden structures in scientific domains.

---

## Detailed Results

**Experimental Scope**
*   **Configurations:** 234 HMM configurations.
*   **Variables:** Dimensions (2â€“64), mixing rates (0.5â€“0.99), and entropy.
*   **Data Volume:** 4,096 sequence pairs of length 2,048 across various context lengths.

**Metrics**
*   **Convergence Time ($T$):** The length of the context window required for predictions to stabilize.
*   **Final Accuracy Gap ($\epsilon$):** The deviation from the optimal inference accuracy.
*   **Hellinger Distance:** Used to measure distributional similarity.

**Performance Analysis**
*   **Optimal Conditions:** Pre-trained LLMs predict HMM sequences with near-optimal accuracy (comparable to Viterbi) under conditions of low entropy and fast mixing.
*   **Limitations:** Performance limitations (increased $\epsilon$ and delayed convergence) were observed with higher entropy and slower mixing rates, reflecting fundamental stochastic limits.

---

*Quality Score: **9/10** | References: **40***