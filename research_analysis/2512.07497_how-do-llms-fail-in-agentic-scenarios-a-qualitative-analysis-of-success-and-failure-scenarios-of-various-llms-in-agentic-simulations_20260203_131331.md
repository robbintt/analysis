---
title: How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and
  Failure Scenarios of Various LLMs in Agentic Simulations
arxiv_id: '2512.07497'
source_url: https://arxiv.org/abs/2512.07497
generated_at: '2026-02-03T13:13:31'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations

*JV Roig*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Traces Analyzed:** 900 execution logs
> *   **Models Tested:** 3 (Llama 4 Maverick, Granite 4 Small, DeepSeek V3.1)
> *   **Benchmark Framework:** Kamiwaza Agentic Merit Index (KAMI) v0.1
> *   **Citations:** 40 references
> *   **Focus:** Agentic robustness & failure mode taxonomy

---

## Executive Summary

This research addresses the critical gap in evaluating Large Language Models (LLMs) within agentic workflows, where static benchmarks fail to capture the complexity of dynamic, multi-step tool use. As organizations increasingly deploy LLMs as autonomous agents, there is a pervasive assumption that model scale correlates directly with reliability and task success. However, aggregate performance scores often obscure specific behavioral deficiencies, making it difficult to diagnose why agents fail in real-world enterprise scenarios involving filesystem operations, data analysis, and SQL interactions.

The key innovation lies in a rigorous qualitative methodology utilizing the **Kamiwaza Agentic Merit Index (KAMI) v0.1**, a benchmark framework designed for controlled, repeatable interactive environments. Rather than relying solely on success metrics, the researchers performed a manual, fine-grained review of **900 execution traces** across three distinct models. This trace-level analysis allowed for the identification of specific behavioral strategies and failure archetypes during complex tasks across 10 distinct scenariosâ€”specifically filesystem operations, SQL interactions, CSV analysis, and text extractionâ€”moving beyond "black box" scoring to a transparent diagnosis of agent reasoning.

The results challenge the scaling hypothesis, demonstrating that parameter count is not a linear predictor of agentic success. The study identified **post-training reinforcement learning** as a more significant reliability factor than raw scale and noted instances where newer models underperformed their predecessors. Furthermore, the analysis revealed a low correlation between standard benchmarks and agentic utilityâ€”an "**Agentic Disconnect**"â€”and isolated four recurrent failure archetypes.

---

## Key Findings

*   **Scale $\neq$ Agentic Robustness:** Model size alone is not a predictor of success. The massive Llama 4 Maverick (400B) showed only marginal improvements over the much smaller Granite 4 Small (32B).
*   **Post-Training is Critical:** Reliability is attributed more to reinforcement learning post-training than to raw parameter scale.
*   **The "Agentic Disconnect":** There is a low correlation between standard benchmarks and actual agentic utility; aggregate scores often obscure specific behavioral strategies.
*   **Four Recurrent Failure Archetypes:**
    1.  **Premature Action:** Acting before fully understanding the context.
    2.  **Over-helpfulness:** Providing unnecessary or incorrect information in an attempt to assist.
    3.  **Context Pollution:** Corrupting the working memory or context window.
    4.  **Fragile Execution:** Failure to recover from minor errors during multi-step tasks.

---

## Methodology

The study employed a qualitative, trace-level analysis framework designed to move beyond simple success/failure metrics.

*   **Benchmark Framework:** Kamiwaza Agentic Merit Index (KAMI) v0.1.
*   **Environment:** Interactive benchmarks with controlled, repeatable environments mimicking enterprise workflows.
*   **Data Volume:** Manual review of 900 agentic execution logs.
*   **Test Scenarios:** 10 distinct scenarios covering:
    *   Filesystem operations
    *   Text extraction
    *   CSV analysis
    *   SQL interactions
*   **Analysis Type:** Fine-grained, per-trial behavioral analysis to identify specific reasoning strategies and failure modes.

---

## Technical Details

| Aspect | Description |
| :--- | :--- |
| **Analysis Type** | Qualitative, trace-level manual review of execution logs. |
| **Benchmark** | Kamiwaza Agentic Merit Index (KAMI) v0.1. |
| **Environment** | Interactive, controlled, repeatable enterprise workflows. |
| **Models Analyzed** | **Llama 4 Maverick** (400B), **Granite 4 Small** (32B), **DeepSeek V3.1/V3**. |
| **Comparisons** | Contrasted agentic performance against traditional benchmarks. |
| **Capabilities Tested** | Filesystem ops, text extraction, CSV analysis, SQL scenarios. |

---

## Results

The study provided empirical evidence challenging the assumption that model scaling leads to proportional reliability improvements.

*   **Scale vs. Performance:** Parameter scale is not a linear predictor of agentic success. Newer models (e.g., Qwen3) occasionally underperformed predecessors.
*   **Impact of Post-Training:** DeepSeek V3.1 outperformed V3, suggesting that reinforcement learning is a more critical reliability factor than raw scale.
*   **Benchmark Correlation:** Confirmed a significant disconnect between standard benchmark scores and real-world agentic utility.

---

## Contributions

This work provides both theoretical and practical advancements in the field of LLM agents:

1.  **Taxonomy of Failure Modes:** Provided a comprehensive classification system for diagnosing agent errors.
2.  **Challenging the Scaling Hypothesis:** Offered empirical evidence that bigger models do not automatically equal better agents.
3.  **Evaluation Guidelines:** Established new guidelines emphasizing interactive grounding and recovery behavior over static scoring.
4.  **Enterprise Design Recommendations:** Suggested deployment strategies focused on verification mechanisms and constraint discovery to ensure reliable execution.