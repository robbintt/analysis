---
title: 'QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware
  Edge Inference'
arxiv_id: '2506.23934'
source_url: https://arxiv.org/abs/2506.23934
generated_at: '2026-02-03T19:07:50'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference

*Xiangchen Li; Saeid Ghafouri; Bo Ji; Hans Vandierendonck; Deepu John; Dimitrios S. Nikolopoulos*

***

### âš¡ Quick Facts

| Metric | Value |
| :--- | :--- |
| **Computation Payload Reduction** | > 80% |
| **Parameter Size Reduction** | Avg. 77% (Range: 62â€“84%) |
| **Accuracy Degradation** | < 1% |
| **Quality Score** | 8/10 |
| **Citations** | 40 |

***

## ðŸ“‹ Executive Summary

Deploying deep learning inference on edge devices is often constrained by the heterogeneity of hardware and strict limitations in memory, power, and computational capacity. Traditional approaches frequently rely on fixed pre-trained models or static partitioning strategies, which fail to dynamically adapt to varying device capabilities and network conditions. This rigidity leads to inefficient resource utilization, resulting in high latency, energy consumption, and operational costs. This paper addresses the critical need for a system that can significantly reduce computational overhead while maintaining high inference accuracy across diverse and unpredictable edge environments.

The core innovation is **QPART**, an accuracy-aware and workload-balanced inference system that introduces a joint optimization framework for model quantization and inference partitioning. Utilizing a collaborative server-edge approach, the system dynamically generates request-specific models tailored to the current device capacity and available bandwidth. QPART employs Post-Training Quantization (PTQ) with a Uniform Asymmetric Quantizer, allowing for the edge segment of the model to undergo layer-wise quantization. Notably, this work represents the first quantization strategy within an inference serving system to optimize layer-wise bit-width based on theoretical accuracy degradation measurements, ensuring strict adherence to accuracy constraints while minimizing execution time and cost.

Evaluations of QPART demonstrate substantial performance improvements over traditional and non-optimized baselines. The system achieved a reduction in computation payloads of over 80% and decreased parameter sizes by an average of 77% (ranging from 62% to 84%). Crucially, these efficiency gains were realized with minimal impact on model performance, maintaining accuracy degradation below 1%. Furthermore, QPART outperformed Autoencoder-based and Pruning-based methods by achieving the lowest overall cost, while also delivering significant reductions in energy consumption and total inference time.

The significance of this research lies in its demonstration that dynamic, collaborative resource management can effectively overcome the limitations of static edge inference. By proving more cost-efficient and robust than fixed pre-trained models, QPART establishes a new paradigm for accuracy-aware edge computing. This work paves the way for deploying sophisticated deep learning models on resource-constrained devices without sacrificing performance, offering a scalable and adaptive solution for the increasingly complex landscape of edge AI applications.

***

## ðŸ”¬ Methodology

The researchers propose **QPART**, an accuracy-aware and workload-balanced inference system designed specifically for edge computing environments. The methodology is characterized by a dynamic, collaborative approach between the server and the edge device.

*   **Joint Optimization:** The system integrates joint model quantization and inference partitioning to solve resource constraints dynamically.
*   **Dynamic Generation:** The server generates request-specific models based on the current capacity of the device and available bandwidth.
*   **Adaptive Partitioning:** Workloads are partitioned adaptively rather than statically.
*   **Optimization Framework:** A novel framework is employed to minimize time and cost subject to strict accuracy constraints.

***

## ðŸ”‘ Key Findings

The evaluation of QPART highlights significant improvements in efficiency and cost-effectiveness:

*   **Payload Reduction:** Achieved a decrease in computation payloads of over **80%** compared to traditional approaches.
*   **Accuracy Retention:** Successfully maintained accuracy degradation below **1%**, ensuring model reliability.
*   **Resource Efficiency:** Demonstrated substantial reductions in overall execution time and power consumption.
*   **Robustness:** Proved to be more cost-efficient and robust than fixed pre-trained models across diverse edge scenarios.

***

## ðŸ—ï¸ Technical Details

The implementation of QPART relies on specific quantization techniques and optimization variables.

| Component | Description |
| :--- | :--- |
| **Core Techniques** | **Post-Training Quantization (PTQ)** combined with **Model Partitioning**. |
| **Quantization Type** | Uses PTQ to quantize the model after training to avoid the need for retraining. |
| **Partitioning** | The model is divided at a partition point **$p$**. The edge segment (layers 1 to $p$) undergoes layer-wise quantization. |
| **Quantizer** | Utilizes a **Uniform Asymmetric Quantizer**. |
| **Optimization Variables** | Variables include the partition point and the bit-width vector for each edge layer. |

***

## ðŸ’¡ Research Contributions

This paper makes three distinct contributions to the field of edge computing and inference:

1.  **Novel System Architecture:** Introduction of a system architecture that dynamically adapts to heterogeneous edge hardware and memory constraints.
2.  **Theoretical Framework:** Development of a theoretical optimization framework that jointly solves for model quantization and inference partitioning.
3.  **Pioneering Quantization Strategy:** The first quantization strategy to optimize layer-wise bit-width within an inference serving system using theoretical accuracy degradation measurement.

***

## ðŸ“Š Performance Results

The comparative results of QPART against baseline methods highlight its efficacy:

*   **Parameter Size:** Reduced by **62% to 84%**, with an average reduction of **77%**.
*   **Computation Payload:** Reduced by over **80%**.
*   **Energy & Time:** Demonstrated significant reductions in both energy consumption and total inference time.
*   **Cost Comparison:** Outperformed Autoencoder-based and Pruning-based methods by achieving the **lowest overall cost**.

***

*Analysis Quality Score: 8/10*