---
title: 'WUSH: Near-Optimal Adaptive Transforms for LLM Quantization'
arxiv_id: '2512.00956'
source_url: https://arxiv.org/abs/2512.00956
generated_at: '2026-02-03T18:40:14'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# WUSH: Near-Optimal Adaptive Transforms for LLM Quantization

*Jiale Chen; Vage Egiazarian; Roberto L. Castro; Torsten Hoefler; Dan Alistarh*

***

> ### **Quick Facts**
> * **Target Model:** Llama-3.1-8B-Instruct
> * **Quantization Format:** W4A4 (MXFP4)
> * **Accuracy Gain:** +2.8 pts (RTN) / +0.7 pts (GPTQ)
> * **Throughput:** Up to **6.6×** over BF16
> * **Optimality:** Proven near-optimal for FP & INT quantizers
> * **Fine-tuning:** Not required

***

## Executive Summary

Low-bit quantization (specifically 4-bit weights and activations, or W4A4) is critical for deploying Large Language Models (LLMs) efficiently, but it often results in significant accuracy degradation due to the presence of activation outliers and the suboptimal dynamic range of quantized formats. Existing methods to mitigate this, such as fixed Hadamard transforms, are data-agnostic and lack theoretical grounding regarding their optimality. Consequently, the field requires a quantization approach that can handle outliers effectively while maintaining high inference throughput without requiring computationally expensive model fine-tuning.

The paper introduces **WUSH**, a novel class of adaptive, non-orthogonal linear transforms that are provably near-optimal for both Floating Point (FP) and Integer (INT) quantizers. Unlike fixed rotations, WUSH synthesizes a traditional Hadamard backbone with a data-dependent second-moment component derived via closed-form solutions. Technically, the transform minimizes L2 output loss by integrating four specific steps: Whitening (via Cholesky decomposition), Alignment (via SVD), Scaling (inverse square root), and a Hadamard rotation. This block-diagonal construction adapts specifically to the data distribution of the model, effectively neutralizing the impact of extreme outliers without the need for retraining.

WUSH demonstrates state-of-the-art performance in W4A4 quantization scenarios, particularly on the Llama-3.1-8B-Instruct model. When utilizing standard Round-to-Nearest (RTN) quantization in the MXFP4 format, WUSH achieved a **+2.8 average point gain** in accuracy over the strongest Hadamard-based baselines. When integrated with GPTQ, it still provided a **+0.7 point gain**. Beyond accuracy, the method offers substantial efficiency improvements, delivering up to **6.6× per-layer throughput** over a BF16 baseline when utilizing FP4 MatMul kernels. These results effectively close the accuracy gap between NVFP and MXFP formats.

This work represents a significant theoretical and practical advancement in LLM quantization. By providing a rigorous derivation for optimal linear transforms, the authors address a longstanding theoretical gap in the understanding of pre-quantization transformations. Practically, WUSH enables high-fidelity 4-bit inference that rivals the accuracy of higher-precision formats while offering the speed necessary for real-time deployment. The method's ability to maintain accuracy without fine-tuning, combined with an efficient fused GPU implementation, positions it as a highly viable solution for the next generation of efficient LLM serving systems.

***

## Key Findings

*   **Significant Accuracy Gains:** WUSH improves W4A4 quantization accuracy over the strongest Hadamard-based baselines, achieving a **+2.8** average point gain on Llama-3.1-8B-Instruct (MXFP4 format) with RTN and a **+0.7** gain with GPTQ.
*   **High Throughput Efficiency:** The method delivers substantial performance improvements, offering up to **6.6×** per-layer throughput over BF16 when utilizing FP4 MatMul.
*   **Theoretical Near-Optimality:** The WUSH transform is provably near-optimal for both Floating Point (FP) and Integer (INT) quantizers under mild assumptions.
*   **Effective Outlier Handling:** By incorporating a data-dependent component, WUSH successfully mitigates the impact of extreme outliers that typically stretch dynamic range and amplify low-bit quantization errors.

***

## Methodology

The authors derive closed-form optimal linear blockwise transforms designed specifically for joint weight-activation quantization. This approach targets standard RTN AbsMax-scaled block quantizers, supporting both integer and floating-point formats.

The resulting construction, **WUSH**, utilizes a non-orthogonal transform that synthesizes a traditional Hadamard backbone with a data-dependent second-moment component. This allows the transform to adapt to the data distribution rather than relying on fixed, data-agnostic rotations.

*   **Derivation:** Focuses on minimizing L2 output loss between original FP matrix multiplication and quantized approximation.
*   **Architecture:** Uses a block-diagonal structure to reduce inference overhead.
*   **Optimization:** The approach is optimized for hardware via an efficient fused GPU implementation.

***

## Contributions

*   **Theoretical Derivation:** The paper provides a rigorous derivation of closed-form optimal linear blockwise transforms for quantization, addressing a theoretical gap where previous methods (like Hadamard rotations) were fixed and their optimality was unproven.
*   **Adaptive Transform Architecture:** Introduction of WUSH, a novel construction that creates a non-orthogonal transform by merging a Hadamard backbone with data-dependent second-moment statistics, moving beyond data-agnostic alternatives.
*   **Empirical Validation:** Demonstrates state-of-the-art performance in W4A4 quantization scenarios, balancing high accuracy retention with significant computational throughput (6.6× over BF16) on large-scale models like Llama-3.1-8B.

***

## Technical Details

**Objective**
WUSH aims to minimize the L2 output loss between the original floating-point matrix multiplication and the quantized approximation. It requires no model fine-tuning.

**Transform Characteristics**
*   **Type:** Closed-form, data-aware, non-orthogonal, adaptive linear blockwise transforms.
*   **Structure:** Block-diagonal to reduce inference overhead.

**Mathematical Construction**
The transform formula combines four distinct steps:
$$T_{wush}(i) = H S^{-1/2}(i) U^{\top}(i) W'^{\top}(i)$$

1.  **Whitening:** Via Cholesky decomposition.
2.  **Alignment:** Via Singular Value Decomposition (SVD).
3.  **Scaling:** Via inverse square root.
4.  **Rotation:** A Hadamard rotation.

**Integration**
*   **Quantizers:** Integrates with RTN and GPTQ (using an interleaved intra/inter-block update schedule).
*   **Inference:** Computes the sum of products of pre-quantized weights and quantized activations.

***

## Results

The performance of WUSH was evaluated on **Llama-3.1-8B-Instruct**:

*   **Accuracy:**
    *   Achieved an average accuracy gain of **+2.8 points** over Hadamard baselines in W4A4 (MXFP4) with RTN.
    *   Achieved a gain of **+0.7 points** with GPTQ.
    *   Successfully closes the accuracy gap between NVFP and MXFP formats.
*   **Performance:**
    *   Metrics show up to **6.6× per-layer throughput** compared to a BF16 baseline using FP4 MatMul.
*   **Theoretical:**
    *   Transforms are provably optimal for FP block quantizers.
    *   Asymptotically optimal for INT block quantizers.
    *   Effectively handles outliers.

***

## Assessment

**Quality Score:** 9/10

**References:** 40 citations