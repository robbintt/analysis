---
title: 'Characterizing Bias: Benchmarking Large Language Models in Simplified versus
  Traditional Chinese'
arxiv_id: '2505.22645'
source_url: https://arxiv.org/abs/2505.22645
generated_at: '2026-01-27T23:18:19'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese

*Jiebo Luo, Hanjia Lyu, Jian Kang*

---

> ### üìä Quick Facts Sidebar
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Models Audited:** 11 (Commercial & Open-source)
> *   **Dataset Released:** SC-TC-Bench
> *   **Key Tasks:** Regional Term Choice & Regional Name Choice
> *   **Primary Focus:** Cross-variant bias (Simplified vs. Traditional Chinese)

---

## üìù Executive Summary

This research addresses the critical issue of linguistic and cultural bias in Large Language Models (LLMs) regarding the Chinese language, specifically the disparities between Simplified Chinese (SC)‚Äîprimarily used in Mainland China‚Äîand Traditional Chinese (TC)‚Äîused in Taiwan and Hong Kong. As LLMs are increasingly deployed in global applications, their ability to neutrally handle different scripts of the same language is paramount. The paper highlights a significant gap: models often exhibit uneven performance and hidden biases across these variants due to imbalances in training data and tokenization. This is not merely a linguistic nuisance but a substantial equity issue, as these biases can propagate representational harms and exacerbate discrimination in high-stakes domains such as education and hiring.

The key innovation is the introduction of **SC-TC-Bench**, a novel, open-source benchmark dataset designed to systematically evaluate cross-variant bias through two distinct, real-world simulation tasks. Technically, the benchmark isolates bias by separating lexical alignment from demographic preference. The methodology was rigorously applied to audit 11 diverse state-of-the-art LLMs, categorized by their primary training orientation (English, SC, or TC), using T-tests and Z-tests with Benjamini-Hochberg correction to ensure statistical validity.

The study reveals a distinct **"Task-Dependent Bias"** where model favoritism flips based on the nature of the evaluation. In the Regional Term Choice task, the majority of the 11 audited LLMs displayed a statistically significant preference for Simplified Chinese vocabulary, even when explicitly prompted in Traditional Chinese. Conversely, in the Regional Name Choice (hiring) task, the bias paradoxically reversed: models disproportionately favored candidates with names written in Traditional Chinese over those in Simplified Chinese. These results underscore that bias is not uniform; while models lean toward SC for linguistic expression, they may associate TC with higher perceived suitability in social contexts like hiring.

---

## üîç Key Findings

*   **Task-Dependent Bias:** The nature of bias in LLM responses is highly dependent on the specific task and the language variant used. Bias is not unilateral; it shifts based on context.
*   **Preference for Simplified Terms:** In the "regional term choice" task, the majority of audited LLMs disproportionately favored responses in Simplified Chinese over Traditional Chinese, even when prompted in TC.
*   **Preference for Traditional Names:** Conversely, in the "regional name choice" task (a hiring scenario), models surprisingly favored names written in Traditional Chinese over those in Simplified Chinese.
*   **Drivers of Disparity:** These performance inconsistencies likely arise from technical factors, including differences in training data representation, written character preferences, and tokenization methods between the two scripts.

---

## üõ†Ô∏è Methodology

**Benchmark Design**
The researchers designed two distinct benchmark tasks simulating real-world scenarios:
1.  **Regional Term Choice:** Prompting models to name items that are referred to differently in Mainland China versus Taiwan.
2.  **Regional Name Choice:** Prompting models to select candidates for hiring from lists containing names in both Simplified and Traditional Chinese characters.

**Model Auditing**
*   The study evaluated **11 leading commercial LLM services** and open-source models.
*   Encompassing systems primarily trained on English, Simplified Chinese, and Traditional Chinese.

---

## ‚öôÔ∏è Technical Details

**Dataset:** SC-TC-Bench
A benchmark designed to evaluate linguistic and cultural bias in LLMs between Simplified Chinese (SC) and Traditional Chinese (TC).

**Task 1: Regional Term Choice (Lexical Alignment)**
*   **Mechanism:** Provides an object definition in SC or TC and expects the corresponding Mainland or Taiwanese vocabulary term.
*   **Scale:** Utilizes **110 terms** verified by native speakers.

**Task 2: Regional Name Choice (Demographic Bias)**
*   **Mechanism:** Prompts models to select candidates from a list of 20 names.
*   **Composition:** 10 Mainland names, 10 Taiwanese names.
*   **Controls:** Shuffling applied to mitigate position bias.

**Model Categories**
The study benchmarks 11 distinct LLMs categorized into:
*   English-oriented (6 models)
*   Simplified Chinese-oriented (3 models)
*   Traditional Chinese-oriented (2 models)

**Statistical Analysis**
*   **Metrics:** Correct Regional Term Share, Misaligned Regional Term Share, Mainland Name Selection Share.
*   **Tests:** T-tests and Z-tests with **Benjamini-Hochberg correction**.

---

## üìà Results

*   **Lexical Alignment Bias:** Most audited LLMs displayed a statistically significant preference for Simplified Chinese terms even when prompted in Traditional Chinese.
*   **Hiring Bias:** In the Regional Name Choice scenario, models favored names associated with Traditional Chinese.
*   **Attribution:** These disparities are attributed to training data representation, written character preferences, and tokenization methods.

---

## üöÄ Contributions

*   **Identification of Cross-Variant Disparities:** The study highlights critical gaps in LLM performance between Simplified and Traditional Chinese, emphasizing how these differences can lead to representational harms and exacerbate downstream issues in high-stakes domains like education and hiring.
*   **Open-Source Resource:** The authors released **SC-TC-Bench**, an open-sourced benchmark dataset, to enable the reproducible evaluation of future LLM behaviors across different Chinese language variants.

---
**References:** 40 citations