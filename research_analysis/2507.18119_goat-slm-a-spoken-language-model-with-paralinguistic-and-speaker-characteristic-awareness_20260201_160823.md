# GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness

*Hongjie Chen; Zehan Li; Yaodong Song; Wenming Deng; Yitong Yao; Yuxin Zhang; Hang Lv; Xuechao Zhu; Jian Kang; Jie Lian; Jie Li; Chao Wang; Shuangyong Song; Yongxiang Li; Zhongjiang He; Xuelong Li*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Model Backbone** | TeleChat2-7B |
| **Total Training Data** | 425k hours (187.7M samples) |
| **Total Compute** | ~11,060 GPU hours |
| **Core Innovation** | Dual-modality head architecture |

---

> **EXECUTIVE SUMMARY**
>
> Current Spoken Language Models (SLMs) typically treat speech merely as a textual representation, overlooking crucial paralinguistic elements such as emotion, dialect, and age. This limitation prevents systems from achieving human-like nuance, resulting in voice interactions that lack social awareness and expressive depth. Addressing this is critical for advancing human-computer interaction, as effective communication relies heavily on these non-semantic acoustic cues to convey meaning, intent, and personality.
>
> GOAT-SLM introduces a dual-modality head architecture built on the TeleChat2-7B backbone, explicitly designed to decouple linguistic modeling from acoustic realization. The system comprises five modulesâ€”Listen, Think, Write, Speak, and Flow-Matchingâ€”separating semantic reasoning from sound generation. A key technical advancement is the three-stage training pipeline, which aligns three distinct data types: linguistic content, paralinguistic attributes (such as emotion), and speaker characteristics (including age and dialect). This architecture ensures that large language model intelligence is preserved during speech generation, specifically by initializing the "Speak" module from the "Write" module.
>
> Validated on the TELEVAL benchmark, GOAT-SLM demonstrates balanced performance across both semantic (textual) and non-semantic (acoustic) tasks. The model was trained on a massive scale of 187.7 million samples (425k hours of speech), which enabled it to outperform existing open-source baselines significantly. Specific findings indicate the model's superior capability in handling complex speech characteristics, including nuanced emotion recognition, distinct dialectal variations, and age-sensitive interactions. The results confirm that the decoupled architecture effectively maintains high-quality text understanding while generating contextually expressive speech.
>
> This research shifts the paradigm of spoken language modeling from text-centric processing to socially aware AI that integrates rich paralinguistic cues. By proving that speaker-specific traits and emotional variations can be effectively modeled in an end-to-end framework, GOAT-SLM establishes a scalable foundation for future voice interfaces. This advancement enables the creation of systems that are not only semantically accurate but also capable of the expressive, human-like interactions required for natural and empathetic human-computer communication.

---

## Key Findings

*   **Balanced Performance:** GOAT-SLM achieves a well-balanced performance across both semantic (linguistic content) and non-semantic (paralinguistic) tasks.
*   **Superiority in Nuance:** The model outperforms existing open-source models in handling specific complex speech characteristics, including emotion, dialectal variation, and age-sensitive interactions.
*   **Benchmark Validation:** These capabilities were validated on TELEVAL, a multi-dimensional evaluation benchmark designed to test various aspects of spoken language modeling.
*   **Efficacy of Decoupling:** The study confirms that the architectural separation of linguistic modeling from acoustic realization enables both robust language understanding and expressive, adaptive speech generation.

---

## Methodology

The research employs a structured approach to modeling spoken language by separating distinct information modalities.

### Architecture
The model utilizes a **dual-modality head architecture** designed to decouple linguistic modeling from acoustic realization. This separation allows the system to process text semantics separately from speech sound characteristics.

### Training Strategy
A **modular, staged training strategy** is employed to enhance efficiency and versatility. This method progressively aligns three distinct types of information:
*   Linguistic data
*   Paralinguistic data
*   Speaker characteristic data

### Data Resources
The training leverages **large-scale speech-text corpora** to facilitate the integration of these diverse information streams.

---

## Technical Details

### System Architecture
GOAT-SLM utilizes a modality-alignment paradigm based on the **TeleChat2-7B backbone**. The architecture consists of five distinct modules:

1.  **Listen:** Whisper-small encoder with CNN/Transformer projector.
2.  **Think:** Bottom 15 layers dedicated to semantic reasoning.
3.  **Write:** Top 15 layers dedicated to text generation.
4.  **Speak:** Top 15 layers for speech generation (initialized from the Write module).
5.  **Flow-Matching:** Speech Decoder.

### Training Pipeline
The model employs a **three-stage training pipeline**:
1.  **Instruction Tuning**
2.  **Alignment**
3.  **Generation**

### Paralinguistic Dimensions
The system handles diverse non-semantic characteristics, including:
*   Emotion
*   Non-speech vocalizations
*   Age
*   Various dialects

---

## Results & Analysis

### Training Statistics
*   **Dataset Size:** ~187.7 million samples (425k hours of speech).
*   **Compute Infrastructure:** A800-80G GPUs.

### Compute Breakdown (GPU Hours)
| Stage | Process | Duration (GPU Hours) |
| :--- | :--- | :--- |
| Stage 1 | Base Initialization | 42.6 |
| Stage 2-1 | Alignment Phase A | 2,305.2 |
| Stage 2-2 | Alignment Phase B | 2,251.2 |
| Stage 3-1 | Generation Phase A | 4,566.4 |
| Stage 3-2 | Generation Phase B | 1,495.2 |
| **Total** | | **~11,060.6** |

### Qualitative Outcomes
*   Using Adam/AdamW optimizers with learning rates between $1e-5$ and $6e-5$, the model achieved balanced semantic and paralinguistic performance.
*   The strategy of initializing the speech generation branch ("Speak") from the text generation branch ("Write") showed **superior preservation of LLM intelligence**.
*   Validated on the TELEVAL benchmark, the model consistently outperformed open-source baselines in emotion and dialect handling.

---

## Contributions

The research makes four primary contributions to the field of spoken language modeling:

1.  **Introduction of GOAT-SLM:** The release of a novel spoken language model that expands the scope of spoken language modeling beyond text semantics to include rich paralinguistic cues (dialect, age, emotion) and non-speech vocalizations.
2.  **Architectural Innovation:** The proposal of a dual-modality framework that treats speech not just as a text carrier, but as a modality capable of carrying expressive and socially relevant information.
3.  **Advancement of Socially Aware AI:** The work advances the development of 'socially aware' spoken language systems by demonstrating the technical feasibility and value of modeling speaker-specific traits and emotional variations in end-to-end interactions.
4.  **Benchmark Validation:** Comprehensive validation on the TELEVAL benchmark, setting a new standard for evaluating multi-dimensional spoken language capabilities.

---
*References: 29 citations*