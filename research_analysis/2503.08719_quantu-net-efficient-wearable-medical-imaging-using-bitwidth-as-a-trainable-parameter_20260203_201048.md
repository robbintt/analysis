---
title: 'QuantU-Net: Efficient Wearable Medical Imaging Using Bitwidth as a Trainable
  Parameter'
arxiv_id: '2503.08719'
source_url: https://arxiv.org/abs/2503.08719
generated_at: '2026-02-03T20:10:48'
quality_score: 9
citation_count: 12
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QuantU-Net: Efficient Wearable Medical Imaging Using Bitwidth as a Trainable Parameter

*Christiaan Boerkamp; Akhil John Thomas*

***

> ### ðŸ“Š Quick Facts
>
> *   **Model Compression:** ~8x reduction in size
> *   **Average Bitwidth:** 4.24 bits
> *   **Validation Accuracy:** 94.25%
> *   **Accuracy Drop:** Only 1.89% vs. FP32 baseline
> *   **Dataset:** 780 images (Breast Ultrasound)
> *   **Framework:** Brevitas (PyTorch)

***

## Executive Summary

The integration of deep learning into wearable medical devices is currently hindered by the significant computational and memory requirements of high-precision models. Standard architectures like U-Net, while effective for biomedical image segmentation, typically rely on 32-bit floating-point arithmetic, making them too resource-intensive for deployment on edge hardware such as wearables and FPGAs. This paper addresses the critical challenge of enabling real-time, high-performance medical imaging on resource-constrained devices. Bridging this gap is essential for advancing Edge AI in healthcare, as it allows for continuous, low-power diagnostic capabilities directly on the patient rather than relying on cloud-based processing.

The core innovation is **QuantU-Net**, a quantized iteration of the U-Net architecture that leverages Quantization-Aware Training (QAT) via the Brevitas PyTorch library. Unlike traditional quantization methods that require manual tuning or extensive search spaces to determine the optimal bitwidth, this approach treats bitwidth as a trainable parameter. Technically, the authors engineered a composite custom loss function that integrates Binary Cross-Entropy (BCE) Loss, Dice Loss, and a novel Bitwidth Loss. This mechanism allows the model to simultaneously optimize for segmentation accuracy and computational efficiency during a single training session, dynamically adjusting the precision of network weights to balance performance against hardware constraints.

Experimental results using the Breast Ultrasound Images Dataset (780 grayscale images across three classes) demonstrate that the proposed method achieves substantial compression without sacrificing clinical utility. QuantU-Net reduced the model size by approximately 8x by lowering precision to an average of 4.24 bits. Despite this aggressive compression, the model maintained a validation accuracy of 94.25%, representing a marginal drop of only 1.89% compared to the floating-point baseline. Furthermore, the optimized training workflow successfully identified the ideal balance of bitwidth and accuracy within a single training run, eliminating the need for iterative, time-consuming hyperparameter searches.

This research represents a significant step forward for the deployment of AI in clinical settings, proving that integer-based quantized models can retain the accuracy required for medical tasks. By validating the suitability of these models for specialized hardware like FPGAs and AI accelerators, the authors facilitate the development of real-time, low-power diagnostic tools.

***

## Key Findings

*   **High-Efficiency Quantization:** Reduced precision to an average of **4.24 bits**, achieving an approximate **8x reduction in model size**.
*   **Maintained Accuracy:** Achieved a validation accuracy of **94.25%**, representing only a **1.89% drop** compared to the floating-point baseline.
*   **Optimized Training Workflow:** Reduced the search space for optimal bitwidth and accuracy combinations to a **single training session** via a custom loss function.
*   **Hardware Suitability:** The use of integer arithmetic makes the model highly suitable for resource-constrained hardware like **FPGAs and AI accelerators**.

***

## Methodology

The researchers utilized **QuantU-Net**, a quantized iteration of the standard U-Net architecture. They employed the **Brevitas PyTorch library** to perform quantization-aware training (QAT).

A **composite custom loss function** was engineered to handle multiple objectives simultaneously:
1.  **Binary Cross-Entropy (BCE) Loss:** To optimize classification probability.
2.  **Dice Loss:** To handle class imbalance common in medical imaging.
3.  **Bitwidth Loss:** To actively reduce model precision during training.

***

## Technical Details

*   **Architecture:** U-Net CNN backbone for biomedical image segmentation (FP32 baseline).
*   **Training Strategy:** Quantization-Aware Training (QAT) using Brevitas.
*   **Innovation:** Bitwidth is treated as a **trainable parameter** to dynamically adjust precision.
*   **Target Hardware:** Optimized for integer arithmetic (FPGAs, Neural Network Accelerators).
*   **Data Preprocessing:**
    *   Resizing images to **128x128** pixels.
    *   Normalizing pixel values to **[0, 1]**.
    *   Converting to PyTorch tensors.
*   **Dataset Split:** Stratified approach:
    *   **70%** Training
    *   **15%** Validation
    *   **15%** Testing

***

## Contributions

*   **Enabling Edge AI in Healthcare:** Bridges the gap between high-performance deep learning and hardware constraints for medical image segmentation on wearable devices.
*   **Joint Optimization Strategy:** Presents a novel method to simultaneously optimize segmentation accuracy and computational efficiency by treating bitwidth as a trainable parameter within the loss function.
*   **Advancement in Real-time Diagnostics:** Facilitates the development of real-time, low-power diagnostic solutions by proving quantized models can retain clinical accuracy on specialized hardware like FPGAs.

***

## Results

*   **Compression:** Achieved an average bitwidth of 4.24 bits, reducing model size by ~8x.
*   **Accuracy:** Validation accuracy of 94.25% (Drop of only 1.89%).
*   **Efficiency:** Training workflow reduced the search for optimal configurations to a single session.
*   **Dataset Utilization:** Breast Ultrasound Images Dataset (780 grayscale images, 3 classes: Normal, Benign, Malignant).

***

**Report Quality Score:** 9/10  
**References:** 12 citations