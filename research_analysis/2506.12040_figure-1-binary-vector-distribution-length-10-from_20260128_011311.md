---
title: 'Figure 1: Binary vector distribution (length 10) from'
arxiv_id: '2506.12040'
source_url: https://arxiv.org/abs/2506.12040
generated_at: '2026-01-28T01:13:11'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Figure 1: Binary vector distribution (length 10) from

*Yike Guo, Bei Liu, Zheyu Wang, Efficient Sub, Binary Codebook, Qiyuan Zhu, Hao Gu, Lujun Li, Learnable Transformation, Sirui Han*

---

<details>
<summary><strong>ðŸ“Š Quick Facts</strong></summary>

| Metric | Detail |
| :--- | :--- |
| **Framework** | BTC-LLM (Binary Codebook-LLM) |
| **Method** | Post-Training Quantization (PTQ) |
| **Compression** | Sub-1-bit |
| **Quality Score** | 8/10 |
| **Memory Reduction** | >32x compared to FP16 |
| **Latency Impact** | Zero overhead (transformation merged) |
| **Outlier Suppression** | Max activation 0.4 (vs 8 in FP16) |

</details>

---

## Executive Summary

**Problem**
This research addresses the critical challenge of sub-1-bit quantization for Large Language Models (LLMs), a frontier in model compression necessary for deploying massive models on resource-constrained edge devices. Existing sub-1-bit methods typically face a triad of debilitating limitations: severe performance deterioration (often resulting in "**performance collapse**" where the model loses most of its accuracy), high computational complexity during inference, and poor hardware compatibility. This incompatibility largely stems from a reliance on sparse mask management, which fails to run efficiently on standard hardware accelerators designed for dense matrix operations. The paper targets the disconnect between theoretical compression limits and practical deployment feasibility, seeking a method that achieves extreme memory reduction without sacrificing accuracy or requiring specialized hardware support.

**Innovation**
The authors introduce **BTC-LLM** (Binary Codebook-LLM), a post-training quantization (PTQ) framework that innovates by replacing traditional sparse mask reliance with a clustering-based compression scheme. The framework operates on two technical pillars:
1.  **Learnable Transformation**, which optimizes an invertible diagonal scaling matrix ($\Lambda$) and an orthogonal rotation matrix ($R$) using Cayley SGD. This mechanism aligns the distribution of binarized weights with their full-precision counterparts to enhance layer-wise representation quality.
2.  **Flash and Accurate Binary Codebook**, which identifies and clusters recurring binary vectors ($\pm 1$) using tailored distance metrics (Hamming distance via XOR/POPCNT) and sign-based centroid updates.

Crucially, the transformation matrices are merged into the weight matrix during inference, ensuring that the optimization process adds **zero computational overhead** at run time.

**Results**
Experimental results on LLaMA-2-7B demonstrate that BTC-LLM significantly outperforms existing state-of-the-art baselines in both stability and efficiency. The framework achieved exceptional activation outlier suppression, reaching a maximum absolute value of **0.4** compared to **8** for FP16, **15** for BiLLM, and **10** for ARB-LLM. In direct comparison to the STBLLM baseline, which suffered "Performance Collapse" (retaining only 51%â€“65% accuracy) and incurred a computational penalty of 2.5x slower speed alongside 3x higher memory usage, BTC-LLM maintained superior accuracy while operating efficiently on standard hardware. Furthermore, the approach validates the standard promise of binary quantization, reducing memory requirements by **over 32x** compared to FP16 baselines.

**Impact**
The significance of this research lies in its removal of the primary barriers to deploying extreme compression models in real-world environments. By eliminating the need for sparse mask management, BTC-LLM enables efficient sub-1-bit inference on standard, commercially available hardware rather than requiring custom, sparsity-aware accelerators. This lowers the barrier to entry for deploying highly efficient LLMs across a wider range of devices. By successfully combining adaptive weight transformation with binary pattern clustering, the paper establishes a new paradigm for post-training quantization that bridges the gap between theoretical compression limits and practical, high-performance deployment.

---

## Key Findings

*   **Resolution of Sub-1-bit Limitations:** The proposed framework (BTC-LLM) successfully overcomes performance deterioration, high computational complexity, and limited hardware compatibility.
*   **Superior Efficiency and Accuracy:** Achieves maximal memory/computational efficiency and superior accuracy through adaptive weight transformation and binary pattern clustering.
*   **Hardware Compatibility:** Eliminates sparse mask reliance to perform efficient inference on standard hardware.
*   **Effective Distribution Alignment:** Effectively aligns binarized weights with full-precision distributions to enhance layer-wise representation quality.

---

## Methodology

The research introduces BTC-LLM, a sub-1-bit quantization framework built upon two technical innovations:

1.  **Learnable Transformation:**
    *   Optimizes invertible scaling and rotation matrices to align weight distributions.
    *   Ensures that binarized weights retain the representational power of full-precision weights.

2.  **Flash and Accurate Binary Codebook:**
    *   Identifies and clusters recurring binary vectors using tailored distance metrics.
    *   Utilizes sign-based centroid updates.
    *   Replaces traditional sparse mask management with a clustering-based compression scheme.

---

## Technical Details

**BTC-LLM (Binary Codebook-LLM)**
*   **Type:** Post-training quantization (PTQ) framework.
*   **Objective:** Sub-1-bit compression of Large Language Models without sparse masks.
*   **Core Components:**
    *   *Learnable Transformation:* Utilizes a diagonal scaling matrix ($\Lambda$) and an orthogonal rotation matrix ($R$). These are optimized via Cayley SGD to align binarized weights with full-precision distributions.
    *   *Flash and Accurate Binary Codebook:* Compresses weights by clustering recurring binary patterns ($\pm 1$) into a compact index codebook using Hamming distance (XOR/POPCNT).
*   **Inference Optimization:** The transformation is merged into the weight matrix during inference, adding zero computational overhead.

---

## Results

**Activation Outlier Suppression (on LLaMA-2-7B)**
*   **BTC-LLM:** Max absolute value of **0.4**
*   **FP16:** Max absolute value of 8
*   **BiLLM:** Max absolute value of 15
*   **ARB-LLM:** Max absolute value of 10

**Comparison vs. STBLLM Baseline**
*   **STBLLM Issues:** Suffered "Performance Collapse" (retaining only **51%â€“65% accuracy**), **2.5x slower** computation, and **3x higher** memory overhead.
*   **BTC-LLM Performance:** Operates efficiently on standard hardware with superior accuracy.

**Memory Efficiency**
*   Binary quantization generally reduces memory requirements by **over 32x** compared to FP16.

---

## Contributions

*   **Framework Development:** Created BTC-LLM, the first framework to combine adaptive weight transformation with binary pattern clustering for sub-1-bit compression without sparsity-aware drawbacks.
*   **Algorithmic Components:**
    *   Introduction of the Learnable Transformation mechanism for weight optimization.
    *   Introduction of the Flash and Accurate Binary Codebook for vector clustering.
*   **Practical Deployment Value:** Lowers the barrier for deploying extreme compression models on standard hardware by eliminating the need for sparse mask management.

---

**References:** 40 citations  
**Quality Score:** 8/10