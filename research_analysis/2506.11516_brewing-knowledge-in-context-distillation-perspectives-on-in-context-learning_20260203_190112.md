---
title: 'Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning'
arxiv_id: '2506.11516'
source_url: https://arxiv.org/abs/2506.11516
generated_at: '2026-02-03T19:01:12'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning
*Chengye Li; Haiyun Liu; Yuanxi Li*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Research Type:** Theoretical Analysis
> *   **Core Mechanism:** Implicit Knowledge Distillation (KD)
> *   **Key Metrics:** Rademacher Complexity, Maximum Mean Discrepancy (MMD)

---

## Executive Summary

In-Context Learning (ICL) has become a dominant paradigm for large language models (LLMs), allowing them to learn tasks from demonstration examples without parameter updates. However, the underlying mechanics of ICL remain theoretically opaque, with existing literature divided between gradient-based and distributional explanations. This fragmentation hampers the optimization of prompting strategies, as practitioners lack a unified mathematical framework to understand why certain demonstrations work better than others. This paper addresses the need for a rigorous theoretical foundation that bridges these disparate views to explain the inference-time behavior of ICL.

The key innovation is the formalization of ICL as an implicit form of Knowledge Distillation (KD) occurring at inference time. The authors model the pre-trained LLM as a "Teacher" and the task-specific reference model formed via attention as a "Student." Technically, the framework maps inference-time attention to Gradient Descent (GD) using a feature mapping function derived from the softmax kernel. The authors decompose the attention output into two components: an initialization component derived from demonstration tokens and a gradient correction component derived from query tokens. Crucially, they identify the attention normalization factor ($1/D'$) as a dynamic learning rate that adjusts based on input token similarity.

The study yields significant theoretical results rather than empirical performance metrics. The authors derived a generalization bound for ICL using Rademacher complexity and established that the bias of distilled weights grows linearly with the Maximum Mean Discrepancy (MMD) between the prompt and target distributions. A critical theoretical finding is the proof of implicit alignment, defined by the condition $\frac{2\eta^*}{N} = \frac{1}{D'}$, which explicitly links the distillation learning rate to the attention normalization constant. Furthermore, domain stationarity analysis confirms that the effective learning rate remains proportional to $1/D'$, dynamically scaling with the similarity of input tokens.

This research significantly advances the field by providing the first formalization of inference-time attention as a distillation process. By unifying prior gradient-based and distributional analyses into a single coherent framework, the paper offers a robust theoretical explanation for empirical phenomena previously lacking mathematical grounding. This work moves the field toward a more principled approach to prompt engineering, suggesting that minimizing distributional divergence (MMD) is essential for reducing model bias. These insights provide a theoretical basis for developing automated tools for demonstration selection and optimizing prompt construction.

---

## Key Findings

*   **Implicit Knowledge Distillation:** In-context learning (ICL) can be interpreted as an implicit form of knowledge distillation (KD), where prompt demonstrations guide the model to form a task-specific reference model during inference.
*   **Bias and Distributional Divergence:** The bias of the distilled weights grows linearly with the Maximum Mean Discrepancy (MMD) between the prompt and target distributions.
*   **Generalization Bounds:** The authors derived a generalization bound based on Rademacher complexity for the ICL process.
*   **Unified Theory:** The proposed framework unifies prior gradient-based and distributional analyses of ICL into a single theoretical explanation for observed empirical phenomena.

---

## Methodology

The authors utilized a theoretical modeling approach, framing inference-time attention as a distillation process. They employed statistical learning theory tools—specifically Rademacher complexity—to establish generalization bounds. Additionally, they used Maximum Mean Discrepancy (MMD) to quantify and analyze the statistical divergence between prompt distributions and target distributions, thereby linking distributional shifts to model bias.

---

## Technical Details

The paper proposes a theoretical framework reinterpreting In-Context Learning (ICL) as an implicit form of Knowledge Distillation (KD). Below are the structured technical components of this framework:

*   **Core Framework:**
    *   **Teacher:** The pre-trained LLM.
    *   **Student:** An internal reference model formed by attention.
    *   **Process:** Attention acts as the mechanism for distillation.

*   **Mathematical Mapping:**
    *   Establishes a mapping of ICL to Gradient Descent (Lemmas 2.1 and 2.2).
    *   Utilizes a feature mapping function derived from the softmax kernel.

*   **Attention Decomposition:**
    *   The attention output is split into:
        1.  **Initialization Component:** Derived from Demonstration Tokens.
        2.  **Gradient Correction Component:** Derived from Query Tokens.

*   **Weight Initialization Mechanism (Theorem 3.1):**
    *   Defines the initialization $W_0$ via a distillation loss.
    *   Minimizes the $l_2$ distance between teacher and student outputs.

*   **Dynamic Learning Rate:**
    *   The attention normalization factor $1/D'$ is identified as a dynamic learning rate.

---

## Results

The provided text focuses on theoretical derivations rather than empirical experimental results. Key theoretical findings include:

*   **Proof of Implicit Alignment:** Derives the condition $\frac{2\eta^*}{N} = \frac{1}{D'}$ linking the distillation learning rate to the attention normalization constant.
*   **Unified Framework:** Unifies prior gradient-based and distributional ICL theories into a single framework, characterizing in-context learning as the extraction of task-specific weights via distillation.
*   **Domain Stationarity Analysis:** Establishes that the effective learning rate is proportional to $1/D'$ and adjusts based on the similarity of input tokens.

---

## Contributions

*   **Formalization of Inference-Time Distillation:** To the best of the authors’ knowledge, this is the first work to formalize inference-time attention specifically as a distillation process.
*   **Theoretical Framework:** The paper provides a rigorous theoretical framework that explains existing empirical phenomena in ICL which were previously lacking solid theoretical grounding.
*   **Practical Guidance:** The research offers theoretical insights that can inform future practices in prompt engineering and the automated selection of demonstrations.