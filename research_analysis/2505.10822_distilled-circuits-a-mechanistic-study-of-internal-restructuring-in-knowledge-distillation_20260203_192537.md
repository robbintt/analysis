---
title: 'Distilled Circuits: A Mechanistic Study of Internal Restructuring in Knowledge
  Distillation'
arxiv_id: '2505.10822'
source_url: https://arxiv.org/abs/2505.10822
generated_at: '2026-02-03T19:25:37'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Distilled Circuits: A Mechanistic Study of Internal Restructuring in Knowledge Distillation

*Reilly Haskins; Benjamin Adams*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Teacher Model** | GPT2-small (85M parameters) |
| **Student Model** | DistilGPT2 (42M parameters) |
| **Secondary Study** | BERT & DistilBERT |
| **Task** | Numeral sequence completion |
| **Dataset Size** | 100 examples |
| **Hardware** | NVIDIA A100 GPU |
| **Quality Score** | 9/10 |
| **References** | 40 citations |

---

## üìù Executive Summary

> Knowledge Distillation (KD) is a ubiquitous technique for compressing large "teacher" models into smaller "student" models, typically by training the student to mimic the teacher's output logits. However, the internal mechanisms governing how the student approximates the teacher‚Äôs function remain poorly understood. The field largely treats KD as a process of simple size reduction or behavioral cloning, overlooking the specific architectural transformations that occur within the model's layers. This gap in understanding is critical because it obscures how distillation fundamentally alters the internal circuitry of neural networks, with potentially significant consequences for model robustness, generalization, and safety that are not apparent when evaluating output performance alone.
>
> This study introduces a mechanistic interpretability framework to reverse-engineer and compare the internal circuitry of teacher and student models. Rather than relying solely on output similarity metrics, the authors employ circuit discovery through iterative pruning and path patching to isolate and compare specific computational sub-graphs. A key technical contribution is a novel "influence-weighted alignment metric," which quantifies functional similarity by comparing the weights and contributions of specific components (Attention Heads and MLPs) rather than final layer activations. The research utilizes this methodology to conduct a comparative case study between GPT2-small (85M parameters) and DistilGPT2 (42M parameters), analyzing how specific functions‚Äîsuch as numeral detection and sequence completion‚Äîare mapped onto different internal structures.
>
> The analysis reveals that student models do not merely compress teacher weights but undergo significant internal restructuring, characterized by the discarding of non-essential functions and the compression of multiple roles into fewer components. This results in a marked decrease in functional redundancy. Quantitative ablation studies on numeral sequence completion tasks demonstrated that student models are significantly more fragile; ablating 'Numeral Detection' components caused a performance drop of 33.18% in the teacher compared to a drastic 87.73% in the student. Similarly, ablating 'Successor Heads' resulted in a 34.94% drop for the teacher versus 77.57% for the student. Additionally, the student model completely discarded certain functionalities, such as 'Similar Member Detection' (which had a -27.83% impact on the teacher), confirming that distillation actively removes circuits deemed less critical for the primary training objective. These trends were validated across architectures, showing consistent results in both GPT-2 and BERT variants.
>
> This research provides the first rigorous mechanistic account of how knowledge distillation reorganizes neural network internals, shifting the paradigm from viewing distillation as simple compression to viewing it as a structural reorganization process. By establishing a direct link between distillation-induced restructuring and the loss of redundant circuits, the authors highlight a critical trade-off: while student models gain efficiency, they often sacrifice robustness and resilience to perturbations. The introduction of an influence-weighted alignment metric offers the field a new, scalable tool for evaluating model similarity that goes beyond surface-level accuracy. These findings are pivotal for the future of model compression, suggesting that practitioners must carefully weigh the benefits of smaller models against the potential vulnerabilities introduced by the elimination of backup computational pathways.

---

## üîë Key Findings

*   **Active Reorganization:** Student models actively reorganize, compress, and discard teacher components rather than just compressing them uniformly.
*   **Reduced Redundancy:** Distillation leads to a stronger reliance on fewer individual components, reducing the model's internal safety nets.
*   **Preserved Behavior, Shifted Mechanics:** Functional behaviors (outputs) are preserved while internal computational mechanisms shift significantly.
*   **Robustness Implications:** These internal shifts have critical implications for model robustness and generalization capabilities.

---

## üß™ Methodology

The study employs a rigorous approach to reverse-engineer neural network functions:

*   **Mechanistic Interpretability:** Used to reverse-engineer neural network functions and understand internal circuits.
*   **Comparative Case Study:** A direct comparison between **GPT2-small** (Teacher) and **DistilGPT2** (Student), analyzing internal circuits and activation patterns.
*   **Novel Alignment Metric:** Introduction of an influence-weighted component similarity metric to quantify functional alignment beyond simple output similarity.

---

## ‚öôÔ∏è Technical Details

### Models & Architecture
*   **Primary Study:** GPT2-small (85M params) $\rightarrow$ DistilGPT2 (42M params).
*   **Secondary Study:** BERT $\rightarrow$ DistilBERT (to confirm generalization).

### Knowledge Distillation Process
*   **Transfer Method:** Knowledge transfer via softened output logits.
*   **Objective:** Minimization of Kullback-Leibler divergence between teacher and student distributions using temperature-scaled softmax.

### Mechanistic Interpretability Workflow
*   **Circuit Discovery:** Iterative pruning and path patching on a corrupted dataset.
*   **Node Ablation:** Utilizing a threshold of 80% of the original logit difference.
*   **Component Analysis:**
    *   **Attention Heads:** Analysis of QK matrices.
    *   **MLPs:** Residual stream decomposition, cosine similarity, and PCA.
*   **Proposed Metric:** Influence-weighted alignment metric for scalable functional comparison.

---

## üìà Results

The experiment focused on **numeral sequence completion** (100 examples on NVIDIA A100 GPU).

### Internal Restructuring
*   **Discarding:** Student models discard less critical functionalities (e.g., 'Similar Member Detection').
*   **Compression:** Multiple functions are compressed into single components.
*   **Reliance:** Increased reliance on individual components indicates reduced redundancy.

### Quantitative Ablation Results
*   **'Similar Member Detection':**
    *   Teacher Impact: **-27.83%**
    *   Student Impact: **None** (Discarded)
*   **'Numeral Detection':**
    *   Teacher Impact: **-33.18%**
    *   Student Impact: **-87.73%** (Significant sensitivity)
*   **'Successor Heads':**
    *   Teacher Impact: **-34.94%**
    *   Student Impact: **-77.57%**

### Generalization
*   **Fragility:** Student models exhibited performance drops **2x to 2.5x** greater than the teacher, indicating a loss of redundant circuits.
*   **Validation:** Restructuring trends were confirmed to generalize across BERT/DistilBERT architectures.

---

## üèÜ Contributions

*   **Mechanistic Understanding:** Provides the first mechanistic understanding of how internal circuitry transforms during distillation.
*   **New Metric:** Introduces a robust metric for quantifying functional alignment beyond output similarity.
*   **Structural Link:** Establishes the critical link between distillation-induced structural changes and downstream model properties like robustness and generalization.