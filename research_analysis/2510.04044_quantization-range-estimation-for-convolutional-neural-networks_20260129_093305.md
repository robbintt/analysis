# Quantization Range Estimation for Convolutional Neural Networks

*Bingtao Yang; Yujia Wang; Mengzhi Jiao; Hongwei Huo*

---

### ðŸ“‹ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 12 Citations |
| **Methodology** | Post-Training Quantization (PTQ) |
| **Key Innovation** | Proof of Local Convexity in Range Estimation |
| **Top Result** | 4-bit ResNet-50 Accuracy: **94.47%** (vs AdaRound 68.44%) |

---

## Executive Summary

This research addresses the critical challenge of determining optimal quantization ranges in Post-Training Quantization (PTQ) for Convolutional Neural Networks (CNNs). While PTQ is essential for deploying deep learning models on resource-constrained hardware without the high computational cost of retraining, existing methods often fail at low bit-widths. Standard max-based range estimation suffers from significant discretization errors when aggressive quantization is applied, leading to accuracy degradation. This paper specifically investigates how to maintain near-floating-point accuracy across various bit-widthsâ€”including the standard 8-bit, efficient 6-bit, and aggressive 4-bit levelsâ€”without requiring expensive fine-tuning or retraining processes.

The authors introduce **REQuant**, a novel framework that formulates range estimation as a convex optimization problem rather than relying on heuristic statistics. The key theoretical innovation is proving that the quantization range estimation problem is locally convex, allowing for the identification of optimal solutions via an efficient search algorithm. Technically, the method operates in two stages within a transformed weights space: first, it applies a non-linear square root transformation ($\text{sign}(w)\sqrt{|w|}$) to map weights into a distribution more conducive to uniform quantization; second, it employs Parameterized Range Estimation, introducing a scaling factor $\alpha$ that minimizes the Mean Squared Error (MSE) by dynamically shrinking the quantization interval layer-by-layer.

Evaluations on CIFAR-10 and CIFAR-100 using ResNet and Inception-v3 architectures demonstrate that REQuant achieves state-of-the-art performance. The method achieves near-lossless compression at 8-bit and 6-bit precisions with almost no loss in top-1 accuracy; for instance, on CIFAR-10, ResNet-50 maintains 95.21% accuracy at 6-bit compared to the FP32 baseline of 95.24%. Significant gains appear at the challenging 4-bit level: on CIFAR-10, REQuant achieves 94.47% accuracy for ResNet-50, dramatically outperforming the AdaRound baseline (68.44%). On CIFAR-100, the method leads at 4-bit, achieving 76.84% accuracy. An ablation study confirms the efficacy of the dual approach, showing that the combined method increases ResNet-18 accuracy on CIFAR-10 from 89.09% (standard method) to 94.67%.

The significance of this work lies in its ability to enable high-accuracy, low-bit quantization entirely post-training, removing the barrier of retraining for model deployment. By establishing the theoretical foundation of local convexity in range estimation, the authors provide a robust mathematical basis for future PTQ research. The empirical results demonstrate that 4-bit quantization is practically viable on standard architectures without the accuracy cliffs typically associated with aggressive compression, offering a highly efficient path for deploying CNNs in latency-sensitive and memory-limited environments.

---

## Key Findings

*   **Near-Lossless Compression:** Achieved results with almost no loss in top-1 accuracy for 8-bit and 6-bit quantization.
*   **Enhanced Low-Bit Accuracy:** Significantly enhances accuracy for challenging 4-bit quantization scenarios.
*   **State-of-the-Art Performance:** Generally outperforms existing techniques on ResNet series and Inception-v3 models.
*   **Theoretical Proof:** Proves that the quantization range estimation problem is locally convex, enabling the identification of optimal solutions.

---

## Methodology

The researchers formulate the range estimation process as an optimization problem designed to minimize quantization errors by identifying layer-wise local minima.

*   **Convexity Proof:** Establishing that this problem is locally convex allowed the team to utilize this mathematical property for more reliable convergence.
*   **Efficient Search:** Based on the convexity proof, they developed an efficient search algorithm.
*   **Transformed Space:** The algorithm is applied to a 'transformed weights space' rather than the original weight distribution. This approach refines precision by operating in a domain where the weight distribution is more amenable to uniform quantization.

---

## Technical Details

The paper proposes **REQuant**, a Post-Training Quantization (PTQ) method specifically designed to address the high discretization errors associated with standard max-based range estimation at low bit-widths.

### Core Strategies

1.  **Parameterized Range Estimation**
    *   Introduces a scaling factor **$\alpha$** to shrink the quantization interval.
    *   Objective: Minimize the Mean Squared Error (MSE) of the weights.

2.  **Weight Reshaping Distribution**
    *   Applies a non-linear square root transformation: **$\text{sign}(w)\sqrt{|w|}$**.
    *   Maps weights to a distribution that is more amenable to uniform quantization.

### Implementation Specifications

*   **Theoretical Guarantee:** The method theoretically guarantees the local convexity of the optimization objective.
*   **Scope:** Implementation is performed layer-by-layer using an asymmetric quantization range.
*   **Exclusions:** Input and final layers are excluded from the transformation process.

---

## Results

Evaluations were conducted on CIFAR-10 and CIFAR-100 datasets utilizing ResNet and Inception-v3 models.

### Performance Highlights

*   **CIFAR-10 (6-bit):**
    *   **REQuant:** 95.21%
    *   **FP32 Baseline:** 95.24%
    *   *Demonstrates near-lossless compression.*

*   **CIFAR-10 (4-bit):**
    *   **REQuant:** 94.47%
    *   **AdaRound (Baseline):** 68.44%
    *   *Significant outperformance of state-of-the-art baselines.*

*   **CIFAR-100 (4-bit):**
    *   REQuant leads performance with **76.84%** accuracy for ResNet-50.

### Ablation Study (CIFAR-10 ResNet-18)

The study validated the contribution of both range estimation and reshaping:
*   **Standard Method:** 89.09% Accuracy
*   **Combined REQuant:** 94.67% Accuracy

---

## Contributions

*   **Novel Framework:** Introduction of a new range estimation framework for post-training quantization modeled explicitly as an optimization problem.
*   **Algorithm & Proof:** Development of an efficient search algorithm backed by a rigorous proof of the problem's local convexity.
*   **Transformed Space Application:** Practical application of the algorithm to a transformed weights space, resulting in enhanced performance.
*   **Empirical Validation:** Comprehensive validation demonstrating state-of-the-art accuracy levels without the need for retraining.