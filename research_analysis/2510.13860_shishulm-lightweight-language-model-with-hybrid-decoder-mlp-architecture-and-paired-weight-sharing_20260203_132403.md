---
title: 'ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture
  and Paired Weight Sharing'
arxiv_id: '2510.1386'
source_url: https://arxiv.org/abs/2510.13860
generated_at: '2026-02-03T13:24:03'
quality_score: 8
citation_count: 30
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing

*Shivanshu Kumar; Gopalakrishnan Srinivasan*

---

> ### **Quick Facts**
> *   **Architecture Type:** Hybrid Decoder-MLP
> *   **Key Innovation:** Paired Weight Sharing & ShishuMLP Blocks
> *   **Memory Reduction:** Up to **25%**
> *   **Latency Improvement:** Up to **40%**
> *   **Evaluation:** MobileLLM-125M (MiniPile)
> *   **Quality Score:** 8/10

---

## Executive Summary

This research addresses the critical inefficiency inherent in standard Transformer architectures, specifically when applied to Small Language Models (SLMs) intended for resource-constrained environments. As SLMs become integral to agentic AI systems, the computational overhead of Multi-Headed Self-Attention (MHSA) and Layer Normalization (LN) creates significant bottlenecks in memory usage and latency. The authors identify that many of these operations are redundant, particularly in moderate-context scenarios, thereby limiting the feasibility of deploying performant models on edge devices. The paper focuses on resolving this trade-off between model capability and resource efficiency without necessitating a massive reduction in parameter count that would compromise linguistic reasoning.

The core innovation is **ShishuLM**, a Hybrid Decoder-MLP Architecture utilizing Paired Weight Sharing, designed to approximate standard Transformer blocks using Multi-Layer Perceptrons (MLPs). The authors leverage insights from AI interpretability to demonstrate that normalization and attention components often behave linearly in moderate contexts. This allows for the removal of the MHSA and the specific *input* LN layers typically preceding attention, while simplifying the forward pass to $x_{i+1} = x_i + \text{MLP}(\text{LN}(x_i))$. In this simplified architecture, the Layer Normalization is retained within the MLP pathway to maintain stability. To further optimize the design, the authors employ Earth Mover’s Distance (EMD) to quantify dissimilarity in weight distributions and implement a paired weight-sharing strategy for consecutive blocks, retaining critical attention layers primarily in the bottom blocks of the network.

ShishuLM demonstrates substantial efficiency gains compared to parent Transformer models, achieving up to a **25% reduction in memory requirements** and up to a **40% improvement in latency** during both training and inference. These results were validated through evaluations on two distinct SLM scales to ensure the approach's viability across different model sizes. The architecture effectively reduces the overall parameter count and KV cache size, validating the feasibility of the hybrid approach. Ablation studies conducted on MobileLLM-125M using the MiniPile dataset reveal that up to two-thirds of attention layers can be removed without significant performance degradation, provided the remaining attention layers are concentrated in the network's foundation. However, the study notes a threshold where reducing attention to less than one-third of the original layers results in a noticeable performance drop-off.

The significance of ShishuLM lies in its validation of a hybrid architectural paradigm that challenges the necessity of dense attention layers throughout the entire depth of a network. By providing theoretical evidence that Transformer blocks can be effectively approximated by MLPs, this work opens new avenues for designing resource-optimized models for on-device and agentic applications. This approach influences the field by offering a pragmatic methodology for reducing inference costs and memory footprints, potentially setting a new standard for the development of lightweight yet capable Small Language Models.

---

## Key Findings

*   **Efficiency Gains:** ShishuLM achieves up to a **25% reduction in memory requirements** and up to a **40% improvement in latency** during training and inference compared to parent transformer models.
*   **Linear Behavior:** In moderate-context scenarios, the computational load of normalization and attention scales linearly with input, allowing entire transformer blocks to be approximated using Multi-Layer Perceptrons (MLPs) without compromising performance.
*   **Resource Reduction:** The architecture successfully reduces both the parameter count and Key-Value (KV) cache requirements.
*   **Threshold Identification:** Ablation studies indicate that up to 2/3rds of attention layers can be removed without substantial loss, but reducing attention to less than 1/3rd causes noticeable degradation.

---

## Methodology

The authors introduce ShishuLM, a Hybrid Decoder-MLP Architecture utilizing Paired Weight Sharing. The design leverages insights from AI interpretability and inference-time layer pruning.

*   **Core Premise:** The approach relies on the observation that normalization and attention components behave linearly in moderate contexts. This enables the replacement of standard transformer blocks with MLP approximations.
*   **Evaluation Scope:** The model was evaluated on two Small Language Models (SLMs) of different scales to assess viability for resource-constrained applications.

---

## Technical Details

The ShishuLM architecture is defined by a hybrid design that optimizes the standard Transformer block.

### Architecture Components
*   **ShishuMLP Blocks:** Replaces standard Transformer decoder blocks by removing the Multi-Headed Self-Attention (MHSA) and input Layer Normalization (LN) layers to reduce computational overhead.
*   **Forward Pass Simplification:** The forward pass is simplified to the following equation:
    $$x_{i+1} = x_i + \text{MLP}(\text{LN}(x_i))$$
*   **Weight Sharing:** Employs a paired weight-sharing strategy for consecutive ShishuMLP blocks to reduce parameter count.
*   **Attention Budget:** Reduces attention and normalization layers, retaining them primarily in the **bottom blocks** of the network to maintain performance in moderate-context scenarios.

### Analytical Approach
*   **Redundancy Identification:** Uses **Earth Mover’s Distance (EMD)** to quantify dissimilarity between weight distributions across layers.

---

## Contributions

*   **Architectural Innovation:** Proposal of a lightweight, hybrid language model architecture (ShishuLM) combining decoders and MLPs with paired weight sharing to eliminate redundancies.
*   **Resource Optimization for SLMs:** Addressing the need for efficient Small Language Models in agentic AI systems by lowering memory and computational overhead.
*   **Theoretical Insights:** Providing analytical findings from a pre-training standpoint that validate the approximation of transformer blocks via MLPs.

---

## Results

*   **Performance Metrics:** Compared to parent Transformer models, the approach reduces memory requirements by up to **25%** and improves latency by up to **40%**. It also successfully decreases parameter count and Key-Value (KV) cache requirements.
*   **Ablation Studies (MobileLLM-125M / MiniPile):**
    *   Up to **2/3rds of attention layers** can be removed without substantial performance loss.
    *   Reducing attention to less than **1/3rd of original layers** causes noticeable degradation.
    *   Performance is best maintained when the remaining attention layers are concentrated in the **bottom blocks** of the model.

---

**References:** 30 citations