# Information-Theoretic Dual Memory System for Continual Learning

*RunQing Wu; KaiHui Huang; HanYi Zhang; QiHe Liu; GuoJin Yu; JingSong Deng; Fei Ye*

---

### ðŸ“‹ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Framework** | ITDMS (Information-Theoretic Dual Memory System) |
| **Core Theory** | Complementary Learning Systems (CLS) |
| **Datasets** | Split-MNIST, Split-CIFAR10 |
| **Buffer Sizes Tested** | 200, 500, 1000 |
| **Key Competitors** | DER++, ER, ICARL, HAL, GEM |
| **Best Performance** | ~93% Acc (Split-MNIST), ~46% Acc (Split-CIFAR10) |
| **Quality Score** | 8/10 |

---

## Executive Summary

Continual learning systems face the persistent challenge of **catastrophic forgetting**, where learning new tasks erodes previously acquired knowledge. Current state-of-the-art methods typically rely on a single, fixed-size memory buffer to store exemplars, creating a fundamental bottleneck that hampers the system's ability to balance adaptability to new data (plasticity) with the retention of old knowledge (stability). This single-buffer architecture struggles to optimize for the conflicting demands of rapidly integrating novel information and preserving diverse historical data, leading to performance degradation as the number of tasks increases and available memory remains constrained.

This paper introduces the **Information-Theoretic Dual Memory System (ITDMS)**, a novel approach grounded in Complementary Learning Systems (CLS) theory to resolve the stability-plasticity dilemma. The architecture decouples memory into two distinct but interacting components: a **Fast Memory Buffer** for rapid integration of novel temporal data via reservoir sampling, and a **Slow Memory Buffer** for long-term retention. These buffers operate concurrently; the Fast Memory ensures short-term plasticity, while the Slow Memory consolidates critical information over time.

Slow Memory utilizes a balanced sample selection procedure driven by information-theoretic principles, explicitly selecting samples to **maximize entropy** (ensuring diversity) and **minimize mutual information** (reducing redundancy). This dynamic interaction allows the system to actively curate the most informative samples, optimizing memory density while training across tasks.

Empirical validation on Class-Incremental Learning benchmarks demonstrates that ITDMS significantly outperforms state-of-the-art baselines such as DER++, ER, and GEM, particularly in severely resource-constrained environments. While standard methods often collapse under low-memory conditions, ITDMS maintains robust performance.

---

## Key Findings

*   **Dual Memory Efficiency:** The proposed system effectively overcomes single-buffer limitations by managing both new (Fast Memory) and learned (Slow Memory) samples concurrently.
*   **Optimization Strategy:** Optimization efficiency is achieved by using **reservoir sampling** for fast memory and **information-theoretic strategies** for slow memory.
*   **Redundancy Reduction:** A balanced sample selection procedure detects and eliminates redundant samples, effectively freeing capacity for new tasks.
*   **Validation:** Empirical results validate the effectiveness of the proposed system across multiple benchmarks.

---

## Methodology

The approach draws on **Complementary Learning Systems (CLS)** theory to propose the Information-Theoretic Dual Memory System (ITDMS). The methodology rests on three pillars:

1.  **Dual Architecture:** Consisting of a Fast Memory Buffer for novel samples and a Slow Memory Buffer for critical historical samples.
2.  **Sampling Strategies:** Fast memory utilizes reservoir sampling for short-term temporal updates, while Slow Memory relies on information-theoretic optimization for long-term stability.
3.  **Balanced Selection:** A procedure to detect and eliminate redundant samples to maintain high information density within the buffers.

---

## Technical Details

The paper proposes the **Information-Theoretic Dual Memory System (ITDMS)**, a decoupled architecture consisting of two distinct components:

### Architecture Components
*   **Fast Memory ($\mathcal{M}_{fast}$):** Utilizes reservoir sampling for short-term retention of novel data.
*   **Slow Memory ($\mathcal{M}_{slow}$):** Utilizes an information-theoretic strategy for long-term stability and knowledge preservation.

### Optimization Objective
The system seeks to minimize the loss across all tasks:
$$ \theta^\star = \text{argmin}_{\theta \in \tilde{\Theta}} \frac{1}{i} \sum_{j=1}^{i} \sum_{j'=1}^{n_j} L(y_{j'}^{j}, f_\theta(x_{j'}^{j})) $$

### Sample Selection Strategy
Sample selection for the Slow Memory is driven by two Information-Theoretic metrics:
*   **Maximizing Entropy ($H(X)$):** Ensures sample diversity.
*   **Minimizing Mutual Information ($I(X; Y)$):** Reduces redundancy between samples.

### Update Mechanism
*   **Fast Memory:** Real-time reservoir updates.
*   **Slow Memory:** 'Info-based sampling' involving pruning via Eq 18 and optimization via Eq 12.
*   **Total Loss:** Combines the current task training loss and memory replay loss.

---

## Contributions

1.  **ITDMS Architecture:** Introduction of a dual memory system that separates novel temporal data and critical historical data into two distinct buffers.
2.  **Memory Optimization Strategy:** Development of a new strategy using information theory to select diverse and informative samples for long-term storage.
3.  **Redundancy Management:** Proposal of a mechanism using balanced sample selection to handle the growing number of tasks by actively eliminating redundancy.

---

## Experimental Results

Experiments were conducted on **Split-MNIST** and **Split-CIFAR10** using Class-Incremental Learning. The system was compared against baselines including **DER++, ER, ICARL, HAL, and GEM** with buffer sizes of 200, 500, and 1000.

### Performance Summary

| Dataset | Buffer Size | ITDMS Accuracy | Note |
| :--- | :---: | :---: | :--- |
| **Split-CIFAR10** | 200 | **~41-42%** | Significantly outperforms baselines in small buffer regimes. |
| **Split-CIFAR10** | 500 | **~45-46%** | Strong performance maintained as capacity increases. |
| **Split-MNIST** | 200 | **~88-90%** | Robustness validated on simpler dataset. |
| **Split-MNIST** | 500 | **~91-92%** | |
| **Split-MNIST** | 1000 | **~92-93%** | |

The results confirm that the information-theoretic strategy is highly efficient in maximizing memory utility, particularly in scenarios where memory is scarce.

---

**References:** 40 citations
**Quality Score:** 8/10