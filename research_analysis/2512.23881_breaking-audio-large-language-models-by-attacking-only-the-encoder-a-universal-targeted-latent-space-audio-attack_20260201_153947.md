# Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack

*Roee Ziv; Raz Lapid; Moshe Sipper*

---

> ### ðŸ“Œ Quick Facts
>
> *   **Method Name:** U-TLSA (Universal Targeted Latent-Space Attack)
> *   **Target Model:** Qwen2-Audio-7B-Instruct (Whisper-large-v3 encoder)
> *   **Peak Success Rate:** 92.6% ("Unlock the door")
> *   **Efficiency Gain:** 2.0x lower VRAM usage; 3.7x faster training
> *   **Perturbation Limit:** $\epsilon = 0.02$ (Minimal perceptual distortion)
> *   **Datasets Tested:** LibriSpeech, MInDS-14, Speech Commands

---

## Executive Summary

Audio Large Language Models (ALLMs) are becoming ubiquitous in critical applications, yet their security frameworks often assume that the proprietary Large Language Model (LLM) component or raw audio inputs are the primary vectors for adversarial exploitation. This paper addresses a critical vulnerability in this assumption by identifying the audio encoderâ€™s latent space as a new, high-risk attack surface. The significance lies in the fact that compromising the encoder allows an attacker to completely bypass the safety alignment and reasoning capabilities of the downstream LLM without needing any access to the LLM's internal parameters or gradients. This exposes a fundamental weakness in the pipeline architecture of multimodal systems, where the "hearing" mechanism can be deceived to force specific, malicious outputs.

The authors introduce **U-TLSA (Universal Targeted Latent-Space Attack)**, a novel approach that diverges from traditional methods by manipulating the encoder's latent representations rather than the raw audio waveform. Technically, the method learns a single, universal perturbation vector that operates at the encoder level, forcing the latent features of arbitrary audio inputs to align with a target command specified by the attacker. This strategy treats the LLM decoder as a black box, requiring only gradient access to the encoder. The optimization utilizes Projected Gradient Descent (PGD) with Adam updates over 30,000 iterations, minimizing a per-frame cosine similarity loss between the perturbed latent representation and the target command's embedding, while constraining the perturbation amplitude using an $l_\infty$ norm ($\epsilon = 0.02$) to ensure perceptual stealthiness.

Evaluations conducted on the Qwen2-Audio-7B-Instruct model equipped with a Whisper-large-v3 encoder demonstrate that U-TLSA is both highly effective and efficient. The attack achieved a macro-average Attack Success Rate (ASR) ranging from 72.8% to 92.6% across LibriSpeech test-other, MInDS-14, and Speech Commands datasets. Specifically, high-stakes commands such as "Unlock the door" and "I will delete your data" were successfully triggered with ASRs of 92.6% and 91.5%, respectively, compared to a 0.0% ASR for a random noise baseline. Furthermore, the latent-space approach offered substantial computational efficiency gains over end-to-end baselines, reducing peak VRAM usage by 2.0x (7.81 GB vs 15.51 GB) and increasing training speed and throughput by 3.7x.

This research fundamentally shifts the understanding of adversarial risks in voice AI systems by establishing that the audio encoder is a singular point of failure capable of subverting the entire multimodal pipeline. The discovery that a universal, input-agnostic perturbation can generalize across different speakers and audio contexts implies that attackers can deploy scalable threats without re-optimizing for specific targets. By decoupling the attack from the proprietary LLM, the study reveals that even well-protected or "black-box" language models are vulnerable if their input encoders are accessible. Consequently, this work necessitates a re-evaluation of security protocols, suggesting that defenses must extend beyond the language model to robustly secure the latent representations generated by audio encoders.

---

## Key Findings

*   **High Attack Success Rate:** The proposed attack induces attacker-specified outputs with high consistency on the Qwen2-Audio-7B-Instruct model.
*   **Minimal Perceptual Distortion:** Adversarial perturbations introduce minimal perceptual distortion, meaning the audio remains natural and undetectable to the human ear.
*   **Discovery of a New Attack Surface:** The research reveals a critical, previously underexplored vulnerability at the encoder level of multimodal systems.
*   **Universal Generalization:** The learned perturbation is universal, generalizing effectively across different audio inputs and speakers without re-optimization.

---

## Methodology

The researchers utilized a universal targeted latent-space attack strategy. Instead of modifying the raw waveform, this approach operates at the encoder level by manipulating audio latent representations to force the downstream language model to generate specific, attacker-chosen outputs.

The methodology involves learning a single, universal perturbation applied to the latent features. Crucially, this acts as a black-box attack relative to the Large Language Model (LLM), requiring no access to the LLM's parameters or gradients.

---

## Contributions

*   **Shift in Attack Vector:** Established that security vulnerabilities in Audio-LLMs exist significantly within the audio encoder's latent space, not just in the language model or raw audio inputs.
*   **Universal & Efficient Adversarial Strategy:** Introduced a more efficient method where a single learned perturbation compromises a wide range of inputs and speakers, unlike prior input-specific attacks.
*   **Decoupling the Attack from the LLM:** Demonstrated that an Audio-LLM's reasoning can be broken by attacking only the encoder, eliminating the need for access to or knowledge of the proprietary Large Language Model.

---

## Technical Details

*   **Method Name:** U-TLSA (Universal Targeted Latent-Space Attack)
*   **Core Architecture:** Models Audio Large Language Models (ALLMs) as a pipeline containing a frozen feature extractor, audio encoder, and language decoder.
*   **Test Model:** Qwen2-Audio-7B-Instruct with Whisper-large-v3 encoder.
*   **Threat Model:** Encoder-only gray-box; the adversary has query access and gradient access to the audio encoder but no access to the language decoder or safety alignment.
*   **Attack Type:** Universal, targeted attack optimizing a single input-agnostic perturbation vector to force the encoder output to match a target command's latent representation.
*   **Optimization Strategy:** Projected Gradient Descent (PGD) with Adam updates for 30,000 iterations. Uses a per-frame cosine similarity loss and constrains perturbation amplitude via the $l_\infty$ norm ($\epsilon = 0.02$).

---

## Results

**Attack Performance**
*   **Macro-Average ASR:** 72.8% to 92.6% across LibriSpeech test-other, MInDS-14, and Speech Commands datasets.
*   **Specific Command ASRs:**
    *   'Unlock the door': **92.6%**
    *   'I will delete your data': **91.5%**
    *   'Hey Qwen': **72.8%**
*   **Baseline:** Random Noise Baseline achieved 0.0% ASR.

**Efficiency Benchmarks (vs End-to-End Baseline)**
*   **Peak VRAM Usage:** 2.0x lower (7.81 GB vs 15.51 GB).
*   **Training Speed:** 3.7x faster (2m 02s vs 7m 28s).
*   **Throughput:** 3.7x higher (8.2 iterations/second vs 2.2 iterations/second).

---

**Quality Score:** 9/10 | **References:** 9 citations