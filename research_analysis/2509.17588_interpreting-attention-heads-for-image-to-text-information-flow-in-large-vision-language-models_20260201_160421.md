# Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models

*Jinyeong Kim; Seil Kang; Jiwoo Park; Junhyeok Kim; Seong Jae Hwang*

---

> ### **Quick Facts**
> 
> *   **Methodology**: Head Attribution (HeAr)
> *   **Models Analyzed**: 10 LVLMs (LLaVA, InternVL2.5, Qwen2-VL)
> *   **Datasets**: COCO (200 images), DomainNet (300 images)
> *   **Key Metric**: Faithfulness >0.8 & Completeness <0.2
> *   **Quality Score**: 9/10
> *   **References**: 40 Citations

---

### **Executive Summary**

Large Vision-Language Models (LVLMs) have achieved remarkable performance in multi-modal tasks, yet the internal mechanisms governing how visual data is processed and integrated into textual generation remain opaque. A critical challenge in the field is understanding the precise "information flow" from image pixels to text tokens within the transformer architecture. Without a granular understanding of which components facilitate this cross-modal interaction, it is difficult to debug models, ensure safety, or improve efficiency. This paper addresses this interpretability gap by investigating the specific roles of attention heads in bridging image and text representations.

The authors propose **"Head Attribution" (HeAr)**, a novel methodology adapted from Component Attribution to isolate and quantify the contribution of individual attention heads. Technically, HeAr works by ablating subsets of attention heads and fitting a linear model to estimate attribution coefficients, thereby identifying which heads are essential for specific outputs. To more accurately block information flow without destabilizing the model, the method employs mean ablation rather than zero ablation.

Evaluated across 10 distinct LVLMs, HeAr outperformed baseline methods. It revealed a distinct **"self-repair" mechanism**; while a specific subset of heads drives the output, significantly more heads are required for completeness than for faithfulness—a gap that widens in larger models. Crucially, Token Level analysis demonstrated that text information propagates to role-related tokens before image information arrives, and that image information is embedded significantly in both object and background tokens.

---

## Key Findings

*   **Specialized Attention Heads**: A distinct subset of attention heads is primarily responsible for facilitating image-to-text information flow.
*   **Semantic Activation**: The selection and activation of these critical attention heads are determined by the semantic content of the input image.
*   **Text Propagation Sequence**: Text information propagates to role-related tokens and the final token *before* the model receives image information.
*   **Image Token Distribution**: Image information is embedded significantly in both object-related tokens and background tokens.

---

## Methodology

The researchers introduce a novel technique called **'Head Attribution' (HeAr)** to isolate and identify consistent patterns among attention heads. The methodology involves a granular analysis at two levels:

1.  **Head Level**: Identifying specific attention heads relied upon to answer questions about the main object.
2.  **Token Level**: Tracing the sequence of information propagation to understand how text and image information are integrated across different tokens.

---

## Technical Details

| Aspect | Specification |
| :--- | :--- |
| **Core Task** | Visual Object Identification |
| **Prompt Format** | `USER: <image> What is the main object in the image? Please answer with a single word. ASSISTANT:` |
| **Method** | Head Attribution (HeAr) – adapts Component Attribution |
| **Mechanism** | Ablating subsets of attention heads and fitting a linear model to estimate contribution via coefficients. |
| **Ablation Strategy** | Mean ablation (used instead of zero ablation to better block information flow). |
| **Analysis Scope** | 10 LVLMs from LLaVA, InternVL2.5, and Qwen2-VL families. |
| **Interaction Focus** | Interactions between text query vectors and image token Key/Value matrices. |

---

## Results

*   **Performance vs. Baselines**: HeAr outperformed baselines (Random, Attention weights, Causal logit diff) by requiring the minimum number of heads to achieve target Faithfulness (>0.8) and Completeness (<0.2).
*   **Self-Repair Mechanisms**: Evidence of self-repair was found, as significantly more heads are needed for completeness than for faithfulness. Notably, this gap widens in larger models.
*   **Single Head Inefficacy**: Single head ablation proved ineffective. In LLaVA-1.5-7B, no single head accounted for >5% of the logit difference in 80% of cases.
*   **Head Location**: Critical heads are located in middle-to-later layers.
*   **Weight Correlation**: No significant correlation exists between attribution coefficients and image attention weights.
*   **Semantic Clustering**: t-SNE visualization revealed that similar heads are used for the same objects across variations, indicating semantic clustering.

---

## Contributions

*   **New Tool**: The introduction of 'head attribution' provides a new, effective tool for interpreting the operations of attention heads within LVLMs.
*   **Empirical Evidence**: The study provides empirical evidence that image-to-text information flow is a structured process governed by semantic meaning.
*   **Future Direction**: The work establishes attention-head level analysis as a promising direction for future research into the internal mechanisms of LVLMs.

---

**References**: 40 citations  
**Quality Score**: 9/10