# Text Classification in the LLM Era -- Where do we stand?
*Sowmya Vajjala; Shwetali Shimangaud*

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 24 References |
| **Scope** | 32 Datasets across 8 Languages |
| **Comparison** | LLMs vs. Pre-trained Language Models (PLMs) |

---

## Executive Summary

### Problem
As Large Language Models (LLMs) rapidly advance, practitioners face a critical strategic decision: whether to replace established smaller pre-trained language models (PLMs) and supervised learning techniques with modern generative approaches. This paper addresses the urgent need to evaluate LLM efficacy against traditional baselines in text classification, specifically targeting the lack of comprehensive multilingual evaluation. The study investigates whether the massive scale and reasoning capabilities of LLMs translate to superior accuracy across diverse languages and task types, or if smaller, fine-tuned models remain the more efficient and accurate choice.

### Innovation
The key innovation is a rigorous comparative framework analyzing four distinct classification strategies across a broad linguistic landscape of **32 datasets spanning 8 languages**:
*   Zero-Shot Prompting
*   Few-Shot Fine-Tuning (using the FastFit algorithm)
*   Synthetic Data Generation
*   Full Supervision baselines

Technically, the study utilized specific open models (Qwen2.5-7B, Aya23-8B, and Aya-Expanse-8B) alongside GPT-4, employing the Python Instructor library for output enforcement. For few-shot scenarios, researchers fine-tuned the 'paraphrase-multilingual-mpnet-base-v2' model using just 10 examples per label.

### Results
Benchmarking across the 32 datasets revealed that LLMs do not universally outperform traditional methods. Smaller PLMs trained on full human-labeled datasets consistently matched or exceeded the accuracy of LLM approaches, often outperforming zero-shot LLMs like GPT-4 by **10–20 F1 points** on complex topic classification tasks.

*   **Sentiment:** Zero-shot strategies demonstrated strong metrics specifically in sentiment classification (achieving **>80% F1** on English datasets).
*   **Synthetic Data:** Classifiers trained on synthetic data generated from multiple LLMs reached **90–95% of the performance** of full-supervision baselines.
*   **Language Gap:** Non-English zero-shot performance often lagged behind English by **15–30%**, highlighting a persistent linguistic bias.

### Impact
This research provides essential strategic guidance for developing multilingual text classification systems, establishing clear metrics-driven guidelines on when to utilize specific LLM strategies versus traditional models. By validating the high viability of synthetic data generation from multiple LLMs—demonstrating it can recover nearly all ground-truth performance—the authors identify a cost-effective pathway for building robust classifiers. The findings challenge the assumption that larger models are inherently better, reinforcing the relevance of smaller, task-specific PLMs for specialized tasks.

---

## Key Findings

*   **Task-Specific Performance:** Zero-shot approaches demonstrate strong performance specifically in sentiment classification tasks but are generally outperformed by other methods (few-shot, supervised) for other types of tasks.
*   **Efficacy of Synthetic Data:** Classifiers built using synthetic data sourced from multiple LLMs outperform classifiers based on zero-shot open LLMs.
*   **Multilingual Disparities:** Significant performance gaps exist across different languages in all evaluated classification scenarios.
*   **Utility of Smaller Models:** While LLMs offer improvements, they do not universally surpass all approaches; smaller pre-trained language models trained on full human-labeled datasets remain competitive or superior depending on the specific context.

---

## Methodology

The researchers conducted a comprehensive comparative analysis to assess the relative performance of Large Language Models versus smaller pre-trained language models.

*   **Scope:** Evaluated **32 datasets** spanning **8 languages**.
*   **Strategies Evaluated:**
    1.  **Zero-shot classification:** Using LLMs without task-specific training.
    2.  **Few-shot fine-tuning:** Utilizing limited labeled examples.
    3.  **Synthetic data-based classifiers:** Training models on data generated by LLMs.
    4.  **Full supervision baselines:** Classifiers built using the complete human-labeled dataset.

---

## Technical Architecture

The study evaluated distinct architectures for Zero-Shot Prompting, Few-Shot Fine-Tuning, and Synthetic Data Generation.

### Zero-Shot Prompting
*   **Models Utilized:** Qwen2.5-7B, Aya23-8B, Aya-Expanse-8B, and GPT-4.
*   **Implementation:** Used the Python Instructor library for output enforcement.
*   **Optimization:** Employed English prompts to optimize performance across the board.

### Few-Shot Fine-Tuning
*   **Algorithm:** FastFit.
*   **Base Model:** 'paraphrase-multilingual-mpnet-base-v2'.
*   **Parameter:** 10 examples per label.

### Synthetic Data Generation
*   **Strategy:** Ensemble approach to ensure diversity.
*   **Generators:** GPT-4, Qwen2.5-7B, and Aya-Expanse-8B.
*   **Cost Management:** Open LLMs were run locally to minimize operational costs.

---

## Contributions

*   **Comprehensive Evaluation:** Provided a broad evaluation of LLM capabilities in text classification by testing across a diverse linguistic landscape and numerous datasets.
*   **Practitioner Guidelines:** Established clear guidelines for developers on when to utilize specific LLM strategies when developing multilingual text classification systems.
*   **Validation of Synthetic Data:** Highlighted the potential value of synthetic data generation from multiple LLMs as a viable strategy for building better classifiers compared to relying solely on zero-shot inference.