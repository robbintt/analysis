---
title: Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster
arxiv_id: '2506.18034'
source_url: https://arxiv.org/abs/2506.18034
generated_at: '2026-01-27T23:23:30'
quality_score: 5
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster

*Zihang Jiang, Kevin Zhou, Fenghe Tang, Xiaodong Tao, Zhiyang He, Wenxin Ma*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Framework** | LLM4Seg |
| **Base Models** | LLaMA3.2-1B, DeepSeek-R1-Distill-Qwen-1.5B |
| **Backbones** | U-Net, CMUNeXt, nnUNet |
| **Parameter Increase** | +4.19M |
| **Top Performance** | 87.66% Avg F1 (CMUNeXt + LLaMA) |
| **Modality Coverage** | Ultrasound, Dermoscopy, 3D CT |
| **Quality Score** | 5/10 |
| **References** | 40 Citations |

---

### üìù Executive Summary

> Robust generalization in medical image segmentation remains a significant hurdle due to the high variability inherent in imaging modalities such as ultrasound, dermoscopy, and 3D CT scans. While traditional convolutional neural networks (CNNs) excel at extracting local features, they often fail to capture the broad, high-level semantic context required to handle the heterogeneity of medical data. This research addresses this limitation by investigating whether the extensive reasoning capabilities and semantic knowledge encoded in pre-trained Large Language Models (LLMs) can be effectively transferred to pixel-level vision tasks to enhance performance and generalizability without the prohibitive computational costs of training massive vision models from scratch.
>
> The authors introduce **LLM4Seg**, a hybrid framework designed to integrate a frozen, pre-trained LLM layer directly into a segmentation encoder-decoder pipeline. Rather than relying on standard CNN blocks, the method injects a specific layer from a powerful foundation model‚Äîsuch as layer 15 of LLaMA3.2-1B or layer 28 of DeepSeek-R1-Distill-Qwen-1.5B‚Äîinto the architecture. Visual tokens extracted by the visual backbone (e.g., U-Net or CMUNeXt) are projected into the LLM's input space via trainable linear adapters, processed by the frozen LLM to leverage its global semantic modeling, and subsequently re-projected back to the visual space. Critically, because the LLM weights remain frozen, only the projection adapters and the visual backbone are updated, ensuring parameter efficiency while preserving the model‚Äôs foundational reasoning capabilities.
>
> **LLM4Seg achieved State-of-the-Art (SOTA) results** across multiple medical imaging benchmarks, including BUSI, TNSCUI, ISIC, Kvasir, and BTCV. When tested against standard baselines, the proposed method demonstrated consistent improvements; the UNet + LLaMA configuration achieved an average Intersection over Union (IoU) of 80.63% and an F1 score of 87.62%, while the CMUNeXt + LLaMA variant achieved an F1 score of 87.66%. These performance gains were realized with a parameter increase of only +4.19 million. Furthermore, ablation studies validated the design's efficiency: the frozen LLM approach yielded an IoU of 72.95%, slightly outperforming fully trainable LLM fine-tuning (72.63% IoU) and significantly surpassing a randomly initialized Transformer block (72.68% IoU), the latter of which required over +65 million additional parameters.
>
> The significance of this work lies in redefining LLMs not merely as text processors, but as powerful, generalizable semantic enhancers for vision tasks. By demonstrating that freezing LLM weights provides comparable or superior performance to fine-tuning, the paper establishes a blueprint for parameter-efficient model transfer that mitigates catastrophic forgetting of semantic knowledge. This approach has broad implications for the medical imaging field, suggesting that future diagnostic AI tools can leverage the reasoning capabilities of foundation models to improve accuracy across diverse clinical domains and modalities without the need for massive computational resources or extensive retraining.

---

### üîë Key Findings

*   Source text indicates that the abstract text is missing.
*   No specific key findings were extracted from the provided text.

---

### üõ†Ô∏è Technical Details

**Architecture Overview**
LLM4Seg is a hybrid segmentation framework that merges a frozen, pre-trained Large Language Model (LLM) into a traditional CNN encoder-decoder architecture.

**Model Implementation**
*   **LLM Layer:** Utilizes a specific frozen layer from large models:
    *   *LLaMA3.2-1B* (Layer 15)
    *   *DeepSeek-R1-Distill-Qwen-1.5B* (Layer 28)
*   **Integration:** Replaces the standard Transformer block with the frozen LLM layer.
*   **Adapters:** Uses linear projection adapters to connect the CNN and LLM.

**Workflow Mechanism**
1.  **Visual Token Extraction:** The CNN backbone (e.g., U-Net, CMUNeXt) extracts visual tokens.
2.  **Projection:** Tokens are projected into the LLM's input space via linear adapters.
3.  **Semantic Processing:** The frozen LLM processes tokens to enhance global modeling.
4.  **Re-projection:** Processed tokens are re-projected back to the visual space.

**Training Strategy**
*   **Parameter Efficiency:** To ensure efficiency, only the projection layers and the visual backbone are trained.
*   **Frozen Weights:** The LLM weights remain frozen throughout training to preserve pre-trained semantic knowledge.

---

### üìà Results

**Performance Benchmarks**
LLM4Seg established new State-of-the-Art (SOTA) results across multiple medical imaging datasets (BUSI, TNSCUI, ISIC, Kvasir, BTCV).

**Specific Metrics**
*   **UNet + LLaMA:**
    *   Average IoU: **80.63%**
    *   F1 Score: **87.62%**
*   **CMUNeXt + LLaMA:**
    *   Average F1 Score: **87.66%**
*   **Parameter Efficiency:** The proposed method added only **+4.19M parameters**.

**Ablation Studies**
*   **Frozen vs. Trainable:** The frozen LLM approach (IoU: 72.95%) outperformed fully trainable LLM fine-tuning (IoU: 72.63%).
*   **vs. Random Initialization:** Frozen LLM significantly outperformed a randomly initialized Transformer block, which had significantly higher parameters (+65.01M params, 72.68% IoU).

**Generalization**
Improvements were consistent across diverse modalities, including Ultrasound, Dermoscopy, and 3D CT scans.

---

### üß™ Methodology

*   *Not provided in the text.*

### ‚ú® Contributions

*   *Not provided in the text.*