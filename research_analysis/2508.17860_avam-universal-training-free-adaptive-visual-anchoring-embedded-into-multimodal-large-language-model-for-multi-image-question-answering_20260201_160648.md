# AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering

*Kang Zeng; Guojin Zhong; Jintao Cheng; Jin Yuan; Zhiyong Li*

---

> ### ðŸ“Š Quick Facts
>
> *   **Efficiency Gain:** 40%â€“70% reduction in visual tokens
> *   **Speed Improvement:** 1.2xâ€“1.8x faster inference & Time-to-Token (TTFT)
> *   **Accuracy Retention:** Within 0â€“2% variance of full-model baselines; preserves >95% accuracy with only 10â€“20% of tokens
> *   **Architecture:** Universal, training-free, plug-and-play module
> *   **Core Mechanism:** Adaptive Visual Anchoring (AVAM) via Unified Semantic Space

---

## Executive Summary

### **Problem**
The transition from Single-image to Multi-Image Visual Question Answering (MVQA) introduces a critical challenge known as visual redundancy, where the volume of visual data substantially degrades both the accuracy and efficiency of Multimodal Large Language Models (MLLMs). As MLLMs process multiple images, the sheer number of visual tokens overwhelms the context window, leading to computational bottlenecks and performance regression. Existing solutions often rely on rigid compression techniques that lack granular control over token selection and tend to generate discrete visual fragments. This fragmentation prevents the model from maintaining holistic scene comprehension.

### **Innovation**
To address these limitations, the paper proposes **Adaptive Visual Anchoring (AVAM)**, a universal, training-free module that operates as a plug-and-play inference-time component for existing MLLMs. AVAM functions by projecting visual and textual tokens into a Unified Semantic Space to establish alignment. It then employs a Query-Guided Anchoring Mechanism to dynamically identify and select high-correlation visual tokensâ€”termed "Anchors"â€”while utilizing a Redundancy Elimination Strategy to suppress low-contribution tokens. To mitigate the loss of context inherent in compression, AVAM introduces a collaborative decoding mechanism that synthesizes model outputs by balancing information from both the original global visual input and the compressed visual representation.

### **Results**
Extensive experiments across standard benchmarks including Mantis-Eval, ScienceQA, and MR-bench demonstrate that AVAM significantly enhances operational efficiency without sacrificing accuracy. When integrated into architectures such as LLaVA-NeXT and Qwen-VL-Chat, the method maintained performance within a narrow 0â€“2% variance of full-model baselines. In terms of efficiency, AVAM achieved a **40% to 70% reduction in visual token count** and accelerated inference speeds and Time-to-Token (TTFT) by factors of **1.2x to 1.8x**.

### **Impact**
The significance of AVAM lies in its provision of a flexible, universally applicable framework for MVQA that requires no model retraining or fine-tuning. By solving the issue of discrete visual fragmentation, the approach ensures that holistic visual comprehension is preserved even with aggressive token reduction. This work influences the field by offering a practical solution to the efficiency-accuracy trade-off in multimodal systems.

---

## Key Findings
*   **MVQA Challenges:** The shift from single to multi-image VQA introduces substantial visual redundancy that negatively impacts MLLM accuracy and efficiency.
*   **Limitations of Current Methods:** Existing compression techniques lack flexibility in token control and produce discrete fragments, preventing holistic comprehension.
*   **AVAM Performance:** The proposed Adaptive Visual Anchoring (AVAM) strategy achieves significant accuracy improvements through training-free adaptive compression.
*   **Broad Applicability:** Extensive experiments confirm consistent performance improvements across various MLLM architectures.

---

## Methodology
The core approach is **Adaptive Visual Anchoring (AVAM)**, a universal training-free module integrated into existing MLLMs to perform adaptive compression of visual tokens.

1.  **Adaptive Compression:** AVAM dynamically reduces the number of visual tokens to manage context window limits.
2.  **Collaborative Decoding:** To mitigate context loss caused by compression, a collaborative decoding mechanism synthesizes outputs by balancing information derived from both:
    *   The global visual input (original context).
    *   The compressed visual input (efficiency-optimized context).

---

## Technical Details
The AVAM framework addresses visual redundancy in Multi-image VQA by dynamically selecting visual tokens based on semantic relevance to the textual query.

**Key Architectural Components:**

*   **Unified Semantic Space Projection:** Aligns visual and textual tokens into a shared space to ensure compatibility and relevance assessment.
*   **Query-Guided Anchoring Mechanism:** Selects high-correlation visual tokens based on the textual query. These selected tokens serve as "Anchors" for the model's reasoning process.
*   **Redundancy Elimination Strategy:** Identifies and suppresses low-contribution tokens that add noise without semantic value.

**Operational Characteristics:**
*   **Training-Free:** The approach operates as a plug-and-play inference-time module.
*   **No Backpropagation/Fine-tuning:** It can be applied to existing models without the need for retraining or parameter updates.

---

## Results
**Benchmarks Evaluated:** Mantis-Eval, ScienceQA, MR-bench

**Accuracy Performance:**
*   **Consistency:** AVAM demonstrates competitive accuracy, maintaining performance within **0â€“2% variance** of full-model baselines.
*   **Architectures Tested:** Validated on LLaVA-NeXT and Qwen-VL-Chat.

**Efficiency Metrics:**
*   **Token Reduction:** Achieved a **40% to 70% reduction** in visual tokens.
*   **Speed Improvement:** Inference speed and Time-to-Token (TTFT) improved by a factor of **1.2x to 1.8x**.

**Ablation Studies:**
*   **Query Guidance:** Confirmed that query-guided selection significantly outperforms uniform sampling methods.
*   **Token Retention:** Using only **10â€“20%** of total tokens as anchors was sufficient to preserve over **95%** of accuracy.

---

## Contributions
*   **Universal Framework:** Introduced AVAM as a universal, plug-and-play framework for MVQA that does not require model retraining.
*   **Holistic Comprehension:** Addressed the issue of holistic visual comprehension by avoiding discrete visual fragment generation through adaptive compression.
*   **Novel Decoding:** Proposed a collaborative decoding mechanism to harmonize the trade-off between efficiency and context richness in visual data processing.

---

*Document Analysis: Quality Score 9/10 | References: 40 citations*