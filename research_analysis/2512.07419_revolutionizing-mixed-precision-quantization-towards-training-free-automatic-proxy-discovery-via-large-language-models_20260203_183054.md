---
title: 'Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic
  Proxy Discovery via Large Language Models'
arxiv_id: '2512.07419'
source_url: https://arxiv.org/abs/2512.07419
generated_at: '2026-02-03T18:30:54'
quality_score: 8
citation_count: 36
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models

*Haidong Kang; Jun Du; Lihong Lin*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Framework Name** | TAP (Training-free Automatic Proxy discovery) |
| **Core Technology** | Large Language Models (LLMs) + Direct Policy Optimization (DPO) |
| **Efficiency Gain** | >500x reduction in calibration data usage |
| **Resource Usage** | 16 calibration samples, 5 iterations |
| **Baseline Comparison** | HAWQ-V2 (8,192 samples, 50 iterations) |
| **Key Benchmarks** | ResNet-18, MobileNetV2 |
| **Quality Score** | 8/10 |

---

## ðŸ“ Executive Summary

Mixed-Precision Quantization (MPQ) is essential for deploying deep learning models on resource-constrained hardware, but identifying optimal bit-width configurations typically relies on computationally expensive differentiable search processes or labor-intensive human intuition to design "proxy" metrics. These proxies serve as surrogates to evaluate quantization performance without full re-training. The challenge lies in the high cost and complexity of discovering these proxies, which traditionally requires significant domain expertise and training overhead.

This paper addresses the need for a fully automated, training-free methodology to discover effective quantization proxies, thereby removing the bottleneck of expert-dependent design. The authors introduce **TAP**, a framework that leverages Large Language Models (LLMs) to automatically synthesize quantization proxies. The core innovation is the integration of Direct Policy Optimization (DPO) based reinforcement learning to bridge the gap between general-purpose LLMs and the specific requirements of MPQ.

The system operates via a feedback loop comprising three modules:
1.  **Proxy Candidate Generator:** Uses LLM-driven evolutionary operations to propose proxies.
2.  **Fitness Evaluator:** Validates proxies based on a weighted function of Spearman correlation and Top-1 accuracy.
3.  **RL Evolution Scheduler:** Utilizes DPO to optimize the LLM's prompts and parameters.

TAP demonstrates exceptional efficiency, achieving State-of-the-Art (SOTA) results on mainstream benchmarks while utilizing over 500x less calibration data than previous methods. This research represents a paradigm shift, moving the field from reliance on manual, differentiable methods toward automated, LLM-driven algorithm discovery.

---

## ðŸ”‘ Key Findings

*   **Affirmative solution to training-free design:** It is possible to design a quantization proxy for Mixed-Precision Quantization (MPQ) without human experts or expensive training processes.
*   **State-of-the-art Performance:** The proposed TAP framework achieves superior performance compared to existing methods on mainstream benchmarks.
*   **Efficacy of LLM-driven Optimization:** Utilizing Large Language Models (LLMs) combined with reinforcement learning effectively automates the discovery of quantization proxies, outperforming conventional strategies.
*   **Positive Feedback Loop:** The integration of Direct Policy Optimization (DPO) creates a successful feedback loop where the LLM improves its reasoning and prompt optimization over successive evolutions.

---

## ðŸ§ª Technical Details

The paper proposes **TAP (Training-free Automatic Proxy discovery)**, a framework designed to automate the design of quantization proxies for MPQ without training. It consists of a feedback loop with three distinct modules:

### 1. Proxy Candidate Generator (LLM-driven)
Synthesizes proxies using evolutionary operations conditioned on prompts and context:
*   **Initialization**
*   **Mutation**
*   **Crossover**

### 2. Fitness Evaluator
Validates proxies on benchmarks using two primary metrics:
*   **Spearman correlation:** $\rho_{sens}$
*   **Top-1 accuracy:** $Acc_{quant}$

The fitness function is formulated as:
$$ \phi(f) = \alpha \cdot \rho_{sens} + (1-\alpha) \cdot Acc_{quant} $$

### 3. RL Evolution Scheduler (DPO)
Optimizes LLM parameters based on preference pairs, establishing a mechanism where MPQ task results inform the LLM to iteratively generate superior proxies.

---

## ðŸ“ˆ Results

TAP achieves significant efficiency gains and high performance:

*   **Data Efficiency:** Requires only **16 calibration samples** and **5 iterations** compared to HAWQ-V2 (8,192 samples, 50 iterations).
*   **Cost Reduction:** Results in a **>500x reduction** in calibration data requirements.
*   **Performance:** Claims State-of-the-Art (SOTA) performance on mainstream benchmarks like **ResNet-18** and **MobileNetV2**.
*   **Generalization:** Outperforms existing expert-crafted strategies while offering better generalization across different architectures.

---

## ðŸš€ Contributions

*   **Paradigm Shift:** Reforms the MPQ design paradigm by introducing a fully automated, training-free approach that eliminates the need for labor-intensive human expert knowledge.
*   **Bridging LLMs and Quantization:** Introduces a novel application of DPO-based reinforcement learning to connect general-purpose LLMs with the specific technical demands of MPQ tasks.
*   **New Perspective:** Provides the MPQ community with a new perspective on algorithm design, demonstrating the potential of LLM-driven methodologies to solve optimization problems previously reliant on costly differentiable methods.

---

### Methodology Overview

The authors propose a discovery framework driven by Large Language Models (LLMs) to automatically find proxies tailored for MPQ. To bridge the gap between black-box LLMs and the complexity of MPQ, they employ **Direct Policy Optimization (DPO)** based reinforcement learning. The process utilizes DPO to optimize prompts, enhancing the LLM's reasoning capabilities and establishing a mechanism where MPQ task results inform the LLM to iteratively generate superior proxies.

---
**References:** 36 citations