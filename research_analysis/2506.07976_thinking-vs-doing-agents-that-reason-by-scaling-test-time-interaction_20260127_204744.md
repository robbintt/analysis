---
title: 'Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction'
arxiv_id: '2506.07976'
source_url: https://arxiv.org/abs/2506.07976
generated_at: '2026-01-27T20:47:44'
quality_score: 6
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction

*Amrith Setlur, Scaling Test, Junhong Shen, Tong Zhang, Time Interaction, Lunjun Zhang, Shengbang Tong, Diego Caples, Hao Bai, Yifei Zhou*

---

### ðŸ“Š Quick Facts
*   **Quality Score:** 6/10
*   **Total Citations:** 40
*   **Key Metric:** Interaction Horizon scaling up to **$h=30$**
*   **Primary Benchmark:** WebVoyager (Amazon, Allrecipes, Cambridge Dictionary, Huggingface)
*   **Core Mechanism:** Test-Time Interaction (TTI)

---

## Executive Summary

> This research addresses the computational bottleneck in Large Language Model (LLM)-based autonomous agents specifically designed for complex web navigation. The core issue lies in the inefficient trade-off between internal reasoning ("thinking") and environmental interaction ("doing"). Existing agents frequently suffer from resource misallocation, either "overthinking" by generating excessive, inefficient Chain-of-Thought (CoT) sequences without taking action, or failing to sufficiently explore the environment.
>
> The authors propose **Test-Time Interaction (TTI)**, a framework that scales agent performance by systematically increasing interactions with the environment during the inference phase. Technically, the architecture utilizes a multiplicative curriculum schedule during training to adjust the interaction horizon, scaling up to a maximum of **$h=30$ steps**. Unlike static models, TTI learns a dynamic trade-off, reducing the number of reasoning tokens per step ("Short CoT") in favor of increased physical trajectory length.
>
> Experiments conducted on the **WebVoyager benchmark validation set** demonstrate that during the high-horizon phase ($h=30$), the agent significantly increases trajectory length and utilizes exploration actions (*'GoBack'*, *'Bing'*) with much higher frequency. Compared to fixed-horizon baselines, TTI achieves superior task success rates while maintaining significantly shorter Chain-of-Thought outputs. The significance of this work lies in its challenge to the assumption that increased internal reasoning invariably leads to better intelligence; instead, reallocating compute from token generation to environment interaction yields measurable gains in complex navigation tasks.

---

## Key Findings

*   The analysis of the provided text indicates that the original abstract section was empty and missing from the request.
*   Consequently, the direct findings presented in the abstract could not be extracted from the input segment.
*   Detailed findings are instead derived from the methodology and results sections provided in the analysis.

## Methodology

*   The provided text does not contain a detailed description of the research methodology within the "Methodology" section.
*   The source text requested the user to paste the full text of the abstract to proceed with a standard analysis.
*   Technical implementation details were found in the "Technical Details" and "Results" sections rather than the methodology segment.

## Contributions

*   The provided text lists no specific contributions in this section, as it functions as a system message.
*   The input text serves as an alert indicating that the input abstract text required for contribution extraction was missing.
*   The core contributions (e.g., the TTI framework) are detailed in the Executive Summary and Technical Details sections.

---

## Technical Details

The paper proposes **TTI (Test-Time Interaction)**, a framework designed to scale performance by increasing interactions during the test phase.

*   **Curriculum Learning Schedule**:
    *   Utilizes a **multiplicative schedule** to adjust the interaction horizon.
    *   Scales the interaction horizon ($h$) up to a maximum of **30 steps**.

*   **Action Space**:
    *   The agent operates using **discrete actions**.
    *   **'GoBack'**: Used specifically for retries and error recovery.
    *   **'Bing'**: Used for external searches to retrieve missing information.

*   **Compute Tradeoff**:
    *   The architecture learns to balance **trajectory length** against **per-step compute**.
    *   **Strategy**: Increases the number of steps while reducing the number of tokens generated per step.
    *   **Objective**: To prevent "overthinking" and avoid excessive internal monologue.

---

## Results

Experiments were performed on the **WebVoyager benchmark validation set** (specifically on Amazon, Allrecipes, Cambridge Dictionary, and Huggingface).

*   **High-Horizon Performance**:
    *   TTI increases trajectory length during the high-horizon phase ($h=30$).
    *   There is a marked increase in the utilization of exploration actions (`GoBack`, `Bing`).
    *   This increased exploration correlates directly with **improved task success rates**.

*   **Comparison with Fixed Horizon**:
    *   **TTI vs. Fixed Horizon**: TTI achieves significantly **shorter Chain-of-Thought** outputs (fewer tokens per step).
    *   **Curriculum Necessity**: Results indicate that simply allowing a long horizon without a curriculum structure is counterproductive.
    *   **Performance**: TTI demonstrates better performance than fixed horizon runs, validating the "doing over thinking" approach in complex navigation.

---

**Analysis Rating:** 6/10  
**Reference Count:** 40 citations