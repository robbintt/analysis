# Bayesian continual learning and forgetting in neural networks

*Djohan Bonnet; Kellian Cottart; Tifenn Hirtzlin; Tarcisius Januel; Thomas Dalgaty; Elisa Vianello; Damien Querlioz*

***

### ðŸ“‹ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Method Name** | Metaplasticity from Synaptic Uncertainty (MESU) |
| **Key Benchmark** | Permuted MNIST (200 Tasks) |
| **Top Accuracy** | **91.37%** (Last 5 Tasks) |
| **OOD ROC AUC** | **0.95** |
| **Approach** | Task-Agnostic / Streaming Capable |
| **Quality Score** | 9/10 |

***

## Executive Summary

Continual learning in neural networks faces the persistent challenge of "catastrophic forgetting," where learning new information erases previously established knowledge. Furthermore, standard approaches typically rely on explicit task boundaries during training, limiting their utility in real-world scenarios where data arrives in a continuous, unlabeled stream. This paper addresses the critical need for a framework that balances **stability**â€”retaining critical past knowledgeâ€”with **plasticity**â€”the ability to integrate new informationâ€”without prior knowledge of task transitions.

The authors introduce **Metaplasticity from Synaptic Uncertainty (MESU)**, a novel Bayesian framework that unifies biological metaplasticity, Bayesian inference, and Hessian-based regularization. MESU models synaptic weights as distributions characterized by a mean ($\mu$) and variance ($\sigma^2$), utilizing a distinct learning rule where parameter updates are scaled by the weight's own uncertainty.

This mechanism creates a local, per-synapse learning rate: weights with high uncertainty become highly plastic to accommodate new data, while weights with low uncertainty achieve high stability to preserve core knowledge. Crucially, the framework mitigates "catastrophic remembering" and facilitates the gradual release of outdated information by employing a Finite Posterior Window.

On benchmarks, MESU significantly outperforms established methods like EWC Online and SI. It sets a new standard for balancing stability and plasticity, offering a viable pathway toward perpetual learning in dynamic AI systems.

***

## Key Findings

*   **Superior Benchmark Performance:**
    *   On **Permuted MNIST (200 Tasks)**, MESU achieved **91.37%** accuracy on the final five tasks, significantly outperforming EWC Online (88.50%), SI (86.96%), and the baseline (55.36%).
    *   Demonstrated **exceptional Memory Rigidity Resilience (Rt)** with a score of **233.27** compared to 3.82 for FOO-VB.
*   **Balanced Plasticity and Stability:**
    *   Successfully balanced plasticity and stability to mitigate catastrophic forgetting while retaining the ability to learn new tasks.
    *   Outperformed conventional techniques on **CIFAR-100** by effectively adapting to streaming data without explicit task boundaries.
*   **Robust Uncertainty Estimation:**
    *   Provided reliable epistemic uncertainty estimates, enabling effective **Out-of-Distribution (OOD) data detection** with an ROC AUC of **0.95**.
    *   Demonstrated distinct separation in epistemic uncertainty for OOD detection, whereas standard SGD showed overlapping aleatoric uncertainty.

***

## Technical Specifications

**Method Name:** Metaplasticity from Synaptic Uncertainty (MESU)

**Core Mechanism:**
*   A **Variational inference approach** for Bayesian Neural Networks (BNNs).
*   Updates synaptic parameters (mean $\mu$ and variance $\sigma^2$) using a distinct learning rule.

**Learning Rule:**
*   Updates are scaled by the parameter's own variance ($\sigma^2$), creating a local, per-synapse learning rate.
*   Formula:
    $$ \Delta\mu = -\sigma^2_{t-1} \frac{\partial C_t}{\partial \mu_{t-1}} + \sigma^2_{t-1} \left(\frac{1}{N \sigma^2_{prior}}\right) (\mu_{prior} - \mu_{t-1}) $$

**Adaptation Strategy:**
*   **High Plasticity:** Weights with high uncertainty (high $\sigma^2$) are updated readily.
*   **High Stability:** Weights with low uncertainty (low $\sigma^2$) are stabilized.

**Forgetting Mechanism:**
*   Utilizes a **Finite Posterior Window** that retains a fixed number of past tasks ($N$) in the effective posterior to mitigate catastrophic remembering.

**Data Processing:**
*   **Streaming Data:** Compatible with streaming data by treating incoming mini-batches as distinct tasks (Task-Agnostic).

**Inference & Optimization:**
*   Uses the **reparameterization trick**: $\omega = \mu + \epsilon \times \sigma$.
*   Conceptually linked to **second-order optimization algorithms** like Newton's method.

***

## Research Contributions

1.  **Introduction of MESU:**
    A biologically-inspired, Bayesian continual learning framework that models uncertainty to regulate parameter updates, directly addressing both catastrophic forgetting and remembering.

2.  **Task-Agnostic Continual Learning:**
    A solution that removes the dependency on explicit task boundaries, allowing for seamless adaptation to streaming data scenarios.

3.  **Theoretical Unification:**
    A synthesis of ideas from biological metaplasticity, Bayesian inference, and Hessian-based regularization to provide a theoretical pathway toward perpetual learning.

***

## Experimental Results

**Permuted MNIST (200 Tasks)**
*   **MESU Accuracy:** 91.37% (Last 5 Tasks)
*   **Comparisons:**
    *   EWC Online: 88.50%
    *   SI: 86.96%
    *   FOO-VB Diagonal: 68.66%
    *   Baseline: 55.36%
*   **Memory Rigidity Resilience (Rt):** 233.27 (MESU) vs. 3.82 (FOO-VB).

**Domain Incremental Learning (Animal Classification)**
*   MESU maintained performance across **4 tasks**.
*   The SGD baseline showed significant catastrophic forgetting.
*   MESU demonstrated distinct separation in epistemic uncertainty for OOD detection.

**CIFAR-10 / CIFAR-100 (Task Incremental)**
*   MESU demonstrated superior average accuracy and sustained higher accuracy curves across **11 tasks**.
*   Outperformed EWC, SI, and the Adam baseline.

***

**Quality Score:** 9/10  
**References:** 38 citations