# Attribution-Guided Distillation of Matryoshka Sparse Autoencoders
*Cristina P. Martin-Linares; Jonathan P. Ling*

---

> ### üìä Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Target Model** | Gemma-2-2B (Layer 12) |
> | **Core Extracted** | 197 Features |
> | **Training Cycles** | 7 Iterations |
> | **Evaluation** | SAEBench Metrics |
> | **Quality Score** | 9/10 |
> | **References** | 34 Citations |

---

## üìù Executive Summary

Standard Sparse Autoencoders (SAEs) are critical tools for mechanistic interpretability, yet they suffer from significant instability regarding feature extraction. Specifically, SAEs often exhibit high run-to-run variability and feature redundancy, where training the model multiple times or at different sparsity levels yields inconsistent sets of latent features. This instability complicates the analysis of neural networks and hinders the development of robust, transferable dictionaries of monosemantic features.

To address these limitations, this paper introduces **DMSAE** (Attribution-Guided Distillation of Matryoshka Sparse Autoencoders), a novel training pipeline designed to extract a compact, reusable core of features. The method utilizes a Matryoshka architecture to enable nested reconstructions at varying sparsity levels, distinguishing itself by employing **"Gradient X activation"** for attribution-guided selection rather than relying solely on activation sparsity. This technique quantifies each feature's specific contribution to next-token loss, allowing the algorithm to iteratively prune less critical latents.

In experiments conducted on the residual stream activations of Gemma-2-2B at Layer 12 over seven training cycles, the DMSAE pipeline successfully distilled a stable core of **197 features** that were consistently selected across runs. This small subset demonstrated remarkable efficiency, explaining a significant fraction of the attribution for next-token loss in the most nested reconstruction. Models trained using this distilled core outperformed baseline approaches on several SAEBench metrics. This research validates that consistent, monosemantic feature sets can be effectively transferred across different sparsity levels, resolving the long-standing challenge of feature redundancy and run-to-run variability.

---

## üîë Key Findings

*   **Stable Core Distillation:** The DMSAE pipeline successfully distilled a stable core of **197 features** that were repeatedly selected across seven training cycles on Gemma-2-2B layer 12 residual stream activations.
*   **Performance Improvement:** Models trained using the distilled core demonstrated improved performance on several **SAEBench metrics** compared to baseline approaches.
*   **Cross-Sparsity Transfer:** The research validates that consistent sets of latent features can be effectively transferred across different sparsity levels, resolving issues of feature redundancy and run-to-run variability.
*   **High Efficiency:** A remarkably small subset of features was sufficient to explain a significant fraction of the attribution for next-token loss in the most nested reconstruction.

---

## üî¨ Methodology

The DMSAE pipeline employs a rigorous cyclic process to ensure feature stability and importance:

1.  **Iterative Distillation Cycle**
    The method employs a cyclic training process involving a Matryoshka SAE to progressively refine the feature set.

2.  **Attribution-Guided Selection**
    Feature importance is quantified using **'Gradient X activation'** to measure each feature's specific contribution to next-token loss, ensuring that only impactful features are retained.

3.  **Pruning Strategy**
    The algorithm retains only the smallest subset of features necessary to explain a fixed fraction of the total attribution, optimizing for compactness.

4.  **Weight Transfer Protocol**
    Across cycles, only the core encoder weight vectors are preserved and transferred; the core decoder and all non-core latents are reinitialized from scratch each cycle to prevent redundancy accumulation.

---

## ‚öôÔ∏è Technical Details

*   **Architecture:** Utilizes a **Matryoshka structure** for nested reconstructions at varying sparsity levels.
*   **Selection Mechanism:** Employs attribution-guided selection to iteratively distill a stable core of features based on attribution contribution.
*   **Target Layer:** Operates specifically on **residual stream activations**.
*   **Core Objective:** To transfer consistent latent features across sparsity levels while maintaining high reconstruction fidelity.

---

## üöÄ Contributions

*   **Introduction of DMSAEs:** A novel training pipeline designed to extract a compact, reusable core of monosemantic features from Sparse Autoencoders.
*   **Solution to Feature Instability:** This work directly addresses the limitation in standard SAEs where features are redundant and difficult to transfer between runs or sparsity levels.
*   **Attribution-Based Pruning:** The use of gradient-based attribution (rather than just activation sparsity) for feature selection provides a robust mechanism for identifying features critical to model performance.

---

## üìà Results

Experiments conducted on **Gemma-2-2B (Layer 12)** over 7 training cycles identified a stable core of 197 features that were repeatedly selected, demonstrating high reproducibility. Key outcomes include:

*   **Metric Improvement:** Models trained on this core showed improved SAEBench metrics compared to baselines.
*   **Attribution Efficiency:** The small subset explained a significant fraction of attribution for next-token loss in the most nested reconstruction.
*   **Stability:** The approach successfully resolved feature redundancy and run-to-run variability, proving the viability of transferable feature dictionaries.

---
*References: 34 citations*