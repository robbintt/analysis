# Polychromic Objectives for Reinforcement Learning
***Jubayer Ibn Hamid; Ifdita Hasan Orney; Ellen Xu; Chelsea Finn; Dorsa Sadigh***

---

### üìã Quick Facts
| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Core Algorithm** | Polychromic PPO |
| **Primary Environments** | BabyAI, Minigrid, Algorithmic Creativity |
| **Key Objective** | Mitigate mode collapse and enhance diversity |

---

## üìÑ Executive Summary

Reinforcement Learning Fine-Tuning (RLFT) is currently hindered by a critical failure mode known as **mode collapse**, where policies converge to a narrow set of high-reward outputs at the expense of behavioral diversity. This homogeneity severely limits a model's ability to generalize to complex environment configurations or large perturbations during testing. Furthermore, it fundamentally undermines the utility of test-time compute scaling; without a diverse repertoire of behaviors to explore and refine during inference, agents cannot effectively leverage additional compute to solve difficult problems.

To address this, the authors introduce **"Set Reinforcement Learning" (Set RL)**, a paradigm that optimizes a novel "polychromic objective" over a set of trajectories rather than a solitary path. The objective function mathematically combines average reward with a state-space diversity metric using the formula $f_{poly}(s, \tau_{1:n}) := \frac{1}{n} \sum_{i=1}^n R(\tau_i) \cdot d(s, \tau_{1:n})$, forcing the policy to jointly maximize success and coverage.

This is operationalized through **Polychromic Proximal Policy Optimization (PPO)**, which modifies standard PPO via Vine Sampling for efficient on-policy rollout collection, adjusts the advantage function to distribute learning signals across the trajectory set, and incorporates a per-state KL penalty to maintain training stability.

The method was rigorously validated across three distinct domains‚ÄîBabyAI, Minigrid, and Algorithmic Creativity‚Äîusing Pass@$k$ and distinct mode counts as primary metrics. The polychromic approach demonstrated superior coverage, achieving a Pass@32 score of approximately **0.96** in the Minigrid FourRooms environment (nearly doubling the performance of standard PPO). By shifting the optimization focus from single-trajectory rewards to set-level diversity, this research provides practitioners with a viable path to build more adaptable, exploratory, and robust generative agents capable of leveraging increased inference compute.

---

## üîç Key Findings

*   **Mitigation of Mode Collapse:** The polychromic objective effectively counters the common failure mode in RLFT where policies lose diversity and converge to a limited set of exploitable outputs.
*   **Superior Generalization:** Policies optimized with this method demonstrate better generalization capabilities under large perturbations and improved success rates in solving complex environment configurations.
*   **Enhanced Coverage in Multi-Attempt Scenarios:** In pass@$k$ experiments, the method achieves substantially higher coverage by maintaining and exploiting a diverse repertoire of strategies.
*   **Performance Across Domains:** The approach was validated successfully across three distinct environments: **BabyAI**, **Minigrid**, and **Algorithmic Creativity**.

---

## üõ†Ô∏è Methodology

The authors introduce a 'polychromic objective' for policy gradient methods, explicitly designed to enforce the exploration and refinement of diverse generations rather than converging to a single optimal path. The method adapts **Proximal Policy Optimization (PPO)** through two key technical modifications:

1.  **Vine Sampling:** Used to collect on-policy rollouts for structured trajectory collection.
2.  **Advantage Function Modification:** The advantage function is altered to reflect the advantage specifically under the new polychromic objective.

---

## ‚öôÔ∏è Technical Details

The proposal centers around **Set Reinforcement Learning (Set RL)**, a paradigm that optimizes an objective over a set of trajectories rather than a single trajectory.

### The Polychromic Objective
The objective, **Polychromic Functions**, jointly maximizes reward and diversity using the following formula:

$$f_{poly}(s, \tau_{1:n}) := \frac{1}{n} \sum_{i=1}^n R(\tau_i) \cdot d(s, \tau_{1:n})$$

### Algorithm: Polychromic PPO
*   **Basis:** Modifies Proximal Policy Optimization (PPO).
*   **Learning Signal:** Uses a shared learning signal (advantage) across trajectories.
*   **Sampling:** Utilizes Vine Sampling to efficiently estimate set-level values.
*   **Stability:** Adds a per-state KL penalty to ensure training stability.

---

## ‚úÖ Contributions

*   **Novel Objective Function:** A new theoretical objective ('polychromic') that specifically targets the preservation of behavioral diversity in RL fine-tuning, addressing a critical limitation of current paradigms.
*   **Enhanced Test-Time Compute Scaling:** By preventing convergence to a few outputs, the method facilitates better exploration, which is identified as essential for amplifying the benefits of test-time compute scaling.
*   **Practical Implementation:** The research provides a concrete adaptation of the widely used PPO algorithm to implement this objective, offering a viable path for practitioners to improve the robustness and versatility of fine-tuned policies.

---

## üìä Results

The primary evaluation metric is **Pass@$k$**, measuring the probability of at least one successful trajectory out of $k$.

*   **Minigrid FourRooms:** Achieved a Pass@32 score of **~0.96**, compared to the baseline PPO which plateaued near **0.60**.
*   **BabyAI KeyDoor:** Achieved a Pass@32 of **0.90**, compared to **0.50** for the baseline.
*   **Algorithmic Creativity:** Identified significantly more distinct successful behaviors (nearly **400** distinct modes vs. roughly **150** for MaxEnt PPO).
*   **Generalization:** The method successfully mitigates entropy collapse and demonstrates better generalization to unseen tasks and large perturbations.

---
*Report generated based on 40 citations and a Quality Score of 8/10.*