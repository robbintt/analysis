---
title: 'ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models
  through Agentic Data Synthesis'
arxiv_id: '2509.23652'
source_url: https://arxiv.org/abs/2509.23652
generated_at: '2026-02-03T12:32:39'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis

*Congzhi Zhang; Zhibin Wang; Yinchao Ma; Jiawei Peng; Yihan Wang; Qiang Zhou; Jun Song; Bo Zheng*

---

> ### ðŸ“Š Quick Facts
>
> *   **Dataset Scale:** 10k descriptions, 170k QA pairs, 135k reasoning traces
> *   **Evaluation Context:** 192 frames per video across 5 benchmarks
> *   **Key Innovation:** Multi-Agent ReAct framework & Observation & Reasoning (O&R) reward
> *   **Performance:** State-of-the-art (SOTA) average performance; up to 50% accuracy improvement on specific benchmarks
> *   **Training Strategy:** Supervised Fine-Tuning (SFT) + Reinforcement Learning with Verifiable Reward (RLVR)

---

## Executive Summary

Current Large Vision-Language Models (LVLMs) face a critical data bottleneck that hinders complex video reasoning capabilities. Specifically, existing datasets lack sufficient multi-hop questions and video-grounded Chain-of-Thought (CoT) data, forcing models to rely on static image priors or textual biases rather than actual video content. This deficiency leads to frequent hallucinations and an inability to perform multi-step temporal reasoning tasks such as tracking state changes, understanding causality, or counting events.

The core innovation of **ReWatch-R1** is a data-centric approach combining a **Multi-Agent ReAct synthesis framework** with a unique reinforcement learning strategy. The authors developed a three-stage pipeline to construct the **ReWatch Dataset**:
1.  **Hierarchical video captioning**
2.  **High-difficulty QA generation** with bias filtering
3.  **Agentic CoT synthesis**

In the synthesis stage, a "Reasoner" agent generates hypotheses while an "Observer" agent verifies them against video frames, simulating a human-like re-watching process to ensure grounding. For model training, ReWatch-R1 utilizes Supervised Fine-Tuning (SFT) followed by **Reinforcement Learning with Verifiable Reward (RLVR)**. This RLVR employs a custom Observation & Reasoning (O&R) reward mechanism that evaluates not only answer accuracy but also the fidelity of the reasoning trace to the visual observations, effectively penalizing hallucinations.

ReWatch-R1 achieves state-of-the-art (SOTA) average performance across five challenging video reasoning benchmarks, significantly outperforming strong baselines such as VideoRFT, Video-Chat-R1, and Qwen2.5-VL-7B. Qualitative analysis confirms the modelâ€™s enhanced ability to handle complex temporal tasks while substantially reducing hallucination rates. This research represents a significant paradigm shift, demonstrating that complex reasoning can be unlocked primarily through high-quality, agentic data synthesis rather than architectural changes alone.

---

## Key Findings

*   **State-of-the-Art Performance:** ReWatch-R1 achieves SOTA average performance across five challenging video reasoning benchmarks.
*   **Data Bottleneck Resolution:** The study successfully addresses the lack of multi-hop questions and video-grounded Chain-of-Thought (CoT) data in current datasets.
*   **Hallucination Reduction:** The Observation & Reasoning (O&R) reward mechanism successfully reduces hallucinations by strictly aligning reasoning traces with actual video content.
*   **Agentic Synthesis:** The Multi-Agent ReAct framework effectively generates high-quality reasoning traces by simulating a human re-watching process.

---

## Methodology

The research follows a dual-track methodology focusing on data synthesis and model training.

### 1. Data Construction
*   **ReWatch Dataset Creation:** Utilization of a multi-stage synthesis pipeline.
*   **Multi-Agent ReAct Framework:** Implementation of a system to simulate human-like re-watching for generating video-grounded reasoning traces.

### 2. Model Development
*   **Baseline Training:** Post-training a baseline Large Vision-Language Model (LVLM) via Supervised Fine-Tuning (SFT) on the ReWatch dataset.
*   **Reinforcement Learning:** Application of Reinforcement Learning with Verifiable Reward (RLVR), utilizing a custom Observation & Reasoning (O&R) reward mechanism to refine the model's reasoning capabilities.

---

## Technical Details

The ReWatch-R1 framework utilizes a data-centric approach featuring a **three-stage data construction pipeline** and a **two-stage post-training process**.

### Data Pipeline
*   **Stage 1 (ReWatch-Caption-10k):** Hierarchical Video Captioning using dynamic frame-rates and timestamp realignment.
*   **Stage 2 (ReWatch-QA-170k):** High-Difficulty QA Generation via contrastive prompting and three-layer filtering:
    *   Answer verification
    *   Text bias elimination
    *   Summary bias elimination
*   **Stage 3 (ReWatch-CoT-135k):** Multi-Agent Chain-of-Thought Synthesis using a ReAct framework involving Reasoner and Observer agents.

### Post-Training Process
*   **Supervised Fine-Tuning (SFT):** Initial training on the synthesized ReWatch dataset.
*   **Reinforcement Learning with Verifiable Reward (RLVR):** Employs a composite reward mechanism to refine textual CoT simulations based on:
    *   **Reasoning**
    *   **Observation**
    *   **Accuracy**
    *   **Format**

---

## Core Contributions

*   **ReWatch Dataset:** Introduction of a large-scale dataset containing multi-hop questions and video-grounded CoT data.
*   **Multi-Agent ReAct Synthesis Framework:** A novel framework for generating grounded reasoning traces by mimicking human retrieval and verification behaviors.
*   **ReWatch-R1 Architecture:** A robust LVLM specifically tailored for complex video reasoning tasks.
*   **Observation & Reasoning (O&R) Reward:** A new reward mechanism that extends evaluation to include reasoning process fidelity to combat hallucinations.

---

## Results & Performance

*   **Dataset Composition:** The final ReWatch dataset comprises 10k video descriptions, 170k QA pairs, and 135k reasoning traces.
*   **Benchmark Dominance:** Evaluated using 192 frames across five benchmarks, ReWatch-R1 outperforms baselines like VideoRFT, Video-Chat-R1, and Qwen2.5-VL-7B.
*   **Accuracy Gains:** The model demonstrates the highest accuracy among tested models, with performance improvements scaling up to **50%** in specific benchmarks.
*   **Qualitative Improvements:** Significant enhancement in multi-step temporal reasoning tasks such as causality, state tracking, and counting, alongside a verified reduction in hallucinations.

---

**Quality Score:** 8/10  
**References:** 40 citations