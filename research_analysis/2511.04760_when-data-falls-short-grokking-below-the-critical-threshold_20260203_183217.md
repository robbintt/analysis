---
title: 'When Data Falls Short: Grokking Below the Critical Threshold'
arxiv_id: '2511.0476'
source_url: https://arxiv.org/abs/2511.04760
generated_at: '2026-02-03T18:32:17'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# When Data Falls Short: Grokking Below the Critical Threshold
*Vaibhav Singh; Eugene Belilovsky; Rahaf Aljundi*

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Data Efficiency** | Works with only 10% of data |
> | **Critical Threshold** | ~25% of full dataset |
> | **Model Architecture** | 1-layer decoder-only Transformer |
> | **Training Acceleration** | Reduced grok time from 10k to 6k iterations |
> | **Hardware** | NVIDIA V100 |

---

## Executive Summary

This research addresses the "below threshold" problem in the context of grokking, a phenomenon where models suddenly generalize after prolonged overfitting. Traditionally, grokking requires a critical mass of training dataâ€”often the entire datasetâ€”to transition from memorizing training samples to learning the underlying algorithm. In real-world scenarios, data is frequently scarce or subject to distribution shifts, making standard grokking impractical. The authors investigate how to achieve reliable generalization when available data falls significantly below this critical threshold, a challenge that hinders the application of algorithmic learning in low-resource and continually evolving environments.

The key innovation is the induction of grokking via Knowledge Distillation (KD) in data-scarce regimes. The authors propose a two-phase distillation framework using a 1-layer decoder-only Transformer. In the first phase, a teacher model is trained to grok a source distribution ($p_1$). In the second phase, a student model is trained on a target distribution ($p_2$) using limited data by minimizing a composite loss function: a weighted combination of standard Cross Entropy and KL Divergence against the teacher's probability outputs. To enhance numerical stability, the method employs StableMax Cross Entropy rather than standard LogSoftmax. Crucially, the experimental scope extends beyond simple transfer to include **joint distribution training** and **continual pretraining**, demonstrating that the student can leverage the inductive biases of a grokked teacher to generalize across varied data configurations.

The study establishes the critical data threshold for generalization at approximately 25% of the full dataset. While standard supervised training fails below this threshold, the proposed KD method enables the student model to achieve grokking using only 25% of the data, with success demonstrated even at extreme scarcity levels of 10%. Notably, in **joint distribution training** experiments, KD significantly outperformed standard supervised training, effectively leveraging individually grokked models to handle combined data distributions. The method also proves effective in **continual pretraining** scenarios, where the student learns new distributions while mitigating catastrophic forgetting through the teacher's regularization signal. Furthermore, the use of StableMax Cross Entropy reduced the time to grok from approximately 10,000 iterations to 6,000 iterations, and experiments refuted the hypothesis that strict weight decay is necessary for grokking, as the KD approach succeeded with zero weight decay even as weight norms increased.

This work provides a critical solution to the data efficiency bottleneck in algorithmic learning, offering both theoretical and practical advancements. By demonstrating that grokking can be induced below the critical data threshold and outperform standard methods on joint distributions, the authors establish a viable framework for continual pretraining in low-resource environments. The findings challenge existing assumptions regarding the mechanics of grokkingâ€”specifically the roles of weight decay and norm dynamicsâ€”and provide a practical pathway for deploying robust models in environments characterized by limited data and evolving distributions.

---

## Key Findings

*   **Induction of Grokking via Knowledge Distillation:** Allows a student model to grok on a target distribution using data significantly below the critical threshold.
*   **Superiority on Joint Distributions:** KD outperforms standard supervised training on joint distributions by effectively leveraging individually grokked models.
*   **Continual Pretraining Benefits:** In continual pretraining scenarios, KD accelerates generalization on new distributions while effectively mitigating catastrophic forgetting.
*   **High Data Efficiency:** The method demonstrates robust performance working with only **10%** of the data.

---

## Methodology

The research investigates grokking in data-scarce regimes and distribution shifts by comparing standard supervised training against Knowledge Distillation (KD). The study employs three primary experimental setups:

1.  **Cross-distribution transfer**
2.  **Joint distribution training**
3.  **Continual pretraining**

---

## Technical Details

### Model Architecture & Task
*   **Architecture:** 1-layer decoder-only Transformer.
*   **Task:** Algorithmic operations of the form $((a \text{ @ } b) \% P)$, specifically focusing on modular addition and subtraction.
*   **Input/Output:** Input sequence formatted as $[a, b, @, P]$, with the output extracted from the final token.

### Distribution Shift Simulation
*   Shift is simulated by altering the modulus $P$.
*   Example transition: $P_1=113$ to $P_2=107$.

### Training Process: Two-Phase Distillation
1.  **Teacher Training:** A teacher model ($f_T$) is trained to 'grok' distribution $p_1$.
2.  **Student Training:** A student model ($f_S$) is trained on distribution $p_2$ using limited data via Knowledge Distillation (KD), targeting the probability outputs of the operator token.

### Loss Functions & Optimization
*   **StableMax Cross Entropy ($L_{StCE}$):** Used to ensure numerical stability.
    *   Defined by $g(x) = \log(x+1)$ if $x \geq 0$
    *   $g(x) = -\log(-x+1)$ if $x < 0$
*   **Total Objective:** Weighted combination of Cross Entropy and KL Divergence:
    $$L(\theta) = (1-\alpha)L_{CE}(\theta) + \alpha L_{KL}(\theta)$$
*   **Optimization Configurations:**
    *   Comparisons made between **Adam** (no weight decay) and **AdamW** (weight decay $\lambda=1$).
    *   Learning Rate: $1e^{-3}$
    *   Batch Size: 2048
    *   Hardware: NVIDIA V100

---

## Results

*   **Threshold Identification:** The critical data threshold for generalization is identified at approximately **25%** of the full dataset.
*   **Sub-threshold Success:** Using KD from a grokked teacher, the student model achieves generalization on $p_2$ with only 25% of the data (potentially as low as 10%), whereas standard training from scratch fails.
*   **Speed Efficiency:** StableMax Cross Entropy reduces the time to grok from approx. 10,000 iterations to **6,000 iterations**.
*   **Weight Decay Insights:** Experiments refuted the dependency on weight decay for grokking. Adam (zero weight decay) combined with KD succeeded, whereas Adam alone failed.
*   **Norm Dynamics:** Grokking occurs even with increasing weight norms, countering theories that strictly decreasing norms are required.
*   **Benchmarks:** Conducted on modular addition ($P=113$) and subtraction ($P=107$) over 15,000â€“30,000 epochs.

---

## Contributions

*   **Solves the 'Below Threshold' Problem:** Provides a practical solution to observe and achieve grokking in data-scarce environments.
*   **Theoretical Insights:** Offers new theoretical and empirical insights into the mechanics of grokking and knowledge transfer.
*   **Practical Framework:** Establishes a framework using KD for generalization in real-world applications involving limited data and evolving distributions.