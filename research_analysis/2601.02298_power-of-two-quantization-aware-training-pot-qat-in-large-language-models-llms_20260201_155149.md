# Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)

*Mahmoud Elgenedy*

---

> ### **Quick Facts**
>
> *   **Memory Efficiency:** Estimated **87.5% reduction** in memory requirements.
> *   **Inference Speed:** Expected **3-10x speedup** via bit shifting vs. floating-point multiplication.
> *   **Accuracy Recovery:** **66% perplexity enhancement** compared to standard PTQ; BERT-Score loss limited to **1%**.
> *   **Evaluated Model:** GPT-2 (124M parameters) on OpenWebText.
> *   **Quality Score:** 9/10

---

## Executive Summary

The exponential growth in parameter count for Large Language Models (LLMs), often reaching trillions, presents a critical barrier to deploying these models on resource-constrained Edge devices. Standard LLM inference requires substantial memory bandwidth and high-performance floating-point compute capabilities, which are typically absent in embedded and mobile environments.

This research addresses the urgent need to reconcile the massive computational footprint of modern LLMs with the severe hardware limitations of Edge devices, aiming to enable on-device AI without the prohibitive costs of cloud-based processing. The paper introduces a **Power-of-Two Quantization-Aware-Training (PoT-QAT)** framework that fundamentally alters model arithmetic to maximize hardware efficiency.

The core innovation lies in strictly constraining weight values to powers of two ($2^n$), allowing the system to represent weights as simple exponents. This architectural shift enables the replacement of computationally expensive floating-point multiplication operations with low-cost bit shifting during inference. To recover the accuracy inevitably lost during such aggressive quantization, the authors employ a Quantization-Aware Training (QAT) pipeline using PyTorch 2’s Export Mode. This process utilizes "fake quantize" modules to simulate noise and the Straight-Through Estimator (STE) to handle the non-differentiability of the logarithmic rounding operations, fine-tuning the model to thrive within the strict PoT constraints.

Evaluation on the GPT-2 (124M) model using the OpenWebText dataset demonstrates that the proposed method achieves substantial efficiency gains with recoverable accuracy. The approach realizes an estimated **87.5% reduction in memory requirements** and a projected **3-10x inference speedup** over full-precision models. While standard Post-Training Quantization (PTQ) caused perplexity to degrade significantly—rising from a baseline of 23.73 to 90.0—the PoT-QAT method successfully recovered performance, achieving a perplexity of 30.47.

This work establishes a significant precedent for the feasibility of running GPT-scale and larger models on Edge hardware, validating that strict Power-of-Two quantization can be paired with QAT to maintain model integrity using integer bit-shifting arithmetic rather than floating-point multiplication.

---

## Key Findings

*   **Significant Memory Efficiency:** The Power-of-Two (PoT) quantization approach results in an estimated memory saving of **87.5%**, as the model stores only exponents rather than full-precision weights.
*   **Inference Speedup:** By replacing multiplication operations with low-cost bit shifting, the method achieves an expected **3-10x speedup** in inference compared to full-precision models.
*   **Recovered Performance via QAT:** While strict PoT quantization typically degrades performance, the application of Quantization Aware Training (QAT) recovered the majority of accuracy. This resulted in a **perplexity enhancement of 66%** and limited BERT-Score loss to just **1%** against the GPT-2 baseline.
*   **Edge Feasibility:** The study demonstrates that the exponential growth of LLM parameters (up to trillions) can be effectively managed to run on resource-constrained Edge devices through aggressive compression and efficient arithmetic.

---

## Methodology

The research employs a two-pronged technical strategy to enable Large Language Model execution on Edge devices:

1.  **Power-of-Two (PoT) Quantization:**
    The weight parameters are strictly quantized to powers of two ($2^n$). This allows the system to represent weights using only their exponents, drastically reducing storage requirements.

2.  **Algorithmic Optimization:**
    The method leverages the properties of PoT numbers to replace computationally expensive multiplication operations with low-cost bit shifting during processing.

3.  **Quantization Aware Training (QAT):**
    To counter the accuracy loss inherent in such strict quantization, the model undergoes additional training. This fine-tuning process adjusts the weights to optimize performance within the constraints of the PoT format.

4.  **Validation:**
    The methodology was evaluated using the GPT-2 124M model, measuring performance against baseline metrics using perplexity and BERT-Score.

---

## Technical Details

### Quantization Schemes
The approach utilizes distinct quantization strategies for activations and weights to optimize hardware efficiency:

*   **Activations:** Quantized using **Uniform (Affine) quantization**:
    $$y = \text{round}(x/\text{scale}) + \text{zp}$$
*   **Weights:** Quantized using **Power-of-Two (PoT) quantization**:
    $$y = 2^{\text{clip}(\text{round}(\log_2(x/\text{scale})))}$$

### Training Framework
*   **Platform:** Based on **PyTorch 2** Export Mode Quantization-Aware Training (QAT).
*   **Process:**
    1.  Model export.
    2.  Insertion of 'fake quantize' modules to simulate noise.
    3.  Fine-tuning.
    4.  Model conversion.
*   **Gradient Handling:** Gradients are handled via the **Straight-Through Estimator (STE)** to manage non-differentiable rounding and logarithmic operations.

### Experimental Setup
*   **Models:** NanoGPT (Shakespeare) and GPT-2 (124M parameters on OpenWebText).
*   **Hyperparameters:**
    *   Context length: 64
    *   Batch size: 12
    *   Learning rate: $0.5 \times 10^{-5}$

---

## Results

### Efficiency Metrics
*   **Memory:** Estimated **87.5% reduction** in requirements.
*   **Speed:** Projected **3-10x inference speedup** over full-precision models.

### Performance on GPT-2 (124M)
The FP32 baseline perplexity was **23.73**.

| Method | Perplexity | Performance Notes |
| :--- | :--- | :--- |
| **PTQ (Post-Training Quantization)** | 90.0 | Significant degradation compared to baseline. |
| **QAT (Quantization-Aware Training)** | **30.47** | **66% reduction** in perplexity compared to PTQ. |

### Performance by Quantization Level
Results varied significantly based on the bit-width and range chosen:

| Bit Width | Range | Perplexity |
| :--- | :--- | :--- |
| 3 bits | $[-2^3, 2^3]$ | 1669 |
| 4 bits | $[-2^5, 2^5]$ | 111 |
| 4 bits | $[-2^7, 2^7]$ | **30.47** |

*Note: NanoGPT converged close to the baseline floating-point loss of ~1.6.*

---

## Contributions

*   **Enabling Edge AI for LLMs:** The work addresses the critical bottleneck of deploying massive LLMs (GPT-3 scale and above) on Edge devices by proposing a solution that circumvents limited memory and processing power.
*   **Novel Quantization Strategy:** It introduces and validates the use of strict Power-of-Two quantization, proving that hardware-friendly arithmetic (bit shifting) can be applied to neural networks without catastrophic performance failure.
*   **Mitigation of Compression Loss:** The research contributes a framework for recovering performance loss from extreme quantization through Quantization Aware Training, establishing a viable trade-off between computational efficiency and model accuracy.

---
*References: 21 citations*