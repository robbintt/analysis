---
title: Exploring Reasoning Reward Model for Agents
arxiv_id: '2601.22154'
source_url: https://arxiv.org/abs/2601.22154
generated_at: '2026-02-03T13:56:20'
quality_score: 8
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Exploring Reasoning Reward Model for Agents

*Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total Benchmarks** | 12 |
| **Top Strategy** | Reagent-U (Unified Feedback) |
| **Best GAIA Score** | 43.7% |
| **Best WebWalkerQA Score** | 46.2% |
| **References** | 11 Citations |

---

## Executive Summary

Current autonomous agents struggle to master complex, multi-step reasoning tasks primarily because traditional training paradigms rely on sparse outcome-based rewards. These systems typically receive feedback only upon the final success or failure of a task, which fails to provide the granular guidance necessary for agents to correct intermediate errors or refine their decision-making processes. This lack of dense, informative feedback creates a significant bottleneck in developing agents capable of robust tool use and long-horizon planning in dynamic environments.

The authors introduce the **Agent Reasoning Reward Model (Agent-RRM)**, a framework designed to move beyond binary rewards by generating structured, multi-faceted feedback. Technically, Agent-RRM leverages the **Group Relative Policy Optimization (GRPO)** framework to provide agents with three distinct components of guidance: internal reasoning traces, targeted critiques, and holistic quality scores. The paper systematically investigates three integration strategiesâ€”**Reagent-C** (text-augmented refinement), **Reagent-R** (reward-augmented guidance), and **Reagent-U** (unified feedback integration)â€”utilizing a toolkit of six distinct capabilities (e.g., Web Browse, Python Interpreter) to optimize policy learning.

Empirical evaluation across 12 diverse benchmarks identifies the unified feedback integration strategy (**Reagent-U**) as the superior approach, yielding substantial performance gains over baseline methods. Specifically, the Agent-RRM framework achieved a score of **43.7%** on the challenging GAIA Benchmark and **46.2%** on the WebWalkerQA Benchmark. Qualitative analysis further demonstrated the modelâ€™s ability to pinpoint specific intermediate failures, such as missing navigation steps, over-reliance on text snippets, and tool inefficiencies, confirming the efficacy of granular feedback in improving complex reasoning behaviors.

This research significantly advances the field of agent learning by validating that dense, structured feedback is far more effective than sparse rewards for training complex autonomous systems. By successfully demonstrating that explicit reasoning traces and critiques can be integrated into the reinforcement learning pipeline, the authors establish a new performance ceiling for agent capabilities. To ensure reproducibility and accelerate future research, the team has released their code, models, and curated datasets.

---

## Key Findings

*   **Superior Integration Strategy:** Unified feedback integration (**Reagent-U**) yielded the most substantial performance improvements among the three investigated strategies.
*   **Benchmark Success:** The method achieved significant performance leaps, scoring **43.7%** on the **GAIA** benchmark and **46.2%** on the **WebWalkerQA** benchmark.
*   **Addressing Sparsity:** The multi-faceted reward model successfully addressed the limitations of sparse outcome-based rewards by providing granular guidance.
*   **Feedback Efficacy:** Providing agents with explicit reasoning traces, focused critiques, and overall scores proved significantly more effective than traditional sparse rewards.

---

## Methodology

The authors introduced the **Agent Reasoning Reward Model (Agent-RRM)**, designed to move beyond sparse outcome-based rewards through structured feedback. The methodology comprises two main components:

### Multi-Faceted Feedback Generation
The model produces three distinct types of output to guide agent learning:
1.  **Explicit Reasoning Traces**
2.  **Focused Critiques**
3.  **Overall Scores**

### Integration Strategies
The study investigated three distinct methods for incorporating reasoning rewards:
*   **Reagent-C:** Text-augmented refinement.
*   **Reagent-R:** Reward-augmented guidance.
*   **Reagent-U:** Unified feedback integration.

These strategies were evaluated across **12 diverse benchmarks** to assess their effectiveness in complex scenarios.

---

## Technical Details

| **Component** | **Description** |
| :--- | :--- |
| **Core Model** | **Agent-RRM** (Agent Reasoning Reward Model) |
| **Framework** | **GRPO** (Group Relative Policy Optimization) - computes advantages by normalizing rewards within a group and minimizes a clipped surrogate loss. |
| **Feedback Structure** | Multi-faceted feedback including: <br> â€¢ Internal Reasoning Traces <br> â€¢ Targeted Critiques <br> â€¢ Holistic Quality Scores |
| **Agent Toolkit** | Equipped with six tools: <br> â€¢ Search <br> â€¢ Web Browse <br> â€¢ Python Code Interpreter <br> â€¢ File Reader <br> â€¢ Image Descriptor <br> â€¢ Audio Converter |
| **Data Pipeline** | Utilizes curated datasets: <br> â€¢ **Reagent-SFT-55.6K** <br> â€¢ **Reagent-RL-709K** <br> *Processed via filtering, deduplication, and difficulty-aware sampling.* |

---

## Results

*   **Strategy Identification:** Unified feedback integration (**Reagent-U**) was identified as the superior strategy over Reagent-C and Reagent-R.
*   **Quantitative Performance:**
    *   **GAIA Benchmark:** 43.7%
    *   **WebWalkerQA Benchmark:** 46.2%
*   **Qualitative Analysis:** The model demonstrated the ability to detect intermediate failures with high specificity, identifying issues such as:
    *   Missing browse steps
    *   Over-reliance on snippets
    *   Interpretation errors
    *   Tool inefficiency

---

## Contributions

*   **Model Introduction:** Introduced **Agent-RRM**, a multi-faceted reward model capable of producing granular structured feedback.
*   **Systematic Investigation:** Conducted a systematic investigation of three distinct integration methodologies (**Reagent-C**, **Reagent-R**, **Reagent-U**) for reasoning rewards.
*   **Empirical Validation:** Provided empirical demonstration that the unified feedback strategy significantly outperforms other methods and establishes new performance levels.
*   **Open Source Release:** Released code, models, and datasets to support future research and ensure reproducibility.