---
title: 'XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression'
arxiv_id: '2510.11236'
source_url: https://arxiv.org/abs/2510.11236
generated_at: '2026-02-03T19:32:41'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression

*Haoqi Yang; Yao Yao; Zuchao Li; Baoyuan Qi; Guoming Liu; Hai Zhao*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **Citations:** 40
> *   **Target Bit-Width:** Sub-1.4 bits
> *   **Training Required:** No (Training-free)
> *   **Key Innovation:** Cross-Layer Compression & Data-Free Calibration
> *   **Primary Benefit:** Reduces KV Cache memory footprint (e.g., from 180 GB) significantly

---

## Executive Summary

**Problem**
The deployment of Large Language Models (LLMs) for long-context tasks is severely hindered by the memory footprint of the Key-Value (KV) cache, which grows linearly with sequence length. For standard 30-billion-parameter models, the KV cache can consume up to **180 GB** of memory, creating a bottleneck that restricts inference on resource-constrained hardware.

**Innovation**
XQuant introduces a training-free, plug-and-play framework that achieves ultra-low bit quantization through two specific technical innovations:
1.  **Data-Free Calibration:** Introduces a calibration parameter ($\eta$) to dynamically adjust scaling factors and zero-points during dequantization.
2.  **Cross-Layer KV Cache Compression:** Adjacent layers share the same quantized integer cache representation, storing only unique calibration parameters for the higher layer.

**Results**
Experimental validation on benchmarks such as **TruthfulQA** and **LongBench** demonstrates that XQuant achieves an equivalent bit-width of **sub-1.4 bits**, significantly outperforming current state-of-the-art methods like KIVI-2bit and AsymKV-1.5bit. The framework incurs only negligible computational overhead.

**Impact**
XQuant establishes a substantially improved trade-off between memory efficiency and model accuracy. By providing a training-free drop-in optimization, it sets a new standard for memory efficiency, enabling the deployment of large-scale models on hardware with significantly lower memory specifications.

---

## Key Findings

*   **Ultra-Low Quantization:** Successfully achieves ultra-low equivalent bit-width quantization for KV caches, reducing requirements to **sub-1.4 bits**.
*   **Superior Performance:** Outperforms current state-of-the-art methods (specifically KIVI-2bit and AsymKV-1.5bit) by maintaining superior performance metrics at lower bit-widths.
*   **Resource Efficiency:** Establishes a significantly improved trade-off between memory efficiency and model accuracy for LLMs deployed in resource-constrained environments.
*   **Zero-Retrain Deployment:** The solution is training-free and plug-and-play, requiring **no model retraining** and incurring negligible computational overhead for calibration.

---

## Methodology

XQuant utilizes a training-free framework designed to compress KV caches without altering model weights. The approach relies on two specific technical innovations:

*   **Computationally Negligible Data-Free Calibration Method**
    Allows the framework to determine quantization parameters without requiring input data or significant processing power.
*   **Cross-Layer KV Cache Compression**
    Leverages information across different layers of the network to enable aggressive quantization (sub-1.4 bits) while preserving the integrity of historical information necessary for long-text understanding.

---

## Technical Details

XQuant is a training-free, plug-and-play framework designed to compress KV caches to ultra-low bit-widths. Its architecture is defined by the following components:

*   **Data-Free Calibration**
    *   **Mechanism:** Reduces quantization error by introducing a calibration parameter $\eta$.
    *   **Function:** Adjusts scaling factors and zero-points dynamically during the dequantization process.

*   **Cross-Layer KV Cache Compression**
    *   **Mechanism:** Adjacent layers share quantized integer caches.
    *   **Optimization:** Instead of storing full cache data for every layer, the framework stores only the calibration parameters for the higher layer, resulting in significant memory savings.

---

## Contributions

*   **Development of a Training-Free Framework**
    Introduced XQuant as a practical, plug-and-play solution for LLM deployment that removes the memory bottleneck associated with KV cache growth during long-text tasks.

*   **Advancement in Quantization Limits**
    Proposed cross-layer compression and data-free calibration to push the boundaries of KV cache quantization beyond current capabilities (sub-1.4 bits), setting a new standard for memory efficiency.

*   **Validation of Superior Performance**
    Through extensive experiments on TruthfulQA and LongBench, demonstrated that ultra-low bit quantization is achievable without sacrificing the accuracy of LLMs, challenging the limitations of previous state-of-the-art methods.

---

## Results

*   **Bit-Width Achievement:** Successfully achieves an equivalent bit-width of **sub-1.4 bits**.
*   **Benchmark Performance:** Outperforms baselines like KIVI-2bit and AsymKV-1.5bit.
*   **Operational Cost:** The method is training-free and incurs **negligible calibration overhead**.
*   **Memory Footprint:** Directly addresses the high memory footprint of KV caches, which can reach **180 GB** for standard 30-billion-parameter models.