---
title: 'EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition'
arxiv_id: '2508.1413'
source_url: https://arxiv.org/abs/2508.14130
generated_at: '2026-02-03T18:32:43'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition

*Hugo Thimonier; Antony Perzo; Renaud Seguier*

---

> ### **Quick Facts**
>
> *   **Model Architecture:** EmoSLLM (Multimodal LLM)
> *   **Base Backbone:** Llama3.2-3B-Instruct
> *   **Optimization Strategy:** Low-Rank Adaptation (LoRA)
> *   **Audio Encoders:** Robust wav2vec 2.0 & WavLM
> *   **Parameter Efficiency:** <50% of parameters compared to SOTA methods
> *   **Input Length:** 3-second audio clips

---

## Executive Summary

Speech Emotion Recognition (SER) presents a complex challenge as it requires the simultaneous interpretation of linguistic content (text) and paralinguistic cues (audio). While Large Language Models (LLMs) offer robust reasoning capabilities, adapting them for multimodal SER has traditionally been computationally prohibitive. Existing approaches often require full fine-tuning of massive models or suffer from parameter inefficiency, making them impractical for deployment in resource-constrained environments. This paper addresses the critical need for an architecture that can effectively integrate audio and text modalities within an LLM framework while maintaining high parameter efficiency to enable broader accessibility and on-device application.

The authors propose EmoSLLM, a novel parameter-efficient multimodal pipeline built upon the Llama3.2-3B-Instruct backbone. The core technical innovation is the **Query Pooling Mapper (QPMapper)**, a learnable interfacing module designed to bridge the modality gap. The system extracts raw audio features using self-supervised encoders, which are then processed by the QPMapper to downsample the sequence and project it directly into the LLMâ€™s textual embedding space. The entire model is optimized using LoRA, freezing the main LLM weights and training only the low-rank decomposition matrices.

EmoSLLM achieves state-of-the-art competitive performance, outperforming nearly all existing Speech-Text LLMs on standard emotion recognition benchmarks. Crucially, this high performance is achieved with extreme parameter efficiency; the model requires less than 50% of the trainable parameters compared to competing state-of-the-art methods. This research establishes a new efficiency standard for applying LLMs to paralinguistic domains, making advanced SER viable for privacy-sensitive, on-device deployment.

---

## Key Findings

*   **High Performance:** The proposed model achieves highly competitive performance, outperforming nearly all existing Speech-Text Large Language Models on standard emotion recognition benchmarks.
*   **Parameter Efficiency:** The approach demonstrates significant efficiency, requiring less than half the number of parameters compared to competing methods.
*   **Multimodal Integration:** The model effectively integrates paralinguistic (audio) and linguistic (text) cues within a shared representation space.
*   **LoRA Sufficiency:** Low-Rank Adaptation (LoRA) proved sufficient for fine-tuning, maintaining high accuracy without the need for full parameter updates.

---

## Methodology

The proposed method utilizes a **parameter-efficient multimodal pipeline** to adapt a pre-trained LLM for speech emotion recognition. The process involves three primary stages:

1.  **Feature Extraction:** Raw paralinguistic audio features are extracted from the input signal.
2.  **Modality Mapping:** These features are mapped into the LLM's textual representation space via a learnable interfacing module.
3.  **Composite Input Construction:** A composite input sequence is created consisting of:
    *   Transformed audio features
    *   The natural language transcript
    *   A task-specific textual prompt

The model is optimized using **Low-Rank Adaptation (LoRA)**. This strategy freezes the main LLM weights, training only the low-rank decomposition matrices to adapt the model to the emotion recognition task.

---

## Technical Details

**Core Architecture**
*   **Base Model:** Llama3.2-3B-Instruct
*   **Adaptation:** Low-Rank Adaptation (LoRA)

**Audio Processing Pipeline**
*   **Encoders:** Utilizes pre-trained self-supervised encoders, specifically **Robust wav2vec 2.0** and **WavLM**.

**Modality Alignment: Query Pooling Mapper (QPMapper)**
To address modality imbalance and bridge the gap between audio and text, a Query Pooling Mapper is employed:
*   **Mechanism:** Introduces learnable queries to the audio sequence.
*   **Processing:** Queries are processed through a Transformer encoder.
*   **Output:** Produces a downsampled representation projected into the LLM's embedding dimension:
    $$h_{ds} \in \mathbb{R}^{n_q \times d_{LLM}}$$

**LLM Input Processing**
The LLM processes a concatenated sequence containing:
1.  Transformed audio features ($h_{ds}$)
2.  Embedded textual prompt ($p$)
3.  Optional text transcript ($z$)

---

## Research Contributions

*   **Novel Multimodal Architecture:** Introduction of a new architecture that bridges audio and text modalities by mapping audio features directly into the LLM embedding space via a learnable interface.
*   **New Efficiency Standard:** Establishment of a new benchmark for Speech-Text LLMs, demonstrating state-of-the-art performance with less than 50% of the trainable parameters.
*   **Validation of LLM Versatility:** Successful validation of applying LLMs to the paralinguistic domain of emotion recognition, expanding their utility beyond traditional Natural Language Processing (NLP) tasks.

---

## Results

The EmoSLLM model demonstrated exceptional results in testing:
*   **Benchmark Performance:** It outperformed all but one existing Speech-Text LLMs on standard emotion recognition benchmarks.
*   **Resource Efficiency:** The model requires less than half the number of parameters compared to state-of-the-art methods like **SIFT-LLM** and **SALOMONN**.
*   **Deployment Viability:** This efficiency makes the model viable for privacy-sensitive, on-device deployment scenarios.
*   **Training Configuration:** The model was successfully trained and validated using 3-second audio clips.

---

**Document Quality Score:** 8/10  
**References:** 40 citations