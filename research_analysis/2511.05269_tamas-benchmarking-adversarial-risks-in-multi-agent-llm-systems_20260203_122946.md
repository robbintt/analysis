---
title: 'TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems'
arxiv_id: '2511.05269'
source_url: https://arxiv.org/abs/2511.05269
generated_at: '2026-02-03T12:29:46'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems

*Authors: Ishan Kavathekar; Hemang Jain; Ameya Rathod; Ponnurangam Kumaraguru; Tanuja Ganu*

***

> ### ðŸ“Š Quick Facts
> 
> | Metric | Detail |
> | :--- | :--- |
> | **Benchmark Name** | TAMAS (Threats and Attacks in Multi-Agent Systems) |
> | **Dataset Size** | 300 adversarial instances |
> | **Models Evaluated** | 10 backbone LLMs |
> | **Frameworks Tested** | Autogen, CrewAI |
> | **New Metric** | Effective Robustness Score (ERS) |
> | **Threat Model** | Grey Box |
> | **References** | 40 Citations |
> | **Quality Score** | 8/10 |

***

## Executive Summary

This research addresses a critical security gap in Multi-Agent LLM Systems (MAS), which are increasingly deployed using frameworks like Autogen and CrewAI but remain vulnerable to adversarial exploitation. While existing safety benchmarks focus primarily on single-agent interactions, they fail to capture the complex emergent risks and novel failure modes arising from agent collaboration and inter-communication. As these systems scale, they face a significant tension between task utility and security, leaving state-of-the-art implementations exposed to attacks that can derail tasks or trigger malicious behaviors.

To bridge this gap, the authors introduce **TAMAS** (Threats and Attacks in Multi-Agent Systems), a comprehensive benchmark comprising 300 adversarial instances. The study formalizes a "Grey Box" threat model and categorizes vulnerabilities into Prompt-level, Agent-level, and Environment-level classes. Evaluations revealed widespread security challenges, demonstrating that state-of-the-art models are highly susceptible to adversarial inputs. A key technical contribution, the **Effective Robustness Score (ERS)**, confirms a statistically significant negative correlation between task effectiveness and robustness. This work establishes a foundational standard for AI safety by providing the first dedicated dataset and testing framework tailored to multi-agent risks.

***

## Key Findings

*   **High Vulnerability:** Multi-agent LLM systems exhibit critical failure modes and are highly susceptible to adversarial attacks.
*   **Benchmarking Gap:** Existing benchmarks fail to capture unique security vulnerabilities arising specifically from multi-agent dynamics.
*   **Safety-Utility Trade-off:** There is a significant trade-off between safety and task effectiveness, quantified by the Effective Robustness Score (ERS).
*   **Widespread Challenges:** Evaluation of ten backbone LLMs highlights that state-of-the-art systems currently face pervasive security issues.

***

## Technical Details

### System Definition
The system is defined as a Multi-Agent System (MAS) comprising a set of agents. Each agent is initialized with a specific system prompt and a toolset, relying on observations derived from actions to function.

### Threat Model
*   **Type:** Grey Box Attacker.
*   **Knowledge:** Knows agent roles and available tools but does not know the underlying LLM parameters.
*   **Objective:** Derail tasks or trigger malicious actions.

### Vulnerability Categories
1.  **Prompt-level**
2.  **Agent-level**
3.  **Environment-level**

### Formalized Attack Vectors
The paper formalizes six distinct attack vectors:
*   **Direct Prompt Injection (DPI):** Malicious instructions are concatenated to target queries.
*   **Impersonation:** False authority claims are utilized to deceive agents.
*   **Indirect Prompt Injection (IPI):** Observations are poisoned to manipulate agent behavior.
*   **Contradicting Agents:** Agents act to contradict one another to disrupt the workflow.
*   **Byzantine Agents:** Agents behave arbitrarily or maliciously.
*   **Colluding Agents:** Multiple agents work together to exploit the system.

***

## Methodology

The researchers employed the following approach to assess the robustness of multi-agent systems:

1.  **Benchmark Development:** Creation of the **TAMAS** benchmark, consisting of **300 adversarial instances**.
2.  **Configuration Testing:** Assessment of ten backbone LLMs across three distinct interaction configurations.
3.  **Framework Utilization:** Testing was conducted using popular frameworks **Autogen** and **CrewAI**.
4.  **Metric Introduction:** Implementation of the **Effective Robustness Score (ERS)** to evaluate the balance between safety and task effectiveness.

***

## Contributions

*   **Paradigm Shift:** Addresses the security gap by moving beyond single-agent safety benchmarks to focus on multi-agent risks.
*   **Standardized Framework:** Provides a standardized testing framework accompanied by a diverse dataset specifically for multi-agent safety.
*   **New Metric:** Introduces the **Effective Robustness Score (ERS)** as a new standard for assessing the safety-utility balance in agentic systems.

***

## Results

*   **ERS Metric:** The paper proposes the Effective Robustness Score (ERS) to quantify the trade-off between safety and task effectiveness, utilizing an Attack Success Indicator function.
*   **Security Challenges:** Evaluations across ten backbone LLMs confirm widespread security challenges and high vulnerability to adversarial attacks in multi-agent settings.
*   **Statistical Trade-off:** A statistically significant trade-off was observed where systems optimized for effectiveness tend to have lower robustness scores.

***

**References:** 40 citations  
**Quality Score:** 8/10