---
title: 'BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight
  Indexing'
arxiv_id: '2506.03515'
source_url: https://arxiv.org/abs/2506.03515
generated_at: '2026-02-03T19:31:46'
quality_score: 9
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing

*Masaya Kawamura; Takuya Hasumi; Yuma Shirahata; Ryuichi Yamamoto*

---

> ### ðŸ“Š Quick Facts
>
> *   **Compression Rate:** 83% size reduction vs. standard 32-bit implementations
> *   **Quantization Precision:** 1.58-bit (ternary values $\{-1, 0, 1\}$)
> *   **Synthesis Quality:** MOS ~4.20 (LJSpeech)
> *   **Storage Strategy:** Weight Indexing for int8 hardware compatibility
> *   **Quality Score:** 9/10

---

## Executive Summary

This paper addresses the critical bottleneck of deploying high-fidelity Neural Text-to-Speech (TTS) systems on resource-constrained edge devices. Standard neural TTS models rely on 32-bit floating-point parameters, resulting in large memory footprints that are often infeasible for on-device hardware. The authors aim to solve this by achieving extreme model compression without incurring the significant degradation in speech synthesis quality typically associated with low-bit quantization, thereby enabling advanced voice assistants on consumer electronics.

The core innovation is the **BitTTS framework**, which couples Quantization-Aware Training (QAT) with a novel "weight indexing" storage mechanism. During training, the model utilizes QAT to reduce 32-bit parameters to a precision of 1.58 bits, constraining the majority of weights to specific ternary values of $\{-1, 0, 1\}$. To bridge the gap between this sub-8-bit representation and standard 8-bit hardware, the authors introduce a weight indexing method; this technique aggregates groups of the quantized weights and stores them as a single `int8` index. This allows the model to maintain the mathematical benefits of ternary quantization while ensuring efficient memory usage on hardware architectures designed for 8-bit units.

The proposed BitTTS model demonstrates substantial efficiency gains and superior synthesis quality on the LJSpeech dataset. The framework achieves an **83% reduction in model size** compared to standard 32-bit implementations. Crucially, the 1.58-bit quantized model achieved a Mean Opinion Score (MOS) of approximately 4.20, significantly outperforming a non-quantized baseline model of equivalent size (which suffers from quality degradation due to reduced capacity) and approaching the performance of much larger full-precision models.

This work challenges the conventional assumption that low-bit representations inevitably degrade audio quality, demonstrating that extreme quantization is viable for high-performance TTS. By successfully integrating 1.58-bit weights with 8-bit hardware storage constraints, BitTTS establishes a new precedent for lightweight neural networks. This research paves the way for the widespread deployment of sophisticated, high-quality voice interaction capabilities on edge devices, overcoming previous limitations regarding memory and power consumption.

---

## Key Findings

*   **Significant Model Compression:** The proposed BitTTS model achieved an **83% reduction** in model size compared to standard implementations.
*   **Superior Synthesis Quality:** Despite aggressive compression, the model outperformed baselines of similar model size (that did not use quantization) in terms of speech synthesis quality.
*   **Effective Ternary Quantization:** The study validates that converting 32-bit parameters to ternary values $\{-1, 0, 1\}$ via quantization-aware training (QAT) is a viable strategy for maintaining TTS performance.
*   **Hardware-Efficient Storage:** The weight indexing method successfully enables the efficient storage of sub-8-bit weights (1.58-bit) on hardware designed for 8-bit units.

---

## Methodology

The proposed BitTTS model utilizes a dual-technique approach to achieve high compactness for on-device deployment:

*   **Quantization-Aware Training (QAT):**
    The model employs QAT to reduce parameter precision during the training process. Specifically, 32-bit parameters are quantized down to **1.58 bits**, where the majority of weights are constrained to ternary values of $\{-1, 0, 1\}$.

*   **Weight Indexing:**
    To address hardware storage constraints, the authors introduce a weight indexing method. This technique aggregates groups of the 1.58-bit quantized weights and stores them as a single `int8` index, ensuring efficient memory usage on hardware that processes data in 8-bit units.

---

## Technical Details

The implementation relies on specific quantization and storage strategies to balance mathematical efficiency with hardware realities.

*   **Quantization Scheme:** Utilizes a 1.58-bit scheme where parameters are discretized into ternary values $\{-1, 0, 1\}$, replacing standard 32-bit floating-point parameters.
*   **Training Process:** Employs Quantization-Aware Training (QAT) to simulate quantization error during gradient descent.
*   **Storage Optimization:** Introduces a specific weight indexing method to efficiently pack and store 1.58-bit weights on hardware architectures designed for 8-bit units.

---

## Research Contributions

*   **Novel Architecture for On-Device TTS:** Introduction of BitTTS, a highly compact and lightweight text-to-speech framework specifically optimized for on-device applications.
*   **Extreme Quantization Technique:** Implementation of a 1.58-bit quantization strategy using QAT that pushes the limits of parameter compression by utilizing ternary representations.
*   **Innovative Storage Mechanism:** Development of the 'weight indexing' method, which bridges the gap between low-bit mathematical representation and standard 8-bit hardware storage requirements.
*   **Performance Benchmarking:** Demonstration that extreme quantization techniques can yield better synthesis quality than non-quantized models of equivalent size, challenging assumptions about the trade-off between model size and audio quality.

---

## Performance Results

The evaluation of BitTTS highlights the success of the proposed methods:

*   **Model Size:** Achieves an **83% reduction** in size compared to standard 32-bit implementations.
*   **Accuracy-to-Size Ratio:** Outperforms baselines with similar model sizes that do not employ quantization, indicating a better accuracy-to-size ratio.
*   **Hardware Feasibility:** Confirms the feasibility of storing sub-8-bit weights on standard 8-bit hardware units without loss of information integrity.

---

## Assessment

**Quality Score:** 9/10