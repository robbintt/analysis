---
title: 'TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation
  Suppression and Outlier Control'
arxiv_id: '2510.27527'
source_url: https://arxiv.org/abs/2510.27527
generated_at: '2026-02-03T18:44:50'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control

*Yuxiang Chen; Xiaoming Xu; Pengle Zhang; Michael Beyer; Martin Rapp; Jun Zhu; Jianfei Chen*

---

> ### **Quick Facts**
> * **Performance Gap Reduction:** 51.3% average reduction compared to prior FP4 methods.
> * **Training Scale:** Validated on models up to **370M parameters** and datasets up to **200B tokens**.
> * **Efficiency:** Up to **3x higher compute performance** and **2x lower memory usage** vs. MXFP8.
> * **Format:** NVFP4 (E2M1 data, E4M3 scaling).
> * **Quality Score:** 9/10

---

## Executive Summary

### Problem
Training Large Language Models (LLMs) requires substantial computational resources and memory, driving the need for low-precision training formats. While 4-bit (FP4) formats offer significant theoretical efficiency gains, they historically suffer from accuracy loss and instability, making them impractical for large-scale pre-training. The core challenge lies in the limited dynamic range of 4-bit representations, which introduces two critical failure modes: **weight oscillation** near quantization decision boundaries and the inability to manage **outlier values**. These issues prevent accurate convergence, leaving a persistent performance gap between low-precision and standard full-precision (FP32/BF16) training methods.

### Innovation
TetraJet-v2 introduces an end-to-end **4-bit Fully-Quantized Training (FQT)** framework leveraging the NVFP4 microscaling format (E2M1 data, E4M3 scaling). To address range limitations, the authors utilize a **Double-Block Quantization** technique employing both a 1x128 global scale and a 1x16 block scale. Crucially, the method quantizes all six matrices involved in linear layer forward and backward passes, applying deterministic rounding forward and stochastic rounding backward to maintain unbiased gradients. The system specifically targets instability through two novel algorithms: **OsciReset**, which suppresses weight oscillation near thresholds, and **OutControl**, which manages outlier propagation to ensure stable convergence.

### Results
In evaluations across models scaling up to 370M parameters and datasets of up to 200B tokens, TetraJet-v2 significantly outperformed prior FP4 training methods. The approach reduced the performance gap between low-precision and full-precision training by an average of **51.3%**, achieving near-lossless accuracy. The implementation demonstrated robust stability, successfully identifying and mitigating weight oscillation phenomena during the final stages of training. Furthermore, the system achieved substantial hardware efficiency improvements, delivering up to **3x higher compute performance** and **2x lower memory usage** compared to the MXFP8 standard.

### Impact
This research establishes a new state-of-the-art for NVFP4-based 4-bit fully-quantized training, effectively proving that stable LLM pre-training is feasible at the 4-bit level. By isolating and resolving the specific issues of weight oscillation and outlier management, TetraJet-v2 removes major technical barriers to the adoption of extreme low-precision formats. The resulting efficiency gains—combining near-lossless accuracy with drastic reductions in compute and memory overhead—suggest a viable path toward significantly lowering the cost and energy consumption of training future foundation models.

---

## Key Findings
*   **Superior Performance:** TetraJet-v2 significantly outperforms prior FP4 training methods in LLM pre-training, successfully addressing weight oscillation and outlier management.
*   **Gap Reduction:** Across models up to 370M parameters and datasets up to 200B tokens, TetraJet-v2 reduced the performance gap between low-precision and full-precision training by an average of **51.3%**.
*   **Near-Lossless Training:** The implementation of NVFP4 for activations, weights, and gradients allows for near-lossless training at the 4-bit level.
*   **Stability:** The system successfully identifies and addresses weight oscillation phenomena near decision boundaries (e.g., 0.25) during final training stages, ensuring stable convergence.

---

## Methodology
TetraJet-v2 utilizes an end-to-end **4-bit Fully-Quantized Training (FQT)** approach, leveraging the NVFP4 format for activations, weights, and gradients across all linear layers. The method employs an **unbiased double-block quantization** technique and introduces two critical components:
*   **OsciReset:** A mechanism designed to suppress weight oscillation.
*   **OutControl:** A method to manage outliers.

This combination ensures that the quantization errors do not accumulate to the point of instability during the training process.

---

## Technical Details
The technical architecture of TetraJet-v2 is built upon the following specifications:

*   **Format:** NVFP4 microscaling format (**E2M1 data**, **E4M3 scaling**).
*   **Group Size:** 16 elements.
*   **Quantization Technique:** Double-Block Quantization utilizing:
    *   Global scale (1x128 block)
    *   Block scale (1x16)
*   **Matrix Coverage:** Quantizes all six matrices involved in linear layer forward/backward passes.
*   **Rounding Strategy:**
    *   *Forward Pass:* Deterministic rounding.
    *   *Backward Pass:* Stochastic rounding (for unbiased gradients).
*   **Stability Mechanisms:** Includes an Oscillation Suppression (**OsciReset**) mechanism to prevent weight instability near quantization thresholds.

---

## Results
The evaluation of TetraJet-v2 demonstrates significant improvements in both accuracy and efficiency:

*   **Accuracy:** Reduced the FP4 to full-precision performance gap by an average of **51.3%** over prior work.
*   **Scale:** Achieved near-lossless training on models up to **370M parameters** and **200B tokens**.
*   **Hardware Efficiency:**
    *   Up to **3x higher compute performance** compared to MXFP8.
    *   **2x lower memory usage** compared to MXFP8.
*   **Convergence:** Successfully identified and addressed weight oscillation phenomena near decision boundaries during final training stages.

---

## Contributions
*   **Problem Identification:** The authors identified weight oscillation and outliers as critical issues preventing accurate low-precision LLM training.
*   **Algorithm Proposal:** Proposed **OsciReset** and **OutControl** algorithms to stabilize 4-bit training.
*   **State-of-the-Art Performance:** By achieving a 51.3% reduction in the performance gap to full-precision training, the work establishes a new state-of-the-art for NVFP4-based 4-bit fully-quantized training.

---
**Paper Quality Score:** 9/10  
**References:** 40 citations