---
title: 'AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding
  and Efficiency in Audio LLMs'
arxiv_id: '2510.07293'
source_url: https://arxiv.org/abs/2510.07293
generated_at: '2026-02-03T20:07:39'
quality_score: 9
citation_count: 34
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs

*Peize He; Zichen Wen; Yubo Wang; Yuxuan Wang; Xiaoqian Liu; Jiajie Huang; Zehui Lei; Zhuangcheng Gu; Xiangqi Jin; Jiabing Yang; Kai Li; Zhifei Liu; Weijia Li; Cunxiang Wang; Conghui He; Linfeng Zhang*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Benchmark Name** | AudioMarathon |
| **Dataset Size** | 6,567 instances |
| **Input Duration** | 90.0 â€“ 300.0 seconds |
| **Task Categories** | Speech, Sound, and Music |
| **Total Tasks** | 10 |
| **Citations** | 34 |
| **Quality Score** | 9/10 |

---

## Executive Summary

**Problem**
Large Audio Language Models (LALMs) have seen rapid advancements, yet they face critical limitations when processing long-form audio, a requirement for real-world applications like meeting analysis and media monitoring. Current architectures suffer from significant performance degradation as audio duration increases, primarily due to the quadratic complexity of attention mechanisms and an inability to effectively model long-range temporal dependencies. Furthermore, the field lacks a standardized, realistic benchmark to evaluate these specific long-context capabilities, leaving a gap in understanding how these models perform on complex, multi-hop reasoning tasks over extended periods.

**Innovation**
The authors introduce **AudioMarathon**, the first comprehensive benchmark specifically designed to evaluate long-context audio understanding and efficiency. Constructed through a rigorous six-stage pipeline, the benchmark comprises 6,567 instances featuring audio inputs ranging from 90 to 300 seconds across diverse domains (speech, sound, and music). It covers 10 distinct tasks categorized into Speech Context Understanding, Audio Scene Understanding, and Voice Characteristic Identification. A key technical aspect of the innovation is the dual evaluation framework, which simultaneously assesses a model's semantic understanding capabilities against its inference efficiency, specifically analyzing the impact of acceleration techniques such as token pruning and KV cache eviction.

**Results**
Experimental evaluation of state-of-the-art LALMs on AudioMarathon reveals substantial performance drops as audio context length increases, particularly in tasks requiring complex reasoning. The dataset is dominated by Speaker Gender Recognition (25%) and Audio Scene Classifier (17%), providing a robust testbed for classification and identification. The analysis highlights that while acceleration techniques can reduce the computational bottleneck associated with long sequences, they introduce significant trade-offs, often resulting in a loss of accuracy. Specifically, current models struggle to maintain coherence and accuracy in multi-hop inference scenarios within long audio contexts, exposing a significant capability gap in existing architectures.

**Impact**
AudioMarathon establishes a critical standardization for the audio LLM community, providing empirical evidence that current models require fundamental improvements in memory efficiency and temporal modeling. By identifying specific failure modes regarding long-range dependencies and the efficiency-accuracy trade-off, this research provides a targeted direction for future architectural advancements. The release of this benchmark serves as a vital resource for researchers, enabling the systematic evaluation of next-generation models and accelerating the development of audio LLMs capable of handling real-world, long-duration inputs.

---

## Key Findings

*   **Performance Degradation:** State-of-the-art Large Audio Language Models (LALMs) exhibit significant performance drops as the duration and context length of the audio input increase.
*   **Computational Bottlenecks:** Current models struggle with the quadratic complexity of attention mechanisms and fail to effectively model long-range temporal dependencies.
*   **Acceleration Trade-offs:** Analysis of acceleration techniques reveals distinct trade-offs between inference speed and accuracy when employing token pruning and KV cache eviction strategies.
*   **Reasoning Gap:** There is a substantial capability gap in current LALMs regarding complex reasoning tasks, specifically those requiring multi-hop inference within long audio contexts.

---

## Methodology

The authors introduce AudioMarathon, a comprehensive benchmark designed specifically to evaluate long-form audio processing. The methodology is built upon:

*   Utilizing audio inputs with durations ranging from **90.0 to 300.0 seconds**.
*   Ensuring domain diversity across **speech, sound, and music**.
*   Designing complex tasks that require **multi-hop inference**.
*   Evaluating state-of-the-art LALMs against this benchmark to assess both understanding and efficiency.
*   Analyzing specific acceleration techniques like **token pruning** and **KV cache eviction**.

---

## Contributions

*   **Benchmark Creation:** The release of AudioMarathon, the first benchmark to address the lack of realistic, long-context evaluation datasets for audio LLMs.
*   **Dual Evaluation Metrics:** Establishment of a standardized framework that simultaneously evaluates a model's understanding capabilities and its inference efficiency.
*   **Identification of Research Needs:** Empirical evidence highlighting the urgent need for architectures that improve memory efficiency and temporal reasoning, providing a targeted direction for future advancements.

---

## Technical Details

### Benchmark Construction Pipeline
The AudioMarathon benchmark was constructed using a rigorous six-stage pipeline:
1.  **Source Selection:** Filtering 30 candidates down to 10 subsets.
2.  **Merge Audio:** Applying specific concatenation logic.
3.  **Tool Design:** Creating custom concatenation scripts.
4.  **Data Processing:** Pairing audio with prompts and multiple-choice options.
5.  **Manual Verification:** Conducting quality control on 10% of samples.
6.  **Benchmark Finalization:** Selecting the final 6,567 instances.

### Evaluation Categories & Tasks
The benchmark evaluates 10 tasks across three primary categories:

*   **Speech Context Understanding:**
    *   SCR (Speech Context Recognition)
    *   SER (Speech Emotion Recognition)
    *   ASR (Automatic Speech Recognition)
*   **Audio Scene Understanding:**
    *   MC (Music Classification)
    *   ASC (Audio Scene Classification)
    *   SED (Sound Event Detection)
*   **Voice Characteristic Identification:**
    *   ER (Emotion Recognition)
    *   SD (Speaker Identification/Diarization)
    *   SAR (Speaker Age Recognition)
    *   SGR (Speaker Gender Recognition)

### Inputs & Scoring
*   **Inputs:** Long-form audio sequences ranging from 90.0 to 300.0 seconds.
*   **Evaluation Criteria:** Requires an exact match to the provided choice, including the complete correct option.

---

## Results

The final benchmark consists of **6,567 instances** distributed across 10 tasks, with Speaker Gender Recognition (25%) and Audio Scene Classifier (17%) comprising the largest portions.

*   **Performance Degradation:** Experimental findings indicate that state-of-the-art LALMs exhibit significant performance degradation as audio duration and context length increase.
*   **Computational Limits:** Current models face computational bottlenecks due to quadratic complexity and struggle with long-range temporal dependencies.
*   **Speed vs. Accuracy:** Analysis of acceleration techniques (token pruning and KV cache eviction) reveals trade-offs between inference speed and accuracy.
*   **Multi-hop Inference Failure:** There is a substantial capability gap in complex reasoning tasks, particularly multi-hop inference within long audio contexts.

---
*Research Paper Analysis Report | Generated by Technical Formatter*