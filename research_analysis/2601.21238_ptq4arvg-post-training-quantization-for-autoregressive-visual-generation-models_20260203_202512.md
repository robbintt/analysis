---
title: 'PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation
  Models'
arxiv_id: '2601.21238'
source_url: https://arxiv.org/abs/2601.21238
generated_at: '2026-02-03T20:25:12'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models

*Xuewen Liu; Zhikai Li; Jing Zhang; Mengjuan Chen; Qingyi Gu*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Target Models** | VAR-d30 (2B), RAR-XXL (1.5B), MAR-Huge (1B), PAR (3B) |
| **Quantization Levels** | **8-bit** and **6-bit** |
| **Training Required** | None (Post-Training) |
| **Latency Note** | Baseline PAR-3B > 3 seconds/image |
| **Key Innovations** | GPS, STWQ, DGC Algorithms |
| **Quality Score** | 9/10 |

---

> ### ðŸ“ Executive Summary
>
> This paper addresses the prohibitive computational costs associated with deploying large-scale AutoRegressive Visual Generation (ARVG) models, which range from 1B to 3B parameters, creating a barrier to their use on resource-constrained edge hardware. While Post-Training Quantization (PTQ) is a standard efficiency technique for general transformers, existing methods fail for ARVGs due to three distinct structural and distributional hurdles: severe channel-wise outliers in Adaptive LayerNorm (AdaLN) activations, highly dynamic token-wise activations (including "sink tokens"), and significant sample-wise distribution mismatches. These challenges render standard quantization approaches ineffective, necessitating a specialized framework to enable efficient inference for these advanced generative models.
>
> The authors propose **PTQ4ARVG**, the first training-free PTQ framework tailored for ARVGs, composed of three algorithmic components designed to overcome these specific hurdles. First, **Gain-Projected Scaling (GPS)** mitigates channel-wise outliers by mathematically deriving optimal per-channel scaling factors using Taylor series expansion and Hessian approximations to bound quantization loss. Second, **Static Token-Wise Quantization (STWQ)** handles dynamic activations without runtime overhead by leveraging fixed sequence lengths; it applies offline, position-specific quantization parameters to AdaLN inputs and separate parameters for sink tokens in linear layers. Third, **Distribution-Guided Calibration (DGC)** resolves sample-wise mismatches by utilizing an entropy-based strategy to select calibration samples that accurately represent the training distribution.
>
> The framework was validated across major model architectures including VAR-d30 (2B), RAR-XXL (1.5B), MAR-Huge (1B), and PAR (3B). PTQ4ARVG successfully maintains visual generation quality comparable to full-precision (FP) baselines while operating at aggressive 6-bit and 8-bit quantization levels. For instance, the method effectively compresses the PAR-3B model, which suffers from a baseline inference latency of greater than 3 seconds per image, without sacrificing fidelity. Crucially, this performance is achieved without requiring retraining, and the STWQ module introduces zero additional calibration overhead, validating the framework's efficiency.

---

## Key Findings

*   **Structural Hurdles Identified:** The study identifies three specific barriers in quantizing ARVG models:
    *   Severe channel-wise outliers.
    *   Highly dynamic token-wise activations.
    *   Sample-wise distribution mismatches.
*   **General Methods Fail:** Current general-purpose quantization methods are ineffective for the specific architecture of ARVG models.
*   **Successful Quantization:** The proposed PTQ4ARVG framework enables **8-bit and 6-bit quantization** without the need for retraining.
*   **Performance Maintenance:** The framework maintains performance comparable to full-precision (FP) counterparts, even at lower bit-widths.

---

## Methodology

The researchers developed **PTQ4ARVG**, a training-free Post-Training Quantization framework. It consists of three core components designed to address the identified hurdles:

*   **Gain-Projected Scaling (GPS)**
    *   **Goal:** Mitigate channel-wise outliers.
    *   **Mechanism:** Utilizes Taylor series expansion to mathematically derive optimal scaling factors.
*   **Static Token-Wise Quantization (STWQ)**
    *   **Goal:** Address token-wise variance.
    *   **Mechanism:** Leverages fixed token lengths and position-invariant distributions to perform static calibration without runtime overhead.
*   **Distribution-Guided Calibration (DGC)**
    *   **Goal:** Resolve sample-wise distribution mismatch.
    *   **Mechanism:** Selects calibration samples based on distributional entropy to ensure representative data.

---

## Technical Details

### Architecture & Challenges
*   **Architecture:** ARVG models utilize a Transformer structure featuring Multi-Head Self-Attention (MHSA), Feedforward Networks (FFN), and Adaptive LayerNorm (AdaLN).
*   **Prediction:** The models predict fixed tokens with bidirectional dependencies.
*   **Specific Challenges:**
    *   **Channel-wise Outliers:** Occur in AdaLN-adjusted activations.
    *   **Token-wise Dynamics:** Includes the presence of 'sink tokens'.
    *   **Sample-wise Mismatches:** Distributional inconsistencies across samples.

### Algorithmic Solutions
1.  **Gain-Projected Scaling (GPS):**
    *   Addresses channel-wise outliers via per-channel scaling factors.
    *   Derivation based on quantization loss bounds and Hessian approximations.
2.  **Static Token-Wise Quantization (STWQ):**
    *   Manages token-wise dynamics using offline, position-specific quantization parameters for AdaLN inputs.
    *   Applies separate parameters for sink tokens within linear layers.
    *   Leverages fixed sequence lengths to ensure zero runtime overhead.

---

## Results

*   **Supported Models:** Validated on large-scale models ranging from **1B to 3B parameters**:
    *   VAR-d30 (2B)
    *   RAR-XXL (1.5B)
    *   MAR-Huge (1B)
    *   PAR (3B)
*   **Efficiency:**
    *   Operates as a pure PTQ approach (no retraining required).
    *   The STWQ module introduces **no additional calibration overhead**.
*   **Accuracy:** Achieves visual generation accuracy comparable to full-precision (FP) models.
*   **Latency Context:** The baseline PAR-3B model has an inference latency of **> 3 seconds per image**; PTQ4ARVG enables compression of this model to facilitate deployment.

---

## Contributions

*   **First Comprehensive Analysis:** Provides the first in-depth analysis of quantization difficulties specific to the ARVG model family.
*   **Novel Framework:** Introduces PTQ4ARVG, the first effective training-free PTQ framework for ARVGs.
*   **Algorithmic Innovations:**
    *   **GPS:** A mathematically rigorous scaling method.
    *   **STWQ:** An efficient static calibration method.
    *   **DGC:** An entropy-based sample selection strategy.
*   **State-of-the-Art Performance:** Demonstrates that SOTA visual generation can be maintained at **6-bit precision**, facilitating deployment on resource-constrained hardware.

---

**References:** 40 citations
**Quality Score:** 9/10