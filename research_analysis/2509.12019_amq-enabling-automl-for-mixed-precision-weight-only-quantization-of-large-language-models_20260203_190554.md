---
title: 'AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large
  Language Models'
arxiv_id: '2509.12019'
source_url: https://arxiv.org/abs/2509.12019
generated_at: '2026-02-03T19:05:54'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models

*Sangjun Lee; Seung-taek Woo; Jungyu Jin; Changhun Lee; Eunhyeok Park*

**Quality Score:** 8/10 | **References:** 40 citations

---

### âš¡ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Optimization Time** | ~2.8 hours (Single A100 GPU) |
| **Search Space** | > $10^{100}$ configurations |
| **Models Tested** | Llama-2 7B, Llama-2 13B |
| **Core Algorithm** | NSGA-II (Genetic Algorithm) |
| **Key Achievement** | 60.57% Accuracy @ 3.0-bit (vs GPTQ 3-bit @ 57.24%) |

---

> **EXECUTIVE SUMMARY**
>
> Deploying Large Language Models (LLMs) on resource-constrained hardware requires effective quantization to reduce memory footprint and latency. While mixed-precision quantization offers superior theoretical performance over uniform methods by assigning different bit-widths to different layers, it presents a formidable computational challenge. The search for optimal layer-wise bit-width assignments is a discrete combinatorial optimization problem that is NP-complete, with a search space exceeding $10^{100}$ configurations. Consequently, standard optimization techniques are infeasible, and existing fixed-precision methods like GPTQ or AWQ fail to account for the varying sensitivity of different layers, resulting in suboptimal accuracy-efficiency trade-offs.
>
> To address the computational intractability of mixed-precision search, the authors introduce **AMQ**, an AutoML framework designed for automated, weight-only quantization. AMQ employs a four-pronged methodology to navigate the vast search space efficiently. It utilizes **Search Space Pruning** to eliminate non-viable configurations and a **Quantization Proxy** to estimate quantization effects without performing expensive format conversions. Furthermore, the framework implements a **Quality Predictor** to approximate model accuracy, minimizing the need for full validation runs, and an **Iterative Search-and-Update** strategy using the NSGA-II genetic algorithm. This approach allows AMQ to rapidly converge on Pareto-optimal solutions while addressing hardware-awareness issues related to irregular data access patterns inherent in mixed-precision formats.
>
> Empirical evaluations on Llama-2 7B and 13B models demonstrate that AMQ drastically reduces the search overhead to approximately **2.8 hours** on a single A100 GPU. Regarding model performance, AMQ achieved an average zero-shot accuracy of **60.57%** on the Llama-2-13B model at an average bit-width of **3.0**. This result substantially outperforms the fixed-precision GPTQ 3-bit baseline (**57.24%**) and approaches the performance of the heavier GPTQ 4-bit configuration (**61.12%**). Furthermore, at a **3.5-bit** average width, AMQ exceeded the accuracy of GPTQ 4-bit, confirming its ability to optimize the accuracy-efficiency trade-off. The method also delivered competitive inference speeds (Tokens/s) and maintained superior WikiText-2 perplexity scores compared to uniform baselines.
>
> AMQ represents a significant advancement in the practical scalability of LLM deployment by solving the computational bottleneck associated with mixed-precision quantization. By providing a cost-effective, automated pipeline that reduces search time from months to hours, this work enables the generation of compact, high-performing models that were previously computationally prohibitive to discover. The ability to rapidly identify Pareto-optimal configurations paves the way for broader adoption of LLMs on edge devices and consumer hardware, establishing a new standard for hardware-aware AutoML in model compression.

---

## Key Findings

*   **Infeasibility of Standard Optimization:** The combinatorial search space for mixed-precision quantization is vast, exceeding $10^{100}$ configurations, rendering standard optimization techniques infeasible.
*   **Computational Efficiency:** The AMQ framework significantly reduces computational overhead by utilizing prior knowledge and quantization proxies.
*   **Pareto Optimality:** AMQ successfully identifies configurations that offer the best trade-off (Pareto frontier) between memory usage and performance quality.
*   **Convergence Strategy:** The framework enables rapid and stable convergence through a quality predictor and an iterative search-and-update strategy.
*   **Validation of Mixed Precision:** Results confirm that mixed precision is necessary due to the varying sensitivity of layers, outperforming fixed-precision baselines like GPTQ and AWQ.

---

## Methodology

The AMQ framework employs a four-pronged automated approach to efficiently navigate the optimization landscape:

1.  **Search Space Pruning:**
    *   Excludes unpromising configurations early in the process to reduce the problem size.
2.  **Quantization Proxy:**
    *   Bypasses expensive format conversions by using proxy metrics to estimate the effects of quantization.
3.  **Quality Prediction:**
    *   Utilizes a predictor model to estimate final model quality, minimizing the evaluation overhead associated with full validation runs.
4.  **Iterative Optimization:**
    *   Implements a search-and-update strategy (specifically NSGA-II) to efficiently navigate the remaining search space and refine solutions.

---

## Technical Details

| Aspect | Specification |
| :--- | :--- |
| **Core Objective** | Identify optimal layer-wise quantization bit-width assignments for LLMs to maximize accuracy under memory constraints. |
| **Problem Formulation** | Discrete combinatorial optimization task. |
| **Complexity Class** | NP-complete (Search space > $10^{100}$). |
| **Algorithm Used** | NSGA-II (Non-dominated Sorting Genetic Algorithm II). |
| **Hardware Awareness** | Addresses issues regarding irregular data access patterns common in mixed-precision formats. |
| **Comparison Targets** | Fixed-precision methods (GPTQ, AWQ). |

### Key Innovations
*   **Search Space Pruning:** Reduces the initial scope of the search.
*   **Quantization Proxy:** Avoids costly on-the-fly format conversions.
*   **Quality Predictor:** Estimates accuracy without full model inference.
*   **Iterative Search-and-Update:** Refines the search iteratively for convergence.

---

## Contributions

*   **Framework Introduction:** Introduced the **AMQ Framework**, an AutoML solution specifically designed for mixed-precision weight-only quantization.
*   **Scalability Solution:** Solved the computational intractability of the search space through advanced pruning and proxy mechanisms.
*   **Pipeline Efficiency:** Developed a cost-effective evaluation pipeline that leverages quality predictors and iterative updates to speed up the search.
*   **Empirical Validation:** Demonstrated the ability to reach the Pareto frontier, generating compact and high-performing LLMs in a practical timeframe.

---

## Results

### Experimental Setup
*   **Models:** Llama 2 13B and 7B.
*   **Datasets:** ARC-Easy, ARC-Challenge, PIQA, HellaSwag, WinoGrande, BoolQ, WikiText-2.
*   **Metrics:** Memory Usage (GB), Average Zero-shot Accuracy (%), WikiText-2 Perplexity, Inference Speed (Tokens/s).

### Performance Highlights
*   **Search Overhead:** Reduced to approximately **2.8 hours** on a single A100 GPU.
*   **Accuracy vs. Baseline:**
    *   At **3.0-bit** average width, AMQ achieved **60.57%** accuracy, significantly beating GPTQ 3-bit (**57.24%**).
    *   At **3.5-bit** average width, AMQ exceeded the accuracy of GPTQ 4-bit (**61.12%**).
*   **Efficiency:** Achieved higher zero-shot accuracy at lower memory bits with superior inference speeds compared to uniform baselines.