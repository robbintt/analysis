---
title: 'The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects
  on User Trust, Exploration, and Workflows'
arxiv_id: '2505.21512'
source_url: https://arxiv.org/abs/2505.21512
generated_at: '2026-01-28T00:59:36'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows

*Harry Li, Kenneth Alperin, Ashley Suh, Gabriel Appleby*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Citations** | 40 |
| **Study Participants** | 14 Practitioners |
| **System Prototyped** | LinkQ |
| **Datasets Used** | Wikidata, BRON |
| **Core Focus** | Trust Calibration & Workflow Visualization |

---

## Executive Summary

This research addresses the critical limitations of using Large Language Models (LLMs) as opaque "black box" systems for querying knowledge graphs, particularly the risks of hallucination and lack of transparency in reasoning processes. Traditional text-only interfaces restrict users to linear question-and-answer workflows, hindering complex exploratory analysis and making data validation difficult. As practitioners struggle to distinguish between accurate and fabricated outputs, there is a pressing need for mechanisms that support non-linear discovery and expose the reasoning paths behind AI-generated content to mitigate both blind trust and excessive skepticism.

The key innovation is **LinkQ**, a "Glass Box" system that hybridizes an LLM with an interactive Knowledge Graph visualization interface. The architecture utilizes a chained prompting protocol where the LLM performs intent inference and constructs queries for RDF-based Knowledge Graphs, supported by a message relay system for data retrieval. By providing multimodal outputsâ€”text for direct answers and dynamic graph visualizations for structural contextâ€”the system enables human-in-the-loop workflows that allow users to preview the LLM's query logic before execution and inspect underlying data connections, shifting from passive consumption to active verification and iterative refinement.

A controlled study with 14 practitioners using Wikidata and BRON datasets revealed that users significantly shifted from linear inquiry to non-linear, exploratory workflows, utilizing visualizations primarily as a verification mechanism to check accuracy. The integration of graph visualizations led to a substantial increase in user trust compared to text-only baselines by exposing reasoning paths and enabling validation. However, the study also identified a nuanced risk of "overtrust," where users placed excessive confidence in incorrect visual results, highlighting the need for precise interface design to balance clarity with error signaling.

This research contributes to the field by establishing a taxonomy of LLM-KG workflows and providing empirical evidence on how visualization influences trust calibration in generative AI. It offers evidence-based design guidelines for 'Glass Box' architectures that render reasoning processes auditable through visual inspection of data structures. By validating the role of visualization in sense-making and context exploration, this work provides a scientific foundation for future hybrid interfaces that prioritize user-centric verification, ultimately making AI-assisted knowledge discovery more transparent and reliable for practitioners.

---

## Key Findings

*   **Visualization as Verification:** Users primarily utilize Knowledge Graph visualizations to verify the accuracy and relevance of LLM outputs, rather than blindly trusting the text generation.
*   **Shift to Non-Linear Workflows:** The integration of visualization transforms user behavior from linear, question-answer interactions to non-linear, exploratory search processes. Users alternate between high-level overviews and deep dives into specific data nodes.
*   **Transparency Drives Trust:** The ability to see underlying graph connections significantly increases user trust compared to text-only interfaces. This transparency enables users to validate the system's reasoning process.
*   **Role Distinction:**
    *   **Text-Based Chat:** Preferred for efficiency and obtaining direct answers.
    *   **Visualization:** Critical for sense-making, uncovering related concepts, and understanding the broader context of the data.

---

## Methodology

The researchers employed a mixed-method approach to evaluate the effectiveness of the hybrid system:

*   **Prototype System:** A controlled user study was conducted using a prototype system that integrates an LLM with a Knowledge Graph visualization interface.
*   **Comparative Analysis:** The study compared different interface modalities, specifically contrasting text-only chat interactions against a hybrid chat-and-visualization interface.
*   **Data Collection:**
    *   **Qualitative Measures:** Semi-structured interviews and think-aloud protocols were used to gather user insights.
    *   **Behavioral Analysis:** Interaction logs were analyzed to understand user strategies, trust calibration, and workflow adaptations.
*   **Tasks:** Participants engaged in targeted queries, open-ended exploration, and specific failure case scenarios to test the robustness of the system.

---

## Technical Details

**System Name:** LinkQ

**Overview:**
LinkQ is a Knowledge Graph exploration system designed specifically for human-in-the-loop assessment of LLM-generated queries. It employs an iterative user-centered design process to bridge the gap between natural language intent and structured data retrieval.

**Architecture Components:**
*   **The LLM:** Responsible for intent inference and the construction of queries based on user input.
*   **Backend System:** Handles message relay and manages data retrieval from the graph database.
*   **The User:** Interacts with the system via natural language, guiding the exploration process.

**Key Protocols & Features:**
*   **Chained Prompting Protocol:** Utilized to refine queries and manage complex interactions.
*   **Data Support:** Supports RDF-based Knowledge Graphs.
*   **Design Goals:**
    *   **Iterative Refinement:** Allows users to hone queries progressively.
    *   **Error Handling:** Specific mechanisms to surface potential hallucinations.
    *   **Verification Support:** Provides previews of query logic before execution.
    *   **Multimodal Output:** Capable of rendering text, tables, and dynamic visualizations.

---

## Results

The qualitative study involving 14 practitioners yielded several significant behavioral insights:

*   **Workflow Adaptation:** Users intuitively shifted to non-linear, exploratory workflows when provided with visualization tools.
*   **Trust & Verification:** Visualizations proved critical for sense-making and verification. While they increased trust through transparency, the study noted a potential risk of **"overtrust,"** where users might place too much confidence in incorrect results if visualized persuasively.
*   **Interface Preference:**
    *   **Text:** Preferred for efficiency in direct answer retrieval.
    *   **Visualization:** Essential for understanding context and relationships.
*   **User Feedback:** The system received praise for the intuitiveness of its visualization designs, indicating that the visual abstraction layer was effective in communicating complex graph structures.

---

## Contributions

This paper provides three major contributions to the field of Human-Computer Interaction (HCI) and AI:

1.  **Characterization of LLM-KG Workflows:** Offers a taxonomy describing how users interact with hybrid systems, defining specific roles for visualization (e.g., verification, context exploration).
2.  **Design Guidelines for 'Glass Box' AI:** Provides evidence-based implications for building systems that use graph visualizations to make LLM reasoning processes transparent and auditable.
3.  **Empirical Evidence on Trust:** Details how visual data structures influence user trust in generative AI models, specifically through user-centric verification behaviors and trust calibration.

---
*Analysis based on 40 references.*