# Vision language models have difficulty recognizing virtual objects

*Tyler Tran; Sangeet Khemlani; J. G. Trafton*

---

> ### ðŸ“Š Quick Facts
>
> *   **Models Evaluated:** Idefics2, Llama3, BLIP
> *   **Dataset:** TABLE TEST (64 distinct objects)
> *   **Top Performer:** Idefics2 (63% Accuracy)
> *   **Human Baseline:** ~100%
> *   **Total Queries:** 112,896
> *   **Key Limitation:** Inability to process non-visual entities in spatial reasoning
> *   **P-Value:** < .001 (Statistically significant differences found)

---

## Executive Summary

This research addresses a fundamental limitation in state-of-the-art Vision Language Models (VLMs): their inability to effectively process **"virtual objects"**â€”entities described textually that are absent from the visual input. While VLMs demonstrate proficiency in high-level semantic tasks like automatic captioning, they fail to integrate non-visual entities into spatial reasoning processes. This gap is critical because robust scene comprehension requires the cognitive flexibility to update internal representations and reason about hypothetical relationships within a physical environment, a capability where current architectures are significantly deficient.

The study introduces a novel evaluation metric centered on the concept of virtual object recognition to probe the depth of scene understanding. By utilizing the **TABLE TEST** dataset, the researchers presented models with a base image and textual prompts requiring the visualization of a new, non-present element. This methodology systematically manipulated variables across 112,896 queries to isolate visuospatial reasoning from semantic processing. Key variables included seven distinct prompt formulations (e.g., "pretend" vs. "if"), verb tense (past vs. present), and numerical cues, tested against SOTA architectures including Idefics2, Llama3, and BLIP.

The empirical evaluation revealed a substantial performance disparity between VLMs and human reasoning, with statistically significant deficits across all tested models. Idefics2 achieved the highest accuracy at **63%**, followed by Llama3 at **57%**, while BLIP trailed at **22%** ($p < .001$). Performance was highly sensitive to linguistic framing: "pretend" prompts yielded 51% accuracy compared to only 40% for "if" prompts. Additionally, models reasoned more effectively in the past tense (51%) than the present tense (44%). In contrast, human performance was estimated at **100%**, underscoring that SOTA models struggle to sensibly update their spatial representations when faced with hypothetical scenarios.

This contribution is significant as it clearly delineates the boundary between the semantic prowess of current VLMs and their inadequate visuospatial comprehension. By identifying that models cannot reliably reason about spatial relations involving virtual objects, the paper establishes a challenging new benchmark for future development. This work directs the field toward addressing dynamic scene updating and spatial imagination, suggesting that achieving human-level reasoning in AI will require architectural advancements beyond static visual-semantic alignment.

---

## Key Findings

*   **Processing Deficit:** State-of-the-art VLMs demonstrate an inadequate ability to process virtual objects (objects described textually but not visually present).
*   **Semantic vs. Visuospatial:** While VLMs excel at complex semantic tasks like automatic captioning, their comprehension of visuospatial properties within scenes is significantly limited.
*   **Failure to Update:** VLMs struggle to update their internal representations or reason sensibly about spatial relations when prompted to imagine hypothetical objects within a physical scene.

---

## Methodology

The researchers employed a specific experimental design to probe scene comprehension capabilities:

*   **Concept Definition:** Utilized 'virtual objects'â€”objects referenced in text prompts but absent from the visual inputâ€”as a probe.
*   **Evaluation Protocol:** Presented VLMs with an image depicting a scene paired with a textual prompt requiring the visualization of a new element.
*   **Assessment Criteria:** Evaluated the AI system's ability to update its representation of the scene and reason about the spatial relationships between existing objects and the imagined entities.
*   **Scope:** Systematic evaluations were conducted specifically on state-of-the-art VLM architectures.

---

## Technical Details

*   **Models Evaluated:**
    *   Idefics2
    *   Llama3
    *   BLIP
*   **Dataset:** TABLE TEST (2-object images, 64 distinct objects)
*   **Task:** Virtual object recognition
*   **Configuration:**
    *   Temperature: 0
    *   Random Seed: Fixed
*   **Variables Analyzed:**
    *   7 prompt formulations
    *   Verb tense (Present vs. Past)
    *   Numerical cues (neutral vs. specific)
*   **Total Volume:** 112,896 queries processed

---

## Results

The study yielded statistically significant results highlighting the divide between model and human performance:

*   **Overall Accuracy:**
    *   **Idefics2:** 63%
    *   **Llama3:** 57%
    *   **BLIP:** 22%
    *   *Note: Statistically significant differences found (Friedman test, p < .001).*
*   **Prompt Formulation Impact:**
    *   **'Pretend' prompts:** Highest accuracy (51%)
    *   **'If' prompts:** Lowest accuracy (40%)
    *   *Note: BLIP specifically failed on 'if' prompts (8%). (Friedman test, p < .001).*
*   **Verb Tense Impact:**
    *   **Past tense:** 51%
    *   **Present tense:** 44%
    *   *Note: Past tense performed significantly better (Wilcoxon test, p < .001).*
*   **Baseline Comparison:**
    *   Human performance is estimated at **100%**, significantly outperforming the best VLM.

---

## Contributions

*   **Deficit Identification:** Identified a specific deficit in VLM functionality: the inability to effectively integrate non-visual entities into spatial reasoning processes.
*   **Novel Evaluation Metric:** Introduced a new evaluation metric using virtual object descriptions as a distinct method for testing and benchmarking scene understanding.
*   **Capability Differentiation:** Clearly differentiated capabilities between high-level semantic processing (where VLMs excel) and visuospatial comprehension (where current models fail).

---

**Paper Quality Score:** 8/10  
**References:** 40 citations