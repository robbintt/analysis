---
title: Compression Scaling Laws:Unifying Sparsity and Quantization
arxiv_id: '2502.1644'
source_url: https://arxiv.org/abs/2502.16440
generated_at: '2026-02-03T18:23:32'
quality_score: 8
citation_count: 4
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Compression Scaling Laws: Unifying Sparsity and Quantization

*Elias Frantar; Utku Evci; Wonpyo Park; Neil Houlsby; Dan Alistarh*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 4 Citations
> *   **Optimal Precision:** 4-bit (Weight-Only)
> *   **Method:** Straight-Through Estimator (STE)
> *   **Key Insight:** Compression adheres to unified scaling laws via "effective parameters."

---

## Executive Summary

**Problem**
This research addresses the critical challenge of optimizing the training efficiency of Large Language Models (LLMs) by understanding how different compression techniques fundamentally impact pretraining dynamics. While scaling laws traditionally describe the relationship between parameter count, data, and compute for dense models, there is a lack of a unified theoretical framework to evaluate how aggressive compressionâ€”specifically sparsity and quantizationâ€”affects these laws. This matters because as model size grows, the computational and memory costs of pretraining become prohibitive; without a principled way to compare compression methods during the training phase, researchers may invest resources in techniques that offer suboptimal trade-offs between model size, speed, and final performance.

**Innovation**
The key innovation is the unification of sparsity and quantization under a single scaling law framework through the concept of "effective parameters." The authors extend established theoretical foundations for sparsity to empirically demonstrate that quantization also adheres to predictable scaling behavior. Technically, the approach builds upon the BitNet methodology, employing vanilla Straight-Through Estimator (STE) for training where forward passes utilize quantized weights. The study analyzes both Weight-Only quantization and Full Quantization (weights and activations for non-embedding layers), computing scales dynamically from weight statistics rather than treating them as learned parameters. This methodology allows for a principled, apples-to-apples comparison of diverse compression techniques based on their parameter efficiency multipliers.

**Results**
The study yields several significant empirical findings regarding the efficiency boundaries of model compression. For Weight-Only quantization, 4-bit precision was found to be relatively lossless, while reducing to 1-bit yielded approximately a 2x improvement in parameter efficiency. In contrast, Full Quantization exhibited diminishing returns at lower bitwidths; while stable training was achieved using STE, bit-widths lower than 4-bit were not Pareto optimal under linear speedup assumptions (representing realistic hardware constraints). However, under quadratic speedup assumptions (theoretical idealization), 2-bit quantization showed noticeable improvement. Although 1-bit Full Quantization remained stable during training, it was determined to be far from optimal. The authors also confirmed that scaling law fits remain reasonable for models utilizing more than 1-bit per parameter.

**Impact**
This work significantly influences the field by providing a robust theoretical lens through which to view model compression, shifting the focus from isolated optimization tricks to systemic scaling behavior. By validating that compression techniques follow unified scaling laws, the paper enables researchers to predict the performance impact of quantization and sparsity before committing to expensive pretraining runs. The findings offer practical guidance for future LLM development: they validate the efficiency of weight-only 4-bit quantization and caution against full quantization below 4-bit in realistic hardware scenarios, thereby helping to streamline the design of more efficient and scalable foundation models.

---

## Key Findings

*   **Extension of Scaling Laws:** The "effective parameter" scaling pattern extends to quantization techniques.
*   **Weight-Only Efficiency:** Weight-only quantization demonstrates strong parameter efficiency multipliers.
*   **Diminishing Returns for Full Quantization:** Full quantization results in diminishing returns at lower bitwidths.
*   **Unified Framework:** Diverse compression techniques can be unified under a common scaling law framework.

---

## Contributions

*   **Unified Theory:** Established a theoretical and empirical unification of sparsity and quantization under a single scaling law framework.
*   **Principled Comparison:** Enabled a principled methodology for comparing and combining different compression techniques.
*   **Quantization Characterization:** Provided specific insights into the scaling behavior of quantization, distinguishing the efficiency of weight-only approaches from the limitations of full quantization at low bitwidths.

---

## Methodology

The researchers investigated the scaling behavior of Large Language Models (LLMs) during the pretraining phase. Their approach involved building upon established theoretical foundations regarding weight sparsity to empirically test and compare the effects of different compression methodsâ€”specifically weight/activation quantization and weight sparsityâ€”on model scaling.

---

## Technical Details

*   **Training Approach:** Follows BitNet methodology; models are trained via vanilla Straight-Through Estimator (STE) where forward passes use quantized weights.
*   **Scale Computation:** Scales are computed dynamically from weight statistics rather than being learned parameters.
*   **Scope of Compression:**
    *   *Weight-Only Quantization:* Included.
    *   *Full Quantization:* Weights and activations for inputs of linear, non-embedding layers (excluding the attention mechanism).
*   **Evaluation Metrics:**
    *   Model size measured in total bits.
    *   Speedup calculated using linear (hardware reality) or quadratic (theoretical) counting methods.

---

## Results

*   **Validation Loss:** Curves confirm scaling laws are data independent.
*   **Weight-Only Performance:** 4-bit quantization is relatively lossless; dropping to 1-bit yields approximately a 2x improvement in parameter efficiency.
*   **Full Quantization Stability:** Allows stable training with STE.
*   **Pareto Optimality:**
    *   *Linear Speedup Assumptions:* Bit-widths lower than 4-bit are not Pareto optimal.
    *   *Quadratic Speedup Assumptions:* 2-bit provides noticeable improvement.
*   **1-bit Assessment:** Although 1-bit is far from optimal, it trains stably.
*   **Model Fit:** Empirical scaling law fits are reasonable for models using more than 1-bit per parameter.