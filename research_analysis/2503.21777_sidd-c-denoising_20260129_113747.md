---
title: SIDD-C denoising
arxiv_id: '2503.21777'
source_url: https://arxiv.org/abs/2503.21777
generated_at: '2026-01-29T11:37:47'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# SIDD-C denoising

*Jiahao Xie, Federico Tombari, Bernt Schiele, Nathalie Rauschmayr, Alessio Tonioni*

***

> ### ⚡ Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Proposed Method:** Visual In-Context Tuning (VICT)
> *   **Key Mechanism:** Role Flipping & Cycle Consistency
> *   **Optimization:** No gradient updates or parameter changes required
> *   **Scope:** High-level (segmentation) & Low-level (restoration) vision
> *   **References:** 40 Citations

---

## Executive Summary

This research addresses the critical issue of poor generalizability in Visual In-Context Learning (VICL) models when faced with distribution shifts at test time. While VICL models, such as Painter, have shown promise by formulating visual tasks as image inpainting problems, they often fail to maintain performance when test data deviates from the training distribution. This limitation is significant because it restricts the deployment of these models in real-world environments where input data is subject to various corruptions and environmental variations, affecting both high-level understanding tasks (like segmentation) and low-level image processing (like denoising).

The authors propose **Visual In-Context Tuning (VICT)**, a novel method designed to adapt existing VICL models to new distributions entirely at test time. The key innovation is a "**Role Flipping**" mechanism, which adapts the model on-the-fly using a single test sample by inverting the roles of the task prompts and the test sample itself. VICT employs a cycle consistency loss to verify the adaptation: the model must successfully reconstruct the original task prompt’s output from the test sample, which indicates the model has acquired awareness of the new data distribution. Crucially, this process requires no gradient updates or parameter modifications, making efficient use of the model's existing in-context capabilities.

Evaluations across 15 common corruptions demonstrate that VICT consistently improves robustness over baseline models. In low-level image processing, Zero-shot PSNR for SIDD-C Denoising improved from 23.22 dB to 24.56 dB, while Rain-C Deraining saw an increase from 17.97 to 19.92. LoL-C Low-light Enhancement also gained, moving from 16.15 to 16.94 PSNR. In high-level tasks, ADE20K-C Semantic Segmentation (mIoU) improved from 28.2% to 29.3% in Zero-shot settings and from 25.8% to 27.0% in One-shot settings. Additionally, COCO-C Panoptic Segmentation (PQ) showed a One-shot improvement from 25.3% to 26.4%.

The significance of VICT lies in its ability to bridge the gap between the theoretical flexibility of in-context learning and the practical demands of robust, real-world application. By providing a mechanism for test-time adaptation that does not require computationally expensive retraining or architectural changes, the authors offer a versatile tool for enhancing model reliability.

---

## Key Findings

*   **Robustness Enhancement:** VICT successfully addresses the generalizability gap in VICL models under distribution shifts without requiring model retraining.
*   **Versatility:** The method is effective across both high-level visual understanding (segmentation) and low-level image processing (denoising, deraining, enhancement).
*   **Efficiency:** The adaptation process computationally efficient, utilizing the model's in-context capabilities without gradient updates or parameter changes.
*   **Performance Gains:** Consistent improvements were observed in Zero-shot settings across 15 common corruptions, including significant PSNR gains in denoising and mIoU improvements in segmentation.

---

## Technical Details

### Core Methodology
The proposed method, **Visual In-Context Tuning (VICT)**, operates on top of existing Visual In-Context Learning (VICL) models like Painter. It aims to handle distribution shifts at test time through a unique adaptation strategy.

### Key Mechanisms

*   **Role Flipping:**
    *   Adapts the model on-the-fly using a single test sample.
    *   Mechanism: Inverts the standard roles of "task prompts" and the "test sample."
*   **Cycle Consistency Loss:**
    *   Used to verify and guide the adaptation.
    *   Logic: Successfully reconstructing the original task prompt’s output *from* the test sample indicates the model has understood the new distribution.

### System Constraints
*   **Parameter Updates:** None required (no gradient updates).
*   **Architectural Changes:** None required (works on top of existing VICL models).
*   **Input:** Single test sample required for adaptation.

---

## Performance Metrics

Robustness was evaluated using 15 common corruptions.

### Low-Level Image Processing (PSNR)

| Task | Setting | Baseline Score | **VICT Score** | Delta |
| :--- | :--- | :--- | :--- | :--- |
| **SIDD-C Denoising** | Zero-shot | 23.22 dB | **24.56 dB** | +1.34 |
| SIDD-C Denoising | One-shot | 23.12 dB | 23.08 dB | -0.04 |
| **Rain-C Deraining** | Zero-shot | 17.97 | **19.92** | +1.95 |
| Rain-C Deraining | One-shot | 17.77 | 17.79 | +0.02 |
| **LoL-C Low-light** | Zero-shot | 16.15 | **16.94** | +0.79 |
| LoL-C Low-light | One-shot | 15.93 | 15.90 | -0.03 |

### High-Level Visual Understanding (mIoU / PQ)

| Task | Metric | Setting | Baseline Score | **VICT Score** |
| :--- | :--- | :--- | :--- | :--- |
| **ADE20K-C Segmentation** | mIoU | Zero-shot | 28.2% | **29.3%** |
| ADE20K-C Segmentation | mIoU | One-shot | 25.8% | **27.0%** |
| **COCO-C Panoptic Seg.** | PQ | One-shot | 25.3% | **26.4%** |