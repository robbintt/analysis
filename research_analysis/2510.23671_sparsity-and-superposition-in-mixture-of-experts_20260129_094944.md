# Sparsity and Superposition in Mixture of Experts

*Marmik Chaudhari; Jeremi Nuer; Rome Thorstenson*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 23 References |
| **Core Metric** | Feature Norm ($\|W_i\|$), Interference ($\langle \vec{f_i}, \vec{f_j} \rangle$) |
| **Key Distinction** | Feature Sparsity vs. Network Sparsity |
| **Visual Indicator** | üü¢ Green (Superposition) / üü£ Purple (Monosemantic) |

---

## üìã Executive Summary

> This paper addresses the challenge of **mechanistic interpretability** in Mixture of Experts (MoE) architectures, specifically investigating whether these sparse models suffer from the same representational instability as dense networks. In dense models, the limitation of available dimensions forces the reliance on "superposition"‚Äîa packing strategy where non-orthogonal feature vectors ($|F| > m$) coexist, leading to computational instability and sudden phase changes.

The authors provide a theoretical innovation by extending the Linear Representation Hypothesis (Elhage et al., 2022) to rigorously distinguish between **"feature sparsity"** (data properties) and **"network sparsity"** (architectural properties). They propose that MoEs leverage network sparsity to assign experts to specific feature combinations, thereby avoiding the non-orthogonal packing that plagues dense models.

The results demonstrate that **network sparsity** is the primary driver of representation in MoEs, yielding significantly less superposition than dense baselines. Unlike dense models, which exhibit discontinuous phase changes, MoEs display continuous and stable transitions. These findings fundamentally redefine "expert specialization," shifting the focus from load balancing efficiency to monosemantic feature representation. By proving that high interpretability and stability can be achieved without degrading capability, the paper suggests a clear pathway for designing inherently explainable large language models.

---

## üîë Key Findings

*   **Network Sparsity is the Primary Driver:** MoE models are best characterized by network sparsity rather than feature sparsity or feature importance.
*   **Lack of Phase Changes:** Unlike dense models, MoE models do **not** exhibit discontinuous phase changes caused by feature sparsity or feature importance; they show continuous transitions.
*   **Link to Monosemanticity:** Models with higher network sparsity exhibit **greater monosemanticity**.
*   **Natural Organization:** Experts naturally organize themselves around coherent feature combinations when initialized appropriately, reducing reliance on load balancing heuristics.
*   **Interpretability without Trade-off:** Network sparsity enables higher interpretability **without sacrificing model capability**.

---

## ‚öôÔ∏è Methodology

The research team employed a comparative analytical approach to understand the internal mechanics of MoE architectures versus dense baselines.

*   **New Metrics Development:** Created specific tools to measure superposition within the context of expert routing and activation.
*   **Sparsity Analysis:** Investigated the impact of different forms of sparsity (feature sparsity vs. network sparsity) on internal representations.
*   **Phase Change Investigation:** Specifically studied the emergence of monosemantic features and the presence (or absence) of phase changes compared to dense networks.

---

## üìê Technical Details

The study extends the framework established by **Elhage et al. (2022)** to Mixture of Experts (MoE) architectures.

### Core Concepts
*   **Linear Representation Hypothesis:** Relies on the hypothesis where hidden states are linear combinations of feature vectors.
*   **Superposition Definition:** Formally defined as the state where the number of features exceeds available dimensions ($|F| > m$), necessitating non-orthogonal packing.
*   **Sparsity Distinction:**
    *   **Feature Sparsity:** Characteristics of the data.
    *   **Network Sparsity:** Characteristics of the architecture.
*   **Mechanism:** Experts utilize network sparsity to reduce reliance on the non-orthogonal packing required in dense models.

### Evaluation Metrics
*   **Feature Norm ($\|W_i\|$):** Used to track individual feature strength.
*   **Interference ($\langle \vec{f_i}, \vec{f_j} \rangle$):** Used to calculate cross-talk between features.
*   **Visual Categorization:**
    *   <span style="color:green">Green</span>: Represents superposition.
    <span style="color:purple">Purple</span>: Represents monosemantic features.
*   **Analysis Methods:** Histogram analysis of feature norms and sparsity ratios.

---

## üß™ Results

*   **Reduced Superposition:** MoEs exhibit less superposition than dense models by dedicating experts to specific feature combinations.
*   **Stability:** MoEs do not display discontinuous phase changes in superposition, showing more continuous transitions instead.
*   **Natural Expert Organization:** Experts naturally organize around coherent feature combinations, suggesting that complex load balancing heuristics may be less critical than previously thought.
*   **Sparsity & Monosemanticity:** Higher network sparsity enables greater monosemanticity without sacrificing capability.
*   **Low Interference:** Interference metrics remain low in MoE architectures, establishing a quantitative link between sparsity and the clarity of specific concepts.

---

## üìù Contributions

1.  **Theoretical Reframing:** Established network sparsity as the critical factor for understanding mechanistic differences in MoEs.
2.  **Novel Metrics:** Introduced specific quantitative tools to measure superposition in expert routing.
3.  **Redefinition of Specialization:** Proposed a new definition of 'expert specialization' rooted in monosemantic feature representation rather than load balancing.
4.  **Practical Implications:** Provided evidence that highly sparse MoE models can be inherently more interpretable, reducing the need for complex auxiliary loss functions.