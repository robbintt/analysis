---
title: 'CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale'
arxiv_id: '2507.05178'
source_url: https://arxiv.org/abs/2507.05178
generated_at: '2026-02-03T13:03:49'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale

*Jonathan Hyun; Nicholas R Waytowich; Boyuan Chen*

---

### üìä Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Simulation Scale** | Up to 2,000 agents |
| **Map Complexity** | 1 million+ cells |
| **Agent Types** | 4 (Firefighters, Bulldozers, Drones, Helicopters) |
| **Platform** | Unity (CREW Platform) |
| **Fire Model** | Cellular Automata (incorporating slope, moisture, wind) |

---

> ### üìù Executive Summary
>
> Current state-of-the-art (SOTA) LLM-based multi-agent frameworks are theoretically designed for complex collaboration, yet their ability to function in real-world scenarios remains largely untested due to a lack of adequate evaluation tools. Existing benchmarks are frequently inadequate, typically focusing on small-scale, deterministic environments that fail to capture the intricacies of real-world operations.
>
> This paper addresses the critical need for rigorous testing environments that introduce **partial observability**, **stochastic dynamics**, and **heterogeneous agent requirements**. The authors highlight that without such benchmarks, it is impossible to accurately assess system robustness or the true viability of these models in handling long-horizon planning and large-scale coordination under uncertainty.
>
> The researchers introduce **CREW-Wildfire**, an open-source benchmark built on the Unity-based CREW simulation platform. Empirical testing revealed substantial performance deficiencies in current SOTA models, specifically in complex coordination, communication, and spatial reasoning. By open-sourcing all code and data, this work establishes a reproducible standard and identifies the critical technical barriers that must be overcome to advance multi-agent systems from theory to practice.

---

## üîç Key Findings

*   **Performance Deficiencies:** Evaluations of SOTA LLM-based multi-agent frameworks revealed substantial deficiencies in the new benchmark, indicating current models cannot handle complex, real-world scale coordination.
*   **Unsolved Challenges:** The study highlighted critical gaps in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty.
*   **Inadequacy of Existing Benchmarks:** Current benchmarks were found lacking, typically focusing on small-scale or low-complexity domains that do not stress-test system robustness.
*   **Necessity of Complexity:** Introducing **partial observability**, **stochastic dynamics**, and **heterogeneous agents** is essential for accurately testing and validating system performance.

---

## üõ†Ô∏è Methodology

The researchers introduced **CREW-Wildfire**, an open-source benchmark built upon the CREW simulation platform. The methodology is characterized by the following components:

*   **Procedural Generation:** Utilization of procedurally generated wildfire response scenarios featuring large maps to ensure diverse testing conditions.
*   **Environmental Realism:** Introduction of realistic complexity through partial observability and stochastic dynamics.
*   **Modular Architecture:** Implementation of a system supporting dual interaction modes:
    *   **Low-level control**
    *   **High-level natural language interactions**
*   **Empirical Testing:** Several SOTA LLM-based multi-agent frameworks were implemented and empirically tested within the environment to measure performance against long-horizon planning objectives.

---

## ‚öôÔ∏è Technical Details

The technical infrastructure of CREW-Wildfire is designed to support massive scale and high fidelity:

### Simulation Environment
*   **Base Platform:** Built on the **CREW platform** using **Unity**.
*   **Terrain Generation:** Uses procedural generation via **Perlin noise** to create realistic terrain and settlements.
*   **Fire Propagation Model:** Employs a **Cellular Automata** model that incorporates physical vectors including slope, moisture, and wind.

### Agents & Architecture
*   **Heterogeneous Agent Types:** Four distinct classes requiring mandatory coordination:
    *   Firefighters
    *   Bulldozers
    *   Drones
    *   Helicopters
*   **System Modules:**
    *   **Perception Module:** Translates raw simulation data into text summaries.
    *   **Execution Module:** Handles the translation of actions back to the environment.
*   **Scalability:** The architecture supports diverse observation and action spaces, scalable to:
    *   **2,000+ Agents**
    *   **1 Million+ Cell Maps**

---

## üìà Results

The outcomes of the benchmarking tests highlight both the capabilities of the simulation environment and the limitations of current AI models:

*   **Framework Failures:** SOTA LLM-based multi-agent frameworks demonstrated substantial deficiencies in handling complex coordination, large-scale communication, spatial reasoning, and long-horizon planning.
*   **Benchmark Validation:** The system successfully validated its ability to run massive simulations, handling up to **2,000 agents** on maps with over **1 million cells**.
*   **Complexity Impact:** Analysis confirms that existing benchmarks are inadequate due to small scale. Unlike these simpler tests, CREW-Wildfire's introduction of partial observability and stochastic dynamics caused significant performance drops in current models.

---

## üöÄ Contributions

This research provides several significant advancements to the field of multi-agent systems:

*   **Advanced Benchmarking Tool:** The release of CREW-Wildfire bridges the gap between simple academic environments and real-world multi-agent tasks.
*   **Scalable Architecture:** Established a scalable architecture and behavioral evaluation metrics that specifically account for uncertainty and partial observability.
*   **Open Source Commitment:** All code, environments, data, and baselines were open-sourced to ensure full reproducibility and support community iteration.
*   **Defining Technical Barriers:** By exposing the limitations of current SOTA models, the paper defines critical barriers‚Äîspecifically in communication, spatial reasoning, and coordination‚Äîthat must be overcome to advance the field.

---

**References:** 40 citations
**Quality Score:** 9/10