---
title: Continual Knowledge Consolidation LORA for Domain Incremental Learning
arxiv_id: '2510.16077'
source_url: https://arxiv.org/abs/2510.16077
generated_at: '2026-02-03T20:09:05'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Continual Knowledge Consolidation LORA for Domain Incremental Learning

*Naeem Paeedeh; Mahardhika Pratama; Weiping Ding; Jimmy Cao; Wolfgang Mayer; Ryszard Kowalczyk*

---

> ### ðŸ“Š Quick Facts
> * **Method:** CONEC-LoRA (Continual Knowledge Consolidation LoRA)
> * **Focus:** Domain Incremental Learning (DIL) & Parameter-Efficient Fine-Tuning (PEFT)
> * **Performance:** >5% accuracy improvement over SOTA
> * **Quality Score:** 9/10
> * **Citations:** 40 references

---

## Executive Summary

This research addresses the critical challenge of **Domain Incremental Learning (DIL)**, where models must learn sequentially from data streams without succumbing to *catastrophic forgetting*. While Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) are popular for adapting large models, existing continual learning approaches often fail to distinguish between domain-specific information and cross-domain shared knowledge. This limitation results in suboptimal performance and reduced plasticity.

The authors propose **CONEC-LoRA**, a parameter-efficient framework utilizing a **dual-LoRA strategy** to explicitly separate task-shared and task-specific knowledge. The architecture introduces a **stochastic classifier** that samples weight parameters from a learned distribution, creating flexible decision boundaries that outperform traditional linear or prototype-based classifiers. Additionally, the system employs an auxiliary network for dynamic LoRA selection during inference and utilizes a gradient redistribution strategy to mitigate forgetting.

Evaluations across four popular benchmark datasets demonstrate that CONEC-LoRA outperforms prior state-of-the-art methods, achieving **accuracy improvements of over 5%**. This research significantly advances the field by resolving the issue of ignored shared knowledge in previous PEFT methods, offering a scalable solution for dynamic environments.

---

## Key Findings

*   **Superior Performance:** The proposed CONEC-LoRA method outperforms prior state-of-the-art methods across four popular benchmark problems, achieving accuracy improvements of **over 5%**.
*   **Effective Knowledge Consolidation:** Combining task-shared and task-specific LoRAs effectively addresses catastrophic forgetting while successfully capturing common knowledge across domains.
*   **Enhanced Generalization:** The implementation of a **stochastic classifier** significantly enhances generalization capabilities compared to traditional linear or prototype-based classifiers.
*   **Improved Inference Accuracy:** The use of an auxiliary network for LoRA selection, combined with a different-depth network structure with local classifiers, improves inference accuracy by leveraging intermediate representations.

---

## Methodology

The paper introduces **CONEC-LoRA** (Continual Knowledge Consolidation LoRA), a parameter-efficient framework designed specifically for Domain Incremental Learning (DIL). The core methodology relies on a hybrid architecture comprising several innovative components:

1.  **Dual-LoRA Strategy:** Utilizes a task-shared LoRA to extract common knowledge and a task-specific LoRA to retain domain-specific information.
2.  **Stochastic Classification:** Employs a classifier where parameters are sampled from a distribution to increase classification probability and flexibility.
3.  **Auxiliary Selection:** Deploys an auxiliary network to optimally predict which task-specific LoRA should be used during inference.
4.  **Bias Mitigation:** Integrates a ball-generator loss and transformation module to address synthetic sample bias within a different-depth network structure.

---

## Technical Details

The technical architecture of CONEC-LoRA combines stochastic classification, gradient redistribution, and an auxiliary domain selection network.

### Core Architecture
*   **Stochastic Classifier:** Utilizes parameters $\phi_j = \{\mu_j, \sigma_j\}$ to sample weight vectors, allowing for flexible decision boundaries.
*   **Backbone:** Features a frozen ViT backbone with independent linear classifiers attached to intermediate layers.
*   **Transformation Module:** Employs a two-layer MLP transformation module to predict domain labels.

### Forgetting Mitigation
*   **Gradient Redistribution Strategy:** Mitigates catastrophic forgetting by redistributing gradients based on the L2 norm of previous up-projection matrix elements.
*   **Knowledge Distillation:** Calculates loss on `[CLS]` token embeddings with temperature scaling fixed at $\tau = 2$.

### Domain Selection
*   **Auxiliary Domain Classifier:** An independent network structure designed to predict domain labels, theoretically more reliable than non-parametric approaches utilizing Mahalanobis distance.

---

## Contributions

*   **Advanced LoRA Architecture:** Developed a consolidation strategy bridging task-shared and task-specific LoRAs, resolving the issue of ignored shared knowledge in existing PEFT methods.
*   **Improved Generalization:** Introduced a stochastic classifier mechanism offering better generalization power than the suboptimal classifiers currently used in DIL.
*   **Inference and Representation Efficiency:** Proposed a different-depth network structure with local classifiers and an auxiliary network for efficient LoRA selection.
*   **Bias Reduction:** Integrated specific modules (ball-generator loss and transformation module) to effectively mitigate synthetic sample bias.

---

## Evaluation Results

The method was rigorously tested against established benchmarks, yielding significant results:

*   **Accuracy Gains:** Achieved consistent accuracy improvements of **over 5%** compared to prior state-of-the-art methods across four benchmarks.
*   **Classifier Performance:** The stochastic classifier demonstrated enhanced generalization over traditional linear or prototype-based classifiers.
*   **Selection Reliability:** The auxiliary domain prediction network proved theoretically more reliable than non-parametric approaches utilizing Mahalanobis distance.
*   **Hyperparameters:** Key to success was the Softmax Temperature, fixed at $\tau = 2$.

---

**Paper Quality Score:** 9/10 | **References:** 40 Citations