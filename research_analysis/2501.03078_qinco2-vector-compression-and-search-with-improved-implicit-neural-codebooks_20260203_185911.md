---
title: 'Qinco2: Vector Compression and Search with Improved Implicit Neural Codebooks'
arxiv_id: '2501.03078'
source_url: https://arxiv.org/abs/2501.03078
generated_at: '2026-02-03T18:59:11'
quality_score: 9
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Qinco2: Vector Compression and Search with Improved Implicit Neural Codebooks

*ThÃ©ophane Vallaeys; Matthew Muckley; Jakob Verbeek; Matthijs Douze*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Key Achievement:** 34% improvement in reconstruction MSE (BigANN).
> *   **Search Boost:** 24% increase in Recall@1 (Deep1M).
> *   **Model Variants:** QINCo2-S, -M, and -L.
> *   **Core Innovation:** Beam-search encoding & implicit neural codebooks.
> *   **Total Citations:** 26

---

## Executive Summary

Efficient vector compression and rapid nearest neighbor search are critical for managing high-dimensional data in large-scale retrieval systems. However, standard quantization techniques, particularly Residual Quantization (RQ), suffer from rate-distortion inefficiencies because they treat quantization code parts as independent. This failure to model the dependencies between code parts limits reconstruction quality and reduces search accuracy, creating a bottleneck for systems handling billion-scale datasets.

This paper introduces **QINCo2**, an advancement over implicit neural codebook frameworks that captures dependencies between code parts to improve compression. Instead of using static codebooks, QINCo2 computes codewords dynamically as a function of previously selected codes using a parameterized neural function. The authors enhance this approach with a sophisticated encoding strategy that combines "Candidate Pre-selection" to efficiently filter options and "Beam Search" to maintain optimal hypotheses. Additionally, a fast approximate decoder utilizing "Pairwise Additive Decoding" and codeword pairs enables the generation of precise short-lists for search within an Inverted File Index (IVF) structure.

QINCo2 demonstrates significant performance improvements over state-of-the-art baselines, including OPQ, RQ, LSQ, and the original QINCo. On the BigANN dataset, the method achieved a **34% reduction in reconstruction Mean Squared Error (MSE)** for 16-byte vector encodings. In terms of retrieval efficacy, QINCo2 realized a **24% increase in search accuracy (Recall@1)** on the Deep1M dataset with 8-byte encodings. These results were consistently validated across four diverse datasets: Deep1B, BigANN, Facebook SimSearchNet++, and Contriever.

By successfully addressing the rate-distortion inefficiencies found in traditional quantization, QINCo2 sets a new standard for vector compression and similarity search. The ability to model complex inter-code dependencies allows for higher fidelity reconstruction and more accurate retrieval without compromising speed. With the inclusion of optimized training procedures and scalable model architectures (S, M, and L variants), this work provides a robust, state-of-the-art solution for practical billion-scale search applications.

---

## Key Findings

*   **Significant MSE Reduction:** Achieved a **34% improvement** in state-of-the-art reconstruction Mean Squared Error (MSE) for 16-byte vector compression on the BigANN dataset.
*   **Enhanced Search Accuracy:** Demonstrated a **24% increase** in search accuracy using 8-byte encodings on the Deep1M dataset.
*   **Broad Validation:** Successfully validated the approach across four datasets (Deep1B, BigANN, Facebook SimSearchNet++, Contriever), showing outstanding results in vector compression and billion-scale nearest neighbor search.
*   **Dependency Modeling:** Successfully addressed rate-distortion inefficiencies found in standard Residual Quantization (RQ) by effectively modeling dependencies between code parts.

---

## Methodology

The research builds upon the QINCo framework, utilizing a neural network to determine quantization codebooks within a Residual Quantization (RQ) structure. The methodology focuses on capturing the dependencies between code parts that traditional methods miss.

*   **Neural Codebooks:** Utilizes a neural network to generate codebooks dynamically, allowing for context-aware quantization.
*   **Encoding Process:** Implements an enhanced vector encoding process using:
    *   **Codeword Pre-selection:** Efficiently narrows down potential candidates.
    *   **Beam-search:** Maintains multiple hypotheses to find the optimal encoding path.
*   **Decoding & Search:** Employs a fast approximate decoder leveraging 'codeword pairs' to establish accurate short-lists for nearest neighbor search.
*   **Optimization:** Utilizes a specifically optimized training procedure and a refined network architecture to maximize performance.

---

## Contributions

The paper introduces several key advancements to the field of vector quantization and retrieval:

1.  **Advanced Encoding Strategy:** Introduced a new encoding strategy utilizing codeword pre-selection and beam-search specifically designed for implicit neural codebooks.
2.  **Efficient Search Mechanism:** Developed a novel search mechanism via an approximate decoder that uses codeword pairs to generate precise short-lists, significantly speeding up retrieval.
3.  **System Optimization:** Contributed system optimizations including an optimized training procedure and improved network architecture that extends the capabilities of the previous QINCo model (QINCo2).

---

## Technical Details

### Core Architecture
QINCo2 introduces **implicit neural codebooks** where codewords are not stored statically but computed as a function of previously selected codes using a parameterized neural function.

*   **Decoding Function:** Defined as:
    $$F_{QI}(c_1, \dots, c_M) = \sum_{m} f_{\theta_m}(c_m | \hat{x}_{m-1})$$
    This adapts the codebook dynamically based on the reconstruction context.

### Encoding Improvements
*   **Candidate Pre-selection ($Q_{QI-A}$):** Uses an efficient function $g_{\phi_m}$ to select a subset of $A$ candidates from the codebook.
*   **Beam Search ($Q_{QI-B}$):** Maintains $B$ hypotheses during the encoding process to ensure high-quality reconstruction.

### Model Variants
The architecture uses residual MLPs with three distinct size variants:
*   **QINCo2-S:** 2 blocks, 128 embedding dimension.
*   **QINCo2-M:** 4 blocks, 384 embedding dimension.
*   **QINCo2-L:** 16 blocks, 384 embedding dimension.

### Search Pipeline
*   **Structure:** Combines Inverted File Index (IVF) with QINCo2.
*   **Shortlisting:** Uses Additive Quantization (AQ) codebooks for shortlisting.
*   **Decoding:** Employs **Pairwise Additive Decoding** to capture dependencies between code elements efficiently.

---

## Results

The performance of QINCo2 was rigorously tested against strong baselines, including OPQ, RQ, LSQ, QINCo, and UNQ.

### Compression Performance
*   **BigANN Dataset:** Achieved a **34% improvement** in reconstruction MSE with 16-byte encodings.

### Retrieval Performance
*   **Deep1M Dataset:** Achieved a **24% increase** in search accuracy (Recall@1) with 8-byte encodings.
*   **Datasets Tested:** Deep1B, BigANN, Facebook SimSearchNet++, and Contriever.

### Configuration
*   **Codebook Size ($K$):** 256
*   **Training Hyperparameters:** $A=16$, $B=32$
*   **Evaluation Parameters:** $A=32$, $B=64$

---
*References: 26 citations*