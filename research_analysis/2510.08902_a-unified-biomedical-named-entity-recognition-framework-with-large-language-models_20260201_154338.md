# A Unified Biomedical Named Entity Recognition Framework with Large Language Models

*Tengxiao Lv; Ling Luo; Juntao Li; Yanhua Wang; Yuchen Pan; Chao Liu; Yanan Wang; Yan Jiang; Huiyi Lv; Yuanyuan Sun; Jian Wang; Hongfei Lin*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Citations** | 32 |
| **Approach** | LLM-based Generative Framework |
| **Key Innovation** | Symbolic Tagging & Contrastive Selector |
| **Best Performance** | 90.82% F1 (NCBI-disease) |

---

## Executive Summary

**Problem**
Biomedical Named Entity Recognition (BioNER) is critical for extracting structured insights from unstructured clinical texts, yet it remains technically challenging due to the linguistic complexity of biomedical data. Existing methodologies struggle to simultaneously address nested entities, entity boundary ambiguity, and cross-lingual generalization. Because these issues are typically treated in isolation, current models lack the robustness required for real-world deployment across diverse, multilingual, and structurally complex corpora.

**Innovation**
The researchers propose a unified framework that reformulates BioNER as a text generation task using Large Language Models (LLMs), enabling the simultaneous processing of flat and nested entities. To address boundary ambiguity, the authors introduce a **"Symbolic Tagging"** strategy that provides explicit boundary annotations within the generative paradigm. The architecture enhances multilingual versatility through **Bilingual Joint Fine-tuning** across Chinese and English datasets, facilitating cross-lingual transfer without architectural changes. Furthermore, a **Contrastive Learning-based Entity Selector** is implemented to filter predictions, utilizing boundary-sensitive samples to distinguish correct entities from noise.

**Results**
The proposed framework achieved State-of-the-Art (SOTA) F1 scores across four standard benchmark datasets, securing **88.86% on BC5CDR**, **75.33% on JNLPBA**, **90.82% on NCBI-disease**, and **68.69% on the multilingual CMeEE dataset**. In direct comparisons, the integration of the contrastive learning selector significantly improved result quality by filtering incorrect predictions, contributing to a performance lift of over **1%** in F1 scores on key benchmarks by effectively reducing false positives. The model also demonstrated robust zero-shot capabilities, successfully generalizing to unseen corpora and different languages with high precision, validating the efficacy of the generative approach and the bilingual fine-tuning strategy.

**Impact**
This work significantly advances biomedical NLP by demonstrating that a unified, LLM-based generative framework can effectively resolve historically persistent issues such as nested entity recognition and boundary ambiguity. By establishing a method that generalizes across languages without architectural modification while achieving quantifiable SOTA metrics, the research offers a scalable, high-precision path toward global clinical data mining. The successful application of contrastive learning for entity selection sets a new precedent for accuracy in information extraction, potentially reducing reliance on expensive, hand-crafted post-processing rules in future biomedical systems.

---

## Key Findings

*   **SOTA Performance:** Achieved State-of-the-Art (SOTA) results across four standard benchmark datasets.
*   **Zero-Shot Generalization:** Demonstrated robust capabilities in zero-shot settings on unseen corpora and different languages.
*   **Complex Entity Resolution:** Effectively resolves persistent challenges in BioNER regarding nested entities and entity boundary ambiguity.
*   **Enhanced Filtering:** The contrastive learning-based entity selector significantly improved result quality by filtering out incorrect predictions.

---

## Methodology

The proposed approach reformulates BioNER as a text generation task, leveraging the power of Large Language Models (LLMs). The methodology is built on three core pillars:

1.  **Symbolic Tagging Strategy:**
    *   Provides explicit boundary annotations within the generative paradigm.
    *   Essential for handling both flat and nested entities accurately.
2.  **Bilingual Joint Fine-tuning:**
    *   Utilizes both Chinese and English datasets simultaneously during training.
    *   Enhances multilingual capabilities without requiring architectural changes.
3.  **Contrastive Learning Entity Selector:**
    *   Implements a selection mechanism using boundary-sensitive samples.
    *   Filters incorrect predictions to refine the final output.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Core Architecture** | Unified Framework leveraging Large Language Models (LLMs) for Biomedical Named Entity Recognition (BioNER). |
| **Paradigm Shift** | Reformulates NER as a text generation task rather than sequence labeling or span classification. |
| **Entity Handling** | Designed natively to handle nested entities, entity boundary ambiguity, and cross-lingual generalization. |
| **Optimization** | Incorporates a Contrastive Learning-based Entity Selector to distinguish between correct predictions and noise. |

---

## Contributions

1.  **Unified Generative Paradigm:** Introduction of a unified LLM-based framework that handles flat and nested biomedical entities within a generative paradigm.
2.  **Symbolic Tagging Strategy:** Development of a tagging strategy for boundary disambiguation to handle entity boundary ambiguity.
3.  **Cross-Lingual Advancement:** Advancement of multilingual NER through bilingual joint fine-tuning, enabling cross-lingual generalization without architectural changes.
4.  **Contrastive Selection:** Novel application of contrastive learning in the entity selection process to reduce noise and improve extraction precision.

---

## Results

The framework's performance was rigorously validated against standard benchmarks:

*   **Benchmark Scores:**
    *   **NCBI-disease:** 90.82% F1
    *   **BC5CDR:** 88.86% F1
    *   **JNLPBA:** 75.33% F1
    *   **CMeEE (Multilingual):** 68.69% F1
*   **Ablation Study:** The contrastive learning selector provided a measurable lift in performance (over **1%** F1 score improvement) by reducing false positives.
*   **Generalization:** Successfully generalized to unseen data in zero-shot settings, proving the model's versatility.