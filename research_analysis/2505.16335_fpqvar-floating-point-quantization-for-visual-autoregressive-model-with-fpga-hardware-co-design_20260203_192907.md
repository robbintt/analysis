---
title: 'FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA
  Hardware Co-design'
arxiv_id: '2505.16335'
source_url: https://arxiv.org/abs/2505.16335
generated_at: '2026-02-03T19:29:07'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design

*Renjie Wei; Songqiang Xu; Qingyu Guo; Meng Li*

---

> ### üìä Quick Facts
>
> *   **Quantization Precision:** 4-bit & 6-bit Floating Point
> *   **Best FID Score (4-bit):** 3.58 (vs SOTA 10.83)
> *   **Best Inception Score (4-bit):** 241.5 (vs SOTA 175.9)
> *   **FPGA Throughput:** 1.1 images/sec
> *   **Energy Efficiency:** 3.6x higher than integer accelerators
> *   **Hardware Platform:** FPGA (LUT-based)

---

## Executive Summary

Visual Autoregressive (VAR) models have demonstrated superior performance in image generation tasks, but their deployment on edge devices is severely hindered by massive parameter sizes and prohibitive computational costs. While quantization is a standard technique for model compression, existing integer-based quantization methods struggle to maintain the high fidelity required for generative tasks, often resulting in significant degradation of image quality. Consequently, there is a critical need for a compression framework that bridges the gap between the resource constraints of edge hardware (specifically FPGAs) and the precision requirements of high-resolution autoregressive generation.

This paper introduces **FPQVAR**, a novel algorithm-hardware co-design framework that leverages low-bit floating-point (FP) quantization rather than traditional integer schemes. Algorithmically, the authors propose **Dual Format Quantization** to manage imbalanced input activations and utilize **Group-wise Hadamard Transformations (GHT)** combined with GHT-Aware Learnable Transformations to suppress time-varying outlier channels. On the hardware side, the study implements the first FPGA-based accelerator for VAR models, featuring custom low-bit FP arithmetic components (quantizers and multipliers) built using lookup tables (LUTs) and a specialized two-level pipeline architecture to maximize throughput.

The FPQVAR framework demonstrates significant improvements over state-of-the-art methods in both generative quality and hardware efficiency. Under 4-bit quantization, the model achieved a Fr√©chet Inception Distance (FID) of **3.58** (a reduction from the SOTA 10.83) and an Inception Score (IS) of **241.5** (an increase from 175.9). Furthermore, the 6-bit quantized model achieves performance comparable to a full-precision FP16 baseline. On the hardware front, the FPGA accelerator attains a throughput of **1.1 images per second**, representing a 3.1x speedup over existing integer-based accelerators, while delivering 3.6x higher energy efficiency than integer baselines and 2.8x higher efficiency than GPU implementations.

This research represents a paradigm shift in the deployment of generative AI on edge devices by validating that low-bit floating-point quantization can outperform integer-based methods in both accuracy and efficiency for autoregressive vision tasks. By enabling the first practical FPGA acceleration of VAR models, FPQVAR opens the door for high-quality, energy-efficient image generation on resource-constrained platforms.

---

## Methodology

The study proposes **FPQVAR**, a post-training floating-point (FP) quantization framework employing a collaborative algorithm and hardware co-design approach.

### Algorithm Level
*   **Dual Format Quantization:** Specifically designed to address imbalanced input activations.
*   **Group-wise Hadamard Transformation (GHT):** Used to mitigate time-varying outlier channels.
*   **GHT-Aware Learnable Transformation:** Works in tandem with GHT to further suppress outliers.

### Hardware Level
*   **Low-bit FP Components:** Design of quantizers and multipliers using FPGA Lookup Tables (LUTs).
*   **VAR Accelerator:** Development of the first FPGA-based VAR accelerator.
*   **Architecture:** Features low-bit FP computation capabilities and a specialized two-level pipeline architecture.

---

## Technical Details

| Feature | Description |
| :--- | :--- |
| **Core Technology** | Floating Point Quantization (vs. Integer-based) |
| **Target Model** | Visual Autoregressive (VAR) Model |
| **Precision Optimizations** | 4-bit and 6-bit precisions |
| **Hardware Implementation** | FPGA using LUTs for arithmetic units |
| **Pipeline Architecture** | Two-level pipeline designed for throughput maximization |

---

## Key Findings

*   **Superior Image Quality (4-bit):** FPQVAR significantly improves image quality metrics compared to state-of-the-art methods, reducing **FID from 10.83 to 3.58** and increasing **IS from 175.9 to 241.5**.
*   **Parity at 6-bit:** The 6-bit quantized VAR model achieves performance comparable to the full-precision FP16 model.
*   **High Throughput:** The implemented FPGA accelerator achieves a throughput of **1.1 image/s**, which is **3.1x higher** than existing integer-based accelerators.
*   **Energy Efficiency:** The proposed design demonstrates **3.6x higher energy efficiency** than integer-based accelerators and **2.8x higher efficiency** compared to GPU baselines.

---

## Contributions

*   **Edge Deployment Enablement:** Addressed deployment bottlenecks related to large parameter size and computation cost, enabling the edge deployment of Visual Autoregressive (VAR) models.
*   **Algorithmic Innovation:** Introduced solutions to specific quantization challenges inherent to VAR models, specifically targeting imbalanced activations and dynamic outlier channels.
*   **Hardware Architecture Pioneering:** Designed the first low-bit floating-point computational units on FPGAs and established the first FPGA-based accelerator for VAR models.
*   **Validation of FP over Integer:** Validated that low-bit floating-point quantization can outperform integer-based methods in both generative quality and hardware efficiency for autoregressive vision tasks.

---

**Paper Quality Score:** 9/10  
**References:** 40 citations