---
title: 'COLA: Continual Learning via Autoencoder Retrieval of Adapters'
arxiv_id: '2510.21836'
source_url: https://arxiv.org/abs/2510.21836
generated_at: '2026-02-03T20:13:54'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# COLA: Continual Learning via Autoencoder Retrieval of Adapters
*Jaya Krishna Mandivarapu*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 40 citations
> *   **Parameter Efficiency:** Up to 4 orders of magnitude reduction
> *   **Memory Efficiency:** 3x reduction in GPU memory usage
> *   **Storage Efficiency:** >1 order of magnitude reduction
> *   **Training Type:** Replay-free (No data retention)

---

## Executive Summary

> Continual learning in Large Language Models (LLMs) is severely limited by "catastrophic forgetting," where learning new tasks overwrites previously acquired knowledge. Current mitigation strategies are impractical for dynamic environments: data replay methods retain past samples, incurring prohibitive memory costs and privacy risks, while parameter-expansion methods require maintaining massive sets of task-specific weights. This paper addresses the critical need for a parameter-efficient, privacy-preserving solution that enables LLMs to learn indefinitely without access to historical data, resolving the conflict between stability (retaining old knowledge) and plasticity (learning new knowledge).
>
> The authors introduce **COLA** (Continual Learning via Autoencoder Retrieval of Adapters), a replay-free framework that decouples task-specific knowledge using Low-Rank Adaptation (LoRA). While LoRA provides the baseline parameter efficiency by freezing the LLM backbone, COLAâ€™s specific novel contribution is the compression of adapter storage via a Lifelong Autoencoder (LAE). The system compresses LoRA weights into low-dimensional latent vectors rather than storing full adapter parameters. It employs a Contractive Autoencoder (CAE) with a loss function combining cosine similarity reconstruction and a contractive penalty. Crucially, the inference mechanism is task-agnostic: it reconstructs adapter weights from the latent space and selects the appropriate adapter by calculating the minimum Perplexity (PPL) of the input sequence against these reconstructed weights.
>
> Empirical evaluations on task-oriented dialogue systems and intent classification datasets demonstrate that COLA outperforms state-of-the-art replay-based and regularization-based methods in preventing catastrophic forgetting. The results distinguish clearly between the benefits of LoRA and the contributions of COLA: while LoRA implementation reduces trainable parameters by up to four orders of magnitude and GPU memory usage by three times, COLAâ€™s autoencoder mechanism specifically reduces storage requirements by over an order of magnitude. Despite this aggressive compression, reconstructed adapters achieved performance matching that of original trained models, validating the precision of the retrieval mechanism. COLA represents a significant advancement in the practical application of LLMs by offering a cost-effective, data-independent solution to the stability-plasticity dilemma.

---

## Key Findings

*   **Mitigation of Catastrophic Forgetting:** The COLA framework successfully prevents the overwriting of existing knowledge in Large Language Models (LLMs) when learning new tasks, effectively solving the common phenomenon known as catastrophic forgetting.
*   **Parameter and Memory Efficiency:** The method achieves a significant reduction in parameter usage and memory footprint compared to existing approaches, avoiding the need for a substantial set of task-specific parameters.
*   **Superior Performance:** Empirical evaluations on diverse datasets (including task-oriented dialogue systems and intent classification) demonstrate that COLA outperforms current state-of-the-art methods.
*   **Replay-Free Learning:** The approach enables efficient continual learning with minimal training and negligible performance degradation on previous tasks, all without retaining earlier training data or using data replay techniques.

---

## Methodology

The paper proposes **COLA** (Continual Learning via Autoencoder Retrieval of Adapters), a novel framework designed to address the limitations of LLMs in continual learning scenarios. The methodology employs an autoencoder to learn and capture low-dimensional embeddings of the weights associated with various tasks. By utilizing these compressed embeddings, the approach facilitates the transfer of knowledge to new tasks while preserving previous knowledge. Crucially, this framework operates without relying on data replay or storing extensive historical training data.

---

## Technical Details

*   **Backbone Architecture:** Utilizes a frozen pre-trained LLM backbone.
*   **Adaptation Mechanism:** Implements task-specific Low-Rank Adaptation (LoRA) modules defined as $\Delta W = BA$.
*   **Compression Model:** Employs a Lifelong Autoencoder (LAE), specifically a Contractive Autoencoder (CAE), to compress adapter weights into latent vectors for storage.
*   **Loss Function:** Combines cosine similarity reconstruction and a contractive penalty ($\lambda = 1 \times 10^{-4}$).
*   **Inference Strategy:** Task-agnostic inference that selects adapters based on the minimum Perplexity (PPL) of the input sequence.
*   **Task Configuration:** Replay-free setup treating tasks as domain-isolated.
*   **Experimental Rank:** LoRA rank experiments conducted with $r = 1-4$.

---

## Performance Results

*   **Parameter Reduction:** LoRA reduces trainable parameters by up to **four orders of magnitude**.
*   **Memory Optimization:** Achieves a **3x reduction** in GPU memory usage.
*   **Storage Efficiency:** Adapter parameters are maintained at **< 2%** of the original layer size, with total storage reduced by over **an order of magnitude**.
*   **Model Fidelity:** Reconstructed adapters achieve performance matching the original trained models.
*   **Benchmarking:** The method prevents catastrophic forgetting and outperforms state-of-the-art baselines on dialogue and intent classification tasks.

---

## Core Contributions

*   **Cost-Effective CL for LLMs:** Addresses the impracticality of frequent re-training of LLMs by providing a solution that requires minimal computational resources for training updates.
*   **Novel Weight Compression Mechanism:** Introduces a unique use of autoencoders to manage task-specific weights via low-dimensional embeddings, enabling efficient knowledge retrieval and transfer.
*   **Data-Independence:** Eliminates the dependency on retaining previous training samples (data replay), offering a solution that is beneficial for privacy-sensitive or memory-constrained applications.
*   **State-of-the-Art Benchmarking:** Provides empirical evidence that a retrieval-based adapter approach can outperform existing methods in balancing plasticity (learning new tasks) and stability (retaining old tasks) across multiple NLP domains.