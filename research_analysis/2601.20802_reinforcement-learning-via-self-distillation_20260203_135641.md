---
title: Reinforcement Learning via Self-Distillation
arxiv_id: '2601.20802'
source_url: https://arxiv.org/abs/2601.20802
generated_at: '2026-02-03T13:56:41'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Reinforcement Learning via Self-Distillation

*Jonas HÃ¼botter; Frederike LÃ¼beck; Lejs Behric; Anton Baumann; Marco Bagatella; Daniel Marta; Ido Hakimi; Idan Shenfeld; Thomas Kleine Buening; Carlos Guestrin; Andreas Krause*

---

> ### ðŸ“Š Quick Facts
>
> *   **Method:** Self-Distillation Policy Optimization (SDPO)
> *   **Core Innovation:** Teacher-free self-distillation using rich textual feedback from verifiable environments.
> *   **Efficiency Gains:**
>     *   **Code Gen:** 4Ã— fewer generations to reach GRPO final accuracy.
>     *   **Chemistry:** 10Ã— speedup in accuracy improvement over GRPO.
>     *   **Test Time:** Matches discovery probability of Best-of-$k$ with 3Ã— fewer attempts.
> *   **Compute Overhead:** Minimal (+5.8% without code env; +17.1% with code env).
> *   **Primary Domains:** Scientific reasoning, tool use, competitive programming.

---

## Executive Summary

Current Reinforcement Learning with Verifiable Rewards (RLVR) methods suffer from a critical credit-assignment bottleneck because they rely primarily on sparse scalar outcomes (e.g., binary pass/fail signals). This approach ignores the wealth of diagnostic informationâ€”such as runtime errors, judge evaluations, and execution logsâ€”available in verifiable domains like scientific reasoning, tool use, and competitive programming. By failing to exploit this rich textual feedback, existing state-of-the-art methods like Group Relative Policy Optimization (GRPO) struggle with sample efficiency and stability, limiting the ability of Large Language Models (LLMs) to learn effectively from complex, interactive environments.

The paper introduces **Self-Distillation Policy Optimization (SDPO)**, a novel teacher-free paradigm that treats the language model as both a student and a "self-teacher" to generate dense learning signals. In this framework, the model first generates an attempt, then conditions itself on the environmentâ€™s rich textual feedback to produce a refined distribution over the next tokens. This feedback-informed "teacher" distribution is distilled back into the original policy by minimizing the KL-divergence between the two. Technically, SDPO employs a stopgrad operator to prevent regression and acts as a logit-level policy gradient, enabling credit assignment at the token level.

Unlike GRPO, which relies on potentially collapsing scalar rewards, SDPO leverages arbitrary token sequences and is compatible with off-policy data and PPO-style clipping for stability. SDPO demonstrates superior performance and sample efficiency across multiple benchmarks. In code generation on LiveCodeBench v6 (using Qwen3-8B), SDPO outperformed GRPO in final accuracy and achieved GRPOâ€™s final performance with **4Ã— fewer generations**. In a chemistry reasoning task (Olmo3-7B-Instruct), the method achieved a **10Ã— speedup** in accuracy improvement relative to wall-clock time and a **7Ã— reduction** in response length. Furthermore, SDPO operates with minimal computational overhead and accelerates discovery at test time. Notably, the method remains robust even in scalar-only environments by utilizing successful rollouts as implicit feedback. This research significantly advances the field of LLM alignment by unlocking the utility of rich feedback within a self-contained training loop.

---

## Key Findings

*   **Superior Performance on Verifiable Domains:** SDPO improves both sample efficiency and final accuracy over strong Reinforcement Learning with Verifiable Rewards (RLVR) baselines across tasks involving scientific reasoning, tool use, and competitive programming (LiveCodeBench v6).
*   **Efficacy in Scalar-Only Environments:** Even in standard RLVR environments that return only scalar rewards (no rich text feedback), SDPO outperforms baselines by using successful rollouts as implicit feedback for failed attempts.
*   **Test-Time Acceleration:** When applied to individual questions at test time, SDPO accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with **3Ã— fewer attempts**.

---

## Methodology

The paper formalizes the setting as **Reinforcement Learning with Rich Feedback**, moving beyond scalar outcome rewards to utilize textual explanations (e.g., runtime errors, judge evaluations) provided by verifiable environments.

The proposed method, **Self-Distillation Policy Optimization (SDPO)**, operates through a specific mechanism:
1.  **Tokenization:** The rich textual feedback provided by the environment is tokenized.
2.  **Conditioning:** The current model conditions itself on this feedback to act as a 'self-teacher'.
3.  **Distillation:** The feedback-informed next-token predictions are distilled back into the policy.

This creates a dense learning signal without requiring an external teacher or an explicit reward model.

---

## Research Contributions

*   **Addressing the Credit-Assignment Bottleneck:**
    The research identifies and mitigates the limitation of current RLVR methods that rely solely on scalar rewards, which fail to exploit the rich diagnostic information available in verifiable domains.

*   **Teacher-Free Self-Distillation:**
    Introduction of a novel self-teaching paradigm where the model improves its own policy by distilling knowledge from its feedback-conditioned state, removing the dependency on external reward models or human annotators.

*   **Versatility of Feedback Utilization:**
    Demonstration that the approach is robust enough to generate implicit feedback in environments lacking explicit rich textual data, broadening the applicability of the method beyond strictly text-feedback-heavy settings.

---

## Technical Details

**Core Architecture**
*   **Single Model Policy:** SDPO utilizes a single language model that acts as both a student (generating attempts based on questions) and a self-teacher (re-evaluating attempts based on questions and rich feedback).

**Loss Function & Optimization**
*   **Objective:** The loss function minimizes the KL-divergence between the student's and self-teacher's next-token distributions.
*   **Stopgrad Operator:** A stopgrad operator is used to prevent regression during the distillation process.
*   **Gradient Nature:** The SDPO gradient acts as a logit-level policy gradient with dense token-level credit assignment based on the log-ratio of teacher to student probabilities.

**Comparison & Stability**
*   **vs. GRPO:** Unlike Group Relative Policy Optimization (GRPO) which relies on scalar rewards that can collapse, SDPO leverages arbitrary token sequences and rich feedback.
*   **Compatibility:** It is compatible with off-policy data via PPO-style clipping.
*   **Stability Enhancements:** Stability is enhanced using a regularized self-teacher (via Exponential Moving Average or interpolation) and Jensen-Shannon divergence.

**Efficiency**
*   **Compute:** Overhead is limited to parallelized log-prob calculations.
*   **Memory:** Memory is optimized using Top-K distillation.

---

## Experimental Results

**Code Generation (LiveCodeBench v6 using Qwen3-8B)**
*   SDPO substantially outperformed GRPO in final accuracy.
*   Achieved GRPO's final accuracy with **4Ã— fewer generations**.

**Chemistry Task (Olmo3-7B-Instruct)**
*   Demonstrated a **10Ã— speedup** in accuracy improvement over GRPO relative to wall-clock time.
*   Achieved a **7Ã— reduction** in response length.

**Computational Efficiency**
*   Added minimal time per step compared to GRPO:
    *   **+5.8%** without code environment.
    *   **+17.1%** with code environment.

**Test-Time Performance**
*   Achieved the same discovery probability as Best-of-$k$ sampling or multi-turn conversations with **3Ã— fewer attempts**.

**Scalar-Only Environments**
*   Even without rich text feedback, SDPO outperformed baselines by using successful rollouts as implicit feedback.

---

**Paper Quality Score:** 8/10
**References:** 40 citations