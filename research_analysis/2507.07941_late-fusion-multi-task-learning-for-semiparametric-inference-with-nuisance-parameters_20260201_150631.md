# Late Fusion Multi-task Learning for Semiparametric Inference with Nuisance Parameters

*SoÂ­hom Bhattacharya; Yongzhuo Chen; Muxuan Liang*

---

> **### ðŸ“ Executive Summary**
>
> This research addresses the complex challenge of improving parameter estimation efficiency in semiparametric models characterized by infinite-dimensional nuisance parameters across heterogeneous, distributed tasks. In fields such as healthcare and social sciences, data is often siloed across distinct institutions (e.g., hospitals or clinical trials), restricting analysis due to privacy regulations. Standard approaches typically analyze tasks in isolation, failing to leverage shared underlying structures, or require raw data pooling, which violates privacy constraints. The core problem lies in borrowing statistical strength across related tasks to enhance inference without compromising data privacy or suffering from negative transferâ€”a degradation in performance caused by significant heterogeneity between tasks.
>
> The key innovation is the "Late Fusion Multi-task Learning" (Lf-MTL) framework, specifically tailored for semiparametric inference in privacy-preserving environments. The method rigorously distinguishes between finite-dimensional structural parameters ($\theta_k$) and infinite-dimensional nuisance parameters ($\eta_k$). It operates in two stages:
> 1.  Obtaining initial Double Machine Learning (DML) estimators for each task independently.
> 2.  Adaptively aggregating these estimators via convex optimization.
>
> This optimization minimizes a quadratic loss function derived from a one-step Taylor expansion of the empirical moment condition, augmented by an $L_2$ regularization term ($\lambda \|u_k - u_0\|_2^2$). This regularization mechanism explicitly exploits cross-task similarities by penalizing the deviation of task-specific parameters from a shared centroid ($u_0$), thereby balancing fidelity to individual task data with the exploitation of global structure.
>
> The study establishes theoretical findings proving that the Lf-MTL framework achieves faster convergence rates for parameter estimation compared to standard individual task learning when tasks share similar parametric components. The method demonstrates high effectiveness in data-scarce environments, maintaining strong performance with datasets containing fewer than 80 examples. By enforcing the Neyman near-orthogonality condition, the framework ensures semiparametric efficiency is preserved or improved following aggregation. Furthermore, the adaptive aggregation mechanism provides robustness against outliers and significant task heterogeneity, showing enhanced estimation accuracy for nuisance parameters when they exhibit structural similarities across tasks.

<br/>

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Optimal Sample Size** | High effectiveness with < 80 examples |
| **Key Innovation** | Privacy-preserving Late Fusion Multi-task Learning |
| **Core Application** | Heterogeneous Treatment Effect (HATE) Estimation |

---

## Key Findings

*   **Improved Convergence Rates:** Achieves faster convergence rates for parameter estimation compared to individual task learning when tasks share similar parametric components.
*   **Adaptive Aggregation:** Effectively balances the exploitation of similarities across tasks with robustness to task-specific differences and heterogeneity.
*   **Small Sample Efficiency:** Demonstrates high effectiveness in scenarios with moderate sample sizes (specifically datasets with fewer than 80 examples).
*   **Enhanced Nuisance Estimation:** Provides superior estimation of nuisance parameters when they exhibit similarities across tasks.
*   **Theoretical Guarantees:** Establishes rigorous theoretical proof of performance improvements relative to standard individual learning approaches.

---

## Methodology

The research proposes a **two-step 'late fusion' multi-task learning framework** designed for semiparametric models involving infinite-dimensional nuisance parameters.

1.  **Step 1: Independent Estimation**
    *   Initial "double machine-learning" (DML) estimators are obtained separately for each task.
    *   This step handles the infinite-dimensional nuisance parameters locally.

2.  **Step 2: Adaptive Aggregation**
    *   The initial estimators are adaptively aggregated to leverage information from related tasks.
    *   The process remains robust to heterogeneity between tasks.
    *   Uses a convex optimization approach to balance shared information and task-specific fidelity.

**Privacy-Preserving Architecture**
The framework is designed to avoid individual-level data sharing. Instead of pooling raw patient-level data, it integrates information while ensuring privacy, making it suitable for sensitive environments like healthcare.

---

## Technical Details

The approach utilizes the **Late Fusion Multi-task Learning (Lf-MTL)** framework with the following specifications:

*   **Environment:** Designed for privacy-preserving environments by sharing summary statistics rather than raw data.
*   **Algorithm Structure:** A two-step process:
    1.  Obtain initial estimators for task-specific parameters ($\theta_k$) and nuisance parameters ($\eta_k$) independently.
    2.  Perform adaptive aggregation via convex optimization.
*   **Optimization Function:**
    *   Minimizes a quadratic loss function derived from a one-step Taylor expansion of the empirical moment condition.
    *   Includes an $L_2$ regularization term: $\lambda \|u_k - u_0\|_2^2$.
    *   This regularization penalizes deviation from a shared centroid ($u_0$), balancing individual fidelity with task similarity.
*   **Statistical Conditions:**
    *   Satisfies the **Neyman near-orthogonality condition** for semiparametric efficiency.
*   **Applications:**
    *   Applied to models such as **partial linear regression** for CATE (Conditional Average Treatment Effect) estimation.

---

## Contributions

*   **Novel Framework:** Introduction of a novel late fusion approach specifically tailored for semiparametric models with infinite-dimensional nuisance parameters.
*   **Rigorous Theory:** Establishment of rigorous theoretical guarantees, proving faster convergence rates relative to standard individual learning approaches.
*   **Privacy-Preserving Learning:** Contribution to privacy-sensitive learning by allowing integration of heterogeneous data sources (e.g., EHRs, clinical trials) without sharing raw patient-level data.
*   **Methodological Advancement:** Advancement of methodologies for heterogeneous treatment effect estimation across diverse data sources.

---

## Results

*   **Qualitative Performance:** The method achieves improved convergence rates compared to individual task learning when tasks share similar parametric components.
*   **Robustness:** It effectively balances the exploitation of similarities across tasks with robustness to outliers.
*   **Sample Size Efficiency:** Demonstrates high effectiveness in moderate sample size scenarios (e.g., less than 80 examples).
*   **Nuisance Parameters:** Enhanced performance is observed in estimating nuisance parameters when they exhibit similarities across tasks.
*   **Note:** Specific quantitative metrics were not available in the provided text.

---
