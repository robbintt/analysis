---
title: 'MARS: Co-evolving Dual-System Deep Research via Multi-Agent Reinforcement
  Learning'
arxiv_id: '2510.04935'
source_url: https://arxiv.org/abs/2510.04935
generated_at: '2026-02-03T13:05:09'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MARS: Co-evolving Dual-System Deep Research via Multi-Agent Reinforcement Learning

*Guoxin Chen; Zile Qiao; Wenqing Wang; Donglei Yu; Xuanzhong Chen; Hao Sun; Minpeng Liao; Kai Fan; Yong Jiang; Penguin Xie; Wayne Xin Zhao; Ruihua Song; Fei Huang*

---

> ### üìä Quick Facts
>
> *   **Model Size:** 8B Parameters
> *   **Training Paradigm:** Zero RL (No Supervised Fine-Tuning)
> *   **Performance Gain:** +8.17% on HLE Benchmark
> *   **Knowledge Improvement:** +8.9% across 7 tasks
> *   **Efficiency Factor:** 4x more efficient than 32B baselines

---

## üìã Executive Summary

Large Reasoning Models (LRMs) face critical challenges regarding efficiency and knowledge currency. Existing approaches often rely on massive parameter counts and static training data, leading to high token consumption and outdated knowledge bases. Furthermore, traditional training paradigms heavily depend on Supervised Fine-Tuning (SFT), which struggles to instill complex, adaptive reasoning capabilities required for deep research tasks. There is a pressing need for architectures that can perform high-level reasoning and information retrieval without the prohibitive computational costs of scaling model size or the limitations of static knowledge distillation.

This paper introduces **MARS** (Multi-Agent Reinforcement Learning for Co-evolving Dual-System Deep Research), a novel framework that utilizes a "System 1" (fast, intuitive) and "System 2" (slow, deliberate) architecture to co-evolve reasoning capabilities. Technically, the authors employ a Multi-Agent Reinforcement Learning (MARL) approach with a "Zero RL" paradigm, optimizing the model exclusively through reinforcement signals without any SFT. The core technical contribution is an Extended Group Relative Policy Optimization (GRPO) algorithm, which features Decoupled Gradient Computation, Bin-packing Optimization for efficient training, and Advantage-weighted Balanced Sampling. This allows the two systems to adapt via a Shared Reward Mechanism, shifting information distillation from a static process to a dynamic, adaptive interaction.

The MARS model demonstrates exceptional parameter efficiency and performance. Using only an 8 Billion (8B) parameter architecture, MARS outperformed the significantly larger WebThinker (32B) model on the HLE benchmark, achieving a performance gain of +8.17% with 4x parameter efficiency. Additionally, the model realized an average improvement of +8.9% across seven knowledge-intensive tasks. These results validate the efficacy of the Zero RL training methodology, proving that high-performance reasoning can be achieved without relying on supervised fine-tuning, while significantly reducing computational overhead compared to larger baselines.

The findings of this paper represent a significant shift in LLM research, demonstrating that sophisticated reasoning does not require scaling model size but can be achieved through better training dynamics and architectural design. By validating the "Zero RL" approach, the authors challenge the industry's reliance on SFT, suggesting a path toward more autonomous and adaptable AI agents. The successful implementation of a dual-system co-evolution via MARL provides a blueprint for future research into efficient, cost-effective deep research agents capable of dynamic knowledge acquisition and complex problem-solving.

---

## üîë Key Findings

*   **Superior Performance on HLE:** The MARS model (8B) achieved a performance gain of **8.17%**, outperforming the significantly larger WebThinker (32B) model.
*   **Efficacy of Zero RL Training:** The model achieved state-of-the-art results without any Supervised Fine-Tuning (SFT), relying entirely on reinforcement learning.
*   **Broad Knowledge Gains:** Demonstrated an average improvement of **8.9%** across 7 distinct knowledge-intensive tasks.
*   **Parameter Efficiency:** A smaller 8B model surpassed the reasoning capabilities of much larger 32B models, proving 4x parameter efficiency.

---

## üß† Methodology

The research employs a sophisticated multi-agent framework to optimize reasoning capabilities:

*   **Dual-System Co-evolution:** Utilizes Multi-Agent Reinforcement Learning (MARL) to jointly optimize two cognitive systems:
    *   **System 1:** Fast, intuitive processing.
    *   **System 2:** Deliberate, slow reasoning.
*   **Shared Reward Mechanism:** Enables the two systems to co-adapt and learn from each other's outcomes.
*   **Extended GRPO Algorithm:** Introduces three specific optimizations for training:
    *   **Decoupled Gradient Computation**
    *   **Bin-packing Optimization**
    *   **Advantage-weighted Balanced Sampling**

---

## ‚öôÔ∏è Technical Details

**Architecture Design**
The system utilizes a dual-system architecture designed to mimic human cognitive processes:

*   **System 1 (Fast/Intuitive):** Handles rapid processing and generates initial proposals.
*   **System 2 (Deliberate/Slow):** Conducts deep research, performs searches, and verifies information.

**Training Paradigm**
*   **Zero RL:** The model relies exclusively on Multi-Agent Reinforcement Learning (MARL) without any Supervised Fine-Tuning (SFT).
*   **Model Scale:** An efficient 8 Billion (8B) parameter architecture.

---

## üìà Results

*   **HLE Benchmark:** The MARS (8B) model achieved a performance gain of **+8.17%**, surpassing the WebThinker (32B) model and demonstrating **4x parameter efficiency**.
*   **Knowledge-Intensive Tasks:** Achieved an average improvement of **+8.9%** across 7 different tasks.
*   **Training Validation:** Successfully achieved these results with **Zero SFT**, relying solely on RL for optimization.

---

## üåü Contributions

1.  **Addressing LRM Limitations:** Tackles critical issues regarding token consumption and outdated knowledge bases in current Large Reasoning Models.
2.  **Adaptive Information Distillation:** Shifts the paradigm from static distillation to dynamic, adaptive processes.
3.  **Algorithmic Innovation:** Introduces a novel multi-agent RL algorithm (Extended GRPO).
4.  **Validation of Zero RL:** Proves that high-performance reasoning can be achieved via Zero RL training without the need for supervised fine-tuning.

---

**Quality Score:** 9/10 | **References:** 40 citations