---
title: Rewarding Explainability in Drug Repurposing with Knowledge Graphs
arxiv_id: '2509.02276'
source_url: https://arxiv.org/abs/2509.02276
generated_at: '2026-02-03T12:25:32'
quality_score: 9
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Rewarding Explainability in Drug Repurposing with Knowledge Graphs

*Susana Nunes; Samy Badreddine; Catia Pesquita*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Total Citations:** 11
> *   **Performance:** MRR 0.481 / Hits@10 0.755
> *   **Key Innovation:** RL-guided path-finding for scientific explanation
> *   **Comparison:** Significantly outperforms MINERVA (MRR 0.321)

---

## Executive Summary

In the domain of drug repurposing, AI-driven methods often face a critical trade-off between predictive accuracy and model interpretability. While link prediction within Knowledge Graphs (KGs) has proven effective for identifying potential drug-disease associations, the resulting predictions frequently lack the explanatory context required for scientific validation and trust. This paper addresses the absence of a framework that simultaneously delivers high predictive performance and generates explanations grounded in established biomedical knowledge. Without such capabilities, AI insights remain difficult to integrate into rigorous scientific workflows where understanding the mechanistic "why" behind a prediction is as vital as the prediction itself.

The authors introduce **REx (Rewarding Explainability)**, a novel framework that treats explanation generation as a path-finding problem optimized through Reinforcement Learning (RL). Technically, REx utilizes an RL agent to traverse the knowledge graph in search of paths connecting hypothesis entities, guided by a reward function balancing three "explanatory virtues": **Fidelity** (ensuring connectivity), **Relevance** (maximized via Information Content), and **Simplicity**. To ensure scientific rigor, the method calculates Information Content using a "Clustered IC" approach to mitigate granularity bias and enriches resulting paths with domain-specific ontologies. This process yields an Explanation Subgraph that traces a logical, evidence-backed trajectory through the data, adhering to established taxonomic standards.

Evaluation across three popular knowledge graph benchmarks demonstrated that REx outperforms state-of-the-art approaches, including PoLo, PREDICT, DeepPath, and MINERVA. In drug repurposing tasks specifically, REx achieved superior predictive performance, recording a Mean Reciprocal Rank (MRR) of **0.481** and a Hits@10 score of **0.755**. This significantly outperformed the closest competitor, MINERVA, which obtained an MRR of **0.321** and a Hits@10 of **0.458**. These quantitative results validate the hypothesis that integrating explainability does not compromise accuracy; rather, the RL-guided approach enhances both precision and the generation of explanatory paths that align with established biomedical knowledge.

This research significantly advances AI-driven scientific discovery by demonstrating that accuracy and explainability can be mutually reinforcing. By grounding predictive insights in ontological structures and explanatory virtues, REx provides a blueprint for developing trustworthy AI systems in high-stakes domains like biomedicine. The framework moves beyond opaque pattern matching to offer evidential and coherential reasoning, lowering the barrier for researchers to adopt AI tools. Ultimately, REx facilitates a transparent hypothesis generation process, enabling scientists to validate drug repurposing candidates against established literature and mechanistic understanding with greater confidence.

---

## Key Findings

*   The **REx approach** outperforms state-of-the-art approaches in predictive performance on drug repurposing tasks.
*   The method successfully generates **explanations** that validate predictive insights against established biomedical knowledge.
*   Evaluation across three popular knowledge graph benchmarks confirmed the framework's effectiveness in supporting **AI-driven scientific discovery**.
*   The integration of **domain-specific ontologies** ensures that explanatory paths are both insightful and scientifically grounded.

---

## Methodology

The paper proposes **REx**, a novel framework based on link prediction within knowledge graphs. The methodology utilizes a reinforcement learning (RL) agent to identify explanatory paths within the graph, guided by specific reward and policy mechanisms designed to prioritize desirable properties of scientific explanations. Additionally, the approach integrates domain-specific ontologies to enrich the identified paths, ensuring the results are grounded in established biomedical knowledge and capable of handling complex, multi-relational data.

---

## Technical Details

The REx framework utilizes Knowledge Graphs (KGs), defined as $G = (E, R, F)$, to generate scientific explanations for drug repurposing by treating explanation generation as a path-finding problem optimized via Reinforcement Learning (RL).

### Core Components
*   **RL Agent:** Searches for paths connecting hypothesis entities using a reward mechanism.
*   **Reward Functions:** Balances three main properties:
    *   **Fidelity:** Ensures connectivity.
    *   **Relevance:** Maximized via Information Content.
    *   **Simplicity:** Enforced via policy constraints.
*   **Information Content (IC):** Measures specificity via the formula:
    $$IC(v) = -\log \frac{deg(v)}{|G|}$$
    *   *Refinement:* Uses **Clustered IC** to mitigate granularity bias.
*   **Pipeline:**
    1.  Computes IC for nodes.
    2.  Finds paths using RL optimization.
    3.  Generates an **Explanation Subgraph** enriched with ontology classes.

### Theoretical Foundation
The approach is grounded in the taxonomy of explanatory virtues (Keas, 2018), specifically targeting:
*   **Evidential Virtues**
*   **Coherential Virtues**
*   **Aesthetic Virtues**

---

## Contributions

*   **Development of REx:** A new method that addresses the dual requirement of accuracy and explainability in predictive modeling for scientific domains.
*   **RL-based Explainability:** The application of reinforcement learning mechanisms, tailored with specific reward functions, to guide the discovery of explanatory paths in knowledge graphs.
*   **Ontological Enrichment:** A strategy for combining knowledge graph paths with domain-specific ontologies to ensure explanations are scientifically rigorous and meaningful.
*   **Validation of Explainability:** Evidence that providing meaningful scientific explanations does not come at the cost of accuracy, but rather enhances predictive performance in drug repurposing scenarios compared to existing state-of-the-art methods.

---

## Results

REx outperforms state-of-the-art approaches in predictive performance specifically on drug repurposing tasks. It successfully generates explanations that validate predictive insights against established biomedical knowledge, with domain-specific ontologies ensuring scientific grounding.

The framework was evaluated across three popular knowledge graph benchmarks, confirming its effectiveness in supporting AI-driven scientific discovery. Compared to existing methods like PoLo, PREDICT, DeepPath, and MINERVA, REx provides greater scientific relevance and explanatory depth.

**Performance Highlights:**
*   **REx MRR:** 0.481
*   **REx Hits@10:** 0.755
*   **MINERVA (Competitor) MRR:** 0.321
*   **MINERVA (Competitor) Hits@10:** 0.458