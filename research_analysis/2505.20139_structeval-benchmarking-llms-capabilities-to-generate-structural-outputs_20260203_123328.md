---
title: 'StructEval: Benchmarking LLMs'' Capabilities to Generate Structural Outputs'
arxiv_id: '2505.20139'
source_url: https://arxiv.org/abs/2505.20139
generated_at: '2026-02-03T12:33:28'
quality_score: 9
citation_count: 25
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs

*Jialin Yang; Dongfu Jiang; Lipeng He; Sherman Siu; Yuxuan Zhang; Disen Liao; Zhuofeng Li; Huaye Zeng; Yiming Jia; Haozhe Wang; Benjamin Schneider; Chi Ruan; Wentao Ma; Zhiheng Lyu; Yifei Wang; Yi Lu; Quy Duc Do; Ziyan Jiang; Ping Nie; Wenhu Chen*

---

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **Dataset Size** | 2,035 examples |
> | **Formats Covered** | 18 formats (Text & Visual) |
> | **Top Performing Model** | o1-mini (Score: 75.58) |
> | **Key Challenge** | Generation tasks > Conversion tasks |
> | **Citations** | 25 references |

---

## Executive Summary

This research addresses the critical challenge of evaluating Large Language Models (LLMs) on their ability to generate syntactically correct and semantically faithful structured outputs, such as code, data formats, and visual layouts. While LLMs have demonstrated proficiency in natural language tasks, ensuring **structural fidelity**â€”the accuracy and validity of outputs like JSON, HTML, or React componentsâ€”remains a significant hurdle. This capability is essential for real-world applications including automated software engineering, data parsing, and dynamic frontend generation, where minor syntax errors or structural deviations can render outputs unusable.

The authors introduce **StructEval**, a comprehensive benchmark designed to assess structural fidelity across 18 diverse formats, categorized into text-only structures (e.g., JSON, YAML, XML) and renderable visual structures (e.g., HTML, React, SVG). Technically, the framework employs a rigorous evaluation pipeline that combines three distinct validation methods: Syntactic Validity Checking, Path-based Keyword Matching utilizing six rule types to verify text elements, and Visual Question Answering (VQA) using Vision Language Models (VLMs) to assess visual correctness.

The dataset, constructed via a three-stage annotation pipeline involving LLM-based synthesis and expert review, comprises 2,035 examples formally represented as triplets $(q, K, Q_v)$ covering 44 task types across both Generation (natural language to structure) and Conversion (structure to structure) tasks.

Empirical analysis reveals that current state-of-the-art (SOTA) models struggle significantly with structural consistency. The top-performing model, **o1-mini**, achieved a modest average score of **75.58**, with open-source alternatives trailing by approximately 10 points. The study highlights distinct performance disparities: Generation tasks are notably more difficult for LLMs than Conversion tasks, and generating correct visual content (StructEval-V) proves more challenging than generating text-only structures (StructEval-T). Furthermore, the evaluation framework itself demonstrated high reliability, with VQA quality control indicating 88.92% fairness and a VLM judge accuracy of 88.66%.

StructEval provides a standardized, holistic framework for evaluating LLMs beyond traditional natural language metrics, filling a vital gap in the assessment of structured and code generation capabilities. By establishing clear baselines and identifying specific failure modes in current SOTA modelsâ€”particularly regarding visual fidelity and generative tasksâ€”this benchmark offers a roadmap for future model development.

---

## Key Findings

*   **Performance Gap:** State-of-the-art models like **o1-mini** achieve a modest average score of **75.58**, while open-source alternatives trail by approximately **10 points**.
*   **Task Difficulty:** **Generation tasks** (converting natural language to structure) are significantly more challenging for LLMs than **Conversion tasks** (translating between structures).
*   **Modality Challenges:** Generating correct **visual content** is harder than generating **text-only structures**.
*   **Structural Fidelity:** Current models struggle to maintain structural fidelity across the wide diversity of formats tested.

---

## Methodology

The researchers utilized the **StructEval** benchmark to conduct a holistic evaluation of LLM capabilities regarding structured outputs.

*   **Scope:** Evaluated structural fidelity across **18 formats** and **44 task types**.
*   **Categorization:**
    *   **Non-renderable:** JSON, YAML, CSV.
    *   **Renderable:** HTML, React, SVG.
*   **Testing Methodologies:**
    *   **Generation Tasks:** Converting natural language inputs into structured outputs.
    *   **Conversion Tasks:** Translating between different structured formats.
*   **Evaluation Metrics:** Development of novel metrics specifically designed to quantify **format adherence** and **structural correctness**.

---

## Contributions

1.  **New Benchmark:** Introduction of **StructEval**, a standardized benchmark specifically for evaluating LLM capabilities in generating structured outputs.
2.  **Holistic Framework:** A comprehensive evaluation framework that spans 18 formats and assesses both generative and conversion capabilities.
3.  **Novel Metrics:** Development of new evaluation metrics to accurately quantify format adherence and structural correctness.
4.  **Empirical Analysis:** Baseline analysis revealing specific limitations of current SOTA and open-source models, particularly the contrast between generation vs. conversion and visual vs. text generation.

---

## Technical Details

### Benchmark Architecture
*   **StructEval-T:** Focuses on text-only structures (e.g., JSON, XML).
*   **StructEval-V:** Focuses on visual outputs (e.g., HTML, React).
*   **Scale:** Covers 21 formats and 44 task types across **2,035 examples**.

### Data Representation & Pipeline
*   **Representation:** Data is formally represented as a triplet: **$(q, K, Q_v)$**.
*   **Annotation Process:** A rigorous three-stage pipeline:
    1.  Task Curation.
    2.  LLM-based synthesis (utilizing GPT-4.1).
    3.  Two-pass expert review.

### Evaluation Framework
The framework combines three distinct validation methods:
1.  **Syntactic Validity Checking:** Ensures the code follows the grammar of the format.
2.  **Keyword Matching:** Utilizes Path-based Evaluation with **6 rule types** to verify text elements.
3.  **Visual Question Answering (VQA):** Employs Vision Language Models (VLMs) to assess visual fidelity.

---

## Results

*   **Dataset Density:** The dataset comprises 2,035 examples with an average of **14.7 keywords** and **8.5 VQA pairs** per example.
*   **Subset Breakdown:**
    *   **SE-T-conv:** The largest subset with 700 examples.
    *   **SE-V-conv:** The highest VQA density (9.0).
*   **Quality Control:**
    *   **VQA Fairness:** 88.92%.
    *   **VLM Judge Accuracy:** 88.66% overall (rising to 98.30% on fair questions).
*   **Model Performance:**
    *   **SOTA (o1-mini):** 75.58 average score.
    *   **Open-Source Gap:** Approximately 10 points behind SOTA.
*   **Analysis:** Confirmed that generation tasks are harder than conversion tasks, and visual content generation is more difficult than text-only structure generation.

---

**Document Quality Score:** 9/10
**References:** 25 citations