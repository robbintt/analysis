# Bridging Vision Language Models and Symbolic Grounding for Video Question Answering

*Haodi Ma; Vyom Pathak; Daisy Zhe Wang*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Framework** | SG-VLM (Symbolic Graph-Vision Language Model) |
| **Key Benchmarks** | NExT-QA, iVQA, ActivityNet-QA |
| **Base Models** | QwenVL, InternVL |
| **Core Innovation** | Integration of Symbolic Scene Graphs with frozen VLMs |

---

> ## üìù Executive Summary
>
> Current Vision Language Models (VLMs) achieve remarkable success in surface-level visual recognition but encounter significant limitations in Video Question Answering (VQA) when tasked with deep semantic reasoning or precise temporal grounding. These models typically rely on shallow statistical correlations rather than robust causal logic, rendering them ineffective at answering complex "why" and "how" questions that require an understanding of event dynamics. This deficiency is a critical barrier to the deployment of VLMs in dynamic, real-world environments where interpretability and accurate temporal localization are essential.
>
> To address these challenges, the researchers introduce **SG-VLM**, a modular neuro-symbolic framework that integrates Symbolic Scene Graphs (SGs) as intermediate grounding signals to bridge the gap between deep learning and symbolic AI. The methodology extracts structured object-relation representations from raw video data and fuses them with frozen VLM backbones‚Äîspecifically QwenVL and InternVL‚Äîvia a sophisticated prompting mechanism and visual localization techniques. Crucially, this approach preserves the generalization capabilities of the pre-trained VLMs by keeping their weights fixed while significantly enhancing their reasoning pipeline with explicit, interpretable symbolic structure.
>
> SG-VLM was rigorously evaluated across three standard benchmarks‚Äî**NExT-QA**, **ActivityNet-QA**, and **iVQA**‚Äîdelivering statistically significant gains over baselines. The data highlights a diminishing return phenomenon, where the weaker QwenVL model sees more substantial relative improvements compared to the more powerful InternVL, indicating that performance gains saturate as the sophistication of the base model increases.
>
> This work provides crucial empirical grounding for the neuro-symbolic AI paradigm, demonstrating that explicit symbolic structure effectively mitigates temporal and causal reasoning deficits inherent in deep learning models. Ultimately, SG-VLM establishes a viable, interpretable path toward more reliable video understanding systems that combine the perceptual power of VLMs with the logical precision of symbolic AI.

---

## üîë Key Findings

*   **Limitation of Current VLMs:** Recent Vision Language Models often rely on shallow correlations, leading to weak temporal grounding and interpretability in Video Question Answering (VQA).
*   **Efficacy of Symbolic Integration:** Integrating symbolic scene graphs (SGs) as intermediate grounding signals successfully improves VQA performance, particularly in causal and temporal reasoning tasks.
*   **Benchmark Superiority:** The proposed SG-VLM framework outperforms prior baselines across multiple benchmarks, including **NExT-QA**, **iVQA**, and **ActivityNet-QA**.
*   **Diminishing Returns:** The method shows constrained performance gains when applied to strong state-of-the-art (SOTA) VLMs, highlighting a ceiling effect in current symbolic integration techniques when applied to highly capable models.

---

## ‚öôÔ∏è Methodology

The researchers propose **SG-VLM**, a modular framework designed to bridge holistic deep learning approaches with symbolic AI. The technical approach includes:

*   **Symbolic Scene Graphs (SGs):** Utilized to provide structured object-relation representations acting as intermediate grounding signals.
*   **Frozen VLM Integration:** The framework connects frozen VLMs (specifically **QwenVL** and **InternVL**) with the scene graph grounding module without modifying the VLM's internal weights.
*   **Fusion Mechanism:** Implementation mechanisms employ advanced prompting and visual localization techniques to fuse symbolic scene graph data with the VLM‚Äôs processing pipeline.

---

## üõ†Ô∏è Technical Details

| Component | Description |
| :--- | :--- |
| **Framework Name** | SG-VLM (Symbolic Graph-Vision Language Model) |
| **Core Input** | Symbolic Scene Graphs (SGs) as intermediate grounding signals |
| **Primary Objective** | Address shallow correlations in VLMs by bridging vision-language models with symbolic data |
| **Specific Improvements** | Enhances causal reasoning and temporal reasoning capabilities while improving interpretability |

---

## üìà Performance Results

The framework demonstrated significant improvements across standard benchmarks. Notably, the relative gains were higher for the less capable base model (QwenVL) compared to the stronger SOTA model (InternVL), suggesting saturation effects.

### Accuracy Improvements by Benchmark

**NExT-QA**
*   **QwenVL:** 58.5% $\rightarrow$ **61.4%**
*   **InternVL:** 63.7% $\rightarrow$ **65.7%**

**ActivityNet-QA**
*   **QwenVL:** 38.6% $\rightarrow$ **44.3%**
*   **InternVL:** 39.4% $\rightarrow$ **43.4%**

**iVQA**
*   **QwenVL:** 44.2% $\rightarrow$ **47.8%**
*   **InternVL:** 52.5% $\rightarrow$ **54.4%**

---

## ‚ú® Contributions

*   **Novel Architecture:** Introduction of SG-VLM, a unique architecture that successfully combines the complementary strengths of frozen VLMs and symbolic scene graphs.
*   **Mitigation of Weaknesses:** Demonstrated that explicit symbolic grounding effectively mitigates specific weaknesses in VLMs, particularly regarding causal and temporal reasoning.
*   **Rigorous Evaluation:** Provided a comprehensive evaluation of this hybrid approach across three distinct benchmarks using multiple state-of-the-art VLM backbones.
*   **Nuanced Industry View:** Offered a realistic perspective on the field by identifying both the potential and the current boundaries of symbolic grounding, specifically noting performance saturation with powerful base models.