---
title: Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?
arxiv_id: '2502.20635'
source_url: https://arxiv.org/abs/2502.20635
generated_at: '2026-01-27T23:20:00'
quality_score: 2
citation_count: 38
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Can LLM Assist in the Evaluation of the Quality of Machine Learning Explanations?

*Additional Key, Data Science, Learning Explanations, Technology Sydney*

***

## ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 2/10 |
| **Total References** | 38 Citations |
| **LLM Judges** | GPT-4o, Mistral-7.2B |
| **Target Task** | Iris Classification |
| **Participants** | 38 (LLM instances & Human judges) |
| **XML Methods** | LIME, Similarity-based, No-explanation baseline |
| **Evaluation Metrics** | Accuracy, 5-point Likert Scale |

***

## Executive Summary

This research addresses the persistent challenge of evaluating the quality of machine learning explanations (XML). Current evaluation metrics often fail to capture the perceived usefulness of an explanation to a human user, while accurate human-in-the-loop studies are resource-intensive and difficult to scale. This problem is critical because robust, human-aligned evaluation methods are essential for the development of trustworthy, interpretable AI systems that can be effectively deployed in high-stakes environments.

The core innovation proposed is the use of Large Language Models (LLMs) to act as automated judges for explanation quality. Technically, the study utilizes Transformer-based architectures with multi-headed self-attentionâ€”specifically GPT-4o and Mistral-7.2Bâ€”to evaluate explanations generated by standard methods such as LIME and Similarity-based techniques. Conducted as a forward simulation on the Iris classification task, the study incorporates a "no-explanation" baseline and compares the evaluations of 38 participants, comprising both LLM instances and human judges, within a Human-grounded evaluation framework.

The study evaluates performance using a dual-metric approach: objective Accuracy scores and subjective 5-point Likert scales across five specific statements regarding explanation quality. The primary experimental outcome involves a correlation analysis between the LLM-based judges (GPT-4o and Mistral-7.2B) and the human judges. This analysis determines the degree of alignment between automated and human assessments, providing specific data on how well different LLMs can mimic human evaluation patterns for the Iris classification task.

The significance of this work lies in its potential to streamline the validation of Explainable AI (XAI) systems by replacing costly manual user studies with scalable LLM-based evaluation. By demonstrating that models like GPT-4o and Mistral-7.2B can correlate with human judgment, the research provides a pathway for more efficient, large-scale assessment of XML methods. This shift could significantly lower the barriers to validating interpretable models, ensuring that future AI systems are assessed for human-centric utility without the prohibitive logistical costs of traditional user studies.

***

## Key Research Findings

*   **Automated LLM Judges:** The study proposes using LLMs as automated evaluators to assess the quality of machine learning explanations, potentially solving the scalability issues of human-in-the-loop evaluations.
*   **Architectural Comparison:** Evaluation includes specific Transformer architectures (GPT-4o and Mistral-7.2B) to test their ability to judge explanation quality against human standards.
*   **Methodology Scope:** The research tests XML methods generated by LIME and Similarity-based approaches against a no-explanation baseline using the Iris dataset.
*   **Metric Alignment:** Key findings focus on the correlation between LLM-generated scores and human scores on a 5-point Likert scale, aiming to validate the LLMs as proxies for human judgment.

***

## Methodology

The study employs a **forward simulation** methodology designed to replicate human evaluation processes using synthetic agents. The framework involves:

*   **Human-grounded Evaluation:** The core framework relies on comparisons derived from human judgment data to benchmark automated methods.
*   **Participant Pool:** A total of 38 participants were analyzed, consisting of both human judges and LLM instances.
*   **Constraint Handling:** The analysis indicates that the original Abstract text was partially missing; the methodology described here is synthesized from the available technical details and executive summary provided in the source text.

***

## Technical Details

### System Architecture
*   **LLM Judges:** GPT-4o and Mistral-7.2B.
*   **Core Technology:** Transformer architectures utilizing multi-headed self-attention mechanisms.

### Target & Methods
*   **Target Task:** Iris classification.
*   **XML Methods Evaluated:**
    *   LIME (Local Interpretable Model-agnostic Explanations)
    *   Similarity-based methods
    *   No-explanation baseline (control)

### Experimental Design
*   **Type:** Forward simulation.
*   **Comparative Analysis:** Correlation analysis between LLM-based judges and Human judges.
*   **Participants:** 38 (Mix of LLM instances and Humans).

***

## Results

While specific quantitative results were not detailed in the provided text, the study defines the following evaluation structure:

*   **Objective Metric:** Accuracy scores.
*   **Subjective Metric:** A 5-point Likert Scale used to rate five specific subjective statements regarding explanation quality.
*   **Primary Comparison:** Correlation analysis is used to measure the alignment between LLM judges (GPT-4o, Mistral-7.2B) and human judges.
*   **Evaluation Focus:** Determining how different judges evaluate explanation quality and specifically how well LLMs compare to human evaluation patterns.

***

## Contributions

The primary contribution of this work is the investigation into whether LLMs can serve as reliable, scalable proxies for human judgment in the evaluation of XAI systems. By validating the correlation between LLM and human assessments, the research offers a pathway to:
1.  Reduce the cost and time associated with user studies.
2.  Enable large-scale validation of interpretable models.
3.  Ensure XML methods are assessed for human-centric utility effectively.

***

*Note: This report was generated based on an analysis where the original abstract text was noted as missing.*