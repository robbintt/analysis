# Histogram-Equalized Quantization for logic-gated Residual Neural Networks

*Van Thien Nguyen; William Guicquero; Gilles Sicard*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 7/10
> *   **References:** 40 Citations
> *   **Best Accuracy (CIFAR-10):** 84.17%
> *   **Key Datasets:** CIFAR-10, STL-10
> *   **Novelty:** Histogram-Equalized Quantization (HEQ)

---

## Executive Summary

**Problem**
Deploying deep learning models on resource-constrained edge hardware requires aggressive quantization to minimize memory and computational demands. However, standard low-precision quantization methods frequently incur accuracy degradation due to sub-optimal quantization thresholds, particularly within non-standard architectures designed specifically for hardware efficiency. The core challenge lies in optimizing the trade-off between weight precision and computational efficiency in architectures that eschew traditional multiply-accumulate (MAC) operations in favor of simpler logic gates, where maintaining high accuracy is difficult without sophisticated quantization strategies.

**Innovation**
The authors introduce Histogram-Equalized Quantization (HEQ), a novel Quantization-Aware Training (QAT) framework designed to optimize linear symmetric quantization. HEQ adaptively determines quantization thresholds and step sizes by analyzing layer-wise weight histograms and utilizing n-quantiles. This mechanism dynamically adjusts parameters based on the data distribution or loss characteristics, ensuring optimal precision retention. This quantization technique is applied to Logic-Gated Residual Networksâ€”specifically ORNet-11 and MUXORNet-11â€”which replace standard arithmetic operations with OR and 2-input MUX logic gates. By relying on thresholded bitcounts for computation rather than power-intensive multipliers, these architectures drastically reduce hardware complexity while the HEQ method ensures quantization does not become a performance bottleneck.

**Results**
Experimental validation demonstrates that the HEQ-driven MUXORNet-11 model achieved **84.17% accuracy** on the CIFAR-10 dataset, establishing a new state-of-the-art result. This performance outperforms the Learned Step Size Quantization (LSQ) baseline of 83.80% by +0.37%, as well as the plain baseline of 83.3%; additionally, ORNet-11 achieved 83.8%. On the STL-10 dataset, the authors demonstrated that the proposed networks achieve higher accuracy compared to previous works, with MUXORNet-11 reaching 75.18%. Crucially, these performance gains were achieved with operational efficiency defined by the replacement of floating-point multipliers with binary logic gates. This shift to thresholded bitcounts significantly reduces the computational cost per layer, offering marked hardware gains in terms of energy efficiency and area usage.

**Impact**
This research significantly advances the field of efficient deep learning by validating that logic-gated architectures can rival or exceed the performance of standard quantized neural networks. By establishing new benchmarks on CIFAR-10 and outperforming previous works on STL-10, the authors demonstrate that replacing complex arithmetic with simple Boolean logic (OR and MUX gates) does not necessitate a sacrifice in accuracy when paired with an intelligent quantization strategy like HEQ. This work paves the way for deploying highly capable neural networks on ultra-low-power edge devices, offering a superior balance between software-level accuracy and hardware-level implementation costs.

---

## Key Findings

*   **State-of-the-art Accuracy:** The proposed Histogram-Equalized Quantization (HEQ) method achieves top-tier accuracy on the CIFAR-10 dataset.
*   **Logic-Gated Success:** HEQ enables the successful training of logic-gated residual networks utilizing OR and MUX gates.
*   **Strong Generalization:** Experiments on the STL-10 dataset demonstrate higher accuracy compared to previous works.
*   **Hardware Efficiency:** The logic-gated residual networks achieve these improvements while operating at significantly lower hardware complexity.

---

## Methodology

The authors introduce **Histogram-Equalized Quantization (HEQ)**, an adaptive framework designed for linear symmetric quantization.

*   **Core Mechanism:** The method automatically adapts quantization thresholds through a unique step size optimization process.
*   **Adaptive Adjustment:** This approach adjusts quantization parameters based on specific data or model loss characteristics rather than using static values.
*   **Application:** The methodology involves applying HEQ to residual neural networks that have been specifically integrated with logic gates (OR and MUX) to replace standard arithmetic operations.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Method Name** | Histogram-Equalized Quantization (HEQ) |
| **Training Type** | Quantization-Aware Training (QAT) |
| **Optimization** | Equalizes layer-wise weight histograms via adaptive step sizes and n-quantiles. |
| **Baselines** | Compared against Learned Step Size Quantization (LSQ). |
| **Architecture 1** | **ORNet-11:** Uses OR gates; relies on thresholded bitcounts. |
| **Architecture 2** | **MUXORNet-11:** Uses 2-input MUX and OR gates; relies on thresholded bitcounts. |

---

## Contributions

1.  **Novel Quantization Method:** Introduction of HEQ as a new method for optimizing linear symmetric quantization via step size optimization.
2.  **Architecture Proposal:** The proposal and successful training of logic-gated residual networks using OR and MUX logic gates.
3.  **Benchmark Performance:** A demonstrated improvement in the trade-off between accuracy and hardware complexity, setting new benchmarks on CIFAR-10 and STL-10.

---

## Results

**CIFAR-10 Dataset:**
*   **Plain Baseline:** 83.3% accuracy
*   **ORNet-11:** 83.8% accuracy
*   **MUXORNet-11:** 84.17% accuracy
    *   **Performance:** Outperformed the LSQ baseline by +0.37%.

**STL-10 Dataset:**
*   Proposed networks demonstrated **higher accuracy** compared to previous works.

**Efficiency Metrics:**
*   The proposed models achieved these results with **fewer parameters** and **lower precision** than the LSQ baseline.
*   The solution incurs negligible hardware overhead while significantly reducing overall complexity.